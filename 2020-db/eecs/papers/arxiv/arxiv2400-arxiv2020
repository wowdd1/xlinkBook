arxiv-1304-2266 | Synaptic Scaling Balances Learning in a Spiking Model of Neocortex | http://arxiv.org/abs/1304.2266 | id:1304.2266 author:Mark Rowan, Samuel Neymotin category:q-bio.NC cs.NE  published:2013-04-08 summary:Learning in the brain requires complementary mechanisms: potentiation and activity-dependent homeostatic scaling. We introduce synaptic scaling to a biologically-realistic spiking model of neocortex which can learn changes in oscillatory rhythms using STDP, and show that scaling is necessary to balance both positive and negative changes in input from potentiation and atrophy. We discuss some of the issues that arise when considering synaptic scaling in such a model, and show that scaling regulates activity whilst allowing learning to remain unaltered. version:1
arxiv-1303-7093 | Relevance As a Metric for Evaluating Machine Learning Algorithms | http://arxiv.org/abs/1303.7093 | id:1303.7093 author:Aravind Kota Gopalakrishna, Tanir Ozcelebi, Antonio Liotta, Johan J. Lukkien category:stat.ML cs.LG  published:2013-03-28 summary:In machine learning, the choice of a learning algorithm that is suitable for the application domain is critical. The performance metric used to compare different algorithms must also reflect the concerns of users in the application domain under consideration. In this work, we propose a novel probability-based performance metric called Relevance Score for evaluating supervised learning algorithms. We evaluate the proposed metric through empirical analysis on a dataset gathered from an intelligent lighting pilot installation. In comparison to the commonly used Classification Accuracy metric, the Relevance Score proves to be more appropriate for a certain class of applications. version:3
arxiv-1303-7390 | Geometric tree kernels: Classification of COPD from airway tree geometry | http://arxiv.org/abs/1303.7390 | id:1303.7390 author:Aasa Feragen, Jens Petersen, Dominik Grimm, Asger Dirksen, Jesper Holst Pedersen, Karsten Borgwardt, Marleen de Bruijne category:cs.CV 68T10  published:2013-03-29 summary:Methodological contributions: This paper introduces a family of kernels for analyzing (anatomical) trees endowed with vector valued measurements made along the tree. While state-of-the-art graph and tree kernels use combinatorial tree/graph structure with discrete node and edge labels, the kernels presented in this paper can include geometric information such as branch shape, branch radius or other vector valued properties. In addition to being flexible in their ability to model different types of attributes, the presented kernels are computationally efficient and some of them can easily be computed for large datasets (N of the order 10.000) of trees with 30-600 branches. Combining the kernels with standard machine learning tools enables us to analyze the relation between disease and anatomical tree structure and geometry. Experimental results: The kernels are used to compare airway trees segmented from low-dose CT, endowed with branch shape descriptors and airway wall area percentage measurements made along the tree. Using kernelized hypothesis testing we show that the geometric airway trees are significantly differently distributed in patients with Chronic Obstructive Pulmonary Disease (COPD) than in healthy individuals. The geometric tree kernels also give a significant increase in the classification accuracy of COPD from geometric tree structure endowed with airway wall thickness measurements in comparison with state-of-the-art methods, giving further insight into the relationship between airway wall thickness and COPD. Software: Software for computing kernels and statistical tests is available at http://image.diku.dk/aasa/software.php. version:2
arxiv-1212-4794 | Parsimonious module inference in large networks | http://arxiv.org/abs/1212.4794 | id:1212.4794 author:Tiago P. Peixoto category:physics.data-an physics.soc-ph stat.ML  published:2012-12-19 summary:We investigate the detectability of modules in large networks when the number of modules is not known in advance. We employ the minimum description length (MDL) principle which seeks to minimize the total amount of information required to describe the network, and avoid overfitting. According to this criterion, we obtain general bounds on the detectability of any prescribed block structure, given the number of nodes and edges in the sampled network. We also obtain that the maximum number of detectable blocks scales as $\sqrt{N}$, where $N$ is the number of nodes in the network, for a fixed average degree $<k>$. We also show that the simplicity of the MDL approach yields an efficient multilevel Monte Carlo inference algorithm with a complexity of $O(\tau N\log N)$, if the number of blocks is unknown, and $O(\tau N)$ if it is known, where $\tau$ is the mixing time of the Markov chain. We illustrate the application of the method on a large network of actors and films with over $10^6$ edges, and a dissortative, bipartite block structure. version:4
arxiv-1304-2133 | Dynamic Amelioration of Resolution Mismatches for Local Feature Based Identity Inference | http://arxiv.org/abs/1304.2133 | id:1304.2133 author:Yongkang Wong, Conrad Sanderson, Sandra Mau, Brian C. Lovell category:cs.CV cs.IR I.5.4; I.4  published:2013-04-08 summary:While existing face recognition systems based on local features are robust to issues such as misalignment, they can exhibit accuracy degradation when comparing images of differing resolutions. This is common in surveillance environments where a gallery of high resolution mugshots is compared to low resolution CCTV probe images, or where the size of a given image is not a reliable indicator of the underlying resolution (eg. poor optics). To alleviate this degradation, we propose a compensation framework which dynamically chooses the most appropriate face recognition system for a given pair of image resolutions. This framework applies a novel resolution detection method which does not rely on the size of the input images, but instead exploits the sensitivity of local features to resolution using a probabilistic multi-region histogram approach. Experiments on a resolution-modified version of the "Labeled Faces in the Wild" dataset show that the proposed resolution detector frontend obtains a 99% average accuracy in selecting the most appropriate face recognition system, resulting in higher overall face discrimination accuracy (across several resolutions) compared to the individual baseline face recognition systems. version:1
arxiv-1304-2109 | Automatic Fingerprint Recognition Using Minutiae Matching Technique for the Large Fingerprint Database | http://arxiv.org/abs/1304.2109 | id:1304.2109 author:S. M. Mohsen, S. M. Zamshed Farhan, M. M. A. Hashem category:cs.CV  published:2013-04-08 summary:Extracting minutiae from fingerprint images is one of the most important steps in automatic fingerprint identification system. Because minutiae matching are certainly the most well-known and widely used method for fingerprint matching, minutiae are local discontinuities in the fingerprint pattern. In this paper a fingerprint matching algorithm is proposed using some specific feature of the minutiae points, also the acquired fingerprint image is considered by minimizing its size by generating a corresponding fingerprint template for a large fingerprint database. The results achieved are compared with those obtained through some other methods also shows some improvement in the minutiae detection process in terms of memory and time required. version:1
arxiv-1205-0793 | A powerful and efficient set test for genetic markers that handles confounders | http://arxiv.org/abs/1205.0793 | id:1205.0793 author:Jennifer Listgarten, Christoph Lippert, Eun Yong Kang, Jing Xiang, Carl M. Kadie, David Heckerman category:q-bio.GN stat.AP stat.ML  published:2012-05-03 summary:Approaches for testing sets of variants, such as a set of rare or common variants within a gene or pathway, for association with complex traits are important. In particular, set tests allow for aggregation of weak signal within a set, can capture interplay among variants, and reduce the burden of multiple hypothesis testing. Until now, these approaches did not address confounding by family relatedness and population structure, a problem that is becoming more important as larger data sets are used to increase power. Results: We introduce a new approach for set tests that handles confounders. Our model is based on the linear mixed model and uses two random effects-one to capture the set association signal and one to capture confounders. We also introduce a computational speedup for two-random-effects models that makes this approach feasible even for extremely large cohorts. Using this model with both the likelihood ratio test and score test, we find that the former yields more power while controlling type I error. Application of our approach to richly structured GAW14 data demonstrates that our method successfully corrects for population structure and family relatedness, while application of our method to a 15,000 individual Crohn's disease case-control cohort demonstrates that it additionally recovers genes not recoverable by univariate analysis. Availability: A Python-based library implementing our approach is available at http://mscompbio.codeplex.com version:3
arxiv-1304-2097 | Solving Linear Equations by Classical Jacobi-SR Based Hybrid Evolutionary Algorithm with Uniform Adaptation Technique | http://arxiv.org/abs/1304.2097 | id:1304.2097 author:R. M. Jalal Uddin Jamali, M. M. A. Hashem, M. Mahfuz Hasan, Md. Bazlar Rahman category:cs.NE cs.NA  published:2013-04-08 summary:Solving a set of simultaneous linear equations is probably the most important topic in numerical methods. For solving linear equations, iterative methods are preferred over the direct methods especially when the coefficient matrix is sparse. The rate of convergence of iteration method is increased by using Successive Relaxation (SR) technique. But SR technique is very much sensitive to relaxation factor, {\omega}. Recently, hybridization of classical Gauss-Seidel based successive relaxation technique with evolutionary computation techniques have successfully been used to solve large set of linear equations in which relaxation factors are self-adapted. In this paper, a new hybrid algorithm is proposed in which uniform adaptive evolutionary computation techniques and classical Jacobi based SR technique are used instead of classical Gauss-Seidel based SR technique. The proposed Jacobi-SR based uniform adaptive hybrid algorithm, inherently, can be implemented in parallel processing environment efficiently. Whereas Gauss-Seidel-SR based hybrid algorithms cannot be implemented in parallel computing environment efficiently. The convergence theorem and adaptation theorem of the proposed algorithm are proved theoretically. And the performance of the proposed Jacobi-SR based uniform adaptive hybrid evolutionary algorithm is compared with Gauss-Seidel-SR based uniform adaptive hybrid evolutionary algorithm as well as with both classical Jacobi-SR method and Gauss-Seidel-SR method in the experimental domain. The proposed Jacobi-SR based hybrid algorithm outperforms the Gauss-Seidel-SR based hybrid algorithm as well as both classical Jacobi-SR method and Gauss-Seidel-SR method in terms of convergence speed and effectiveness. version:1
arxiv-1304-2014 | Image Compression predicated on Recurrent Iterated Function Systems | http://arxiv.org/abs/1304.2014 | id:1304.2014 author:Chol-Hui Yun, W. Metzler, M. Barski category:math.DS cs.CV math.GT  published:2013-04-07 summary:Recurrent iterated function systems (RIFSs) are improvements of iterated function systems (IFSs) using elements of the theory of Marcovian stochastic processes which can produce more natural looking images. We construct new RIFSs consisting substantially of a vertical contraction factor function and nonlinear transformations. These RIFSs are applied to image compression. version:1
arxiv-1304-1972 | Facial transformations of ancient portraits: the face of Caesar | http://arxiv.org/abs/1304.1972 | id:1304.1972 author:Amelia Carolina Sparavigna category:cs.CV  published:2013-04-07 summary:Some software solutions used to obtain the facial transformations can help investigating the artistic metamorphosis of the ancient portraits of the same person. An analysis with a freely available software of portraitures of Julius Caesar is proposed, showing his several "morphs". The software helps enhancing the mood the artist added to a portrait. version:1
arxiv-1304-1930 | Client-Driven Content Extraction Associated with Table | http://arxiv.org/abs/1304.1930 | id:1304.1930 author:K. C. Santosh, Abdel Belaïd category:cs.CV cs.IR  published:2013-04-06 summary:The goal of the project is to extract content within table in document images based on learnt patterns. Real-world users i.e., clients first provide a set of key fields within the table which they think are important. These are first used to represent the graph where nodes are labelled with semantics including other features and edges are attributed with relations. Attributed relational graph (ARG) is then employed to mine similar graphs from a document image. Each mined graph will represent an item within the table, and hence a set of such graphs will compose a table. We have validated the concept by using a real-world industrial problem. version:1
arxiv-1109-4928 | RPA: Probabilistic analysis of probe performance and robust summarization | http://arxiv.org/abs/1109.4928 | id:1109.4928 author:Leo Lahti, Laura L. Elo, Tero Aittokallio, Samuel Kaski category:cs.CE stat.AP stat.ML  published:2011-09-22 summary:Probe-level models have led to improved performance in microarray studies but the various sources of probe-level contamination are still poorly understood. Data-driven analysis of probe performance can be used to quantify the uncertainty in individual probes and to highlight the relative contribution of different noise sources. Improved understanding of the probe-level effects can lead to improved preprocessing techniques and microarray design. We have implemented probabilistic tools for probe performance analysis and summarization on short oligonucleotide arrays. In contrast to standard preprocessing approaches, the methods provide quantitative estimates of probe-specific noise and affinity terms and tools to investigate these parameters. Tools to incorporate prior information of the probes in the analysis are provided as well. Comparisons to known probe-level error sources and spike-in data sets validate the approach. Implementation is freely available in R/BioConductor: http://www.bioconductor.org/packages/release/bioc/html/RPA.html version:2
arxiv-1304-1677 | Bug Classification: Feature Extraction and Comparison of Event Model using Naïve Bayes Approach | http://arxiv.org/abs/1304.1677 | id:1304.1677 author:Sunil Joy Dommati, Ruchi Agrawal, Ram Mohana Reddy G., S. Sowmya Kamath category:cs.SE cs.IR cs.LG  published:2013-04-05 summary:In software industries, individuals at different levels from customer to an engineer apply diverse mechanisms to detect to which class a particular bug should be allocated. Sometimes while a simple search in Internet might help, in many other cases a lot of effort is spent in analyzing the bug report to classify the bug. So there is a great need of a structured mining algorithm - where given a crash log, the existing bug database could be mined to find out the class to which the bug should be allocated. This would involve Mining patterns and applying different classification algorithms. This paper focuses on the feature extraction, noise reduction in data and classification of network bugs using probabilistic Na\"ive Bayes approach. Different event models like Bernoulli and Multinomial are applied on the extracted features. When new, unseen bugs are given as input to the algorithms, the performance comparison of different algorithms is done on the basis of accuracy and recall parameters. version:1
arxiv-1208-3822 | Joint-ViVo: Selecting and Weighting Visual Words Jointly for Bag-of-Features based Tissue Classification in Medical Images | http://arxiv.org/abs/1208.3822 | id:1208.3822 author:Jingyan Wang category:cs.CV stat.ML  published:2012-08-19 summary:Automatically classifying the tissues types of Region of Interest (ROI) in medical imaging has been an important application in Computer-Aided Diagnosis (CAD), such as classification of breast parenchymal tissue in the mammogram, classify lung disease patterns in High-Resolution Computed Tomography (HRCT) etc. Recently, bag-of-features method has shown its power in this field, treating each ROI as a set of local features. In this paper, we investigate using the bag-of-features strategy to classify the tissue types in medical imaging applications. Two important issues are considered here: the visual vocabulary learning and weighting. Although there are already plenty of algorithms to deal with them, all of them treat them independently, namely, the vocabulary learned first and then the histogram weighted. Inspired by Auto-Context who learns the features and classifier jointly, we try to develop a novel algorithm that learns the vocabulary and weights jointly. The new algorithm, called Joint-ViVo, works in an iterative way. In each iteration, we first learn the weights for each visual word by maximizing the margin of ROI triplets, and then select the most discriminate visual words based on the learned weights for the next iteration. We test our algorithm on three tissue classification tasks: identifying brain tissue type in magnetic resonance imaging (MRI), classifying lung tissue in HRCT images, and classifying breast tissue density in mammograms. The results show that Joint-ViVo can perform effectively for classifying tissues. version:2
arxiv-1304-1574 | Generalization Bounds for Domain Adaptation | http://arxiv.org/abs/1304.1574 | id:1304.1574 author:Chao Zhang, Lei Zhang, Jieping Ye category:cs.LG math.PR  published:2013-04-04 summary:In this paper, we provide a new framework to obtain the generalization bounds of the learning process for domain adaptation, and then apply the derived bounds to analyze the asymptotical convergence of the learning process. Without loss of generality, we consider two kinds of representative domain adaptation: one is with multiple sources and the other is combining source and target data. In particular, we use the integral probability metric to measure the difference between two domains. For either kind of domain adaptation, we develop a related Hoeffding-type deviation inequality and a symmetrization inequality to achieve the corresponding generalization bound based on the uniform entropy number. We also generalized the classical McDiarmid's inequality to a more general setting where independent random variables can take values from different domains. By using this inequality, we then obtain generalization bounds based on the Rademacher complexity. Afterwards, we analyze the asymptotic convergence and the rate of convergence of the learning process for such kind of domain adaptation. Meanwhile, we discuss the factors that affect the asymptotic behavior of the learning process and the numerical experiments support our theoretical findings as well. version:1
arxiv-1304-1571 | Hiding Image in Image by Five Modulus Method for Image Steganography | http://arxiv.org/abs/1304.1571 | id:1304.1571 author:Firas A. Jassim category:cs.MM cs.CV  published:2013-04-04 summary:This paper is to create a practical steganographic implementation to hide color image (stego) inside another color image (cover). The proposed technique uses Five Modulus Method to convert the whole pixels within both the cover and the stego images into multiples of five. Since each pixels inside the stego image is divisible by five then the whole stego image could be divided by five to get new range of pixels 0..51. Basically, the reminder of each number that is not divisible by five is either 1,2,3 or 4 when divided by 5. Subsequently, then a 4-by-4 window size has been implemented to accommodate the proposed technique. For each 4-by-4 window inside the cover image, a number from 1 to 4 could be embedded secretly from the stego image. The previous discussion must be applied separately for each of the R, G, and B arrays. Moreover, a stego-key could be combined with the proposed algorithm to make it difficult for any adversary to extract the secret image from the cover image. Based on the PSNR value, the extracted stego image has high PSNR value. Hence this new steganography algorithm is very efficient to hide color images. version:1
arxiv-1304-1568 | Multiscale Fractal Descriptors Applied to Texture Classification | http://arxiv.org/abs/1304.1568 | id:1304.1568 author:João Batista Florindo, Odemir Martinez Bruno category:cs.CV  published:2013-04-04 summary:This work proposes the combination of multiscale transform with fractal descriptors employed in the classification of gray-level texture images. We apply the space-scale transform (derivative + Gaussian filter) over the Bouligand-Minkowski fractal descriptors, followed by a threshold over the filter response, aiming at attenuating noise effects caused by the final part of this response. The method is tested in the classification of a well-known data set (Brodatz) and compared with other classical texture descriptor techniques. The results demonstrate the advantage of the proposed approach, achieving a higher success rate with a reduced amount of descriptors. version:1
arxiv-1304-1419 | Integration of spatio-temporal contrast sensitivity with a multi-slice channelized Hotelling observer | http://arxiv.org/abs/1304.1419 | id:1304.1419 author:Ali N. Avanaki, Kathryn S. Espig, Cedric Marchessoux, Elizabeth A. Krupinski, Predrag R. Bakic, Tom R. L. Kimpe, Andrew D. A. Maidment category:cs.CV  published:2013-04-04 summary:Barten's model of spatio-temporal contrast sensitivity function of human visual system is embedded in a multi-slice channelized Hotelling observer. This is done by 3D filtering of the stack of images with the spatio-temporal contrast sensitivity function and feeding the result (i.e., the perceived image stack) to the multi-slice channelized Hotelling observer. The proposed procedure of considering spatio-temporal contrast sensitivity function is generic in the sense that it can be used with observers other than multi-slice channelized Hotelling observer. Detection performance of the new observer in digital breast tomosynthesis is measured in a variety of browsing speeds, at two spatial sampling rates, using computer simulations. Our results show a peak in detection performance in mid browsing speeds. We compare our results to those of a human observer study reported earlier (I. Diaz et al. SPIE MI 2011). The effects of display luminance, contrast and spatial sampling rate, with and without considering foveal vision, are also studied. Reported simulations are conducted with real digital breast tomosynthesis image stacks, as well as stacks from an anthropomorphic software breast phantom (P. Bakic et al. Med Phys. 2011). Lesion cases are simulated by inserting single micro-calcifications or masses. Limitations of our methods and ways to improve them are discussed. version:1
arxiv-1304-1408 | Restoration of Images Corrupted by Impulse Noise and Mixed Gaussian Impulse Noise using Blind Inpainting | http://arxiv.org/abs/1304.1408 | id:1304.1408 author:Ming Yan category:math.OC cs.CV math.NA  published:2013-04-04 summary:This article studies the problem of image restoration of observed images corrupted by impulse noise and mixed Gaussian impulse noise. Since the pixels damaged by impulse noise contain no information about the true image, how to find this set correctly is a very important problem. We propose two methods based on blind inpainting and $\ell_0$ minimization that can simultaneously find the damaged pixels and restore the image. By iteratively restoring the image and updating the set of damaged pixels, these methods have better performance than other methods, as shown in the experiments. In addition, we provide convergence analysis for these methods, these algorithms will converge to coordinatewise minimum points. In addition, they will converge to local minimum points (or with probability one) with some modifications in the algorithms. version:1
arxiv-1204-1259 | Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback | http://arxiv.org/abs/1204.1259 | id:1204.1259 author:Balázs Hidasi, Domonkos Tikk category:cs.LG cs.IR cs.NA  published:2012-04-05 summary:Albeit, the implicit feedback based recommendation problem - when only the user history is available but there are no ratings - is the most typical setting in real-world applications, it is much less researched than the explicit feedback case. State-of-the-art algorithms that are efficient on the explicit case cannot be straightforwardly transformed to the implicit case if scalability should be maintained. There are few if any implicit feedback benchmark datasets, therefore new ideas are usually experimented on explicit benchmarks. In this paper, we propose a generic context-aware implicit feedback recommender algorithm, coined iTALS. iTALS apply a fast, ALS-based tensor factorization learning method that scales linearly with the number of non-zero elements in the tensor. The method also allows us to incorporate diverse context information into the model while maintaining its computational efficiency. In particular, we present two such context-aware implementation variants of iTALS. The first incorporates seasonality and enables to distinguish user behavior in different time intervals. The other views the user history as sequential information and has the ability to recognize usage pattern typical to certain group of items, e.g. to automatically tell apart product types or categories that are typically purchased repetitively (collectibles, grocery goods) or once (household appliances). Experiments performed on three implicit datasets (two proprietary ones and an implicit variant of the Netflix dataset) show that by integrating context-aware information with our factorization framework into the state-of-the-art implicit recommender algorithm the recommendation quality improves significantly. version:2
arxiv-1304-1391 | Fast SVM training using approximate extreme points | http://arxiv.org/abs/1304.1391 | id:1304.1391 author:Manu Nandan, Pramod P. Khargonekar, Sachin S. Talathi category:cs.LG  published:2013-04-04 summary:Applications of non-linear kernel Support Vector Machines (SVMs) to large datasets is seriously hampered by its excessive training time. We propose a modification, called the approximate extreme points support vector machine (AESVM), that is aimed at overcoming this burden. Our approach relies on conducting the SVM optimization over a carefully selected subset, called the representative set, of the training dataset. We present analytical results that indicate the similarity of AESVM and SVM solutions. A linear time algorithm based on convex hulls and extreme points is used to compute the representative set in kernel space. Extensive computational experiments on nine datasets compared AESVM to LIBSVM \citep{LIBSVM}, CVM \citep{Tsang05}, BVM \citep{Tsang07}, LASVM \citep{Bordes05}, $\text{SVM}^{\text{perf}}$ \citep{Joachims09}, and the random features method \citep{rahimi07}. Our AESVM implementation was found to train much faster than the other methods, while its classification accuracy was similar to that of LIBSVM in all cases. In particular, for a seizure detection dataset, AESVM training was almost $10^3$ times faster than LIBSVM and LASVM and more than forty times faster than CVM and BVM. Additionally, AESVM also gave competitively fast classification times. version:1
arxiv-1212-1362 | Stochastic model for the vocabulary growth in natural languages | http://arxiv.org/abs/1212.1362 | id:1212.1362 author:Martin Gerlach, Eduardo G. Altmann category:physics.soc-ph cs.CL physics.data-an  published:2012-12-06 summary:We propose a stochastic model for the number of different words in a given database which incorporates the dependence on the database size and historical changes. The main feature of our model is the existence of two different classes of words: (i) a finite number of core-words which have higher frequency and do not affect the probability of a new word to be used; and (ii) the remaining virtually infinite number of noncore-words which have lower frequency and once used reduce the probability of a new word to be used in the future. Our model relies on a careful analysis of the google-ngram database of books published in the last centuries and its main consequence is the generalization of Zipf's and Heaps' law to two scaling regimes. We confirm that these generalizations yield the best simple description of the data among generic descriptive models and that the two free parameters depend only on the language but not on the database. From the point of view of our model the main change on historical time scales is the composition of the specific words included in the finite list of core-words, which we observe to decay exponentially in time with a rate of approximately 30 words per year for English. version:3
arxiv-1304-1262 | Classification of Human Epithelial Type 2 Cell Indirect Immunofluoresence Images via Codebook Based Descriptors | http://arxiv.org/abs/1304.1262 | id:1304.1262 author:Arnold Wiliem, Yongkang Wong, Conrad Sanderson, Peter Hobson, Shaokang Chen, Brian C. Lovell category:q-bio.CB cs.CV q-bio.QM  published:2013-04-04 summary:The Anti-Nuclear Antibody (ANA) clinical pathology test is commonly used to identify the existence of various diseases. A hallmark method for identifying the presence of ANAs is the Indirect Immunofluorescence method on Human Epithelial (HEp-2) cells, due to its high sensitivity and the large range of antigens that can be detected. However, the method suffers from numerous shortcomings, such as being subjective as well as time and labour intensive. Computer Aided Diagnostic (CAD) systems have been developed to address these problems, which automatically classify a HEp-2 cell image into one of its known patterns (eg., speckled, homogeneous). Most of the existing CAD systems use handpicked features to represent a HEp-2 cell image, which may only work in limited scenarios. In this paper, we propose a cell classification system comprised of a dual-region codebook-based descriptor, combined with the Nearest Convex Hull Classifier. We evaluate the performance of several variants of the descriptor on two publicly available datasets: ICPR HEp-2 cell classification contest dataset and the new SNPHEp-2 dataset. To our knowledge, this is the first time codebook-based descriptors are applied and studied in this domain. Experiments show that the proposed system has consistent high performance and is more robust than two recent CAD systems. version:1
arxiv-1304-1250 | Fast Approximate L_infty Minimization: Speeding Up Robust Regression | http://arxiv.org/abs/1304.1250 | id:1304.1250 author:Fumin Shen, Chunhua Shen, Rhys Hill, Anton van den Hengel, Zhenmin Tang category:cs.CV stat.CO  published:2013-04-04 summary:Minimization of the $L_\infty$ norm, which can be viewed as approximately solving the non-convex least median estimation problem, is a powerful method for outlier removal and hence robust regression. However, current techniques for solving the problem at the heart of $L_\infty$ norm minimization are slow, and therefore cannot scale to large problems. A new method for the minimization of the $L_\infty$ norm is presented here, which provides a speedup of multiple orders of magnitude for data with high dimension. This method, termed Fast $L_\infty$ Minimization, allows robust regression to be applied to a class of problems which were previously inaccessible. It is shown how the $L_\infty$ norm minimization problem can be broken up into smaller sub-problems, which can then be solved extremely efficiently. Experimental results demonstrate the radical reduction in computation time, along with robustness against large numbers of outliers in a few model-fitting problems. version:1
arxiv-1304-1233 | Shadow Detection: A Survey and Comparative Evaluation of Recent Methods | http://arxiv.org/abs/1304.1233 | id:1304.1233 author:Andres Sanin, Conrad Sanderson, Brian C. Lovell category:cs.CV cs.RO  published:2013-04-04 summary:This paper presents a survey and a comparative evaluation of recent techniques for moving cast shadow detection. We identify shadow removal as a critical step for improving object detection and tracking. The survey covers methods published during the last decade, and places them in a feature-based taxonomy comprised of four categories: chromacity, physical, geometry and textures. A selection of prominent methods across the categories is compared in terms of quantitative performance measures (shadow detection and discrimination rates, colour desaturation) as well as qualitative observations. Furthermore, we propose the use of tracking performance as an unbiased approach for determining the practical usefulness of shadow detection methods. The evaluation indicates that all shadow detection approaches make different contributions and all have individual strength and weaknesses. Out of the selected methods, the geometry-based technique has strict assumptions and is not generalisable to various environments, but it is a straightforward choice when the objects of interest are easy to model and their shadows have different orientation. The chromacity based method is the fastest to implement and run, but it is sensitive to noise and less effective in low saturated scenes. The physical method improves upon the accuracy of the chromacity method by adapting to local shadow models, but fails when the spectral properties of the objects are similar to that of the background. The small-region texture based method is especially robust for pixels whose neighbourhood is textured, but may take longer to implement and is the most computationally expensive. The large-region texture based method produces the most accurate results, but has a significant computational load due to its multiple processing steps. version:1
arxiv-1304-1209 | Highly comparative time-series analysis: The empirical structure of time series and their methods | http://arxiv.org/abs/1304.1209 | id:1304.1209 author:Ben D. Fulcher, Max A. Little, Nick S. Jones category:physics.data-an cs.CV physics.bio-ph q-bio.QM stat.ML  published:2013-04-03 summary:The process of collecting and organizing sets of observations represents a common theme throughout the history of science. However, despite the ubiquity of scientists measuring, recording, and analyzing the dynamics of different processes, an extensive organization of scientific time-series data and analysis methods has never been performed. Addressing this, annotated collections of over 35 000 real-world and model-generated time series and over 9000 time-series analysis algorithms are analyzed in this work. We introduce reduced representations of both time series, in terms of their properties measured by diverse scientific methods, and of time-series analysis methods, in terms of their behaviour on empirical time series, and use them to organize these interdisciplinary resources. This new approach to comparing across diverse scientific data and methods allows us to organize time-series datasets automatically according to their properties, retrieve alternatives to particular analysis methods developed in other scientific disciplines, and automate the selection of useful methods for time-series classification and regression tasks. The broad scientific utility of these tools is demonstrated on datasets of electroencephalograms, self-affine time series, heart beat intervals, speech signals, and others, in each case contributing novel analysis techniques to the existing literature. Highly comparative techniques that compare across an interdisciplinary literature can thus be used to guide more focused research in time-series analysis for applications across the scientific disciplines. version:1
arxiv-1304-1192 | Efficient Distance Metric Learning by Adaptive Sampling and Mini-Batch Stochastic Gradient Descent (SGD) | http://arxiv.org/abs/1304.1192 | id:1304.1192 author:Qi Qian, Rong Jin, Jinfeng Yi, Lijun Zhang, Shenghuo Zhu category:cs.LG  published:2013-04-03 summary:Distance metric learning (DML) is an important task that has found applications in many domains. The high computational cost of DML arises from the large number of variables to be determined and the constraint that a distance metric has to be a positive semi-definite (PSD) matrix. Although stochastic gradient descent (SGD) has been successfully applied to improve the efficiency of DML, it can still be computationally expensive because in order to ensure that the solution is a PSD matrix, it has to, at every iteration, project the updated distance metric onto the PSD cone, an expensive operation. We address this challenge by developing two strategies within SGD, i.e. mini-batch and adaptive sampling, to effectively reduce the number of updates (i.e., projections onto the PSD cone) in SGD. We also develop hybrid approaches that combine the strength of adaptive sampling with that of mini-batch online learning techniques to further improve the computational efficiency of SGD for DML. We prove the theoretical guarantees for both adaptive sampling and mini-batch based approaches for DML. We also conduct an extensive empirical study to verify the effectiveness of the proposed algorithms for DML. version:1
arxiv-1209-3330 | Predator confusion is sufficient to evolve swarming behavior | http://arxiv.org/abs/1209.3330 | id:1209.3330 author:Randal S. Olson, Arend Hintze, Fred C. Dyer, David B. Knoester, Christoph Adami category:q-bio.PE cs.NE nlin.AO q-bio.NC  published:2012-09-14 summary:Swarming behaviors in animals have been extensively studied due to their implications for the evolution of cooperation, social cognition, and predator-prey dynamics. An important goal of these studies is discerning which evolutionary pressures favor the formation of swarms. One hypothesis is that swarms arise because the presence of multiple moving prey in swarms causes confusion for attacking predators, but it remains unclear how important this selective force is. Using an evolutionary model of a predator-prey system, we show that predator confusion provides a sufficient selection pressure to evolve swarming behavior in prey. Furthermore, we demonstrate that the evolutionary effect of predator confusion on prey could in turn exert pressure on the structure of the predator's visual field, favoring the frontally oriented, high-resolution visual systems commonly observed in predators that feed on swarming animals. Finally, we provide evidence that when prey evolve swarming in response to predator confusion, there is a change in the shape of the functional response curve describing the predator's consumption rate as prey density increases. Thus, we show that a relatively simple perceptual constraint--predator confusion--could have pervasive evolutionary effects on prey behavior, predator sensory mechanisms, and the ecological interactions between predators and prey. version:3
arxiv-1301-5063 | Heteroscedastic Conditional Ordinal Random Fields for Pain Intensity Estimation from Facial Images | http://arxiv.org/abs/1301.5063 | id:1301.5063 author:Ognjen Rudovic, Maja Pantic, Vladimir Pavlovic category:cs.CV cs.LG stat.ML  published:2013-01-22 summary:We propose a novel method for automatic pain intensity estimation from facial images based on the framework of kernel Conditional Ordinal Random Fields (KCORF). We extend this framework to account for heteroscedasticity on the output labels(i.e., pain intensity scores) and introduce a novel dynamic features, dynamic ranks, that impose temporal ordinal constraints on the static ranks (i.e., intensity scores). Our experimental results show that the proposed approach outperforms state-of-the art methods for sequence classification with ordinal data and other ordinal regression models. The approach performs significantly better than other models in terms of Intra-Class Correlation measure, which is the most accepted evaluation measure in the tasks of facial behaviour intensity estimation. version:2
arxiv-1304-1022 | A software for aging faces applied to ancient marble busts | http://arxiv.org/abs/1304.1022 | id:1304.1022 author:Amelia Carolina Sparavigna category:cs.CV  published:2013-04-03 summary:The study and development of software able to show the effect of aging of faces is one of the tasks of face recognition technologies. Some software solutions are used for investigations, some others to show the effects of drugs on healthy appearance, however some other applications can be proposed for the analysis of visual arts. Here we use a freely available software, which is providing interesting results, for the comparison of ancient marble busts. An analysis of Augustus busts is proposed. version:1
arxiv-1303-0479 | Scale Selection of Adaptive Kernel Regression by Joint Saliency Map for Nonrigid Image Registration | http://arxiv.org/abs/1303.0479 | id:1303.0479 author:Zhuangming Shen, Jiuai Sun, Hui Zhang, Binjie Qin category:cs.CV  published:2013-03-03 summary:Joint saliency map (JSM) [1] was developed to assign high joint saliency values to the corresponding saliency structures (called Joint Saliency Structures, JSSs) but zero or low joint saliency values to the outliers (or mismatches) that are introduced by missing correspondence or local large deformations between the reference and moving images to be registered. JSM guides the local structure matching in nonrigid registration by emphasizing these JSSs' sparse deformation vectors in adaptive kernel regression of hierarchical sparse deformation vectors for iterative dense deformation reconstruction. By designing an effective superpixel-based local structure scale estimator to compute the reference structure's structure scale, we further propose to determine the scale (the width) of kernels in the adaptive kernel regression through combining the structure scales to JSM-based scales of mismatch between the local saliency structures. Therefore, we can adaptively select the sample size of sparse deformation vectors to reconstruct the dense deformation vectors for accurately matching the every local structures in the two images. The experimental results demonstrate better accuracy of our method in aligning two images with missing correspondence and local large deformation than the state-of-the-art methods. version:2
arxiv-1209-4115 | Transferring Subspaces Between Subjects in Brain-Computer Interfacing | http://arxiv.org/abs/1209.4115 | id:1209.4115 author:Wojciech Samek, Frank C. Meinecke, Klaus-Robert Müller category:stat.ML cs.HC cs.LG  published:2012-09-18 summary:Compensating changes between a subjects' training and testing session in Brain Computer Interfacing (BCI) is challenging but of great importance for a robust BCI operation. We show that such changes are very similar between subjects, thus can be reliably estimated using data from other users and utilized to construct an invariant feature space. This novel approach to learning from other subjects aims to reduce the adverse effects of common non-stationarities, but does not transfer discriminative information. This is an important conceptual difference to standard multi-subject methods that e.g. improve the covariance matrix estimation by shrinking it towards the average of other users or construct a global feature space. These methods do not reduces the shift between training and test data and may produce poor results when subjects have very different signal characteristics. In this paper we compare our approach to two state-of-the-art multi-subject methods on toy data and two data sets of EEG recordings from subjects performing motor imagery. We show that it can not only achieve a significant increase in performance, but also that the extracted change patterns allow for a neurophysiologically meaningful interpretation. version:2
arxiv-1208-3845 | Adaptive Graph via Multiple Kernel Learning for Nonnegative Matrix Factorization | http://arxiv.org/abs/1208.3845 | id:1208.3845 author:Jing-Yan Wang, Mustafa AbdulJabbar category:cs.LG cs.CV stat.ML  published:2012-08-19 summary:Nonnegative Matrix Factorization (NMF) has been continuously evolving in several areas like pattern recognition and information retrieval methods. It factorizes a matrix into a product of 2 low-rank non-negative matrices that will define parts-based, and linear representation of nonnegative data. Recently, Graph regularized NMF (GrNMF) is proposed to find a compact representation,which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure. In GNMF, an affinity graph is constructed from the original data space to encode the geometrical information. In this paper, we propose a novel idea which engages a Multiple Kernel Learning approach into refining the graph structure that reflects the factorization of the matrix and the new data space. The GrNMF is improved by utilizing the graph refined by the kernel learning, and then a novel kernel learning method is introduced under the GrNMF framework. Our approach shows encouraging results of the proposed algorithm in comparison to the state-of-the-art clustering algorithms like NMF, GrNMF, SVD etc. version:3
arxiv-1208-3839 | Discriminative Sparse Coding on Multi-Manifold for Data Representation and Classification | http://arxiv.org/abs/1208.3839 | id:1208.3839 author:Jing-Yan Wang category:cs.CV cs.LG stat.ML  published:2012-08-19 summary:Sparse coding has been popularly used as an effective data representation method in various applications, such as computer vision, medical imaging and bioinformatics, etc. However, the conventional sparse coding algorithms and its manifold regularized variants (graph sparse coding and Laplacian sparse coding), learn the codebook and codes in a unsupervised manner and neglect the class information available in the training set. To address this problem, in this paper we propose a novel discriminative sparse coding method based on multi-manifold, by learning discriminative class-conditional codebooks and sparse codes from both data feature space and class labels. First, the entire training set is partitioned into multiple manifolds according to the class labels. Then, we formulate the sparse coding as a manifold-manifold matching problem and learn class-conditional codebooks and codes to maximize the manifold margins of different classes. Lastly, we present a data point-manifold matching error based strategy to classify the unlabeled data point. Experimental results on somatic mutations identification and breast tumors classification in ultrasonic images tasks demonstrate the efficacy of the proposed data representation-classification approach. version:2
arxiv-1304-0886 | Improved Anomaly Detection in Crowded Scenes via Cell-based Analysis of Foreground Speed, Size and Texture | http://arxiv.org/abs/1304.0886 | id:1304.0886 author:Vikas Reddy, Conrad Sanderson, Brian C. Lovell category:cs.CV  published:2013-04-03 summary:A robust and efficient anomaly detection technique is proposed, capable of dealing with crowded scenes where traditional tracking based approaches tend to fail. Initial foreground segmentation of the input frames confines the analysis to foreground objects and effectively ignores irrelevant background dynamics. Input frames are split into non-overlapping cells, followed by extracting features based on motion, size and texture from each cell. Each feature type is independently analysed for the presence of an anomaly. Unlike most methods, a refined estimate of object motion is achieved by computing the optical flow of only the foreground pixels. The motion and size features are modelled by an approximated version of kernel density estimation, which is computationally efficient even for large training datasets. Texture features are modelled by an adaptively grown codebook, with the number of entries in the codebook selected in an online fashion. Experiments on the recently published UCSD Anomaly Detection dataset show that the proposed method obtains considerably better results than three recent approaches: MPPCA, social force, and mixture of dynamic textures (MDT). The proposed method is also several orders of magnitude faster than MDT, the next best performing method. version:1
arxiv-1203-0594 | Learning DNF Expressions from Fourier Spectrum | http://arxiv.org/abs/1203.0594 | id:1203.0594 author:Vitaly Feldman category:cs.LG cs.CC cs.DS  published:2012-03-03 summary:Since its introduction by Valiant in 1984, PAC learning of DNF expressions remains one of the central problems in learning theory. We consider this problem in the setting where the underlying distribution is uniform, or more generally, a product distribution. Kalai, Samorodnitsky and Teng (2009) showed that in this setting a DNF expression can be efficiently approximated from its "heavy" low-degree Fourier coefficients alone. This is in contrast to previous approaches where boosting was used and thus Fourier coefficients of the target function modified by various distributions were needed. This property is crucial for learning of DNF expressions over smoothed product distributions, a learning model introduced by Kalai et al. (2009) and inspired by the seminal smoothed analysis model of Spielman and Teng (2001). We introduce a new approach to learning (or approximating) a polynomial threshold functions which is based on creating a function with range [-1,1] that approximately agrees with the unknown function on low-degree Fourier coefficients. We then describe conditions under which this is sufficient for learning polynomial threshold functions. Our approach yields a new, simple algorithm for approximating any polynomial-size DNF expression from its "heavy" low-degree Fourier coefficients alone. Our algorithm greatly simplifies the proof of learnability of DNF expressions over smoothed product distributions. We also describe an application of our algorithm to learning monotone DNF expressions over product distributions. Building on the work of Servedio (2001), we give an algorithm that runs in time $\poly((s \cdot \log{(s/\eps)})^{\log{(s/\eps)}}, n)$, where $s$ is the size of the target DNF expression and $\eps$ is the accuracy. This improves on $\poly((s \cdot \log{(ns/\eps)})^{\log{(s/\eps)} \cdot \log{(1/\eps)}}, n)$ bound of Servedio (2001). version:3
arxiv-1304-0840 | A Fast Semidefinite Approach to Solving Binary Quadratic Problems | http://arxiv.org/abs/1304.0840 | id:1304.0840 author:Peng Wang, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG  published:2013-04-03 summary:Many computer vision problems can be formulated as binary quadratic programs (BQPs). Two classic relaxation methods are widely used for solving BQPs, namely, spectral methods and semidefinite programming (SDP), each with their own advantages and disadvantages. Spectral relaxation is simple and easy to implement, but its bound is loose. Semidefinite relaxation has a tighter bound, but its computational complexity is high for large scale problems. We present a new SDP formulation for BQPs, with two desirable properties. First, it has a similar relaxation bound to conventional SDP formulations. Second, compared with conventional SDP methods, the new SDP formulation leads to a significantly more efficient and scalable dual optimization approach, which has the same degree of complexity as spectral methods. Extensive experiments on various applications including clustering, image segmentation, co-segmentation and registration demonstrate the usefulness of our SDP formulation for solving large-scale BQPs. version:1
arxiv-1304-0839 | Multiscale Hybrid Non-local Means Filtering Using Modified Similarity Measure | http://arxiv.org/abs/1304.0839 | id:1304.0839 author:Zahid Hussain Shamsi, Dai-Gyoung Kim category:cs.CV 68U10  68U05  65D18  published:2013-04-03 summary:A new multiscale implementation of non-local means filtering for image denoising is proposed. The proposed algorithm also introduces a modification of similarity measure for patch comparison. The standard Euclidean norm is replaced by weighted Euclidean norm for patch based comparison. Assuming the patch as an oriented surface, notion of normal vector patch is being associated with each patch. The inner product of these normal vector patches is then used in weighted Euclidean distance of photometric patches as the weight factor. The algorithm involves two steps: The first step is multiscale implementation of an accelerated non-local means filtering in the stationary wavelet domain to obtain a refined version of the noisy patches for later comparison. This step is inspired by a preselection phase of finding similar patches in various non-local means approaches. The next step is to apply the modified non-local means filtering to the noisy image using the reference patches obtained in the first step. These refined patches contain less noise, and consequently the computation of normal vectors and partial derivatives is more accurate. Experimental results indicate equivalent or better performance of proposed algorithm as compared to various state of the art algorithms. version:1
arxiv-1304-0823 | Lie Algebrized Gaussians for Image Representation | http://arxiv.org/abs/1304.0823 | id:1304.0823 author:Liyu Gong, Meng Chen, Chunlong Hu category:cs.CV  published:2013-04-03 summary:We present an image representation method which is derived from analyzing Gaussian probability density function (\emph{pdf}) space using Lie group theory. In our proposed method, images are modeled by Gaussian mixture models (GMMs) which are adapted from a globally trained GMM called universal background model (UBM). Then we vectorize the GMMs based on two facts: (1) components of image-specific GMMs are closely grouped together around their corresponding component of the UBM due to the characteristic of the UBM adaption procedure; (2) Gaussian \emph{pdf}s form a Lie group, which is a differentiable manifold rather than a vector space. We map each Gaussian component to the tangent vector space (named Lie algebra) of Lie group at the manifold position of UBM. The final feature vector, named Lie algebrized Gaussians (LAG) is then constructed by combining the Lie algebrized Gaussian components with mixture weights. We apply LAG features to scene category recognition problem and observe state-of-the-art performance on 15Scenes benchmark. version:1
arxiv-1304-0740 | O(logT) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions | http://arxiv.org/abs/1304.0740 | id:1304.0740 author:Lijun Zhang, Tianbao Yang, Rong Jin, Xiaofei He category:cs.LG  published:2013-04-02 summary:Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as positive semi-definite cones, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batch, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal $O(1/T)$ rate of convergence with only $O(\log T)$ projections. Our empirical study verifies the theoretical result. version:1
arxiv-1304-0730 | Representation, Approximation and Learning of Submodular Functions Using Low-rank Decision Trees | http://arxiv.org/abs/1304.0730 | id:1304.0730 author:Vitaly Feldman, Pravesh Kothari, Jan Vondrak category:cs.LG cs.CC cs.DS  published:2013-04-02 summary:We study the complexity of approximate representation and learning of submodular functions over the uniform distribution on the Boolean hypercube $\{0,1\}^n$. Our main result is the following structural theorem: any submodular function is $\epsilon$-close in $\ell_2$ to a real-valued decision tree (DT) of depth $O(1/\epsilon^2)$. This immediately implies that any submodular function is $\epsilon$-close to a function of at most $2^{O(1/\epsilon^2)}$ variables and has a spectral $\ell_1$ norm of $2^{O(1/\epsilon^2)}$. It also implies the closest previous result that states that submodular functions can be approximated by polynomials of degree $O(1/\epsilon^2)$ (Cheraghchi et al., 2012). Our result is proved by constructing an approximation of a submodular function by a DT of rank $4/\epsilon^2$ and a proof that any rank-$r$ DT can be $\epsilon$-approximated by a DT of depth $\frac{5}{2}(r+\log(1/\epsilon))$. We show that these structural results can be exploited to give an attribute-efficient PAC learning algorithm for submodular functions running in time $\tilde{O}(n^2) \cdot 2^{O(1/\epsilon^{4})}$. The best previous algorithm for the problem requires $n^{O(1/\epsilon^{2})}$ time and examples (Cheraghchi et al., 2012) but works also in the agnostic setting. In addition, we give improved learning algorithms for a number of related settings. We also prove that our PAC and agnostic learning algorithms are essentially optimal via two lower bounds: (1) an information-theoretic lower bound of $2^{\Omega(1/\epsilon^{2/3})}$ on the complexity of learning monotone submodular functions in any reasonable model; (2) computational lower bound of $n^{\Omega(1/\epsilon^{2/3})}$ based on a reduction to learning of sparse parities with noise, widely-believed to be intractable. These are the first lower bounds for learning of submodular functions over the uniform distribution. version:1
arxiv-1212-0142 | Pedestrian Detection with Unsupervised Multi-Stage Feature Learning | http://arxiv.org/abs/1212.0142 | id:1212.0142 author:Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann LeCun category:cs.CV cs.LG  published:2012-12-01 summary:Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage. version:2
arxiv-0907-2337 | Sparsistent Estimation of Time-Varying Discrete Markov Random Fields | http://arxiv.org/abs/0907.2337 | id:0907.2337 author:Mladen Kolar, Eric P. Xing category:stat.ML  published:2009-07-14 summary:Network models have been popular for modeling and representing complex relationships and dependencies between observed variables. When data comes from a dynamic stochastic process, a single static network model cannot adequately capture transient dependencies, such as, gene regulatory dependencies throughout a developmental cycle of an organism. Kolar et al (2010b) proposed a method based on kernel-smoothing l1-penalized logistic regression for estimating time-varying networks from nodal observations collected from a time-series of observational data. In this paper, we establish conditions under which the proposed method consistently recovers the structure of a time-varying network. This work complements previous empirical findings by providing sound theoretical guarantees for the proposed estimation procedure. For completeness, we include numerical simulations in the paper. version:2
arxiv-1304-0640 | Event management for large scale event-driven digital hardware spiking neural networks | http://arxiv.org/abs/1304.0640 | id:1304.0640 author:Louis-Charles Caron, \and Michiel D'Haene, \and Frédéric Mailhot, \and Benjamin Schrauwen, \and Jean Rouat category:cs.NE cs.AI cs.DC  published:2013-04-02 summary:The interest in brain-like computation has led to the design of a plethora of innovative neuromorphic systems. Individually, spiking neural networks (SNNs), event-driven simulation and digital hardware neuromorphic systems get a lot of attention. Despite the popularity of event-driven SNNs in software, very few digital hardware architectures are found. This is because existing hardware solutions for event management scale badly with the number of events. This paper introduces the structured heap queue, a pipelined digital hardware data structure, and demonstrates its suitability for event management. The structured heap queue scales gracefully with the number of events, allowing the efficient implementation of large scale digital hardware event-driven SNNs. The scaling is linear for memory, logarithmic for logic resources and constant for processing time. The use of the structured heap queue is demonstrated on field-programmable gate array (FPGA) with an image segmentation experiment and a SNN of 65~536 neurons and 513~184 synapses. Events can be processed at the rate of 1 every 7 clock cycles and a 406$\times$158 pixel image is segmented in 200 ms. version:1
arxiv-1304-0596 | A Semiparametric Bayesian Extreme Value Model Using a Dirichlet Process Mixture of Gamma Densities | http://arxiv.org/abs/1304.0596 | id:1304.0596 author:Jairo Fuquene category:stat.ML stat.AP stat.ME  published:2013-04-02 summary:In this paper we propose a model with a Dirichlet process mixture of gamma densities in the bulk part below threshold and a generalized Pareto density in the tail for extreme value estimation. The proposed model is simple and flexible allowing us posterior density estimation and posterior inference for high quantiles. The model works well even for small sample sizes and in the absence of prior information. We evaluate the performance of the proposed model through a simulation study. Finally, the proposed model is applied to a real environmental data. version:1
arxiv-1209-3818 | Evolution and the structure of learning agents | http://arxiv.org/abs/1209.3818 | id:1209.3818 author:Alok Raj category:cs.AI cs.LG I.2; I.2.6  published:2012-09-18 summary:This paper presents the thesis that all learning agents of finite information size are limited by their informational structure in what goals they can efficiently learn to achieve in a complex environment. Evolutionary change is critical for creating the required structure for all learning agents in any complex environment. The thesis implies that there is no efficient universal learning algorithm. An agent can go past the learning limits imposed by its structure only by slow evolutionary change or blind search which in a very complex environment can only give an agent an inefficient universal learning capability that can work only in evolutionary timescales or improbable luck. version:4
arxiv-1304-0421 | Stroke-Based Cursive Character Recognition | http://arxiv.org/abs/1304.0421 | id:1304.0421 author:K. C. Santosh, E. Iwata category:cs.CV  published:2013-04-01 summary:Human eye can see and read what is written or displayed either in natural handwriting or in printed format. The same work in case the machine does is called handwriting recognition. Handwriting recognition can be broken down into two categories: off-line and on-line. ... version:1
arxiv-1304-0401 | An improved quasar detection method in EROS-2 and MACHO LMC datasets | http://arxiv.org/abs/1304.0401 | id:1304.0401 author:Karim Pichara, Pavlos Protopapas, Dae-Won Kim, Jean-Baptiste Marquette, Patrick Tisserand category:astro-ph.IM stat.ML  published:2013-04-01 summary:We present a new classification method for quasar identification in the EROS-2 and MACHO datasets based on a boosted version of Random Forest classifier. We use a set of variability features including parameters of a continuous auto regressive model. We prove that continuous auto regressive parameters are very important discriminators in the classification process. We create two training sets (one for EROS-2 and one for MACHO datasets) using known quasars found in the LMC. Our model's accuracy in both EROS-2 and MACHO training sets is about 90% precision and 86% recall, improving the state of the art models accuracy in quasar detection. We apply the model on the complete, including 28 million objects, EROS-2 and MACHO LMC datasets, finding 1160 and 2551 candidates respectively. To further validate our list of candidates, we crossmatched our list with a previous 663 known strong candidates, getting 74% of matches for MACHO and 40% in EROS-2. The main difference on matching level is because EROS-2 is a slightly shallower survey which translates to significantly lower signal-to-noise ratio lightcurves. version:1
arxiv-1204-1800 | On Power-law Kernels, corresponding Reproducing Kernel Hilbert Space and Applications | http://arxiv.org/abs/1204.1800 | id:1204.1800 author:Debarghya Ghoshdastidar, Ambedkar Dukkipati category:cs.LG cs.IT math.IT stat.ML  published:2012-04-09 summary:The role of kernels is central to machine learning. Motivated by the importance of power-law distributions in statistical modeling, in this paper, we propose the notion of power-law kernels to investigate power-laws in learning problem. We propose two power-law kernels by generalizing Gaussian and Laplacian kernels. This generalization is based on distributions, arising out of maximization of a generalized information measure known as nonextensive entropy that is very well studied in statistical mechanics. We prove that the proposed kernels are positive definite, and provide some insights regarding the corresponding Reproducing Kernel Hilbert Space (RKHS). We also study practical significance of both kernels in classification and regression, and present some simulation results. version:2
arxiv-1305-2388 | Fast Feature Reduction in intrusion detection datasets | http://arxiv.org/abs/1305.2388 | id:1305.2388 author:Shafigh Parsazad, Ehsan Saboori, Amin Allahyar category:cs.CR cs.LG  published:2013-04-01 summary:In the most intrusion detection systems (IDS), a system tries to learn characteristics of different type of attacks by analyzing packets that sent or received in network. These packets have a lot of features. But not all of them is required to be analyzed to detect that specific type of attack. Detection speed and computational cost is another vital matter here, because in these types of problems, datasets are very huge regularly. In this paper we tried to propose a very simple and fast feature selection method to eliminate features with no helpful information on them. Result faster learning in process of redundant feature omission. We compared our proposed method with three most successful similarity based feature selection algorithm including Correlation Coefficient, Least Square Regression Error and Maximal Information Compression Index. After that we used recommended features by each of these algorithms in two popular classifiers including: Bayes and KNN classifier to measure the quality of the recommendations. Experimental result shows that although the proposed method can't outperform evaluated algorithms with high differences in accuracy, but in computational cost it has huge superiority over them. version:1
arxiv-1304-0715 | A cookbook of translating English to Xapi | http://arxiv.org/abs/1304.0715 | id:1304.0715 author:Ladislau Bölöni category:cs.AI cs.CL  published:2013-03-31 summary:The Xapagy cognitive architecture had been designed to perform narrative reasoning: to model and mimic the activities performed by humans when witnessing, reading, recalling, narrating and talking about stories. Xapagy communicates with the outside world using Xapi, a simplified, "pidgin" language which is strongly tied to the internal representation model (instances, scenes and verb instances) and reasoning techniques (shadows and headless shadows). While not fully a semantic equivalent of natural language, Xapi can represent a wide range of complex stories. We illustrate the representation technique used in Xapi through examples taken from folk physics, folk psychology as well as some more unusual literary examples. We argue that while the Xapi model represents a conceptual shift from the English representation, the mapping is logical and consistent, and a trained knowledge engineer can translate between English and Xapi at near-native speed. version:1
arxiv-1304-0243 | Compressive adaptive computational ghost imaging | http://arxiv.org/abs/1304.0243 | id:1304.0243 author:Marc Aßmann, Manfred Bayer category:physics.optics cs.CV  published:2013-03-31 summary:Compressive sensing is considered a huge breakthrough in signal acquisition. It allows recording an image consisting of $N^2$ pixels using much fewer than $N^2$ measurements if it can be transformed to a basis where most pixels take on negligibly small values. Standard compressive sensing techniques suffer from the computational overhead needed to reconstruct an image with typical computation times between hours and days and are thus not optimal for applications in physics and spectroscopy. We demonstrate an adaptive compressive sampling technique that performs measurements directly in a sparse basis. It needs much fewer than $N^2$ measurements without any computational overhead, so the result is available instantly. version:1
arxiv-1105-4278 | Is the Multiverse Hypothesis capable of explaining the Fine Tuning of Nature Laws and Constants? The Case of Cellular Automata | http://arxiv.org/abs/1105.4278 | id:1105.4278 author:Francisco José Soler Gil, Manuel Alfonseca category:nlin.CG astro-ph.CO cs.NE  published:2011-05-21 summary:The objective of this paper is analyzing to which extent the multiverse hypothesis provides a real explanation of the peculiarities of the laws and constants in our universe. First we argue in favor of the thesis that all multiverses except Tegmark's <<mathematical multiverse>> are too small to explain the fine tuning, so that they merely shift the problem up one level. But the <<mathematical multiverse>> is surely too large. To prove this assessment, we have performed a number of experiments with cellular automata of complex behavior, which can be considered as universes in the mathematical multiverse. The analogy between what happens in some automata (in particular Conway's <<Game of Life>>) and the real world is very strong. But if the results of our experiments can be extrapolated to our universe, we should expect to inhabit -- in the context of the multiverse -- a world in which at least some of the laws and constants of nature should show a certain time dependence. Actually, the probability of our existence in a world such as ours would be mathematically equal to zero. In consequence, the results presented in this paper can be considered as an inkling that the hypothesis of the multiverse, whatever its type, does not offer an adequate explanation for the peculiarities of the physical laws in our world. A slightly reduced version of this paper has been published in the Journal for General Philosophy of Science, Springer, March 2013, DOI: 10.1007/s10838-013-9215-7. version:3
arxiv-1304-0104 | Meaning-focused and Quantum-inspired Information Retrieval | http://arxiv.org/abs/1304.0104 | id:1304.0104 author:Diederik Aerts, Jan Broekaert, Sandro Sozzo, Tomas Veloz category:cs.IR cs.CL quant-ph  published:2013-03-30 summary:In recent years, quantum-based methods have promisingly integrated the traditional procedures in information retrieval (IR) and natural language processing (NLP). Inspired by our research on the identification and application of quantum structures in cognition, more specifically our work on the representation of concepts and their combinations, we put forward a 'quantum meaning based' framework for structured query retrieval in text corpora and standardized testing corpora. This scheme for IR rests on considering as basic notions, (i) 'entities of meaning', e.g., concepts and their combinations and (ii) traces of such entities of meaning, which is how documents are considered in this approach. The meaning content of these 'entities of meaning' is reconstructed by solving an 'inverse problem' in the quantum formalism, consisting of reconstructing the full states of the entities of meaning from their collapsed states identified as traces in relevant documents. The advantages with respect to traditional approaches, such as Latent Semantic Analysis (LSA), are discussed by means of concrete examples. version:1
arxiv-1304-0090 | A Neuromorphic VLSI Design for Spike Timing and Rate Based Synaptic Plasticity | http://arxiv.org/abs/1304.0090 | id:1304.0090 author:Mostafa Rahimi Azghadi, Said Al-Sarawi, Derek Abbott, Nicolangelo Iannella category:cs.NE  published:2013-03-30 summary:Triplet-based Spike Timing Dependent Plasticity (TSTDP) is a powerful synaptic plasticity rule that acts beyond conventional pair-based STDP (PSTDP). Here, the TSTDP is capable of reproducing the outcomes from a variety of biological experiments, while the PSTDP rule fails to reproduce them. Additionally, it has been shown that the behaviour inherent to the spike rate-based Bienenstock-Cooper-Munro (BCM) synaptic plasticity rule can also emerge from the TSTDP rule. This paper proposes an analog implementation of the TSTDP rule. The proposed VLSI circuit has been designed using the AMS 0.35 um CMOS process and has been simulated using design kits for Synopsys and Cadence tools. Simulation results demonstrate how well the proposed circuit can alter synaptic weights according to the timing difference amongst a set of different patterns of spikes. Furthermore, the circuit is shown to give rise to a BCM-like learning rule, which is a rate-based rule. To mimic implementation environment, a 1000 run Monte Carlo (MC) analysis was conducted on the proposed circuit. The presented MC simulation analysis and the simulation result from fine-tuned circuits show that, it is possible to mitigate the effect of process variations in the proof of concept circuit, however, a practical variation aware design technique is required to promise a high circuit performance in a large scale neural network. We believe that the proposed design can play a significant role in future VLSI implementations of both spike timing and rate based neuromorphic learning systems. version:1
arxiv-1210-1316 | Learning Locality-Constrained Collaborative Representation for Face Recognition | http://arxiv.org/abs/1210.1316 | id:1210.1316 author:Xi Peng, Lei Zhang, Zhang Yi, Kok Kiong Tan category:cs.CV  published:2012-10-04 summary:The model of low-dimensional manifold and sparse representation are two well-known concise models that suggest each data can be described by a few characteristics. Manifold learning is usually investigated for dimension reduction by preserving some expected local geometric structures from the original space to a low-dimensional one. The structures are generally determined by using pairwise distance, e.g., Euclidean distance. Alternatively, sparse representation denotes a data point as a linear combination of the points from the same subspace. In practical applications, however, the nearby points in terms of pairwise distance may not belong to the same subspace, and vice versa. Consequently, it is interesting and important to explore how to get a better representation by integrating these two models together. To this end, this paper proposes a novel coding algorithm, called Locality-Constrained Collaborative Representation (LCCR), which improves the robustness and discrimination of data representation by introducing a kind of local consistency. The locality term derives from a biologic observation that the similar inputs have similar code. The objective function of LCCR has an analytical solution, and it does not involve local minima. The empirical studies based on four public facial databases, ORL, AR, Extended Yale B, and Multiple PIE, show that LCCR is promising in recognizing human faces from frontal views with varying expression and illumination, as well as various corruptions and occlusions. version:2
arxiv-1207-0805 | Anatomical Structure Segmentation in Liver MRI Images | http://arxiv.org/abs/1207.0805 | id:1207.0805 author:G. Geethu Lakshmi category:cs.CV  published:2012-07-03 summary:Segmentation of medical images is a challenging task owing to their complexity. A standard segmentation problem within Magnetic Resonance Imaging (MRI) is the task of labeling voxels according to their tissue type. Image segmentation provides volumetric quantification of liver area and thus helps in the diagnosis of disorders, such as Hepatitis, Cirrhosis, Jaundice, Hemochromatosis etc.This work deals with comparison of segmentation by applying Level Set Method,Fuzzy Level Information C-Means Clustering Algorithm and Gradient Vector Flow Snake Algorithm.The results are compared using the parameters such as Number of pixels correctly classified, and percentage of area segmented. version:3
arxiv-1304-8052 | Registration of Images with Outliers Using Joint Saliency Map | http://arxiv.org/abs/1304.8052 | id:1304.8052 author:Binjie Qin, Zhijun Gu, Xianjun Sun, Yisong Lv category:cs.CV  published:2013-03-29 summary:Mutual information (MI) is a popular similarity measure for image registration, whereby good registration can be achieved by maximizing the compactness of the clusters in the joint histogram. However, MI is sensitive to the "outlier" objects that appear in one image but not the other, and also suffers from local and biased maxima. We propose a novel joint saliency map (JSM) to highlight the corresponding salient structures in the two images, and emphatically group those salient structures into the smoothed compact clusters in the weighted joint histogram. This strategy could solve both the outlier and the local maxima problems. Experimental results show that the JSM-MI based algorithm is not only accurate but also robust for registration of challenging image pairs with outliers. version:1
arxiv-1204-4710 | Regret in Online Combinatorial Optimization | http://arxiv.org/abs/1204.4710 | id:1204.4710 author:Jean-Yves Audibert, Sébastien Bubeck, Gábor Lugosi category:cs.LG stat.ML  published:2012-04-20 summary:We address online linear optimization problems when the possible actions of the decision maker are represented by binary vectors. The regret of the decision maker is the difference between her realized loss and the best loss she would have achieved by picking, in hindsight, the best possible action. Our goal is to understand the magnitude of the best possible (minimax) regret. We study the problem under three different assumptions for the feedback the decision maker receives: full information, and the partial information models of the so-called "semi-bandit" and "bandit" problems. Combining the Mirror Descent algorithm and the INF (Implicitely Normalized Forecaster) strategy, we are able to prove optimal bounds for the semi-bandit case. We also recover the optimal bounds for the full information setting. In the bandit case we discuss existing results in light of a new lower bound, and suggest a conjecture on the optimal regret in that case. Finally we also prove that the standard exponentially weighted average forecaster is provably suboptimal in the setting of online combinatorial optimization. version:2
arxiv-1304-0035 | Translation-Invariant Shrinkage/Thresholding of Group Sparse Signals | http://arxiv.org/abs/1304.0035 | id:1304.0035 author:Po-Yu Chen, Ivan W. Selesnick category:cs.CV cs.LG cs.SD  published:2013-03-29 summary:This paper addresses signal denoising when large-amplitude coefficients form clusters (groups). The L1-norm and other separable sparsity models do not capture the tendency of coefficients to cluster (group sparsity). This work develops an algorithm, called 'overlapping group shrinkage' (OGS), based on the minimization of a convex cost function involving a group-sparsity promoting penalty function. The groups are fully overlapping so the denoising method is translation-invariant and blocking artifacts are avoided. Based on the principle of majorization-minimization (MM), we derive a simple iterative minimization algorithm that reduces the cost function monotonically. A procedure for setting the regularization parameter, based on attenuating the noise to a specified level, is also described. The proposed approach is illustrated on speech enhancement, wherein the OGS approach is applied in the short-time Fourier transform (STFT) domain. The denoised speech produced by OGS does not suffer from musical noise. version:1
arxiv-1207-5259 | Optimal discovery with probabilistic expert advice: finite time analysis and macroscopic optimality | http://arxiv.org/abs/1207.5259 | id:1207.5259 author:Sebastien Bubeck, Damien Ernst, Aurelien Garivier category:cs.LG stat.ML  published:2012-07-22 summary:We consider an original problem that arises from the issue of security analysis of a power system and that we name optimal discovery with probabilistic expert advice. We address it with an algorithm based on the optimistic paradigm and on the Good-Turing missing mass estimator. We prove two different regret bounds on the performance of this algorithm under weak assumptions on the probabilistic experts. Under more restrictive hypotheses, we also prove a macroscopic optimality result, comparing the algorithm both with an oracle strategy and with uniform sampling. Finally, we provide numerical experiments illustrating these theoretical findings. version:3
arxiv-1302-5337 | Obtaining error-minimizing estimates and universal entry-wise error bounds for low-rank matrix completion | http://arxiv.org/abs/1302.5337 | id:1302.5337 author:Franz J. Király, Louis Theran category:stat.ML math.AG math.CO  published:2013-02-21 summary:We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually. In the noiseless case our algorithm is exact. For rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-are Nuclear Norm and OptSpace methods. version:2
arxiv-1304-0019 | Age group and gender recognition from human facial images | http://arxiv.org/abs/1304.0019 | id:1304.0019 author:Tizita Nesibu Shewaye category:cs.CV  published:2013-03-29 summary:This work presents an automatic human gender and age group recognition system based on human facial images. It makes an extensive experiment with row pixel intensity valued features and Discrete Cosine Transform (DCT) coefficient features with Principal Component Analysis and k-Nearest Neighbor classification to identify the best recognition approach. The final results show approaches using DCT coefficient outperform their counter parts resulting in a 99% correct gender recognition rate and 68% correct age group recognition rate (considering four distinct age groups) in unseen test images. Detailed experimental settings and obtained results are clearly presented and explained in this report. version:1
arxiv-1303-7474 | Independent Vector Analysis: Identification Conditions and Performance Bounds | http://arxiv.org/abs/1303.7474 | id:1303.7474 author:Matthew Anderson, Geng-Shen Fu, Ronald Phlypo, Tülay Adalı category:cs.LG cs.IT math.IT stat.ML  published:2013-03-29 summary:Recently, an extension of independent component analysis (ICA) from one to multiple datasets, termed independent vector analysis (IVA), has been the subject of significant research interest. IVA has also been shown to be a generalization of Hotelling's canonical correlation analysis. In this paper, we provide the identification conditions for a general IVA formulation, which accounts for linear, nonlinear, and sample-to-sample dependencies. The identification conditions are a generalization of previous results for ICA and for IVA when samples are independently and identically distributed. Furthermore, a principal aim of IVA is the identification of dependent sources between datasets. Thus, we provide the additional conditions for when the arbitrary ordering of the sources within each dataset is common. Performance bounds in terms of the Cramer-Rao lower bound are also provided for the demixing matrices and interference to source ratio. The performance of two IVA algorithms are compared to the theoretical bounds. version:1
arxiv-1212-5711 | Normalized Compression Distance of Multisets with Applications | http://arxiv.org/abs/1212.5711 | id:1212.5711 author:Andrew R. Cohen, Paul M. B. Vitanyi category:cs.CV cs.IT math.IT physics.data-an  published:2012-12-22 summary:Normalized compression distance (NCD) is a parameter-free, feature-free, alignment-free, similarity measure between a pair of finite objects based on compression. However, it is not sufficient for all applications. We propose an NCD of finite multisets (a.k.a. multiples) of finite objects that is also a metric. Previously, attempts to obtain such an NCD failed. We cover the entire trajectory from theoretical underpinning to feasible practice. The new NCD for multisets is applied to retinal progenitor cell classification questions and to related synthetically generated data that were earlier treated with the pairwise NCD. With the new method we achieved significantly better results. Similarly for questions about axonal organelle transport. We also applied the new NCD to handwritten digit recognition and improved classification accuracy significantly over that of pairwise NCD by incorporating both the pairwise and NCD for multisets. In the analysis we use the incomputable Kolmogorov complexity that for practical purposes is approximated from above by the length of the compressed version of the file involved, using a real-world compression program. Index Terms--- Normalized compression distance, multisets or multiples, pattern recognition, data mining, similarity, classification, Kolmogorov complexity, retinal progenitor cells, synthetic data, organelle transport, handwritten character recognition version:4
arxiv-1303-7310 | Exploring the Role of Logically Related Non-Question Phrases for Answering Why-Questions | http://arxiv.org/abs/1303.7310 | id:1303.7310 author:Niraj Kumar, Rashmi Gangadharaiah, Kannan Srinathan, Vasudeva Varma category:cs.CL cs.IR H.3.m  published:2013-03-29 summary:In this paper, we show that certain phrases although not present in a given question/query, play a very important role in answering the question. Exploring the role of such phrases in answering questions not only reduces the dependency on matching question phrases for extracting answers, but also improves the quality of the extracted answers. Here matching question phrases means phrases which co-occur in given question and candidate answers. To achieve the above discussed goal, we introduce a bigram-based word graph model populated with semantic and topical relatedness of terms in the given document. Next, we apply an improved version of ranking with a prior-based approach, which ranks all words in the candidate document with respect to a set of root words (i.e. non-stopwords present in the question and in the candidate document). As a result, terms logically related to the root words are scored higher than terms that are not related to the root words. Experimental results show that our devised system performs better than state-of-the-art for the task of answering Why-questions. version:1
arxiv-1304-0480 | A problem dependent analysis of SOCP algorithms in noisy compressed sensing | http://arxiv.org/abs/1304.0480 | id:1304.0480 author:Mihailo Stojnic category:cs.IT math.IT stat.ML  published:2013-03-29 summary:Under-determined systems of linear equations with sparse solutions have been the subject of an extensive research in last several years above all due to results of \cite{CRT,CanRomTao06,DonohoPol}. In this paper we will consider \emph{noisy} under-determined linear systems. In a breakthrough \cite{CanRomTao06} it was established that in \emph{noisy} systems for any linear level of under-determinedness there is a linear sparsity that can be \emph{approximately} recovered through an SOCP (second order cone programming) optimization algorithm so that the approximate solution vector is (in an $\ell_2$-norm sense) guaranteed to be no further from the sparse unknown vector than a constant times the noise. In our recent work \cite{StojnicGenSocp10} we established an alternative framework that can be used for statistical performance analysis of the SOCP algorithms. To demonstrate how the framework works we then showed in \cite{StojnicGenSocp10} how one can use it to precisely characterize the \emph{generic} (worst-case) performance of the SOCP. In this paper we present a different set of results that can be obtained through the framework of \cite{StojnicGenSocp10}. The results will relate to \emph{problem dependent} performance analysis of SOCP's. We will consider specific types of unknown sparse vectors and characterize the SOCP performance when used for recovery of such vectors. We will also show that our theoretical predictions are in a solid agreement with the results one can get through numerical simulations. version:1
arxiv-1212-5877 | Blinking Molecule Tracking | http://arxiv.org/abs/1212.5877 | id:1212.5877 author:Andreas Karrenbauer, Dominik Wöll category:cs.CV cs.DM  published:2012-12-24 summary:We discuss a method for tracking individual molecules which globally optimizes the likelihood of the connections between molecule positions fast and with high reliability even for high spot densities and blinking molecules. Our method works with cost functions which can be freely chosen to combine costs for distances between spots in space and time and which can account for the reliability of positioning a molecule. To this end, we describe a top-down polyhedral approach to the problem of tracking many individual molecules. This immediately yields an effective implementation using standard linear programming solvers. Our method can be applied to 2D and 3D tracking. version:2
arxiv-1303-7264 | Scalable Text and Link Analysis with Mixed-Topic Link Models | http://arxiv.org/abs/1303.7264 | id:1303.7264 author:Yaojia Zhu, Xiaoran Yan, Lise Getoor, Cristopher Moore category:cs.LG cs.IR cs.SI physics.data-an stat.ML  published:2013-03-28 summary:Many data sets contain rich information about objects, as well as pairwise relations between them. For instance, in networks of websites, scientific papers, and other documents, each node has content consisting of a collection of words, as well as hyperlinks or citations to other nodes. In order to perform inference on such data sets, and make predictions and recommendations, it is useful to have models that are able to capture the processes which generate the text at each node and the links between them. In this paper, we combine classic ideas in topic modeling with a variant of the mixed-membership block model recently developed in the statistical physics community. The resulting model has the advantage that its parameters, including the mixture of topics of each document and the resulting overlapping communities, can be inferred with a simple and scalable expectation-maximization algorithm. We test our model on three data sets, performing unsupervised topic classification and link prediction. For both tasks, our model outperforms several existing state-of-the-art methods, achieving higher accuracy with significantly less computation, analyzing a data set with 1.3 million words and 44 thousand links in a few minutes. version:1
arxiv-1303-7226 | Detecting Overlapping Temporal Community Structure in Time-Evolving Networks | http://arxiv.org/abs/1303.7226 | id:1303.7226 author:Yudong Chen, Vikas Kawadia, Rahul Urgaonkar category:cs.SI cs.LG physics.soc-ph stat.ML  published:2013-03-28 summary:We present a principled approach for detecting overlapping temporal community structure in dynamic networks. Our method is based on the following framework: find the overlapping temporal community structure that maximizes a quality function associated with each snapshot of the network subject to a temporal smoothness constraint. A novel quality function and a smoothness constraint are proposed to handle overlaps, and a new convex relaxation is used to solve the resulting combinatorial optimization problem. We provide theoretical guarantees as well as experimental results that reveal community structure in real and synthetic networks. Our main insight is that certain structures can be identified only when temporal correlation is considered and when communities are allowed to overlap. In general, discovering such overlapping temporal community structure can enhance our understanding of real-world complex networks by revealing the underlying stability behind their seemingly chaotic evolution. version:1
arxiv-1303-5508 | Sparse Projections of Medical Images onto Manifolds | http://arxiv.org/abs/1303.5508 | id:1303.5508 author:George H. Chen, Christian Wachinger, Polina Golland category:cs.CV cs.LG stat.ML  published:2013-03-22 summary:Manifold learning has been successfully applied to a variety of medical imaging problems. Its use in real-time applications requires fast projection onto the low-dimensional space. To this end, out-of-sample extensions are applied by constructing an interpolation function that maps from the input space to the low-dimensional manifold. Commonly used approaches such as the Nystr\"{o}m extension and kernel ridge regression require using all training points. We propose an interpolation function that only depends on a small subset of the input training data. Consequently, in the testing phase each new point only needs to be compared against a small number of input training data in order to project the point onto the low-dimensional space. We interpret our method as an out-of-sample extension that approximates kernel ridge regression. Our method involves solving a simple convex optimization problem and has the attractive property of guaranteeing an upper bound on the approximation error, which is crucial for medical applications. Tuning this error bound controls the sparsity of the resulting interpolation function. We illustrate our method in two clinical applications that require fast mapping of input images onto a low-dimensional space. version:2
arxiv-1303-7186 | Large-Scale Automatic Reconstruction of Neuronal Processes from Electron Microscopy Images | http://arxiv.org/abs/1303.7186 | id:1303.7186 author:Verena Kaynig, Amelio Vazquez-Reina, Seymour Knowles-Barley, Mike Roberts, Thouis R. Jones, Narayanan Kasthuri, Eric Miller, Jeff Lichtman, Hanspeter Pfister category:q-bio.NC cs.CV  published:2013-03-28 summary:Automated sample preparation and electron microscopy enables acquisition of very large image data sets. These technical advances are of special importance to the field of neuroanatomy, as 3D reconstructions of neuronal processes at the nm scale can provide new insight into the fine grained structure of the brain. Segmentation of large-scale electron microscopy data is the main bottleneck in the analysis of these data sets. In this paper we present a pipeline that provides state-of-the art reconstruction performance while scaling to data sets in the GB-TB range. First, we train a random forest classifier on interactive sparse user annotations. The classifier output is combined with an anisotropic smoothing prior in a Conditional Random Field framework to generate multiple segmentation hypotheses per image. These segmentations are then combined into geometrically consistent 3D objects by segmentation fusion. We provide qualitative and quantitative evaluation of the automatic segmentation and demonstrate large-scale 3D reconstructions of neuronal processes from a $\mathbf{27,000}$ $\mathbf{\mu m^3}$ volume of brain tissue over a cube of $\mathbf{30 \; \mu m}$ in each dimension corresponding to 1000 consecutive image sections. We also introduce Mojo, a proofreading tool including semi-automated correction of merge errors based on sparse user scribbles. version:1
arxiv-1302-2325 | Conditional Gradient Algorithms for Norm-Regularized Smooth Convex Optimization | http://arxiv.org/abs/1302.2325 | id:1302.2325 author:Zaid Harchaoui, Anatoli Juditsky, Arkadi Nemirovski category:math.OC stat.CO stat.ML  published:2013-02-10 summary:Motivated by some applications in signal processing and machine learning, we consider two convex optimization problems where, given a cone $K$, a norm $\ \cdot\ $ and a smooth convex function $f$, we want either 1) to minimize the norm over the intersection of the cone and a level set of $f$, or 2) to minimize over the cone the sum of $f$ and a multiple of the norm. We focus on the case where (a) the dimension of the problem is too large to allow for interior point algorithms, (b) $\ \cdot\ $ is "too complicated" to allow for computationally cheap Bregman projections required in the first-order proximal gradient algorithms. On the other hand, we assume that {it is relatively easy to minimize linear forms over the intersection of $K$ and the unit $\ \cdot\ $-ball}. Motivating examples are given by the nuclear norm with $K$ being the entire space of matrices, or the positive semidefinite cone in the space of symmetric matrices, and the Total Variation norm on the space of 2D images. We discuss versions of the Conditional Gradient algorithm capable to handle our problems of interest, provide the related theoretical efficiency estimates and outline some applications. version:4
arxiv-1302-2073 | pROST : A Smoothed Lp-norm Robust Online Subspace Tracking Method for Realtime Background Subtraction in Video | http://arxiv.org/abs/1302.2073 | id:1302.2073 author:Florian Seidel, Clemens Hage, Martin Kleinsteuber category:cs.CV  published:2013-02-08 summary:An increasing number of methods for background subtraction use Robust PCA to identify sparse foreground objects. While many algorithms use the L1-norm as a convex relaxation of the ideal sparsifying function, we approach the problem with a smoothed Lp-norm and present pROST, a method for robust online subspace tracking. The algorithm is based on alternating minimization on manifolds. Implemented on a graphics processing unit it achieves realtime performance. Experimental results on a state-of-the-art benchmark for background subtraction on real-world video data indicate that the method succeeds at a broad variety of background subtraction scenarios, and it outperforms competing approaches when video quality is deteriorated by camera jitter. version:2
arxiv-1303-7043 | Inductive Hashing on Manifolds | http://arxiv.org/abs/1303.7043 | id:1303.7043 author:Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin Tang category:cs.LG  published:2013-03-28 summary:Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes that preserve the Euclidean distance in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original high-dimensional data. The complexity of these models, and the problems with out-of-sample data, have previously rendered them unsuitable for application to large-scale embedding, however. In this work, we consider how to learn compact binary embeddings on their intrinsic manifolds. In order to address the above-mentioned difficulties, we describe an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method. Our proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. We particularly show that hashing on the basis of t-SNE . version:1
arxiv-1303-4845 | On Constructing the Value Function for Optimal Trajectory Problem and its Application to Image Processing | http://arxiv.org/abs/1303.4845 | id:1303.4845 author:Myong-Song Ho, Gwang-Hui Ju, Yong-Bom O, Gwang-Ho Jong category:cs.CV  published:2013-03-20 summary:We proposed an algorithm for solving Hamilton-Jacobi equation associated to an optimal trajectory problem for a vehicle moving inside the pre-specified domain with the speed depending upon the direction of the motion and current position of the vehicle. The dynamics of the vehicle is defined by an ordinary differential equation, the right hand of which is given by product of control(a time dependent fuction) and a function dependent on trajectory and control. At some unspecified terminal time, the vehicle reaches the boundary of the pre-specified domain and incurs a terminal cost. We also associate the traveling cost with a type of integral to the trajectory followed by vehicle. We are interested in a numerical method for finding a trajectory that minimizes the sum of the traveling cost and terminal cost. We developed an algorithm solving the value function for general trajectory optimization problem. Our algorithm is closely related to the Tsitsiklis's Fast Marching Method and J. A. Sethian's OUM and SLF-LLL[1-4] and is a generalization of them. On the basis of these results, We applied our algorithm to the image processing such as fingerprint verification. version:2
arxiv-1301-7473 | Information driven self-organization of complex robotic behaviors | http://arxiv.org/abs/1301.7473 | id:1301.7473 author:Georg Martius, Ralf Der, Nihat Ay category:cs.RO cs.IT cs.LG math.IT I.2.9; H.1.1; I.2.6  published:2013-01-30 summary:Information theory is a powerful tool to express principles to drive autonomous systems because it is domain invariant and allows for an intuitive interpretation. This paper studies the use of the predictive information (PI), also called excess entropy or effective measure complexity, of the sensorimotor process as a driving force to generate behavior. We study nonlinear and nonstationary systems and introduce the time-local predicting information (TiPI) which allows us to derive exact results together with explicit update rules for the parameters of the controller in the dynamical systems framework. In this way the information principle, formulated at the level of behavior, is translated to the dynamics of the synapses. We underpin our results with a number of case studies with high-dimensional robotic systems. We show the spontaneous cooperativity in a complex physical system with decentralized control. Moreover, a jointly controlled humanoid robot develops a high behavioral variety depending on its physics and the environment it is dynamically embedded into. The behavior can be decomposed into a succession of low-dimensional modes that increasingly explore the behavior space. This is a promising way to avoid the curse of dimensionality which hinders learning systems to scale well. version:2
arxiv-1304-3447 | Developing and Analyzing Boundary Detection Operators Using Probabilistic Models | http://arxiv.org/abs/1304.3447 | id:1304.3447 author:David Sher category:cs.CV  published:2013-03-27 summary:Most feature detectors such as edge detectors or circle finders are statistical, in the sense that they decide at each point in an image about the presence of a feature, this paper describes the use of Bayesian feature detectors. version:1
arxiv-1304-3432 | Machine Learning, Clustering, and Polymorphy | http://arxiv.org/abs/1304.3432 | id:1304.3432 author:Stephen Jose Hanson, Malcolm Bauer category:cs.AI cs.CL cs.LG  published:2013-03-27 summary:This paper describes a machine induction program (WITT) that attempts to model human categorization. Properties of categories to which human subjects are sensitive includes best or prototypical members, relative contrasts between putative categories, and polymorphy (neither necessary or sufficient features). This approach represents an alternative to usual Artificial Intelligence approaches to generalization and conceptual clustering which tend to focus on necessary and sufficient feature rules, equivalence classes, and simple search and match schemes. WITT is shown to be more consistent with human categorization while potentially including results produced by more traditional clustering schemes. Applications of this approach in the domains of expert systems and information retrieval are also discussed. version:1
arxiv-1304-3098 | Evidential Reasoning in Parallel Hierarchical Vision Programs | http://arxiv.org/abs/1304.3098 | id:1304.3098 author:Ze-Nian Li, Leonard Uhr category:cs.AI cs.CV  published:2013-03-27 summary:This paper presents an efficient adaptation and application of the Dempster-Shafer theory of evidence, one that can be used effectively in a massively parallel hierarchical system for visual pattern perception. It describes the techniques used, and shows in an extended example how they serve to improve the system's performance as it applies a multiple-level set of processes. version:1
arxiv-1304-3092 | Imprecise Meanings as a Cause of Uncertainty in Medical Knowledge-Based Systems | http://arxiv.org/abs/1304.3092 | id:1304.3092 author:Steven J. Henkind category:cs.AI cs.CL  published:2013-03-27 summary:There has been a considerable amount of work on uncertainty in knowledge-based systems. This work has generally been concerned with uncertainty arising from the strength of inferences and the weight of evidence. In this paper we discuss another type of uncertainty: that which is due to imprecision in the underlying primitives used to represent the knowledge of the system. In particular, a given word may denote many similar but not identical entities. Such words are said to be lexically imprecise. Lexical imprecision has caused widespread problems in many areas. Unless this phenomenon is recognized and appropriately handled, it can degrade the performance of knowledge-based systems. In particular, it can lead to difficulties with the user interface, and with the inferencing processes of these systems. Some techniques are suggested for coping with this phenomenon. version:1
arxiv-1304-2749 | Evidential Reasoning in Image Understanding | http://arxiv.org/abs/1304.2749 | id:1304.2749 author:Minchuan Zhang, Su-shing Chen category:cs.CV cs.AI  published:2013-03-27 summary:In this paper, we present some results of evidential reasoning in understanding multispectral images of remote sensing systems. The Dempster-Shafer approach of combination of evidences is pursued to yield contextual classification results, which are compared with previous results of the Bayesian context free classification, contextual classifications of dynamic programming and stochastic relaxation approaches. version:1
arxiv-1304-2743 | Comparisons of Reasoning Mechanisms for Computer Vision | http://arxiv.org/abs/1304.2743 | id:1304.2743 author:Ze-Nian Li category:cs.CV cs.AI  published:2013-03-27 summary:An evidential reasoning mechanism based on the Dempster-Shafer theory of evidence is introduced. Its performance in real-world image analysis is compared with other mechanisms based on the Bayesian formalism and a simple weight combination method. version:1
arxiv-1304-2367 | Utility-Based Control for Computer Vision | http://arxiv.org/abs/1304.2367 | id:1304.2367 author:Tod S. Levitt, Thomas O. Binford, Gil J. Ettinger, Patrice Gelband category:cs.CV cs.AI cs.SY  published:2013-03-27 summary:Several key issues arise in implementing computer vision recognition of world objects in terms of Bayesian networks. Computational efficiency is a driving force. Perceptual networks are very deep, typically fifteen levels of structure. Images are wide, e.g., an unspecified-number of edges may appear anywhere in an image 512 x 512 pixels or larger. For efficiency, we dynamically instantiate hypotheses of observed objects. The network is not fixed, but is created incrementally at runtime. Generation of hypotheses of world objects and indexing of models for recognition are important, but they are not considered here [4,11]. This work is aimed at near-term implementation with parallel computation in a radar surveillance system, ADRIES [5, 15], and a system for industrial part recognition, SUCCESSOR [2]. For many applications, vision must be faster to be practical and so efficiently controlling the machine vision process is critical. Perceptual operators may scan megapixels and may require minutes of computation time. It is necessary to avoid unnecessary sensor actions and computation. Parallel computation is available at several levels of processor capability. The potential for parallel, distributed computation for high-level vision means distributing non-homogeneous computations. This paper addresses the problem of task control in machine vision systems based on Bayesian probability models. We separate control and inference to extend the previous work [3] to maximize utility instead of probability. Maximizing utility allows adopting perceptual strategies for efficient information gathering with sensors and analysis of sensor data. Results of controlling machine vision via utility to recognize military situations are presented in this paper. Future work extends this to industrial part recognition for SUCCESSOR. version:1
arxiv-1304-2363 | Multiple decision trees | http://arxiv.org/abs/1304.2363 | id:1304.2363 author:Suk Wah Kwok, Chris Carter category:cs.LG cs.AI stat.ML  published:2013-03-27 summary:This paper describes experiments, on two domains, to investigate the effect of averaging over predictions of multiple decision trees, instead of using a single tree. Other authors have pointed out theoretical and commonsense reasons for preferring the multiple tree approach. Ideally, we would like to consider predictions from all trees, weighted by their probability. However, there is a vast number of different trees, and it is difficult to estimate the probability of each tree. We sidestep the estimation problem by using a modified version of the ID3 algorithm to build good trees, and average over only these trees. Our results are encouraging. For each domain, we managed to produce a small number of good trees. We find that it is best to average across sets of trees with different structure; this usually gives better performance than any of the constituent trees, including the ID3 tree. version:1
arxiv-1303-6938 | Expectation Propagation for Neural Networks with Sparsity-promoting Priors | http://arxiv.org/abs/1303.6938 | id:1303.6938 author:Pasi Jylänki, Aapo Nummenmaa, Aki Vehtari category:stat.ML  published:2013-03-27 summary:We propose a novel approach for nonlinear regression using a two-layer neural network (NN) model structure with sparsity-favoring hierarchical priors on the network weights. We present an expectation propagation (EP) approach for approximate integration over the posterior distribution of the weights, the hierarchical scale parameters of the priors, and the residual scale. Using a factorized posterior approximation we derive a computationally efficient algorithm, whose complexity scales similarly to an ensemble of independent sparse linear models. The approach enables flexible definition of weight priors with different sparseness properties such as independent Laplace priors with a common scale parameter or Gaussian automatic relevance determination (ARD) priors with different relevance parameters for all inputs. The approach can be extended beyond standard activation functions and NN model structures to form flexible nonlinear predictors from multiple sparse linear models. The effects of the hierarchical priors and the predictive performance of the algorithm are assessed using both simulated and real-world data. Comparisons are made to two alternative models with ARD priors: a Gaussian process with a NN covariance function and marginal maximum a posteriori estimates of the relevance parameters, and a NN with Markov chain Monte Carlo integration over all the unknown model parameters. version:1
arxiv-1304-1517 | Model-based Influence Diagrams for Machine Vision | http://arxiv.org/abs/1304.1517 | id:1304.1517 author:Tod S. Levitt, John Mark Agosta, Thomas O. Binford category:cs.CV cs.AI  published:2013-03-27 summary:We show an approach to automated control of machine vision systems based on incremental creation and evaluation of a particular family of influence diagrams that represent hypotheses of imagery interpretation and possible subsequent processing decisions. In our approach, model-based machine vision techniques are integrated with hierarchical Bayesian inference to provide a framework for representing and matching instances of objects and relationships in imagery and for accruing probabilities to rank order conflicting scene interpretations. We extend a result of Tatman and Shachter to show that the sequence of processing decisions derived from evaluating the diagrams at each stage is the same as the sequence that would have been derived by evaluating the final influence diagram that contains all random variables created during the run of the vision system. version:1
arxiv-1303-6935 | Efficiently Using Second Order Information in Large l1 Regularization Problems | http://arxiv.org/abs/1303.6935 | id:1303.6935 author:Xiaocheng Tang, Katya Scheinberg category:stat.ML cs.LG  published:2013-03-27 summary:We propose a novel general algorithm LHAC that efficiently uses second-order information to train a class of large-scale l1-regularized problems. Our method executes cheap iterations while achieving fast local convergence rate by exploiting the special structure of a low-rank matrix, constructed via quasi-Newton approximation of the Hessian of the smooth loss function. A greedy active-set strategy, based on the largest violations in the dual constraints, is employed to maintain a working set that iteratively estimates the complement of the optimal active set. This allows for smaller size of subproblems and eventually identifies the optimal active set. Empirical comparisons confirm that LHAC is highly competitive with several recently proposed state-of-the-art specialized solvers for sparse logistic regression and sparse inverse covariance matrix selection. version:1
arxiv-1303-6927 | An investigation towards wavelet based optimization of automatic image registration techniques | http://arxiv.org/abs/1303.6927 | id:1303.6927 author:Arun P. V., Dr. S. K. Katiyar category:cs.CV  published:2013-03-27 summary:Image registration is the process of transforming different sets of data into one coordinate system and is required for various remote sensing applications like change detection, image fusion, and other related areas. The effect of increased relief displacement, requirement of more control points, and increased data volume are the challenges associated with the registration of high resolution image data. The objective of this research work is to study the most efficient techniques and to investigate the extent of improvement achievable by enhancing them with Wavelet transform. The SIFT feature based method uses the Eigen value for extracting thousands of key points based on scale invariant features and these feature points when further enhanced by the wavelet transform yields the best results. version:1
arxiv-1303-6926 | A Comparative Analysis on the Applicability of Entropy in remote sensing | http://arxiv.org/abs/1303.6926 | id:1303.6926 author:Dr. S. K. Katiyar, Arun P. V. category:cs.CV  published:2013-03-27 summary:Entropy is the measure of uncertainty in any data and is adopted for maximisation of mutual information in many remote sensing operations. The availability of wide entropy variations motivated us for an investigation over the suitability preference of these versions to specific operations. Methodologies were implemented in Matlab and were enhanced with entropy variations. Evaluation of various implementations was based on different statistical parameters with reference to the study area The popular available versions like Tsalli's, Shanon's, and Renyi's entropies were analysed in context of various remote sensing operations namely thresholding, clustering and registration. version:1
arxiv-1301-3764 | Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients | http://arxiv.org/abs/1301.3764 | id:1301.3764 author:Tom Schaul, Yann LeCun category:cs.LG cs.AI stat.ML  published:2013-01-16 summary:Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free. version:2
arxiv-1303-6390 | A Note on k-support Norm Regularized Risk Minimization | http://arxiv.org/abs/1303.6390 | id:1303.6390 author:Matthew Blaschko category:cs.LG  published:2013-03-26 summary:The k-support norm has been recently introduced to perform correlated sparsity regularization. Although Argyriou et al. only reported experiments using squared loss, here we apply it to several other commonly used settings resulting in novel machine learning algorithms with interesting and familiar limit cases. Source code for the algorithms described here is available. version:2
arxiv-1303-6811 | Sparse approximation and recovery by greedy algorithms in Banach spaces | http://arxiv.org/abs/1303.6811 | id:1303.6811 author:Vladimir Temlyakov category:stat.ML math.FA 41A65  published:2013-03-27 summary:We study sparse approximation by greedy algorithms. We prove the Lebesgue-type inequalities for the Weak Chebyshev Greedy Algorithm (WCGA), a generalization of the Weak Orthogonal Matching Pursuit to the case of a Banach space. The main novelty of these results is a Banach space setting instead of a Hilbert space setting. The results are proved for redundant dictionaries satisfying certain conditions. Then we apply these general results to the case of bases. In particular, we prove that the WCGA provides almost optimal sparse approximation for the trigonometric system in $L_p$, $2\le p<\infty$. version:1
arxiv-1211-1513 | K-Plane Regression | http://arxiv.org/abs/1211.1513 | id:1211.1513 author:Naresh Manwani, P. S. Sastry category:cs.LG  published:2012-11-07 summary:In this paper, we present a novel algorithm for piecewise linear regression which can learn continuous as well as discontinuous piecewise linear functions. The main idea is to repeatedly partition the data and learn a liner model in in each partition. While a simple algorithm incorporating this idea does not work well, an interesting modification results in a good algorithm. The proposed algorithm is similar in spirit to $k$-means clustering algorithm. We show that our algorithm can also be viewed as an EM algorithm for maximum likelihood estimation of parameters under a reasonable probability model. We empirically demonstrate the effectiveness of our approach by comparing its performance with the state of art regression learning algorithms on some real world datasets. version:2
arxiv-1303-6750 | Sequential testing over multiple stages and performance analysis of data fusion | http://arxiv.org/abs/1303.6750 | id:1303.6750 author:Gaurav Thakur category:stat.ML cs.LG  published:2013-03-27 summary:We describe a methodology for modeling the performance of decision-level data fusion between different sensor configurations, implemented as part of the JIEDDO Analytic Decision Engine (JADE). We first discuss a Bayesian network formulation of classical probabilistic data fusion, which allows elementary fusion structures to be stacked and analyzed efficiently. We then present an extension of the Wald sequential test for combining the outputs of the Bayesian network over time. We discuss an algorithm to compute its performance statistics and illustrate the approach on some examples. This variant of the sequential test involves multiple, distinct stages, where the evidence accumulated from each stage is carried over into the next one, and is motivated by a need to keep certain sensors in the network inactive unless triggered by other sensors. version:1
arxiv-1303-6711 | An intelligent approach towards automatic shape modeling and object extraction from satellite images using cellular automata based algorithm | http://arxiv.org/abs/1303.6711 | id:1303.6711 author:P. V. Arun, S. K. Katiyar category:cs.CV  published:2013-03-27 summary:Automatic feature extraction domain has witnessed the application of many intelligent methodologies over past decade; however detection accuracy of these approaches were limited as object geometry and contextual knowledge were not given enough consideration. In this paper, we propose a frame work for accurate detection of features along with automatic interpolation, and interpretation by modeling feature shape as well as contextual knowledge using advanced techniques such as SVRF, Cellular Neural Network, Core set, and MACA. Developed methodology has been compared with contemporary methods using different statistical measures. Investigations over various satellite images revealed that considerable success was achieved with the CNN approach. CNN has been effective in modeling different complex features effectively and complexity of the approach has been considerably reduced using corset optimization. The system has dynamically used spectral and spatial information for representing contextual knowledge using CNN-prolog approach. System has been also proved to be effective in providing intelligent interpolation and interpretation of random features. version:1
arxiv-1303-6619 | An N-dimensional approach towards object based classification of remotely sensed imagery | http://arxiv.org/abs/1303.6619 | id:1303.6619 author:Arun p V, S. K. Katiyar category:cs.CV  published:2013-03-26 summary:Remote sensing techniques are widely used for land cover classification and urban analysis. The availability of high resolution remote sensing imagery limits the level of classification accuracy attainable from pixel-based approach. In this paper object-based classification scheme based on a hierarchical support vector machine is introduced. By combining spatial and spectral information, the amount of overlap between classes can be decreased; thereby yielding higher classification accuracy and more accurate land cover maps. We have adopted certain automatic approaches based on the advanced techniques as Cellular automata and Genetic Algorithm for kernel and tuning parameter selection. Performance evaluation of the proposed methodology in comparison with the existing approaches is performed with reference to the Bhopal city study area. version:1
arxiv-1303-6455 | Performance Evaluation of Edge-Directed Interpolation Methods for Images | http://arxiv.org/abs/1303.6455 | id:1303.6455 author:Shaode Yu, Qingsong Zhu, Shibin Wu, Yaoqin Xie category:cs.CV I.4.1  published:2013-03-26 summary:Many interpolation methods have been developed for high visual quality, but fail for inability to preserve image structures. Edges carry heavy structural information for detection, determination and classification. Edge-adaptive interpolation approaches become a center of focus. In this paper, performance of four edge-directed interpolation methods comparing with two traditional methods is evaluated on two groups of images. These methods include new edge-directed interpolation (NEDI), edge-guided image interpolation (EGII), iterative curvature-based interpolation (ICBI), directional cubic convolution interpolation (DCCI) and two traditional approaches, bi-linear and bi-cubic. Meanwhile, no parameters are mentioned to measure edge-preserving ability of edge-adaptive interpolation approaches and we proposed two. One evaluates accuracy and the other measures robustness of edge-preservation ability. Performance evaluation is based on six parameters. Objective assessment and visual analysis are illustrated and conclusions are drawn from theoretical backgrounds and practical results. version:1
arxiv-1204-5309 | Analysis Operator Learning and Its Application to Image Reconstruction | http://arxiv.org/abs/1204.5309 | id:1204.5309 author:Simon Hawe, Martin Kleinsteuber, Klaus Diepold category:cs.LG cs.CV I.4.5  published:2012-04-24 summary:Exploiting a priori known structural information lies at the core of many image reconstruction methods that can be stated as inverse problems. The synthesis model, which assumes that images can be decomposed into a linear combination of very few atoms of some dictionary, is now a well established tool for the design of image reconstruction algorithms. An interesting alternative is the analysis model, where the signal is multiplied by an analysis operator and the outcome is assumed to be the sparse. This approach has only recently gained increasing interest. The quality of reconstruction methods based on an analysis model severely depends on the right choice of the suitable operator. In this work, we present an algorithm for learning an analysis operator from training images. Our method is based on an $\ell_p$-norm minimization on the set of full rank matrices with normalized columns. We carefully introduce the employed conjugate gradient method on manifolds, and explain the underlying geometry of the constraints. Moreover, we compare our approach to state-of-the-art methods for image denoising, inpainting, and single image super-resolution. Our numerical results show competitive performance of our general approach in all presented applications compared to the specialized state-of-the-art techniques. version:3
arxiv-1110-3564 | Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems | http://arxiv.org/abs/1110.3564 | id:1110.3564 author:David R. Karger, Sewoong Oh, Devavrat Shah category:cs.LG cs.DS cs.HC stat.ML  published:2011-10-17 summary:Crowdsourcing systems, in which numerous tasks are electronically distributed to numerous "information piece-workers", have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all such systems must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in an appropriate manner, e.g. majority voting. In this paper, we consider a general model of such crowdsourcing tasks and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers' answers. We show that our algorithm, inspired by belief propagation and low-rank matrix approximation, significantly outperforms majority voting and, in fact, is optimal through comparison to an oracle that knows the reliability of every worker. Further, we compare our approach with a more general class of algorithms which can dynamically assign tasks. By adaptively deciding which questions to ask to the next arriving worker, one might hope to reduce uncertainty more efficiently. We show that, perhaps surprisingly, the minimum price necessary to achieve a target reliability scales in the same manner under both adaptive and non-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under both scenarios. This strongly relies on the fact that workers are fleeting and can not be exploited. Therefore, architecturally, our results suggest that building a reliable worker-reputation system is essential to fully harnessing the potential of adaptive designs. version:4
arxiv-1303-6377 | Simulation of Fractional Brownian Surfaces via Spectral Synthesis on Manifolds | http://arxiv.org/abs/1303.6377 | id:1303.6377 author:Zachary Gelbaum, Mathew Titus category:cs.CG cs.CV math.PR  published:2013-03-26 summary:Using the spectral decomposition of the Laplace-Beltrami operator we simulate fractal surfaces as random series of eigenfunctions. This approach allows us to generate random fields over smooth manifolds of arbitrary dimension, generalizing previous work with fractional Brownian motion with multi-dimensional parameter. We give examples of surfaces with and without boundary and discuss implementation. version:1
arxiv-1303-6370 | Convex Tensor Decomposition via Structured Schatten Norm Regularization | http://arxiv.org/abs/1303.6370 | id:1303.6370 author:Ryota Tomioka, Taiji Suzuki category:stat.ML cs.LG cs.NA  published:2013-03-26 summary:We discuss structured Schatten norms for tensor decomposition that includes two recently proposed norms ("overlapped" and "latent") for convex-optimization-based tensor decomposition, and connect tensor decomposition with wider literature on structured sparsity. Based on the properties of the structured Schatten norms, we mathematically analyze the performance of "latent" approach for tensor decomposition, which was empirically found to perform better than the "overlapped" approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a specific mode, this approach performs as good as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structures Schatten norms, establish the consistency, and discuss the identifiability of this approach. We confirm through numerical simulations that our theoretical prediction can precisely predict the scaling behavior of the mean squared error. version:1
arxiv-1303-6361 | Video Face Matching using Subset Selection and Clustering of Probabilistic Multi-Region Histograms | http://arxiv.org/abs/1303.6361 | id:1303.6361 author:Sandra Mau, Shaokang Chen, Conrad Sanderson, Brian C. Lovell category:cs.CV cs.IR  published:2013-03-26 summary:Balancing computational efficiency with recognition accuracy is one of the major challenges in real-world video-based face recognition. A significant design decision for any such system is whether to process and use all possible faces detected over the video frames, or whether to select only a few "best" faces. This paper presents a video face recognition system based on probabilistic Multi-Region Histograms to characterise performance trade-offs in: (i) selecting a subset of faces compared to using all faces, and (ii) combining information from all faces via clustering. Three face selection metrics are evaluated for choosing a subset: face detection confidence, random subset, and sequential selection. Experiments on the recently introduced MOBIO dataset indicate that the usage of all faces through clustering always outperformed selecting only a subset of faces. The experiments also show that the face selection metric based on face detection confidence generally provides better recognition performance than random or sequential sampling. Moreover, the optimal number of faces varies drastically across selection metric and subsets of MOBIO. Given the trade-offs between computational effort, recognition accuracy and robustness, it is recommended that face feature clustering would be most advantageous in batch processing (particularly for video-based watchlists), whereas face selection methods should be limited to applications with significant computational restrictions. version:1
arxiv-1211-4518 | Hypothesis Testing in Feedforward Networks with Broadcast Failures | http://arxiv.org/abs/1211.4518 | id:1211.4518 author:Zhenliang Zhang, Edwin K. P. Chong, Ali Pezeshki, William Moran category:cs.IT cs.LG math.IT  published:2012-11-19 summary:Consider a countably infinite set of nodes, which sequentially make decisions between two given hypotheses. Each node takes a measurement of the underlying truth, observes the decisions from some immediate predecessors, and makes a decision between the given hypotheses. We consider two classes of broadcast failures: 1) each node broadcasts a decision to the other nodes, subject to random erasure in the form of a binary erasure channel; 2) each node broadcasts a randomly flipped decision to the other nodes in the form of a binary symmetric channel. We are interested in whether there exists a decision strategy consisting of a sequence of likelihood ratio tests such that the node decisions converge in probability to the underlying truth. In both cases, we show that if each node only learns from a bounded number of immediate predecessors, then there does not exist a decision strategy such that the decisions converge in probability to the underlying truth. However, in case 1, we show that if each node learns from an unboundedly growing number of predecessors, then the decisions converge in probability to the underlying truth, even when the erasure probabilities converge to 1. We also derive the convergence rate of the error probability. In case 2, we show that if each node learns from all of its previous predecessors, then the decisions converge in probability to the underlying truth when the flipping probabilities of the binary symmetric channels are bounded away from 1/2. In the case where the flipping probabilities converge to 1/2, we derive a necessary condition on the convergence rate of the flipping probabilities such that the decisions still converge to the underlying truth. We also explicitly characterize the relationship between the convergence rate of the error probability and the convergence rate of the flipping probabilities. version:3
arxiv-1303-6223 | Random Intersection Trees | http://arxiv.org/abs/1303.6223 | id:1303.6223 author:Rajen Dinesh Shah, Nicolai Meinshausen category:stat.ML stat.CO stat.ME  published:2013-03-25 summary:Finding interactions between variables in large and high-dimensional datasets is often a serious computational challenge. Most approaches build up interaction sets incrementally, adding variables in a greedy fashion. The drawback is that potentially informative high-order interactions may be overlooked. Here, we propose at an alternative approach for classification problems with binary predictor variables, called Random Intersection Trees. It works by starting with a maximal interaction that includes all variables, and then gradually removing variables if they fail to appear in randomly chosen observations of a class of interest. We show that informative interactions are retained with high probability, and the computational complexity of our procedure is of order $p^\kappa$ for a value of $\kappa$ that can reach values as low as 1 for very sparse data; in many more general settings, it will still beat the exponent $s$ obtained when using a brute force search constrained to order $s$ interactions. In addition, by using some new ideas based on min-wise hash schemes, we are able to further reduce the computational cost. Interactions found by our algorithm can be used for predictive modelling in various forms, but they are also often of interest in their own right as useful characterisations of what distinguishes a certain class from others. version:1
arxiv-1110-4531 | Regression for sets of polynomial equations | http://arxiv.org/abs/1110.4531 | id:1110.4531 author:Franz Johannes Király, Paul von Bünau, Jan Saputra Müller, Duncan Blythe, Frank Meinecke, Klaus-Robert Müller category:stat.ML  published:2011-10-20 summary:We propose a method called ideal regression for approximating an arbitrary system of polynomial equations by a system of a particular type. Using techniques from approximate computational algebraic geometry, we show how we can solve ideal regression directly without resorting to numerical optimization. Ideal regression is useful whenever the solution to a learning problem can be described by a system of polynomial equations. As an example, we demonstrate how to formulate Stationary Subspace Analysis (SSA), a source separation problem, in terms of ideal regression, which also yields a consistent estimator for SSA. We then compare this estimator in simulations with previous optimization-based approaches for SSA. version:4
arxiv-1303-6175 | Compression as a universal principle of animal behavior | http://arxiv.org/abs/1303.6175 | id:1303.6175 author:R. Ferrer-i-Cancho, A. Hernández-Fernández, D. Lusseau, G. Agoramoorthy, M. J. Hsu, S. Semple category:q-bio.NC cs.CL cs.IT math.IT physics.data-an q-bio.QM  published:2013-03-25 summary:A key aim in biology and psychology is to identify fundamental principles underpinning the behavior of animals, including humans. Analyses of human language and the behavior of a range of non-human animal species have provided evidence for a common pattern underlying diverse behavioral phenomena: words follow Zipf's law of brevity (the tendency of more frequently used words to be shorter), and conformity to this general pattern has been seen in the behavior of a number of other animals. It has been argued that the presence of this law is a sign of efficient coding in the information theoretic sense. However, no strong direct connection has been demonstrated between the law and compression, the information theoretic principle of minimizing the expected length of a code. Here we show that minimizing the expected code length implies that the length of a word cannot increase as its frequency increases. Furthermore, we show that the mean code length or duration is significantly small in human language, and also in the behavior of other species in all cases where agreement with the law of brevity has been found. We argue that compression is a general principle of animal behavior, that reflects selection for efficiency of coding. version:1
arxiv-1303-6086 | On Sparsity Inducing Regularization Methods for Machine Learning | http://arxiv.org/abs/1303.6086 | id:1303.6086 author:Andreas Argyriou, Luca Baldassarre, Charles A. Micchelli, Massimiliano Pontil category:cs.LG stat.ML  published:2013-03-25 summary:During the past years there has been an explosion of interest in learning methods based on sparsity regularization. In this paper, we discuss a general class of such methods, in which the regularizer can be expressed as the composition of a convex function $\omega$ with a linear function. This setting includes several methods such the group Lasso, the Fused Lasso, multi-task learning and many more. We present a general approach for solving regularization problems of this kind, under the assumption that the proximity operator of the function $\omega$ is available. Furthermore, we comment on the application of this approach to support vector machines, a technique pioneered by the groundbreaking work of Vladimir Vapnik. version:1
arxiv-1206-6389 | Poisoning Attacks against Support Vector Machines | http://arxiv.org/abs/1206.6389 | id:1206.6389 author:Battista Biggio, Blaine Nelson, Pavel Laskov category:cs.LG cs.CR stat.ML  published:2012-06-27 summary:We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error. version:3
arxiv-1303-6021 | Spatio-Temporal Covariance Descriptors for Action and Gesture Recognition | http://arxiv.org/abs/1303.6021 | id:1303.6021 author:Andres Sanin, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell category:cs.CV cs.HC  published:2013-03-25 summary:We propose a new action and gesture recognition method based on spatio-temporal covariance descriptors and a weighted Riemannian locality preserving projection approach that takes into account the curved space formed by the descriptors. The weighted projection is then exploited during boosting to create a final multiclass classification algorithm that employs the most useful spatio-temporal regions. We also show how the descriptors can be computed quickly through the use of integral video representations. Experiments on the UCF sport, CK+ facial expression and Cambridge hand gesture datasets indicate superior performance of the proposed method compared to several recent state-of-the-art techniques. The proposed method is robust and does not require additional processing of the videos, such as foreground detection, interest-point detection or tracking. version:1
arxiv-1303-6001 | Generalizing k-means for an arbitrary distance matrix | http://arxiv.org/abs/1303.6001 | id:1303.6001 author:Balázs Szalkai category:cs.LG cs.CV stat.ML  published:2013-03-24 summary:The original k-means clustering method works only if the exact vectors representing the data points are known. Therefore calculating the distances from the centroids needs vector operations, since the average of abstract data points is undefined. Existing algorithms can be extended for those cases when the sole input is the distance matrix, and the exact representing vectors are unknown. This extension may be named relational k-means after a notation for a similar algorithm invented for fuzzy clustering. A method is then proposed for generalizing k-means for scenarios when the data points have absolutely no connection with a Euclidean space. version:1
arxiv-1303-5984 | Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems | http://arxiv.org/abs/1303.5984 | id:1303.5984 author:Morteza Ibrahimi, Adel Javanmard, Benjamin Van Roy category:stat.ML cs.LG math.OC  published:2013-03-24 summary:We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes. More recently, for the average cost LQ problem, a regret bound of ${O}(\sqrt{T})$ was shown, apart form logarithmic factors. However, this bound scales exponentially with $p$, the dimension of the state space. In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large. We present an adaptive control scheme that achieves a regret bound of ${O}(p \sqrt{T})$, apart from logarithmic factors. In particular, our algorithm has an average cost of $(1+\eps)$ times the optimum cost after $T = \polylog(p) O(1/\eps^2)$. This is in comparison to previous work on the dense dynamics where the algorithm requires time that scales exponentially with dimension in order to achieve regret of $\eps$ times the optimal cost. We believe that our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks. version:1
arxiv-1303-5976 | On Learnability, Complexity and Stability | http://arxiv.org/abs/1303.5976 | id:1303.5976 author:Silvia Villa, Lorenzo Rosasco, Tomaso Poggio category:stat.ML cs.LG  published:2013-03-24 summary:We consider the fundamental question of learnability of a hypotheses class in the supervised learning setting and in the general learning setting introduced by Vladimir Vapnik. We survey classic results characterizing learnability in term of suitable notions of complexity, as well as more recent results that establish the connection between learnability and stability of a learning algorithm. version:1
arxiv-1303-5913 | A Diffusion Process on Riemannian Manifold for Visual Tracking | http://arxiv.org/abs/1303.5913 | id:1303.5913 author:Marcus Chen, Cham Tat Jen, Pang Sze Kim, Alvina Goh category:cs.CV cs.LG cs.RO stat.ML  published:2013-03-24 summary:Robust visual tracking for long video sequences is a research area that has many important applications. The main challenges include how the target image can be modeled and how this model can be updated. In this paper, we model the target using a covariance descriptor, as this descriptor is robust to problems such as pixel-pixel misalignment, pose and illumination changes, that commonly occur in visual tracking. We model the changes in the template using a generative process. We introduce a new dynamical model for the template update using a random walk on the Riemannian manifold where the covariance descriptors lie in. This is done using log-transformed space of the manifold to free the constraints imposed inherently by positive semidefinite matrices. Modeling template variations and poses kinetics together in the state space enables us to jointly quantify the uncertainties relating to the kinematic states and the template in a principled way. Finally, the sequential inference of the posterior distribution of the kinematic states and the template is done using a particle filter. Our results shows that this principled approach can be robust to changes in illumination, poses and spatial affine transformation. In the experiments, our method outperformed the current state-of-the-art algorithm - the incremental Principal Component Analysis method, particularly when a target underwent fast poses changes and also maintained a comparable performance in stable target tracking cases. version:1
arxiv-1210-3098 | Near-optimal compressed sensing guarantees for total variation minimization | http://arxiv.org/abs/1210.3098 | id:1210.3098 author:Deanna Needell, Rachel Ward category:math.NA cs.CV cs.IT math.IT  published:2012-10-11 summary:Consider the problem of reconstructing a multidimensional signal from an underdetermined set of measurements, as in the setting of compressed sensing. Without any additional assumptions, this problem is ill-posed. However, for signals such as natural images or movies, the minimal total variation estimate consistent with the measurements often produces a good approximation to the underlying signal, even if the number of measurements is far smaller than the ambient dimensionality. This paper extends recent reconstruction guarantees for two-dimensional images to signals of arbitrary dimension d>1 and to isotropic total variation problems. To be precise, we show that a multidimensional signal x can be reconstructed from O(sd*log(N^d)) linear measurements using total variation minimization to within a factor of the best s-term approximation of its gradient. The reconstruction guarantees we provide are necessarily optimal up to polynomial factors in the spatial dimension d. version:2
arxiv-1210-6321 | High quality topic extraction from business news explains abnormal financial market volatility | http://arxiv.org/abs/1210.6321 | id:1210.6321 author:Ryohei Hisano, Didier Sornette, Takayuki Mizuno, Takaaki Ohnishi, Tsutomu Watanabe category:stat.ML cs.LG cs.SI physics.soc-ph q-fin.ST  published:2012-10-23 summary:Understanding the mutual relationships between information flows and social activity in society today is one of the cornerstones of the social sciences. In financial economics, the key issue in this regard is understanding and quantifying how news of all possible types (geopolitical, environmental, social, financial, economic, etc.) affect trading and the pricing of firms in organized stock markets. In this article, we seek to address this issue by performing an analysis of more than 24 million news records provided by Thompson Reuters and of their relationship with trading activity for 206 major stocks in the S&P US stock index. We show that the whole landscape of news that affect stock price movements can be automatically summarized via simple regularized regressions between trading activity and news information pieces decomposed, with the help of simple topic modeling techniques, into their "thematic" features. Using these methods, we are able to estimate and quantify the impacts of news on trading. We introduce network-based visualization techniques to represent the whole landscape of news information associated with a basket of stocks. The examination of the words that are representative of the topic distributions confirms that our method is able to extract the significant pieces of information influencing the stock market. Our results show that one of the most puzzling stylized fact in financial economies, namely that at certain times trading volumes appear to be "abnormally large," can be partially explained by the flow of news. In this sense, our results prove that there is no "excess trading," when restricting to times when news are genuinely novel and provide relevant financial information. version:4
arxiv-1211-0358 | Deep Gaussian Processes | http://arxiv.org/abs/1211.0358 | id:1211.0358 author:Andreas C. Damianou, Neil D. Lawrence category:stat.ML cs.LG math.PR 60G15  58E30 G.3; G.1.2; I.2.6  published:2012-11-02 summary:In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples. version:2
arxiv-1303-5778 | Speech Recognition with Deep Recurrent Neural Networks | http://arxiv.org/abs/1303.5778 | id:1303.5778 author:Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton category:cs.NE cs.CL  published:2013-03-22 summary:Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score. version:1
arxiv-1303-0642 | Bayesian Compressed Regression | http://arxiv.org/abs/1303.0642 | id:1303.0642 author:Rajarshi Guhaniyogi, David B. Dunson category:stat.ML cs.LG  published:2013-03-04 summary:As an alternative to variable selection or shrinkage in high dimensional regression, we propose to randomly compress the predictors prior to analysis. This dramatically reduces storage and computational bottlenecks, performing well when the predictors can be projected to a low dimensional linear subspace with minimal loss of information about the response. As opposed to existing Bayesian dimensionality reduction approaches, the exact posterior distribution conditional on the compressed data is available analytically, speeding up computation by many orders of magnitude while also bypassing robustness issues due to convergence and mixing problems with MCMC. Model averaging is used to reduce sensitivity to the random projection matrix, while accommodating uncertainty in the subspace dimension. Strong theoretical support is provided for the approach by showing near parametric convergence rates for the predictive density in the large p small n asymptotic paradigm. Practical performance relative to competitors is illustrated in simulations and real data applications. version:2
arxiv-1303-5691 | Cortical Surface Co-Registration based on MRI Images and Photos | http://arxiv.org/abs/1303.5691 | id:1303.5691 author:Benjamin Berkels, Ivan Cabrilo, Sven Haller, Martin Rumpf, Carlo Schaller category:cs.CV  published:2013-03-22 summary:Brain shift, i.e. the change in configuration of the brain after opening the dura mater, is a key problem in neuronavigation. We present an approach to co-register intra-operative microscope images with pre-operative MRI to adapt and optimize intra-operative neuronavigation. The tools are a robust classification of sulci on MRI extracted cortical surfaces, guided user marking of most prominent sulci on a microscope image, and the actual variational registration method with a fidelity energy for 3D deformations of the cortical surface combined with a higher order, linear elastica type prior energy. Furthermore, the actual registration is validated on an artificial testbed with known ground truth deformation and on real data of a neuro clinical patient. version:1
arxiv-1207-6656 | Measuring the Complexity of Ultra-Large-Scale Adaptive Systems | http://arxiv.org/abs/1207.6656 | id:1207.6656 author:Michele Amoretti, Carlos Gershenson category:cs.NE cs.NI nlin.AO  published:2012-07-27 summary:Ultra-large scale (ULS) systems are becoming pervasive. They are inherently complex, which makes their design and control a challenge for traditional methods. Here we propose the design and analysis of ULS systems using measures of complexity, emergence, self-organization, and homeostasis based on information theory. These measures allow the evaluation of ULS systems and thus can be used to guide their design. We evaluate the proposal with a ULS computing system provided with adaptation mechanisms. We show the evolution of the system with stable and also changing workload, using different fitness functions. When the adaptive plan forces the system to converge to a predefined performance level, the nodes may result in highly unstable configurations, that correspond to a high variance in time of the measured complexity. Conversely, if the adaptive plan is less "aggressive", the system may be more stable, but the optimal performance may not be achieved. version:2
arxiv-1303-5613 | Network Detection Theory and Performance | http://arxiv.org/abs/1303.5613 | id:1303.5613 author:Steven T. Smith, Kenneth D. Senne, Scott Philips, Edward K. Kao, Garrett Bernstein category:cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH  published:2013-03-22 summary:Network detection is an important capability in many areas of applied research in which data can be represented as a graph of entities and relationships. Oftentimes the object of interest is a relatively small subgraph in an enormous, potentially uninteresting background. This aspect characterizes network detection as a "big data" problem. Graph partitioning and network discovery have been major research areas over the last ten years, driven by interest in internet search, cyber security, social networks, and criminal or terrorist activities. The specific problem of network discovery is addressed as a special case of graph partitioning in which membership in a small subgraph of interest must be determined. Algebraic graph theory is used as the basis to analyze and compare different network detection methods. A new Bayesian network detection framework is introduced that partitions the graph based on prior information and direct observations. The new approach, called space-time threat propagation, is proved to maximize the probability of detection and is therefore optimum in the Neyman-Pearson sense. This optimality criterion is compared to spectral community detection approaches which divide the global graph into subsets or communities with optimal connectivity properties. We also explore a new generative stochastic model for covert networks and analyze using receiver operating characteristics the detection performance of both classes of optimal detection techniques. version:1
arxiv-1303-5588 | Robust and Trend Following Student's t Kalman Smoothers | http://arxiv.org/abs/1303.5588 | id:1303.5588 author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:math.OC math.NA stat.AP stat.ML 62F35  65K10  published:2013-03-22 summary:We present a Kalman smoothing framework based on modeling errors using the heavy tailed Student's t distribution, along with algorithms, convergence theory, open-source general implementation, and several important applications. The computational effort per iteration grows linearly with the length of the time series, and all smoothers allow nonlinear process and measurement models. Robust smoothers form an important subclass of smoothers within this framework. These smoothers work in situations where measurements are highly contaminated by noise or include data unexplained by the forward model. Highly robust smoothers are developed by modeling measurement errors using the Student's t distribution, and outperform the recently proposed L1-Laplace smoother in extreme situations with data containing 20% or more outliers. A second special application we consider in detail allows tracking sudden changes in the state. It is developed by modeling process noise using the Student's t distribution, and the resulting smoother can track sudden changes in the state. These features can be used separately or in tandem, and we present a general smoother algorithm and open source implementation, together with convergence analysis that covers a wide range of smoothers. A key ingredient of our approach is a technique to deal with the non-convexity of the Student's t loss function. Numerical results for linear and nonlinear models illustrate the performance of the new smoothers for robust and tracking applications, as well as for mixed problems that have both types of features. version:1
arxiv-1211-2361 | Genetic Algorithm for Designing a Convenient Facility Layout for a Circular Flow Path | http://arxiv.org/abs/1211.2361 | id:1211.2361 author:Hossein Jahandideh, Ardavan Asef-Vaziri, Mohammad Modarres category:cs.NE  published:2012-11-11 summary:In this paper, we present a heuristic for designing facility layouts that are convenient for designing a unidirectional loop for material handling. We use genetic algorithm where the objective function and crossover and mutation operators have all been designed specifically for this purpose. Our design is made under flexible bay structure and comparisons are made with other layouts from the literature that were designed under flexible bay structure. version:2
arxiv-1303-5515 | Adverse Conditions and ASR Techniques for Robust Speech User Interface | http://arxiv.org/abs/1303.5515 | id:1303.5515 author:Urmila Shrawankar, VM Thakare category:cs.CL cs.SD  published:2013-03-22 summary:The main motivation for Automatic Speech Recognition (ASR) is efficient interfaces to computers, and for the interfaces to be natural and truly useful, it should provide coverage for a large group of users. The purpose of these tasks is to further improve man-machine communication. ASR systems exhibit unacceptable degradations in performance when the acoustical environments used for training and testing the system are not the same. The goal of this research is to increase the robustness of the speech recognition systems with respect to changes in the environment. A system can be labeled as environment-independent if the recognition accuracy for a new environment is the same or higher than that obtained when the system is retrained for that environment. Attaining such performance is the dream of the researchers. This paper elaborates some of the difficulties with Automatic Speech Recognition (ASR). These difficulties are classified into Speakers characteristics and environmental conditions, and tried to suggest some techniques to compensate variations in speech signal. This paper focuses on the robustness with respect to speakers variations and changes in the acoustical environment. We discussed several different external factors that change the environment and physiological differences that affect the performance of a speech recognition system followed by techniques that are helpful to design a robust ASR system. version:1
arxiv-1303-5513 | Parameters Optimization for Improving ASR Performance in Adverse Real World Noisy Environmental Conditions | http://arxiv.org/abs/1303.5513 | id:1303.5513 author:Urmila Shrawankar, Vilas Thakare category:cs.CL cs.SD  published:2013-03-22 summary:From the existing research it has been observed that many techniques and methodologies are available for performing every step of Automatic Speech Recognition (ASR) system, but the performance (Minimization of Word Error Recognition-WER and Maximization of Word Accuracy Rate- WAR) of the methodology is not dependent on the only technique applied in that method. The research work indicates that, performance mainly depends on the category of the noise, the level of the noise and the variable size of the window, frame, frame overlap etc is considered in the existing methods. The main aim of the work presented in this paper is to use variable size of parameters like window size, frame size and frame overlap percentage to observe the performance of algorithms for various categories of noise with different levels and also train the system for all size of parameters and category of real world noisy environment to improve the performance of the speech recognition system. This paper presents the results of Signal-to-Noise Ratio (SNR) and Accuracy test by applying variable size of parameters. It is observed that, it is really very hard to evaluate test results and decide parameter size for ASR performance improvement for its resultant optimization. Hence, this study further suggests the feasible and optimum parameter size using Fuzzy Inference System (FIS) for enhancing resultant accuracy in adverse real world noisy environmental conditions. This work will be helpful to give discriminative training of ubiquitous ASR system for better Human Computer Interaction (HCI). version:1
arxiv-1206-1846 | Warped Mixtures for Nonparametric Cluster Shapes | http://arxiv.org/abs/1206.1846 | id:1206.1846 author:Tomoharu Iwata, David Duvenaud, Zoubin Ghahramani category:stat.ML I.5.3  published:2012-06-08 summary:A mixture of Gaussians fit to a single curved or heavy-tailed cluster will report that the data contains many clusters. To produce more appropriate clusterings, we introduce a model which warps a latent mixture of Gaussians to produce nonparametric cluster shapes. The possibly low-dimensional latent mixture model allows us to summarize the properties of the high-dimensional clusters (or density manifolds) describing the data. The number of manifolds, as well as the shape and dimension of each manifold is automatically inferred. We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function. We show that our model is effective for density estimation, performs better than infinite Gaussian mixture models at recovering the true number of clusters, and produces interpretable summaries of high-dimensional datasets. version:2
arxiv-1301-3485 | A Semantic Matching Energy Function for Learning with Multi-relational Data | http://arxiv.org/abs/1301.3485 | id:1301.3485 author:Xavier Glorot, Antoine Bordes, Jason Weston, Yoshua Bengio category:cs.LG  published:2013-01-15 summary:Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature. version:2
arxiv-1303-5244 | Separable Dictionary Learning | http://arxiv.org/abs/1303.5244 | id:1303.5244 author:Simon Hawe, Matthias Seibert, Martin Kleinsteuber category:cs.CV cs.LG stat.ML  published:2013-03-21 summary:Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary properties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD. version:1
arxiv-1303-5050 | Using evolutionary design to interactively sketch car silhouettes and stimulate designer's creativity | http://arxiv.org/abs/1303.5050 | id:1303.5050 author:François Cluzel, Bernard Yannou, Markus Dihlmann category:cs.NE cs.HC physics.med-ph  published:2013-03-17 summary:An Interactive Genetic Algorithm is proposed to progressively sketch the desired side-view of a car profile. It adopts a Fourier decomposition of a 2D profile as the genotype, and proposes a cross-over mechanism. In addition, a formula function of two genes' discrepancies is fitted to the perceived dissimilarity between two car profiles. This similarity index is intensively used, throughout a series of user tests, to highlight the added value of the IGA compared to a systematic car shape exploration, to prove its ability to create superior satisfactory designs and to stimulate designer's creativity. These tests have involved six designers with a design goal defined by a semantic attribute. The results reveal that if "friendly" is diversely interpreted in terms of car shapes, "sportive" denotes a very conventional representation which may be a limitation for shape renewal. version:2
arxiv-1303-5148 | Estimating Confusions in the ASR Channel for Improved Topic-based Language Model Adaptation | http://arxiv.org/abs/1303.5148 | id:1303.5148 author:Damianos Karakos, Mark Dredze, Sanjeev Khudanpur category:cs.CL cs.LG  published:2013-03-21 summary:Human language is a combination of elemental languages/domains/styles that change across and sometimes within discourses. Language models, which play a crucial role in speech recognizers and machine translation systems, are particularly sensitive to such changes, unless some form of adaptation takes place. One approach to speech language model adaptation is self-training, in which a language model's parameters are tuned based on automatically transcribed audio. However, transcription errors can misguide self-training, particularly in challenging settings such as conversational speech. In this work, we propose a model that considers the confusions (errors) of the ASR channel. By modeling the likely confusions in the ASR output instead of using just the 1-best, we improve self-training efficacy by obtaining a more reliable reference transcription estimate. We demonstrate improved topic-based language modeling adaptation results over both 1-best and lattice self-training using our ASR channel confusion estimates on telephone conversations. version:1
arxiv-1301-3630 | Behavior Pattern Recognition using A New Representation Model | http://arxiv.org/abs/1301.3630 | id:1301.3630 author:Qifeng Qiao, Peter A. Beling category:cs.LG  published:2013-01-16 summary:We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents' behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for recognition problems. version:4
arxiv-1301-3577 | Saturating Auto-Encoders | http://arxiv.org/abs/1301.3577 | id:1301.3577 author:Rostislav Goroshin, Yann LeCun category:cs.LG  published:2013-01-16 summary:We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders. version:3
arxiv-1303-4959 | Analytic solution of a model of language competition with bilingualism and interlinguistic similarity | http://arxiv.org/abs/1303.4959 | id:1303.4959 author:Victoria Otero-Espinar, Luís F. Seoane, Juan J. Nieto, Jorge Mira category:physics.soc-ph cs.CL  published:2013-03-20 summary:An in-depth analytic study of a model of language dynamics is presented: a model which tackles the problem of the coexistence of two languages within a closed community of speakers taking into account bilingualism and incorporating a parameter to measure the distance between languages. After previous numerical simulations, the model yielded that coexistence might lead to survival of both languages within monolingual speakers along with a bilingual community or to extinction of the weakest tongue depending on different parameters. In this paper, such study is closed with thorough analytical calculations to settle the results in a robust way and previous results are refined with some modifications. From the present analysis it is possible to almost completely assay the number and nature of the equilibrium points of the model, which depend on its parameters, as well as to build a phase space based on them. Also, we obtain conclusions on the way the languages evolve with time. Our rigorous considerations also suggest ways to further improve the model and facilitate the comparison of its consequences with those from other approaches or with real data. version:1
arxiv-1303-4866 | A Robust Rapid Approach to Image Segmentation with Optimal Thresholding and Watershed Transform | http://arxiv.org/abs/1303.4866 | id:1303.4866 author:Ankit R. Chadha, Neha S. Satam category:cs.CV  published:2013-03-20 summary:This paper describes a novel method for partitioning image into meaningful segments. The proposed method employs watershed transform, a well-known image segmentation technique. Along with that, it uses various auxiliary schemes such as Binary Gradient Masking, dilation which segment the image in proper way. The algorithm proposed in this paper considers all these methods in effective way and takes little time. It is organized in such a manner so that it operates on input image adaptively. Its robustness and efficiency makes it more convenient and suitable for all types of images. version:1
arxiv-1303-4840 | Asynchronous Cellular Operations on Gray Images Extracting Topographic Shape Features and Their Relations | http://arxiv.org/abs/1303.4840 | id:1303.4840 author:Igor Polkovnikov category:cs.CV  published:2013-03-20 summary:A variety of operations of cellular automata on gray images is presented. All operations are of a wave-front nature finishing in a stable state. They are used to extract shape descripting gray objects robust to a variety of pattern distortions. Topographic terms are used: "lakes", "dales", "dales of dales". It is shown how mutual object relations like "above" can be presented in terms of gray image analysis and how it can be used for character classification and for gray pattern decomposition. Algorithms can be realized with a parallel asynchronous architecture. Keywords: Pattern Recognition, Mathematical Morphology, Cellular Automata, Wave-front Algorithms, Gray Image Analysis, Topographical Shape Descriptors, Asynchronous Parallel Processors, Holes, Cavities, Concavities, Graphs. version:1
arxiv-1303-4839 | The State of the Art Recognize in Arabic Script through Combination of Online and Offline | http://arxiv.org/abs/1303.4839 | id:1303.4839 author:Dr. Firoj Parwej category:cs.CV  published:2013-03-20 summary:Handwriting recognition refers to the identification of written characters. Handwriting recognition has become an acute research area in recent years for the ease of access of computer science. In this paper primarily discussed On-line and Off-line handwriting recognition methods for Arabic words which are often used among then across the Middle East and North Africa People. Arabic word online handwriting recognition is a very challenging task due to its cursive nature. Because of the characteristic of the whole body of the Arabic script, namely connectivity between the characters, thereby the segmentation of An Arabic script is very difficult. In this paper we introduced an Arabic script multiple classifier system for recognizing notes written on a Starboard. This Arabic script multiple classifier system combines one off-line and on-line handwriting recognition systems. The Arabic script recognizers are all based on Hidden Markov Models but vary in the way of preprocessing and normalization. To combine the Arabic script output sequences of the recognizers, we incrementally align the word sequences using a norm string matching algorithm. The Arabic script combination we could increase the system performance over the excellent character recognizer by about 3%. The proposed technique is also the necessary step towards character recognition, person identification, personality determination where input data is processed from all perspectives. version:1
arxiv-1303-4803 | A Survey of Appearance Models in Visual Object Tracking | http://arxiv.org/abs/1303.4803 | id:1303.4803 author:Xi Li, Weiming Hu, Chunhua Shen, Zhongfei Zhang, Anthony Dick, Anton van den Hengel category:cs.CV  published:2013-03-20 summary:Visual object tracking is a significant computer vision task which can be applied to many domains such as visual surveillance, human computer interaction, and video compression. In the literature, researchers have proposed a variety of 2D appearance models. To help readers swiftly learn the recent advances in 2D appearance models for visual object tracking, we contribute this survey, which provides a detailed review of the existing 2D appearance models. In particular, this survey takes a module-based architecture that enables readers to easily grasp the key points of visual object tracking. In this survey, we first decompose the problem of appearance modeling into two different processing stages: visual representation and statistical modeling. Then, different 2D appearance models are categorized and discussed with respect to their composition modules. Finally, we address several issues of interest as well as the remaining challenges for future research on this topic. The contributions of this survey are four-fold. First, we review the literature of visual representations according to their feature-construction mechanisms (i.e., local and global). Second, the existing statistical modeling schemes for tracking-by-detection are reviewed according to their model-construction mechanisms: generative, discriminative, and hybrid generative-discriminative. Third, each type of visual representations or statistical modeling techniques is analyzed and discussed from a theoretical or practical viewpoint. Fourth, the existing benchmark resources (e.g., source code and video datasets) are examined in this survey. version:1
arxiv-1301-3666 | Zero-Shot Learning Through Cross-Modal Transfer | http://arxiv.org/abs/1301.3666 | id:1301.3666 author:Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert Bastani, Christopher D. Manning, Andrew Y. Ng category:cs.CV cs.LG  published:2013-01-16 summary:This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images. version:2
arxiv-1209-0016 | On the convergence of maximum variance unfolding | http://arxiv.org/abs/1209.0016 | id:1209.0016 author:Ery Arias-Castro, Bruno Pelletier category:stat.ML  published:2012-08-31 summary:Maximum Variance Unfolding is one of the main methods for (nonlinear) dimensionality reduction. We study its large sample limit, providing specific rates of convergence under standard assumptions. We find that it is consistent when the underlying submanifold is isometric to a convex subset, and we provide some simple examples where it fails to be consistent. version:2
arxiv-1303-0073 | A Method for Comparing Hedge Funds | http://arxiv.org/abs/1303.0073 | id:1303.0073 author:Uri Kartoun category:q-fin.ST cs.IR cs.LG stat.ML  published:2013-03-01 summary:The paper presents new machine learning methods: signal composition, which classifies time-series regardless of length, type, and quantity; and self-labeling, a supervised-learning enhancement. The paper describes further the implementation of the methods on a financial search engine system to identify behavioral similarities among time-series representing monthly returns of 11,312 hedge funds operated during approximately one decade (2000 - 2010). The presented approach of cross-category and cross-location classification assists the investor to identify alternative investments. version:2
arxiv-1303-0076 | Bio-Signals-based Situation Comparison Approach to Predict Pain | http://arxiv.org/abs/1303.0076 | id:1303.0076 author:Uri Kartoun category:stat.AP cs.LG stat.ML  published:2013-03-01 summary:This paper describes a time-series-based classification approach to identify similarities between bio-medical-based situations. The proposed approach allows classifying collections of time-series representing bio-medical measurements, i.e., situations, regardless of the type, the length and the quantity of the time-series a situation comprised of. version:2
arxiv-1303-0283 | Inverse Signal Classification for Financial Instruments | http://arxiv.org/abs/1303.0283 | id:1303.0283 author:Uri Kartoun category:cs.LG cs.IR q-fin.ST stat.ML  published:2013-03-01 summary:The paper presents new machine learning methods: signal composition, which classifies time-series regardless of length, type, and quantity; and self-labeling, a supervised-learning enhancement. The paper describes further the implementation of the methods on a financial search engine system using a collection of 7,881 financial instruments traded during 2011 to identify inverse behavior among the time-series. version:2
arxiv-1301-1459 | A proximal Newton framework for composite minimization: Graph learning without Cholesky decompositions and matrix inversions | http://arxiv.org/abs/1301.1459 | id:1301.1459 author:Quoc Tran Dinh, Anastasios Kyrillidis, Volkan Cevher category:stat.ML math.OC  published:2013-01-08 summary:We propose an algorithmic framework for convex minimization problems of a composite function with two terms: a self-concordant function and a possibly nonsmooth regularization term. Our method is a new proximal Newton algorithm that features a local quadratic convergence rate. As a specific instance of our framework, we consider the sparse inverse covariance matrix estimation in graph learning problems. Via a careful dual formulation and a novel analytic step-size selection procedure, our approach for graph learning avoids Cholesky decompositions and matrix inversions in its iteration making it attractive for parallel and distributed implementations. version:3
arxiv-1301-3775 | Discriminative Recurrent Sparse Auto-Encoders | http://arxiv.org/abs/1301.3775 | id:1301.3775 author:Jason Tyler Rolfe, Yann LeCun category:cs.LG cs.CV  published:2013-01-16 summary:We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST. version:4
arxiv-1303-4664 | Large-Scale Learning with Less RAM via Randomization | http://arxiv.org/abs/1303.4664 | id:1303.4664 author:Daniel Golovin, D. Sculley, H. Brendan McMahan, Michael Young category:cs.LG  published:2013-03-19 summary:We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50% during training and by up to 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs. version:1
arxiv-1303-4614 | Handwritten and Printed Text Separation in Real Document | http://arxiv.org/abs/1303.4614 | id:1303.4614 author:Abdel Belaïd, K. C. Santosh, Vincent Poulain D'Andecy category:cs.CV  published:2013-03-19 summary:The aim of the paper is to separate handwritten and printed text from a real document embedded with noise, graphics including annotations. Relying on run-length smoothing algorithm (RLSA), the extracted pseudo-lines and pseudo-words are used as basic blocks for classification. To handle this, a multi-class support vector machine (SVM) with Gaussian kernel performs a first labelling of each pseudo-word including the study of local neighbourhood. It then propagates the context between neighbours so that we can correct possible labelling errors. Considering running time complexity issue, we propose linear complexity methods where we use k-NN with constraint. When using a kd-tree, it is almost linearly proportional to the number of pseudo-words. The performance of our system is close to 90%, even when very small learning dataset where samples are basically composed of complex administrative documents. version:1
arxiv-1212-0634 | Better subset regression | http://arxiv.org/abs/1212.0634 | id:1212.0634 author:Shifeng Xiong category:stat.ME math.ST stat.CO stat.ML stat.TH 62J07 D.2.2  published:2012-12-04 summary:To find efficient screening methods for high dimensional linear regression models, this paper studies the relationship between model fitting and screening performance. Under a sparsity assumption, we show that a subset that includes the true submodel always yields smaller residual sum of squares (i.e., has better model fitting) than all that do not in a general asymptotic setting. This indicates that, for screening important variables, we could follow a "better fitting, better screening" rule, i.e., pick a "better" subset that has better model fitting. To seek such a better subset, we consider the optimization problem associated with best subset regression. An EM algorithm, called orthogonalizing subset screening, and its accelerating version are proposed for searching for the best subset. Although the two algorithms cannot guarantee that a subset they yield is the best, their monotonicity property makes the subset have better model fitting than initial subsets generated by popular screening methods, and thus the subset can have better screening performance asymptotically. Simulation results show that our methods are very competitive in high dimensional variable screening even for finite sample sizes. version:2
arxiv-1301-3527 | Block Coordinate Descent for Sparse NMF | http://arxiv.org/abs/1301.3527 | id:1301.3527 author:Vamsi K. Potluru, Sergey M. Plis, Jonathan Le Roux, Barak A. Pearlmutter, Vince D. Calhoun, Thomas P. Hayes category:cs.LG cs.NA  published:2013-01-15 summary:Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets. version:2
arxiv-1303-4434 | A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems | http://arxiv.org/abs/1303.4434 | id:1303.4434 author:Pinghua Gong, Changshui Zhang, Zhaosong Lu, Jianhua Huang, Jieping Ye category:cs.LG cs.NA stat.CO stat.ML  published:2013-03-18 summary:Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the Barzilai-Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets. version:1
arxiv-1303-4431 | Generalized Thompson Sampling for Sequential Decision-Making and Causal Inference | http://arxiv.org/abs/1303.4431 | id:1303.4431 author:Pedro A. Ortega, Daniel A. Braun category:cs.AI stat.ML  published:2013-03-18 summary:Recently, it has been shown how sampling actions from the predictive distribution over the optimal action-sometimes called Thompson sampling-can be applied to solve sequential adaptive control problems, when the optimal policy is known for each possible environment. The predictive distribution can then be constructed by a Bayesian superposition of the optimal policies weighted by their posterior probability that is updated by Bayesian inference and causal calculus. Here we discuss three important features of this approach. First, we discuss in how far such Thompson sampling can be regarded as a natural consequence of the Bayesian modeling of policy uncertainty. Second, we show how Thompson sampling can be used to study interactions between multiple adaptive agents, thus, opening up an avenue of game-theoretic analysis. Third, we show how Thompson sampling can be applied to infer causal relationships when interacting with an environment in a sequential fashion. In summary, our results suggest that Thompson sampling might not merely be a useful heuristic, but a principled method to address problems of adaptive sequential decision-making and causal inference. version:1
arxiv-1301-6316 | Hierarchical Data Representation Model - Multi-layer NMF | http://arxiv.org/abs/1301.6316 | id:1301.6316 author:Hyun Ah Song, Soo-Young Lee category:cs.LG  published:2013-01-27 summary:In this paper, we propose a data representation model that demonstrates hierarchical feature learning using nsNMF. We extend unit algorithm into several layers. Experiments with document and image data successfully discovered feature hierarchies. We also prove that proposed method results in much better classification and reconstruction performance, especially for small number of features. feature hierarchies. version:3
arxiv-1212-0504 | Machine learning prediction of cancer cell sensitivity to drugs based on genomic and chemical properties | http://arxiv.org/abs/1212.0504 | id:1212.0504 author:Michael P. Menden, Francesco Iorio, Mathew Garnett, Ultan McDermott, Cyril Benes, Pedro J. Ballester, Julio Saez-Rodriguez category:q-bio.GN cs.CE cs.LG q-bio.CB  published:2012-12-03 summary:Predicting the response of a specific cancer to a therapy is a major goal in modern oncology that should ultimately lead to a personalised treatment. High-throughput screenings of potentially active compounds against a panel of genomically heterogeneous cancer cell lines have unveiled multiple relationships between genomic alterations and drug responses. Various computational approaches have been proposed to predict sensitivity based on genomic features, while others have used the chemical properties of the drugs to ascertain their effect. In an effort to integrate these complementary approaches, we developed machine learning models to predict the response of cancer cell lines to drug treatment, quantified through IC50 values, based on both the genomic features of the cell lines and the chemical properties of the considered drugs. Models predicted IC50 values in a 8-fold cross-validation and an independent blind test with coefficient of determination R2 of 0.72 and 0.64 respectively. Furthermore, models were able to predict with comparable accuracy (R2 of 0.61) IC50s of cell lines from a tissue not used in the training stage. Our in silico models can be used to optimise the experimental design of drug-cell screenings by estimating a large proportion of missing IC50 values rather than experimentally measure them. The implications of our results go beyond virtual drug screening design: potentially thousands of drugs could be probed in silico to systematically test their potential efficacy as anti-tumour agents based on their structure, thus providing a computational framework to identify new drug repositioning opportunities as well as ultimately be useful for personalized medicine by linking the genomic traits of patients to drug sensitivity. version:3
arxiv-1303-3664 | Topic Discovery through Data Dependent and Random Projections | http://arxiv.org/abs/1303.3664 | id:1303.3664 author:Weicong Ding, Mohammad H. Rohban, Prakash Ishwar, Venkatesh Saligrama category:stat.ML cs.LG  published:2013-03-15 summary:We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms based on data-dependent and random projections of word-frequency patterns to identify novel words and associated topics. We will also discuss the statistical guarantees of the data-dependent projections method based on two mild assumptions on the prior density of topic document matrix. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and real-world datasets to demonstrate qualitative and quantitative merits of our scheme. version:2
arxiv-1303-4227 | Genetic algorithms for finding the weight enumerator of binary linear block codes | http://arxiv.org/abs/1303.4227 | id:1303.4227 author:Said Nouh, Mostafa Belkasmi category:cs.IT cs.NE math.IT  published:2013-03-18 summary:In this paper we present a new method for finding the weight enumerator of binary linear block codes by using genetic algorithms. This method consists in finding the binary weight enumerator of the code and its dual and to create from the famous MacWilliams identity a linear system (S) of integer variables for which we add all known information obtained from the structure of the code. The knowledge of some subgroups of the automorphism group, under which the code remains invariant, permits to give powerful restrictions on the solutions of (S) and to approximate the weight enumerator. By applying this method and by using the stability of the Extended Quadratic Residue codes (ERQ) by the Projective Special Linear group PSL2, we find a list of all possible values of the weight enumerators for the two ERQ codes of lengths 192 and 200. We also made a good approximation of the true value for these two enumerators. version:1
arxiv-1210-6539 | Towards Swarm Calculus: Urn Models of Collective Decisions and Universal Properties of Swarm Performance | http://arxiv.org/abs/1210.6539 | id:1210.6539 author:Heiko Hamann category:cs.NE cs.AI  published:2012-10-24 summary:Methods of general applicability are searched for in swarm intelligence with the aim of gaining new insights about natural swarms and to develop design methodologies for artificial swarms. An ideal solution could be a `swarm calculus' that allows to calculate key features of swarms such as expected swarm performance and robustness based on only a few parameters. To work towards this ideal, one needs to find methods and models with high degrees of generality. In this paper, we report two models that might be examples of exceptional generality. First, an abstract model is presented that describes swarm performance depending on swarm density based on the dichotomy between cooperation and interference. Typical swarm experiments are given as examples to show how the model fits to several different results. Second, we give an abstract model of collective decision making that is inspired by urn models. The effects of positive feedback probability, that is increasing over time in a decision making system, are understood by the help of a parameter that controls the feedback based on the swarm's current consensus. Several applicable methods, such as the description as Markov process, calculation of splitting probabilities, mean first passage times, and measurements of positive feedback, are discussed and applications to artificial and natural swarms are reported. version:3
arxiv-1301-3389 | The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization | http://arxiv.org/abs/1301.3389 | id:1301.3389 author:Hugo Van hamme category:cs.NA cs.LG  published:2013-01-15 summary:Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems in text mining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware. version:2
arxiv-1302-2553 | Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning | http://arxiv.org/abs/1302.2553 | id:1302.2553 author:Odalric-Ambrym Maillard, Phuong Nguyen, Ronald Ortner, Daniil Ryabko category:cs.LG  published:2013-02-11 summary:We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order $O(T^{2/3})$ with an additive term constant yet exponential in some characteristics of the optimal MDP. We propose an algorithm whose regret after $T$ time steps is $O(\sqrt{T})$, with all constants reasonably small. This is optimal in $T$ since $O(\sqrt{T})$ is the optimal regret in the setting of learning in a (single discrete) MDP. version:2
arxiv-1303-4172 | Margins, Shrinkage, and Boosting | http://arxiv.org/abs/1303.4172 | id:1303.4172 author:Matus Telgarsky category:cs.LG stat.ML  published:2013-03-18 summary:This manuscript shows that AdaBoost and its immediate variants can produce approximate maximum margin classifiers simply by scaling step size choices with a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedman's empirically successful "shrinkage" procedure for gradient boosting (Friedman, 2000). Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss. version:1
arxiv-1301-3323 | Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences | http://arxiv.org/abs/1301.3323 | id:1301.3323 author:Sainbayar Sukhbaatar, Takaki Makino, Kazuyuki Aihara category:cs.CV cs.LG  published:2013-01-15 summary:Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features. version:4
arxiv-1303-4169 | Markov Chain Monte Carlo for Arrangement of Hyperplanes in Locality-Sensitive Hashing | http://arxiv.org/abs/1303.4169 | id:1303.4169 author:Yui Noma, Makiko Konoshima category:cs.LG  published:2013-03-18 summary:Since Hamming distances can be calculated by bitwise computations, they can be calculated with less computational load than L2 distances. Similarity searches can therefore be performed faster in Hamming distance space. The elements of Hamming distance space are bit strings. On the other hand, the arrangement of hyperplanes induce the transformation from the feature vectors into feature bit strings. This transformation method is a type of locality-sensitive hashing that has been attracting attention as a way of performing approximate similarity searches at high speed. Supervised learning of hyperplane arrangements allows us to obtain a method that transforms them into feature bit strings reflecting the information of labels applied to higher-dimensional feature vectors. In this p aper, we propose a supervised learning method for hyperplane arrangements in feature space that uses a Markov chain Monte Carlo (MCMC) method. We consider the probability density functions used during learning, and evaluate their performance. We also consider the sampling method for learning data pairs needed in learning, and we evaluate its performance. We confirm that the accuracy of this learning method when using a suitable probability density function and sampling method is greater than the accuracy of existing learning methods. version:1
arxiv-1303-4164 | Neurally Implementable Semantic Networks | http://arxiv.org/abs/1303.4164 | id:1303.4164 author:Garrett N. Evans, John C. Collins category:q-bio.NC cs.NE I.2.4; I.2.6  published:2013-03-18 summary:We propose general principles for semantic networks allowing them to be implemented as dynamical neural networks. Major features of our scheme include: (a) the interpretation that each node in a network stands for a bound integration of the meanings of all nodes and external events the node links with; (b) the systematic use of nodes that stand for categories or types, with separate nodes for instances of these types; (c) an implementation of relationships that does not use intrinsically typed links between nodes. version:1
arxiv-1303-4160 | Improved Foreground Detection via Block-based Classifier Cascade with Probabilistic Decision Integration | http://arxiv.org/abs/1303.4160 | id:1303.4160 author:Vikas Reddy, Conrad Sanderson, Brian C. Lovell category:cs.CV  published:2013-03-18 summary:Background subtraction is a fundamental low-level processing task in numerous computer vision applications. The vast majority of algorithms process images on a pixel-by-pixel basis, where an independent decision is made for each pixel. A general limitation of such processing is that rich contextual information is not taken into account. We propose a block-based method capable of dealing with noise, illumination variations and dynamic backgrounds, while still obtaining smooth contours of foreground objects. Specifically, image sequences are analysed on an overlapping block-by-block basis. A low-dimensional texture descriptor obtained from each block is passed through an adaptive classifier cascade, where each stage handles a distinct problem. A probabilistic foreground mask generation approach then exploits block overlaps to integrate interim block-level decisions into final pixel-level foreground segmentation. Unlike many pixel-based methods, ad-hoc post-processing of foreground masks is not required. Experiments on the difficult Wallflower and I2R datasets show that the proposed approach obtains on average better results (both qualitatively and quantitatively) than several prominent methods. We furthermore propose the use of tracking performance as an unbiased approach for assessing the practical usefulness of foreground segmentation methods, and show that the proposed approach leads to considerable improvements in tracking accuracy on the CAVIAR dataset. version:1
arxiv-1303-4385 | Modeling a Sensor to Improve its Efficacy | http://arxiv.org/abs/1303.4385 | id:1303.4385 author:N. K. Malakar, D. Gladkov, K. H. Knuth category:physics.ins-det astro-ph.IM stat.ML  published:2013-03-18 summary:Robots rely on sensors to provide them with information about their surroundings. However, high-quality sensors can be extremely expensive and cost-prohibitive. Thus many robotic systems must make due with lower-quality sensors. Here we demonstrate via a case study how modeling a sensor can improve its efficacy when employed within a Bayesian inferential framework. As a test bed we employ a robotic arm that is designed to autonomously take its own measurements using an inexpensive LEGO light sensor to estimate the position and radius of a white circle on a black field. The light sensor integrates the light arriving from a spatially distributed region within its field of view weighted by its Spatial Sensitivity Function (SSF). We demonstrate that by incorporating an accurate model of the light sensor SSF into the likelihood function of a Bayesian inference engine, an autonomous system can make improved inferences about its surroundings. The method presented here is data-based, fairly general, and made with plug-and play in mind so that it could be implemented in similar problems. version:1
arxiv-1207-5437 | Generalization Bounds for Metric and Similarity Learning | http://arxiv.org/abs/1207.5437 | id:1207.5437 author:Qiong Cao, Zheng-Chu Guo, Yiming Ying category:cs.LG stat.ML  published:2012-07-23 summary:Recently, metric learning and similarity learning have attracted a large amount of interest. Many models and optimisation algorithms have been proposed. However, there is relatively little work on the generalization analysis of such methods. In this paper, we derive novel generalization bounds of metric and similarity learning. In particular, we first show that the generalization analysis reduces to the estimation of the Rademacher average over "sums-of-i.i.d." sample-blocks related to the specific matrix norm. Then, we derive generalization bounds for metric/similarity learning with different matrix-norm regularisers by estimating their specific Rademacher complexities. Our analysis indicates that sparse metric/similarity learning with $L^1$-norm regularisation could lead to significantly better bounds than those with Frobenius-norm regularisation. Our novel generalization analysis develops and refines the techniques of U-statistics and Rademacher complexity analysis. version:2
arxiv-1301-3545 | Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines | http://arxiv.org/abs/1301.3545 | id:1301.3545 author:Guillaume Desjardins, Razvan Pascanu, Aaron Courville, Yoshua Bengio category:cs.LG cs.NE stat.ML  published:2013-01-16 summary:This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitely storing the natural gradient metric $L$. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the variance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive. version:2
arxiv-1303-3987 | $l_{2,p}$ Matrix Norm and Its Application in Feature Selection | http://arxiv.org/abs/1303.3987 | id:1303.3987 author:Liping Wang, Songcan Chen category:cs.LG cs.CV stat.ML  published:2013-03-16 summary:Recently, $l_{2,1}$ matrix norm has been widely applied to many areas such as computer vision, pattern recognition, biological study and etc. As an extension of $l_1$ vector norm, the mixed $l_{2,1}$ matrix norm is often used to find jointly sparse solutions. Moreover, an efficient iterative algorithm has been designed to solve $l_{2,1}$-norm involved minimizations. Actually, computational studies have showed that $l_p$-regularization ($0<p<1$) is sparser than $l_1$-regularization, but the extension to matrix norm has been seldom considered. This paper presents a definition of mixed $l_{2,p}$ $(p\in (0, 1])$ matrix pseudo norm which is thought as both generalizations of $l_p$ vector norm to matrix and $l_{2,1}$-norm to nonconvex cases $(0<p<1)$. Fortunately, an efficient unified algorithm is proposed to solve the induced $l_{2,p}$-norm $(p\in (0, 1])$ optimization problems. The convergence can also be uniformly demonstrated for all $p\in (0, 1]$. Typical $p\in (0,1]$ are applied to select features in computational biology and the experimental results show that some choices of $0<p<1$ do improve the sparse pattern of using $p=1$. version:1
arxiv-1303-3948 | An Adaptive Methodology for Ubiquitous ASR System | http://arxiv.org/abs/1303.3948 | id:1303.3948 author:Urmila Shrawankar, Vilas Thakare category:cs.CL cs.HC cs.SD  published:2013-03-16 summary:Achieving and maintaining the performance of ubiquitous (Automatic Speech Recognition) ASR system is a real challenge. The main objective of this work is to develop a method that will improve and show the consistency in performance of ubiquitous ASR system for real world noisy environment. An adaptive methodology has been developed to achieve an objective with the help of implementing followings, -Cleaning speech signal as much as possible while preserving originality / intangibility using various modified filters and enhancement techniques. -Extracting features from speech signals using various sizes of parameter. -Train the system for ubiquitous environment using multi-environmental adaptation training methods. -Optimize the word recognition rate with appropriate variable size of parameters using fuzzy technique. The consistency in performance is tested using standard noise databases as well as in real world environment. A good improvement is noticed. This work will be helpful to give discriminative training of ubiquitous ASR system for better Human Computer Interaction (HCI) using Speech User Interface (SUI). version:1
arxiv-1301-3618 | Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors | http://arxiv.org/abs/1301.3618 | id:1301.3618 author:Danqi Chen, Richard Socher, Christopher D. Manning, Andrew Y. Ng category:cs.CL cs.LG  published:2013-01-16 summary:Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 75.8%. version:2
arxiv-1301-4168 | Herded Gibbs Sampling | http://arxiv.org/abs/1301.4168 | id:1301.4168 author:Luke Bornn, Yutian Chen, Nando de Freitas, Mareija Eskelin, Jing Fang, Max Welling category:cs.LG stat.CO stat.ML  published:2013-01-17 summary:The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem. version:2
arxiv-1301-3541 | Deep Predictive Coding Networks | http://arxiv.org/abs/1301.3541 | id:1301.3541 author:Rakesh Chalasani, Jose C. Principe category:cs.LG cs.CV stat.ML  published:2013-01-16 summary:The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise. version:3
arxiv-1303-3866 | Variational Semi-blind Sparse Deconvolution with Orthogonal Kernel Bases and its Application to MRFM | http://arxiv.org/abs/1303.3866 | id:1303.3866 author:Se Un Park, Nicolas Dobigeon, Alfred O. Hero category:physics.data-an stat.ML  published:2013-03-15 summary:We present a variational Bayesian method of joint image reconstruction and point spread function (PSF) estimation when the PSF of the imaging device is only partially known. To solve this semi-blind deconvolution problem, prior distributions are specified for the PSF and the 3D image. Joint image reconstruction and PSF estimation is then performed within a Bayesian framework, using a variational algorithm to estimate the posterior distribution. The image prior distribution imposes an explicit atomic measure that corresponds to image sparsity. Importantly, the proposed Bayesian deconvolution algorithm does not require hand tuning. Simulation results clearly demonstrate that the semi-blind deconvolution algorithm compares favorably with previous Markov chain Monte Carlo (MCMC) version of myopic sparse reconstruction. It significantly outperforms mismatched non-blind algorithms that rely on the assumption of the perfect knowledge of the PSF. The algorithm is illustrated on real data from magnetic resonance force microscopy (MRFM). version:1
arxiv-1302-7263 | Online Similarity Prediction of Networked Data from Known and Unknown Graphs | http://arxiv.org/abs/1302.7263 | id:1302.7263 author:Claudio Gentile, Mark Herbster, Stephen Pasteris category:cs.LG  published:2013-02-28 summary:We consider online similarity prediction problems over networked data. We begin by relating this task to the more standard class prediction problem, showing that, given an arbitrary algorithm for class prediction, we can construct an algorithm for similarity prediction with "nearly" the same mistake bound, and vice versa. After noticing that this general construction is computationally infeasible, we target our study to {\em feasible} similarity prediction algorithms on networked data. We initially assume that the network structure is {\em known} to the learner. Here we observe that Matrix Winnow \cite{w07} has a near-optimal mistake guarantee, at the price of cubic prediction time per round. This motivates our effort for an efficient implementation of a Perceptron algorithm with a weaker mistake guarantee but with only poly-logarithmic prediction time. Our focus then turns to the challenging case of networks whose structure is initially {\em unknown} to the learner. In this novel setting, where the network structure is only incrementally revealed, we obtain a mistake-bounded algorithm with a quadratic prediction time per round. version:3
arxiv-1303-3754 | A Last-Step Regression Algorithm for Non-Stationary Online Learning | http://arxiv.org/abs/1303.3754 | id:1303.3754 author:Edward Moroshko, Koby Crammer category:cs.LG  published:2013-03-15 summary:The goal of a learner in standard online learning is to maintain an average loss close to the loss of the best-performing single function in some class. In many real-world problems, such as rating or ranking items, there is no single best target function during the runtime of the algorithm, instead the best (local) target function is drifting over time. We develop a novel last-step minmax optimal algorithm in context of a drift. We analyze the algorithm in the worst-case regret framework and show that it maintains an average loss close to that of the best slowly changing sequence of linear functions, as long as the total of drift is sublinear. In some situations, our bound improves over existing bounds, and additionally the algorithm suffers logarithmic regret when there is no drift. We also build on the H_infinity filter and its bound, and develop and analyze a second algorithm for drifting setting. Synthetic simulations demonstrate the advantages of our algorithms in a worst-case constant drift setting. version:1
arxiv-1205-5868 | Sparse estimation via nonconcave penalized likelihood in a factor analysis model | http://arxiv.org/abs/1205.5868 | id:1205.5868 author:Kei Hirose, Michio Yamamoto category:stat.ME stat.CO stat.ML  published:2012-05-26 summary:We consider the problem of sparse estimation in a factor analysis model. A traditional estimation procedure in use is the following two-step approach: the model is estimated by maximum likelihood method and then a rotation technique is utilized to find sparse factor loadings. However, the maximum likelihood estimates cannot be obtained when the number of variables is much larger than the number of observations. Furthermore, even if the maximum likelihood estimates are available, the rotation technique does not often produce a sufficiently sparse solution. In order to handle these problems, this paper introduces a penalized likelihood procedure that imposes a nonconvex penalty on the factor loadings. We show that the penalized likelihood procedure can be viewed as a generalization of the traditional two-step approach, and the proposed methodology can produce sparser solutions than the rotation technique. A new algorithm via the EM algorithm along with coordinate descent is introduced to compute the entire solution path, which permits the application to a wide variety of convex and nonconvex penalties. Monte Carlo simulations are conducted to investigate the performance of our modeling strategy. A real data example is also given to illustrate our procedure. version:3
arxiv-1303-3716 | Subspace Clustering via Thresholding and Spectral Clustering | http://arxiv.org/abs/1303.3716 | id:1303.3716 author:Reinhard Heckel, Helmut Bölcskei category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH  published:2013-03-15 summary:We consider the problem of clustering a set of high-dimensional data points into sets of low-dimensional linear subspaces. The number of subspaces, their dimensions, and their orientations are unknown. We propose a simple and low-complexity clustering algorithm based on thresholding the correlations between the data points followed by spectral clustering. A probabilistic performance analysis shows that this algorithm succeeds even when the subspaces intersect, and when the dimensions of the subspaces scale (up to a log-factor) linearly in the ambient dimension. Moreover, we prove that the algorithm also succeeds for data points that are subject to erasures with the number of erasures scaling (up to a log-factor) linearly in the ambient dimension. Finally, we propose a simple scheme that provably detects outliers. version:1
arxiv-1303-3145 | Convex Hull-Based Multi-objective Genetic Programming for Maximizing ROC Performance | http://arxiv.org/abs/1303.3145 | id:1303.3145 author:Pu Wang, Michael Emmerich, Rui Li, Ke Tang, Thomas Baeck, Xin Yao category:cs.NE  published:2013-03-13 summary:ROC is usually used to analyze the performance of classifiers in data mining. ROC convex hull (ROCCH) is the least convex major-ant (LCM) of the empirical ROC curve, and covers potential optima for the given set of classifiers. Generally, ROC performance maximization could be considered to maximize the ROCCH, which also means to maximize the true positive rate (tpr) and minimize the false positive rate (fpr) for each classifier in the ROC space. However, tpr and fpr are conflicting with each other in the ROCCH optimization process. Though ROCCH maximization problem seems like a multi-objective optimization problem (MOP), the special characters make it different from traditional MOP. In this work, we will discuss the difference between them and propose convex hull-based multi-objective genetic programming (CH-MOGP) to solve ROCCH maximization problems. Convex hull-based sort is an indicator based selection scheme that aims to maximize the area under convex hull, which serves as a unary indicator for the performance of a set of points. A selection procedure is described that can be efficiently implemented and follows similar design principles than classical hyper-volume based optimization algorithms. It is hypothesized that by using a tailored indicator-based selection scheme CH-MOGP gets more efficient for ROC convex hull approximation than algorithms which compute all Pareto optimal points. To test our hypothesis we compare the new CH-MOGP to MOGP with classical selection schemes, including NSGA-II, MOEA/D) and SMS-EMOA. Meanwhile, CH-MOGP is also compared with traditional machine learning algorithms such as C4.5, Naive Bayes and Prie. Experimental results based on 22 well-known UCI data sets show that CH-MOGP outperforms significantly traditional EMOAs. version:2
arxiv-1303-3632 | Statistical Regression to Predict Total Cumulative CPU Usage of MapReduce Jobs | http://arxiv.org/abs/1303.3632 | id:1303.3632 author:Nikzad Babaii Rizvandi, Javid Taheri, Reza Moraveji, Albert Y. Zomaya category:cs.DC cs.LG cs.PF  published:2013-03-14 summary:Recently, businesses have started using MapReduce as a popular computation framework for processing large amount of data, such as spam detection, and different data mining tasks, in both public and private clouds. Two of the challenging questions in such environments are (1) choosing suitable values for MapReduce configuration parameters e.g., number of mappers, number of reducers, and DFS block size, and (2) predicting the amount of resources that a user should lease from the service provider. Currently, the tasks of both choosing configuration parameters and estimating required resources are solely the users responsibilities. In this paper, we present an approach to provision the total CPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce job, a profile of total CPU usage in clock cycles is built from the job past executions with different values of two configuration parameters e.g., number of mappers, and number of reducers. Then, a polynomial regression is used to model the relation between these configuration parameters and total CPU usage in clock cycles of the job. We also briefly study the influence of input data scaling on measured total CPU usage in clock cycles. This derived model along with the scaling result can then be used to provision the total CPU usage in clock cycles of the same jobs with different input data size. We validate the accuracy of our models using three realistic applications (WordCount, Exim MainLog parsing, and TeraSort). Results show that the predicted total CPU usage in clock cycles of generated resource provisioning options are less than 8% of the measured total CPU usage in clock cycles in our 20-node virtual Hadoop cluster. version:1
arxiv-1303-3605 | A survey on sensing methods and feature extraction algorithms for SLAM problem | http://arxiv.org/abs/1303.3605 | id:1303.3605 author:Adheen Ajay, D. Venkataraman category:cs.RO cs.CV cs.LG  published:2013-03-14 summary:This paper is a survey work for a bigger project for designing a Visual SLAM robot to generate 3D dense map of an unknown unstructured environment. A lot of factors have to be considered while designing a SLAM robot. Sensing method of the SLAM robot should be determined by considering the kind of environment to be modeled. Similarly the type of environment determines the suitable feature extraction method. This paper goes through the sensing methods used in some recently published papers. The main objective of this survey is to conduct a comparative study among the current sensing methods and feature extraction algorithms and to extract out the best for our work. version:1
arxiv-1301-3583 | Big Neural Networks Waste Capacity | http://arxiv.org/abs/1301.3583 | id:1301.3583 author:Yann N. Dauphin, Yoshua Bengio category:cs.LG cs.CV  published:2013-01-16 summary:This article exposes the failure of some big neural networks to leverage added capacity to reduce underfitting. Past research suggest diminishing returns when increasing the size of neural networks. Our experiments on ImageNet LSVRC-2010 show that this may be due to the fact there are highly diminishing returns for capacity in terms of training error, leading to underfitting. This suggests that the optimization method - first order gradient descent - fails at this regime. Directly attacking this problem, either through the optimization method or the choices of parametrization, may allow to improve the generalization error on large datasets, for which a large capacity is required. version:4
arxiv-1303-3592 | Expressing Ethnicity through Behaviors of a Robot Character | http://arxiv.org/abs/1303.3592 | id:1303.3592 author:Maxim Makatchev, Reid Simmons, Majd Sakr, Micheline Ziadee category:cs.CL cs.CY cs.RO  published:2013-03-14 summary:Achieving homophily, or association based on similarity, between a human user and a robot holds a promise of improved perception and task performance. However, no previous studies that address homophily via ethnic similarity with robots exist. In this paper, we discuss the difficulties of evoking ethnic cues in a robot, as opposed to a virtual agent, and an approach to overcome those difficulties based on using ethnically salient behaviors. We outline our methodology for selecting and evaluating such behaviors, and culminate with a study that evaluates our hypotheses of the possibility of ethnic attribution of a robot character through verbal and nonverbal behaviors and of achieving the homophily effect. version:1
arxiv-1301-3572 | Indoor Semantic Segmentation using depth information | http://arxiv.org/abs/1301.3572 | id:1301.3572 author:Camille Couprie, Clément Farabet, Laurent Najman, Yann LeCun category:cs.CV  published:2013-01-16 summary:This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA. version:2
arxiv-0908-3148 | Another Look at Quantum Neural Computing | http://arxiv.org/abs/0908.3148 | id:0908.3148 author:Subhash Kak category:cs.NE cs.AI  published:2009-08-21 summary:The term quantum neural computing indicates a unity in the functioning of the brain. It assumes that the neural structures perform classical processing and that the virtual particles associated with the dynamical states of the structures define the underlying quantum state. We revisit the concept and also summarize new arguments related to the learning modes of the brain in response to sensory input that may be aggregated in three types: associative, reorganizational, and quantum. The associative and reorganizational types are quite apparent based on experimental findings; it is much harder to establish that the brain as an entity exhibits quantum properties. We argue that the reorganizational behavior of the brain may be viewed as inner adjustment corresponding to its quantum behavior at the system level. Not only neural structures but their higher abstractions also may be seen as whole entities. We consider the dualities associated with the behavior of the brain and how these dualities are bridged. version:2
arxiv-1303-3469 | Hybrid Evolutionary Computation for Continuous Optimization | http://arxiv.org/abs/1303.3469 | id:1303.3469 author:Hassan A. Bashir, Richard S. Neville category:cs.NE  published:2013-03-14 summary:Hybrid optimization algorithms have gained popularity as it has become apparent there cannot be a universal optimization strategy which is globally more beneficial than any other. Despite their popularity, hybridization frameworks require more detailed categorization regarding: the nature of the problem domain, the constituent algorithms, the coupling schema and the intended area of application. This report proposes a hybrid algorithm for solving small to large-scale continuous global optimization problems. It comprises evolutionary computation (EC) algorithms and a sequential quadratic programming (SQP) algorithm; combined in a collaborative portfolio. The SQP is a gradient based local search method. To optimize the individual contributions of the EC and SQP algorithms for the overall success of the proposed hybrid system, improvements were made in key features of these algorithms. The report proposes enhancements in: i) the evolutionary algorithm, ii) a new convergence detection mechanism was proposed; and iii) in the methods for evaluating the search directions and step sizes for the SQP local search algorithm. The proposed hybrid design aim was to ensure that the two algorithms complement each other by exploring and exploiting the problem search space. Preliminary results justify that an adept hybridization of evolutionary algorithms with a suitable local search method, could yield a robust and efficient means of solving wide range of global optimization problems. Finally, a discussion of the outcomes of the initial investigation and a review of the associated challenges and inherent limitations of the proposed method is presented to complete the investigation. The report highlights extensive research, particularly, some potential case studies and application areas. version:1
arxiv-1301-2820 | Clustering Learning for Robotic Vision | http://arxiv.org/abs/1301.2820 | id:1301.2820 author:Eugenio Culurciello, Jordan Bates, Aysegul Dundar, Jose Carrasco, Clement Farabet category:cs.CV  published:2013-01-13 summary:We present the clustering learning technique applied to multi-layer feedforward deep neural networks. We show that this unsupervised learning technique can compute network filters with only a few minutes and a much reduced set of parameters. The goal of this paper is to promote the technique for general-purpose robotic vision systems. We report its use in static image datasets and object tracking datasets. We show that networks trained with clustering learning can outperform large networks trained for many hours on complex datasets. version:3
arxiv-1204-3972 | EigenGP: Sparse Gaussian process models with data-dependent eigenfunctions | http://arxiv.org/abs/1204.3972 | id:1204.3972 author:Yuan Qi, Bo Dai, Yao Zhu category:cs.LG stat.CO stat.ML  published:2012-04-18 summary:Gaussian processes (GPs) provide a nonparametric representation of functions. However, classical GP inference suffers from high computational cost and it is difficult to design nonstationary GP priors in practice. In this paper, we propose a sparse Gaussian process model, EigenGP, based on the Karhunen-Loeve (KL) expansion of a GP prior. We use the Nystrom approximation to obtain data dependent eigenfunctions and select these eigenfunctions by evidence maximization. This selection reduces the number of eigenfunctions in our model and provides a nonstationary covariance function. To handle nonlinear likelihoods, we develop an efficient expectation propagation (EP) inference algorithm, and couple it with expectation maximization for eigenfunction selection. Because the eigenfunctions of a Gaussian kernel are associated with clusters of samples - including both the labeled and unlabeled - selecting relevant eigenfunctions enables EigenGP to conduct semi-supervised learning. Our experimental results demonstrate improved predictive performance of EigenGP over alternative state-of-the-art sparse GP and semisupervised learning methods for regression, classification, and semisupervised classification. version:3
arxiv-1303-3170 | Types and forgetfulness in categorical linguistics and quantum mechanics | http://arxiv.org/abs/1303.3170 | id:1303.3170 author:Peter Hines category:cs.CL math.CT quant-ph  published:2013-03-13 summary:The role of types in categorical models of meaning is investigated. A general scheme for how typed models of meaning may be used to compare sentences, regardless of their grammatical structure is described, and a toy example is used as an illustration. Taking as a starting point the question of whether the evaluation of such a type system 'loses information', we consider the parametrized typing associated with connectives from this viewpoint. The answer to this question implies that, within full categorical models of meaning, the objects associated with types must exhibit a simple but subtle categorical property known as self-similarity. We investigate the category theory behind this, with explicit reference to typed systems, and their monoidal closed structure. We then demonstrate close connections between such self-similar structures and dagger Frobenius algebras. In particular, we demonstrate that the categorical structures implied by the polymorphically typed connectives give rise to a (lax unitless) form of the special forms of Frobenius algebras known as classical structures, used heavily in abstract categorical approaches to quantum mechanics. version:1
arxiv-1303-3152 | Material quality assessment of silk nanofibers based on swarm intelligence | http://arxiv.org/abs/1303.3152 | id:1303.3152 author:Bruno Brandoli Machado, Wesley Nunes Gonçalves, Odemir Martinez Bruno category:cs.CV  published:2013-03-13 summary:In this paper, we propose a novel approach for texture analysis based on artificial crawler model. Our method assumes that each agent can interact with the environment and each other. The evolution process converges to an equilibrium state according to the set of rules. For each textured image, the feature vector is composed by signatures of the live agents curve at each time. Experimental results revealed that combining the minimum and maximum signatures into one increase the classification rate. In addition, we pioneer the use of autonomous agents for characterizing silk fibroin scaffolds. The results strongly suggest that our approach can be successfully employed for texture analysis. version:1
arxiv-1303-5403 | An Entropy-based Learning Algorithm of Bayesian Conditional Trees | http://arxiv.org/abs/1303.5403 | id:1303.5403 author:Dan Geiger category:cs.LG cs.AI cs.CV  published:2013-03-13 summary:This article offers a modification of Chow and Liu's learning algorithm in the context of handwritten digit recognition. The modified algorithm directs the user to group digits into several classes consisting of digits that are hard to distinguish and then constructing an optimal conditional tree representation for each class of digits instead of for each single digit as done by Chow and Liu (1968). Advantages and extensions of the new method are discussed. Related works of Wong and Wang (1977) and Wong and Poon (1989) which offer a different entropy-based learning algorithm are shown to rest on inappropriate assumptions. version:1
arxiv-1303-3134 | Egocentric vision IT technologies for Alzheimer disease assessment and studies | http://arxiv.org/abs/1303.3134 | id:1303.3134 author:Hugo Boujut, Vincent Buso, Guillaume Bourmaud, Jenny Benois-Pineau, Rémi Mégret, Jean-Philippe Domenger, Yann Gaëstel, Jean-François Dartigues category:cs.HC cs.CV  published:2013-03-13 summary:Egocentric vision technology consists in capturing the actions of persons from their own visual point of view using wearable camera sensors. We apply this new paradigm to instrumental activities monitoring with the objective of providing new tools for the clinical evaluation of the impact of the disease on persons with dementia. In this paper, we introduce the current state of the development of this technology and focus on two technology modules: automatic location estimation and visual saliency estimation for content interpretation. version:1
arxiv-1303-4638 | On Improving Energy Efficiency within Green Femtocell Networks: A Hierarchical Reinforcement Learning Approach | http://arxiv.org/abs/1303.4638 | id:1303.4638 author:Xianfu Chen, Honggang Zhang, Tao Chen, Mika Lasanen, Jacques Palicot category:cs.LG cs.GT  published:2013-03-13 summary:One of the efficient solutions of improving coverage and increasing capacity in cellular networks is the deployment of femtocells. As the cellular networks are becoming more complex, energy consumption of whole network infrastructure is becoming important in terms of both operational costs and environmental impacts. This paper investigates energy efficiency of two-tier femtocell networks through combining game theory and stochastic learning. With the Stackelberg game formulation, a hierarchical reinforcement learning framework is applied for studying the joint expected utility maximization of macrocells and femtocells subject to the minimum signal-to-interference-plus-noise-ratio requirements. In the learning procedure, the macrocells act as leaders and the femtocells are followers. At each time step, the leaders commit to dynamic strategies based on the best responses of the followers, while the followers compete against each other with no further information but the leaders' transmission parameters. In this paper, we propose two reinforcement learning based intelligent algorithms to schedule each cell's stochastic power levels. Numerical experiments are presented to validate the investigations. The results show that the two learning algorithms substantially improve the energy efficiency of the femtocell networks. version:1
arxiv-1303-3087 | Statistical Texture Features based Handwritten and Printed Text Classification in South Indian Documents | http://arxiv.org/abs/1303.3087 | id:1303.3087 author:Mallikarjun Hangarge, K. C. Santosh, Srikanth Doddamani, Rajmohan Pardeshi category:cs.CV  published:2013-03-13 summary:In this paper, we use statistical texture features for handwritten and printed text classification. We primarily aim for word level classification in south Indian scripts. Words are first extracted from the scanned document. For each extracted word, statistical texture features are computed such as mean, standard deviation, smoothness, moment, uniformity, entropy and local range including local entropy. These feature vectors are then used to classify words via k-NN classifier. We have validated the approach over several different datasets. Scripts like Kannada, Telugu, Malayalam and Hindi i.e., Devanagari are primarily employed where an average classification rate of 99.26% is achieved. In addition, to provide an extensibility of the approach, we address Roman script by using publicly available dataset and interesting results are reported. version:1
arxiv-1303-3517 | Iterative MapReduce for Large Scale Machine Learning | http://arxiv.org/abs/1303.3517 | id:1303.3517 author:Joshua Rosen, Neoklis Polyzotis, Vinayak Borkar, Yingyi Bu, Michael J. Carey, Markus Weimer, Tyson Condie, Raghu Ramakrishnan category:cs.DC cs.DB cs.LG  published:2013-03-13 summary:Large datasets ("Big Data") are becoming ubiquitous because the potential value in deriving insights from data, across a wide range of business and scientific applications, is increasingly recognized. In particular, machine learning - one of the foundational disciplines for data analysis, summarization and inference - on Big Data has become routine at most organizations that operate large clouds, usually based on systems such as Hadoop that support the MapReduce programming paradigm. It is now widely recognized that while MapReduce is highly scalable, it suffers from a critical weakness for machine learning: it does not support iteration. Consequently, one has to program around this limitation, leading to fragile, inefficient code. Further, reliance on the programmer is inherently flawed in a multi-tenanted cloud environment, since the programmer does not have visibility into the state of the system when his or her program executes. Prior work has sought to address this problem by either developing specialized systems aimed at stylized applications, or by augmenting MapReduce with ad hoc support for saving state across iterations (driven by an external loop). In this paper, we advocate support for looping as a first-class construct, and propose an extension of the MapReduce programming paradigm called {\em Iterative MapReduce}. We then develop an optimizer for a class of Iterative MapReduce programs that cover most machine learning techniques, provide theoretical justifications for the key optimization steps, and empirically demonstrate that system-optimized programs for significant machine learning tasks are competitive with state-of-the-art specialized solutions. version:1
arxiv-1303-3067 | Computing Motion with 3D Memristive Grid | http://arxiv.org/abs/1303.3067 | id:1303.3067 author:Chuan Kai Kenneth. Lim, T. Prodromakis category:cs.CV q-bio.NC  published:2013-03-13 summary:Computing the relative motion of objects is an important navigation task that we routinely perform by relying on inherently unreliable biological cells in the retina. The non-linear and adaptive response of memristive devices make them excellent building blocks for realizing complex synaptic-like architectures that are common in the human retina. Here, we introduce a novel memristive thresholding scheme that facilitates the detection of moving edges. In addition, a double-layered 3-D memristive network is employed for modeling the motion computations that take place in both the Outer Plexiform Layer (OPL) and Inner Plexiform Layer (IPL) that enables the detection of on-center and off-center transient responses. Applying the transient detection results, it is shown that it is possible to generate an estimation of the speed and direction a moving object. version:1
arxiv-1303-3055 | Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions | http://arxiv.org/abs/1303.3055 | id:1303.3055 author:Yasin Abbasi-Yadkori, Peter L. Bartlett, Csaba Szepesvari category:cs.LG stat.ML  published:2013-03-12 summary:We study the problem of learning Markov decision processes with finite state and action spaces when the transition probability distributions and loss functions are chosen adversarially and are allowed to change with time. We introduce an algorithm whose regret with respect to any policy in a comparison class grows as the square root of the number of rounds of the game, provided the transition probabilities satisfy a uniform mixing condition. Our approach is efficient as long as the comparison class is polynomial and we can compute expectations over sample paths for each policy. Designing an efficient algorithm with small regret for the general case remains an open problem. version:1
arxiv-1209-4360 | Variational Inference in Nonconjugate Models | http://arxiv.org/abs/1209.4360 | id:1209.4360 author:Chong Wang, David M. Blei category:stat.ML  published:2012-09-19 summary:Mean-field variational methods are widely used for approximate posterior inference in many probabilistic models. In a typical application, mean-field methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest---like the correlated topic model and Bayesian logistic regression---are nonconjuate. In these models, mean-field methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconjugate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for specific models; and they work well on real-world datasets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression. version:4
arxiv-1303-3036 | Type-theoretical natural language semantics: on the system F for meaning assembly | http://arxiv.org/abs/1303.3036 | id:1303.3036 author:Christian Retoré category:cs.LO cs.CL math.LO  published:2013-03-12 summary:This paper presents and extends our type theoretical framework for a compositional treatment of natural language semantics with some lexical features like coercions (e.g. of a town into a football club) and copredication (e.g. on a town as a set of people and as a location). The second order typed lambda calculus was shown to be a good framework, and here we discuss how to introduced predefined types and coercive subtyping which are much more natural than internally coded similar constructs. Linguistic applications of these new features are also exemplified. version:1
arxiv-1202-6429 | Stable image reconstruction using total variation minimization | http://arxiv.org/abs/1202.6429 | id:1202.6429 author:Deanna Needell, Rachel Ward category:cs.CV cs.IT math.IT math.NA  published:2012-02-29 summary:This article presents near-optimal guarantees for accurate and robust image recovery from under-sampled noisy measurements using total variation minimization. In particular, we show that from O(slog(N)) nonadaptive linear measurements, an image can be reconstructed to within the best s-term approximation of its gradient up to a logarithmic factor, and this factor can be removed by taking slightly more measurements. Along the way, we prove a strengthened Sobolev inequality for functions lying in the null space of suitably incoherent matrices. version:9
arxiv-1303-2892 | Toward Optimal Stratification for Stratified Monte-Carlo Integration | http://arxiv.org/abs/1303.2892 | id:1303.2892 author:Alexandra Carpentier, Remi Munos category:stat.ML  published:2013-03-12 summary:We consider the problem of adaptive stratified sampling for Monte Carlo integration of a noisy function, given a finite budget n of noisy evaluations to the function. We tackle in this paper the problem of adapting to the function at the same time the number of samples into each stratum and the partition itself. More precisely, it is interesting to refine the partition of the domain in area where the noise to the function, or where the variations of the function, are very heterogeneous. On the other hand, having a (too) refined stratification is not optimal. Indeed, the more refined the stratification, the more difficult it is to adjust the allocation of the samples to the stratification, i.e. sample more points where the noise or variations of the function are larger. We provide in this paper an algorithm that selects online, among a large class of partitions, the partition that provides the optimal trade-off, and allocates the samples almost optimally on this partition. version:1
arxiv-1303-2844 | A Stochastic Grammar for Natural Shapes | http://arxiv.org/abs/1303.2844 | id:1303.2844 author:Pedro F. Felzenszwalb category:cs.CV  published:2013-03-12 summary:We consider object detection using a generic model for natural shapes. A common approach for object recognition involves matching object models directly to images. Another approach involves building intermediate representations via a generic grouping processes. We argue that these two processes (model-based recognition and grouping) may use similar computational mechanisms. By defining a generic model for shapes we can use model-based techniques to implement a mid-level vision grouping process. version:1
arxiv-1303-2827 | Linear system identification using stable spline kernels and PLQ penalties | http://arxiv.org/abs/1303.2827 | id:1303.2827 author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:stat.ML math.OC stat.CO 47N30  65K10  published:2013-03-12 summary:The classical approach to linear system identification is given by parametric Prediction Error Methods (PEM). In this context, model complexity is often unknown so that a model order selection step is needed to suitably trade-off bias and variance. Recently, a different approach to linear system identification has been introduced, where model order determination is avoided by using a regularized least squares framework. In particular, the penalty term on the impulse response is defined by so called stable spline kernels. They embed information on regularity and BIBO stability, and depend on a small number of parameters which can be estimated from data. In this paper, we provide new nonsmooth formulations of the stable spline estimator. In particular, we consider linear system identification problems in a very broad context, where regularization functionals and data misfits can come from a rich set of piecewise linear quadratic functions. Moreover, our anal- ysis includes polyhedral inequality constraints on the unknown impulse response. For any formulation in this class, we show that interior point methods can be used to solve the system identification problem, with complexity O(n3)+O(mn2) in each iteration, where n and m are the number of impulse response coefficients and measurements, respectively. The usefulness of the framework is illustrated via a numerical experiment where output measurements are contaminated by outliers. version:1
arxiv-1303-2826 | Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA | http://arxiv.org/abs/1303.2826 | id:1303.2826 author:William M. Darling, Fei Song category:cs.CL  published:2013-03-12 summary:This article presents a probabilistic generative model for text based on semantic topics and syntactic classes called Part-of-Speech LDA (POSLDA). POSLDA simultaneously uncovers short-range syntactic patterns (syntax) and long-range semantic patterns (topics) that exist in document collections. This results in word distributions that are specific to both topics (sports, education, ...) and parts-of-speech (nouns, verbs, ...). For example, multinomial distributions over words are uncovered that can be understood as "nouns about weather" or "verbs about law". We describe the model and an approximate inference algorithm and then demonstrate the quality of the learned topics both qualitatively and quantitatively. Then, we discuss an NLP application where the output of POSLDA can lead to strong improvements in quality: unsupervised part-of-speech tagging. We describe algorithms for this task that make use of POSLDA-learned distributions that result in improved performance beyond the state of the art. version:1
arxiv-1303-2789 | A Cooperative Q-learning Approach for Real-time Power Allocation in Femtocell Networks | http://arxiv.org/abs/1303.2789 | id:1303.2789 author:Hussein Saad, Amr Mohamed, Tamer ElBatt category:cs.MA cs.LG  published:2013-03-12 summary:In this paper, we address the problem of distributed interference management of cognitive femtocells that share the same frequency range with macrocells (primary user) using distributed multi-agent Q-learning. We formulate and solve three problems representing three different Q-learning algorithms: namely, centralized, distributed and partially distributed power control using Q-learning (CPC-Q, DPC-Q and PDPC-Q). CPCQ, although not of practical interest, characterizes the global optimum. Each of DPC-Q and PDPC-Q works in two different learning paradigms: Independent (IL) and Cooperative (CL). The former is considered the simplest form for applying Qlearning in multi-agent scenarios, where all the femtocells learn independently. The latter is the proposed scheme in which femtocells share partial information during the learning process in order to strike a balance between practical relevance and performance. In terms of performance, the simulation results showed that the CL paradigm outperforms the IL paradigm and achieves an aggregate femtocells capacity that is very close to the optimal one. For the practical relevance issue, we evaluate the robustness and scalability of DPC-Q, in real time, by deploying new femtocells in the system during the learning process, where we showed that DPC-Q in the CL paradigm is scalable to large number of femtocells and more robust to the network dynamics compared to the IL paradigm version:1
arxiv-1303-2783 | Combined Learning of Salient Local Descriptors and Distance Metrics for Image Set Face Verification | http://arxiv.org/abs/1303.2783 | id:1303.2783 author:Conrad Sanderson, Mehrtash T. Harandi, Yongkang Wong, Brian C. Lovell category:cs.CV  published:2013-03-12 summary:In contrast to comparing faces via single exemplars, matching sets of face images increases robustness and discrimination performance. Recent image set matching approaches typically measure similarities between subspaces or manifolds, while representing faces in a rigid and holistic manner. Such representations are easily affected by variations in terms of alignment, illumination, pose and expression. While local feature based representations are considerably more robust to such variations, they have received little attention within the image set matching area. We propose a novel image set matching technique, comprised of three aspects: (i) robust descriptors of face regions based on local features, partly inspired by the hierarchy in the human visual system, (ii) use of several subspace and exemplar metrics to compare corresponding face regions, (iii) jointly learning which regions are the most discriminative while finding the optimal mixing weights for combining metrics. Face recognition experiments on LFW, PIE and MOBIO face datasets show that the proposed algorithm obtains considerably better performance than several recent state-of-the-art techniques, such as Local Principal Angle and the Kernel Affine Hull Method. version:1
arxiv-1303-2751 | Gaussian Mixture Model for Handwritten Script Identification | http://arxiv.org/abs/1303.2751 | id:1303.2751 author:Mallikarjun Hangarge category:cs.CV  published:2013-03-12 summary:This paper presents a Gaussian Mixture Model (GMM) to identify the script of handwritten words of Roman, Devanagari, Kannada and Telugu scripts. It emphasizes the significance of directional energies for identification of script of the word. It is robust to varied image sizes and different styles of writing. A GMM is modeled using a set of six novel features derived from directional energy distributions of the underlying image. The standard deviation of directional energy distributions are computed by decomposing an image matrix into right and left diagonals. Furthermore, deviation of horizontal and vertical distributions of energies is also built-in to GMM. A dataset of 400 images out of 800 (200 of each script) are used for training GMM and the remaining is for testing. An exhaustive experimentation is carried out at bi-script, tri-script and multi-script level and achieved script identification accuracies in percentage as 98.7, 98.16 and 96.91 respectively. version:1
arxiv-1303-2745 | Evolutionary Approaches to Expensive Optimisation | http://arxiv.org/abs/1303.2745 | id:1303.2745 author:Maumita Bhattacharya category:cs.NE 97R40  published:2013-03-12 summary:Surrogate assisted evolutionary algorithms (EA) are rapidly gaining popularity where applications of EA in complex real world problem domains are concerned. Although EAs are powerful global optimizers, finding optimal solution to complex high dimensional, multimodal problems often require very expensive fitness function evaluations. Needless to say, this could brand any population-based iterative optimization technique to be the most crippling choice to handle such problems. Use of approximate model or surrogates provides a much cheaper option. However, naturally this cheaper option comes with its own price. This paper discusses some of the key issues involved with use of approximation in evolutionary algorithm, possible best practices and solutions. Answers to the following questions have been sought: what type of fitness approximation to be used; which approximation model to use; how to integrate the approximation model in EA; how much approximation to use; and how to ensure reliable approximation. version:1
arxiv-1303-2739 | Machine Learning for Bioclimatic Modelling | http://arxiv.org/abs/1303.2739 | id:1303.2739 author:Maumita Bhattacharya category:cs.LG stat.AP 97R30  published:2013-03-12 summary:Many machine learning (ML) approaches are widely used to generate bioclimatic models for prediction of geographic range of organism as a function of climate. Applications such as prediction of range shift in organism, range of invasive species influenced by climate change are important parameters in understanding the impact of climate change. However, success of machine learning-based approaches depends on a number of factors. While it can be safely said that no particular ML technique can be effective in all applications and success of a technique is predominantly dependent on the application or the type of the problem, it is useful to understand their behavior to ensure informed choice of techniques. This paper presents a comprehensive review of machine learning-based bioclimatic model generation and analyses the factors influencing success of such models. Considering the wide use of statistical techniques, in our discussion we also include conventional statistical techniques used in bioclimatic modelling. version:1
arxiv-1303-2685 | Bilateral Filter: Graph Spectral Interpretation and Extensions | http://arxiv.org/abs/1303.2685 | id:1303.2685 author:Akshay Gadde, Sunil K Narang, Antonio Ortega category:cs.CV  published:2013-03-11 summary:In this paper we study the bilateral filter proposed by Tomasi and Manduchi, as a spectral domain transform defined on a weighted graph. The nodes of this graph represent the pixels in the image and a graph signal defined on the nodes represents the intensity values. Edge weights in the graph correspond to the bilateral filter coefficients and hence are data adaptive. Spectrum of a graph is defined in terms of the eigenvalues and eigenvectors of the graph Laplacian matrix. We use this spectral interpretation to generalize the bilateral filter and propose more flexible and application specific spectral designs of bilateral-like filters. We show that these spectral filters can be implemented with k-iterative bilateral filtering operations and do not require expensive diagonalization of the Laplacian matrix. version:1
arxiv-1202-6258 | A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets | http://arxiv.org/abs/1202.6258 | id:1202.6258 author:Nicolas Le Roux, Mark Schmidt, Francis Bach category:math.OC cs.LG  published:2012-02-28 summary:We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly. version:4
arxiv-1303-2643 | Revealing Cluster Structure of Graph by Path Following Replicator Dynamic | http://arxiv.org/abs/1303.2643 | id:1303.2643 author:Hairong Liu, Longin Jan Latecki, Shuicheng Yan category:cs.LG cs.GT  published:2013-03-11 summary:In this paper, we propose a path following replicator dynamic, and investigate its potentials in uncovering the underlying cluster structure of a graph. The proposed dynamic is a generalization of the discrete replicator dynamic. The replicator dynamic has been successfully used to extract dense clusters of graphs; however, it is often sensitive to the degree distribution of a graph, and usually biased by vertices with large degrees, thus may fail to detect the densest cluster. To overcome this problem, we introduce a dynamic parameter, called path parameter, into the evolution process. The path parameter can be interpreted as the maximal possible probability of a current cluster containing a vertex, and it monotonically increases as evolution process proceeds. By limiting the maximal probability, the phenomenon of some vertices dominating the early stage of evolution process is suppressed, thus making evolution process more robust. To solve the optimization problem with a fixed path parameter, we propose an efficient fixed point algorithm. The time complexity of the path following replicator dynamic is only linear in the number of edges of a graph, thus it can analyze graphs with millions of vertices and tens of millions of edges on a common PC in a few minutes. Besides, it can be naturally generalized to hypergraph and graph with edges of different orders. We apply it to four important problems: maximum clique problem, densest k-subgraph problem, structure fitting, and discovery of high-density regions. The extensive experimental results clearly demonstrate its advantages, in terms of robustness, scalability and flexility. version:1
arxiv-1303-2108 | Classification of Segments in PolSAR Imagery by Minimum Stochastic Distances Between Wishart Distributions | http://arxiv.org/abs/1303.2108 | id:1303.2108 author:Wagner Barreto da Silva, Corina da Costa Freitas, Sidnei João Siqueira Sant'Anna, Alejandro C. Frery category:cs.CV stat.AP  published:2013-03-11 summary:A new classifier for Polarimetric SAR (PolSAR) images is proposed and assessed in this paper. Its input consists of segments, and each one is assigned the class which minimizes a stochastic distance. Assuming the complex Wishart model, several stochastic distances are obtained from the h-phi family of divergences, and they are employed to derive hypothesis test statistics that are also used in the classification process. This article also presents, as a novelty, analytic expressions for the test statistics based on the following stochastic distances between complex Wishart models: Kullback-Leibler, Bhattacharyya, Hellinger, R\'enyi, and Chi-Square; also, the test statistic based on the Bhattacharyya distance between multivariate Gaussian distributions is presented. The classifier performance is evaluated using simulated and real PolSAR data. The simulated data are based on the complex Wishart model, aiming at the analysis of the proposal well controlled data. The real data refer to the complex L-band image, acquired during the 1994 SIR-C mission. The results of the proposed classifier are compared with those obtained by a Wishart per-pixel/contextual classifier, and we show the better performance of the region-based classification. The influence of the statistical modeling is assessed by comparing the results using the Bhattacharyya distance between multivariate Gaussian distributions for amplitude data. The results with simulated data indicate that the proposed classification method has a very good performance when the data follow the Wishart model. The proposed classifier also performs better than the per-pixel/contextual classifier and the Bhattacharyya Gaussian distance using SIR-C PolSAR data. version:1
arxiv-1303-2610 | Kernel Sparse Models for Automated Tumor Segmentation | http://arxiv.org/abs/1303.2610 | id:1303.2610 author:Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy, Deepta Rajan, Anup Puri, David Frakes, Andreas Spanias category:cs.CV  published:2013-03-11 summary:In this paper, we propose sparse coding-based approaches for segmentation of tumor regions from MR images. Sparse coding with data-adapted dictionaries has been successfully employed in several image recovery and vision problems. The proposed approaches obtain sparse codes for each pixel in brain magnetic resonance images considering their intensity values and location information. Since it is trivial to obtain pixel-wise sparse codes, and combining multiple features in the sparse coding setup is not straightforward, we propose to perform sparse coding in a high-dimensional feature space where non-linear similarities can be effectively modeled. We use the training data from expert-segmented images to obtain kernel dictionaries with the kernel K-lines clustering procedure. For a test image, sparse codes are computed with these kernel dictionaries, and they are used to identify the tumor regions. This approach is completely automated, and does not require user intervention to initialize the tumor regions in a test image. Furthermore, a low complexity segmentation approach based on kernel sparse codes, which allows the user to initialize the tumor region, is also presented. Results obtained with both the proposed approaches are validated against manual segmentation by an expert radiologist, and the proposed methods lead to accurate tumor identification. version:1
arxiv-1301-3476 | Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities | http://arxiv.org/abs/1301.3476 | id:1301.3476 author:Tommi Vatanen, Tapani Raiko, Harri Valpola, Yann LeCun category:cs.LG cs.CV stat.ML  published:2013-01-15 summary:Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero. version:3
arxiv-1301-3391 | Feature grouping from spatially constrained multiplicative interaction | http://arxiv.org/abs/1301.3391 | id:1301.3391 author:Felix Bauer, Roland Memisevic category:cs.LG I.2.6  published:2013-01-15 summary:We present a feature learning model that learns to encode relationships between images. The model is defined as a Gated Boltzmann Machine, which is constrained such that hidden units that are nearby in space can gate each other's connections. We show how frequency/orientation "columns" as well as topographic filter maps follow naturally from training the model on image pairs. The model also helps explain why square-pooling models yield feature groups with similar grouping properties. Experimental results on synthetic image transformations show that spatially constrained gating is an effective way to reduce the number of parameters and thereby to regularize a transformation-learning model. version:3
arxiv-1303-2517 | Refinement revisited with connections to Bayes error, conditional entropy and calibrated classifiers | http://arxiv.org/abs/1303.2517 | id:1303.2517 author:Hamed Masnadi-Shirazi category:stat.ML  published:2013-03-11 summary:The concept of refinement from probability elicitation is considered for proper scoring rules. Taking directions from the axioms of probability, refinement is further clarified using a Hilbert space interpretation and reformulated into the underlying data distribution setting where connections to maximal marginal diversity and conditional entropy are considered and used to derive measures that provide arbitrarily tight bounds on the Bayes error. Refinement is also reformulated into the classifier output setting and its connections to calibrated classifiers and proper margin losses are established. version:1
arxiv-1303-2506 | Monte-Carlo utility estimates for Bayesian reinforcement learning | http://arxiv.org/abs/1303.2506 | id:1303.2506 author:Christos Dimitrakakis category:cs.LG stat.ML  published:2013-03-11 summary:This paper introduces a set of algorithms for Monte-Carlo Bayesian reinforcement learning. Firstly, Monte-Carlo estimation of upper bounds on the Bayes-optimal value function is employed to construct an optimistic policy. Secondly, gradient-based algorithms for approximate upper and lower bounds are introduced. Finally, we introduce a new class of gradient algorithms for Bayesian Bellman error minimisation. We theoretically show that the gradient methods are sound. Experimentally, we demonstrate the superiority of the upper bound method in terms of reward obtained. However, we also show that the Bayesian Bellman error method is a close second, despite its significant computational simplicity. version:1
arxiv-1303-2488 | Visualizing and Interacting with Concept Hierarchies | http://arxiv.org/abs/1303.2488 | id:1303.2488 author:Michel Crampes, Michel Plantié category:stat.ML  published:2013-03-11 summary:Concept Hierarchies and Formal Concept Analysis are theoretically well grounded and largely experimented methods. They rely on line diagrams called Galois lattices for visualizing and analysing object-attribute sets. Galois lattices are visually seducing and conceptually rich for experts. However they present important drawbacks due to their concept oriented overall structure: analysing what they show is difficult for non experts, navigation is cumbersome, interaction is poor, and scalability is a deep bottleneck for visual interpretation even for experts. In this paper we introduce semantic probes as a means to overcome many of these problems and extend usability and application possibilities of traditional FCA visualization methods. Semantic probes are visual user centred objects which extract and organize reduced Galois sub-hierarchies. They are simpler, clearer, and they provide a better navigation support through a rich set of interaction possibilities. Since probe driven sub-hierarchies are limited to users focus, scalability is under control and interpretation is facilitated. After some successful experiments, several applications are being developed with the remaining problem of finding a compromise between simplicity and conceptual expressivity. version:1
arxiv-1303-1993 | Optimization viewpoint on Kalman smoothing, with applications to robust and sparse estimation | http://arxiv.org/abs/1303.1993 | id:1303.1993 author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:math.OC stat.CO stat.ML 62F35  65K10  published:2013-03-08 summary:In this paper, we present the optimization formulation of the Kalman filtering and smoothing problems, and use this perspective to develop a variety of extensions and applications. We first formulate classic Kalman smoothing as a least squares problem, highlight special structure, and show that the classic filtering and smoothing algorithms are equivalent to a particular algorithm for solving this problem. Once this equivalence is established, we present extensions of Kalman smoothing to systems with nonlinear process and measurement models, systems with linear and nonlinear inequality constraints, systems with outliers in the measurements or sudden changes in the state, and systems where the sparsity of the state sequence must be accounted for. All extensions preserve the computational efficiency of the classic algorithms, and most of the extensions are illustrated with numerical examples, which are part of an open source Kalman smoothing Matlab/Octave package. version:2
arxiv-1303-2465 | A Low-Complexity Algorithm for Static Background Estimation from Cluttered Image Sequences in Surveillance Contexts | http://arxiv.org/abs/1303.2465 | id:1303.2465 author:Vikas Reddy, Conrad Sanderson, Brian C. Lovell category:cs.CV I.4.5; I.4.8; G.3  published:2013-03-11 summary:For the purposes of foreground estimation, the true background model is unavailable in many practical circumstances and needs to be estimated from cluttered image sequences. We propose a sequential technique for static background estimation in such conditions, with low computational and memory requirements. Image sequences are analysed on a block-by-block basis. For each block location a representative set is maintained which contains distinct blocks obtained along its temporal line. The background estimation is carried out in a Markov Random Field framework, where the optimal labelling solution is computed using iterated conditional modes. The clique potentials are computed based on the combined frequency response of the candidate block and its neighbourhood. It is assumed that the most appropriate block results in the smoothest response, indirectly enforcing the spatial continuity of structures within a scene. Experiments on real-life surveillance videos demonstrate that the proposed method obtains considerably better background estimates (both qualitatively and quantitatively) than median filtering and the recently proposed "intervals of stable intensity" method. Further experiments on the Wallflower dataset suggest that the combination of the proposed method with a foreground segmentation algorithm results in improved foreground segmentation. version:1
arxiv-1303-2449 | Using qualia information to identify lexical semantic classes in an unsupervised clustering task | http://arxiv.org/abs/1303.2449 | id:1303.2449 author:Lauren Romeo, Sara Mendes, Núria Bel category:cs.CL  published:2013-03-11 summary:Acquiring lexical information is a complex problem, typically approached by relying on a number of contexts to contribute information for classification. One of the first issues to address in this domain is the determination of such contexts. The work presented here proposes the use of automatically obtained FORMAL role descriptors as features used to draw nouns from the same lexical semantic class together in an unsupervised clustering task. We have dealt with three lexical semantic classes (HUMAN, LOCATION and EVENT) in English. The results obtained show that it is possible to discriminate between elements from different lexical semantic classes using only FORMAL role information, hence validating our initial hypothesis. Also, iterating our method accurately accounts for fine-grained distinctions within lexical classes, namely distinctions involving ambiguous expressions. Moreover, a filtering and bootstrapping strategy employed in extracting FORMAL role descriptors proved to minimize effects of sparse data and noise in our task. version:1
arxiv-1303-2448 | Automatic Detection of Non-deverbal Event Nouns for Quick Lexicon Production | http://arxiv.org/abs/1303.2448 | id:1303.2448 author:Núria Bel, Maria Coll, Gabriela Resnik category:cs.CL  published:2013-03-11 summary:In this work we present the results of our experimental work on the develop-ment of lexical class-based lexica by automatic means. The objective is to as-sess the use of linguistic lexical-class based information as a feature selection methodology for the use of classifiers in quick lexical development. The results show that the approach can help in re-ducing the human effort required in the development of language resources sig-nificantly. version:1
arxiv-1303-4293 | A Multilingual Semantic Wiki Based on Attempto Controlled English and Grammatical Framework | http://arxiv.org/abs/1303.4293 | id:1303.4293 author:Kaarel Kaljurand, Tobias Kuhn category:cs.CL cs.HC  published:2013-03-11 summary:We describe a semantic wiki system with an underlying controlled natural language grammar implemented in Grammatical Framework (GF). The grammar restricts the wiki content to a well-defined subset of Attempto Controlled English (ACE), and facilitates a precise bidirectional automatic translation between ACE and language fragments of a number of other natural languages, making the wiki content accessible multilingually. Additionally, our approach allows for automatic translation into the Web Ontology Language (OWL), which enables automatic reasoning over the wiki content. The developed wiki environment thus allows users to build, query and view OWL knowledge bases via a user-friendly multilingual natural language interface. As a further feature, the underlying multilingual grammar is integrated into the wiki and can be collaboratively edited to extend the vocabulary of the wiki or even customize its sentence structures. This work demonstrates the combination of the existing technologies of Attempto Controlled English and Grammatical Framework, and is implemented as an extension of the existing semantic wiki engine AceWiki. version:1
arxiv-1303-2439 | Voxel-wise Weighted MR Image Enhancement using an Extended Neighborhood Filter | http://arxiv.org/abs/1303.2439 | id:1303.2439 author:Joseph Suresh Paul, Joshin John Mathew, Souparnika Kandoth Naroth, Chandrasekar Kesavadas category:cs.CV  published:2013-03-11 summary:We present an edge preserving and denoising filter for enhancing the features in images, which contain an ROI having a narrow spatial extent. Typical examples include angiograms, or ROI spatially distributed in multiple locations and contained within an outlying region, such as in multiple-sclerosis. The filtering involves determination of multiplicative weights in the spatial domain using an extended set of neighborhood directions. Equivalently, the filtering operation may be interpreted as a combination of directional filters in the frequency domain, with selective weighting for spatial frequencies contained within each direction. The advantages of the proposed filter in comparison to specialized non-linear filters, which operate on diffusion principle, are illustrated using numerical phantom data. The performance evaluation is carried out on simulated images from BrainWeb database for multiple-sclerosis, acute ischemic stroke using clinically acquired FLAIR images and MR angiograms. version:1
arxiv-1303-2437 | Least-Squares FIR Models of Low-Resolution MR data for Efficient Phase-Error Compensation with Simultaneous Artefact Removal | http://arxiv.org/abs/1303.2437 | id:1303.2437 author:Joseph Suresh Paul, Uma Krishna Swamy Pillai, Nyjin Thomas category:cs.CV  published:2013-03-11 summary:Signal space models in both phase-encode, and frequency-encode directions are presented for extrapolation of 2D partial kspace. Using the boxcar representation of low-resolution spatial data, and a geometrical representation of signal space vectors in both positive and negative phase-encode directions, a robust predictor is constructed using a series of signal space projections. Compared to some of the existing phase-correction methods that require acquisition of a pre-determined set of fractional kspace lines, the proposed predictor is found to be more efficient, due to its capability of exhibiting an equivalent degree of performance using only half the number of fractional lines. Robust filtering of noisy data is achieved using a second signal space model in the frequency-encode direction, bypassing the requirement of a prior highpass filtering operation. The signal space is constructed from Fourier Transformed samples of each row in the low-resolution image. A set of FIR filters are estimated by fitting a least squares model to this signal space. Partial kspace extrapolation using the FIR filters is shown to result in artifact-free reconstruction, particularly in respect of Gibbs ringing and streaking type artifacts. version:1
arxiv-1304-0725 | Improved Performance of Unsupervised Method by Renovated K-Means | http://arxiv.org/abs/1304.0725 | id:1304.0725 author:P. Ashok, G. M Kadhar Nawaz, E. Elayaraja, V. Vadivel category:cs.LG cs.CV stat.ML  published:2013-03-11 summary:Clustering is a separation of data into groups of similar objects. Every group called cluster consists of objects that are similar to one another and dissimilar to objects of other groups. In this paper, the K-Means algorithm is implemented by three distance functions and to identify the optimal distance function for clustering methods. The proposed K-Means algorithm is compared with K-Means, Static Weighted K-Means (SWK-Means) and Dynamic Weighted K-Means (DWK-Means) algorithm by using Davis Bouldin index, Execution Time and Iteration count methods. Experimental results show that the proposed K-Means algorithm performed better on Iris and Wine dataset when compared with other three clustering methods. version:1
arxiv-1303-2417 | Linear NDCG and Pair-wise Loss | http://arxiv.org/abs/1303.2417 | id:1303.2417 author:Xiao-Bo Jin, Guang-Gang Geng category:cs.LG stat.ML  published:2013-03-11 summary:Linear NDCG is used for measuring the performance of the Web content quality assessment in ECML/PKDD Discovery Challenge 2010. In this paper, we will prove that the DCG error equals a new pair-wise loss. version:1
arxiv-1303-2395 | State estimation under non-Gaussian Levy noise: A modified Kalman filtering method | http://arxiv.org/abs/1303.2395 | id:1303.2395 author:Xu Sun, Jinqiao Duan, Xiaofan Li, Xiangjun Wang category:math.DS cs.IT cs.LG math.IT math.PR stat.ML  published:2013-03-10 summary:The Kalman filter is extensively used for state estimation for linear systems under Gaussian noise. When non-Gaussian L\'evy noise is present, the conventional Kalman filter may fail to be effective due to the fact that the non-Gaussian L\'evy noise may have infinite variance. A modified Kalman filter for linear systems with non-Gaussian L\'evy noise is devised. It works effectively with reasonable computational cost. Simulation results are presented to illustrate this non-Gaussian filtering method. version:1
arxiv-1303-2330 | Image compression using anti-forensics method | http://arxiv.org/abs/1303.2330 | id:1303.2330 author:M. S. Sreelakshmi, D. Venkataraman category:cs.MM cs.CV  published:2013-03-10 summary:A large number of image forensics methods are available which are capable of identifying image tampering. But these techniques are not capable of addressing the anti-forensics method which is able to hide the trace of image tampering. In this paper anti-forensics method for digital image compression has been proposed. This anti-forensics method is capable of removing the traces of image compression. Additionally, technique is also able to remove the traces of blocking artifact that are left by image compression algorithms that divide an image into segments during compression process. This method is targeted to remove the compression fingerprints of JPEG compression. version:1
arxiv-1211-0053 | The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains | http://arxiv.org/abs/1211.0053 | id:1211.0053 author:David I Shuman, Sunil K. Narang, Pascal Frossard, Antonio Ortega, Pierre Vandergheynst category:cs.DM cs.LG cs.SI  published:2012-10-31 summary:In applications such as social, energy, transportation, sensor, and neuronal networks, high-dimensional data naturally reside on the vertices of weighted graphs. The emerging field of signal processing on graphs merges algebraic and spectral graph theoretic concepts with computational harmonic analysis to process such signals on graphs. In this tutorial overview, we outline the main challenges of the area, discuss different ways to define graph spectral domains, which are the analogues to the classical frequency domain, and highlight the importance of incorporating the irregular structures of graph data domains when processing signals on graphs. We then review methods to generalize fundamental operations such as filtering, translation, modulation, dilation, and downsampling to the graph setting, and survey the localized, multiscale transforms that have been proposed to efficiently extract information from high-dimensional data on graphs. We conclude with a brief discussion of open issues and possible extensions. version:2
arxiv-1303-2314 | Mini-Batch Primal and Dual Methods for SVMs | http://arxiv.org/abs/1303.2314 | id:1303.2314 author:Martin Takáč, Avleen Bijral, Peter Richtárik, Nathan Srebro category:cs.LG math.OC  published:2013-03-10 summary:We address the issue of using mini-batches in stochastic optimization of SVMs. We show that the same quantity, the spectral norm of the data, controls the parallelization speedup obtained for both primal stochastic subgradient descent (SGD) and stochastic dual coordinate ascent (SCDA) methods and use it to derive novel variants of mini-batched SDCA. Our guarantees for both methods are expressed in terms of the original nonsmooth primal problem based on the hinge-loss. version:1
arxiv-1303-2292 | Intelligent Approaches to interact with Machines using Hand Gesture Recognition in Natural way: A Survey | http://arxiv.org/abs/1303.2292 | id:1303.2292 author:Ankit Chaudhary, J. L. Raheja, Karen Das, Sonia Raheja category:cs.HC cs.CV  published:2013-03-10 summary:Hand gestures recognition (HGR) is one of the main areas of research for the engineers, scientists and bioinformatics. HGR is the natural way of Human Machine interaction and today many researchers in the academia and industry are working on different application to make interactions more easy, natural and convenient without wearing any extra device. HGR can be applied from games control to vision enabled robot control, from virtual reality to smart home systems. In this paper we are discussing work done in the area of hand gesture recognition where focus is on the intelligent approaches including soft computing based methods like artificial neural network, fuzzy logic, genetic algorithms etc. The methods in the preprocessing of image for segmentation and hand image construction also taken into study. Most researchers used fingertips for hand detection in appearance based modeling. Finally the comparison of results given by different researchers is also presented. version:1
arxiv-1106-6104 | Deterministic Sequencing of Exploration and Exploitation for Multi-Armed Bandit Problems | http://arxiv.org/abs/1106.6104 | id:1106.6104 author:Sattar Vakili, Keqin Liu, Qing Zhao category:math.OC cs.LG cs.SY math.PR math.ST stat.TH  published:2011-06-30 summary:In the Multi-Armed Bandit (MAB) problem, there is a given set of arms with unknown reward models. At each time, a player selects one arm to play, aiming to maximize the total expected reward over a horizon of length T. An approach based on a Deterministic Sequencing of Exploration and Exploitation (DSEE) is developed for constructing sequential arm selection policies. It is shown that for all light-tailed reward distributions, DSEE achieves the optimal logarithmic order of the regret, where regret is defined as the total expected reward loss against the ideal case with known reward models. For heavy-tailed reward distributions, DSEE achieves O(T^1/p) regret when the moments of the reward distributions exist up to the pth order for 1<p<=2 and O(T^1/(1+p/2)) for p>2. With the knowledge of an upperbound on a finite moment of the heavy-tailed reward distributions, DSEE offers the optimal logarithmic regret order. The proposed DSEE approach complements existing work on MAB by providing corresponding results for general reward distributions. Furthermore, with a clearly defined tunable parameter-the cardinality of the exploration sequence, the DSEE approach is easily extendable to variations of MAB, including MAB with various objectives, decentralized MAB with multiple players and incomplete reward observations under collisions, MAB with unknown Markov dynamics, and combinatorial MAB with dependent arms that often arise in network optimization problems such as the shortest path, the minimum spanning, and the dominating set problems under unknown random weights. version:3
arxiv-1303-2221 | Clustering on Multi-Layer Graphs via Subspace Analysis on Grassmann Manifolds | http://arxiv.org/abs/1303.2221 | id:1303.2221 author:Xiaowen Dong, Pascal Frossard, Pierre Vandergheynst, Nikolai Nefedov category:cs.LG cs.CV cs.SI stat.ML  published:2013-03-09 summary:Relationships between entities in datasets are often of multiple nature, like geographical distance, social relationships, or common interests among people in a social network, for example. This information can naturally be modeled by a set of weighted and undirected graphs that form a global multilayer graph, where the common vertex set represents the entities and the edges on different layers capture the similarities of the entities in term of the different modalities. In this paper, we address the problem of analyzing multi-layer graphs and propose methods for clustering the vertices by efficiently merging the information provided by the multiple modalities. To this end, we propose to combine the characteristics of individual graph layers using tools from subspace analysis on a Grassmann manifold. The resulting combination can then be viewed as a low dimensional representation of the original data which preserves the most important information from diverse relationships between entities. We use this information in new clustering methods and test our algorithm on several synthetic and real world datasets where we demonstrate superior or competitive performances compared to baseline and state-of-the-art techniques. Our generic framework further extends to numerous analysis and learning problems that involve different types of information on graphs. version:1
arxiv-1303-2215 | Expensive Optimisation: A Metaheuristics Perspective | http://arxiv.org/abs/1303.2215 | id:1303.2215 author:Maumita Bhattacharya category:cs.NE 97R40  published:2013-03-09 summary:Stochastic, iterative search methods such as Evolutionary Algorithms (EAs) are proven to be efficient optimizers. However, they require evaluation of the candidate solutions which may be prohibitively expensive in many real world optimization problems. Use of approximate models or surrogates is being explored as a way to reduce the number of such evaluations. In this paper we investigated three such methods. The first method (DAFHEA) partially replaces an expensive function evaluation by its approximate model. The approximation is realized with support vector machine (SVM) regression models. The second method (DAFHEA II) is an enhancement on DAFHEA to accommodate for uncertain environments. The third one uses surrogate ranking with preference learning or ordinal regression. The fitness of the candidates is estimated by modeling their rank. The techniques' performances on some of the benchmark numerical optimization problems have been reported. The comparative benefits and shortcomings of both techniques have been identified. version:1
arxiv-1303-2211 | Medical Information Embedding in Compressed Watermarked Intravascular Ultrasound Video | http://arxiv.org/abs/1303.2211 | id:1303.2211 author:Nilanjan Dey, Suvojit Acharjee, Debalina Biswas, Achintya Das, Sheli Sinha Chaudhuri category:cs.MM cs.CV  published:2013-03-09 summary:In medical field, intravascular ultrasound (IVUS) is a tomographic imaging modality, which can identify the boundaries of different layers of blood vessels. IVUS can detect myocardial infarction (heart attack) that remains ignored and unattended when only angioplasty is done. During the past decade, it became easier for some individuals or groups to copy and transmits digital information without the permission of the owner. For increasing authentication and security of copyrights, digital watermarking, an information hiding technique, was introduced. Achieving watermarking technique with lesser amount of distortion in biomedical data is a challenging task. Watermark can be embedded into an image or in a video. As video data is a huge amount of information, therefore a large storage area is needed which is not feasible. In this case motion vector based video compression is done to reduce size. In this present paper, an Electronic Patient Record (EPR) is embedded as watermark within an IVUS video and then motion vector is calculated. This proposed method proves robustness as the extracted watermark has good PSNR value and less MSE. version:1
arxiv-1304-2310 | Embedding of Blink Frequency in Electrooculography Signal using Difference Expansion based Reversible Watermarking Technique | http://arxiv.org/abs/1304.2310 | id:1304.2310 author:Nilanjan Dey, Prasenjit Maji, Poulami Das, Shouvik Biswas, Achintya Das, Sheli Sinha Chaudhuri category:cs.CV cs.IR  published:2013-03-09 summary:In the past few years, like other fields, rapid expansion of digitization and globalization has influenced the medical field as well. For progress of diagnostic results most of the reputed hospitals and diagnostic centres all over the world have started exchanging medical information. In this proposed method, the calculated diagnostic parametric values of the original Electrooculography (EOG) signal are embedded as a watermark by using Difference Expansion (DE) algorithm based reversible watermarking technique. The extracted watermark provides the required parametric values at the recipient end without any post computation of the recovered EOG signal. By computing the parametric values from the recovered signal, the integrity of the extracted watermark can be validated. The time domain features of EOG signal are calculated for the generation of watermark. In the current work, various features are studied and two major features related to blink frequency are used to generate the watermark. The high Signal to Noise Ratio (SNR) and the Bit Error Rate (BER) claim the robustness of the proposed method. version:1
arxiv-1303-2104 | Transfer Learning for Voice Activity Detection: A Denoising Deep Neural Network Perspective | http://arxiv.org/abs/1303.2104 | id:1303.2104 author:Xiao-Lei Zhang, Ji Wu category:cs.LG  published:2013-03-08 summary:Mismatching problem between the source and target noisy corpora severely hinder the practical use of the machine-learning-based voice activity detection (VAD). In this paper, we try to address this problem in the transfer learning prospective. Transfer learning tries to find a common learning machine or a common feature subspace that is shared by both the source corpus and the target corpus. The denoising deep neural network is used as the learning machine. Three transfer techniques, which aim to learn common feature representations, are used for analysis. Experimental results demonstrate the effectiveness of the transfer learning schemes on the mismatch problem. version:1
arxiv-1303-2096 | Gene-Machine, a new search heuristic algorithm | http://arxiv.org/abs/1303.2096 | id:1303.2096 author:Alfredo Garcia Woods category:cs.NE cs.AI  published:2013-03-08 summary:This paper introduces Gene-Machine, an efficient and new search heuristic algorithm, based in the building-block hypothesis. It is inspired by natural evolution, but does not use some of the concepts present in genetic algorithms like population, mutation and generation. This heuristic exhibits good performance in comparison with genetic algorithms, and can be used to generate useful solutions to optimization and search problems. version:1
arxiv-1301-3605 | Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks | http://arxiv.org/abs/1301.3605 | id:1301.3605 author:Dong Yu, Michael L. Seltzer, Jinyu Li, Jui-Ting Huang, Frank Seide category:cs.LG cs.CL cs.NE  published:2013-01-16 summary:Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. We show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth, which leads to better speech recognition performance with deeper networks. We also show that DNNs cannot extrapolate to test samples that are substantially different from the training examples. If the training data are sufficiently representative, however, internal features learned by the DNN are relatively stable with respect to speaker differences, bandwidth differences, and environment distortion. This enables DNN-based recognizers to perform as well or better than state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization. version:3
arxiv-1303-2054 | Mining Representative Unsubstituted Graph Patterns Using Prior Similarity Matrix | http://arxiv.org/abs/1303.2054 | id:1303.2054 author:Wajdi Dhifli, Rabie Saidi, Engelbert Mephu Nguifo category:cs.CE cs.LG  published:2013-03-08 summary:One of the most powerful techniques to study protein structures is to look for recurrent fragments (also called substructures or spatial motifs), then use them as patterns to characterize the proteins under study. An emergent trend consists in parsing proteins three-dimensional (3D) structures into graphs of amino acids. Hence, the search of recurrent spatial motifs is formulated as a process of frequent subgraph discovery where each subgraph represents a spatial motif. In this scope, several efficient approaches for frequent subgraph discovery have been proposed in the literature. However, the set of discovered frequent subgraphs is too large to be efficiently analyzed and explored in any further process. In this paper, we propose a novel pattern selection approach that shrinks the large number of discovered frequent subgraphs by selecting the representative ones. Existing pattern selection approaches do not exploit the domain knowledge. Yet, in our approach we incorporate the evolutionary information of amino acids defined in the substitution matrices in order to select the representative subgraphs. We show the effectiveness of our approach on a number of real datasets. The results issued from our experiments show that our approach is able to considerably decrease the number of motifs while enhancing their interestingness. version:1
arxiv-1303-2017 | Security Assessment of Software Design using Neural Network | http://arxiv.org/abs/1303.2017 | id:1303.2017 author:A. Adebiyi, Johnnes Arreymbi, Chris Imafidon category:cs.CR cs.NE  published:2013-03-08 summary:Security flaws in software applications today has been attributed mostly to design flaws. With limited budget and time to release software into the market, many developers often consider security as an afterthought. Previous research shows that integrating security into software applications at a later stage of software development lifecycle (SDLC) has been found to be more costly than when it is integrated during the early stages. To assist in the integration of security early in the SDLC stages, a new approach for assessing security during the design phase by neural network is investigated in this paper. Our findings show that by training a back propagation neural network to identify attack patterns, possible attacks can be identified from design scenarios presented to it. The result of performance of the neural network is presented in this paper. version:1
arxiv-1301-3342 | Barnes-Hut-SNE | http://arxiv.org/abs/1301.3342 | id:1301.3342 author:Laurens van der Maaten category:cs.LG cs.CV stat.ML  published:2013-01-15 summary:The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects. version:2
arxiv-1303-1932 | Mining and Exploiting Domain-Specific Corpora in the PANACEA Platform | http://arxiv.org/abs/1303.1932 | id:1303.1932 author:Núria Bel, Vassilis Papavasiliou, Prokopis Prokopidis, Antonio Toral, Victoria Arranz category:cs.CL  published:2013-03-08 summary:The objective of the PANACEA ICT-2007.2.2 EU project is to build a platform that automates the stages involved in the acquisition, production, updating and maintenance of the large language resources required by, among others, MT systems. The development of a Corpus Acquisition Component (CAC) for extracting monolingual and bilingual data from the web is one of the most innovative building blocks of PANACEA. The CAC, which is the first stage in the PANACEA pipeline for building Language Resources, adopts an efficient and distributed methodology to crawl for web documents with rich textual content in specific languages and predefined domains. The CAC includes modules that can acquire parallel data from sites with in-domain content available in more than one language. In order to extrinsically evaluate the CAC methodology, we have conducted several experiments that used crawled parallel corpora for the identification and extraction of parallel sentences using sentence alignment. The corpora were then successfully used for domain adaptation of Machine Translation Systems. version:1
arxiv-1303-1931 | A Classification of Adjectives for Polarity Lexicons Enhancement | http://arxiv.org/abs/1303.1931 | id:1303.1931 author:Silvia Vázquez, Núria Bel category:cs.CL  published:2013-03-08 summary:Subjective language detection is one of the most important challenges in Sentiment Analysis. Because of the weight and frequency in opinionated texts, adjectives are considered a key piece in the opinion extraction process. These subjective units are more and more frequently collected in polarity lexicons in which they appear annotated with their prior polarity. However, at the moment, any polarity lexicon takes into account prior polarity variations across domains. This paper proves that a majority of adjectives change their prior polarity value depending on the domain. We propose a distinction between domain dependent and domain independent adjectives. Moreover, our analysis led us to propose a further classification related to subjectivity degree: constant, mixed and highly subjective adjectives. Following this classification, polarity values will be a better support for Sentiment Analysis. version:1
arxiv-1303-1930 | Automatic lexical semantic classification of nouns | http://arxiv.org/abs/1303.1930 | id:1303.1930 author:Núria Bel, Lauren Romeo, Muntsa Padró category:cs.CL  published:2013-03-08 summary:The work we present here addresses cue-based noun classification in English and Spanish. Its main objective is to automatically acquire lexical semantic information by classifying nouns into previously known noun lexical classes. This is achieved by using particular aspects of linguistic contexts as cues that identify a specific lexical class. Here we concentrate on the task of identifying such cues and the theoretical background that allows for an assessment of the complexity of the task. The results show that, despite of the a-priori complexity of the task, cue-based classification is a useful tool in the automatic acquisition of lexical semantic classes. version:1
arxiv-1303-1929 | Towards the Fully Automatic Merging of Lexical Resources: A Step Forward | http://arxiv.org/abs/1303.1929 | id:1303.1929 author:Muntsa Padró, Núria Bel, Silvia Necsulescu category:cs.CL  published:2013-03-08 summary:This article reports on the results of the research done towards the fully automatically merging of lexical resources. Our main goal is to show the generality of the proposed approach, which have been previously applied to merge Spanish Subcategorization Frames lexica. In this work we extend and apply the same technique to perform the merging of morphosyntactic lexica encoded in LMF. The experiments showed that the technique is general enough to obtain good results in these two different tasks which is an important step towards performing the merging of lexical resources fully automatically. version:1
arxiv-1303-1913 | Design and Development of Artificial Neural Networking (ANN) system using sigmoid activation function to predict annual rice production in Tamilnadu | http://arxiv.org/abs/1303.1913 | id:1303.1913 author:S. Arun Balaji, K. Baskaran category:cs.NE F.2.2; I.2.7  published:2013-03-08 summary:Prediction of annual rice production in all the 31 districts of Tamilnadu is an important decision for the Government of Tamilnadu. Rice production is a complex process and non linear problem involving soil, crop, weather, pest, disease, capital, labour and management parameters. ANN software was designed and developed with Feed Forward Back Propagation (FFBP) network to predict rice production. The input layer has six independent variables like area of cultivation and rice production in three seasons like Kuruvai, Samba and Kodai. The popular sigmoid activation function was adopted to convert input data into sigmoid values. The hidden layer computes the summation of six sigmoid values with six sets of weightages. The final output was converted into sigmoid values using a sigmoid transfer function. ANN outputs are the predicted results. The error between original data and ANN output values were computed. A threshold value of 10-9 was used to test whether the error is greater than the threshold level. If the error is greater than threshold then updating of weights was done all summations were done by back propagation. This process was repeated until error equal to zero. The predicted results were printed and it was found to be exactly matching with the expected values. It shows that the ANN prediction was 100% accurate. version:1
arxiv-1302-5474 | On the performance of a hybrid genetic algorithm in dynamic environments | http://arxiv.org/abs/1302.5474 | id:1302.5474 author:Quan Yuan, Zhixin Yang category:cs.NE math.OC 68T20  published:2013-02-22 summary:The ability to track the optimum of dynamic environments is important in many practical applications. In this paper, the capability of a hybrid genetic algorithm (HGA) to track the optimum in some dynamic environments is investigated for different functional dimensions, update frequencies, and displacement strengths in different types of dynamic environments. Experimental results are reported by using the HGA and some other existing evolutionary algorithms in the literature. The results show that the HGA has better capability to track the dynamic optimum than some other existing algorithms. version:2
arxiv-1210-4792 | Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear Multivariate Regression and Granger Causality | http://arxiv.org/abs/1210.4792 | id:1210.4792 author:Vikas Sindhwani, Minh Ha Quang, Aurelie C. Lozano category:stat.ML cs.LG  published:2012-10-17 summary:We propose a general matrix-valued multiple kernel learning framework for high-dimensional nonlinear multivariate regression problems. This framework allows a broad class of mixed norm regularizers, including those that induce sparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel Hilbert Spaces. We develop a highly scalable and eigendecomposition-free algorithm that orchestrates two inexact solvers for simultaneously learning both the input and output components of separable matrix-valued kernels. As a key application enabled by our framework, we show how high-dimensional causal inference tasks can be naturally cast as sparse function estimation problems, leading to novel nonlinear extensions of a class of Graphical Granger Causality techniques. Our algorithmic developments and extensive empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds. version:2
arxiv-1303-1868 | Estimation of soil moisture in paddy field using Artificial Neural Networks | http://arxiv.org/abs/1303.1868 | id:1303.1868 author:Chusnul Arif, Masaru Mizoguchi, Budi Indra Setiawan, Ryoichi Doi category:cs.NE physics.ao-ph  published:2013-03-08 summary:In paddy field, monitoring soil moisture is required for irrigation scheduling and water resource allocation, management and planning. The current study proposes an Artificial Neural Networks (ANN) model to estimate soil moisture in paddy field with limited meteorological data. Dynamic of ANN model was adopted to estimate soil moisture with the inputs of reference evapotranspiration (ETo) and precipitation. ETo was firstly estimated using the maximum, average and minimum values of air temperature as the inputs of model. The models were performed under different weather conditions between the two paddy cultivation periods. Training process of model was carried out using the observation data in the first period, while validation process was conducted based on the observation data in the second period. Dynamic of ANN model estimated soil moisture with R2 values of 0.80 and 0.73 for training and validation processes, respectively, indicated that tight linear correlations between observed and estimated values of soil moisture were observed. Thus, the ANN model reliably estimates soil moisture with limited meteorological data. version:1
arxiv-1303-1829 | Watersheds on edge or node weighted graphs "par l'exemple" | http://arxiv.org/abs/1303.1829 | id:1303.1829 author:Fernand Meyer category:cs.CV  published:2013-03-07 summary:Watersheds have been defined both for node and edge weighted graphs. We show that they are identical: for each edge (resp.\ node) weighted graph exists a node (resp. edge) weighted graph with the same minima and catchment basin. version:1
arxiv-1210-2687 | Deconvolving Images with Unknown Boundaries Using the Alternating Direction Method of Multipliers | http://arxiv.org/abs/1210.2687 | id:1210.2687 author:Mariana S. C. Almeida, Mário A. T. Figueiredo category:math.OC cs.CV 68U10 I.4.4  published:2012-10-09 summary:The alternating direction method of multipliers (ADMM) has recently sparked interest as a flexible and efficient optimization tool for imaging inverse problems, namely deconvolution and reconstruction under non-smooth convex regularization. ADMM achieves state-of-the-art speed by adopting a divide and conquer strategy, wherein a hard problem is split into simpler, efficiently solvable sub-problems (e.g., using fast Fourier or wavelet transforms, or simple proximity operators). In deconvolution, one of these sub-problems involves a matrix inversion (i.e., solving a linear system), which can be done efficiently (in the discrete Fourier domain) if the observation operator is circulant, i.e., under periodic boundary conditions. This paper extends ADMM-based image deconvolution to the more realistic scenario of unknown boundary, where the observation operator is modeled as the composition of a convolution (with arbitrary boundary conditions) with a spatial mask that keeps only pixels that do not depend on the unknown boundary. The proposed approach also handles, at no extra cost, problems that combine the recovery of missing pixels (i.e., inpainting) with deconvolution. We show that the resulting algorithms inherit the convergence guarantees of ADMM and illustrate its performance on non-periodic deblurring (with and without inpainting of interior pixels) under total-variation and frame-based regularization. version:2
arxiv-1303-1761 | Improving Automatic Emotion Recognition from speech using Rhythm and Temporal feature | http://arxiv.org/abs/1303.1761 | id:1303.1761 author:Mayank Bhargava, Tim Polzehl category:cs.CV  published:2013-03-07 summary:This paper is devoted to improve automatic emotion recognition from speech by incorporating rhythm and temporal features. Research on automatic emotion recognition so far has mostly been based on applying features like MFCCs, pitch and energy or intensity. The idea focuses on borrowing rhythm features from linguistic and phonetic analysis and applying them to the speech signal on the basis of acoustic knowledge only. In addition to this we exploit a set of temporal and loudness features. A segmentation unit is employed in starting to separate the voiced/unvoiced and silence parts and features are explored on different segments. Thereafter different classifiers are used for classification. After selecting the top features using an IGR filter we are able to achieve a recognition rate of 80.60 % on the Berlin Emotion Database for the speaker dependent framework. version:1
arxiv-1004-0314 | Visualization of Manifold-Valued Elements by Multidimensional Scaling | http://arxiv.org/abs/1004.0314 | id:1004.0314 author:Simone Fiori category:stat.ML  published:2010-04-02 summary:The present contribution suggests the use of a multidimensional scaling (MDS) algorithm as a visualization tool for manifold-valued elements. A visualization tool of this kind is useful in signal processing and machine learning whenever learning/adaptation algorithms insist on high-dimensional parameter manifolds. version:2
arxiv-1303-1703 | Concept-based indexing in text information retrieval | http://arxiv.org/abs/1303.1703 | id:1303.1703 author:Fatiha Boubekeur, Wassila Azzoug category:cs.IR cs.CL  published:2013-03-07 summary:Traditional information retrieval systems rely on keywords to index documents and queries. In such systems, documents are retrieved based on the number of shared keywords with the query. This lexical-focused retrieval leads to inaccurate and incomplete results when different keywords are used to describe the documents and queries. Semantic-focused retrieval approaches attempt to overcome this problem by relying on concepts rather than on keywords to indexing and retrieval. The goal is to retrieve documents that are semantically relevant to a given user query. This paper addresses this issue by proposing a solution at the indexing level. More precisely, we propose a novel approach for semantic indexing based on concepts identified from a linguistic resource. In particular, our approach relies on the joint use of WordNet and WordNetDomains lexical databases for concept identification. Furthermore, we propose a semantic-based concept weighting scheme that relies on a novel definition of concept centrality. The resulting system is evaluated on the TIME test collection. Experimental results show the effectiveness of our proposition over traditional IR approaches. version:1
arxiv-1303-1700 | K-Nearest Neighbour algorithm coupled with logistic regression in medical case-based reasoning systems. Application to prediction of access to the renal transplant waiting list in Brittany | http://arxiv.org/abs/1303.1700 | id:1303.1700 author:Boris Campillo-Gimenez, Wassim Jouini, Sahar Bayat, Marc Cuggia category:cs.AI stat.ML  published:2013-03-07 summary:Introduction. Case Based Reasoning (CBR) is an emerg- ing decision making paradigm in medical research where new cases are solved relying on previously solved similar cases. Usually, a database of solved cases is provided, and every case is described through a set of attributes (inputs) and a label (output). Extracting useful information from this database can help the CBR system providing more reliable results on the yet to be solved cases. Objective. For that purpose we suggest a general frame- work where a CBR system, viz. K-Nearest Neighbor (K-NN) algorithm, is combined with various information obtained from a Logistic Regression (LR) model. Methods. LR is applied, on the case database, to assign weights to the attributes as well as the solved cases. Thus, five possible decision making systems based on K-NN and/or LR were identified: a standalone K-NN, a standalone LR and three soft K-NN algorithms that rely on the weights based on the results of the LR. The evaluation of the described approaches is performed in the field of renal transplant access waiting list. Results and conclusion. The results show that our suggested approach, where the K-NN algorithm relies on both weighted attributes and cases, can efficiently deal with non relevant attributes, whereas the four other approaches suffer from this kind of noisy setups. The robustness of this approach suggests interesting perspectives for medical problem solving tools using CBR methodology. version:1
arxiv-1211-3831 | Objective Improvement in Information-Geometric Optimization | http://arxiv.org/abs/1211.3831 | id:1211.3831 author:Youhei Akimoto, Yann Ollivier category:cs.LG cs.AI math.OC stat.ML  published:2012-11-16 summary:Information-Geometric Optimization (IGO) is a unified framework of stochastic algorithms for optimization problems. Given a family of probability distributions, IGO turns the original optimization problem into a new maximization problem on the parameter space of the probability distributions. IGO updates the parameter of the probability distribution along the natural gradient, taken with respect to the Fisher metric on the parameter manifold, aiming at maximizing an adaptive transform of the objective function. IGO recovers several known algorithms as particular instances: for the family of Bernoulli distributions IGO recovers PBIL, for the family of Gaussian distributions the pure rank-mu CMA-ES update is recovered, and for exponential families in expectation parametrization the cross-entropy/ML method is recovered. This article provides a theoretical justification for the IGO framework, by proving that any step size not greater than 1 guarantees monotone improvement over the course of optimization, in terms of q-quantile values of the objective function f. The range of admissible step sizes is independent of f and its domain. We extend the result to cover the case of different step sizes for blocks of the parameters in the IGO algorithm. Moreover, we prove that expected fitness improves over time when fitness-proportional selection is applied, in which case the RPP algorithm is recovered. version:3
arxiv-1303-1667 | ALPRS - A New Approach for License Plate Recognition using the Sift Algorithm | http://arxiv.org/abs/1303.1667 | id:1303.1667 author:Francisco Assis da Silva, Almir Olivette Artero, Maria Stela Veludo de Paiva, Ricardo Luis Barbosa category:cs.CV  published:2013-03-07 summary:This paper presents a new approach for the automatic license plate recognition, which includes the SIFT algorithm in step to locate the plate in the input image. In this new approach, besides the comparison of the features obtained with the SIFT algorithm, the correspondence between the spatial orientations and the positioning associated with the keypoints is also observed. Afterwards, an algorithm is used for the character recognition of the plates, very fast, which makes it possible its application in real time. The results obtained with the proposed approach presented very good success rates, so much for locating the characters in the input image, as for their recognition. version:1
arxiv-1303-1624 | On Robust Face Recognition via Sparse Encoding: the Good, the Bad, and the Ugly | http://arxiv.org/abs/1303.1624 | id:1303.1624 author:Yongkang Wong, Mehrtash T. Harandi, Conrad Sanderson category:cs.CV  published:2013-03-07 summary:In the field of face recognition, Sparse Representation (SR) has received considerable attention during the past few years. Most of the relevant literature focuses on holistic descriptors in closed-set identification applications. The underlying assumption in SR-based methods is that each class in the gallery has sufficient samples and the query lies on the subspace spanned by the gallery of the same class. Unfortunately, such assumption is easily violated in the more challenging face verification scenario, where an algorithm is required to determine if two faces (where one or both have not been seen before) belong to the same person. In this paper, we first discuss why previous attempts with SR might not be applicable to verification problems. We then propose an alternative approach to face verification via SR. Specifically, we propose to use explicit SR encoding on local image patches rather than the entire face. The obtained sparse signals are pooled via averaging to form multiple region descriptors, which are then concatenated to form an overall face descriptor. Due to the deliberate loss spatial relations within each region (caused by averaging), the resulting descriptor is robust to misalignment & various image deformations. Within the proposed framework, we evaluate several SR encoding techniques: l1-minimisation, Sparse Autoencoder Neural Network (SANN), and an implicit probabilistic technique based on Gaussian Mixture Models. Thorough experiments on AR, FERET, exYaleB, BANCA and ChokePoint datasets show that the proposed local SR approach obtains considerably better and more robust performance than several previous state-of-the-art holistic SR methods, in both verification and closed-set identification problems. The experiments also show that l1-minimisation based encoding has a considerably higher computational than the other techniques, but leads to higher recognition rates. version:1
arxiv-1303-1599 | Efficient learning strategy of Chinese characters based on network approach | http://arxiv.org/abs/1303.1599 | id:1303.1599 author:Xiao-Yong Yan, Ying Fan, Zengru Di, Shlomo Havlin, Jinshan Wu category:physics.soc-ph cs.CL cs.SI  published:2013-03-07 summary:Based on network analysis of hierarchical structural relations among Chinese characters, we develop an efficient learning strategy of Chinese characters. We regard a more efficient learning method if one learns the same number of useful Chinese characters in less effort or time. We construct a node-weighted network of Chinese characters, where character usage frequencies are used as node weights. Using this hierarchical node-weighted network, we propose a new learning method, the distributed node weight (DNW) strategy, which is based on a new measure of nodes' importance that takes into account both the weight of the nodes and the hierarchical structure of the network. Chinese character learning strategies, particularly their learning order, are analyzed as dynamical processes over the network. We compare the efficiency of three theoretical learning methods and two commonly used methods from mainstream Chinese textbooks, one for Chinese elementary school students and the other for students learning Chinese as a second language. We find that the DNW method significantly outperforms the others, implying that the efficiency of current learning methods of major textbooks can be greatly improved. version:1
arxiv-1208-3719 | Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms | http://arxiv.org/abs/1208.3719 | id:1208.3719 author:Chris Thornton, Frank Hutter, Holger H. Hoos, Kevin Leyton-Brown category:cs.LG  published:2012-08-18 summary:Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection/hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance. version:2
arxiv-1303-1460 | On Considering Uncertainty and Alternatives in Low-Level Vision | http://arxiv.org/abs/1303.1460 | id:1303.1460 author:Steven M. LaValle, Seth A. Hutchinson category:cs.AI cs.CV  published:2013-03-06 summary:In this paper we address the uncertainty issues involved in the low-level vision task of image segmentation. Researchers in computer vision have worked extensively on this problem, in which the goal is to partition (or segment) an image into regions that are homogeneous or uniform in some sense. This segmentation is often utilized by some higher level process, such as an object recognition system. We show that by considering uncertainty in a Bayesian formalism, we can use statistical image models to build an approximate representation of a probability distribution over a space of alternative segmentations. We give detailed descriptions of the various levels of uncertainty associated with this problem, discuss the interaction of prior and posterior distributions, and provide the operations for constructing this representation. version:1
arxiv-1303-1312 | A Fast Iterative Bayesian Inference Algorithm for Sparse Channel Estimation | http://arxiv.org/abs/1303.1312 | id:1303.1312 author:Niels Lovmand Pedersen, Carles Navarro Manch category:stat.ML cs.IT math.IT  published:2013-03-06 summary:In this paper, we present a Bayesian channel estimation algorithm for multicarrier receivers based on pilot symbol observations. The inherent sparse nature of wireless multipath channels is exploited by modeling the prior distribution of multipath components' gains with a hierarchical representation of the Bessel K probability density function; a highly efficient, fast iterative Bayesian inference method is then applied to the proposed model. The resulting estimator outperforms other state-of-the-art Bayesian and non-Bayesian estimators, either by yielding lower mean squared estimation error or by attaining the same accuracy with improved convergence rate, as shown in our numerical evaluation. version:1
arxiv-1303-1280 | Large-Margin Metric Learning for Partitioning Problems | http://arxiv.org/abs/1303.1280 | id:1303.1280 author:Rémi Lajugie, Sylvain Arlot, Francis Bach category:cs.LG stat.ML  published:2013-03-06 summary:In this paper, we consider unsupervised partitioning problems, such as clustering, image segmentation, video segmentation and other change-point detection problems. We focus on partitioning problems based explicitly or implicitly on the minimization of Euclidean distortions, which include mean-based change-point detection, K-means, spectral clustering and normalized cuts. Our main goal is to learn a Mahalanobis metric for these unsupervised problems, leading to feature weighting and/or selection. This is done in a supervised way by assuming the availability of several potentially partially labelled datasets that share the same metric. We cast the metric learning problem as a large-margin structured prediction problem, with proper definition of regularizers and losses, leading to a convex optimization problem which can be solved efficiently with iterative techniques. We provide experiments where we show how learning the metric may significantly improve the partitioning performance in synthetic examples, bioinformatics, video segmentation and image segmentation problems. version:1
arxiv-1303-1264 | Discovery of factors in matrices with grades | http://arxiv.org/abs/1303.1264 | id:1303.1264 author:Radim Belohlavek, Vilem Vychodil category:cs.LG cs.NA  published:2013-03-06 summary:We present an approach to decomposition and factor analysis of matrices with ordinal data. The matrix entries are grades to which objects represented by rows satisfy attributes represented by columns, e.g. grades to which an image is red, a product has a given feature, or a person performs well in a test. We assume that the grades form a bounded scale equipped with certain aggregation operators and conforms to the structure of a complete residuated lattice. We present a greedy approximation algorithm for the problem of decomposition of such matrix in a product of two matrices with grades under the restriction that the number of factors be small. Our algorithm is based on a geometric insight provided by a theorem identifying particular rectangular-shaped submatrices as optimal factors for the decompositions. These factors correspond to formal concepts of the input data and allow an easy interpretation of the decomposition. We present illustrative examples and experimental evaluation. version:1
arxiv-1303-1243 | A Generalized Hybrid Real-Coded Quantum Evolutionary Algorithm Based on Particle Swarm Theory with Arithmetic Crossover | http://arxiv.org/abs/1303.1243 | id:1303.1243 author:Md. Amjad Hossain, Md. Kawser Hossain, M. M. A. Hashem category:cs.NE  published:2013-03-06 summary:This paper proposes a generalized Hybrid Real-coded Quantum Evolutionary Algorithm (HRCQEA) for optimizing complex functions as well as combinatorial optimization. The main idea of HRCQEA is to devise a new technique for mutation and crossover operators. Using the evolutionary equation of PSO a Single-Multiple gene Mutation (SMM) is designed and the concept of Arithmetic Crossover (AC) is used in the new Crossover operator. In HRCQEA, each triploid chromosome represents a particle and the position of the particle is updated using SMM and Quantum Rotation Gate (QRG), which can make the balance between exploration and exploitation. Crossover is employed to expand the search space, Hill Climbing Selection (HCS) and elitism help to accelerate the convergence speed. Simulation results on Knapsack Problem and five benchmark complex functions with high dimension show that HRCQEA performs better in terms of ability to discover the global optimum and convergence speed. version:1
arxiv-1303-1232 | Japanese-Spanish Thesaurus Construction Using English as a Pivot | http://arxiv.org/abs/1303.1232 | id:1303.1232 author:Jessica Ramírez, Masayuki Asahara, Yuji Matsumoto category:cs.CL cs.AI  published:2013-03-06 summary:We present the results of research with the goal of automatically creating a multilingual thesaurus based on the freely available resources of Wikipedia and WordNet. Our goal is to increase resources for natural language processing tasks such as machine translation targeting the Japanese-Spanish language pair. Given the scarcity of resources, we use existing English resources as a pivot for creating a trilingual Japanese-Spanish-English thesaurus. Our approach consists of extracting the translation tuples from Wikipedia, disambiguating them by mapping them to WordNet word senses. We present results comparing two methods of disambiguation, the first using VSM on Wikipedia article texts and WordNet definitions, and the second using categorical information extracted from Wikipedia, We find that mixing the two methods produces favorable results. Using the proposed method, we have constructed a multilingual Spanish-Japanese-English thesaurus consisting of 25,375 entries. The same method can be applied to any pair of languages that are linked to English in Wikipedia. version:1
arxiv-1303-1217 | Impulsive Noise Mitigation in Powerline Communications Using Sparse Bayesian Learning | http://arxiv.org/abs/1303.1217 | id:1303.1217 author:Jing Lin, Marcel Nassar, Brian L. Evans category:stat.ML cs.IT math.IT  published:2013-03-05 summary:Additive asynchronous and cyclostationary impulsive noise limits communication performance in OFDM powerline communication (PLC) systems. Conventional OFDM receivers assume additive white Gaussian noise and hence experience degradation in communication performance in impulsive noise. Alternate designs assume a parametric statistical model of impulsive noise and use the model parameters in mitigating impulsive noise. These receivers require overhead in training and parameter estimation, and degrade due to model and parameter mismatch, especially in highly dynamic environments. In this paper, we model impulsive noise as a sparse vector in the time domain without any other assumptions, and apply sparse Bayesian learning methods for estimation and mitigation without training. We propose three iterative algorithms with different complexity vs. performance trade-offs: (1) we utilize the noise projection onto null and pilot tones to estimate and subtract the noise impulses; (2) we add the information in the data tones to perform joint noise estimation and OFDM detection; (3) we embed our algorithm into a decision feedback structure to further enhance the performance of coded systems. When compared to conventional OFDM PLC receivers, the proposed receivers achieve SNR gains of up to 9 dB in coded and 10 dB in uncoded systems in the presence of impulsive noise. version:1
arxiv-1303-1051 | A Genetic algorithm to solve the container storage space allocation problem | http://arxiv.org/abs/1303.1051 | id:1303.1051 author:I. Ayachi, R. Kammarti, M. Ksouri, P. Borne, :, LAGIS, Ecole Centrale de Lille, :, LACS, Ecole Nationale des  category:cs.NE  published:2013-03-05 summary:This paper presented a genetic algorithm (GA) to solve the container storage problem in the port. This problem is studied with different container types such as regular, open side, open top, tank, empty and refrigerated containers. The objective of this problem is to determine an optimal containers arrangement, which respects customers delivery deadlines, reduces the rehandle operations of containers and minimizes the stop time of the container ship. In this paper, an adaptation of the genetic algorithm to the container storage problem is detailed and some experimental results are presented and discussed. The proposed approach was compared to a Last In First Out (LIFO) algorithm applied to the same problem and has recorded good results version:1
arxiv-1303-0964 | GBM Volumetry using the 3D Slicer Medical Image Computing Platform | http://arxiv.org/abs/1303.0964 | id:1303.0964 author:Jan Egger, Tina Kapur, Andriy Fedorov, Steve Pieper, James V. Miller, Harini Veeraraghavan, Bernd Freisleben, Alexandra Golby, Christopher Nimsky, Ron Kikinis category:cs.CV  published:2013-03-05 summary:Volumetric change in glioblastoma multiforme (GBM) over time is a critical factor in treatment decisions. Typically, the tumor volume is computed on a slice-by-slice basis using MRI scans obtained at regular intervals. (3D)Slicer - a free platform for biomedical research - provides an alternative to this manual slice-by-slice segmentation process, which is significantly faster and requires less user interaction. In this study, 4 physicians segmented GBMs in 10 patients, once using the competitive region-growing based GrowCut segmentation module of Slicer, and once purely by drawing boundaries completely manually on a slice-by-slice basis. Furthermore, we provide a variability analysis for three physicians for 12 GBMs. The time required for GrowCut segmentation was on an average 61% of the time required for a pure manual segmentation. A comparison of Slicer-based segmentation with manual slice-by-slice segmentation resulted in a Dice Similarity Coefficient of 88.43 +/- 5.23% and a Hausdorff Distance of 2.32 +/- 5.23 mm. version:1
arxiv-1303-0934 | GURLS: a Least Squares Library for Supervised Learning | http://arxiv.org/abs/1303.0934 | id:1303.0934 author:Andrea Tacchetti, Pavan K Mallapragada, Matteo Santoro, Lorenzo Rosasco category:cs.LG cs.AI cs.MS  published:2013-03-05 summary:We present GURLS, a least squares, modular, easy-to-extend software library for efficient supervised learning. GURLS is targeted to machine learning practitioners, as well as non-specialists. It offers a number state-of-the-art training strategies for medium and large-scale learning, and routines for efficient model selection. The library is particularly well suited for multi-output problems (multi-category/multi-label). GURLS is currently available in two independent implementations: Matlab and C++. It takes advantage of the favorable properties of regularized least squares algorithm to exploit advanced tools in linear algebra. Routines to handle computations with very large matrices by means of memory-mapped storage and distributed task execution are available. The package is distributed under the BSD licence and is available for download at https://github.com/CBCL/GURLS. version:1
arxiv-1302-5145 | Prediction and Clustering in Signed Networks: A Local to Global Perspective | http://arxiv.org/abs/1302.5145 | id:1302.5145 author:Kai-Yang Chiang, Cho-Jui Hsieh, Nagarajan Natarajan, Ambuj Tewari, Inderjit S. Dhillon category:cs.SI cs.LG  published:2013-02-20 summary:The study of social networks is a burgeoning research area. However, most existing work deals with networks that simply encode whether relationships exist or not. In contrast, relationships in signed networks can be positive ("like", "trust") or negative ("dislike", "distrust"). The theory of social balance shows that signed networks tend to conform to some local patterns that, in turn, induce certain global characteristics. In this paper, we exploit both local as well as global aspects of social balance theory for two fundamental problems in the analysis of signed networks: sign prediction and clustering. Motivated by local patterns of social balance, we first propose two families of sign prediction methods: measures of social imbalance (MOIs), and supervised learning using high order cycles (HOCs). These methods predict signs of edges based on triangles and \ell-cycles for relatively small values of \ell. Interestingly, by examining measures of social imbalance, we show that the classic Katz measure, which is used widely in unsigned link prediction, actually has a balance theoretic interpretation when applied to signed networks. Furthermore, motivated by the global structure of balanced networks, we propose an effective low rank modeling approach for both sign prediction and clustering. For the low rank modeling approach, we provide theoretical performance guarantees via convex relaxations, scale it up to large problem sizes using a matrix factorization based algorithm, and provide extensive experimental validation including comparisons with local approaches. Our experimental results indicate that, by adopting a more global viewpoint of balance structure, we get significant performance and computational gains in prediction and clustering tasks on signed networks. Our work therefore highlights the usefulness of the global aspect of balance theory for the analysis of signed networks. version:2
arxiv-1212-2958 | Spike and Tyke, the Quantized Neuron Model | http://arxiv.org/abs/1212.2958 | id:1212.2958 author:M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny category:cs.NE cs.AI  published:2012-12-07 summary:Modeling spike firing assumes that spiking statistics are Poisson, but real data violates this assumption. To capture non-Poissonian features, in order to fix the inevitable inherent irregularity, researchers rescale the time axis with tedious computational overhead instead of searching for another distribution. Spikes or action potentials are precisely-timed changes in the ionic transport through synapses adjusting the synaptic weight, successfully modeled and developed as a memristor. Memristance value is multiples of initial resistance. This reminds us with the foundations of quantum mechanics. We try to quantize potential and resistance, as done with energy. After reviewing Planck curve for blackbody radiation, we propose the quantization equations. We introduce and prove a theorem that quantizes the resistance. Then we define the tyke showing its basic characteristics. Finally we give the basic transformations to model spiking and link an energy quantum to a tyke. Investigation shows how this perfectly models the neuron spiking, with over 97% match. version:2
arxiv-1303-0742 | Multivariate Temporal Dictionary Learning for EEG | http://arxiv.org/abs/1303.0742 | id:1303.0742 author:Quentin Barthélemy, Cédric Gouy-Pailler, Yoann Isaac, Antoine Souloumiac, Anthony Larue, Jérôme I. Mars category:cs.LG q-bio.NC stat.ML  published:2013-03-04 summary:This article addresses the issue of representing electroencephalographic (EEG) signals in an efficient way. While classical approaches use a fixed Gabor dictionary to analyze EEG signals, this article proposes a data-driven method to obtain an adapted dictionary. To reach an efficient dictionary learning, appropriate spatial and temporal modeling is required. Inter-channels links are taken into account in the spatial multivariate model, and shift-invariance is used for the temporal model. Multivariate learned kernels are informative (a few atoms code plentiful energy) and interpretable (the atoms can have a physiological meaning). Using real EEG data, the proposed method is shown to outperform the classical multichannel matching pursuit used with a Gabor dictionary, as measured by the representative power of the learned dictionary and its spatial flexibility. Moreover, dictionary learning can capture interpretable patterns: this ability is illustrated on real data, learning a P300 evoked potential. version:1
arxiv-1301-3468 | Boltzmann Machines and Denoising Autoencoders for Image Denoising | http://arxiv.org/abs/1301.3468 | id:1301.3468 author:Kyunghyun Cho category:stat.ML cs.CV cs.LG  published:2013-01-15 summary:Image denoising based on a probabilistic model of local image patches has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good model for this. In this paper, we propose that another popular family of models in the field of deep learning, called Boltzmann machines, can perform image denoising as well as, or in certain cases of high level of noise, better than denoising autoencoders. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments confirmed our claim and revealed that the performance can be improved by adding more hidden layers, especially when the level of noise is high. version:6
arxiv-1303-0663 | Denoising Deep Neural Networks Based Voice Activity Detection | http://arxiv.org/abs/1303.0663 | id:1303.0663 author:Xiao-Lei Zhang, Ji Wu category:cs.LG cs.SD stat.ML  published:2013-03-04 summary:Recently, the deep-belief-networks (DBN) based voice activity detection (VAD) has been proposed. It is powerful in fusing the advantages of multiple features, and achieves the state-of-the-art performance. However, the deep layers of the DBN-based VAD do not show an apparent superiority to the shallower layers. In this paper, we propose a denoising-deep-neural-network (DDNN) based VAD to address the aforementioned problem. Specifically, we pre-train a deep neural network in a special unsupervised denoising greedy layer-wise mode, and then fine-tune the whole network in a supervised way by the common back-propagation algorithm. In the pre-training phase, we take the noisy speech signals as the visible layer and try to extract a new feature that minimizes the reconstruction cross-entropy loss between the noisy speech signals and its corresponding clean speech signals. Experimental results show that the proposed DDNN-based VAD not only outperforms the DBN-based VAD but also shows an apparent performance improvement of the deep layers over shallower layers. version:1
arxiv-1303-0647 | Spatial Fuzzy C Means PET Image Segmentation of Neurodegenerative Disorder | http://arxiv.org/abs/1303.0647 | id:1303.0647 author:A. Meena, R. Raja category:cs.CV  published:2013-03-04 summary:Nuclear image has emerged as a promising research work in medical field. Images from different modality meet its own challenge. Positron Emission Tomography (PET) image may help to precisely localize disease to assist in planning the right treatment for each case and saving valuable time. In this paper, a novel approach of Spatial Fuzzy C Means (PET SFCM) clustering algorithm is introduced on PET scan image datasets. The proposed algorithm is incorporated the spatial neighborhood information with traditional FCM and updating the objective function of each cluster. This algorithm is implemented and tested on huge data collection of patients with brain neuro degenerative disorder such as Alzheimers disease. It has demonstrated its effectiveness by testing it for real world patient data sets. Experimental results are compared with conventional FCM and K Means clustering algorithm. The performance of the PET SFCM provides satisfactory results compared with other two algorithms version:1
arxiv-1303-0645 | Symmetry Based Cluster Approach for Automatic Recognition of the Epileptic Focus in Brain Using PET Scan Image : An Analysis | http://arxiv.org/abs/1303.0645 | id:1303.0645 author:A. Meena, R. Raja category:cs.CV  published:2013-03-04 summary:Recognition of epileptic focal point is the important diagnosis when screening the epilepsy patients for latent surgical cures. The accurate localization is challenging one because of the low spatial resolution images with more noisy data. Positron Emission Tomography (PET) has now replaced the issues and caring a high resolution. This paper focuses the research of automated localization of epileptic seizures in brain functional images using symmetry based cluster approach. This approach presents a fully automated symmetry based brain abnormality detection method for PET sequences. PET images are spatially normalized to Digital Imaging and Communications in Medicine (DICOM) standard and then it has been trained using symmetry based cluster approach using Medical Image Processing, Analysis & Visualization (MIPAV) tool. The performance evolution is considered by the metric like accuracy of diagnosis. The obtained result is surely assists the surgeon for the automated identification of seizures focus. version:1
arxiv-1303-0644 | Automatic symmetry based cluster approach for anomalous brain identification in PET scan image : An Analysis | http://arxiv.org/abs/1303.0644 | id:1303.0644 author:A. Meena, K. Raja category:cs.CV  published:2013-03-04 summary:Medical image segmentation is referred to the segmentation of known anatomic structures from different medical images. Normally, the medical data researches are more complicated and an exclusive structures. This computer aided diagnosis is used for assisting doctors in evaluating medical imagery or in recognizing abnormal findings in a medical image. To integrate the specialized knowledge for medical data processing is helpful to form a real useful healthcare decision making system. This paper studies the different symmetry based distances applied in clustering algorithms and analyzes symmetry approach for Positron Emission Tomography (PET) scan image segmentation. Unlike CT and MRI, the PET scan identifies the structure of blood flow to and from organs. PET scan also helps in early diagnosis of cancer and heart, brain and gastro intestinal ailments and to detect the progress of treatment. In this paper, the scope diagnostic task expands for PET image in various brain functions. version:1
arxiv-1303-0635 | Recognition of Facial Expression Using Eigenvector Based Distributed Features and Euclidean Distance Based Decision Making Technique | http://arxiv.org/abs/1303.0635 | id:1303.0635 author:Jeemoni Kalita, Karen Das category:cs.CV  published:2013-03-04 summary:In this paper, an Eigenvector based system has been presented to recognize facial expressions from digital facial images. In the approach, firstly the images were acquired and cropping of five significant portions from the image was performed to extract and store the Eigenvectors specific to the expressions. The Eigenvectors for the test images were also computed, and finally the input facial image was recognized when similarity was obtained by calculating the minimum Euclidean distance between the test image and the different expressions. version:1
arxiv-1303-0634 | Indian Sign Language Recognition Using Eigen Value Weighted Euclidean Distance Based Classification Technique | http://arxiv.org/abs/1303.0634 | id:1303.0634 author:Joyeeta Singha, Karen Das category:cs.CV  published:2013-03-04 summary:Sign Language Recognition is one of the most growing fields of research today. Many new techniques have been developed recently in these fields. Here in this paper, we have proposed a system using Eigen value weighted Euclidean distance as a classification technique for recognition of various Sign Languages of India. The system comprises of four parts: Skin Filtering, Hand Cropping, Feature Extraction and Classification. Twenty four signs were considered in this paper, each having ten samples, thus a total of two hundred forty images was considered for which recognition rate obtained was 97 percent. version:1
arxiv-1303-0633 | Omega Model for Human Detection and Counting for application in Smart Surveillance System | http://arxiv.org/abs/1303.0633 | id:1303.0633 author:Subra Mukherjee, Karen Das category:cs.CV  published:2013-03-04 summary:Driven by the significant advancements in technology and social issues such as security management, there is a strong need for Smart Surveillance System in our society today. One of the key features of a Smart Surveillance System is efficient human detection and counting such that the system can decide and label events on its own. In this paper we propose a new, novel and robust model, The Omega Model, for detecting and counting human beings present in the scene. The proposed model employs a set of four distinct descriptors for identifying the unique features of the head, neck and shoulder regions of a person. This unique head neck shoulder signature given by the Omega Model exploits the challenges such as inter person variations in size and shape of peoples head, neck and shoulder regions to achieve robust detection of human beings even under partial occlusion, dynamically changing background and varying illumination conditions. After experimentation we observe and analyze the influences of each of the four descriptors on the system performance and computation speed and conclude that a weight based decision making system produces the best results. Evaluation results on a number of images indicate the validation of our method in actual situation. version:1
arxiv-1303-0489 | A Semantic approach for effective document clustering using WordNet | http://arxiv.org/abs/1303.0489 | id:1303.0489 author:Leena H. Patil, Mohammed Atique category:cs.CL cs.IR  published:2013-03-03 summary:Now a days, the text document is spontaneously increasing over the internet, e-mail and web pages and they are stored in the electronic database format. To arrange and browse the document it becomes difficult. To overcome such problem the document preprocessing, term selection, attribute reduction and maintaining the relationship between the important terms using background knowledge, WordNet, becomes an important parameters in data mining. In these paper the different stages are formed, firstly the document preprocessing is done by removing stop words, stemming is performed using porter stemmer algorithm, word net thesaurus is applied for maintaining relationship between the important terms, global unique words, and frequent word sets get generated, Secondly, data matrix is formed, and thirdly terms are extracted from the documents by using term selection approaches tf-idf, tf-df, and tf2 based on their minimum threshold value. Further each and every document terms gets preprocessed, where the frequency of each term within the document is counted for representation. The purpose of this approach is to reduce the attributes and find the effective term selection method using WordNet for better clustering accuracy. Experiments are evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and ship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group (Hardware), 20 News group (Computer Graphics) etc. version:1
arxiv-1201-0490 | Scikit-learn: Machine Learning in Python | http://arxiv.org/abs/1201.0490 | id:1201.0490 author:Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay category:cs.LG cs.MS  published:2012-01-02 summary:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net. version:2
arxiv-1303-0462 | Distributed Evolutionary Computation: A New Technique for Solving Large Number of Equations | http://arxiv.org/abs/1303.0462 | id:1303.0462 author:Moslema Jahan, M. M. A. Hashem, Gazi Abdullah Shahriar category:cs.NE  published:2013-03-03 summary:Evolutionary computation techniques have mostly been used to solve various optimization and learning problems successfully. Evolutionary algorithm is more effective to gain optimal solution(s) to solve complex problems than traditional methods. In case of problems with large set of parameters, evolutionary computation technique incurs a huge computational burden for a single processing unit. Taking this limitation into account, this paper presents a new distributed evolutionary computation technique, which decomposes decision vectors into smaller components and achieves optimal solution in a short time. In this technique, a Jacobi-based Time Variant Adaptive (JBTVA) Hybrid Evolutionary Algorithm is distributed incorporating cluster computation. Moreover, two new selection methods named Best All Selection (BAS) and Twin Selection (TS) are introduced for selecting best fit solution vector. Experimental results show that optimal solution is achieved for different kinds of problems having huge parameters and a considerable speedup is obtained in proposed distributed system. version:1
arxiv-1303-0460 | Genetic Programming for Document Segmentation and Region Classification Using Discipulus | http://arxiv.org/abs/1303.0460 | id:1303.0460 author:N. Priyadharshini, M. S. Vijaya category:cs.CV cs.NE  published:2013-03-03 summary:Document segmentation is a method of rending the document into distinct regions. A document is an assortment of information and a standard mode of conveying information to others. Pursuance of data from documents involves ton of human effort, time intense and might severely prohibit the usage of data systems. So, automatic information pursuance from the document has become a big issue. It is been shown that document segmentation will facilitate to beat such problems. This paper proposes a new approach to segment and classify the document regions as text, image, drawings and table. Document image is divided into blocks using Run length smearing rule and features are extracted from every blocks. Discipulus tool has been used to construct the Genetic programming based classifier model and located 97.5% classification accuracy. version:1
arxiv-1303-0446 | Statistical sentiment analysis performance in Opinum | http://arxiv.org/abs/1303.0446 | id:1303.0446 author:Boyan Bonev, Gema Ramírez-Sánchez, Sergio Ortiz Rojas category:cs.CL  published:2013-03-03 summary:The classification of opinion texts in positive and negative is becoming a subject of great interest in sentiment analysis. The existence of many labeled opinions motivates the use of statistical and machine-learning methods. First-order statistics have proven to be very limited in this field. The Opinum approach is based on the order of the words without using any syntactic and semantic information. It consists of building one probabilistic model for the positive and another one for the negative opinions. Then the test opinions are compared to both models and a decision and confidence measure are calculated. In order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most named-entities with wildcards. Opinum presents an accuracy above 81% for Spanish opinions in the financial products domain. In this work we discuss which are the most important factors that have impact on the classification performance. version:1
arxiv-1303-0445 | Detecting and resolving spatial ambiguity in text using named entity extraction and self learning fuzzy logic techniques | http://arxiv.org/abs/1303.0445 | id:1303.0445 author:Kanagavalli V R, Raja. K category:cs.IR cs.CL  published:2013-03-03 summary:Information extraction identifies useful and relevant text in a document and converts unstructured text into a form that can be loaded into a database table. Named entity extraction is a main task in the process of information extraction and is a classification problem in which words are assigned to one or more semantic classes or to a default non-entity class. A word which can belong to one or more classes and which has a level of uncertainty in it can be best handled by a self learning Fuzzy Logic Technique. This paper proposes a method for detecting the presence of spatial uncertainty in the text and dealing with spatial ambiguity using named entity extraction techniques coupled with self learning fuzzy logic techniques version:1
arxiv-1304-0751 | A Cumulative Multi-Niching Genetic Algorithm for Multimodal Function Optimization | http://arxiv.org/abs/1304.0751 | id:1304.0751 author:Matthew Hall category:cs.NE  published:2013-03-03 summary:This paper presents a cumulative multi-niching genetic algorithm (CMN GA), designed to expedite optimization problems that have computationally-expensive multimodal objective functions. By never discarding individuals from the population, the CMN GA makes use of the information from every objective function evaluation as it explores the design space. A fitness-related population density control over the design space reduces unnecessary objective function evaluations. The algorithm's novel arrangement of genetic operations provides fast and robust convergence to multiple local optima. Benchmark tests alongside three other multi-niching algorithms show that the CMN GA has a greater convergence ability and provides an order-of-magnitude reduction in the number of objective function evaluations required to achieve a given level of convergence. version:1
arxiv-1303-0362 | Inductive Sparse Subspace Clustering | http://arxiv.org/abs/1303.0362 | id:1303.0362 author:Xi Peng, Lei Zhang, Zhang Yi category:cs.LG  published:2013-03-02 summary:Sparse Subspace Clustering (SSC) has achieved state-of-the-art clustering quality by performing spectral clustering over a $\ell^{1}$-norm based similarity graph. However, SSC is a transductive method which does not handle with the data not used to construct the graph (out-of-sample data). For each new datum, SSC requires solving $n$ optimization problems in O(n) variables for performing the algorithm over the whole data set, where $n$ is the number of data points. Therefore, it is inefficient to apply SSC in fast online clustering and scalable graphing. In this letter, we propose an inductive spectral clustering algorithm, called inductive Sparse Subspace Clustering (iSSC), which makes SSC feasible to cluster out-of-sample data. iSSC adopts the assumption that high-dimensional data actually lie on the low-dimensional manifold such that out-of-sample data could be grouped in the embedding space learned from in-sample data. Experimental results show that iSSC is promising in clustering out-of-sample data. version:1
arxiv-1303-0350 | Structure-semantics interplay in complex networks and its effects on the predictability of similarity in texts | http://arxiv.org/abs/1303.0350 | id:1303.0350 author:Diego R. Amancio, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:cs.CL physics.soc-ph  published:2013-03-02 summary:There are different ways to define similarity for grouping similar texts into clusters, as the concept of similarity may depend on the purpose of the task. For instance, in topic extraction similar texts mean those within the same semantic field, whereas in author recognition stylistic features should be considered. In this study, we introduce ways to classify texts employing concepts of complex networks, which may be able to capture syntactic, semantic and even pragmatic features. The interplay between the various metrics of the complex networks is analyzed with three applications, namely identification of machine translation (MT) systems, evaluation of quality of machine translated texts and authorship recognition. We shall show that topological features of the networks representing texts can enhance the ability to identify MT systems in particular cases. For evaluating the quality of MT texts, on the other hand, high correlation was obtained with methods capable of capturing the semantics. This was expected because the golden standards used are themselves based on word co-occurrence. Notwithstanding, the Katz similarity, which involves semantic and structure in the comparison of texts, achieved the highest correlation with the NIST measurement, indicating that in some cases the combination of both approaches can improve the ability to quantify quality in MT. In authorship recognition, again the topological features were relevant in some contexts, though for the books and authors analyzed good results were obtained with semantic features as well. Because hybrid approaches encompassing semantic and topological features have not been extensively used, we believe that the methodology proposed here may be useful to enhance text classification considerably, as it combines well-established strategies. version:1
arxiv-1303-0347 | Probing the statistical properties of unknown texts: application to the Voynich Manuscript | http://arxiv.org/abs/1303.0347 | id:1303.0347 author:Diego R. Amancio, Eduardo G. Altmann, Diego Rybski, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:physics.soc-ph cs.CL physics.data-an  published:2013-03-02 summary:While the use of statistical physics methods to analyze large corpora has been useful to unveil many patterns in texts, no comprehensive investigation has been performed investigating the properties of statistical measurements across different languages and texts. In this study we propose a framework that aims at determining if a text is compatible with a natural language and which languages are closest to it, without any knowledge of the meaning of the words. The approach is based on three types of statistical measurements, i.e. obtained from first-order statistics of word properties in a text, from the topology of complex networks representing text, and from intermittency concepts where text is treated as a time series. Comparative experiments were performed with the New Testament in 15 different languages and with distinct books in English and Portuguese in order to quantify the dependency of the different measurements on the language and on the story being told in the book. The metrics found to be informative in distinguishing real texts from their shuffled versions include assortativity, degree and selectivity of words. As an illustration, we analyze an undeciphered medieval manuscript known as the Voynich Manuscript. We show that it is mostly compatible with natural languages and incompatible with random texts. We also obtain candidates for key-words of the Voynich Manuscript which could be helpful in the effort of deciphering it. Because we were able to identify statistical measurements that are more dependent on the syntax than on the semantics, the framework may also serve for text analysis in language-dependent applications. version:1
arxiv-1303-0339 | Learning Hash Functions Using Column Generation | http://arxiv.org/abs/1303.0339 | id:1303.0339 author:Xi Li, Guosheng Lin, Chunhua Shen, Anton van den Hengel, Anthony Dick category:cs.LG  published:2013-03-02 summary:Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning data-dependent hash functions have been developed. In this work, we propose a column generation based method for learning data-dependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information, our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure, the best hash function is selected. Unlike most other hashing methods, our method generalizes to new data points naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets. version:1
arxiv-1302-7051 | Polyploidy and Discontinuous Heredity Effect on Evolutionary Multi-Objective Optimization | http://arxiv.org/abs/1302.7051 | id:1302.7051 author:Wesam Elshamy, Hassan M Emara, Ahmed Bahgat category:cs.NE 60G15  published:2013-02-28 summary:This paper examines the effect of mimicking discontinuous heredity caused by carrying more than one chromosome in some living organisms cells in Evolutionary Multi-Objective Optimization algorithms. In this representation, the phenotype may not fully reflect the genotype. By doing so we are mimicking living organisms inheritance mechanism, where traits may be silently carried for many generations to reappear later. Representations with different number of chromosomes in each solution vector are tested on different benchmark problems with high number of decision variables and objectives. A comparison with Non-Dominated Sorting Genetic Algorithm-II is done on all problems. version:2
arxiv-1303-0323 | Clubs-based Particle Swarm Optimization | http://arxiv.org/abs/1303.0323 | id:1303.0323 author:Wesam Elshamy, Hassan M Emara, Ahmed Bahgat category:cs.NE 68T20  published:2013-03-02 summary:This paper introduces a new dynamic neighborhood network for particle swarm optimization. In the proposed Clubs-based Particle Swarm Optimization (C-PSO) algorithm, each particle initially joins a default number of what we call 'clubs'. Each particle is affected by its own experience and the experience of the best performing member of the clubs it is a member of. Clubs membership is dynamic, where the worst performing particles socialize more by joining more clubs to learn from other particles and the best performing particles are made to socialize less by leaving clubs to reduce their strong influence on other members. Particles return gradually to default membership level when they stop showing extreme performance. Inertia weights of swarm members are made random within a predefined range. This proposed dynamic neighborhood algorithm is compared with other two algorithms having static neighborhood topologies on a set of classic benchmark problems. The results showed superior performance for C-PSO regarding escaping local optima and convergence speed. version:1
arxiv-1303-0268 | Maximal Information Divergence from Statistical Models defined by Neural Networks | http://arxiv.org/abs/1303.0268 | id:1303.0268 author:Guido Montufar, Johannes Rauh, Nihat Ay category:math.ST stat.ML stat.TH 62E17  94A17  60E05  published:2013-03-01 summary:We review recent results about the maximal values of the Kullback-Leibler information divergence from statistical models defined by neural networks, including naive Bayes models, restricted Boltzmann machines, deep belief networks, and various classes of exponential families. We illustrate approaches to compute the maximal divergence from a given model starting from simple sub- or super-models. We give a new result for deep and narrow belief networks with finite-valued units. version:1
arxiv-1302-7175 | Estimating the Maximum Expected Value: An Analysis of (Nested) Cross Validation and the Maximum Sample Average | http://arxiv.org/abs/1302.7175 | id:1302.7175 author:Hado van Hasselt category:stat.ML cs.AI cs.LG stat.ME  published:2013-02-28 summary:We investigate the accuracy of the two most common estimators for the maximum expected value of a general set of random variables: a generalization of the maximum sample average, and cross validation. No unbiased estimator exists and we show that it is non-trivial to select a good estimator without knowledge about the distributions of the random variables. We investigate and bound the bias and variance of the aforementioned estimators and prove consistency. The variance of cross validation can be significantly reduced, but not without risking a large bias. The bias and variance of different variants of cross validation are shown to be very problem-dependent, and a wrong choice can lead to very inaccurate estimates. version:2
arxiv-1303-0166 | On a link between kernel mean maps and Fraunhofer diffraction, with an application to super-resolution beyond the diffraction limit | http://arxiv.org/abs/1303.0166 | id:1303.0166 author:Stefan Harmeling, Michael Hirsch, Bernhard Schölkopf category:physics.optics stat.ML  published:2013-03-01 summary:We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation, it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction, showing that imaging processes with arbitrarily small apertures can in principle be invertible, i.e., do not lose information, provided the objects to be imaged satisfy a generic condition. A real world experiment shows that we can super-resolve beyond the Rayleigh limit. version:1
arxiv-1303-0156 | Exploiting the Accumulated Evidence for Gene Selection in Microarray Gene Expression Data | http://arxiv.org/abs/1303.0156 | id:1303.0156 author:G. Prat, Ll. Belanche category:cs.CE cs.LG q-bio.QM I.5.2  published:2013-03-01 summary:Machine Learning methods have of late made significant efforts to solving multidisciplinary problems in the field of cancer classification using microarray gene expression data. Feature subset selection methods can play an important role in the modeling process, since these tasks are characterized by a large number of features and a few observations, making the modeling a non-trivial undertaking. In this particular scenario, it is extremely important to select genes by taking into account the possible interactions with other gene subsets. This paper shows that, by accumulating the evidence in favour (or against) each gene along the search process, the obtained gene subsets may constitute better solutions, either in terms of predictive accuracy or gene size, or in both. The proposed technique is extremely simple and applicable at a negligible overhead in cost. version:1
arxiv-1303-0140 | Second-Order Non-Stationary Online Learning for Regression | http://arxiv.org/abs/1303.0140 | id:1303.0140 author:Nina Vaits, Edward Moroshko, Koby Crammer category:cs.LG stat.ML  published:2013-03-01 summary:The goal of a learner, in standard online learning, is to have the cumulative loss not much larger compared with the best-performing function from some fixed class. Numerous algorithms were shown to have this gap arbitrarily close to zero, compared with the best function that is chosen off-line. Nevertheless, many real-world applications, such as adaptive filtering, are non-stationary in nature, and the best prediction function may drift over time. We introduce two novel algorithms for online regression, designed to work well in non-stationary environment. Our first algorithm performs adaptive resets to forget the history, while the second is last-step min-max optimal in context of a drift. We analyze both algorithms in the worst-case regret framework and show that they maintain an average loss close to that of the best slowly changing sequence of linear functions, as long as the cumulative drift is sublinear. In addition, in the stationary case, when no drift occurs, our algorithms suffer logarithmic regret, as for previous algorithms. Our bounds improve over the existing ones, and simulations demonstrate the usefulness of these algorithms compared with other state-of-the-art approaches. version:1
