arxiv-1412-7299 | Particle Metropolis-adjusted Langevin algorithms | http://arxiv.org/abs/1412.7299 | id:1412.7299 author:Christopher Nemeth, Chris Sherlock, Paul Fearnhead category:stat.ME stat.CO stat.ML  published:2014-12-23 summary:This paper proposes a new sampling scheme based on Langevin dynamics that is applicable within pseudo-marginal and particle Markov chain Monte Carlo algorithms. We investigate this algorithm's theoretical properties under standard asymptotics, which correspond to an increasing dimension of the parameters, $n$. Our results show that the behaviour of the algorithm depends crucially on how accurately one can estimate the gradient of the log target density. If the error in the estimate of the gradient is not sufficiently controlled as dimension increases, then asymptotically there will be no advantage over the simpler random-walk algorithm. However, if the error is sufficiently well-behaved, then the optimal scaling of this algorithm will be $O(n^{-1/6})$ compared to $O(n^{-1/2})$ for the random walk. Our theory also gives guidelines on how to tune the number of Monte Carlo samples in the likelihood estimate and the proposal step-size. version:3
arxiv-1506-03805 | Mondrian Forests for Large-Scale Regression when Uncertainty Matters | http://arxiv.org/abs/1506.03805 | id:1506.03805 author:Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh category:stat.ML cs.LG  published:2015-06-11 summary:Many real-world regression problems demand a measure of the uncertainty associated with each prediction. Standard decision forests deliver efficient state-of-the-art predictive performance, but high-quality uncertainty estimates are lacking. Gaussian processes (GPs) deliver uncertainty estimates, but scaling GPs to large-scale data sets comes at the cost of approximating the uncertainty estimates. We extend Mondrian forests, first proposed by Lakshminarayanan et al. (2014) for classification problems, to the large-scale non-parametric regression setting. Using a novel hierarchical Gaussian prior that dovetails with the Mondrian forest framework, we obtain principled uncertainty estimates, while still retaining the computational advantages of decision forests. Through a combination of illustrative examples, real-world large-scale datasets, and Bayesian optimization benchmarks, we demonstrate that Mondrian forests outperform approximate GPs on large-scale regression tasks and deliver better-calibrated uncertainty assessments than decision-forest-based methods. version:4
arxiv-1605-08576 | Merging MCMC Subposteriors through Gaussian-Process Approximations | http://arxiv.org/abs/1605.08576 | id:1605.08576 author:Christopher Nemeth, Chris Sherlock category:stat.CO stat.ML  published:2016-05-27 summary:Markov chain Monte Carlo (MCMC) algorithms have become powerful tools for Bayesian inference. However, they do not scale well to large-data problems. Divide-and-conquer strategies, which split the data into batches and, for each batch, run independent MCMC algorithms targeting the corresponding subposterior, can spread the computational burden across a number of separate workers. The challenge with such strategies is in recombining the subposteriors to approximate the full posterior. By creating a Gaussian-process approximation for each log-subposterior density we create a tractable approximation for the full posterior. This approximation is exploited through three methodologies: firstly a Hamiltonian Monte Carlo algorithm targeting the expectation of the posterior density provides a sample from an approximation to the posterior; secondly, evaluating the true posterior at the sampled points leads to an importance sampler that, asymptotically, targets the true posterior expectations; finally, an alternative importance sampler uses the full Gaussian-process distribution of the approximation to the log-posterior density to re-weight any initial sample and provide both an estimate of the posterior expectation and a measure of the uncertainty in it. version:1
arxiv-1606-01093 | Extraction of clinical information from the non-invasive fetal electrocardiogram | http://arxiv.org/abs/1606.01093 | id:1606.01093 author:Joachim Behar category:cs.CV  published:2016-05-27 summary:Estimation of the fetal heart rate (FHR) has gained interest in the last century, low heart rate variability has been studied to identify intrauterine growth restricted fetuses (prepartum), and abnormal FHR patterns have been associated with fetal distress during delivery (intrapartum). Several monitoring techniques have been proposed for FHR estimation, including auscultation and Doppler ultrasound. This thesis focuses on the extraction of the non-invasive fetal electrocardiogram (NI-FECG) recorded from a limited set of abdominal sensors. The main challenge with NI-FECG extraction techniques is the low signal-to-noise ratio of the FECG signal on the abdominal mixture signal which consists of a dominant maternal ECG component, FECG and noise. However the NI-FECG offers many advantages over the alternative fetal monitoring techniques, the most important one being the opportunity to enable morphological analysis of the FECG which is vital for determining whether an observed FHR event is normal or pathological. In order to advance the field of NI-FECG signal processing, the development of standardised public databases and benchmarking of a number of published and novel algorithms was necessary. version:1
arxiv-1602-02282 | Ladder Variational Autoencoders | http://arxiv.org/abs/1602.02282 | id:1602.02282 author:Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, Ole Winther category:stat.ML cs.LG  published:2016-02-06 summary:Variational Autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers. version:3
arxiv-1605-08543 | Lazy Evaluation of Convolutional Filters | http://arxiv.org/abs/1605.08543 | id:1605.08543 author:Sam Leroux, Steven Bohez, Cedric De Boom, Elias De Coninck, Tim Verbelen, Bert Vankeirsbilck, Pieter Simoens, Bart Dhoedt category:cs.CV cs.NE  published:2016-05-27 summary:In this paper we propose a technique which avoids the evaluation of certain convolutional filters in a deep neural network. This allows to trade-off the accuracy of a deep neural network with the computational and memory requirements. This is especially important on a constrained device unable to hold all the weights of the network in memory. version:1
arxiv-1605-08535 | Deep API Learning | http://arxiv.org/abs/1605.08535 | id:1605.08535 author:Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim category:cs.SE cs.CL cs.LG cs.NE D.2.13  published:2016-05-27 summary:Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bag-of-words (i.e., keyword matching or word-to-word alignment) and lack a deep understanding of the semantics of the query. We propose DeepAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bags-of-words assumption, it learns the sequence of words in a query and the sequence of associated APIs. DeepAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches. version:1
arxiv-1605-01600 | AVEC 2016 - Depression, Mood, and Emotion Recognition Workshop and Challenge | http://arxiv.org/abs/1605.01600 | id:1605.01600 author:Michel Valstar, Jonathan Gratch, Bjorn Schuller, Fabien Ringeval, Denis Lalanne, Mercedes Torres Torres, Stefan Scherer, Guiota Stratou, Roddy Cowie, Maja Pantic category:cs.CV cs.HC cs.MM  published:2016-05-05 summary:The Audio/Visual Emotion Challenge and Workshop (AVEC 2016) "Depression, Mood and Emotion" will be the sixth competition event aimed at comparison of multimedia processing and machine learning methods for automatic audio, visual and physiological depression and emotion analysis, with all participants competing under strictly the same conditions. The goal of the Challenge is to provide a common benchmark test set for multi-modal information processing and to bring together the depression and emotion recognition communities, as well as the audio, video and physiological processing communities, to compare the relative merits of the various approaches to depression and emotion recognition under well-defined and strictly comparable conditions and establish to what extent fusion of the approaches is possible and beneficial. This paper presents the challenge guidelines, the common data used, and the performance of the baseline system on the two tasks. version:3
arxiv-1605-08527 | Stochastic Optimization for Large-scale Optimal Transport | http://arxiv.org/abs/1605.08527 | id:1605.08527 author:Genevay Aude, Marco Cuturi, Gabriel Peyré, Francis Bach category:math.OC cs.LG math.NA  published:2016-05-27 summary:Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation ; (b) entropic regularization of the primal OT problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional OT solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, apart from computing OT on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems. version:1
arxiv-1605-08512 | SNN: Stacked Neural Networks | http://arxiv.org/abs/1605.08512 | id:1605.08512 author:Milad Mohammadi, Subhasis Das category:cs.LG cs.CV cs.NE  published:2016-05-27 summary:It has been proven that transfer learning provides an easy way to achieve state-of-the-art accuracies on several vision tasks by training a simple classifier on top of features obtained from pre-trained neural networks. The goal of this work is to generate better features for transfer learning from multiple publicly available pre-trained neural networks. To this end, we propose a novel architecture called Stacked Neural Networks which leverages the fast training time of transfer learning while simultaneously being much more accurate. We show that using a stacked NN architecture can result in up to 8% improvements in accuracy over state-of-the-art techniques using only one pre-trained network for transfer learning. A second aim of this work is to make network fine- tuning retain the generalizability of the base network to unseen tasks. To this end, we propose a new technique called "joint fine-tuning" that is able to give accuracies comparable to finetuning the same network individually over two datasets. We also show that a jointly finetuned network generalizes better to unseen tasks when compared to a network finetuned over a single task. version:1
arxiv-1511-05678 | Expressiveness of Rectifier Networks | http://arxiv.org/abs/1511.05678 | id:1511.05678 author:Xingyuan Pan, Vivek Srikumar category:cs.LG  published:2015-11-18 summary:Rectified Linear Units (ReLUs) have been shown to ameliorate the vanishing gradient problem, allow for efficient backpropagation, and empirically promote sparsity in the learned parameters. They have led to state-of-the-art results in a variety of applications. However, unlike threshold and sigmoid networks, ReLU networks are less explored from the perspective of their expressiveness. This paper studies the expressiveness of ReLU networks. We characterize the decision boundary of two-layer ReLU networks by constructing functionally equivalent threshold networks. We show that while the decision boundary of a two-layer ReLU network can be captured by a threshold network, the latter may require an exponentially larger number of hidden units. We also formulate sufficient conditions for a corresponding logarithmic reduction in the number of hidden units to represent a sign network as a ReLU network. Finally, we experimentally compare threshold networks and their much smaller ReLU counterparts with respect to their ability to learn from synthetically generated data. version:3
arxiv-1602-06042 | Structured Sparse Regression via Greedy Hard-Thresholding | http://arxiv.org/abs/1602.06042 | id:1602.06042 author:Prateek Jain, Nikhil Rao, Inderjit Dhillon category:stat.ML cs.LG  published:2016-02-19 summary:Several learning applications require solving high-dimensional regression problems where the relevant features belong to a small number of (overlapping) groups. For very large datasets and under standard sparsity constraints, hard thresholding methods have proven to be extremely efficient, but such methods require NP hard projections when dealing with overlapping groups. In this paper, we show that such NP-hard projections can not only be avoided by appealing to submodular optimization, but such methods come with strong theoretical guarantees even in the presence of poorly conditioned data (i.e. say when two features have correlation $\geq 0.99$), which existing analyses cannot handle. These methods exhibit an interesting computation-accuracy trade-off and can be extended to significantly harder problems such as sparse overlapping groups. Experiments on both real and synthetic data validate our claims and demonstrate that the proposed methods are orders of magnitude faster than other greedy and convex relaxation techniques for learning with group-structured sparsity. version:2
arxiv-1506-02396 | ARock: an Algorithmic Framework for Asynchronous Parallel Coordinate Updates | http://arxiv.org/abs/1506.02396 | id:1506.02396 author:Zhimin Peng, Yangyang Xu, Ming Yan, Wotao Yin category:math.OC cs.DC stat.ML  published:2015-06-08 summary:Finding a fixed point to a nonexpansive operator, i.e., $x^*=Tx^*$, abstracts many problems in numerical linear algebra, optimization, and other areas of scientific computing. To solve fixed-point problems, we propose ARock, an algorithmic framework in which multiple agents (machines, processors, or cores) update $x$ in an asynchronous parallel fashion. Asynchrony is crucial to parallel computing since it reduces synchronization wait, relaxes communication bottleneck, and thus speeds up computing significantly. At each step of ARock, an agent updates a randomly selected coordinate $x_i$ based on possibly out-of-date information on $x$. The agents share $x$ through either global memory or communication. If writing $x_i$ is atomic, the agents can read and write $x$ without memory locks. Theoretically, we show that if the nonexpansive operator $T$ has a fixed point, then with probability one, ARock generates a sequence that converges to a fixed points of $T$. Our conditions on $T$ and step sizes are weaker than comparable work. Linear convergence is also obtained. We propose special cases of ARock for linear systems, convex optimization, machine learning, as well as distributed and decentralized consensus problems. Numerical experiments of solving sparse logistic regression problems are presented. version:5
arxiv-1605-08501 | Local Region Sparse Learning for Image-on-Scalar Regression | http://arxiv.org/abs/1605.08501 | id:1605.08501 author:Yao Chen, Xiao Wang, Linglong Kong, Hongtu Zhu category:stat.ML cs.LG  published:2016-05-27 summary:Identification of regions of interest (ROI) associated with certain disease has a great impact on public health. Imposing sparsity of pixel values and extracting active regions simultaneously greatly complicate the image analysis. We address these challenges by introducing a novel region-selection penalty in the framework of image-on-scalar regression. Our penalty combines the Smoothly Clipped Absolute Deviation (SCAD) regularization, enforcing sparsity, and the SCAD of total variation (TV) regularization, enforcing spatial contiguity, into one group, which segments contiguous spatial regions against zero-valued background. Efficient algorithm is based on the alternative direction method of multipliers (ADMM) which decomposes the non-convex problem into two iterative optimization problems with explicit solutions. Another virtue of the proposed method is that a divide and conquer learning algorithm is developed, thereby allowing scaling to large images. Several examples are presented and the experimental results are compared with other state-of-the-art approaches. version:1
arxiv-1602-05352 | Recommendations as Treatments: Debiasing Learning and Evaluation | http://arxiv.org/abs/1602.05352 | id:1602.05352 author:Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, Thorsten Joachims category:cs.LG cs.AI cs.IR  published:2016-02-17 summary:Most data for evaluating and training recommender systems is subject to selection biases, either through self-selection by the users or through the actions of the recommendation system itself. In this paper, we provide a principled approach to handling selection biases, adapting models and estimation techniques from causal inference. The approach leads to unbiased performance estimators despite biased data, and to a matrix factorization method that provides substantially improved prediction performance on real-world data. We theoretically and empirically characterize the robustness of the approach, finding that it is highly practical and scalable. version:2
arxiv-1605-08497 | Universum Learning for SVM Regression | http://arxiv.org/abs/1605.08497 | id:1605.08497 author:Sauptik Dhar, Vladimir Cherkassky category:cs.LG  published:2016-05-27 summary:This paper extends the idea of Universum learning [18, 19] to regression problems. We propose new Universum-SVM formulation for regression problems that incorporates a priori knowledge in the form of additional data samples. These additional data samples or Universum belong to the same application domain as the training samples, but they follow a different distribution. Several empirical comparisons are presented to illustrate the utility of the proposed approach. version:1
arxiv-1604-02027 | Combinatorial Topic Models using Small-Variance Asymptotics | http://arxiv.org/abs/1604.02027 | id:1604.02027 author:Ke Jiang, Suvrit Sra, Brian Kulis category:cs.LG cs.CL stat.ML  published:2016-04-07 summary:Topic models have emerged as fundamental tools in unsupervised machine learning. Most modern topic modeling algorithms take a probabilistic view and derive inference algorithms based on Latent Dirichlet Allocation (LDA) or its variants. In contrast, we study topic modeling as a combinatorial optimization problem, and propose a new objective function derived from LDA by passing to the small-variance limit. We minimize the derived objective by using ideas from combinatorial optimization, which results in a new, fast, and high-quality topic modeling algorithm. In particular, we show that our results are competitive with popular LDA-based topic modeling approaches, and also discuss the (dis)similarities between our approach and its probabilistic counterparts. version:2
arxiv-1603-05691 | Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)? | http://arxiv.org/abs/1603.05691 | id:1603.05691 author:Gregor Urban, Krzysztof J. Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, Matt Richardson category:stat.ML cs.LG  published:2016-03-17 summary:Yes, apparently they do. Previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic. In this paper we train models with a varying number of convolutional layers to mimic a state-of-the-art CIFAR-10 model. We are unable to train models without multiple layers of convolution to mimic deep convolutional models: the student models do not have to be as deep as the teacher model they mimic, but the students need multiple convolutional layers to learn functions of comparable accuracy as the teacher. Even when trained via distillation, deep convolutional nets need to be deep and convolutional. version:2
arxiv-1605-08491 | Provable Algorithms for Inference in Topic Models | http://arxiv.org/abs/1605.08491 | id:1605.08491 author:Sanjeev Arora, Rong Ge, Frederic Koehler, Tengyu Ma, Ankur Moitra category:cs.LG stat.ML  published:2016-05-27 summary:Recently, there has been considerable progress on designing algorithms with provable guarantees -- typically using linear algebraic methods -- for parameter learning in latent variable models. But designing provable algorithms for inference has proven to be more challenging. Here we take a first step towards provable inference in topic models. We leverage a property of topic models that enables us to construct simple linear estimators for the unknown topic proportions that have small variance, and consequently can work with short documents. Our estimators also correspond to finding an estimate around which the posterior is well-concentrated. We show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics. Finally, we give empirical results that demonstrate that our algorithm works on realistic topic models. It yields good solutions on synthetic data and runs in time comparable to a {\em single} iteration of Gibbs sampling. version:1
arxiv-1509-07892 | Evasion and Hardening of Tree Ensemble Classifiers | http://arxiv.org/abs/1509.07892 | id:1509.07892 author:Alex Kantchelian, J. D. Tygar, Anthony D. Joseph category:cs.LG cs.CR stat.ML  published:2015-09-25 summary:Classifier evasion consists in finding for a given instance $x$ the nearest instance $x'$ such that the classifier predictions of $x$ and $x'$ are different. We present two novel algorithms for systematically computing evasions for tree ensembles such as boosted trees and random forests. Our first algorithm uses a Mixed Integer Linear Program solver and finds the optimal evading instance under an expressive set of constraints. Our second algorithm trades off optimality for speed by using symbolic prediction, a novel algorithm for fast finite differences on tree ensembles. On a digit recognition task, we demonstrate that both gradient boosted trees and random forests are extremely susceptible to evasions. Finally, we harden a boosted tree model without loss of predictive accuracy by augmenting the training set of each boosting round with evading instances, a technique we call adversarial boosting. version:2
arxiv-1605-07221 | Global Optimality of Local Search for Low Rank Matrix Recovery | http://arxiv.org/abs/1605.07221 | id:1605.07221 author:Srinadh Bhojanapalli, Behnam Neyshabur, Nathan Srebro category:stat.ML cs.LG math.OC  published:2016-05-23 summary:We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements. With noisy measurements we show all local minima are very close to a global optimum. Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent {\em from random initialization}. version:2
arxiv-1605-08481 | Open Problem: Best Arm Identification: Almost Instance-Wise Optimality and the Gap Entropy Conjecture | http://arxiv.org/abs/1605.08481 | id:1605.08481 author:Lijie Chen, Jian Li category:cs.LG  published:2016-05-27 summary:The best arm identification problem (BEST-1-ARM) is the most basic pure exploration problem in stochastic multi-armed bandits. The problem has a long history and attracted significant attention for the last decade. However, we do not yet have a complete understanding of the optimal sample complexity of the problem: The state-of-the-art algorithms achieve a sample complexity of $O(\sum_{i=2}^{n} \Delta_{i}^{-2}(\ln\delta^{-1} + \ln\ln\Delta_i^{-1}))$ ($\Delta_{i}$ is the difference between the largest mean and the $i^{th}$ mean), while the best known lower bound is $\Omega(\sum_{i=2}^{n} \Delta_{i}^{-2}\ln\delta^{-1})$ for general instances and $\Omega(\Delta^{-2} \ln\ln \Delta^{-1})$ for the two-arm instances. We propose to study the instance-wise optimality for the BEST-1-ARM problem. Previous work has proved that it is impossible to have an instance optimal algorithm for the 2-arm problem. However, we conjecture that modulo the additive term $\Omega(\Delta_2^{-2} \ln\ln \Delta_2^{-1})$ (which is an upper bound and worst case lower bound for the 2-arm problem), there is an instance optimal algorithm for BEST-1-ARM. Moreover, we introduce a new quantity, called the gap entropy for a best-arm problem instance, and conjecture that it is the instance-wise lower bound. Hence, resolving this conjecture would provide a final answer to the old and basic problem. version:1
arxiv-1504-00624 | Structure Learning of Partitioned Markov Networks | http://arxiv.org/abs/1504.00624 | id:1504.00624 author:Song Liu, Taiji Suzuki, Masashi Sugiyama, Kenji Fukumizu category:stat.ML  published:2015-04-02 summary:We learn the structure of a Markov Network between two groups of random variables from joint observations. Since modelling and learning the full MN structure may be hard, learning the links between two groups directly may be a preferable option. We introduce a novel concept called the \emph{partitioned ratio} whose factorization directly associates with the Markovian properties of random variables across two groups. A simple one-shot convex optimization procedure is proposed for learning the \emph{sparse} factorizations of the partitioned ratio and it is theoretically guaranteed to recover the correct inter-group structure under mild conditions. The performance of the proposed method is experimentally compared with the state of the art MN structure learning methods using ROC curves. Real applications on analyzing bipartisanship in US congress and pairwise DNA/time-series alignments are also reported. version:5
arxiv-1603-08861 | Revisiting Semi-Supervised Learning with Graph Embeddings | http://arxiv.org/abs/1603.08861 | id:1603.08861 author:Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov category:cs.LG  published:2016-03-29 summary:We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models. version:2
arxiv-1605-08478 | Model-Free Imitation Learning with Policy Optimization | http://arxiv.org/abs/1605.08478 | id:1605.08478 author:Jonathan Ho, Jayesh K. Gupta, Stefano Ermon category:cs.LG cs.AI  published:2016-05-26 summary:In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima. version:1
arxiv-1605-08470 | A Feature based Approach for Video Compression | http://arxiv.org/abs/1605.08470 | id:1605.08470 author:Rajer Sindhu category:cs.CV cs.MM  published:2016-05-26 summary:It is a high cost problem for panoramic image stitching via image matching algorithm and not practical for real-time performance. In this paper, we take full advantage ofHarris corner invariant characterization method light intensity parallel meaning, translation and rotation, and made a realtime panoramic image stitching algorithm. According to the basic characteristics and performance FPGA classical algorithm, several modules such as the feature point extraction, and matching description is to optimize the feature-based logic. Real-time optimization system to achieve high precision match. The new algorithm process the image from pixel domain and obtained from CCD camera Xilinx Spartan-6 hardware platform. After the image stitching algorithm, will eventually form a portable interface to output high-definition content on the display. The results showed that, the proposed algorithm has higher precision with good real-time performance and robustness. version:1
arxiv-1605-08464 | Low-Cost Scene Modeling using a Density Function Improves Segmentation Performance | http://arxiv.org/abs/1605.08464 | id:1605.08464 author:Vivek Sharma, Sule Yildirim-Yayilgan, Luc Van Gool category:cs.CV cs.AI cs.HC cs.RO  published:2016-05-26 summary:We propose a low cost and effective way to combine a free simulation software and free CAD models for modeling human-object interaction in order to improve human & object segmentation. It is intended for research scenarios related to safe human-robot collaboration (SHRC) and interaction (SHRI) in the industrial domain. The task of human and object modeling has been used for detecting activity, and for inferring and predicting actions, different from those works, we do human and object modeling in order to learn interactions in RGB-D data for improving segmentation. For this purpose, we define a novel density function to model a three dimensional (3D) scene in a virtual environment (VREP). This density function takes into account various possible configurations of human-object and object-object relationships and interactions governed by their affordances. Using this function, we synthesize a large, realistic and highly varied synthetic RGB-D dataset that we use for training. We train a random forest classifier, and the pixelwise predictions obtained is integrated as a unary term in a pairwise conditional random fields (CRF). Our evaluation shows that modeling these interactions improves segmentation performance by ~7\% in mean average precision and recall over state-of-the-art methods that ignore these interactions in real-world data. Our approach is computationally efficient, robust and can run real-time on consumer hardware. version:1
arxiv-1605-08455 | Suppressing Background Radiation Using Poisson Principal Component Analysis | http://arxiv.org/abs/1605.08455 | id:1605.08455 author:P. Tandon, P. Huggins, A. Dubrawski, S. Labov, K. Nelson category:cs.LG physics.data-an stat.ML  published:2016-05-26 summary:Performance of nuclear threat detection systems based on gamma-ray spectrometry often strongly depends on the ability to identify the part of measured signal that can be attributed to background radiation. We have successfully applied a method based on Principal Component Analysis (PCA) to obtain a compact null-space model of background spectra using PCA projection residuals to derive a source detection score. We have shown the method's utility in a threat detection system using mobile spectrometers in urban scenes (Tandon et al 2012). While it is commonly assumed that measured photon counts follow a Poisson process, standard PCA makes a Gaussian assumption about the data distribution, which may be a poor approximation when photon counts are low. This paper studies whether and in what conditions PCA with a Poisson-based loss function (Poisson PCA) can outperform standard Gaussian PCA in modeling background radiation to enable more sensitive and specific nuclear threat detection. version:1
arxiv-1605-08454 | Linear dynamical neural population models through nonlinear embeddings | http://arxiv.org/abs/1605.08454 | id:1605.08454 author:Yuanjun Gao, Evan Archer, Liam Paninski, John P. Cunningham category:q-bio.NC stat.ML  published:2016-05-26 summary:A body of recent work in modeling neural activity focuses on recovering low-dimensional latent features that capture the statistical structure of large-scale neural populations. Most such approaches have focused on linear generative models, where inference is computationally tractable. Here, we propose fLDS, a general class of nonlinear generative models that permits the firing rate of each neuron to vary as an arbitrary smooth function of a latent, linear dynamical state. This extra flexibility allows the model to capture a richer set of neural variability than a purely linear model, but retains an easily visualizable low-dimensional latent space. To fit this class of non-conjugate models we propose a variational inference scheme, along with a novel approximate posterior capable of capturing rich temporal correlations across time. We show that our techniques permit inference in a wide class of generative models.We also show in application to two neural datasets that, compared to state-of-the-art neural population models, fLDS captures a much larger proportion of neural variability with a small number of latent dimensions, providing superior predictive performance and interpretability. version:1
arxiv-1503-05140 | ProtVec: A Continuous Distributed Representation of Biological Sequences | http://arxiv.org/abs/1503.05140 | id:1503.05140 author:Ehsaneddin Asgari, Mohammad R. K. Mofrad category:q-bio.QM cs.AI cs.LG q-bio.GN  published:2015-03-17 summary:We introduce a new representation and feature extraction method for biological sequences. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. In the present paper, we focus on protein-vectors that can be utilized in a wide array of bioinformatics investigations such as family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction. In this method, we adopt artificial neural network approaches and represent a protein sequence with a single dense n-dimensional vector. To evaluate this method, we apply it in classification of 324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein families, where an average family classification accuracy of 93%+-0.06% is obtained, outperforming existing family classification methods. In addition, we use ProtVec representation to predict disordered proteins from structured proteins. Two databases of disordered sequences are used: the DisProt database as well as a database featuring the disordered regions of nucleoporins rich with phenylalanine-glycine repeats (FG-Nups). Using support vector machine classifiers, FG-Nup sequences are distinguished from structured protein sequences found in Protein Data Bank (PDB) with a 99.8% accuracy, and unstructured DisProt sequences are differentiated from structured DisProt sequences with 100.0% accuracy. These results indicate that by only providing sequence data for various proteins into this model, accurate information about protein structure can be determined. version:2
arxiv-1505-07818 | Domain-Adversarial Training of Neural Networks | http://arxiv.org/abs/1505.07818 | id:1505.07818 author:Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, Victor Lempitsky category:stat.ML cs.LG cs.NE  published:2015-05-28 summary:We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application. version:4
arxiv-1605-08412 | CITlab ARGUS for historical handwritten documents | http://arxiv.org/abs/1605.08412 | id:1605.08412 author:Gundram Leifert, Tobias Strauß, Tobias Grüning, Roger Labahn category:cs.CV cs.AI cs.NE 68T10  68T05  published:2016-05-26 summary:We describe CITlab's recognition system for the HTRtS competition attached to the 13. International Conference on Document Analysis and Recognition, ICDAR 2015. The task comprises the recognition of historical handwritten documents. The core algorithms of our system are based on multi-dimensional recurrent neural networks (MDRNN) and connectionist temporal classification (CTC). The software modules behind that as well as the basic utility technologies are essentially powered by PLANET's ARGUS framework for intelligent text recognition and image processing. version:1
arxiv-1605-08401 | Dense Volume-to-Volume Vascular Boundary Detection | http://arxiv.org/abs/1605.08401 | id:1605.08401 author:Jameson Merkow, David Kriegman, Alison Marsden, Zhuowen Tu category:cs.CV  published:2016-05-26 summary:In this work, we present a novel 3D-Convolutional Neural Network (CNN) architecture called I2I-3D that predicts boundary location in volumetric data. Our fine-to-fine, deeply supervised framework addresses three critical issues to 3D boundary detection: (1) efficient, holistic, end-to-end volumetric label training and prediction (2) precise voxel-level prediction to capture fine scale structures prevalent in medical data and (3) directed multi-scale, multi-level feature learning. We evaluate our approach on a dataset consisting of 93 medical image volumes with a wide variety of anatomical regions and vascular structures. In the process, we also introduce HED-3D, a 3D extension of the state-of-the-art 2D edge detector (HED). We show that our deep learning approach out-performs, the current state-of-the-art in 3D vascular boundary detection (structured forests 3D), by a large margin, as well as HED applied to slices, and HED-3D while successfully localizing fine structures. With our approach, boundary detection takes about one minute on a typical 512x512x512 volume. version:1
arxiv-1605-08400 | Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of Linear Smoothers | http://arxiv.org/abs/1605.08400 | id:1605.08400 author:Veeranjaneyulu Sadhanala, Yu-Xiang Wang, Ryan Tibshirani category:math.ST stat.ML stat.TH  published:2016-05-26 summary:We consider the problem of estimating a function defined over $n$ locations on a $d$-dimensional grid (having all side lengths equal to $n^{1/d}$). When the function is constrained to have discrete total variation bounded by $C_n$, we derive the minimax optimal (squared) $\ell_2$ estimation error rate, parametrized by $n$ and $C_n$. Total variation denoising, also known as the fused lasso, is seen to be rate optimal. Several simpler estimators exist, such as Laplacian smoothing and Laplacian eigenmaps. A natural question is: can these simpler estimators perform just as well? We prove that these estimators, and more broadly all estimators given by linear transformations of the input data, are suboptimal over the class of functions with bounded variation. This extends fundamental findings of Donoho and Johnstone [1998] on 1-dimensional total variation spaces to higher dimensions. The implication is that the computationally simpler methods cannot be used for such sophisticated denoising tasks, without sacrificing statistical accuracy. We also derive minimax rates for discrete Sobolev spaces over $d$-dimensional grids, which are, in some sense, smaller than the total variation function spaces. Indeed, these are small enough spaces that linear estimators can be optimal---and a few well-known ones are, such as Laplacian smoothing and Laplacian eigenmaps, as we show. Lastly, we investigate the problem of adaptivity of the total variation denoiser to these smaller Sobolev function spaces. version:1
arxiv-1605-08397 | Domain Transfer Multi-Instance Dictionary Learning | http://arxiv.org/abs/1605.08397 | id:1605.08397 author:Ke Wang, Jiayong Liu, Daniel González category:cs.CV  published:2016-05-26 summary:In this paper, we invest the domain transfer learning problem with multi-instance data. We assume we already have a well-trained multi-instance dictionary and its corresponding classifier from the source domain, which can be used to represent and classify the bags. But it cannot be directly used to the target domain. Thus we propose to adapt them to the target domain by adding an adaptive term to the source domain classifier. The adaptive function is a linear function based a domain transfer multi-instance dictionary. Given a target domain bag, we first map it to a bag-level feature space using the domain transfer dictionary, and then apply a the linear adaptive function to its bag-level feature vector. To learn the domain-transfer dictionary and the adaptive function parameter, we simultaneously minimize the average classification error of the target domain classifier over the target domain training set, and the complexities of both the adaptive function parameter and the domain transfer dictionary. The minimization problem is solved by an iterative algorithm which update the dictionary and the function parameter alternately. Experiments over several benchmark data sets show the advantage of the proposed method over existing state-of-the-art domain transfer multi-instance learning methods. version:1
arxiv-1605-08396 | Robust Downbeat Tracking Using an Ensemble of Convolutional Networks | http://arxiv.org/abs/1605.08396 | id:1605.08396 author:S. Durand, J. P. Bello, B. David, G. Richard category:cs.SD cs.NE  published:2016-05-26 summary:In this paper, we present a novel state of the art system for automatic downbeat tracking from music signals. The audio signal is first segmented in frames which are synchronized at the tatum level of the music. We then extract different kind of features based on harmony, melody, rhythm and bass content to feed convolutional neural networks that are adapted to take advantage of each feature characteristics. This ensemble of neural networks is combined to obtain one downbeat likelihood per tatum. The downbeat sequence is finally decoded with a flexible and efficient temporal model which takes advantage of the metrical continuity of a song. We then perform an evaluation of our system on a large base of 9 datasets, compare its performance to 4 other published algorithms and obtain a significant increase of 16.8 percent points compared to the second best system, for altogether a moderate cost in test and training. The influence of each step of the method is studied to show its strengths and shortcomings. version:1
arxiv-1605-08375 | Generalization Properties and Implicit Regularization for Multiple Passes SGM | http://arxiv.org/abs/1605.08375 | id:1605.08375 author:Junhong Lin, Raffaello Camoriano, Lorenzo Rosasco category:cs.LG stat.ML  published:2016-05-26 summary:We study the generalization properties of stochastic gradient methods for learning with convex loss functions and linearly parameterized functions. We show that, in the absence of penalizations or constraints, the stability and approximation properties of the algorithm can be controlled by tuning either the step-size or the number of passes over the data. In this view, these parameters can be seen to control a form of implicit regularization. Numerical results complement the theoretical findings. version:1
arxiv-1605-08374 | Kronecker Determinantal Point Processes | http://arxiv.org/abs/1605.08374 | id:1605.08374 author:Zelda Mariet, Suvrit Sra category:cs.LG cs.AI stat.ML  published:2016-05-26 summary:Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of $N$ items. They have recently gained prominence in several applications that rely on "diverse" subsets. However, their applicability to large problems is still limited due to the $\mathcal O(N^3)$ complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KronDPP. version:1
arxiv-1605-08370 | Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent | http://arxiv.org/abs/1605.08370 | id:1605.08370 author:Chi Jin, Sham M. Kakade, Praneeth Netrapalli category:cs.LG math.OC stat.ML  published:2016-05-26 summary:Matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications. Most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. However, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing. While existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting. In this paper, we propose the first provable, efficient online algorithm for matrix completion. Our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (SGD). After every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime. Our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms. Our proofs introduce a general framework to show that SGD updates tend to stay away from saddle surfaces and could be of broader interests for other non-convex problems to prove tight rates. version:1
arxiv-1605-08359 | Pairwise Decomposition of Image Sequences for Active Multi-View Recognition | http://arxiv.org/abs/1605.08359 | id:1605.08359 author:Edward Johns, Stefan Leutenegger, Andrew J. Davison category:cs.CV cs.RO  published:2016-05-26 summary:A multi-view image sequence provides a much richer capacity for object recognition than from a single image. However, most existing solutions to multi-view recognition typically adopt hand-crafted, model-based geometric methods, which do not readily embrace recent trends in deep learning. We propose to bring Convolutional Neural Networks to generic multi-view recognition, by decomposing an image sequence into a set of image pairs, classifying each pair independently, and then learning an object classifier by weighting the contribution of each pair. This allows for recognition over arbitrary camera trajectories, without requiring explicit training over the potentially infinite number of camera paths and lengths. Building these pairwise relationships then naturally extends to the next-best-view problem in an active recognition framework. To achieve this, we train a second Convolutional Neural Network to map directly from an observed image to next viewpoint. Finally, we incorporate this into a trajectory optimisation task, whereby the best recognition confidence is sought for a given trajectory length. We present state-of-the-art results in both guided and unguided multi-view recognition on the ModelNet dataset, and show how our method can be used with depth images, greyscale images, or both. version:1
arxiv-1512-00809 | Optimal whitening and decorrelation | http://arxiv.org/abs/1512.00809 | id:1512.00809 author:Agnan Kessy, Alex Lewin, Korbinian Strimmer category:stat.ME stat.ML  published:2015-12-02 summary:Whitening, or sphering, is a common preprocessing step in statistical analysis to transform random variables to orthogonality. However, due to rotational freedom there are infinitely many possible whitening procedures. Consequently, there is a diverse range of sphering methods in use, for example based on principal component analysis (PCA), Cholesky matrix decomposition and zero-phase component analysis (ZCA), among others. Here we provide an overview of the underlying theory and discuss five natural whitening procedures. Subsequently, we demonstrate that investigating the cross-covariance and the cross-correlation matrix between sphered and original variables allows to break the rotational invariance and to identify optimal whitening transformations. As a result we recommend two particular approaches: ZCA-cor whitening to produce sphered variables that are maximally similar to the original variables, and PCA-cor whitening to obtain sphered variables that maximally compress the original variables. version:2
arxiv-1605-07596 | Local Minimax Complexity of Stochastic Convex Optimization | http://arxiv.org/abs/1605.07596 | id:1605.07596 author:Yuancheng Zhu, Sabyasachi Chatterjee, John Duchi, John Lafferty category:stat.ML  published:2016-05-24 summary:We extend the traditional worst-case, minimax analysis of stochastic convex optimization by introducing a localized form of minimax complexity for individual functions. Our main result gives function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize either the function or its "hardest local alternative" to a given numerical precision. The bounds are expressed in terms of a localized and computational analogue of the modulus of continuity that is central to statistical minimax analysis. We show how the computational modulus of continuity can be explicitly calculated in concrete cases, and relates to the curvature of the function at the optimum. We also prove a superefficiency result that demonstrates it is a meaningful benchmark, acting as a computational analogue of the Fisher information in statistical estimation. The nature and practical implications of the results are demonstrated in simulations. version:3
arxiv-1605-08350 | Benign-Malignant Lung Nodule Classification with Geometric and Appearance Histogram Features | http://arxiv.org/abs/1605.08350 | id:1605.08350 author:Tizita Nesibu Shewaye, Alhayat Ali Mekonnen category:cs.CV  published:2016-05-26 summary:Lung cancer accounts for the highest number of cancer deaths globally. Early diagnosis of lung nodules is very important to reduce the mortality rate of patients by improving the diagnosis and treatment of lung cancer. This work proposes an automated system to classify lung nodules as malignant and benign in CT images. It presents extensive experimental results using a combination of geometric and histogram lung nodule image features and different linear and non-linear discriminant classifiers. The proposed approach is experimentally validated on the LIDC-IDRI public lung cancer screening thoracic computed tomography (CT) dataset containing nodule level diagnostic data. The obtained results are very encouraging correctly classifying 82% of malignant and 93% of benign nodules on unseen test data at best. version:1
arxiv-1605-08346 | Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks | http://arxiv.org/abs/1605.08346 | id:1605.08346 author:Adam Charles, Dong Yin, Christopher Rozell category:cs.IT math.IT stat.ML  published:2016-05-26 summary:Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low-dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the ambient input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs. version:1
arxiv-1605-07588 | A Consistent Regularization Approach for Structured Prediction | http://arxiv.org/abs/1605.07588 | id:1605.07588 author:Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco category:cs.LG stat.ML  published:2016-05-24 summary:We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space. We exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques. We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed methods. Experimental results are provided to demonstrate the practical usefulness of the proposed approach. version:2
arxiv-1605-07969 | Adaptive Neural Compilation | http://arxiv.org/abs/1605.07969 | id:1605.07969 author:Rudy Bunel, Alban Desmaison, Pushmeet Kohli, Philip H. S. Torr, M. Pawan Kumar category:cs.AI cs.LG  published:2016-05-25 summary:This paper proposes an adaptive neural-compilation framework to address the problem of efficient program learning. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target distribution of inputs. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate. version:2
arxiv-1511-03722 | Doubly Robust Off-policy Value Evaluation for Reinforcement Learning | http://arxiv.org/abs/1511.03722 | id:1511.03722 author:Nan Jiang, Lihong Li category:cs.LG cs.AI cs.SY stat.ME stat.ML  published:2015-11-11 summary:We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem is often a critical step when applying RL in real-world problems. Despite its importance, existing general methods either have uncontrolled bias or suffer high variance. In this work, we extend the doubly robust estimator for bandits to sequential decision-making problems, which gets the best of both worlds: it is guaranteed to be unbiased and can have a much lower variance than the popular importance sampling estimators. We demonstrate the estimator's accuracy in several benchmark problems, and illustrate its use as a subroutine in safe policy improvement. We also provide theoretical results on the hardness of the problem, and show that our estimator can match the lower bound in certain scenarios. version:3
arxiv-1603-01450 | Sequential ranking under random semi-bandit feedback | http://arxiv.org/abs/1603.01450 | id:1603.01450 author:Hossein Vahabi, Paul Lagrée, Claire Vernade, Olivier Cappé category:cs.DS cs.LG  published:2016-03-04 summary:In many web applications, a recommendation is not a single item suggested to a user but a list of possibly interesting contents that may be ranked in some contexts. The combinatorial bandit problem has been studied quite extensively these last two years and many theoretical results now exist : lower bounds on the regret or asymptotically optimal algorithms. However, because of the variety of situations that can be considered, results are designed to solve the problem for a specific reward structure such as the Cascade Model. The present work focuses on the problem of ranking items when the user is allowed to click on several items while scanning the list from top to bottom. version:2
arxiv-1602-02373 | Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings | http://arxiv.org/abs/1602.02373 | id:1602.02373 author:Rie Johnson, Tong Zhang category:stat.ML cs.CL cs.LG  published:2016-02-07 summary:One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson & Zhang, 2015). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of `text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets. version:2
arxiv-1605-08325 | Theano-MPI: a Theano-based Distributed Training Framework | http://arxiv.org/abs/1605.08325 | id:1605.08325 author:He Ma, Fei Mao, Graham W. Taylor category:cs.LG cs.DC  published:2016-05-26 summary:We develop a scalable and extendable training framework that can utilize GPUs across nodes in a cluster and accelerate the training of deep learning models based on data parallelism. Both synchronous and asynchronous training are implemented in our framework, where parameter exchange among GPUs is based on CUDA-aware MPI. In this report, we analyze the convergence and capability of the framework to reduce training time when scaling the synchronous training of AlexNet and GoogLeNet from 2 GPUs to 8 GPUs. In addition, we explore novel ways to reduce the communication overhead caused by exchanging parameters. Finally, we release the framework as open-source for further research on distributed deep learning version:1
arxiv-1605-08323 | Aerial image geolocalization from recognition and matching of roads and intersections | http://arxiv.org/abs/1605.08323 | id:1605.08323 author:Dragos Costea, Marius Leordeanu category:cs.CV  published:2016-05-26 summary:Aerial image analysis at a semantic level is important in many applications with strong potential impact in industry and consumer use, such as automated mapping, urban planning, real estate and environment monitoring, or disaster relief. The problem is enjoying a great interest in computer vision and remote sensing, due to increased computer power and improvement in automated image understanding algorithms. In this paper we address the task of automatic geolocalization of aerial images from recognition and matching of roads and intersections. Our proposed method is a novel contribution in the literature that could enable many applications of aerial image analysis when GPS data is not available. We offer a complete pipeline for geolocalization, from the detection of roads and intersections, to the identification of the enclosing geographic region by matching detected intersections to previously learned manually labeled ones, followed by accurate geometric alignment between the detected roads and the manually labeled maps. We test on a novel dataset with aerial images of two European cities and use the publicly available OpenStreetMap project for collecting ground truth roads annotations. We show in extensive experiments that our approach produces highly accurate localizations in the challenging case when we train on images from one city and test on the other and the quality of the aerial images is relatively poor. We also show that the the alignment between detected roads and pre-stored manual annotations can be effectively used for improving the quality of the road detection results. version:1
arxiv-1605-08313 | A Light-powered, Always-On, Smart Camera with Compressed Domain Gesture Detection | http://arxiv.org/abs/1605.08313 | id:1605.08313 author:Anvesha A, Shaojie Xu, Ningyuan Cao, Justin Romberg, Arijit Raychowdhury category:cs.CV  published:2016-05-26 summary:In this paper we propose an energy-e?cient camera-based gesture recognition system powered by light energy for \al- ways on" applications. Low energy consumption is achieved by directly extracting gesture features from the compressed measurements, which are the block averages and the linear combinations of the image sensor's pixel values. The ges- tures are recognized using a nearest-neighbour (NN) classi- ?er followed by Dynamic Time Warping (DTW). The sys- tem has been implemented on an Analog Devices Black Fin ULP vision processor and powered by PV cells whose output is regulated by TI's DC-DC buck converter with Maximum Power Point Tracking (MPPT). Measured data reveals that with only 400 compressed measurements (768? compression ratio) per frame, the system is able to recognize key wake- up gestures with greater than 80% accuracy and only 95mJ of energy per frame. Owing to its fully self-powered op- eration, the proposed system can ?nd wide applications in \always-on" vision systems such as in surveillance, robotics and consumer electronics with touch-less operation. version:1
arxiv-1605-08301 | Predictive Coarse-Graining | http://arxiv.org/abs/1605.08301 | id:1605.08301 author:Markus Schöberl, Phaedon-Stelios Koutsourelakis, Nicholas Zabaras category:stat.ML  published:2016-05-26 summary:We propose a data-driven, coarse-graining formulation in the context of equilibrium statistical mechanics. In contrast to existing techniques which are based on a fine-to-coarse map, we adopt the opposite strategy by prescribing a {\em probabilistic coarse-to-fine} map. This corresponds to a directed probabilistic model where the coarse variables play the role of latent generators of the fine scale (all-atom) data. From an information-theoretic perspective, the framework proposed provides an improvement upon the relative entropy method that quantifies the uncertainty due to the information loss that unavoidably takes place during the CG process. Furthermore, it can be readily extended to a fully Bayesian model where various sources of uncertainties are reflected in the parameters' posterior. The latter can be used to produce not only point estimates of fine-scale reconstructions or macroscopic observables, but more importantly, predictive posterior distributions on these quantities. These quantify the confidence of the model as a function of the amount of data and the level of coarse-graining. The issues of model complexity and model selection are seamlessly addressed by employing a hierarchical prior that favors the discovery of sparse solutions, revealing the most prominent features in the coarse-grained model. A flexible and parallelizable, Monte Carlo - Expectation-Maximization (MC-EM) scheme is proposed for carrying out inference and learning tasks. A comparative assessment of the proposed methodology is presented for a lattice spin system and the SPC/E water model. version:1
arxiv-1605-08299 | High-Dimensional Trimmed Estimators: A General Framework for Robust Structured Estimation | http://arxiv.org/abs/1605.08299 | id:1605.08299 author:Eunho Yang, Aurelie Lozano, Aleksandr Aravkin category:stat.ML  published:2016-05-26 summary:We consider the problem of robustifying high-dimensional structured estimation. Robust techniques are key in real-world applications, as these often involve outliers and data corruption. We focus on trimmed versions of structurally regularized M-estimators in the high-dimensional setting, including the popular Least Trimmed Squared estimator, as well as analogous estimators for generalized linear models, graphical models, using possibly non-convex loss functions. We present a general analysis of their statistical convergence rates and consistency, and show how to extend any algorithms for M-estimators to fit trimmed variants. We then take a closer look at the $\ell_1$-regularized Least Trimmed Squared estimator as a special case. Our results show that this estimator can tolerate a larger fraction of corrupted observations than state-of-the-art alternatives. The competitive performance of high-dimensional trimmed estimators is illustrated numerically using both simulated and real-world genomics data. version:1
arxiv-1402-4844 | Subspace Learning with Partial Information | http://arxiv.org/abs/1402.4844 | id:1402.4844 author:Alon Gonen, Dan Rosenbaum, Yonina Eldar, Shai Shalev-Shwartz category:cs.LG stat.ML  published:2014-02-19 summary:The goal of subspace learning is to find a $k$-dimensional subspace of $\mathbb{R}^d$, such that the expected squared distance between instance vectors and the subspace is as small as possible. In this paper we study subspace learning in a partial information setting, in which the learner can only observe $r \le d$ attributes from each instance vector. We propose several efficient algorithms for this task, and analyze their sample complexity version:2
arxiv-1605-08285 | Solving Random Systems of Quadratic Equations via Truncated Generalized Gradient Flow | http://arxiv.org/abs/1605.08285 | id:1605.08285 author:Gang Wang, Georgios B. Giannakis category:stat.ML cs.IT math.IT math.OC  published:2016-05-26 summary:This paper puts forth a novel algorithm, termed \emph{truncated generalized gradient flow} (TGGF), to solve for $\bm{x}\in\mathbb{R}^n/\mathbb{C}^n$ a system of $m$ quadratic equations $y_i= \langle\bm{a}_i,\bm{x}\rangle ^2$, $i=1,2,\ldots,m$, which even for $\left\{\bm{a}_i\in\mathbb{R}^n/\mathbb{C}^n\right\}_{i=1}^m$ random is known to be \emph{NP-hard} in general. We prove that as soon as the number of equations $m$ is on the order of the number of unknowns $n$, TGGF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data $\left\{\left(\bm{a}_i;\,y_i\right)\right\}_{i=1}^m$. Specifically, TGGF proceeds in two stages: s1) A novel \emph{orthogonality-promoting} initialization that is obtained with simple power iterations; and, s2) a refinement of the initial estimate by successive updates of scalable \emph{truncated generalized gradient iterations}. The former is in sharp contrast to the existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth \emph{amplitude-based} cost function. Numerical tests demonstrate that: i) The novel orthogonality-promoting initialization method returns more accurate and robust estimates relative to its spectral counterparts; and ii) even with the same initialization, our refinement/truncation outperforms Wirtinger-based alternatives, all corroborating the superior performance of TGGF over state-of-the-art algorithms. version:1
arxiv-1605-08283 | Discrete Deep Feature Extraction: A Theory and New Architectures | http://arxiv.org/abs/1605.08283 | id:1605.08283 author:Thomas Wiatowski, Michael Tschannen, Aleksandar Stanić, Philipp Grohs, Helmut Bölcskei category:cs.LG cs.CV cs.IT cs.NE math.IT stat.ML  published:2016-05-26 summary:First steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made---for the continuous-time case---in Mallat, 2012, and Wiatowski and B\"olcskei, 2015. This paper considers the discrete case, introduces new convolutional neural network architectures, and proposes a mathematical framework for their analysis. Specifically, we establish deformation and translation sensitivity results of local and global nature, and we investigate how certain structural properties of the input signal are reflected in the corresponding feature vectors. Our theory applies to general filters and general Lipschitz-continuous non-linearities and pooling operators. Experiments on handwritten digit classification and facial landmark detection---including feature importance evaluation---complement the theoretical findings. version:1
arxiv-1602-05531 | On the Use of Deep Learning for Blind Image Quality Assessment | http://arxiv.org/abs/1602.05531 | id:1602.05531 author:Simone Bianco, Luigi Celona, Paolo Napoletano, Raimondo Schettini category:cs.CV  published:2016-02-17 summary:In this work we investigate the use of deep learning for distortion-generic blind image quality assessment. We report on different design choices, ranging from the use of features extracted from pre-trained Convolutional Neural Networks (CNNs) as a generic image description, to the use of features extracted from a CNN fine-tuned for the image quality task. Our best proposal, named DeepBIQ, estimates the image quality by average pooling the scores predicted on multiple sub-regions of the original image. The score of each sub-region is computed using a Support Vector Regression (SVR) machine taking as input features extracted using a CNN fine-tuned for category-based image quality assessment. Experimental results on the LIVE In the Wild Image Quality Challenge Database and on the LIVE Image Quality Assessment Database show that DeepBIQ outperforms the state-of-the-art methods compared, having a Linear Correlation Coefficient (LCC) with human subjective scores of almost 0.91 and 0.98 respectively. Furthermore, in most of the cases, the quality score predictions of DeepBIQ are closer to the average observer than those of a generic human observer. version:3
arxiv-1605-07367 | Riemannian stochastic variance reduced gradient on Grassmann manifold | http://arxiv.org/abs/1605.07367 | id:1605.07367 author:Hiroyuki Kasai, Hiroyuki Sato, Bamdev Mishra category:cs.LG cs.NA math.OC stat.ML  published:2016-05-24 summary:Stochastic variance reduction algorithms have recently become popular for minimizing the average of a large, but finite, number of loss functions. In this paper, we propose a novel Riemannian extension of the Euclidean stochastic variance reduced gradient algorithm (R-SVRG) to a compact manifold search space. To this end, we show the developments on the Grassmann manifold. The key challenges of averaging, addition, and subtraction of multiple gradients are addressed with notions like logarithm mapping and parallel translation of vectors on the Grassmann manifold. We present a global convergence analysis of the proposed algorithm with decay step-sizes and a local convergence rate analysis under fixed step-size with some natural assumptions. The proposed algorithm is applied on a number of problems on the Grassmann manifold like principal components analysis, low-rank matrix completion, and the Karcher mean computation. In all these cases, the proposed algorithm outperforms the standard Riemannian stochastic gradient descent algorithm. version:2
arxiv-1605-08257 | Low-rank tensor completion: a Riemannian manifold preconditioning approach | http://arxiv.org/abs/1605.08257 | id:1605.08257 author:Hiroyuki Kasai, Bamdev Mishra category:cs.LG cs.NA math.OC stat.ML  published:2016-05-26 summary:We propose a novel Riemannian manifold preconditioning approach for the tensor completion problem with rank constraint. A novel Riemannian metric or inner product is proposed that exploits the least-squares structure of the cost function and takes into account the structured symmetry that exists in Tucker decomposition. The specific metric allows to use the versatile framework of Riemannian optimization on quotient manifolds to develop preconditioned nonlinear conjugate gradient and stochastic gradient descent algorithms for batch and online setups, respectively. Concrete matrix representations of various optimization-related ingredients are listed. Numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets. version:1
arxiv-1605-08254 | Margin Preservation of Deep Neural Networks | http://arxiv.org/abs/1605.08254 | id:1605.08254 author:Jure Sokolic, Raja Giryes, Guillermo Sapiro, Miguel R. D. Rodrigues category:stat.ML cs.LG cs.NE  published:2016-05-26 summary:The generalization error of deep neural networks via their classification margin is studied in this work, providing novel generalization error bounds that are independent of the network depth, thereby avoiding the common exponential depth-dependency which is unrealistic for current networks with hundreds of layers. We show that a large margin linear classifier operating at the output of a deep neural network induces a large classification margin at the input of the network, provided that the network preserves distances in directions normal to the decision boundary. The distance preservation is characterized by the average behaviour of the network's Jacobian matrix in the neighbourhood of the training samples. The introduced theory also leads to a margin preservation regularization scheme that outperforms weight decay both theoretically and empirically. version:1
arxiv-1605-08247 | cvpaper.challenge in 2015 - A review of CVPR2015 and DeepSurvey | http://arxiv.org/abs/1605.08247 | id:1605.08247 author:Hirokatsu Kataoka, Yudai Miyashita, Tomoaki Yamabe, Soma Shirakabe, Shin'ichi Sato, Hironori Hoshino, Ryo Kato, Kaori Abe, Takaaki Imanari, Naomichi Kobayashi, Shinichiro Morita, Akio Nakamura category:cs.CV cs.LG cs.MM cs.RO  published:2016-05-26 summary:The "cvpaper.challenge" is a group composed of members from AIST, Tokyo Denki Univ. (TDU), and Univ. of Tsukuba that aims to systematically summarize papers on computer vision, pattern recognition, and related fields. For this particular review, we focused on reading the ALL 602 conference papers presented at the CVPR2015, the premier annual computer vision event held in June 2015, in order to grasp the trends in the field. Further, we are proposing "DeepSurvey" as a mechanism embodying the entire process from the reading through all the papers, the generation of ideas, and to the writing of paper. version:1
arxiv-1511-06247 | Predicting online user behaviour using deep learning algorithms | http://arxiv.org/abs/1511.06247 | id:1511.06247 author:Armando Vieira category:cs.LG stat.ML  published:2015-11-19 summary:We propose a robust classifier to predict buying intentions based on user behaviour within a large e-commerce website. In this work we compare traditional machine learning techniques with the most advanced deep learning approaches. We show that both Deep Belief Networks and Stacked Denoising auto-Encoders achieved a substantial improvement by extracting features from high dimensional data during the pre-train phase. They prove also to be more convenient to deal with severe class imbalance. version:3
arxiv-1605-08242 | Neighborhood Sensitive Mapping for Zero-Shot Classification using Independently Learned Semantic Embeddings | http://arxiv.org/abs/1605.08242 | id:1605.08242 author:Gaurav Singh, Fabrizio Silvestri, John Shawe-Taylor category:cs.LG  published:2016-05-26 summary:In a traditional setting, classifiers are trained to approximate a target function $f:X \rightarrow Y$ where at least a sample for each $y \in Y$ is presented to the training algorithm. In a zero-shot setting we have a subset of the labels $\hat{Y} \subset Y$ for which we do not observe any corresponding training instance. Still, the function $f$ that we train must be able to correctly assign labels also on $\hat{Y}$. In practice, zero-shot problems are very important especially when the label set is large and the cost of editorially label samples for all possible values in the label set might be prohibitively high. Most recent approaches to zero-shot learning are based on finding and exploiting relationships between labels using semantic embeddings. We show in this paper that semantic embeddings, despite being very good at capturing relationships between labels, are not very good at capturing the relationships among labels in a data-dependent manner. For this reason, we propose a novel two-step process for learning a zero-shot classifier. In the first step, we learn what we call a \emph{property embedding space} capturing the "\emph{learnable}" features of the label set. Then, we exploit the learned properties in order to reduce the generalization error for a linear nearest neighbor-based classifier. version:1
arxiv-1602-02660 | Exploiting Cyclic Symmetry in Convolutional Neural Networks | http://arxiv.org/abs/1602.02660 | id:1602.02660 author:Sander Dieleman, Jeffrey De Fauw, Koray Kavukcuoglu category:cs.LG cs.CV cs.NE  published:2016-02-08 summary:Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models. version:2
arxiv-1604-03853 | Hierarchical Compound Poisson Factorization | http://arxiv.org/abs/1604.03853 | id:1604.03853 author:Mehmet E. Basbug, Barbara E. Engelhardt category:cs.LG cs.AI stat.ML  published:2016-04-13 summary:Non-negative matrix factorization models based on a hierarchical Gamma-Poisson structure capture user and item behavior effectively in extremely sparse data sets, making them the ideal choice for collaborative filtering applications. Hierarchical Poisson factorization (HPF) in particular has proved successful for scalable recommendation systems with extreme sparsity. HPF, however, suffers from a tight coupling of sparsity model (absence of a rating) and response model (the value of the rating), which limits the expressiveness of the latter. Here, we introduce hierarchical compound Poisson factorization (HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to high-dimensional extremely sparse matrices. More importantly, HCPF decouples the sparsity model from the response model, allowing us to choose the most suitable distribution for the response. HCPF can capture binary, non-negative discrete, non-negative continuous, and zero-inflated continuous responses. We compare HCPF with HPF on nine discrete and three continuous data sets and conclude that HCPF captures the relationship between sparsity and response better than HPF. version:2
arxiv-1603-00375 | Easy-First Dependency Parsing with Hierarchical Tree LSTMs | http://arxiv.org/abs/1603.00375 | id:1603.00375 author:Eliyahu Kiperwasser, Yoav Goldberg category:cs.CL  published:2016-03-01 summary:We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving state-of-the-art accuracies for English and Chinese, without relying on external word embeddings. The parser's implementation is available for download at the first author's webpage. version:2
arxiv-1603-04351 | Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations | http://arxiv.org/abs/1603.04351 | id:1603.04351 author:Eliyahu Kiperwasser, Yoav Goldberg category:cs.CL  published:2016-03-14 summary:We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese. version:2
arxiv-1605-08201 | Towards optimal nonlinearities for sparse recovery using higher-order statistics | http://arxiv.org/abs/1605.08201 | id:1605.08201 author:Steffen Limmer, Sławomir Stańczak category:cs.IT math.IT stat.ML  published:2016-05-26 summary:We consider machine learning techniques to develop low-latency approximate solutions to a class of inverse problems. More precisely, we use a probabilistic approach for the problem of recovering sparse stochastic signals that are members of the $\ell_p$-balls. In this context, we analyze the Bayesian mean-square-error (MSE) for two types of estimators: (i) a linear estimator and (ii) a structured estimator composed of a linear operator followed by a Cartesian product of univariate nonlinear mappings. By construction, the complexity of the proposed nonlinear estimator is comparable to that of its linear counterpart since the nonlinear mapping can be implemented efficiently in hardware by means of look-up tables (LUTs). The proposed structure lends itself to neural networks and iterative shrinkage/thresholding-type algorithms restricted to a single iterate (e.g. due to imposed hardware or latency constraints). By resorting to an alternating minimization technique, we obtain a sequence of optimized linear operators and nonlinear mappings that convergence in the objective. The resulting solution is attractive for real-time applications where general iterative and convex optimization methods are infeasible. version:1
arxiv-1605-08188 | Learning Multivariate Log-concave Distributions | http://arxiv.org/abs/1605.08188 | id:1605.08188 author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.LG cs.IT math.IT math.ST stat.TH  published:2016-05-26 summary:We study the problem of estimating multivariate log-concave probability density functions. We prove the first sample complexity upper bound for learning log-concave densities on $\mathbb{R}^d$, for all $d \geq 1$. Prior to our work, no upper bound on the sample complexity of this learning problem was known for the case of $d>3$. In more detail, we give an estimator that, for any $d \ge 1$ and $\epsilon>0$, draws $\tilde{O}_d \left( (1/\epsilon)^{(d+5)/2} \right)$ samples from an unknown target log-concave density on $\mathbb{R}^d$, and outputs a hypothesis that (with high probability) is $\epsilon$-close to the target, in total variation distance. Our upper bound on the sample complexity comes close to the known lower bound of $\Omega_d \left( (1/\epsilon)^{(d+1)/2} \right)$ for this problem. version:1
arxiv-1604-07666 | $\ell_p$-Box ADMM: A Versatile Framework for Integer Programming | http://arxiv.org/abs/1604.07666 | id:1604.07666 author:Baoyuan Wu, Bernard Ghanem category:cs.CV cs.DS  published:2016-04-26 summary:This paper revisits the integer programming (IP) problem, which plays a fundamental role in many computer vision and machine learning applications. The literature abounds with many seminal works that address this problem, some focusing on continuous approaches (e.g. linear program relaxation) while others on discrete ones (e.g., min-cut). However, a limited number of them are designed to handle the general IP form and even these methods cannot adequately satisfy the simultaneous requirements of accuracy, feasibility, and scalability. To this end, we propose a novel and versatile framework called $\ell_p$-box ADMM, which is based on two parts. (1) The discrete constraint is equivalently replaced by the intersection of a box and a $(n-1)$-dimensional sphere (defined through the $\ell_p$ norm). (2) We infuse this equivalence into the ADMM (Alternating Direction Method of Multipliers) framework to handle these continuous constraints separately and to harness its attractive properties. More importantly, the ADMM update steps can lead to manageable sub-problems in the continuous domain. To demonstrate its efficacy, we consider an instance of the framework, namely $\ell_2$-box ADMM applied to binary quadratic programming (BQP). Here, the ADMM steps are simple, computationally efficient, and theoretically guaranteed to converge to a KKT point. We demonstrate the applicability of $\ell_2$-box ADMM on three important applications: MRF energy minimization, graph matching, and clustering. Results clearly show that it significantly outperforms existing generic IP solvers both in runtime and objective. It also achieves very competitive performance vs. state-of-the-art methods specific to these applications. version:2
arxiv-1602-02350 | Solving Ridge Regression using Sketched Preconditioned SVRG | http://arxiv.org/abs/1602.02350 | id:1602.02350 author:Alon Gonen, Francesco Orabona, Shai Shalev-Shwartz category:cs.LG  published:2016-02-07 summary:We develop a novel preconditioning method for ridge regression, based on recent linear sketching methods. By equipping Stochastic Variance Reduced Gradient (SVRG) with this preconditioning process, we obtain a significant speed-up relative to fast stochastic methods such as SVRG, SDCA and SAG. version:2
arxiv-1605-08179 | Discovering Causal Signals in Images | http://arxiv.org/abs/1605.08179 | id:1605.08179 author:David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Schölkopf, Léon Bottou category:stat.ML cs.CV  published:2016-05-26 summary:The purpose of this paper is to point out and assay observable causal signals within collections of static images. We achieve this goal in two steps. First, we take a learning approach to observational causal inference, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, when given samples from their joint distribution. Second, we use our causal direction finder to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of (1) a relation between the direction of causality and the difference between objects and their contexts, and (2) observable causal signals in collections of static images. version:1
arxiv-1605-08174 | Adiabatic Persistent Contrastive Divergence Learning | http://arxiv.org/abs/1605.08174 | id:1605.08174 author:Hyeryung Jang, Hyungwon Choi, Yung Yi, Jinwoo Shin category:cs.LG stat.ML  published:2016-05-26 summary:This paper studies the problem of parameter learning in probabilistic graphical models having latent variables, where the standard approach is the expectation maximization algorithm alternating expectation (E) and maximization (M) steps. However, both E and M steps are computationally intractable for high dimensional data, while the substitution of one step to a faster surrogate for combating against intractability can often cause failure in convergence. We propose a new learning algorithm which is computationally efficient and provably ensures convergence to a correct optimum. Its key idea is to run only a few cycles of Markov Chains (MC) in both E and M steps. Such an idea of running incomplete MC has been well studied only for M step in the literature, called Contrastive Divergence (CD) learning. While such known CD-based schemes find approximated gradients of the log-likelihood via the mean-field approach in E step, our proposed algorithm does exact ones via MC algorithms in both steps due to the multi-time-scale stochastic approximation theory. Despite its theoretical guarantee in convergence, the proposed scheme might suffer from the slow mixing of MC in E step. To tackle it, we also propose a hybrid approach applying both mean-field and MC approximation in E step, where the hybrid approach outperforms the bare mean-field CD scheme in our experiments on real-world datasets. version:1
arxiv-1605-08165 | Highly-Smooth Zero-th Order Online Optimization Vianney Perchet | http://arxiv.org/abs/1605.08165 | id:1605.08165 author:Francis Bach, Vianney Perchet category:cs.LG math.OC  published:2016-05-26 summary:The minimization of convex functions which are only available through partial and noisy information is a key methodological problem in many disciplines. In this paper we consider convex optimization with noisy zero-th order information, that is noisy function evaluations at any desired point. We focus on problems with high degrees of smoothness, such as logistic regression. We show that as opposed to gradient-based algorithms, high-order smoothness may be used to improve estimation rates, with a precise dependence of our upper-bounds on the degree of smoothness. In particular, we show that for infinitely differentiable functions, we recover the same dependence on sample size as gradient-based algorithms, with an extra dimension-dependent factor. This is done for both convex and strongly-convex functions, with finite horizon and anytime algorithms. Finally, we also recover similar results in the online optimization setting. version:1
arxiv-1605-08163 | Multiple target tracking based on sets of trajectories | http://arxiv.org/abs/1605.08163 | id:1605.08163 author:Ángel F. García-Fernández, Lennart Svensson, Mark R. Morelande category:cs.CV cs.SY  published:2016-05-26 summary:This paper proposes the set of target trajectories as the state variable for multiple target tracking. The main objective of multiple target tracking is to estimate an unknown number of target trajectories given a sequence of measurements. This quantity of interest is perfectly represented as a set of trajectories without the need of arbitrary parameters such as labels or ordering. We use finite-set statistics to pose this problem in the Bayesian framework and formulate a state space model where the random state is a random finite set that contains trajectories. All information of interest is thus contained in the multitrajectory filtering probability density function (PDF), which represents the multitrajectory PDF of the set of trajectories given the measurements. For the standard measurement and dynamic models, we describe a family of PDFs that is conjugate in the sense that the multitrajectory filtering PDF remains within that family during both the prediction and update steps. version:1
arxiv-1605-00937 | Dictionary Learning for Massive Matrix Factorization | http://arxiv.org/abs/1605.00937 | id:1605.00937 author:Arthur Mensch, Julien Mairal, Bertrand Thirion, Gaël Varoquaux category:stat.ML cs.LG q-bio.QM  published:2016-05-03 summary:Sparse matrix factorization is a popular tool to obtain interpretable data decompositions, which are also effective to perform data completion or denoising. Its applicability to large datasets has been addressed with online and randomized methods, that reduce the complexity in one of the matrix dimension, but not in both of them. In this paper, we tackle very large matrices in both dimensions. We propose a new factoriza-tion method that scales gracefully to terabyte-scale datasets, that could not be processed by previous algorithms in a reasonable amount of time. We demonstrate the efficiency of our approach on massive functional Magnetic Resonance Imaging (fMRI) data, and on matrix completion problems for recommender systems, where we obtain significant speed-ups compared to state-of-the art coordinate descent methods. version:2
arxiv-1605-08154 | A single scale retinex based method for palm vein extraction | http://arxiv.org/abs/1605.08154 | id:1605.08154 author:Chongyang Wang, Ming Peng, Lingfeng Xu, Tong Chen category:cs.CV  published:2016-05-26 summary:Palm vein recognition is a novel biometric identification technology. But how to gain a better vein extraction result from the raw palm image is still a challenging problem, especially when the raw data collection has the problem of asymmetric illumination. This paper proposes a method based on single scale Retinex algorithm to extract palm vein image when strong shadow presents due to asymmetric illumination and uneven geometry of the palm. We test our method on a multispectral palm image. The experimental result shows that the proposed method is robust to the influence of illumination angle and shadow. Compared to the traditional extraction methods, the proposed method can obtain palm vein lines with better visualization performance (the contrast ratio increases by 18.4%, entropy increases by 1.07%, and definition increases by 18.8%). version:1
arxiv-1605-08153 | DeepMovie: Using Optical Flow and Deep Neural Networks to Stylize Movies | http://arxiv.org/abs/1605.08153 | id:1605.08153 author:Alexander G. Anderson, Cory P. Berg, Daniel P. Mossing, Bruno A. Olshausen category:cs.CV cs.NE  published:2016-05-26 summary:A recent paper by Gatys et al. describes a method for rendering an image in the style of another image. First, they use convolutional neural network features to build a statistical model for the style of an image. Then they create a new image with the content of one image but the style statistics of another image. Here, we extend this method to render a movie in a given artistic style. The naive solution that independently renders each frame produces poor results because the features of the style move substantially from one frame to the next. The other naive method that initializes the optimization for the next frame using the rendered version of the previous frame also produces poor results because the features of the texture stay fixed relative to the frame of the movie instead of moving with objects in the scene. The main contribution of this paper is to use optical flow to initialize the style transfer optimization so that the texture features move with the objects in the video. Finally, we suggest a method to incorporate optical flow explicitly into the cost function. version:1
arxiv-1605-08151 | Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning | http://arxiv.org/abs/1605.08151 | id:1605.08151 author:Soravit Changpinyo, Wei-Lun Chao, Fei Sha category:cs.CV  published:2016-05-26 summary:Leveraging class semantic descriptions and examples of known objects, zero-shot learning makes it possible to train a recognition model for an object class whose examples are not available. In this paper, we propose a novel zero-shot learning model that takes advantage of clustering structures in the semantic embedding space. The key idea is to impose the structural constraint that semantic representations must be predictive of the locations of its corresponding visual exemplars. To this end, this reduces to training multiple kernel-based regressors from semantic representation-exemplar pairs from labeled data of the seen object categories. Despite its simplicity, our approach significantly outperforms existing zero-shot learning methods in three out of four benchmark datasets, including the ImageNet dataset with more than 20,000 unseen categories. version:1
arxiv-1503-08356 | Online Low-Rank Subspace Clustering by Basis Dictionary Pursuit | http://arxiv.org/abs/1503.08356 | id:1503.08356 author:Jie Shen, Ping Li, Huan Xu category:stat.ML  published:2015-03-28 summary:Low-Rank Representation~(LRR) has been a significant method for segmenting data that are generated from a union of subspaces. It is also known that solving LRR is challenging in terms of time complexity and memory footprint, in that the size of the nuclear norm regularized matrix is $n$-by-$n$ (where $n$ is the number of samples). In this paper, we thereby develop a novel online implementation of LRR that reduces the memory cost from $O(n^2)$ to $O(pd)$, with $p$ being the ambient dimension and $d$ being some estimated rank~($d < p \ll n$). We also establish the theoretical guarantee that the sequence of solutions produced by our algorithm converges to a stationary point of the expected loss function asymptotically. Extensive experiments on synthetic and realistic datasets further substantiate that our algorithm is fast, robust and memory efficient. version:3
arxiv-1601-07460 | Information-theoretic limits of learning the structure of Bayesian networks | http://arxiv.org/abs/1601.07460 | id:1601.07460 author:Asish Ghoshal, Jean Honorio category:cs.LG cs.IT math.IT stat.ML  published:2016-01-27 summary:In this paper, we study the information-theoretic limits of learning the structure of Bayesian networks, on discrete as well as continuous random variables, from a finite amount of data. We show that under certain parameterizations of the Bayesian network --- the minimum number of samples required to learn the "true" network grows as $\mathcal{O}(M)$ and $\mathcal{O}(S\log M)$ for non-sparse and sparse Bayesian networks respectively --- where $M$ is the number of variables in the network and $S$ is the maximum number of parents of any node in the network. We study various commonly used Bayesian networks, such as Conditional Probability Table (CPT) based networks, Noisy-OR networks, Logistic regression (LR) networks, and Gaussian networks. We identify various important parameters of the conditional distributions that affect the complexity of learning such models, like the maximum inverse probability for CPT networks, the failure probability for Noisy-OR networks, the $\ell_2$ norm of weight vectors for LR networks and the signal and noise parameters for Gaussian networks. We also show that an existing procedure calle SparsityBoost, by Brenner and Sontag, for learning binary CPT networks is information-theoretically optimal in the number of variables. version:2
arxiv-1605-08140 | Temporal attention filters for human activity recognition in videos | http://arxiv.org/abs/1605.08140 | id:1605.08140 author:AJ Piergiovanni, Chenyou Fan, Michael S. Ryoo category:cs.CV  published:2016-05-26 summary:In this paper, we newly introduce the concept of temporal attention filters, and describe how they can be used for human activity recognition from videos. Many high-level activities are often composed of multiple temporal parts (e.g., sub-events) with different duration/speed, and our objective is to make the model explicitly consider such temporal structure using multiple temporal filters. Our attention filters are designed to be fully differentiable, allowing end-of-end training of the temporal filters together with the underlying frame-based or segment-based convolutional neural network architectures. The paper not only presents an approach of learning optimal static temporal attention filters to be shared across different videos, but also describes an approach of dynamically adjusting attention filters per testing video using recurrent long short-term memory networks (LSTMs). We experimentally confirm that the proposed concept of temporal attention filters benefits the activity recognition tasks by capturing the temporal structure in videos. version:1
arxiv-1605-05969 | Randomized Primal-Dual Proximal Block Coordinate Updates | http://arxiv.org/abs/1605.05969 | id:1605.05969 author:Xiang Gao, Yangyang Xu, Shuzhong Zhang category:math.OC math.NA stat.ML 90C25  95C06  68W20  published:2016-05-19 summary:In this paper we propose a randomized primal-dual proximal block coordinate updating framework for a general multi-block convex optimization model with coupled objective function and linear constraints. Assuming mere convexity, we establish its $O(1/t)$ convergence rate in terms of the objective value and feasibility measure. The framework includes several existing algorithms as special cases such as a primal-dual method for bilinear saddle-point problems (PD-S), the proximal Jacobian ADMM (Prox-JADMM) and a randomized variant of the ADMM method for multi-block convex optimization. Our analysis recovers and/or strengthens the convergence properties of several existing algorithms. For example, for PD-S our result leads to the same order of convergence rate without the previously assumed boundedness condition on the constraint sets, and for Prox-JADMM the new result provides convergence rate in terms of the objective value and the feasibility violation. It is well known that the original ADMM may fail to converge when the number of blocks exceeds two. Our result shows that if an appropriate randomization procedure is invoked to select the updating blocks, then a sublinear rate of convergence in expectation can be guaranteed for multi-block ADMM, without assuming any strong convexity. The new approach is also extended to solve problems where only a stochastic approximation of the (sub-)gradient of the objective is available, and we establish an $O(1/\sqrt{t})$ convergence rate of the extended approach for solving stochastic programming. version:2
arxiv-1605-08754 | Faster Eigenvector Computation via Shift-and-Invert Preconditioning | http://arxiv.org/abs/1605.08754 | id:1605.08754 author:Dan Garber, Elad Hazan, Chi Jin, Sham M. Kakade, Cameron Musco, Praneeth Netrapalli, Aaron Sidford category:cs.DS cs.LG math.NA math.OC  published:2016-05-26 summary:We give faster algorithms and improved sample complexities for estimating the top eigenvector of a matrix $\Sigma$ -- i.e. computing a unit vector $x$ such that $x^T \Sigma x \ge (1-\epsilon)\lambda_1(\Sigma)$: Offline Eigenvector Estimation: Given an explicit $A \in \mathbb{R}^{n \times d}$ with $\Sigma = A^TA$, we show how to compute an $\epsilon$ approximate top eigenvector in time $\tilde O([nnz(A) + \frac{d*sr(A)}{gap^2} ]* \log 1/\epsilon )$ and $\tilde O([\frac{nnz(A)^{3/4} (d*sr(A))^{1/4}}{\sqrt{gap}} ] * \log 1/\epsilon )$. Here $nnz(A)$ is the number of nonzeros in $A$, $sr(A)$ is the stable rank, $gap$ is the relative eigengap. By separating the $gap$ dependence from the $nnz(A)$ term, our first runtime improves upon the classical power and Lanczos methods. It also improves prior work using fast subspace embeddings [AC09, CW13] and stochastic optimization [Sha15c], giving significantly better dependencies on $sr(A)$ and $\epsilon$. Our second running time improves these further when $nnz(A) \le \frac{d*sr(A)}{gap^2}$. Online Eigenvector Estimation: Given a distribution $D$ with covariance matrix $\Sigma$ and a vector $x_0$ which is an $O(gap)$ approximate top eigenvector for $\Sigma$, we show how to refine to an $\epsilon$ approximation using $ O(\frac{var(D)}{gap*\epsilon})$ samples from $D$. Here $var(D)$ is a natural notion of variance. Combining our algorithm with previous work to initialize $x_0$, we obtain improved sample complexity and runtime results under a variety of assumptions on $D$. We achieve our results using a general framework that we believe is of independent interest. We give a robust analysis of the classic method of shift-and-invert preconditioning to reduce eigenvector computation to approximately solving a sequence of linear systems. We then apply fast stochastic variance reduced gradient (SVRG) based system solvers to achieve our claims. version:1
arxiv-1512-07349 | Incremental Method for Spectral Clustering of Increasing Orders | http://arxiv.org/abs/1512.07349 | id:1512.07349 author:Pin-Yu Chen, Baichuan Zhang, Mohammad Al Hasan, Alfred O. Hero category:cs.SI cs.NA stat.ML  published:2015-12-23 summary:The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs) of a graph Laplacian matrix have been widely used for spectral clustering and community detection. However, in real-life applications the number of clusters or communities (say, $K$) is generally unknown a-priori. Consequently, the majority of the existing methods either choose $K$ heuristically or they repeat the clustering method with different choices of $K$ and accept the best clustering result. The first option, more often, yields suboptimal result, while the second option is computationally expensive. In this work, we propose an incremental method for constructing the eigenspectrum of the graph Laplacian matrix. This method leverages the eigenstructure of graph Laplacian matrix to obtain the $K$-th eigenpairs of the Laplacian matrix given a collection of all the $K-1$ smallest eigenpairs. Our proposed method adapts the Laplacian matrix such that the batch eigenvalue decomposition problem transforms into an efficient sequential leading eigenpair computation problem. As a practical application, we consider user-guided spectral clustering. Specifically, we demonstrate that users can utilize the proposed incremental method for effective eigenpair computation and determining the desired number of clusters based on multiple clustering metrics. version:3
arxiv-1604-08806 | 3D Interest Point Detection Based on Geometric Measures and Sparse Refinement | http://arxiv.org/abs/1604.08806 | id:1604.08806 author:Xinyu Lin, Ce Zhu, Qian Zhang, Yipeng Liu category:cs.CV  published:2016-04-29 summary:Three dimensional (3D) interest point detection plays a fundamental role in computer vision. In this paper, we introduce a new method for detecting 3D interest points of 3D mesh models based on geometric measures and sparse refinement (GMSR). The key point of our approach is to calculate the 3D saliency measure using two novel geometric measures, which are defined in multi-scale space to effectively distinguish 3D interest points from edges and flat areas. Those points with local maxima of 3D saliency measure are selected as the candidates of 3D interest points. Finally, we utilize an $l_0$ norm based optimization method to refine the candidates of 3D interest points by constraining the number of 3D interest points. Numerical experiments show that the proposed GMSR based 3D interest point detector outperforms current six state-of-the-art methods for different kinds of 3D mesh models. version:2
arxiv-1605-08125 | Automatic Action Annotation in Weakly Labeled Videos | http://arxiv.org/abs/1605.08125 | id:1605.08125 author:Waqas Sultani, Mubarak Shah category:cs.CV  published:2016-05-26 summary:Manual spatio-temporal annotation of human action in videos is laborious, requires several annotators and contains human biases. In this paper, we present a weakly supervised approach to automatically obtain spatio-temporal annotations of an actor in action videos. We first obtain a large number of action proposals in each video. To capture a few most representative action proposals in each video and evade processing thousands of them, we rank them using optical flow and saliency in a 3D-MRF based framework and select a few proposals using MAP based proposal subset selection method. We demonstrate that this ranking preserves the high quality action proposals. Several such proposals are generated for each video of the same action. Our next challenge is to iteratively select one proposal from each video so that all proposals are globally consistent. We formulate this as Generalized Maximum Clique Graph problem using shape, global and fine grained similarity of proposals across the videos. The output of our method is the most action representative proposals from each video. Our method can also annotate multiple instances of the same action in a video. We have validated our approach on three challenging action datasets: UCF Sport, sub-JHMDB and THUMOS'13 and have obtained promising results compared to several baseline methods. Moreover, on UCF Sports, we demonstrate that action classifiers trained on these automatically obtained spatio-temporal annotations have comparable performance to the classifiers trained on ground truth annotation. version:1
arxiv-1605-08110 | Video Summarization with Long Short-term Memory | http://arxiv.org/abs/1605.08110 | id:1605.08110 author:Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman category:cs.CV cs.LG  published:2016-05-26 summary:We propose a novel supervised learning technique for summarizing videos by automatically selecting keyframes or key subshots. Casting the problem as a structured prediction problem on sequential data, our main idea is to use Long Short-Term Memory (LSTM), a special type of recurrent neural networks to model the variable-range dependencies entailed in the task of video summarization. Our learning models attain the state-of-the-art results on two benchmark video datasets. Detailed analysis justifies the design of the models. In particular, we show that it is crucial to take into consideration the sequential structures in videos and model them. Besides advances in modeling techniques, we introduce techniques to address the need of a large number of annotated data for training complex learning models. There, our main idea is to exploit the existence of auxiliary annotated video datasets, albeit heterogeneous in visual styles and contents. Specifically, we show domain adaptation techniques can improve summarization by reducing the discrepancies in statistical properties across those datasets. version:1
arxiv-1605-07696 | Exact Exponent in Optimal Rates for Crowdsourcing | http://arxiv.org/abs/1605.07696 | id:1605.07696 author:Chao Gao, Yu Lu, Dengyong Zhou category:stat.ML math.ST stat.TH  published:2016-05-25 summary:In many machine learning applications, crowdsourcing has become the primary means for label collection. In this paper, we study the optimal error rate for aggregating labels provided by a set of non-expert workers. Under the classic Dawid-Skene model, we establish matching upper and lower bounds with an exact exponent $mI(\pi)$ in which $m$ is the number of workers and $I(\pi)$ the average Chernoff information that characterizes the workers' collective ability. Such an exact characterization of the error exponent allows us to state a precise sample size requirement $m>\frac{1}{I(\pi)}\log\frac{1}{\epsilon}$ in order to achieve an $\epsilon$ misclassification error. In addition, our results imply the optimality of various EM algorithms for crowdsourcing initialized by consistent estimators. version:2
arxiv-1605-08108 | FLAG: Fast Linearly-Coupled Adaptive Gradient Method | http://arxiv.org/abs/1605.08108 | id:1605.08108 author:Xiang Cheng, Farbod Roosta-Khorasani, Peter L. Bartlett, Michael W. Mahoney category:math.OC cs.LG stat.ML  published:2016-05-26 summary:The celebrated Nesterov's accelerated gradient method offers great speed-ups compared to the classical gradient descend method as it attains the optimal first-order oracle complexity for smooth convex optimization. On the other hand, the popular AdaGrad algorithm competes with mirror descent under the best regularizer by adaptively scaling the gradient. Recently, it has been shown that the accelerated gradient descent can be viewed as a linear combination of gradient descent and mirror descent steps. Here, we draw upon these ideas and present a fast linearly-coupled adaptive gradient method (FLAG) as an accelerated version of AdaGrad, and show that our algorithm can indeed offer the best of both worlds. Like Nesterov's accelerated algorithm and its proximal variant, FISTA, our method has a convergence rate of $1/T^2$ after $T$ iterations. Like AdaGrad our method adaptively chooses a regularizer, in a way that performs almost as well as the best choice of regularizer in hindsight. version:1
arxiv-1511-06464 | Unitary Evolution Recurrent Neural Networks | http://arxiv.org/abs/1511.06464 | id:1511.06464 author:Martin Arjovsky, Amar Shah, Yoshua Bengio category:cs.LG cs.NE  published:2015-11-20 summary:Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies. version:4
arxiv-1502-02613 | Projected Nesterov's Proximal-Gradient Algorithm for Sparse Signal Reconstruction with a Convex Constraint | http://arxiv.org/abs/1502.02613 | id:1502.02613 author:Renliang Gu, Aleksandar Dogandžić category:stat.CO stat.ML  published:2015-02-09 summary:We develop a projected Nesterov's proximal-gradient (PNPG) approach for sparse signal reconstruction that combines adaptive step size with Nesterov's momentum acceleration. The objective function that we wish to minimize is the sum of a convex differentiable data-fidelity (negative log-likelihood (NLL)) term and a convex regularization term. We apply sparse signal regularization where the signal belongs to a closed convex set within the closure of the domain of the NLL; the convex-set constraint facilitates flexible NLL domains and accurate signal recovery. Signal sparsity is imposed using the $\ell_1$-norm penalty on the signal's linear transform coefficients or gradient map, respectively. The PNPG approach employs projected Nesterov's acceleration step with restart and an inner iteration to compute the proximal mapping. We propose an adaptive step-size selection scheme to obtain a good local majorizing function of the NLL and reduce the time spent backtracking. Thanks to step-size adaptation, PNPG does not require Lipschitz continuity of the gradient of the NLL. We present an integrated derivation of the momentum acceleration and its $\mathcal{O}(k^{-2})$ convergence-rate and iterate convergence proofs, which account for adaptive step-size selection, inexactness of the iterative proximal mapping, and the convex-set constraint. The tuning of PNPG is largely application-independent. Tomographic and compressed-sensing reconstruction experiments with Poisson generalized linear and Gaussian linear measurement models demonstrate the performance of the proposed approach. version:5
arxiv-1603-01912 | Partition Functions from Rao-Blackwellized Tempered Sampling | http://arxiv.org/abs/1603.01912 | id:1603.01912 author:David Carlson, Patrick Stinson, Ari Pakman, Liam Paninski category:stat.ML  published:2016-03-07 summary:Partition functions of probability distributions are important quantities for model evaluation and comparisons. We present a new method to compute partition functions of complex and multimodal distributions. Such distributions are often sampled using simulated tempering, which augments the target space with an auxiliary inverse temperature variable. Our method exploits the multinomial probability law of the inverse temperatures, and provides estimates of the partition function in terms of a simple quotient of Rao-Blackwellized marginal inverse temperature probability estimates, which are updated while sampling. We show that the method has interesting connections with several alternative popular methods, and offers some significant advantages. In particular, we empirically find that the new method provides more accurate estimates than Annealed Importance Sampling when calculating partition functions of large Restricted Boltzmann Machines (RBM); moreover, the method is sufficiently accurate to track training and validation log-likelihoods during learning of RBMs, at minimal computational cost. version:3
arxiv-1605-08068 | Real-Time Human Motion Capture with Multiple Depth Cameras | http://arxiv.org/abs/1605.08068 | id:1605.08068 author:Alireza Shafaei, James J. Little category:cs.CV  published:2016-05-25 summary:Commonly used human motion capture systems require intrusive attachment of markers that are visually tracked with multiple cameras. In this work we present an efficient and inexpensive solution to markerless motion capture using only a few Kinect sensors. Unlike the previous work on 3d pose estimation using a single depth camera, we relax constraints on the camera location and do not assume a co-operative user. We apply recent image segmentation techniques to depth images and use curriculum learning to train our system on purely synthetic data. Our method accurately localizes body parts without requiring an explicit shape model. The body joint locations are then recovered by combining evidence from multiple views in real-time. We also introduce a dataset of ~6 million synthetic depth frames for pose estimation from multiple cameras and exceed state-of-the-art results on the Berkeley MHAD dataset. version:1
arxiv-1605-05579 | Low-Rank Matrices on Graphs: Generalized Recovery & Applications | http://arxiv.org/abs/1605.05579 | id:1605.05579 author:Nauman Shahid, Nathanael Perraudin, Pierre Vandergheynst category:cs.CV  published:2016-05-18 summary:Many real world datasets subsume a linear or non-linear low-rank structure in a very low-dimensional space. Unfortunately, one often has very little or no information about the geometry of the space, resulting in a highly under-determined recovery problem. Under certain circumstances, state-of-the-art algorithms provide an exact recovery for linear low-rank structures but at the expense of highly inscalable algorithms which use nuclear norm. However, the case of non-linear structures remains unresolved. We revisit the problem of low-rank recovery from a totally different perspective, involving graphs which encode pairwise similarity between the data samples and features. Surprisingly, our analysis confirms that it is possible to recover many approximate linear and non-linear low-rank structures with recovery guarantees with a set of highly scalable and efficient algorithms. We call such data matrices as \textit{Low-Rank matrices on graphs} and show that many real world datasets satisfy this assumption approximately due to underlying stationarity. Our detailed theoretical and experimental analysis unveils the power of the simple, yet very novel recovery framework \textit{Fast Robust PCA on Graphs} version:3
arxiv-1512-08562 | Taming the Noise in Reinforcement Learning via Soft Updates | http://arxiv.org/abs/1512.08562 | id:1512.08562 author:Roy Fox, Ari Pakman, Naftali Tishby category:cs.LG cs.IT math.IT  published:2015-12-28 summary:Model-free reinforcement learning algorithms, such as Q-learning, perform poorly in the early stages of learning in noisy environments, because much effort is spent unlearning biased estimates of the state-action value function. The bias results from selecting, among several noisy estimates, the apparent optimum, which may actually be suboptimal. We propose G-learning, a new off-policy learning algorithm that regularizes the value estimates by penalizing deterministic policies in the beginning of the learning process. We show that this method reduces the bias of the value-function estimation, leading to faster convergence to the optimal value and the optimal policy. Moreover, G-learning enables the natural incorporation of prior domain knowledge, when available. The stochastic nature of G-learning also makes it avoid some exploration costs, a property usually attributed only to on-policy algorithms. We illustrate these ideas in several examples, where G-learning results in significant improvements of the convergence rate and the cost of the learning process. version:2
arxiv-1602-02720 | Multimodal Remote Sensing Image Registration with Accuracy Estimation at Local and Global Scales | http://arxiv.org/abs/1602.02720 | id:1602.02720 author:M. L. Uss, B. Vozel, V. V. Lukin, K. Chehdi category:cs.CV  published:2016-02-08 summary:This paper focuses on potential accuracy of remote sensing images registration. We investigate how this accuracy can be estimated without ground truth available and used to improve registration quality of mono- and multi-modal pair of images. At the local scale of image fragments, the Cramer-Rao lower bound (CRLB) on registration error is estimated for each local correspondence between coarsely registered pair of images. This CRLB is defined by local image texture and noise properties. Opposite to the standard approach, where registration accuracy is only evaluated at the output of the registration process, such valuable information is used by us as an additional input knowledge. It greatly helps detecting and discarding outliers and refining the estimation of geometrical transformation model parameters. Based on these ideas, a new area-based registration method called RAE (Registration with Accuracy Estimation) is proposed. In addition to its ability to automatically register very complex multimodal image pairs with high accuracy, the RAE method provides registration accuracy at the global scale as covariance matrix of estimation error of geometrical transformation model parameters or as point-wise registration Standard Deviation. This accuracy does not depend on any ground truth availability and characterizes each pair of registered images individually. Thus, the RAE method can identify image areas for which a predefined registration accuracy is guaranteed. The RAE method is proved successful with reaching subpixel accuracy while registering eight complex mono/multimodal and multitemporal image pairs including optical to optical, optical to radar, optical to Digital Elevation Model (DEM) images and DEM to radar cases. Other methods employed in comparisons fail to provide in a stable manner accurate results on the same test cases. version:2
arxiv-1604-03628 | Joint Unsupervised Learning of Deep Representations and Image Clusters | http://arxiv.org/abs/1604.03628 | id:1604.03628 author:Jianwei Yang, Devi Parikh, Dhruv Batra category:cs.CV cs.LG  published:2016-04-13 summary:In this paper, we propose a recurrent framework for joint unsupervised learning of deep representations and image clusters. In our framework, successive operations in a clustering algorithm are expressed as steps in a recurrent process, stacked on top of representations output by a Convolutional Neural Network (CNN). During training, image clusters and representations are updated jointly: image clustering is conducted in the forward pass, while representation learning in the backward pass. Our key idea behind this framework is that good representations are beneficial to image clustering and clustering results provide supervisory signals to representation learning. By integrating two processes into a single model with a unified weighted triplet loss and optimizing it end-to-end, we can obtain not only more powerful representations, but also more precise image clusters. Extensive experiments show that our method outperforms the state-of-the-art on image clustering across a variety of image datasets. Moreover, the learned representations generalize well when transferred to other tasks. version:2
arxiv-1506-02142 | Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning | http://arxiv.org/abs/1506.02142 | id:1506.02142 author:Yarin Gal, Zoubin Ghahramani category:stat.ML cs.LG  published:2015-06-06 summary:Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning. version:5
arxiv-1506-02157 | Dropout as a Bayesian Approximation: Appendix | http://arxiv.org/abs/1506.02157 | id:1506.02157 author:Yarin Gal, Zoubin Ghahramani category:stat.ML  published:2015-06-06 summary:We show that a neural network with arbitrary depth and non-linearities, with dropout applied before every weight layer, is mathematically equivalent to an approximation to a well known Bayesian model. This interpretation might offer an explanation to some of dropout's key properties, such as its robustness to over-fitting. Our interpretation allows us to reason about uncertainty in deep learning, and allows the introduction of the Bayesian machinery into existing deep learning frameworks in a principled way. This document is an appendix for the main paper "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning" by Gal and Ghahramani, 2015. version:5
arxiv-1605-08003 | Tight Complexity Bounds for Optimizing Composite Objectives | http://arxiv.org/abs/1605.08003 | id:1605.08003 author:Blake Woodworth, Nathan Srebro category:math.OC cs.LG stat.ML  published:2016-05-25 summary:We provide tight upper and lower bounds on the complexity of minimizing the average of m convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and Katyusha are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing AGD that improve over methods using just gradient accesses. version:1
arxiv-1510-05610 | Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues | http://arxiv.org/abs/1510.05610 | id:1510.05610 author:Nihar B. Shah, Sivaraman Balakrishnan, Adityanand Guntuboyina, Martin J. Wainwright category:stat.ML cs.IT cs.LG math.IT  published:2015-10-19 summary:There are various parametric models for analyzing pairwise comparison data, including the Bradley-Terry-Luce (BTL) and Thurstone models, but their reliance on strong parametric assumptions is limiting. In this work, we study a flexible model for pairwise comparisons, under which the probabilities of outcomes are required only to satisfy a natural form of stochastic transitivity. This class includes parametric models including the BTL and Thurstone models as special cases, but is considerably more general. We provide various examples of models in this broader stochastically transitive class for which classical parametric models provide poor fits. Despite this greater flexibility, we show that the matrix of probabilities can be estimated at the same rate as in standard parametric models. On the other hand, unlike in the BTL and Thurstone models, computing the minimax-optimal estimator in the stochastically transitive model is non-trivial, and we explore various computationally tractable alternatives. We show that a simple singular value thresholding algorithm is statistically consistent but does not achieve the minimax rate. We then propose and study algorithms that achieve the minimax rate over interesting sub-classes of the full stochastically transitive class. We complement our theoretical results with thorough numerical simulations. version:3
arxiv-1605-07999 | Toward a general, scaleable framework for Bayesian teaching with applications to topic models | http://arxiv.org/abs/1605.07999 | id:1605.07999 author:Baxter S. Eaves Jr, Patrick Shafto category:cs.LG cs.AI stat.ML  published:2016-05-25 summary:Machines, not humans, are the world's dominant knowledge accumulators but humans remain the dominant decision makers. Interpreting and disseminating the knowledge accumulated by machines requires expertise, time, and is prone to failure. The problem of how best to convey accumulated knowledge from computers to humans is a critical bottleneck in the broader application of machine learning. We propose an approach based on human teaching where the problem is formalized as selecting a small subset of the data that will, with high probability, lead the human user to the correct inference. This approach, though successful for modeling human learning in simple laboratory experiments, has failed to achieve broader relevance due to challenges in formulating general and scalable algorithms. We propose general-purpose teaching via pseudo-marginal sampling and demonstrate the algorithm by teaching topic models. Simulation results show our sampling-based approach: effectively approximates the probability where ground-truth is possible via enumeration, results in data that are markedly different from those expected by random sampling, and speeds learning especially for small amounts of data. Application to movie synopsis data illustrates differences between teaching and random sampling for teaching distributions and specific topics, and demonstrates gains in scalability and applicability to real-world problems. version:1
arxiv-1605-07991 | Efficient Distributed Learning with Sparsity | http://arxiv.org/abs/1605.07991 | id:1605.07991 author:Jialei Wang, Mladen Kolar, Nathan Srebro, Tong Zhang category:stat.ML cs.LG  published:2016-05-25 summary:We propose a novel, efficient approach for distributed sparse learning in high-dimensions, where observations are randomly partitioned across machines. Computationally, at each round our method only requires the master machine to solve a shifted ell_1 regularized M-estimation problem, and other workers to compute the gradient. In respect of communication, the proposed approach provably matches the estimation error bound of centralized methods within constant rounds of communications (ignoring logarithmic factors). We conduct extensive experiments on both simulated and real world datasets, and demonstrate encouraging performances on high-dimensional regression and classification tasks. version:1
arxiv-1512-05287 | A Theoretically Grounded Application of Dropout in Recurrent Neural Networks | http://arxiv.org/abs/1512.05287 | id:1512.05287 author:Yarin Gal category:stat.ML  published:2015-12-16 summary:Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning. version:3
arxiv-1605-07960 | Multi-Object Tracking and Identification over Sets | http://arxiv.org/abs/1605.07960 | id:1605.07960 author:Aijun Bai category:cs.CV cs.RO  published:2016-05-25 summary:The ability for an autonomous agent or robot to track and identify potentially multiple objects in a dynamic environment is essential for many applications, such as automated surveillance, traffic monitoring, human-robot interaction, etc. The main challenge is due to the noisy and incomplete perception including inevitable false negative and false positive errors from a low-level detector. In this paper, we propose a novel multi-object tracking and identification over sets approach to address this challenge. We define joint states and observations both as finite sets, and develop motion and observation functions accordingly. The object identification problem is then formulated and solved by using expectation-maximization methods. The set formulation enables us to avoid directly performing observation-to-object association. We empirically confirm that the overall algorithm outperforms the state-of-the-art in a popular PETS dataset. version:1
arxiv-1605-07950 | A First Order Free Lunch for SQRT-Lasso | http://arxiv.org/abs/1605.07950 | id:1605.07950 author:Xingguo Li, Jarvis Haupt, Raman Arora, Han Liu, Mingyi Hong, Tuo Zhao category:cs.LG math.OC stat.ML  published:2016-05-25 summary:Many statistical machine learning techniques sacrifice convenient computational structures to gain estimation robustness and modeling flexibility. In this paper, we study this fundamental tradeoff through a SQRT-Lasso problem for sparse linear regression and sparse precision matrix estimation in high dimensions. We explain how novel optimization techniques help address these computational challenges. Particularly, we propose a pathwise iterative smoothing shrinkage thresholding algorithm for solving the SQRT-Lasso optimization problem. We further provide a novel model-based perspective for analyzing the smoothing optimization framework, which allows us to establish a nearly linear convergence (R-linear convergence) guarantee for our proposed algorithm. This implies that solving the SQRT-Lasso optimization is almost as easy as solving the Lasso optimization. Moreover, we show that our proposed algorithm can also be applied to sparse precision matrix estimation, and enjoys good computational properties. Numerical experiments are provided to support our theory. version:1
arxiv-1605-07162 | Pure Exploration of Multi-armed Bandit Under Matroid Constraints | http://arxiv.org/abs/1605.07162 | id:1605.07162 author:Lijie Chen, Anupam Gupta, Jian Li category:cs.LG cs.DS  published:2016-05-23 summary:We study the pure exploration problem subject to a matroid constraint (Best-Basis) in a stochastic multi-armed bandit game. In a Best-Basis instance, we are given $n$ stochastic arms with unknown reward distributions, as well as a matroid $\mathcal{M}$ over the arms. Let the weight of an arm be the mean of its reward distribution. Our goal is to identify a basis of $\mathcal{M}$ with the maximum total weight, using as few samples as possible. The problem is a significant generalization of the best arm identification problem and the top-$k$ arm identification problem, which have attracted significant attentions in recent years. We study both the exact and PAC versions of Best-Basis, and provide algorithms with nearly-optimal sample complexities for these versions. Our results generalize and/or improve on several previous results for the top-$k$ arm identification problem and the combinatorial pure exploration problem when the combinatorial constraint is a matroid. version:3
arxiv-1605-07918 | Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks with Feedback Negative Sampling | http://arxiv.org/abs/1605.07918 | id:1605.07918 author:Byungsoo Kim, Hwanjo Yu, Gary Geunbae Lee category:cs.CL cs.AI cs.NE  published:2016-05-25 summary:Previous studies in Open Information Extraction (Open IE) are mainly based on extraction patterns. They manually define patterns or automatically learn them from a large corpus. However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations. In this paper, we address this problem with the following methods. First, we exploit long short-term memory (LSTM) networks to extract higher-level features along the shortest dependency paths, connecting headwords of relations and arguments. The path-level features from LSTM networks provide useful clues regarding contextual information and the validity of arguments. Second, we constructed samples to train LSTM networks without the need for manual labeling. In particular, feedback negative sampling picks highly negative samples among non-positive samples through a model trained with positive samples. The experimental results show that our approach produces more precise and abundant extractions than state-of-the-art open IE systems. To the best of our knowledge, this is the first work to apply deep learning to Open IE. version:1
arxiv-1605-07906 | How priors of initial hyperparameters affect Gaussian process regression models | http://arxiv.org/abs/1605.07906 | id:1605.07906 author:Zexun Chen, Bo Wang category:stat.ML  published:2016-05-25 summary:Gaussian Process Regression (GPR) is a kernel-based nonparametric method and has been proved to be effective and powerful. Its performance, however, relies on appropriate selection of kernel and the involving hyperparameters. The hyperparameters for a specified kernel are often estimated from the data via the maximum marginal likelihood. Unfortunately, the marginal likelihood functions are not usually convex with respect to the hyperparameters, therefore the optimization may not converge to global maxima. A common approach to tackle this issue is to use multiple starting points randomly selected from a specific prior distribution. Therefore, the choice of prior distribution may play a vital rule in the usefulness of this approach. In this paper, we study the sensitivity of prior distributions to the hyperparameter estimation and the performance of GPR. We consider different types of priors, including vague and data-dominated, for the initial values of hyperparameters for some commonly used kernels and investigate the influence of the priors on the performance of GPR models. The results show that the sensitivity of the hyperparameter estimation depends on the choice of kernels, but the priors have little influence on the performance of the GPR models in terms of predictability. version:1
arxiv-1605-07895 | Automatic Extraction of Causal Relations from Natural Language Texts: A Comprehensive Survey | http://arxiv.org/abs/1605.07895 | id:1605.07895 author:Nabiha Asghar category:cs.AI cs.CL cs.IR  published:2016-05-25 summary:Automatic extraction of cause-effect relationships from natural language texts is a challenging open problem in Artificial Intelligence. Most of the early attempts at its solution used manually constructed linguistic and syntactic rules on small and domain-specific data sets. However, with the advent of big data, the availability of affordable computing power and the recent popularization of machine learning, the paradigm to tackle this problem has slowly shifted. Machines are now expected to learn generic causal extraction rules from labelled data with minimal supervision, in a domain independent-manner. In this paper, we provide a comprehensive survey of causal relation extraction techniques from both paradigms, and analyse their relative strengths and weaknesses, with recommendations for future work. version:1
arxiv-1605-07891 | Query Expansion with Locally-Trained Word Embeddings | http://arxiv.org/abs/1605.07891 | id:1605.07891 author:Fernando Diaz, Bhaskar Mitra, Nick Craswell category:cs.IR cs.CL  published:2016-05-25 summary:Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for information retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, underperform corpus and query specific embeddings for retrieval tasks. These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings. version:1
arxiv-1605-07874 | BattRAE: Bidimensional Attention-Based Recursive Autoencoders for Learning Bilingual Phrase Embeddings | http://arxiv.org/abs/1605.07874 | id:1605.07874 author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL  published:2016-05-25 summary:In this paper, we propose a bidimensional attention based recursive autoencoder (BattRAE) to integrate cues and source-target interactions at multiple levels of granularity into bilingual phrase representations. We employ recursive autoencoders to generate tree structures of phrase with embeddings at different levels of granularity (e.g., words, sub-phrases, phrases). Over these embeddings on the source and target side, we introduce a bidimensional attention network to learn their interactions encoded in a bidimensional attention matrix, from which we extract two soft attention weight distributions simultaneously. The weight distributions enable BattRAE to generate compositive phrase representations via convolution. Based on the learned phrase representations, we further use a bilinear neural model, trained via a max-margin method, to measure bilingual semantic similarity. In order to evaluate the effectiveness of BattRAE, we incorporate this semantic similarity as an additional feature into a state-of-the-art SMT system. Extensive experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.82 BLEU points over the baseline. version:1
arxiv-1603-04186 | Visual Concept Recognition and Localization via Iterative Introspection | http://arxiv.org/abs/1603.04186 | id:1603.04186 author:Amir Rosenfeld, Shimon Ullman category:cs.CV cs.LG  published:2016-03-14 summary:Convolutional neural networks have been shown to develop internal representations, which correspond closely to semantically meaningful objects and parts, although trained solely on class labels. Class Activation Mapping (CAM) is a recent method that makes it possible to easily highlight the image regions contributing to a network's classification decision. We build upon these two developments to enable a network to re-examine informative image regions, which we term introspection. We propose a weakly-supervised iterative scheme, which shifts its center of attention to increasingly discriminative regions as it progresses, by alternating stages of classification and introspection. We evaluate our method and show its effectiveness over a range of several datasets, where we obtain competitive or state-of-the-art results: on Stanford-40 Actions, we set a new state-of the art of 81.74%. On FGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvements over baselines, some of which include significantly more supervision. version:2
arxiv-1605-07870 | Simultaneous Sparse Dictionary Learning and Pruning | http://arxiv.org/abs/1605.07870 | id:1605.07870 author:Simeng Qu, Xiao Wang category:stat.ML  published:2016-05-25 summary:Dictionary learning is a cutting-edge area in imaging processing, that has recently led to state-of-the-art results in many signal processing tasks. The idea is to conduct a linear decomposition of a signal using a few atoms of a learned and usually over-completed dictionary instead of a pre-defined basis. Determining a proper size of the to-be-learned dictionary is crucial for both precision and efficiency of the process, while most of the existing dictionary learning algorithms choose the size quite arbitrarily. In this paper, a novel regularization method called the Grouped Smoothly Clipped Absolute Deviation (GSCAD) is employed for learning the dictionary. The proposed method can simultaneously learn a sparse dictionary and select the appropriate dictionary size. Efficient algorithm is designed based on the alternative direction method of multipliers (ADMM) which decomposes the joint non-convex problem with the non-convex penalty into two convex optimization problems. Several examples are presented for image denoising and the experimental results are compared with other state-of-the-art approaches. version:1
arxiv-1605-07869 | Variational Neural Machine Translation | http://arxiv.org/abs/1605.07869 | id:1605.07869 author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL  published:2016-05-25 summary:Models of neural machine translation are often from a discriminative family of encoder-decoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoder-decoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform an efficient posterior inference, we build a neural posterior approximator that is conditioned only on the source side. Additionally, we employ a reparameterization technique to estimate the variational lower bound so as to enable standard stochastic gradient optimization and large-scale training for the variational model. Experiments on NIST Chinese-English translation tasks show that the proposed variational neural machine translation achieves significant improvements over both state-of-the-art statistical and neural machine translation baselines. version:1
arxiv-1603-05106 | One-Shot Generalization in Deep Generative Models | http://arxiv.org/abs/1603.05106 | id:1603.05106 author:Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, Daan Wierstra category:stat.ML cs.AI cs.LG  published:2016-03-16 summary:Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning. version:2
arxiv-1605-07852 | Unsupervised Formation Matching in Highly Inflected Languages | http://arxiv.org/abs/1605.07852 | id:1605.07852 author:Javid Dadashkarimi, Hossein Nasr Esfahani, Heshaam Faili, Azadeh Shakery category:cs.IR cs.CL  published:2016-05-25 summary:There have been multiple attempts to resolve the various formation matching problem in information retrieval. Stemming is a common strategy to this end. Among many approaches for stemming, statistical stemming has shown to be effective in a few number of languages, particularly those highly inflected ones. Common statistical approaches heavily relies on string similarity in terms of prefix and suffix matching. In this paper we propose a method that is able to find any popular affix in any position of a word; specifically finding infixes is a required task in Persian, Arabic, and Malay particularly in irregular formations and is a future challenge in informal texts. The proposed method aims at finding a number of transformation rules for expanding a word with its inflectional/derivation formations. Our experimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks indicate that the proposed method significantly outperforms all the baselines in terms of MAP. version:1
arxiv-1602-03001 | A Convolutional Attention Network for Extreme Summarization of Source Code | http://arxiv.org/abs/1602.03001 | id:1602.03001 author:Miltiadis Allamanis, Hao Peng, Charles Sutton category:cs.LG cs.CL cs.SE  published:2016-02-09 summary:Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model's attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network's performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms. version:2
arxiv-1605-07844 | Dimension Projection among Languages based on Pseudo-relevant Documents for Query Translation | http://arxiv.org/abs/1605.07844 | id:1605.07844 author:Javid Dadashkarimi, Mahsa S. Shahshahani, Amirhossein Tebbifakhr, Heshaam Faili, Azadeh Shakery category:cs.IR cs.AI cs.CL  published:2016-05-25 summary:Taking advantage of top-ranked documents in response to a query for improving quality of query translation has been shown to be an effective approach for cross-language information retrieval. In this paper, we propose a new method for query translation based on dimension projection of embedded vectors from the pseudo-relevant documents in the source language to their equivalents in the target language. To this end, first we learn low-dimensional representations of the words in the pseudo-relevant collections separately and then aim at finding a query-dependent transformation matrix between the vectors of translation pairs. At the next step, representation of each query term is projected to the target language and then, after using a softmax function, a query-dependent translation model is built. Finally, the model is used for query translation. Our experiments on four CLEF collections in French, Spanish, German, and Persian demonstrate that the proposed method outperforms all competitive baselines in language modelling, particularly when it is combined with a collection-dependent translation model. version:1
arxiv-1605-07843 | Unsupervised Word and Dependency Path Embeddings for Aspect Term Extraction | http://arxiv.org/abs/1605.07843 | id:1605.07843 author:Yichun Yin, Furu Wei, Li Dong, Kaimeng Xu, Ming Zhang, Ming Zhou category:cs.CL  published:2016-05-25 summary:In this paper, we develop a novel approach to aspect term extraction based on unsupervised learning of distributed representations of words and dependency paths. The basic idea is to connect two words (w1 and w2) with the dependency path (r) between them in the embedding space. Specifically, our method optimizes the objective w1 + r = w2 in the low-dimensional space, where the multi-hop dependency paths are treated as a sequence of grammatical relations and modeled by a recurrent neural network. Then, we design the embedding features that consider linear context and dependency context information, for the conditional random field (CRF) based aspect term extraction. Experimental results on the SemEval datasets show that, (1) with only embedding features, we can achieve state-of-the-art results; (2) our embedding method which incorporates the syntactic information among words yields better performance than other representative ones in aspect term extraction. version:1
arxiv-1605-07833 | Effective Blind Source Separation Based on the Adam Algorithm | http://arxiv.org/abs/1605.07833 | id:1605.07833 author:Michele Scarpiniti, Simone Scardapane, Danilo Comminiello, Raffaele Parisi, Aurelio Uncini category:cs.LG  published:2016-05-25 summary:In this paper, we derive a modified InfoMax algorithm for the solution of Blind Signal Separation (BSS) problems by using advanced stochastic methods. The proposed approach is based on a novel stochastic optimization approach known as the Adaptive Moment Estimation (Adam) algorithm. The proposed BSS solution can benefit from the excellent properties of the Adam approach. In order to derive the new learning rule, the Adam algorithm is introduced in the derivation of the cost function maximization in the standard InfoMax algorithm. The natural gradient adaptation is also considered. Finally, some experimental results show the effectiveness of the proposed approach. version:1
arxiv-1605-07826 | Asymptotically exact conditional inference in deep generative models and differentiable simulators | http://arxiv.org/abs/1605.07826 | id:1605.07826 author:Matthew M. Graham, Amos Storkey category:stat.CO stat.ML  published:2016-05-25 summary:Many generative models can be expressed as a deterministic differentiable function $\mathrm{\mathbf{g}}(\boldsymbol{u})$ of random variables $\boldsymbol{u}$ drawn from some simple base density. This framework includes both deep generative architectures such as Variational Autoencoders and Generative Adversial Nets, and a large class of dynamical system simulators. We present a method for performing efficient MCMC inference in such models under conditioning constraints on the model output. For some models this offers an asymptotically exact inference method where Approximate Bayesian Computation might otherwise be employed. We use the intuition that conditional inference corresponds to integrating the base density across a manifold corresponding to the set of $\boldsymbol{u}$ consistent with the conditioning. This motivates the use of a constrained variant of Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to coherently move between states satisfying the constraint. We validate the method by performing inference tasks in a diverse set of models: parameter inference in a dynamical predator-prey simulation, joint 3D pose and camera model inference from a 2D projection and image in-painting with a generative model of MNIST digit images. version:1
arxiv-1605-07824 | Action Classification via Concepts and Attributes | http://arxiv.org/abs/1605.07824 | id:1605.07824 author:Amir Rosenfeld, Shimon Ullman category:cs.CV cs.LG  published:2016-05-25 summary:Classes in natural images tend to follow long tail distributions. This is problematic when there are insufficient training examples for rare classes. This effect is emphasized in compound classes, involving the conjunction of several concepts, such as those appearing in action-recognition datasets. In this paper, we propose to address this issue by learning how to utilize common visual concepts which are readily available. We detect the presence of prominent concepts in images and use them to infer the target labels instead of using visual features directly, combining tools from vision and natural-language processing. We validate our method on the recently introduced HICO dataset reaching a mAP of 31.54% and on the Stanford-40 Actions dataset, where the proposed method outperforms current state-of-the art and, combined with direct visual features, obtains an accuracy 83.12%. Moreover, the method provides for each class a semantically meaningful list of keywords and relevant image regions relating it to its constituent concepts. version:1
arxiv-1605-04515 | Machine Translation Evaluation: A Survey | http://arxiv.org/abs/1605.04515 | id:1605.04515 author:Aaron Li-Feng Han, Derek Fai Wong category:cs.CL I.2.7; I.2.1  published:2016-05-15 summary:This paper introduces the state-of-the-art machine translation (MT) evaluation survey that contains both manual and automatic evaluation methods. The traditional human evaluation criteria mainly include the intelligibility, fidelity, fluency, adequacy, comprehension, and informativeness. The advanced human assessments include task-oriented measures, post-editing, segment ranking, and extended criteriea, etc. We classify the automatic evaluation methods into two categories, including lexical similarity scenario and linguistic features application. The lexical similarity methods contain edit distance, precision, recall, F-measure, and word order. The linguistic features can be divided into syntactic features and semantic features respectively. The syntactic features include part of speech tag, phrase types and sentence structures, and the semantic features include named entity, synonyms, textual entailment, paraphrase, semantic roles, and language models. Subsequently, we also introduce the evaluation methods for MT evaluation including different correlation scores, and the recent quality estimation (QE) tasks for MT. This paper differs from the existing works \cite{GALEprogram2009,EuroMatrixProject2007} from several aspects, by introducing some recent development of MT evaluation measures, the different classifications from manual to automatic evaluation measures, the introduction of recent QE tasks of MT, and the concise construction of the content.\footnote{Part of this work was done when working in NLP2CT lab in Macau}. \end{abstract} version:5
arxiv-1604-05921 | Jansen-MIDAS: a multi-level photomicrograph segmentation software based on isotropic undecimated wavelets | http://arxiv.org/abs/1604.05921 | id:1604.05921 author:Alexandre Fioravante de Siqueira, Flávio Camargo Cabrera, Wagner Massayuki Nakasuga, Aylton Pagamisse, Aldo Eloizo Job category:cs.CV 68T10  published:2016-04-20 summary:Image segmentation, the process of separating the elements within an image, is frequently used for obtaining information from photomicrographs. However, segmentation methods should be used with reservations: incorrect segmentation can mislead when interpreting regions of interest (ROI), thus decreasing the success rate of additional procedures. Multi-Level Starlet Segmentation (MLSS) and Multi-Level Starlet Optimal Segmentation (MLSOS) were developed to address the photomicrograph segmentation deficiency on general tools. These methods gave rise to Jansen-MIDAS, an open-source software which a scientist can use to obtain a multi-level threshold segmentation of his/hers photomicrographs. This software is presented in two versions: a text-based version, for GNU Octave, and a graphical user interface (GUI) version, for MathWorks MATLAB. It can be used to process several types of images, becoming a reliable alternative to the scientist. version:2
arxiv-1605-07805 | Learning Moore Machines from Input-Output Traces | http://arxiv.org/abs/1605.07805 | id:1605.07805 author:Georgios Giantamidis, Stavros Tripakis category:cs.FL cs.LG  published:2016-05-25 summary:The problem of learning automata from example traces (but no equivalence or membership queries) is fundamental in automata learning theory and practice. In this paper we study this problem for finite state machines with inputs and outputs, and in particular for Moore machines. We introduce three algorithms for solving this problem: (1) the PTAP algorithm, which transforms a set of input-output traces into an incomplete Moore machine and then completes that machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI always learns the right machine when the training set is a characteristic sample, which is generally not true for the other two algorithms. We also compare the algorithms experimentally in terms of the size of the learned machine and several notions of accuracy, introduced in this paper. Finally, we compare with OSTIA, an algorithm that learns a more general class of transducers, and find that OSTIA generally does not learn a Moore machine, even when fed with a characteristic sample. version:1
arxiv-1605-07785 | Geometry-aware stationary subspace analysis | http://arxiv.org/abs/1605.07785 | id:1605.07785 author:Inbal Horev, Florian Yger, Masashi Sugiyama category:cs.LG  published:2016-05-25 summary:In many real-world applications data exhibits non-stationarity, i.e., its distribution changes over time. One approach to handling non-stationarity is to remove or minimize it before attempting to analyze the data. In the context of brain computer interface (BCI) data analysis this may be done by means of stationary subspace analysis (SSA). The classic SSA method finds a matrix that projects the data onto a stationary subspace by optimizing a cost function based on a matrix divergence. In this work we present an alternative method for SSA based on a symmetrized version of this matrix divergence. We show that this frames the problem in terms of distances between symmetric positive definite (SPD) matrices, suggesting a geometric interpretation of the problem. Stemming from this geometric viewpoint, we introduce and analyze a method which utilizes the geometry of the SPD matrix manifold and the invariance properties of its metrics. Most notably we show that these invariances alleviate the need to whiten the input matrices, a common step in many SSA methods which often introduces errors. We demonstrate the usefulness of our technique in experiments on both synthesized and real-world data. version:1
arxiv-1605-07784 | Fast Algorithms for Robust PCA via Gradient Descent | http://arxiv.org/abs/1605.07784 | id:1605.07784 author:Xinyang Yi, Dohyung Park, Yudong Chen, Constantine Caramanis category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH  published:2016-05-25 summary:We consider the problem of Robust PCA in the the fully and partially observed settings. Without corruptions, this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomial-time algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms. In particular, in the fully observed case, with $r$ denoting rank and $d$ dimension, we reduce the complexity from $\mathcal{O}(r^2d^2\log(1/\varepsilon))$ to $\mathcal{O}(rd^2\log(1/\varepsilon))$ -- a big savings when the rank is big. For the partially observed case, we show the complexity of our algorithm is no more than $\mathcal{O}(r^4d \log d \log(1/\varepsilon))$. Not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where $r$ is small compared to $d$, it also allows for near-linear-in-$d$ run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations. version:1
arxiv-1605-07779 | Neural Universal Discrete Denoiser | http://arxiv.org/abs/1605.07779 | id:1605.07779 author:Taesup Moon, Seon Woo Min category:cs.LG  published:2016-05-25 summary:We present a new framework of applying deep neural networks (DNN) to devise a universal discrete denoiser. Unlike other approaches that utilize supervised learning for denoising, we do not require any additional training data. In such setting, while the ground-truth label, i.e., the clean data, is not available, we devise "pseudo-labels" and a novel objective function such that DNN can be trained in a same way as supervised learning to become a discrete denoiser. We experimentally show that our resulting algorithm, dubbed as Neural DUDE, significantly outperforms the previous state-of-the-art in several applications with a systematic rule of choosing the hyperparameter, which is an attractive feature in practice. version:1
arxiv-1605-07774 | Generalized Mirror Descents in Congestion Games | http://arxiv.org/abs/1605.07774 | id:1605.07774 author:Po-An Chen, Chi-Jen Lu category:cs.GT cs.LG  published:2016-05-25 summary:Different types of dynamics have been studied in repeated game play, and one of them which has received much attention recently consists of those based on "no-regret" algorithms from the area of machine learning. It is known that dynamics based on generic no-regret algorithms may not converge to Nash equilibria in general, but to a larger set of outcomes, namely coarse correlated equilibria. Moreover, convergence results based on generic no-regret algorithms typically use a weaker notion of convergence: the convergence of the average plays instead of the actual plays. Some work has been done showing that when using a specific no-regret algorithm, the well-known multiplicative updates algorithm, convergence of actual plays to equilibria can be shown and better quality of outcomes in terms of the price of anarchy can be reached for atomic congestion games and load balancing games. Are there more cases of natural no-regret dynamics that perform well in suitable classes of games in terms of convergence and quality of outcomes that the dynamics converge to? We answer this question positively in the bulletin-board model by showing that when employing the mirror-descent algorithm, a well-known generic no-regret algorithm, the actual plays converge quickly to equilibria in nonatomic congestion games. Furthermore, the bandit model considers a probably more realistic and prevalent setting with only partial information, in which at each time step each player only knows the cost of her own currently played strategy, but not any costs of unplayed strategies. For the class of atomic congestion games, we propose a family of bandit algorithms based on the mirror-descent algorithms previously presented, and show that when each player individually adopts such a bandit algorithm, their joint (mixed) strategy profile quickly converges with implications. version:1
arxiv-1602-06693 | Preconditioning Kernel Matrices | http://arxiv.org/abs/1602.06693 | id:1602.06693 author:Kurt Cutajar, Michael A. Osborne, John P. Cunningham, Maurizio Filippone category:stat.ML stat.CO stat.ME  published:2016-02-22 summary:The computational and storage complexity of kernel machines presents the primary barrier to their scaling to large, modern, datasets. A common way to tackle the scalability issue is to use the conjugate gradient algorithm, which relieves the constraints on both storage (the kernel matrix need not be stored) and computation (both stochastic gradients and parallelization can be used). Even so, conjugate gradient is not without its own issues: the conditioning of kernel matrices is often such that conjugate gradients will have poor convergence in practice. Preconditioning is a common approach to alleviating this issue. Here we propose preconditioned conjugate gradients for kernel machines, and develop a broad range of preconditioners particularly useful for kernel matrices. We describe a scalable approach to both solving kernel machines and learning their hyperparameters. We show this approach is exact in the limit of iterations and outperforms state-of-the-art approximations for a given computational budget. version:2
arxiv-1605-07766 | Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction | http://arxiv.org/abs/1605.07766 | id:1605.07766 author:Kim Anh Nguyen, Sabine Schulte im Walde, Ngoc Thang Vu category:cs.CL  published:2016-05-25 summary:We propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66-0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex-999, and on distinguishing antonyms from synonyms. version:1
arxiv-1604-00359 | COCO: The Bi-objective Black Box Optimization Benchmarking (bbob-biobj) Test Suite | http://arxiv.org/abs/1604.00359 | id:1604.00359 author:Tea Tusar, Dimo Brockhoff, Nikolaus Hansen, Anne Auger category:cs.AI cs.NE  published:2016-04-01 summary:The bbob-biobj test suite contains 55 bi-objective functions in continuous domain which are derived from combining functions of the well-known single-objective noiseless bbob test suite. Besides giving the actual function definitions and presenting their (known) properties, this documentation also aims at giving the rationale behind our approach in terms of function groups, instances, and potential objective space normalization. version:2
arxiv-1605-07747 | NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization | http://arxiv.org/abs/1605.07747 | id:1605.07747 author:Davood Hajinezhad, Mingyi Hong, Tuo Zhao, Zhaoran Wang category:math.OC stat.ML  published:2016-05-25 summary:We study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of $N$ nonconvex $L_i/N$-smooth functions, plus a nonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into $N$ subproblems, and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling, a version of NESTT achieves $\epsilon$-stationary solution using $\mathcal{O}((\sum_{i=1}^N\sqrt{L_i/N})^2/\epsilon)$ gradient evaluations, which can be up to $\mathcal{O}(N)$ times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex $\ell_1$ penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between {\it primal-dual} based methods and a few {\it primal only} methods such as IAG/SAG/SAGA. version:1
arxiv-1605-06359 | Learning to Discover Graphical Model Structures | http://arxiv.org/abs/1605.06359 | id:1605.06359 author:Eugene Belilovsky, Kyle Kastner, Gaël Varoquaux, Matthew Blaschko category:stat.ML  published:2016-05-20 summary:We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a penalized maximum likelihood objective on the precision matrix. Adapting this objective to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is a very indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired properties. We propose here to leverage this latter source of information in order to learn a function that maps from empirical covariance matrices to estimated graph structures. Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can more directly be tailored to the specific problem of edge structure discovery. We apply this framework to several critical real world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional block structure of matrix inversion. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive (and often superior) performance, compared with analytical methods. version:2
arxiv-1603-08785 | COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Setting | http://arxiv.org/abs/1603.08785 | id:1603.08785 author:Nikolaus Hansen, Anne Auger, Olaf Mersmann, Tea Tusar, Dimo Brockhoff category:cs.AI stat.ML  published:2016-03-29 summary:COCO is a platform for Comparing Continuous Optimizers in a black-box setting. It aims at automatizing the tedious and repetitive task of benchmarking numerical optimization algorithms to the greatest possible extent. We present the rationals behind the development of the platform as a general proposition for a guideline towards better benchmarking. We detail underlying fundamental concepts of COCO such as its definition of a problem, the idea of instances, the relevance of target values, and runtime as central performance measure. Finally, we give a quick overview of the basic code structure and the available test suites. version:2
arxiv-1605-07740 | Improving energy efficiency and classification accuracy of neuromorphic chips by learning binary synaptic crossbars | http://arxiv.org/abs/1605.07740 | id:1605.07740 author:Antonio Jimeno Yepes, Jianbin Tang category:cs.NE  published:2016-05-25 summary:Deep Neural Networks (DNN) have achieved human level performance in many image analytics tasks but DNNs are mostly deployed to GPU platforms that consume a considerable amount of power. Brain-inspired spiking neuromorphic chips consume low power and can be highly parallelized. However, for deploying DNNs to energy efficient neuromorphic chips the incompatibility between continuous neurons and synaptic weights of traditional DNNs, discrete spiking neurons and synapses of neuromorphic chips has to be overcome. Previous work has achieved this by training a network to learn continuous probabilities and deployment to a neuromorphic architecture by random sampling these probabilities. An ensemble of sampled networks is needed to approximate the performance of the trained network. In the work presented in this paper, we have extended previous research by directly learning binary synaptic crossbars. Results on MNIST show that better performance can be achieved with a small network in one time step (92.7% maximum observed accuracy vs 95.98% accuracy in our work). Top results on a larger network are similar to previously published results (99.42% maximum observed accuracy vs 99.45% accuracy in our work). More importantly, in our work a smaller ensemble is needed to achieve similar or better accuracy than previous work, which translates into significantly decreased energy consumption for both networks. Results of our work are stable since they do not require random sampling. version:1
arxiv-1605-07736 | Learning Multiagent Communication with Backpropagation | http://arxiv.org/abs/1605.07736 | id:1605.07736 author:Sainbayar Sukhbaatar, Arthur Szlam, Rob Fergus category:cs.LG cs.AI  published:2016-05-25 summary:Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNN, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand. version:1
arxiv-1604-04198 | Estimating parameters of nonlinear systems using the elitist particle filter based on evolutionary strategies | http://arxiv.org/abs/1604.04198 | id:1604.04198 author:Christian Huemmer, Christian Hofmann, Roland Maas, Walter Kellermann category:stat.ML  published:2016-04-14 summary:In this article, we present the elitist particle filter based on evolutionary strategies (EPFES) as an efficient approach for nonlinear system identification. The EPFES is derived from the frequently-employed state-space model, where the relevant information of the nonlinear system is captured by an unknown state vector. Similar to classical particle filtering, the EPFES consists of a set of particles and respective weights which represent different realizations of the latent state vector and their likelihood of being the solution of the optimization problem. As main innovation, the EPFES includes an evolutionary elitist-particle selection which combines long-term information with instantaneous sampling from an approximated continuous posterior distribution. In this article, we propose two advancements of the previously-published elitist-particle selection process. Further, the EPFES is shown to be a generalization of the widely-used Gaussian particle filter and thus evaluated with respect to the latter for two completely different scenarios: First, we consider the so-called univariate nonstationary growth model with time-variant latent state variable, where the evolutionary selection of elitist particles is evaluated for non-recursively calculated particle weights. Second, the problem of nonlinear acoustic echo cancellation is addressed in a simulated scenario with speech as input signal: By using long-term fitness measures, we highlight the efficacy of the well-generalizing EPFES in estimating the nonlinear system even for large search spaces. Finally, we illustrate similarities between the EPFES and evolutionary algorithms to outline future improvements by fusing the achievements of both fields of research. version:4
arxiv-1605-07735 | Design and development a children's speech database | http://arxiv.org/abs/1605.07735 | id:1605.07735 author:Radoslava Kraleva category:cs.CL cs.HC cs.SD  published:2016-05-25 summary:The report presents the process of planning, designing and the development of a database of spoken children's speech whose native language is Bulgarian. The proposed model is designed for children between the age of 4 and 6 without speech disorders, and reflects their specific capabilities. At this age most children cannot read, there is no sustained concentration, they are emotional, etc. The aim is to unite all the media information accompanying the recording and processing of spoken speech, thereby to facilitate the work of researchers in the field of speech recognition. This database will be used for the development of systems for children's speech recognition, children's speech synthesis systems, games which allow voice control, etc. As a result of the proposed model a prototype system for speech recognition is presented. version:1
arxiv-1605-07733 | On model architecture for a children's speech recognition interactive dialog system | http://arxiv.org/abs/1605.07733 | id:1605.07733 author:Radoslava Kraleva, Velin Kralev category:cs.HC cs.CL cs.SD  published:2016-05-25 summary:This report presents a general model of the architecture of information systems for the speech recognition of children. It presents a model of the speech data stream and how it works. The result of these studies and presented veins architectural model shows that research needs to be focused on acoustic-phonetic modeling in order to improve the quality of children's speech recognition and the sustainability of the systems to noise and changes in transmission environment. Another important aspect is the development of more accurate algorithms for modeling of spontaneous child speech. version:1
arxiv-1605-07725 | Virtual Adversarial Training for Semi-Supervised Text Classification | http://arxiv.org/abs/1605.07725 | id:1605.07725 author:Takeru Miyato, Andrew M. Dai, Ian Goodfellow category:stat.ML cs.LG  published:2016-05-25 summary:Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting. version:1
arxiv-1605-07723 | Data Programming: Creating Large Training Sets, Quickly | http://arxiv.org/abs/1605.07723 | id:1605.07723 author:Alexander Ratner, Christopher De Sa, Sen Wu, Daniel Selsam, Christopher Ré category:stat.ML cs.AI cs.LG  published:2016-05-25 summary:Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users provide a set of labeling functions, which are programs that heuristically label large subsets of data points, albeit noisily. By viewing these labeling functions as implicitly describing a generative model for this noise, we show that we can recover the parameters of this model to "denoise" the training set. Then, we show how to modify a discriminative loss function to make it noise-aware. We demonstrate our method over a range of discriminative models including logistic regression and LSTMs. We establish theoretically that we can recover the parameters of these generative models in a handful of settings. Experimentally, on the 2014 TAC-KBP relation extraction challenge, we show that data programming would have obtained a winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a supervised LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way to create machine learning models for non-experts. version:1
arxiv-1605-07722 | Yum-me: Personalized Healthy Meal Recommender System | http://arxiv.org/abs/1605.07722 | id:1605.07722 author:Longqi Yang, Cheng-Kang Hsieh, Hongjian Yang, Nicola Dell, Serge Belongie, Deborah Estrin category:cs.HC cs.AI cs.CV cs.IR H.5.m  published:2016-05-25 summary:Many ubiquitous computing projects have addressed health and wellness behaviors such as healthy eating. Healthy meal recommendations have the potential to help individuals prevent or manage conditions such as diabetes and obesity. However, learning people's food preferences and making healthy recommendations that appeal to their palate is challenging. Existing approaches either only learn high-level preferences or require a prolonged learning period. We propose Yum-me, a personalized healthy meal recommender system designed to meet individuals' health goals, dietary restrictions, and fine-grained food preferences. Marrying ideas from user preference learning and healthy eating promotion, Yum-me enables a simple and accurate food preference profiling procedure via an image-based online learning framework, and projects the learned profile into the domain of healthy food options to find ones that will appeal to the user. We present the design and implementation of Yum-me, and further discuss the most critical component of it: FoodDist, a state-of-the-art food image analysis model. We demonstrate FoodDist's superior performance through careful benchmarking, and discuss its applicability across a wide array of dietary applications. We validate the feasibility and effectiveness of Yum-me through a 60-person user study, in which Yum-me improves the recommendation acceptance rate by 42.63% over the traditional food preference survey. version:1
arxiv-1605-07719 | Reshaped Wirtinger Flow for Solving Quadratic Systems of Equations | http://arxiv.org/abs/1605.07719 | id:1605.07719 author:Huishuai Zhang, Yingbin Liang category:stat.ML cs.LG  published:2016-05-25 summary:We study the problem of recovering a vector $\boldsymbol{x}\in \mathbb{R}^n$ from its magnitude measurements $y_i= \langle \boldsymbol{a}_i, \boldsymbol{x}\rangle , i=1,..., m$. Our work is along the line of the Wirtinger flow (WF) approach, which solves the problem by minimizing a nonconvex loss function via a gradient algorithm and can be shown to converge to a global optimal point under good initialization. In contrast to the smooth loss function used in WF, we adopt a nonsmooth but lower-order loss function, and design a gradient-like algorithm (referred to as reshaped-WF). We show that for random Gaussian measurements, reshaped-WF enjoys geometric convergence to a global optimal point as long as the number $m$ of measurements is at the order of $\mathcal{O}(n)$, where $n$ is the dimension of the unknown $\boldsymbol{x}$. This improves the sample complexity of WF, and achieves the same sample complexity as truncated-WF but without truncation at gradient step. Furthermore, reshaped-WF costs less computationally than WF, and runs faster numerically than both WF and truncated-WF. Bypassing higher-order variables in the loss function and truncations in the gradient loop, analysis of reshaped-WF is substantially simplified. version:1
arxiv-1605-07716 | Deeply-Fused Nets | http://arxiv.org/abs/1605.07716 | id:1605.07716 author:Jingdong Wang, Zhen Wei, Ting Zhang, Wenjun Zeng category:cs.CV  published:2016-05-25 summary:In this paper, we present a novel deep learning approach, deeply-fused nets. The central idea of our approach is deep fusion, i.e., combine the intermediate representations of base networks, where the fused output serves as the input of the remaining part of each base network, and perform such combinations deeply over several intermediate representations. The resulting deeply fused net enjoys several benefits. First, it is able to learn multi-scale representations as it enjoys the benefits of more base networks, which could form the same fused network, other than the initial group of base networks. Second, in our suggested fused net formed by one deep and one shallow base networks, the flows of the information from the earlier intermediate layer of the deep base network to the output and from the input to the later intermediate layer of the deep base network are both improved. Last, the deep and shallow base networks are jointly learnt and can benefit from each other. More interestingly, the essential depth of a fused net composed from a deep base network and a shallow base network is reduced because the fused net could be composed from a less deep base network, and thus training the fused net is less difficult than training the initial deep base network. Empirical results demonstrate that our approach achieves superior performance over two closely-related methods, ResNet and Highway, and competitive performance compared to the state-of-the-arts. version:1
arxiv-1511-03796 | Learning Nonparametric Forest Graphical Models with Prior Information | http://arxiv.org/abs/1511.03796 | id:1511.03796 author:Yuancheng Zhu, Zhe Liu, Siqi Sun category:stat.ME cs.LG stat.ML  published:2015-11-12 summary:We present a framework for incorporating prior information into nonparametric estimation of graphical models. To avoid distributional assumptions, we restrict the graph to be a forest and build on the work of forest density estimation (FDE). We reformulate the FDE approach from a Bayesian perspective, and introduce prior distributions on the graphs. As two concrete examples, we apply this framework to estimating scale-free graphs and learning multiple graphs with similar structures. The resulting algorithms are equivalent to finding a maximum spanning tree of a weighted graph with a penalty term on the connectivity pattern of the graph. We solve the optimization problem via a minorize-maximization procedure with Kruskal's algorithm. Simulations show that the proposed methods outperform competing parametric methods, and are robust to the true data distribution. They also lead to improvement in predictive power and interpretability in two real data sets. version:2
arxiv-1506-03877 | Bidirectional Helmholtz Machines | http://arxiv.org/abs/1506.03877 | id:1506.03877 author:Jorg Bornschein, Samira Shabanian, Asja Fischer, Yoshua Bengio category:cs.LG stat.ML  published:2015-06-12 summary:Efficient unsupervised training and inference in deep generative models remains a challenging problem. One basic approach, called Helmholtz machine, involves training a top-down directed generative model together with a bottom-up auxiliary model used for approximate inference. Recent results indicate that better generative models can be obtained with better approximate inference procedures. Instead of improving the inference procedure, we here propose a new model which guarantees that the top-down and bottom-up distributions can efficiently invert each other. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the model distribution to be the geometric mean of these two. We present a lower-bound for the likelihood of this model and we show that optimizing this bound regularizes the model so that the Bhattacharyya distance between the bottom-up and top-down approximate distributions is minimized. This approach results in state of the art generative models which prefer significantly deeper architectures while it allows for orders of magnitude more efficient approximate inference. version:5
arxiv-1506-08301 | A Novel Approach for Stable Selection of Informative Redundant Features from High Dimensional fMRI Data | http://arxiv.org/abs/1506.08301 | id:1506.08301 author:Yilun Wang, Zhiqiang Li, Yifeng Wang, Xiaona Wang, Junjie Zheng, Xujuan Duan, Huafu Chen category:cs.CV cs.LG stat.ML I.5.2  published:2015-06-27 summary:Feature selection is among the most important components because it not only helps enhance the classification accuracy, but also or even more important provides potential biomarker discovery. However, traditional multivariate methods is likely to obtain unstable and unreliable results in case of an extremely high dimensional feature space and very limited training samples, where the features are often correlated or redundant. In order to improve the stability, generalization and interpretations of the discovered potential biomarker and enhance the robustness of the resultant classifier, the redundant but informative features need to be also selected. Therefore we introduced a novel feature selection method which combines a recent implementation of the stability selection approach and the elastic net approach. The advantage in terms of better control of false discoveries and missed discoveries of our approach, and the resulted better interpretability of the obtained potential biomarker is verified in both synthetic and real fMRI experiments. In addition, we are among the first to demonstrate the robustness of feature selection benefiting from the incorporation of stability selection and also among the first to demonstrate the possible unrobustness of the classical univariate two-sample t-test method. Specifically, we show the robustness of our feature selection results in existence of noisy (wrong) training labels, as well as the robustness of the resulted classifier based on our feature selection results in the existence of data variation, demonstrated by a multi-center attention-deficit/hyperactivity disorder (ADHD) fMRI data. version:2
arxiv-1605-07708 | 2D Visual Place Recognition for Domestic Service Robots at Night | http://arxiv.org/abs/1605.07708 | id:1605.07708 author:James Mount, Michael Milford category:cs.RO cs.CV  published:2016-05-25 summary:Domestic service robots such as lawn mowing and vacuum cleaning robots are the most numerous consumer robots in existence today. While early versions employed random exploration, recent systems fielded by most of the major manufacturers have utilized range-based and visual sensors and user-placed beacons to enable robots to map and localize. However, active range and visual sensing solutions have the disadvantages of being intrusive, expensive, or only providing a 1D scan of the environment, while the requirement for beacon placement imposes other practical limitations. In this paper we present a passive and potentially cheap vision-based solution to 2D localization at night that combines easily obtainable day-time maps with low resolution contrast-normalized image matching algorithms, image sequence-based matching in two-dimensions, place match interpolation and recent advances in conventional low light camera technology. In a range of experiments over a domestic lawn and in a lounge room, we demonstrate that the proposed approach enables 2D localization at night, and analyse the effect on performance of varying odometry noise levels, place match interpolation and sequence matching length. Finally we benchmark the new low light camera technology and show how it can enable robust place recognition even in an environment lit only by a moonless sky, raising the tantalizing possibility of being able to apply all conventional vision algorithms, even in the darkest of nights. version:1
arxiv-1605-07700 | Learning Purposeful Behaviour in the Absence of Rewards | http://arxiv.org/abs/1605.07700 | id:1605.07700 author:Marlos C. Machado, Michael Bowling category:cs.LG cs.AI  published:2016-05-25 summary:Artificial intelligence is commonly defined as the ability to achieve goals in the world. In the reinforcement learning framework, goals are encoded as reward functions that guide agent behaviour, and the sum of observed rewards provide a notion of progress. However, some domains have no such reward signal, or have a reward signal so sparse as to appear absent. Without reward feedback, agent behaviour is typically random, often dithering aimlessly and lacking intentionality. In this paper we present an algorithm capable of learning purposeful behaviour in the absence of rewards. The algorithm proceeds by constructing temporally extended actions (options), through the identification of purposes that are "just out of reach" of the agent's current behaviour. These purposes establish intrinsic goals for the agent to learn, ultimately resulting in a suite of behaviours that encourage the agent to visit different parts of the state space. Moreover, the approach is particularly suited for settings where rewards are very sparse, and such behaviours can help in the exploration of the environment until reward is observed. version:1
arxiv-1605-07699 | Describing Human Aesthetic Perception by Deeply-learned Attributes from Flickr | http://arxiv.org/abs/1605.07699 | id:1605.07699 author:L. Zhang category:cs.CV  published:2016-05-25 summary:Many aesthetic models in computer vision suffer from two shortcomings: 1) the low descriptiveness and interpretability of those hand-crafted aesthetic criteria (i.e., nonindicative of region-level aesthetics), and 2) the difficulty of engineering aesthetic features adaptively and automatically toward different image sets. To remedy these problems, we develop a deep architecture to learn aesthetically-relevant visual attributes from Flickr1, which are localized by multiple textual attributes in a weakly-supervised setting. More specifically, using a bag-ofwords (BoW) representation of the frequent Flickr image tags, a sparsity-constrained subspace algorithm discovers a compact set of textual attributes (e.g., landscape and sunset) for each image. Then, a weakly-supervised learning algorithm projects the textual attributes at image-level to the highly-responsive image patches at pixel-level. These patches indicate where humans look at appealing regions with respect to each textual attribute, which are employed to learn the visual attributes. Psychological and anatomical studies have shown that humans perceive visual concepts hierarchically. Hence, we normalize these patches and feed them into a five-layer convolutional neural network (CNN) to mimick the hierarchy of human perceiving the visual attributes. We apply the learned deep features on image retargeting, aesthetics ranking, and retrieval. Both subjective and objective experimental results thoroughly demonstrate the competitiveness of our approach. version:1
arxiv-1511-05644 | Adversarial Autoencoders | http://arxiv.org/abs/1511.05644 | id:1511.05644 author:Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey category:cs.LG  published:2015-11-18 summary:In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks. version:2
arxiv-1605-07689 | Communication-efficient distributed statistical learning | http://arxiv.org/abs/1605.07689 | id:1605.07689 author:Michael I. Jordan, Jason D. Lee, Yun Yang category:stat.ML cs.IT cs.LG math.IT math.OC stat.ME  published:2016-05-25 summary:We present the Communication-efficient Surrogate Likelihood (CSL) framework for solving distributed statistical learning problems. CSL provides a communication-efficient surrogate to the global likelihood that can be used for low-dimensional estimation, high-dimensional regularized estimation and Bayesian inference. For low-dimensional estimation, CSL provably improves upon the averaging schemes and facilitates the construction of confidence intervals. For high-dimensional regularized estimation, CSL leads to a minimax optimal estimator with minimal communication cost. For Bayesian inference, CSL can be used to form a communication-efficient quasi-posterior distribution that converges to the true posterior. This quasi-posterior procedure significantly improves the computational efficiency of MCMC algorithms even in a non-distributed setting. The methods are illustrated through empirical studies. version:1
arxiv-1511-02674 | Semantic Segmentation with Boundary Neural Fields | http://arxiv.org/abs/1511.02674 | id:1511.02674 author:Gedas Bertasius, Jianbo Shi, Lorenzo Torresani category:cs.CV  published:2015-11-09 summary:The state-of-the-art in semantic segmentation is currently represented by fully convolutional networks (FCNs). However, FCNs use large receptive fields and many pooling layers, both of which cause blurring and low spatial resolution in the deep layers. As a result FCNs tend to produce segmentations that are poorly localized around object boundaries. Prior work has attempted to address this issue in post-processing steps, for example using a color-based CRF on top of the FCN predictions. However, these approaches require additional parameters and low-level features that are difficult to tune and integrate into the original network architecture. Additionally, most CRFs use color-based pixel affinities, which are not well suited for semantic segmentation and lead to spatially disjoint predictions. To overcome these problems, we introduce a Boundary Neural Field (BNF), which is a global energy model integrating FCN predictions with boundary cues. The boundary information is used to enhance semantic segment coherence and to improve object localization. Specifically, we first show that the convolutional filters of semantic FCNs provide good features for boundary detection. We then employ the predicted boundaries to define pairwise potentials in our energy. Finally, we show that our energy decomposes semantic segmentation into multiple binary problems, which can be relaxed for efficient global optimization. We report extensive experiments demonstrating that minimization of our global boundary-based energy yields results superior to prior globalization methods, both quantitatively as well as qualitatively. version:2
arxiv-1605-07686 | Local Perturb-and-MAP for Structured Prediction | http://arxiv.org/abs/1605.07686 | id:1605.07686 author:Gedas Bertasius, Qiang Liu, Lorenzo Torresani, Jianbo Shi category:cs.CV  published:2016-05-24 summary:Two fundamental problems in the context of probabilistic graphical models are learning and inference. Many traditional probabilistic methods resort to approximations in either learning, inference, or even both steps due to their large complexity cost. This leads to algorithms where the learning and inference steps are disjoint, which often degrades algorithm performance. In this work, we present a Local Perturb-and-MAP (locPMAP) method, a novel framework for structured prediction based on local optimization over randomly perturbed potential functions. Unlike most prior methods, our proposed scheme does not rely on approximations and also unifies the learning and inference steps. This allows our approach to outperform other methods on several vision tasks, where structured models are commonly used. Additionally, we demonstrate how to apply our proposed scheme for an end-to-end training of a deep structured network. Finally, our framework gives a novel interpretation for pseudolikelihood, enabling various powerful extensions, such as the use of pseudolikelihood under the Bayesian framework. version:1
arxiv-1602-00287 | Additive Approximations in High Dimensional Nonparametric Regression via the SALSA | http://arxiv.org/abs/1602.00287 | id:1602.00287 author:Kirthevasan Kandasamy, Yaoliang Yu category:stat.ML cs.LG  published:2016-01-31 summary:High dimensional nonparametric regression is an inherently difficult problem with known lower bounds depending exponentially in dimension. A popular strategy to alleviate this curse of dimensionality has been to use additive models of \emph{first order}, which model the regression function as a sum of independent functions on each dimension. Though useful in controlling the variance of the estimate, such models are often too restrictive in practical settings. Between non-additive models which often have large variance and first order additive models which have large bias, there has been little work to exploit the trade-off in the middle via additive models of intermediate order. In this work, we propose SALSA, which bridges this gap by allowing interactions between variables, but controls model capacity by limiting the order of interactions. SALSA minimises the residual sum of squares with squared RKHS norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression with an additive kernel. When the regression function is additive, the excess risk is only polynomial in dimension. Using the Girard-Newton formulae, we efficiently sum over a combinatorial number of terms in the additive expansion. Via a comparison on $15$ real datasets, we show that our method is competitive against $21$ other alternatives. version:3
arxiv-1605-07681 | Convolutional Random Walk Networks for Semantic Image Segmentation | http://arxiv.org/abs/1605.07681 | id:1605.07681 author:Gedas Bertasius, Lorenzo Torresani, Stella X. Yu, Jianbo Shi category:cs.CV  published:2016-05-24 summary:Most current semantic segmentation methods rely on fully convolutional networks (FCNs). However, the use of large receptive fields and many pooling layers, cause blurring and low spatial resolution inside the deep layers, which often lead to spatially fragmented FCN predictions. In this work, we address this problem by introducing Convolutional Random Walk Networks (RWNs) that combine the strengths of FCNs and random walk based methods. Our proposed RWN jointly optimizes pixelwise affinity and semantic segmentation learning objectives, and combines these two sources of information via a novel random walk layer that enforces consistent spatial grouping in the deep layers of the network. We show that such a grouping mechanism improves the semantic segmentation accuracy when applied in the deep low spatial resolution FCN layers. Our proposed RWN fully integrates pixelwise affinity learning and the random walk process. This makes it possible to train the whole network in an end-to-end fashion with the standard back-propagation algorithm. Additionally, our RWN needs just 131 additional parameters compared to the state-of-the-art DeepLab network, and yet it produces an improvement of 1.5% according to the mean IOU evaluation metric on Pascal SBD dataset. version:1
arxiv-1511-06335 | Unsupervised Deep Embedding for Clustering Analysis | http://arxiv.org/abs/1511.06335 | id:1511.06335 author:Junyuan Xie, Ross Girshick, Ali Farhadi category:cs.LG cs.CV  published:2015-11-19 summary:Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods. version:2
arxiv-1603-06277 | Composing graphical models with neural networks for structured representations and fast inference | http://arxiv.org/abs/1603.06277 | id:1603.06277 author:Matthew J. Johnson, David Duvenaud, Alexander B. Wiltschko, Sandeep R. Datta, Ryan P. Adams category:stat.ML  published:2016-03-20 summary:We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping. version:2
arxiv-1602-02218 | Strongly-Typed Recurrent Neural Networks | http://arxiv.org/abs/1602.02218 | id:1602.02218 author:David Balduzzi, Muhammad Ghifary category:cs.LG cs.NE  published:2016-02-06 summary:Recurrent neural networks are increasing popular models for sequential learning. Unfortunately, although the most effective RNN architectures are perhaps excessively complicated, extensive searches have not found simpler alternatives. This paper imports ideas from physics and functional programming into RNN design to provide guiding principles. From physics, we introduce type constraints, analogous to the constraints that forbids adding meters to seconds. From functional programming, we require that strongly-typed architectures factorize into stateless learnware and state-dependent firmware, reducing the impact of side-effects. The features learned by strongly-typed nets have a simple semantic interpretation via dynamic average-pooling on one-dimensional convolutions. We also show that strongly-typed gradients are better behaved than in classical architectures, and characterize the representational power of strongly-typed nets. Finally, experiments show that, despite being more constrained, strongly-typed architectures achieve lower training and comparable generalization error to classical architectures. version:2
arxiv-1605-07659 | Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy | http://arxiv.org/abs/1605.07659 | id:1605.07659 author:Aryan Mokhtari, Alejandro Ribeiro category:cs.LG math.OC  published:2016-05-24 summary:We consider empirical risk minimization for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton's method with adaptive sample sizes. The main idea of Ada Newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set. This allows to exploit the quadratic convergence property of Newton's method and reach the statistical accuracy of each training set with only one iteration of Newton's method. We show theoretically and empirically that Ada Newton can double the size of the training set in each iteration to achieve the statistical accuracy of the full training set with about two passes over the dataset. version:1
arxiv-1605-07651 | Self Paced Deep Learning for Weakly Supervised Object Detection | http://arxiv.org/abs/1605.07651 | id:1605.07651 author:Enver Sangineto, Moin Nabi, Dubravko Culibrk, Nicu Sebe category:cs.CV  published:2016-05-24 summary:In a weakly-supervised scenario, object detectors need to be trained using image-level annotation only. Since bounding-box-level ground truth is not available, most of the solutions proposed so far are based on an iterative approach in which the classifier, obtained in the previous iteration, is used to predict the objects' positions which are used for training in the current iteration. However, the errors in these predictions can make the process drift. In this paper we propose a self-paced learning protocol to alleviate this problem. The main idea is to iteratively select a subset of samples that are most likely correct, which are used for training. While similar strategies have been recently adopted for SVMs and other classifiers, as far as we know, we are the first showing that a self-paced approach can be used with deep-net-based classifiers. We show results on Pascal VOC and ImageNet, outperforming the previous state of the art on both datasets and specifically obtaining more than 100% relative improvement on ImageNet. version:1
arxiv-1605-07650 | Blind Analysis of CT Image Noise Using Residual Denoised Images | http://arxiv.org/abs/1605.07650 | id:1605.07650 author:Sohini Roychowdhury, Nathan Hollraft, Adam Alessio category:cs.CV physics.med-ph  published:2016-05-24 summary:CT protocol design and quality control would benefit from automated tools to estimate the quality of generated CT images. These tools could be used to identify erroneous CT acquisitions or refine protocols to achieve certain signal to noise characteristics. This paper investigates blind estimation methods to determine global signal strength and noise levels in chest CT images. Methods: We propose novel performance metrics corresponding to the accuracy of noise and signal estimation. We implement and evaluate the noise estimation performance of six spatial- and frequency- based methods, derived from conventional image filtering algorithms. Algorithms were tested on patient data sets from whole-body repeat CT acquisitions performed with a higher and lower dose technique over the same scan region. Results: The proposed performance metrics can evaluate the relative tradeoff of filter parameters and noise estimation performance. The proposed automated methods tend to underestimate CT image noise at low-flux levels. Initial application of methodology suggests that anisotropic diffusion and Wavelet-transform based filters provide optimal estimates of noise. Furthermore, methodology does not provide accurate estimates of absolute noise levels, but can provide estimates of relative change and/or trends in noise levels. version:1
arxiv-1605-07648 | FractalNet: Ultra-Deep Neural Networks without Residuals | http://arxiv.org/abs/1605.07648 | id:1605.07648 author:Gustav Larsson, Michael Maire, Gregory Shakhnarovich category:cs.CV  published:2016-05-24 summary:We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a single expansion rule generates an extremely deep network whose structural layout is precisely a truncated fractal. Such a network contains interacting subpaths of different lengths, but does not include any pass-through connections: every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. This property stands in stark contrast to the current approach of explicitly structuring very deep networks so that training is a residual learning problem. Our experiments demonstrate that residual representation is not fundamental to the success of extremely deep convolutional neural networks. A fractal design achieves an error rate of 22.85% on CIFAR-100, matching the state-of-the-art held by residual networks. Fractal networks exhibit intriguing properties beyond their high performance. They can be regarded as a computationally efficient implicit union of subnetworks of every depth. We explore consequences for training, touching upon connection with student-teacher behavior, and, most importantly, demonstrating the ability to extract high-performance fixed-depth subnetworks. To facilitate this latter task, we develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. With such regularization, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer. version:1
arxiv-1605-07604 | Posterior Dispersion Indices | http://arxiv.org/abs/1605.07604 | id:1605.07604 author:Alp Kucukelbir, David M. Blei category:stat.ML cs.AI stat.CO  published:2016-05-24 summary:Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance. Evaluation drives the cycle, as we revise our model based on how it performs. This requires a metric. Traditionally, predictive accuracy prevails. Yet, predictive accuracy does not tell the whole story. We propose to evaluate a model through posterior dispersion. The idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure. We propose a family of posterior dispersion indices (PDI) that capture this idea. A PDI identifies rich patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics. version:1
arxiv-1506-08189 | Correlation Clustering and Biclustering with Locally Bounded Errors | http://arxiv.org/abs/1506.08189 | id:1506.08189 author:Gregory J. Puleo, Olgica Milenkovic category:cs.DS cs.LG  published:2015-06-26 summary:We consider a generalized version of the correlation clustering problem, defined as follows. Given a complete graph $G$ whose edges are labeled with $+$ or $-$, we wish to partition the graph into clusters while trying to avoid errors: $+$ edges between clusters or $-$ edges within clusters. Classically, one seeks to minimize the total number of such errors. We introduce a new framework that allows the objective to be a more general function of the number of errors at each vertex (for example, we may wish to minimize the number of errors at the worst vertex) and provide a rounding algorithm which converts "fractional clusterings" into discrete clusterings while causing only a constant-factor blowup in the number of errors at each vertex. This rounding algorithm yields constant-factor approximation algorithms for the discrete problem under a wide variety of objective functions. version:3
arxiv-1511-02667 | An Efficient Multilinear Optimization Framework for Hypergraph Matching | http://arxiv.org/abs/1511.02667 | id:1511.02667 author:Quynh Nguyen, Francesco Tudisco, Antoine Gautier, Matthias Hein category:cs.CV cs.DS math.OC  published:2015-11-09 summary:Hypergraph matching has recently become a popular approach for solving correspondence problems in computer vision as it allows to integrate higher-order geometric information. Hypergraph matching can be formulated as a third-order optimization problem subject to the assignment constraints which turns out to be NP-hard. In recent work, we have proposed an algorithm for hypergraph matching which first lifts the third-order problem to a fourth-order problem and then solves the fourth-order problem via optimization of the corresponding multilinear form. This leads to a tensor block coordinate ascent scheme which has the guarantee of providing monotonic ascent in the original matching score function and leads to state-of-the-art performance both in terms of achieved matching score and accuracy. In this paper we show that the lifting step to a fourth-order problem can be avoided yielding a third-order scheme with the same guarantees and performance but being two times faster. Moreover, we introduce a homotopy type method which further improves the performance. version:2
arxiv-1605-07586 | Natural Scene Image Segmentation Based on Multi-Layer Feature Extraction | http://arxiv.org/abs/1605.07586 | id:1605.07586 author:Fariba Zohrizadeh, Mohsen Kheirandishfard, Farhad Kamangar category:cs.CV  published:2016-05-24 summary:This paper addresses the problem of natural image segmentation by extracting information from a multi-layer array which is constructed based on color, gradient, and statistical properties of the local neighborhoods in an image. A Gaussian Mixture Model (GMM) is used to improve the effectiveness of local spectral histogram features. Grouping these features leads to forming a rough initial over-segmented layer which contains coherent regions of pixels. The regions are merged by using two proposed functions for calculating the distance between two neighboring regions and making decisions about their merging. Extensive experiments are performed on the Berkeley Segmentation Dataset to evaluate the performance of our proposed method and compare the results with the recent state-of-the-art methods. The experimental results indicate that our method achieves higher level of accuracy for natural images compared to recent methods. version:1
arxiv-1603-08270 | Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing | http://arxiv.org/abs/1603.08270 | id:1603.08270 author:Steven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy, Alexander Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, Dharmendra S. Modha category:cs.NE  published:2016-03-28 summary:Deep networks are now able to achieve human-level performance on a broad spectrum of recognition tasks. Independently, neuromorphic computing has now demonstrated unprecedented energy-efficiency through a new chip architecture based on spiking neurons, low precision synapses, and a scalable communication network. Here, we demonstrate that neuromorphic computing, despite its novel architectural primitives, can implement deep convolution networks that i) approach state-of-the-art classification accuracy across 8 standard datasets, encompassing vision and speech, ii) perform inference while preserving the hardware's underlying energy-efficiency and high throughput, running on the aforementioned datasets at between 1200 and 2600 frames per second and using between 25 and 275 mW (effectively > 6000 frames / sec / W) and iii) can be specified and trained using backpropagation with the same ease-of-use as contemporary deep learning. For the first time, the algorithmic power of deep learning can be merged with the efficiency of neuromorphic processors, bringing the promise of embedded, intelligent, brain-inspired computing one step closer. version:2
arxiv-1507-07646 | Learning 3D Deformation of Animals from 2D Images | http://arxiv.org/abs/1507.07646 | id:1507.07646 author:Angjoo Kanazawa, Shahar Kovalsky, Ronen Basri, David W. Jacobs category:cs.CV  published:2015-07-28 summary:Understanding how an animal can deform and articulate is essential for a realistic modification of its 3D model. In this paper, we show that such information can be learned from user-clicked 2D images and a template 3D model of the target animal. We present a volumetric deformation framework that produces a set of new 3D models by deforming a template 3D model according to a set of user-clicked images. Our framework is based on a novel locally-bounded deformation energy, where every local region has its own stiffness value that bounds how much distortion is allowed at that location. We jointly learn the local stiffness bounds as we deform the template 3D mesh to match each user-clicked image. We show that this seemingly complex task can be solved as a sequence of convex optimization problems. We demonstrate the effectiveness of our approach on cats and horses, which are highly deformable and articulated animals. Our framework produces new 3D models of animals that are significantly more plausible than methods without learned stiffness. version:3
arxiv-1605-07571 | Sequential Neural Models with Stochastic Layers | http://arxiv.org/abs/1605.07571 | id:1605.07571 author:Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, Ole Winther category:stat.ML cs.LG  published:2016-05-24 summary:How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling. version:1
arxiv-1605-03621 | ASP Vision: Optically Computing the First Layer of Convolutional Neural Networks using Angle Sensitive Pixels | http://arxiv.org/abs/1605.03621 | id:1605.03621 author:Huaijin Chen, Suren Jayasuriya, Jiyue Yang, Judy Stephen, Sriram Sivaramakrishnan, Ashok Veeraraghavan, Alyosha Molnar category:cs.CV  published:2016-05-11 summary:Deep learning using convolutional neural networks (CNNs) is quickly becoming the state-of-the-art for challenging computer vision applications. However, deep learning's power consumption and bandwidth requirements currently limit its application in embedded and mobile systems with tight energy budgets. In this paper, we explore the energy savings of optically computing the first layer of CNNs. To do so, we utilize bio-inspired Angle Sensitive Pixels (ASPs), custom CMOS diffractive image sensors which act similar to Gabor filter banks in the V1 layer of the human visual cortex. ASPs replace both image sensing and the first layer of a conventional CNN by directly performing optical edge filtering, saving sensing energy, data bandwidth, and CNN FLOPS to compute. Our experimental results (both on synthetic data and a hardware prototype) for a variety of vision tasks such as digit recognition, object recognition, and face identification demonstrate up to 90% reduction in image sensor power consumption and 90% reduction in data bandwidth from sensor to CPU, while achieving similar performance compared to traditional deep learning pipelines. version:2
arxiv-1605-06676 | Learning to Communicate with Deep Multi-Agent Reinforcement Learning | http://arxiv.org/abs/1605.06676 | id:1605.06676 author:Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, Shimon Whiteson category:cs.AI cs.LG cs.MA  published:2016-05-21 summary:We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains. version:2
arxiv-1411-1420 | Basis Learning as an Algorithmic Primitive | http://arxiv.org/abs/1411.1420 | id:1411.1420 author:Mikhail Belkin, Luis Rademacher, James Voss category:cs.LG  published:2014-11-05 summary:A number of important problems in theoretical computer science and machine learning can be interpreted as recovering a certain basis. These include symmetric matrix eigendecomposition, certain tensor decompositions, Independent Component Analysis (ICA), spectral clustering and Gaussian mixture learning. Each of these problems reduces to an instance of our general model, which we call a "Basis Encoding Function" (BEF). We show that learning a basis within this model can then be provably and efficiently achieved using a first order iteration algorithm (gradient iteration). Our algorithm goes beyond tensor methods while generalizing a number of existing algorithms---e.g., the power method for symmetric matrices, the tensor power iteration for orthogonal decomposable tensors, and cumulant-based FastICA---all within a broader function-based dynamical systems framework. Our framework also unifies the unusual phenomenon observed in these domains that they can be solved using efficient non-convex optimization. Specifically, we describe a class of BEFs such that their local maxima on the unit sphere are in one-to-one correspondence with the basis elements. This description relies on a certain "hidden convexity" property of these functions. We provide a complete theoretical analysis of the gradient iteration even when the BEF is perturbed. We show convergence and complexity bounds polynomial in dimension and other relevant parameters, such as perturbation size. Our perturbation results can be considered as a non-linear version of the classical Davis-Kahan theorem for perturbations of eigenvectors of symmetric matrices. In addition we show that our algorithm exhibits fast (superlinear) convergence and relate the speed of convergence to the properties of the BEF. Moreover, the gradient iteration algorithm can be easily and efficiently implemented in practice. version:4
arxiv-1601-07243 | On the Sample Complexity of Learning Sparse Graphical Games | http://arxiv.org/abs/1601.07243 | id:1601.07243 author:Jean Honorio category:cs.GT cs.LG stat.ML  published:2016-01-27 summary:We analyze the sample complexity of learning sparse graphical games from purely behavioral data. That is, we assume that we can only observe the players' joint actions and not their payoffs. We analyze the sufficient and necessary number of samples for the correct recovery of the set of pure-strategy Nash equilibria (PSNE) of the true game. Our analysis focuses on sparse directed graphs with $n$ nodes and at most $k$ parents per node. By using VC dimension arguments, we show that if the number of samples is greater than ${O(k n \log^2{n})}$, then maximum likelihood estimation correctly recovers the PSNE with high probability. By using information-theoretic arguments, we show that if the number of samples is less than ${\Omega(k n \log^2{n})}$, then any conceivable method fails to recover the PSNE with arbitrary probability. version:2
arxiv-1604-02619 | TextProposals: a Text-specific Selective Search Algorithm for Word Spotting in the Wild | http://arxiv.org/abs/1604.02619 | id:1604.02619 author:Lluis Gomez-Bigorda, Dimosthenis Karatzas category:cs.CV  published:2016-04-10 summary:Motivated by the success of powerful while expensive techniques to recognize words in a holistic way, object proposals techniques emerge as an alternative to the traditional text detectors. In this paper we introduce a novel object proposals method that is specifically designed for text. We rely on a similarity based region grouping algorithm that generates a hierarchy of word hypotheses. Over the nodes of this hierarchy it is possible to apply a holistic word recognition method in an efficient way. Our experiments demonstrate that the presented method is superior in its ability of producing good quality word proposals when compared with class-independent algorithms. We show impressive recall rates with a few thousand proposals in different standard benchmarks, including focused or incidental text datasets, and multi-language scenarios. Moreover, the combination of our object proposals with existing whole-word recognizers shows competitive performance in end-to-end word spotting, and, in some benchmarks, outperforms previously published results. Concretely, in the challenging ICDAR2015 Incidental Text dataset, we overcome in more than 10 percent f-score the best-performing method in the last ICDAR Robust Reading Competition. Source code of the complete end-to-end system is available at https://github.com/lluisgomez/TextProposals version:2
arxiv-1605-07541 | Inductive quantum learning: Why you are doing it almost right | http://arxiv.org/abs/1605.07541 | id:1605.07541 author:Alex Monràs, Gael Sentís, Peter Wittek category:cs.LG quant-ph stat.ML  published:2016-05-24 summary:In supervised learning, an inductive learning algorithm extracts general rules from observed training instances, then the rules are applied to test instances. We show that this splitting of training and application arises naturally, in the classical setting, from a simple independence requirement with a physical interpretation of being non-signalling. Thus, two seemingly different definitions of inductive learning happen to coincide. This follows from very specific properties of classical information, which break down in the quantum setup. We prove a quantum de Finetti theorem for quantum channels, which shows that in the quantum case, the equivalence holds in the asymptotic setting (for large number of test instances). This reveals a natural analogy between classical learning protocols and their quantum counterparts, thus allowing to naturally enquire about standard elements in computational learning theory, such as structural risk minimization, model and sample complexity. version:1
arxiv-1511-07053 | ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation | http://arxiv.org/abs/1511.07053 | id:1511.07053 author:Francesco Visin, Marco Ciccone, Adriana Romero, Kyle Kastner, Kyunghyun Cho, Yoshua Bengio, Matteo Matteucci, Aaron Courville category:cs.CV cs.LG  published:2015-11-22 summary:We propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse, Oxford Flower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on https://github.com/fvisin/reseg. version:3
arxiv-1605-07515 | Neural Semantic Role Labeling with Dependency Path Embeddings | http://arxiv.org/abs/1605.07515 | id:1605.07515 author:Michael Roth, Mirella Lapata category:cs.CL  published:2016-05-24 summary:This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as sub-sequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method. version:1
arxiv-1511-06430 | Deconstructing the Ladder Network Architecture | http://arxiv.org/abs/1511.06430 | id:1511.06430 author:Mohammad Pezeshki, Linxi Fan, Philemon Brakel, Aaron Courville, Yoshua Bengio category:cs.LG  published:2015-11-19 summary:The Manual labeling of data is and will remain a costly endeavor. For this reason, semi-supervised learning remains a topic of practical importance. The recently proposed Ladder Network is one such approach that has proven to be very successful. In addition to the supervised objective, the Ladder Network also adds an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. Although the empirical results are impressive, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. In order to help elucidate and disentangle the different ingredients in the Ladder Network recipe, this paper presents an extensive experimental investigation of variants of the Ladder Network in which we replace or remove individual components to gain more insight into their relative importance. We find that all of the components are necessary for achieving optimal performance, but they do not contribute equally. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connection, followed by the application of noise, and finally the choice of what we refer to as the `combinator function' in the decoder path. We also find that as the number of labeled training examples increases, the lateral connections and reconstruction criterion become less important, with most of the improvement in generalization being due to the injection of noise in each layer. Furthermore, we present a new type of combinator function that outperforms the original design in both fully- and semi-supervised tasks, reducing record test error rates on Permutation-Invariant MNIST to 0.57% for the supervised setting, and to 0.97% and 1.0% for semi-supervised settings with 1000 and 100 labeled examples respectively. version:4
arxiv-1605-07511 | A note on privacy preserving iteratively reweighted least squares | http://arxiv.org/abs/1605.07511 | id:1605.07511 author:Mijung Park, Max Welling category:cs.CR cs.AI stat.AP stat.ML  published:2016-05-24 summary:Iteratively reweighted least squares (IRLS) is a widely-used method in machine learning to estimate the parameters in the generalised linear models. In particular, IRLS for L1 minimisation under the linear model provides a closed-form solution in each step, which is a simple multiplication between the inverse of the weighted second moment matrix and the weighted first moment vector. When dealing with privacy sensitive data, however, developing a privacy preserving IRLS algorithm faces two challenges. First, due to the inversion of the second moment matrix, the usual sensitivity analysis in differential privacy incorporating a single datapoint perturbation gets complicated and often requires unrealistic assumptions. Second, due to its iterative nature, a significant cumulative privacy loss occurs. However, adding a high level of noise to compensate for the privacy loss hinders from getting accurate estimates. Here, we develop a practical algorithm that overcomes these challenges and outputs privatised and accurate IRLS solutions. In our method, we analyse the sensitivity of each moments separately and treat the matrix inversion and multiplication as a post-processing step, which simplifies the sensitivity analysis. Furthermore, we apply the {\it{concentrated differential privacy}} formalism, a more relaxed version of differential privacy, which requires adding a significantly less amount of noise for the same level of privacy guarantee, compared to the conventional and advanced compositions of differentially private mechanisms. version:1
arxiv-1506-00406 | Monolingually Derived Phrase Scores for Phrase Based SMT Using Neural Networks Vector Representations | http://arxiv.org/abs/1506.00406 | id:1506.00406 author:Amir Pouya Aghasadeghi, Mohadeseh Bastan category:cs.CL  published:2015-06-01 summary:In this paper, we propose two new features for estimating phrase-based machine translation parameters from mainly monolingual data. Our method is based on two recently introduced neural network vector representation models for words and sentences. It is the first time that these models have been used in an end to end phrase-based machine translation system. Scores obtained from our method can recover more than 80% of BLEU loss caused by removing phrase table probabilities. We also show that our features combined with the phrase table probabilities improve the BLEU score by absolute 0.74 points. version:3
arxiv-1605-07498 | Leveraging Over Priors for Boosting Control of Prosthetic Hands | http://arxiv.org/abs/1605.07498 | id:1605.07498 author:Valentina Gregori category:cs.LG  published:2016-05-24 summary:The Electromyography (EMG) signal is the electrical activity produced by cells of skeletal muscles in order to provide a movement. The non-invasive prosthetic hand works with several electrodes, placed on the stump of an amputee, that record this signal. In order to favour the control of prosthesis, the EMG signal is analyzed with algorithms based on machine learning theory to decide the movement that the subject is going to do. In order to obtain a significant control of the prosthesis and avoid mismatch between desired and performed movements, a long training period is needed when we use the traditional algorithm of machine learning (i.e. Support Vector Machines). An actual challenge in this field concerns the reduction of the time necessary for an amputee to learn how to use the prosthesis. Recently, several algorithms that exploit a form of prior knowledge have been proposed. In general, we refer to prior knowledge as a past experience available in the form of models. In our case an amputee, that attempts to perform some movements with the prosthesis, could use experience from different subjects that are already able to perform those movements. The aim of this work is to verify, with a computational investigation, if for an amputee this kind of previous experience is useful in order to reduce the training time and boost the prosthetic control. Furthermore, we want to understand if and how the final results change when the previous knowledge of intact or amputated subjects is used for a new amputee. Our experiments indicate that: (1) the use of experience, from other subjects already trained to perform a task, makes us able to reduce the training time of about an order of magnitude; (2) it seems that an amputee that tries to learn to use the prosthesis doesn't reach different results when he/she exploits previous experience of amputees or intact. version:1
arxiv-1605-07496 | Alternating Optimisation and Quadrature for Robust Reinforcement Learning | http://arxiv.org/abs/1605.07496 | id:1605.07496 author:Supratik Paul, Kamil Ciosek, Michael A. Osborne, Shimon Whiteson category:cs.LG cs.AI stat.ML  published:2016-05-24 summary:Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables - state features that are randomly determined by the environment in a physical setting but are controllable in a simulator. This paper considers the problem of finding an optimal policy while taking into account the impact of environment variables. We present the alternating optimisation and quadrature algorithm which uses Bayesian optimisation and Bayesian quadrature to address such settings and is robust to the presence of significant rare events, which may not be observable under random sampling but have a considerable impact on determining the optimal policy. Our experimental results show that our approach learns better and faster than existing methods. version:1
arxiv-1602-02722 | Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations | http://arxiv.org/abs/1602.02722 | id:1602.02722 author:Akshay Krishnamurthy, Alekh Agarwal, John Langford category:cs.LG stat.ML  published:2016-02-08 summary:We propose and study a new tractable model for reinforcement learning with rich observations called Contextual-MDPs, generalizing contextual bandits to sequential decision making. These models require an agent to take actions based on observations (features) with the goal of achieving long-term performance competitive with a large set of policies. To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, Contextual-MDPs can be summarized by a small number of hidden states and long-term rewards are predictable by a reactive function class. In this setting, we design a new reinforcement learning algorithm that engages in global exploration and analyze its sample complexity. We prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. This represents an exponential improvement over all existing alternative approaches and provides theoretical justification for reinforcement learning with function approximation. version:3
arxiv-1605-02536 | Random Fourier Features for Operator-Valued Kernels | http://arxiv.org/abs/1605.02536 | id:1605.02536 author:Romain Brault, Florence d'Alché-Buc, Markus Heinonen category:cs.LG  published:2016-05-09 summary:Devoted to multi-task learning and structured output learning, operator-valued kernels provide a flexible tool to build vector-valued functions in the context of Reproducing Kernel Hilbert Spaces. To scale up these methods, we extend the celebrated Random Fourier Feature methodology to get an approximation of operator-valued kernels. We propose a general principle for Operator-valued Random Fourier Feature construction relying on a generalization of Bochner's theorem for translation-invariant operator-valued Mercer kernels. We prove the uniform convergence of the kernel approximation for bounded and unbounded operator random Fourier features using appropriate Bernstein matrix concentration inequality. An experimental proof-of-concept shows the quality of the approximation and the efficiency of the corresponding linear models on example datasets. version:3
arxiv-1605-07427 | Hierarchical Memory Networks | http://arxiv.org/abs/1605.07427 | id:1605.07427 author:Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, Yoshua Bengio category:stat.ML cs.CL cs.LG cs.NE  published:2016-05-24 summary:Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task. version:1
arxiv-1605-01677 | Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm | http://arxiv.org/abs/1605.01677 | id:1605.01677 author:Junpei Komiyama, Junya Honda, Hiroshi Nakagawa category:stat.ML cs.LG  published:2016-05-05 summary:We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. The hardness of recommending Copeland winners, the arms that beat the greatest number of other arms, is characterized by deriving an asymptotic regret bound. We propose Copeland Winners Relative Minimum Empirical Divergence (CW-RMED) and derive an asymptotically optimal regret bound for it. However, it is not known whether the algorithm can be efficiently computed or not. To address this issue, we devise an efficient version (ECW-RMED) and derive its asymptotic regret bound. Experimental comparisons of dueling bandit algorithms show that ECW-RMED significantly outperforms existing ones. version:2
arxiv-1605-07422 | Web-scale Topic Models in Spark: An Asynchronous Parameter Server | http://arxiv.org/abs/1605.07422 | id:1605.07422 author:Rolf Jagerman, Carsten Eickhoff category:cs.DC cs.IR cs.LG stat.ML  published:2016-05-24 summary:In this paper, we train a Latent Dirichlet Allocation (LDA) topic model on the ClueWeb12 data set, a 27-terabyte Web crawl. We extend Spark, a popular framework for performing large-scale data analysis, with an asynchronous parameter server. Such a parameter server provides a distributed and concurrently accessed parameter space for the model. A Metropolis-Hastings based collapsed Gibbs sampler is implemented using this parameter server achieving an amortized O(1) sampling complexity. We compare our implementation to the default Spark implementations and show that it is significantly faster and more scalable without sacrificing model quality. A topic model with 1,000 topics is trained on the full ClueWeb12 data set, uncovering some of the prevalent themes that appear on the Web. version:1
arxiv-1605-07416 | Refined Lower Bounds for Adversarial Bandits | http://arxiv.org/abs/1605.07416 | id:1605.07416 author:Sébastien Gerchinovitz, Tor Lattimore category:math.ST cs.LG stat.ML stat.TH  published:2016-05-24 summary:We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total lossof the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting. version:1
arxiv-1506-00779 | Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays | http://arxiv.org/abs/1506.00779 | id:1506.00779 author:Junpei Komiyama, Junya Honda, Hiroshi Nakagawa category:stat.ML cs.LG  published:2015-06-02 summary:We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS for binary rewards has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance. version:2
arxiv-1603-04015 | Learning zeroth class dictionary for human action recognition | http://arxiv.org/abs/1603.04015 | id:1603.04015 author:Jia-xin Cai, Xin Tang, Lifang Zhang, Guocan Feng category:cs.CV  published:2016-03-13 summary:In this paper, a discriminative two-phase dictionary learning framework is proposed for classifying human action by sparse shape representations, in which the first-phase dictionary is learned on the selected discriminative frames and the second-phase dictionary is built for recognition using reconstruction errors of the first-phase dictionary as input features. We propose a "zeroth class" trick for detecting undiscriminating frames of the test video and eliminating them before voting on the action categories. Experimental results on benchmarks demonstrate the effectiveness of our method. version:2
arxiv-1605-07371 | Semiparametric energy-based probabilistic models | http://arxiv.org/abs/1605.07371 | id:1605.07371 author:Jan Humplik, Gašper Tkačik category:q-bio.NC cond-mat.stat-mech stat.ML  published:2016-05-24 summary:Probabilistic models can be defined by an energy function, where the probability of each state is proportional to the exponential of the state's negative energy. This paper considers a generalization of energy-based models in which the probability of a state is proportional to an arbitrary positive, strictly decreasing, and twice differentiable function of the state's energy. The precise shape of the nonlinear map from energies to unnormalized probabilities has to be learned from data together with the parameters of the energy function. As a case study we show that the above generalization of a fully visible Boltzmann machine yields an accurate model of neural activity of retinal ganglion cells. We attribute this success to the model's ability to easily capture distributions whose probabilities span a large dynamic range, a possible consequence of latent variables that globally couple the system. Similar features have recently been observed in many datasets, suggesting that our new method has wide applicability. version:1
arxiv-1605-07369 | Quickest Moving Object Detection | http://arxiv.org/abs/1605.07369 | id:1605.07369 author:Dong Lao, Ganesh Sundaramoorthi category:cs.CV  published:2016-05-24 summary:We present a general framework and method for simultaneous detection and segmentation of an object in a video that moves (or comes into view of the camera) at some unknown time in the video. The method is an online approach based on motion segmentation, and it operates under dynamic backgrounds caused by a moving camera or moving nuisances. The goal of the method is to detect and segment the object as soon as it moves. Due to stochastic variability in the video and unreliability of the motion signal, several frames are needed to reliably detect the object. The method is designed to detect and segment with minimum delay subject to a constraint on the false alarm rate. The method is derived as a problem of Quickest Change Detection. Experiments on a dataset show the effectiveness of our method in minimizing detection delay subject to false alarm constraints. version:1
arxiv-1605-04812 | Off-policy evaluation for slate recommendation | http://arxiv.org/abs/1605.04812 | id:1605.04812 author:Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudík, John Langford, Damien Jose, Imed Zitouni category:cs.LG cs.AI stat.ML  published:2016-05-16 summary:This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context---a common scenario in web search, ads and recommender systems. We develop the first practical technique for evaluating page-level metrics of such policies offline using logged past data, alleviating the need for online A/B tests. Our method models the observed quality of the recommended set (e.g., time to success in web search) as an additive decomposition across items. Crucially, the per-item quality is not directly observed or easily modeled from the item's features. A thorough empirical evaluation reveals that this model fits many realistic measures of quality and theoretical analysis shows exponential savings in the amount of required data compared with prior off-policy evaluation approaches. version:2
arxiv-1605-07366 | Experiments in Linear Template Combination using Genetic Algorithms | http://arxiv.org/abs/1605.07366 | id:1605.07366 author:Nikhilesh Bhatnagar, Radhika Mamidi category:cs.CL 68T50  published:2016-05-24 summary:Natural Language Generation systems typically have two parts - strategic ('what to say') and tactical ('how to say'). We present our experiments in building an unsupervised corpus-driven template based tactical NLG system. We consider templates as a sequence of words containing gaps. Our idea is based on the observation that templates are grammatical locally (within their textual span). We posit the construction of a sentence as a highly restricted sequence of such templates. This work is an attempt to explore the resulting search space using Genetic Algorithms to arrive at acceptable solutions. We present a baseline implementation of this approach which outputs gapped text. version:1
arxiv-1605-07363 | Spatio-Temporal Image Boundary Extrapolation | http://arxiv.org/abs/1605.07363 | id:1605.07363 author:Apratim Bhattacharyya, Mateusz Malinowski, Mario Fritz category:cs.CV  published:2016-05-24 summary:Boundary prediction in images as well as video has been a very active topic of research and organizing visual information into boundaries and segments is believed to be a corner stone of visual perception. While prior work has focused on predicting boundaries for observed frames, our work aims at predicting boundaries of future unobserved frames. This requires our model to learn about the fate of boundaries and extrapolate motion patterns. We experiment on established real-world video segmentation dataset, which provides a testbed for this new task. We show for the first time spatio-temporal boundary extrapolation in this challenging scenario. Furthermore, we show long-term prediction of boundaries in situations where the motion is governed by the laws of physics. We successfully predict boundaries in a billiard scenario without any assumptions of a strong parametric model or any object notion. We argue that our model has with minimalistic model assumptions derived a notion of 'intuitive physics' that can be applied to novel scenes. version:1
arxiv-1605-07358 | Consistency Analysis for the Doubly Stochastic Dirichlet Process | http://arxiv.org/abs/1605.07358 | id:1605.07358 author:Xing Sun, Nelson H. C. Yung, Edmund Y. Lam, Hayden K. -H. So category:cs.IT math.IT stat.ML  published:2016-05-24 summary:This technical report proves components consistency for the Doubly Stochastic Dirichlet Process with exponential convergence of posterior probability. We also present the fundamental properties for DSDP as well as inference algorithms. Simulation toy experiment and real-world experiment results for single and multi-cluster also support the consistency proof. This report is also a support document for the paper "Computationally Efficient Hyperspectral Data Learning Based on the Doubly Stochastic Dirichlet Process". version:1
arxiv-1605-07346 | Multi-Level Analysis and Annotation of Arabic Corpora for Text-to-Sign Language MT | http://arxiv.org/abs/1605.07346 | id:1605.07346 author:Abdelaziz Lakhfif, Mohammed T. Laskri, Eric Atwell category:cs.CL  published:2016-05-24 summary:In this paper, we present an ongoing effort in lexical semantic analysis and annotation of Modern Standard Arabic (MSA) text, a semi automatic annotation tool concerned with the morphologic, syntactic, and semantic levels of description. version:1
arxiv-1605-07334 | Near-optimal Bayesian Active Learning with Correlated and Noisy Tests | http://arxiv.org/abs/1605.07334 | id:1605.07334 author:Yuxin Chen, S. Hamed Hassani, Andreas Krause category:cs.LG cs.AI  published:2016-05-24 summary:We consider the Bayesian active learning and experimental design problem, where the goal is to learn the value of some unknown target variable through a sequence of informative, noisy tests. In contrast to prior work, we focus on the challenging, yet practically relevant setting where test outcomes can be conditionally dependent given the hidden target variable. Under such assumptions, common heuristics, such as greedily performing tests that maximize the reduction in uncertainty of the target, often perform poorly. In this paper, we propose ECED, a novel, computationally efficient active learning algorithm, and prove strong theoretical guarantees that hold with correlated, noisy tests. Rather than directly optimizing the prediction error, at each step, ECED picks the test that maximizes the gain in a surrogate objective, which takes into account the dependencies between tests. Our analysis relies on an information-theoretic auxiliary function to track the progress of ECED, and utilizes adaptive submodularity to attain the near-optimal bound. We demonstrate strong empirical performance of ECED on two problem instances, including a Bayesian experimental design task intended to distinguish among economic theories of how people make risky decisions, and an active preference learning task via pairwise comparisons. version:1
arxiv-1605-07333 | Combining Recurrent and Convolutional Neural Networks for Relation Classification | http://arxiv.org/abs/1605.07333 | id:1605.07333 author:Ngoc Thang Vu, Heike Adel, Pankaj Gupta, Hinrich Schütze category:cs.CL  published:2016-05-24 summary:This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task. version:1
arxiv-1605-07332 | Relevant sparse codes with variational information bottleneck | http://arxiv.org/abs/1605.07332 | id:1605.07332 author:Matthew Chalk, Olivier Marre, Gasper Tkacik category:stat.ML  published:2016-05-24 summary:In many applications, it is desirable to extract only the relevant aspects of data. A principled way to do this is the information bottleneck (IB) method, where one seeks a code that maximizes information about a 'relevance' variable, Y, while constraining the information encoded about the original data, X. Unfortunately however, the IB method is computationally demanding when data are high-dimensional and/or non-gaussian. Here we propose an approximate variational scheme for maximizing a lower bound on the IB objective, analogous to variational EM. Using this method, we derive an IB algorithm to recover features that are both relevant and sparse. Finally, we demonstrate how kernelized versions of the algorithm can be used to address a broad range of problems with non-linear relation between X and Y. version:1
arxiv-1602-01064 | Minimum Regret Search for Single- and Multi-Task Optimization | http://arxiv.org/abs/1602.01064 | id:1602.01064 author:Jan Hendrik Metzen category:stat.ML cs.IT cs.LG cs.RO math.IT  published:2016-02-02 summary:We propose minimum regret search (MRS), a novel acquisition function for Bayesian optimization. MRS bears similarities with information-theoretic approaches such as entropy search (ES). However, while ES aims in each query at maximizing the information gain with respect to the global maximum, MRS aims at minimizing the expected simple regret of its ultimate recommendation for the optimum. While empirically ES and MRS perform similar in most of the cases, MRS produces fewer outliers with high simple regret than ES. We provide empirical results both for a synthetic single-task optimization problem as well as for a simulated multi-task robotic control problem. version:3
arxiv-1605-07314 | DeepText: A Unified Framework for Text Proposal Generation and Text Detection in Natural Images | http://arxiv.org/abs/1605.07314 | id:1605.07314 author:Zhuoyao Zhong, Lianwen Jin, Shuye Zhang, Ziyong Feng category:cs.CV  published:2016-05-24 summary:In this paper, we develop a novel unified framework called DeepText for text region proposal generation and text detection in natural images via a fully convolutional neural network (CNN). First, we propose the inception region proposal network (Inception-RPN) and design a set of text characteristic prior bounding boxes to achieve high word recall with only hundred level candidate proposals. Next, we present a powerful textdetection network that embeds ambiguous text category (ATC) information and multilevel region-of-interest pooling (MLRP) for text and non-text classification and accurate localization. Finally, we apply an iterative bounding box voting scheme to pursue high recall in a complementary manner and introduce a filtering algorithm to retain the most suitable bounding box, while removing redundant inner and outer boxes for each text instance. Our approach achieves an F-measure of 0.83 and 0.85 on the ICDAR 2011 and 2013 robust text detection benchmarks, outperforming previous state-of-the-art results. version:1
arxiv-1605-05422 | Optimization Beyond Prediction: Prescriptive Price Optimization | http://arxiv.org/abs/1605.05422 | id:1605.05422 author:Shinji Ito, Ryohei Fujimaki category:math.OC cs.LG stat.ML  published:2016-05-18 summary:This paper addresses a novel data science problem, prescriptive price optimization, which derives the optimal price strategy to maximize future profit/revenue on the basis of massive predictive formulas produced by machine learning. The prescriptive price optimization first builds sales forecast formulas of multiple products, on the basis of historical data, which reveal complex relationships between sales and prices, such as price elasticity of demand and cannibalization. Then, it constructs a mathematical optimization problem on the basis of those predictive formulas. We present that the optimization problem can be formulated as an instance of binary quadratic programming (BQP). Although BQP problems are NP-hard in general and computationally intractable, we propose a fast approximation algorithm using a semi-definite programming (SDP) relaxation, which is closely related to the Goemans-Williamson's Max-Cut approximation. Our experiments on simulation and real retail datasets show that our prescriptive price optimization simultaneously derives the optimal prices of tens/hundreds products with practical computational time, that potentially improve 8.2% of gross profit of those products. version:2
arxiv-1304-5504 | Optimal Stochastic Strongly Convex Optimization with a Logarithmic Number of Projections | http://arxiv.org/abs/1304.5504 | id:1304.5504 author:Jianhui Chen, Tianbao Yang, Qihang Lin, Lijun Zhang, Yi Chang category:cs.LG stat.ML  published:2013-04-19 summary:We consider stochastic strongly convex optimization with a complex inequality constraint. This complex inequality constraint may lead to computationally expensive projections in algorithmic iterations of the stochastic gradient descent~(SGD) methods. To reduce the computation costs pertaining to the projections, we propose an Epoch-Projection Stochastic Gradient Descent~(Epro-SGD) method. The proposed Epro-SGD method consists of a sequence of epochs; it applies SGD to an augmented objective function at each iteration within the epoch, and then performs a projection at the end of each epoch. Given a strongly convex optimization and for a total number of $T$ iterations, Epro-SGD requires only $\log(T)$ projections, and meanwhile attains an optimal convergence rate of $O(1/T)$, both in expectation and with a high probability. To exploit the structure of the optimization problem, we propose a proximal variant of Epro-SGD, namely Epro-ORDA, based on the optimal regularized dual averaging method. We apply the proposed methods on real-world applications; the empirical results demonstrate the effectiveness of our methods. version:6
arxiv-1605-07289 | EventNet Version 1.1 Technical Report | http://arxiv.org/abs/1605.07289 | id:1605.07289 author:Dongang Wang, Zheng Shou, Hongyi Liu, Shih-Fu Chang category:cs.CV  published:2016-05-24 summary:EventNet is a large-scale video corpus and event ontology consisting of 500 events associated with event-specific concepts. In order to improve the quality of the current EventNet, we conduct the following steps and introduce EventNet version 1.1: (1) manually verify the correctness of event labels for all videos; (2) remove the YouTube user bias by limiting the maximum number of videos in each event from the same YouTube user as 3; (3) remove the videos which are currently not accessible online; (4) remove the video belonging to multiple event categories. After the above procedure, some events may contain only a small number of videos, and therefore we crawl more videos for those events to ensure every event will contain more than 50 videos. Finally, EventNet version 1.1 contains 67,641 videos, 500 events, and 5,028 event-specific concepts. In addition, we train a Convolutional Neural Network (CNN) model for event classification via fine-tuning AlexNet using EventNet version 1.1. Then we use the trained CNN model to extract FC7 layer feature and train binary classifiers using linear SVM for each event-specific concept. We believe this new version of EventNet will significantly significantly facilitate research in computer vision and multimedia, and will put it online for public downloading in the future. version:1
arxiv-1511-08531 | Structured learning of metric ensembles with application to person re-identification | http://arxiv.org/abs/1511.08531 | id:1511.08531 author:Sakrapee Paisitkriangkrai, Lin Wu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2015-11-27 summary:Matching individuals across non-overlapping camera networks, known as person re-identification, is a fundamentally challenging problem due to the large visual appearance changes caused by variations of viewpoints, lighting, and occlusion. Approaches in literature can be categoried into two streams: The first stream is to develop reliable features against realistic conditions by combining several visual features in a pre-defined way; the second stream is to learn a metric from training data to ensure strong inter-class differences and intra-class similarities. However, seeking an optimal combination of visual features which is generic yet adaptive to different benchmarks is a unsoved problem, and metric learning models easily get over-fitted due to the scarcity of training data in person re-identification. In this paper, we propose two effective structured learning based approaches which explore the adaptive effects of visual features in recognizing persons in different benchmark data sets. Our framework is built on the basis of multiple low-level visual features with an optimal ensemble of their metrics. We formulate two optimization algorithms, CMCtriplet and CMCstruct, which directly optimize evaluation measures commonly used in person re-identification, also known as the Cumulative Matching Characteristic (CMC) curve. version:2
arxiv-1511-09337 | Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning | http://arxiv.org/abs/1511.09337 | id:1511.09337 author:Yu-An Chung, Hsuan-Tien Lin, Shao-Wen Yang category:cs.LG cs.NE  published:2015-11-30 summary:Deep learning has been one of the most prominent machine learning techniques nowadays, being the state-of-the-art on a broad range of applications where automatic feature extraction is needed. Many such applications also demand varying costs for different types of mis-classification errors, but it is not clear whether or how such cost information can be incorporated into deep learning to improve performance. In this work, we propose a novel cost-aware algorithm that takes into account the cost information into not only the training stage but also the pre-training stage of deep learning. The approach allows deep learning to conduct automatic feature extraction with the cost information effectively. Extensive experimental results demonstrate that the proposed approach outperforms other deep learning models that do not digest the cost information in the pre-training stage. version:3
arxiv-1605-07277 | Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples | http://arxiv.org/abs/1605.07277 | id:1605.07277 author:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow category:cs.CR cs.LG  published:2016-05-24 summary:Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure. version:1
arxiv-1605-07272 | Matrix Completion has No Spurious Local Minimum | http://arxiv.org/abs/1605.07272 | id:1605.07272 author:Rong Ge, Jason D. Lee, Tengyu Ma category:cs.LG cs.DS stat.ML  published:2016-05-24 summary:Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for matrix completion has no spurious local minima -- all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve matrix completion with \textit{arbitrary} initialization in polynomial time. version:1
arxiv-1605-07270 | Learning a Metric Embedding for Face Recognition using the Multibatch Method | http://arxiv.org/abs/1605.07270 | id:1605.07270 author:Oren Tadmor, Yonatan Wexler, Tal Rosenwein, Shai Shalev-Shwartz, Amnon Shashua category:cs.CV  published:2016-05-24 summary:This work is motivated by the engineering task of achieving a near state-of-the-art face recognition on a minimal computing budget running on an embedded system. Our main technical contribution centers around a novel training method, called Multibatch, for similarity learning, i.e., for the task of generating an invariant "face signature" through training pairs of "same" and "not-same" face images. The Multibatch method first generates signatures for a mini-batch of $k$ face images and then constructs an unbiased estimate of the full gradient by relying on all $k^2-k$ pairs from the mini-batch. We prove that the variance of the Multibatch estimator is bounded by $O(1/k^2)$, under some mild conditions. In contrast, the standard gradient estimator that relies on random $k/2$ pairs has a variance of order $1/k$. The smaller variance of the Multibatch estimator significantly speeds up the convergence rate of stochastic gradient descent. Using the Multibatch method we train a deep convolutional neural network that achieves an accuracy of $98.2\%$ on the LFW benchmark, while its prediction runtime takes only $30$msec on a single ARM Cortex A9 core. Furthermore, the entire training process took only 12 hours on a single Titan X GPU. version:1
arxiv-1605-07268 | Classifying discourse in a CSCL platform to evaluate correlations with Teacher Participation and Progress | http://arxiv.org/abs/1605.07268 | id:1605.07268 author:Eliana Scheihing, Matthieu Vernier, Javiera Born, Julio Guerra, Luis Carcamo category:cs.CY cs.CL  published:2016-05-24 summary:In Computer-Supported learning, monitoring and engaging a group of learners is a complex task for teachers, especially when learners are working collaboratively: Are my students motivated? What kind of progress are they making? Should I intervene? Is my communication and the didactic design adapted to my students? Our hypothesis is that the analysis of natural language interactions between students, and between students and teachers, provide very valuable information and could be used to produce qualitative indicators to help teachers' decisions. We develop an automatic approach in three steps (1) to explore the discursive functions of messages in a CSCL platform, (2) to classify the messages automatically and (3) to evaluate correlations between discursive attitudes and other variables linked to the learning activity. Results tend to show that some types of discourse are correlated with a notion of Progress on the learning activities and the importance of emotive participation from the Teacher. version:1
arxiv-1605-07264 | Trajectory probability hypothesis density filter | http://arxiv.org/abs/1605.07264 | id:1605.07264 author:Ángel F. García-Fernández, Lennart Svensson category:stat.AP cs.CV  published:2016-05-24 summary:This paper presents the probability hypothesis density (PHD) filter for sets of trajectories. The resulting filter, which is referred to as trajectory probability density filter (TPHD), is capable of estimating trajectories in a principled way without requiring to evaluate all measurement-to-target association hypotheses. As the PHD filter, the TPHD filter is based on recursively obtaining the best Poisson approximation to the multitrajectory filtering density in the sense of minimising the Kullback-Leibler divergence. We also propose a Gaussian mixture implementation of the TPHD recursion, the Gaussian mixture TPHD (GMTPHD), and a computationally efficient implementation, the L-scan GMTPHD, which only updates the PDF of the trajectory states of the last L time steps. version:1
arxiv-1605-07262 | Measuring Neural Net Robustness with Constraints | http://arxiv.org/abs/1605.07262 | id:1605.07262 author:Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, Antonio Criminisi category:cs.LG cs.CV cs.NE  published:2016-05-24 summary:Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness "overfit" to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics. version:1
arxiv-1605-06892 | Accelerated Stochastic Mirror Descent Algorithms For Composite Non-strongly Convex Optimization | http://arxiv.org/abs/1605.06892 | id:1605.06892 author:Le Thi Khanh Hien, Canyi Lu, Huan Xu, Jiashi Feng category:math.OC stat.ML  published:2016-05-23 summary:We consider the problem of minimizing the sum of the average function consisting of a large number of smooth convex component functions and a general convex function that can be non-differentiable. Although many methods have been proposed to solve the problem with the assumption that the sum is strongly convex, few methods support the non-strongly convex cases. Adding a small quadratic regularization is the common trick used to tackle non-strongly convex problems; however, it may worsen certain qualities of solutions or weaken the performance of the algorithms. Avoiding this trick, we extend the deterministic accelerated proximal gradient methods of Paul Tseng to randomized versions for solving the problem without the strongly convex assumption. Our algorithms achieve the optimal convergence rate $O(\nicefrac{1}{k^2})$. Tuning involved parameters helps our algorithms get better complexity compared with the deterministic accelerated proximal gradient methods. We also propose a scheme for non-smooth problem. version:2
arxiv-1605-07254 | Convergence guarantees for kernel-based quadrature rules in misspecified settings | http://arxiv.org/abs/1605.07254 | id:1605.07254 author:Motonobu Kanagawa, Bharath K. Sriperumbudur, Kenji Fukumizu category:stat.ML  published:2016-05-24 summary:Kernel-based quadrature rules are powerful tools for numerical integration which yield convergence rates much faster than usual Mote Carlo methods. These rules are constructed based on the assumption that the integrand has a certain degree of smoothness, and this assumption is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However, in practice such an assumption can be violated, and no general theory has been established for the convergence in such misspecified cases. In this paper, we prove that kernel quadrature rules can be consistent even when an integrand does not belong to an assumed RKHS, i.e., when the integrand is less smooth than assumed. We derive convergence rates that depend on the (unknown) smoothness of the integrand, where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces. version:1
arxiv-1605-07252 | Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models | http://arxiv.org/abs/1605.07252 | id:1605.07252 author:Marc Vuffray, Sidhant Misra, Andrey Y. Lokhov, Michael Chertkov category:cs.LG stat.ML  published:2016-05-24 summary:We consider the problem of learning the underlying graph of an unknown Ising model on p spins from a collection of i.i.d. samples generated from the model. We suggest a new estimator that is computationally efficient and requires a number of samples that is near-optimal with respect to previously established information-theoretic lower-bound. Our statistical estimator has a physical interpretation in terms of "interaction screening". The estimator is consistent and is efficiently implemented using convex optimization. We prove that with appropriate regularization, the estimator recovers the underlying graph using a number of samples that is logarithmic in the system size p and exponential in the maximum coupling-intensity and maximum node-degree. version:1
arxiv-1605-07251 | Dense CNN Learning with Equivalent Mappings | http://arxiv.org/abs/1605.07251 | id:1605.07251 author:Jianxin Wu, Chen-Wei Xie, Jian-Hao Luo category:cs.CV  published:2016-05-24 summary:Large receptive field and dense prediction are both important for achieving high accuracy in pixel labeling tasks such as semantic segmentation. These two properties, however, contradict with each other. A pooling layer (with stride 2) quadruples the receptive field size but reduces the number of predictions to 25\%. Some existing methods lead to dense predictions using computations that are not equivalent to the original model. In this paper, we propose the equivalent convolution (eConv) and equivalent pooling (ePool) layers, leading to predictions that are both dense and equivalent to the baseline CNN model. Dense prediction models learned using eConv and ePool can transfer the baseline CNN's parameters as a starting point, and can inverse transfer the learned parameters in a dense model back to the original one, which has both fast testing speed and high accuracy. The proposed eConv and ePool layers have achieved higher accuracy than baseline CNN in various tasks, including semantic segmentation, object localization, image categorization and apparent age estimation, not only in those tasks requiring dense pixel labeling. version:1
arxiv-1601-07932 | Information-Theoretic Lower Bounds for Recovery of Diffusion Network Structures | http://arxiv.org/abs/1601.07932 | id:1601.07932 author:Keehwan Park, Jean Honorio category:cs.LG cs.IT math.IT stat.ML  published:2016-01-28 summary:We study the information-theoretic lower bound of the sample complexity of the correct recovery of diffusion network structures. We introduce a discrete-time diffusion model based on the Independent Cascade model for which we obtain a lower bound of order $\Omega(k \log p)$, for directed graphs of $p$ nodes, and at most $k$ parents per node. Next, we introduce a continuous-time diffusion model, for which a similar lower bound of order $\Omega(k \log p)$ is obtained. Our results show that the algorithm of Pouget-Abadie et al. is statistically optimal for the discrete-time regime. Our work also opens the question of whether it is possible to devise an optimal algorithm for the continuous-time regime. version:2
arxiv-1505-05561 | Why Regularized Auto-Encoders learn Sparse Representation? | http://arxiv.org/abs/1505.05561 | id:1505.05561 author:Devansh Arpit, Yingbo Zhou, Hung Ngo, Venu Govindaraju category:stat.ML cs.CV cs.LG  published:2015-05-21 summary:While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- \textit{Internal Covariate Shift}-- the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size $ 1 $ during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \textit{Normalization Propagation}. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers. version:4
arxiv-1602-01407 | A Kronecker-factored approximate Fisher matrix for convolution layers | http://arxiv.org/abs/1602.01407 | id:1602.01407 author:Roger Grosse, James Martens category:stat.ML cs.LG  published:2016-02-03 summary:Second-order optimization methods such as natural gradient descent have the potential to speed up training of neural networks by correcting for the curvature of the loss function. Unfortunately, the exact natural gradient is impractical to compute for large models, and most approximations either require an expensive iterative procedure or make crude approximations to the curvature. We present Kronecker Factors for Convolution (KFC), a tractable approximation to the Fisher matrix for convolutional networks based on a structured probabilistic model for the distribution over backpropagated derivatives. Similarly to the recently proposed Kronecker-Factored Approximate Curvature (K-FAC), each block of the approximate Fisher matrix decomposes as the Kronecker product of small matrices, allowing for efficient inversion. KFC captures important curvature information while still yielding comparably efficient updates to stochastic gradient descent (SGD). We show that the updates are invariant to commonly used reparameterizations, such as centering of the activations. In our experiments, approximate natural gradient descent with KFC was able to train convolutional networks several times faster than carefully tuned SGD. Furthermore, it was able to train the networks in 10-20 times fewer iterations than SGD, suggesting its potential applicability in a distributed setting. version:2
arxiv-1605-07230 | Deep Portfolio Theory | http://arxiv.org/abs/1605.07230 | id:1605.07230 author:J. B. Heaton, N. G. Polson, J. H. Witte category:q-fin.PM cs.LG  published:2016-05-23 summary:We construct a deep portfolio theory. By building on Markowitz's classic risk-return trade-off, we develop a self-contained four-step routine of encode, calibrate, validate and verify to formulate an automated and general portfolio selection process. At the heart of our algorithm are deep hierarchical compositions of portfolios constructed in the encoding step. The calibration step then provides multivariate payouts in the form of deep hierarchical portfolios that are designed to target a variety of objective functions. The validate step trades-off the amount of regularization used in the encode and calibrate steps. The verification step uses a cross validation approach to trace out an ex post deep portfolio efficient frontier. We demonstrate all four steps of our portfolio theory numerically. version:1
arxiv-1409-5495 | Efficient Feature Group Sequencing for Anytime Linear Prediction | http://arxiv.org/abs/1409.5495 | id:1409.5495 author:Hanzhang Hu, Alexander Grubb, J. Andrew Bagnell, Martial Hebert category:cs.LG  published:2014-09-19 summary:We consider anytime linear prediction in the common machine learning setting, where features are in groups that have costs. We achieve anytime (or interruptible) predictions by sequencing the computation of feature groups and reporting results using the computed features at interruption. We extend Orthogonal Matching Pursuit (OMP) and Forward Regression (FR) to learn the sequencing greedily under this group setting with costs. We theoretically guarantee that our algorithms achieve near-optimal linear predictions at each budget when a feature group is chosen. With a novel analysis of OMP, we improve its theoretical bound to the same strength as that of FR. In addition, we develop a novel algorithm that consumes cost $4B$ to approximate the optimal performance of any cost $B$, and prove that with cost less than $4B$, such an approximation is impossible. To our knowledge, these are the first anytime bounds at all budgets. We test our algorithms on two real-world data-sets and evaluate them in terms of anytime linear prediction performance against cost-weighted Group Lasso and alternative greedy algorithms. version:3
arxiv-1508-02096 | Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation | http://arxiv.org/abs/1508.02096 | id:1508.02096 author:Wang Ling, Tiago Luís, Luís Marujo, Ramón Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W. Black, Isabel Trancoso category:cs.CL  published:2015-08-09 summary:We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish). version:2
arxiv-1412-7461 | Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models | http://arxiv.org/abs/1412.7461 | id:1412.7461 author:Aki Vehtari, Tommi Mononen, Ville Tolvanen, Tuomas Sivula, Ole Winther category:stat.CO stat.ML  published:2014-12-23 summary:The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) cross-validation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators. That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods. version:3
arxiv-1605-05721 | Generalized Min-Max Kernel and Generalized Consistent Weighted Sampling | http://arxiv.org/abs/1605.05721 | id:1605.05721 author:Ping Li category:cs.LG cs.IR stat.ML  published:2016-05-18 summary:We propose the "generalized min-max" (GMM) kernel as a measure of data similarity, where data vectors can have both positive and negative entries. GMM is positive definite as there is an associate hashing method named "generalized consistent weighted sampling" (GCWS) which linearizes this (nonlinear) kernel. A natural competitor of GMM is the radial basis function (RBF) kernel, whose corresponding hashing method is known as the "random Fourier features" (RFF). An extensive experimental study on classifications of \textbf{50} publicly available datasets demonstrates that both the GMM and RBF kernels can often substantially improve over linear classifiers. Furthermore, the GCWS hashing method typically requires substantially fewer samples than RFF in order to achieve similar classification accuracies. To understand the property of random Fourier features (RFF), we derive the theoretical variance of RFF, which reveals that the variance of RFF has a term that does not vanish at any similarity. In comparison, the variance of GCWS approaches zero at certain similarities. Overall, the relative (to the expectation) variance of RFF is substantially larger than the relative variance of GCWS. This helps explain the superb empirical results of GCWS compared to RFF. We expect that GMM and GCWS will be adopted in practice for large-scale statistical machine learning applications and efficient near neighbor search (as GMM generates discrete hash values). version:2
arxiv-1605-07174 | Kernel-based Reconstruction of Graph Signals | http://arxiv.org/abs/1605.07174 | id:1605.07174 author:Daniel Romero, Meng Ma, Georgios B. Giannakis category:stat.ML cs.LG  published:2016-05-23 summary:A number of applications in engineering, social sciences, physics, and biology involve inference over networks. In this context, graph signals are widely encountered as descriptors of vertex attributes or features in graph-structured data. Estimating such signals in all vertices given noisy observations of their values on a subset of vertices has been extensively analyzed in the literature of signal processing on graphs (SPoG). This paper advocates kernel regression as a framework generalizing popular SPoG modeling and reconstruction and expanding their capabilities. Formulating signal reconstruction as a regression task on reproducing kernel Hilbert spaces of graph signals permeates benefits from statistical learning, offers fresh insights, and allows for estimators to leverage richer forms of prior information than existing alternatives. A number of SPoG notions such as bandlimitedness, graph filters, and the graph Fourier transform are naturally accommodated in the kernel framework. Additionally, this paper capitalizes on the so-called representer theorem to devise simpler versions of existing Thikhonov regularized estimators, and offers a novel probabilistic interpretation of kernel methods on graphs based on graphical models. Motivated by the challenges of selecting the bandwidth parameter in SPoG estimators or the kernel map in kernel-based methods, the present paper further proposes two multi-kernel approaches with complementary strengths. Whereas the first enables estimation of the unknown bandwidth of bandlimited signals, the second allows for efficient graph filter selection. Numerical tests with synthetic as well as real data demonstrate the merits of the proposed methods relative to state-of-the-art alternatives. version:1
arxiv-1605-07156 | Genetic Architect: Discovering Genomic Structure with Learned Neural Architectures | http://arxiv.org/abs/1605.07156 | id:1605.07156 author:Laura Deming, Sasha Targ, Nate Sauder, Diogo Almeida, Chun Jimmie Ye category:cs.LG cs.AI cs.NE stat.ML  published:2016-05-23 summary:Each human genome is a 3 billion base pair set of encoding instructions. Decoding the genome using deep learning fundamentally differs from most tasks, as we do not know the full structure of the data and therefore cannot design architectures to suit it. As such, architectures that fit the structure of genomics should be learned not prescribed. Here, we develop a novel search algorithm, applicable across domains, that discovers an optimal architecture which simultaneously learns general genomic patterns and identifies the most important sequence motifs in predicting functional genomic outcomes. The architectures we find using this algorithm succeed at using only RNA expression data to predict gene regulatory structure, learn human-interpretable visualizations of key sequence motifs, and surpass state-of-the-art results on benchmark genomics challenges. version:1
arxiv-1605-07154 | Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations | http://arxiv.org/abs/1605.07154 | id:1605.07154 author:Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro category:cs.LG cs.NE  published:2016-05-23 summary:We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes. version:1
arxiv-1605-07148 | Backprop KF: Learning Discriminative Deterministic State Estimators | http://arxiv.org/abs/1605.07148 | id:1605.07148 author:Tuomas Haarnoja, Anurag Ajay, Sergey Levine, Pieter Abbeel category:cs.LG cs.AI  published:2016-05-23 summary:Generative state estimators based on probabilistic filters and smoothers are one of the most popular classes of state estimators for robots and autonomous vehicles. However, generative models have limited capacity to handle rich sensory observations, such as camera images, since they must model the entire distribution over sensor readings. Discriminative models do not suffer from this limitation, but are typically more complex to train as latent variable models for state estimation. We present an alternative approach where the parameters of the latent state distribution are directly optimized as a deterministic computation graph, resulting in a simple and effective gradient descent algorithm for training discriminative state estimators. We show that this procedure can be used to train state estimators that use complex input, such as raw camera images, which must be processed using expressive nonlinear function approximators such as convolutional neural networks. Our model can be viewed as a type of recurrent neural network, and the connection to probabilistic filtering allows us to design a network architecture that is particularly well suited for state estimation. We evaluate our approach on tracking task with raw image inputs. The results show significant improvement over both standard generative approaches and regular recurrent neural networks. version:1
arxiv-1605-07147 | Fast stochastic optimization on Riemannian manifolds | http://arxiv.org/abs/1605.07147 | id:1605.07147 author:Hongyi Zhang, Sashank J. Reddi, Suvrit Sra category:math.OC cs.LG  published:2016-05-23 summary:We study optimization of finite sums of \emph{geodesically} smooth functions on Riemannian manifolds. Although variance reduction techniques for optimizing finite-sum problems have witnessed a huge surge of interest in recent years, all existing work is limited to vector space problems. We introduce \emph{Riemannian SVRG}, a new variance reduced Riemannian optimization method. We analyze this method for both geodesically smooth \emph{convex} and \emph{nonconvex} functions. Our analysis reveals that Riemannian SVRG comes with advantages of the usual SVRG method, but with factors depending on manifold curvature that influence its convergence. To the best of our knowledge, ours is the first \emph{fast} stochastic Riemannian method. Moreover, our work offers the first non-asymptotic complexity analysis for nonconvex Riemannian optimization (even for the batch setting). Our results have several implications; for instance, they offer a Riemannian perspective on variance reduced PCA, which promises a short, transparent convergence analysis. version:1
arxiv-1605-07146 | Wide Residual Networks | http://arxiv.org/abs/1605.07146 | id:1605.07146 author:Sergey Zagoruyko, Nikos Komodakis category:cs.CV cs.LG cs.NE  published:2016-05-23 summary:Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN. version:1
arxiv-1605-07145 | Towards Optimality Conditions for Non-Linear Networks | http://arxiv.org/abs/1605.07145 | id:1605.07145 author:Devansh Arpit, Hung Q. Ngo, Yingbo Zhou, Nils Napp, Venu Govindaraju category:stat.ML cs.LG cs.NE  published:2016-05-23 summary:Training non-linear neural networks is a challenging task, but over the years, various approaches coming from different perspectives have been proposed to improve performance. However, insights into what fundamentally constitutes \textit{optimal} network parameters remains obscure. Similarly, given what properties of data can we hope for a non-linear network to learn is also not well studied. In order to address these challenges, we take a novel approach by analysing neural network from a data generating perspective, where we assume hidden layers generate the observed data. This perspective allows us to connect seemingly disparate approaches explored independently in the machine learning community such as batch normalization, Independent Component Analysis, orthogonal weight initialization, etc, as parts of a bigger picture and provide insights into non-linear networks in terms of properties of parameter and data that lead to better performance. version:1
arxiv-1605-07139 | Fairness in Learning: Classic and Contextual Bandits | http://arxiv.org/abs/1605.07139 | id:1605.07139 author:Matthew Joseph, Michael Kearns, Jamie Morgenstern, Aaron Roth category:cs.LG stat.ML  published:2016-05-23 summary:We introduce the study of fairness in multi-armed bandit problems. Our fairness definition can be interpreted as demanding that given a pool of applicants (say, for college admission or mortgages), a worse applicant is never favored over a better one, despite a learning algorithm's uncertainty over the true payoffs. We prove results of two types. First, in the important special case of the classic stochastic bandits problem (i.e., in which there are no contexts), we provide a provably fair algorithm based on "chained" confidence intervals, and provide a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence. When combined with regret bounds for standard non-fair algorithms such as UCB, this proves a strong separation between fair and unfair learning, which extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm, and conversely any fair contextual bandit algorithm can be transformed into a KWIK learning algorithm. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms version:1
arxiv-1605-07133 | Towards Multi-Agent Communication-Based Language Learning | http://arxiv.org/abs/1605.07133 | id:1605.07133 author:Angeliki Lazaridou, Nghia The Pham, Marco Baroni category:cs.CL cs.CV cs.LG  published:2016-05-23 summary:We propose an interactive multimodal framework for language learning. Instead of being passively exposed to large amounts of natural text, our learners (implemented as feed-forward neural networks) engage in cooperative referential games starting from a tabula rasa setup, and thus develop their own language from the need to communicate in order to succeed at the game. Preliminary experiments provide promising results, but also suggest that it is important to ensure that agents trained in this way do not develop an adhoc communication code only effective for the game they are playing version:1
arxiv-1605-07127 | Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks | http://arxiv.org/abs/1605.07127 | id:1605.07127 author:Stefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-Velez, Steffen Udluft category:stat.ML cs.LG  published:2016-05-23 summary:We present an algorithm for model-based reinforcement learning that combines Bayesian neural networks (BNNs) with random roll-outs and stochastic optimization for policy learning. The BNNs are trained by minimizing $\alpha$-divergences, allowing us to capture complicated statistical patterns in the transition dynamics, e.g. multi-modality and heteroskedasticity, which are usually missed by other common modeling approaches. We illustrate the performance of our method by solving a challenging benchmark where model-based approaches usually fail and by obtaining promising results in a real-world scenario for controlling a gas turbine. version:1
arxiv-1603-09064 | Semi-Supervised Learning on Graphs through Reach and Distance Diffusion | http://arxiv.org/abs/1603.09064 | id:1603.09064 author:Edith Cohen category:cs.LG  published:2016-03-30 summary:Semi-supervised learning algorithms are an indispensable tool when labeled examples are scarce and there are many unlabeled examples [Blum and Chawla 2001, Zhu et. al. 2003]. With graph-based methods, entities (examples) correspond to nodes in a graph and edges correspond to related entities. The graph structure is used to infer implicit pairwise affinity values (kernel) which are used to compute the learned labels. Two powerful techniques to define such a kernel are "symmetric" spectral methods and Personalized Page Rank (PPR). With spectral methods, labels can be scalably learned using Jacobi iterations, but an inherent limiting issue is that they are applicable to {\em symmetric} (undirected) graphs, whereas often, such as with like, follow, or hyperlinks, relations between entities are inherently asymmetric. PPR naturally works with directed graphs but even with state of the art techniques does not scale when we want to learn billions of labels. Aiming at both high scalability and handling of directed relations, we propose here {\em Reach Diffusion} and {\em Distance Diffusion} kernels. Our design is inspired by models for influence diffusion in social networks, formalized and spawned from the seminal work of [Kempe, Kleinberg, and Tardos 2003]. We tailor these models to define a natural asymmetric "kernel" and design highly scalable algorithms for parameter setting and label learning. version:2
arxiv-1506-07615 | Completing Low-Rank Matrices with Corrupted Samples from Few Coefficients in General Basis | http://arxiv.org/abs/1506.07615 | id:1506.07615 author:Hongyang Zhang, Zhouchen Lin, Chao Zhang category:cs.IT cs.LG cs.NA math.IT math.NA stat.ML 68T05 G.1.6; K.3.2  published:2015-06-25 summary:Subspace recovery from corrupted and missing data is crucial for various applications in signal processing and information theory. To complete missing values and detect column corruptions, existing robust Matrix Completion (MC) methods mostly concentrate on recovering a low-rank matrix from few corrupted coefficients w.r.t. standard basis, which, however, does not apply to more general basis, e.g., Fourier basis. In this paper, we prove that the range space of an $m\times n$ matrix with rank $r$ can be exactly recovered from few coefficients w.r.t. general basis, though $r$ and the number of corrupted samples are both as high as $O(\min\{m,n\}/\log^3 (m+n))$. Our model covers previous ones as special cases, and robust MC can recover the intrinsic matrix with a higher rank. Moreover, we suggest a universal choice of the regularization parameter, which is $\lambda=1/\sqrt{\log n}$. By our $\ell_{2,1}$ filtering algorithm, which has theoretical guarantees, we can further reduce the computational cost of our model. As an application, we also find that the solutions to extended robust Low-Rank Representation and to our extended robust MC are mutually expressible, so both our theory and algorithm can be applied to the subspace clustering problem with missing values under certain conditions. Experiments verify our theories. version:2
arxiv-1605-07116 | A Formal Evaluation of PSNR as Quality Measurement Parameter for Image Segmentation Algorithms | http://arxiv.org/abs/1605.07116 | id:1605.07116 author:Fernando A. Fardo, Victor H. Conforto, Francisco C. de Oliveira, Paulo S. Rodrigues category:cs.CV  published:2016-05-23 summary:Quality evaluation of image segmentation algorithms are still subject of debate and research. Currently, there is no generic metric that could be applied to any algorithm reliably. This article contains an evaluation for the PSRN (Peak Signal-To-Noise Ratio) as a metric which has been used to evaluate threshold level selection as well as the number of thresholds in the case of multi-level segmentation. The results obtained in this study suggest that the PSNR is not an adequate quality measurement for segmentation algorithms. version:1
arxiv-1605-07110 | Deep Learning without Poor Local Minima | http://arxiv.org/abs/1605.07110 | id:1605.07110 author:Kenji Kawaguchi category:stat.ML cs.LG math.OC  published:2016-05-23 summary:In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice. version:1
arxiv-1605-07104 | Generic Instance Search and Re-identification from One Example via Attributes and Categories | http://arxiv.org/abs/1605.07104 | id:1605.07104 author:Ran Tao, Arnold W. M. Smeulders, Shih-Fu Chang category:cs.CV  published:2016-05-23 summary:This paper aims for generic instance search from one example where the instance can be an arbitrary object like shoes, not just near-planar and one-sided instances like buildings and logos. First, we evaluate state-of-the-art instance search methods on this problem. We observe that what works for buildings loses its generality on shoes. Second, we propose to use automatically learned category-specific attributes to address the large appearance variations present in generic instance search. Searching among instances from the same category as the query, the category-specific attributes outperform existing approaches by a large margin on shoes and cars and perform on par with the state-of-the-art on buildings. Third, we treat person re-identification as a special case of generic instance search. On the popular VIPeR dataset, we reach state-of-the-art performance with the same method. Fourth, we extend our method to search objects without restriction to the specifically known category. We show that the combination of category-level information and the category-specific attributes is superior to the alternative method combining category-level information with low-level features such as Fisher vector. version:1
arxiv-1605-07094 | Optimal Coding in Biological and Artificial Neural Networks | http://arxiv.org/abs/1605.07094 | id:1605.07094 author:Sebastian Weichwald, Tatiana Fomina, Bernhard Schölkopf, Moritz Grosse-Wentrup category:q-bio.NC cs.IT cs.LG math.IT stat.ML  published:2016-05-23 summary:Feature representations in both, biological neural networks in the primate ventral stream and artificial convolutional neural networks trained on object recognition, incresase in complexity and receptive field size with layer depth. Somewhat strikingly, empirical evidence indicates that this analogy extends to the specific representations learned in each layer. This suggests that biological and artificial neural networks share a fundamental organising principle. We shed light on this principle in the framework of optimal coding. Specifically, we first investigate which properties of a code render it robust to transmission over noisy channels and formally prove that for equientropic channels an upper bound on the expected minimum decoding error is attained for codes with maximum marginal entropy. We then show that the pairwise correlation of units in a deep layer of a neural network, that has been trained on an object recognition task, increases when perturbing the distribution of input images, i. e., that the network exhibits properties of an optimally coding system. By analogy, this suggests that the layer-wise similarity of feature representations in biological and artificial neural networks is a result of optimal coding that enables robust transmission of object information over noisy channels. Because we find that in equientropic channels the upper bound on the expected minimum decoding error is independent of the class-conditional entropy, our work further provides a plausible explanation why optimal codes can be learned in unsupervised settings. version:1
arxiv-1603-02618 | The red one!: On learning to refer to things based on their discriminative properties | http://arxiv.org/abs/1603.02618 | id:1603.02618 author:Angeliki Lazaridou, Nghia The Pham, Marco Baroni category:cs.CL cs.CV  published:2016-03-08 summary:As a first step towards agents learning to communicate about their visual environment, we propose a system that, given visual representations of a referent (cat) and a context (sofa), identifies their discriminative attributes, i.e., properties that distinguish them (has_tail). Moreover, despite the lack of direct supervision at the attribute level, the model learns to assign plausible attributes to objects (sofa-has_cushion). Finally, we present a preliminary experiment confirming the referential success of the predicted discriminative attributes. version:2
arxiv-1605-07081 | Depth from a Single Image by Harmonizing Overcomplete Local Network Predictions | http://arxiv.org/abs/1605.07081 | id:1605.07081 author:Ayan Chakrabarti, Jingyu Shao, Gregory Shakhnarovich category:cs.CV  published:2016-05-23 summary:A single color image can contain many cues informative towards different aspects of local geometric structure. We approach the problem of monocular depth estimation by using a neural network to produce a mid-level representation that summarizes these cues. This network is trained to characterize local scene geometry by predicting, at every image location, depth derivatives of different orders, orientations and scales. However, instead of a single estimate for each derivative, the network outputs probability distributions that allow it to express confidence about some coefficients, and ambiguity about others. Scene depth is then estimated by harmonizing this overcomplete set of network predictions, using a globalization procedure that finds a single consistent depth map that best matches all the local derivative distributions. We demonstrate the efficacy of this approach through evaluation on the NYU v2 depth data set. version:1
arxiv-1605-07079 | Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets | http://arxiv.org/abs/1605.07079 | id:1605.07079 author:Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, Frank Hutter category:cs.LG cs.AI stat.ML  published:2016-05-23 summary:Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. But it is still costly if each evaluation of the objective requires training and validating the algorithm being optimized, which, for large datasets, often takes hours, days, or even weeks. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed FABOLAS, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that FABOLAS often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods. version:1
arxiv-1605-07078 | Learning Sensor Multiplexing Design through Back-propagation | http://arxiv.org/abs/1605.07078 | id:1605.07078 author:Ayan Chakrabarti category:cs.LG stat.ML  published:2016-05-23 summary:Recent progress on many imaging and vision tasks has been driven by the use of deep feed-forward neural networks, which are trained by propagating gradients of a loss defined on the final output, back through the network up to the first layer that operates directly on the image. We propose back-propagating one step further---to learn camera sensor designs jointly with networks that carry out inference on the images they capture. In this paper, we specifically consider the design and inference problems in a typical color camera---where the sensor is able to measure only one color channel at each pixel location, and computational inference is required to reconstruct a full color image. We learn the camera sensor's color multiplexing pattern by encoding it as layer whose learnable weights determine which color channel, from among a fixed set, will be measured at each location. These weights are jointly trained with those of a reconstruction network that operates on the corresponding sensor measurements to produce a full color image. Our network achieves significant improvements in accuracy over the traditional Bayer pattern used in most color cameras. It automatically learns to employ a sparse color measurement approach similar to that of a recent design, and moreover, improves upon that design by learning an optimal layout for these measurements. version:1
arxiv-1605-06422 | Fast Randomized Semi-Supervised Clustering | http://arxiv.org/abs/1605.06422 | id:1605.06422 author:Alaa Saade, Florent Krzakala, Marc Lelarge, Lenka Zdeborová category:cs.LG math.PR math.ST stat.ML stat.TH  published:2016-05-20 summary:We consider the problem of clustering partially labeled data from a minimal number of randomly chosen pairwise comparisons between the items. We introduce an efficient local algorithm based on a power iteration of the non-backtracking operator and study its performance on a simple model. For the case of two clusters, we give bounds on the classification error and show that a small error can be achieved from $O(n)$ randomly chosen measurements, where $n$ is the number of items in the dataset. Our algorithm is therefore efficient both in terms of time and space complexities. We also investigate numerically the performance of the algorithm on synthetic and real world data. version:2
arxiv-1507-05259 | Learning Fair Classifiers | http://arxiv.org/abs/1507.05259 | id:1507.05259 author:Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P. Gummadi category:stat.ML cs.LG  published:2015-07-19 summary:Automated data-driven decision systems are ubiquitous across a wide variety of online services, from online social networking and e-commerce to e-government. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes have a disproportionally large adverse impact on particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers in a principled manner, by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism on two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control of the level of fairness, often at a minimal cost in terms of accuracy. version:4
arxiv-1605-07066 | A Unifying Framework for Sparse Gaussian Process Approximation using Power Expectation Propagation | http://arxiv.org/abs/1605.07066 | id:1605.07066 author:Thang D. Bui, Josiah Yan, Richard E. Turner category:stat.ML cs.LG  published:2016-05-23 summary:Pseudo-point based sparse approximation methods sidestep the computational and analytical intractability in Gaussian process models at the cost of accuracy, reducing the complexity from cubic to linear in the number of data points. We propose a new sparse posterior approximation framework using Power Expectation Propagation (Power EP), unifying various existing methods into one single computational and algorithmic viewpoint. This allows us to bring a new posterior approximation perspective to methods that are currently understood to perform prior approximation. Crucially, we demonstrate that the proposed approach outperforms current sparse methods on regression, classification and state space modelling by leveraging the interpolation between Power EP's special cases, variational inference and EP. version:1
arxiv-1602-08332 | Bounded Rational Decision-Making in Feedforward Neural Networks | http://arxiv.org/abs/1602.08332 | id:1602.08332 author:Felix Leibfried, Daniel Alexander Braun category:cs.AI cs.LG cs.NE  published:2016-02-26 summary:Bounded rational decision-makers transform sensory input into motor output under limited computational resources. Mathematically, such decision-makers can be modeled as information-theoretic channels with limited transmission rate. Here, we apply this formalism for the first time to multilayer feedforward neural networks. We derive synaptic weight update rules for two scenarios, where either each neuron is considered as a bounded rational decision-maker or the network as a whole. In the update rules, bounded rationality translates into information-theoretically motivated types of regularization in weight space. In experiments on the MNIST benchmark classification task for handwritten digits, we show that such information-theoretic regularization successfully prevents overfitting across different architectures and attains results that are competitive with other recent techniques like dropout, dropconnect and Bayes by backprop, for both ordinary and convolutional neural networks. version:2
arxiv-1605-07061 | On Restricted Nonnegative Matrix Factorization | http://arxiv.org/abs/1605.07061 | id:1605.07061 author:Dmitry Chistikov, Stefan Kiefer, Ines Marušić, Mahsa Shirmohammadi, James Worrell category:cs.FL cs.CC cs.LG  published:2016-05-23 summary:Nonnegative matrix factorization (NMF) is the problem of decomposing a given nonnegative $n \times m$ matrix $M$ into a product of a nonnegative $n \times d$ matrix $W$ and a nonnegative $d \times m$ matrix $H$. Restricted NMF requires in addition that the column spaces of $M$ and $W$ coincide. Finding the minimal inner dimension $d$ is known to be NP-hard, both for NMF and restricted NMF. We show that restricted NMF is closely related to a question about the nature of minimal probabilistic automata, posed by Paz in his seminal 1971 textbook. We use this connection to answer Paz's question negatively, thus falsifying a positive answer claimed in 1974. Furthermore, we investigate whether a rational matrix $M$ always has a restricted NMF of minimal inner dimension whose factors $W$ and $H$ are also rational. We show that this holds for matrices $M$ of rank at most $3$ and we exhibit a rank-$4$ matrix for which $W$ and $H$ require irrational entries. version:1
arxiv-1605-07057 | Bayesian Model Selection of Stochastic Block Models | http://arxiv.org/abs/1605.07057 | id:1605.07057 author:Xiaoran Yan category:stat.ML cs.LG cs.SI  published:2016-05-23 summary:A central problem in analyzing networks is partitioning them into modules or communities. One of the best tools for this is the stochastic block model, which clusters vertices into blocks with statistically homogeneous pattern of links. Despite its flexibility and popularity, there has been a lack of principled statistical model selection criteria for the stochastic block model. Here we propose a Bayesian framework for choosing the number of blocks as well as comparing it to the more elaborate degree- corrected block models, ultimately leading to a universal model selection framework capable of comparing multiple modeling combinations. We will also investigate its connection to the minimum description length principle. version:1
arxiv-1605-07051 | Convergence Analysis for Rectangular Matrix Completion Using Burer-Monteiro Factorization and Gradient Descent | http://arxiv.org/abs/1605.07051 | id:1605.07051 author:Qinqing Zheng, John Lafferty category:stat.ML cs.LG  published:2016-05-23 summary:We address the rectangular matrix completion problem by lifting the unknown matrix to a positive semidefinite matrix in higher dimension, and optimizing a nonconvex objective over the semidefinite factor using a simple gradient descent scheme. With $O( \mu r^2 \kappa^2 n \max(\mu, \log n))$ random observations of a $n_1 \times n_2$ $\mu$-incoherent matrix of rank $r$ and condition number $\kappa$, where $n = \max(n_1, n_2)$, the algorithm linearly converges to the global optimum with high probability. version:1
arxiv-1603-02514 | Semi-supervised Variational Autoencoders for Sequence Classification | http://arxiv.org/abs/1603.02514 | id:1603.02514 author:Weidi Xu, Haoze Sun, Chao Deng, Ying Tan category:cs.CL cs.LG  published:2016-03-08 summary:Although semi-supervised learning method based on variational autoencoder (\emph{SemiVAE}) works well in image classification tasks, it fails in text classification tasks if using vanilla LSTM as its conditional generative model. We find that the model with this setting is unable to utilize the positive feedback mechanism of \emph{SemiVAE} and hence fails to boost the performance. To tackle this problem, a conditional Long Short-Term Memory network (conditional LSTM) is presented, which receives the conditional information all the time-steps. In addtion, auxiliary variable is also found to be useful in our method in terms of both training speed and prediction accuracy. Experimental results on Large Movie Review Dataset (IMDB) show that the proposed approach significantly improves the classification accuracy compared with pure-supervised classifier and achieves competitive performance against previous pre-training based methods. Additional improvement can be obtained by integrating pre-training based methods. version:2
arxiv-1605-07026 | Spontaneous vs. Posed smiles - can we tell the difference? | http://arxiv.org/abs/1605.07026 | id:1605.07026 author:Bappaditya Mandal, Nizar Ouarti category:cs.CV cs.AI  published:2016-05-23 summary:Smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways. Generally, it shows happy state of the mind, however, `smiles' can be deceptive, for example people can give a smile when they feel happy and sometimes they might also give a smile (in a different way) when they feel pity for others. This work aims to distinguish spontaneous (felt) smile expressions from posed (deliberate) smiles by extracting and analyzing both global (macro) motion of the face and subtle (micro) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow. Specifically the eyes and lips features are captured and used for analysis. It aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large database show promising results as compared to other relevant methods. version:1
arxiv-1605-07025 | Tucker Gaussian Process for Regression and Collaborative Filtering | http://arxiv.org/abs/1605.07025 | id:1605.07025 author:Hyunjik Kim, Xiaoyu Lu, Seth Flaxman, Yee Whye Teh category:stat.ML cs.IR cs.LG  published:2016-05-23 summary:We introduce the Tucker Gaussian Process (TGP), an approach to scalable GP learning based on low-rank tensor decompositions. We show that our model is applicable to general regression problems, and is particularly well-suited to grid-structured data and problems where the dependence on covariates is close to being separable. Furthermore, when applied to collaborative filtering, our model provides an effective GP based method that has a low-rank matrix factorisation at its core, and gives a natural and elegant method for incorporating side information. version:1
arxiv-1605-07018 | Online Learning with Feedback Graphs Without the Graphs | http://arxiv.org/abs/1605.07018 | id:1605.07018 author:Alon Cohen, Tamir Hazan, Tomer Koren category:cs.LG stat.ML  published:2016-05-23 summary:We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is \emph{never fully revealed} to the learner. We show a large gap between the adversarial and the stochastic cases. In the adversarial case, we prove that even for dense feedback graphs, the learner cannot improve upon a trivial regret bound obtained by ignoring any additional feedback besides her own loss. In contrast, in the stochastic case we give an algorithm that achieves $\widetilde \Theta(\sqrt{\alpha T})$ regret over $T$ rounds, provided that the independence numbers of the hidden feedback graphs are at most $\alpha$. We also extend our results to a more general feedback model, in which the learner does not necessarily observe her own loss, and show that, even in simple cases, concealing the feedback graphs might render a learnable problem unlearnable. version:1
arxiv-1605-03364 | Active Uncertainty Calibration in Bayesian ODE Solvers | http://arxiv.org/abs/1605.03364 | id:1605.03364 author:Hans Kersting, Philipp Hennig category:cs.NA cs.LG math.NA stat.ML  published:2016-05-11 summary:There is resurging interest, in statistics and machine learning, in solvers for ordinary differential equations (ODEs) that return probability measures instead of point estimates. Recently, Conrad et al. introduced a sampling-based class of methods that are 'well-calibrated' in a specific sense. But the computational cost of these methods is significantly above that of classic methods. On the other hand, Schober et al. pointed out a precise connection between classic Runge-Kutta ODE solvers and Gaussian filters, which gives only a rough probabilistic calibration, but at negligible cost overhead. By formulating the solution of ODEs as approximate inference in linear Gaussian SDEs, we investigate a range of probabilistic ODE solvers, that bridge the trade-off between computational cost and probabilistic calibration, and identify the inaccurate gradient measurement as the crucial source of uncertainty. We propose the novel filtering-based method Bayesian Quadrature filtering (BQF) which uses Bayesian quadrature to actively learn the imprecision in the gradient measurement by collecting multiple gradient evaluations. version:2
arxiv-1605-07009 | BMOBench: Black-Box Multi-Objective Optimization Benchmarking Platform | http://arxiv.org/abs/1605.07009 | id:1605.07009 author:Abdullah Al-Dujaili, S. Suresh category:math.OC cs.NE  published:2016-05-23 summary:This document briefly describes the Black-Box Multi-Objective Optimization Benchmarking (BMOBench) platform. It presents the test problems, evaluation procedure, and experimental setup. To this end, the BMOBench is demonstrated by comparing recent multi-objective solvers from the literature, namely SMS-EMOA, DMS, and MO-SOO. version:1
arxiv-1602-02018 | Compressive Spectral Clustering | http://arxiv.org/abs/1602.02018 | id:1602.02018 author:Nicolas Tremblay, Gilles Puy, Remi Gribonval, Pierre Vandergheynst category:cs.DS cs.LG stat.ML  published:2016-02-05 summary:Spectral clustering has become a popular technique due to its high performance in many contexts. It comprises three main steps: create a similarity graph between N objects to cluster, compute the first k eigenvectors of its Laplacian matrix to define a feature vector for each object, and run k-means on these features to separate objects into k classes. Each of these three steps becomes computationally intensive for large N and/or k. We propose to speed up the last two steps based on recent results in the emerging field of graph signal processing: graph filtering of random signals, and random sampling of bandlimited graph signals. We prove that our method, with a gain in computation time that can reach several orders of magnitude, is in fact an approximation of spectral clustering, for which we are able to control the error. We test the performance of our method on artificial and real-world network data. version:2
arxiv-1602-04052 | Image Restoration and Reconstruction using Variable Splitting and Class-adapted Image Priors | http://arxiv.org/abs/1602.04052 | id:1602.04052 author:Afonso M. Teodoro, José M. Bioucas-Dias, Mário A. T. Figueiredo category:cs.CV 94A08  68U10  47N10 I.4.5; I.4.4  published:2016-02-12 summary:This paper proposes using a Gaussian mixture model as a prior, for solving two image inverse problems, namely image deblurring and compressive imaging. We capitalize on the fact that variable splitting algorithms, like ADMM, are able to decouple the handling of the observation operator from that of the regularizer, and plug a state-of-the-art algorithm into the pure denoising step. Furthermore, we show that, when applied to a specific type of image, a Gaussian mixture model trained from an database of images of the same type is able to outperform current state-of-the-art methods. version:2
arxiv-1605-07003 | Image Restoration with Locally Selected Class-Adapted Models | http://arxiv.org/abs/1605.07003 | id:1605.07003 author:Afonso M. Teodoro, José M. Bioucas-Dias, Mário A. T. Figueiredo category:cs.CV 94A08  68U10  47N10 I.4.5; I.4.4  published:2016-05-23 summary:In this paper, we propose an automatic way to select class-adapted Gaussian mixture priors, in image denoising or deblurring tasks. We follow the Bayesian perspective and use a maximum a posteriori criterion to determine the model that best explains each observed patch. In situations where it is not possible to learn a model from the observed image, e.g. blurred images, this approach, in general, leads to better results in comparison with using only a generic model. In particular, when the input image belongs to a class for which we have a pre-computed model, the improvement is more pronounced. version:1
arxiv-1603-00162 | Convolutional Rectifier Networks as Generalized Tensor Decompositions | http://arxiv.org/abs/1603.00162 | id:1603.00162 author:Nadav Cohen, Amnon Shashua category:cs.NE cs.LG  published:2016-03-01 summary:Convolutional rectifier networks, i.e. convolutional neural networks with rectified linear activation and max or average pooling, are the cornerstone of modern deep learning. However, despite their wide use and success, our theoretical understanding of the expressive properties that drive these networks is partial at best. On the other hand, we have a much firmer grasp of these issues in the world of arithmetic circuits. Specifically, it is known that convolutional arithmetic circuits possess the property of "complete depth efficiency", meaning that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be implemented (or even approximated) by a shallow network. In this paper we describe a construction based on generalized tensor decompositions, that transforms convolutional arithmetic circuits into convolutional rectifier networks. We then use mathematical tools available from the world of arithmetic circuits to prove new results. First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners. version:2
arxiv-1605-06995 | Practical Privacy For Expectation Maximization | http://arxiv.org/abs/1605.06995 | id:1605.06995 author:Mijung Park, Jimmy Foulds, Kamalika Chaudhuri, Max Welling category:cs.LG cs.AI cs.CR stat.ME stat.ML  published:2016-05-23 summary:Expectation maximization (EM) is an iterative algorithm that computes maximum likelihood and maximum a posteriori estimates for models with unobserved variables. While widely used, the iterative nature of EM presents challenges for privacy-preserving estimation. Multiple iterations are required to obtain accurate parameter estimates, yet each iteration increases the amount of noise that must be added to achieve a reasonable degree of privacy. We propose a practical algorithm that overcomes this challenge and outputs EM parameter estimates that are both accurate and private. Our algorithm focuses on the frequent use case of models whose joint distribution over observed and unobserved variables remains in the exponential family. For these models, the EM parameters are functions of moments of the data. Our algorithm leverages this to preserve privacy by perturbing the moments, for which the amount of additive noise scales naturally with the data. In addition, our algorithm uses a relaxed notion of the differential privacy (DP) gold standard, called concentrated differential privacy (CDP). Rather than focusing on single-query loss, CDP provides high probability bounds for cumulative privacy loss, which is well suited for iterative algorithms. For mixture models, we show that our method requires a significantly smaller privacy budget for the same estimation accuracy compared to both DP and its (epsilon, delta)-DP relaxation. Our general approach of moment perturbation equipped with CDP can be readily extended to many iterative machine learning algorithms, which opens up various exciting future directions. version:1
arxiv-1602-05003 | The Multivariate Generalised von Mises: Inference and applications | http://arxiv.org/abs/1602.05003 | id:1602.05003 author:Alexandre K. W. Navarro, Jes Frellsen, Richard E. Turner category:stat.ML  published:2016-02-16 summary:Circular variables arise in a multitude of data-modelling contexts ranging from robotics to the social sciences, but they have been largely overlooked by the machine learning community. This paper partially redresses this imbalance by extending some standard probabilistic modelling tools to the circular domain. First we introduce a new multivariate distribution over circular variables, called the multivariate Generalised von Mises (mGvM) distribution. This distribution can be constructed by restricting and renormalising a general multivariate Gaussian distribution to the unit hyper-torus. Previously proposed multivariate circular distributions are shown to be special cases of this construction. Second, we introduce a new probabilistic model for circular regression, that is inspired by Gaussian Processes, and a method for probabilistic principal component analysis with circular hidden variables. These models can leverage standard modelling tools (e.g. covariance functions and methods for automatic relevance determination). Third, we show that the posterior distribution in these models is a mGvM distribution which enables development of an efficient variational free-energy scheme for performing approximate inference and approximate maximum-likelihood learning. version:4
arxiv-1605-00388 | Highly Accurate Prediction of Jobs Runtime Classes | http://arxiv.org/abs/1605.00388 | id:1605.00388 author:Anat Reiner-Benaim, Anna Grabarnick, Edi Shmueli category:stat.ML  published:2016-05-02 summary:Separating the short jobs from the long is a known technique to improve scheduling performance. In this paper we describe a method we developed for accurately predicting the runtimes classes of the jobs to enable this separation. Our method uses the fact that the runtimes can be represented as a mixture of overlapping Gaussian distributions, in order to train a CART classifier to provide the prediction. The threshold that separates the short jobs from the long jobs is determined during the evaluation of the classifier to maximize prediction accuracy. Our results indicate overall accuracy of 90% for the data set used in our study, with sensitivity and specificity both above 90%. version:2
arxiv-1512-06559 | Analysis of Vessel Connectivities in Retinal Images by Cortically Inspired Spectral Clustering | http://arxiv.org/abs/1512.06559 | id:1512.06559 author:Marta Favali, Samaneh Abbasi-Sureshjani, Bart ter Haar Romeny, Alessandro Sarti category:cs.CV  published:2015-12-21 summary:Retinal images provide early signs of diabetic retinopathy, glaucoma, and hypertension. These signs can be investigated based on microaneurysms or smaller vessels. The diagnostic biomarkers are the change of vessel widths and angles especially at junctions, which are investigated using the vessel segmentation or tracking. Vessel paths may also be interrupted; crossings and bifurcations may be disconnected. This paper addresses a novel contextual method based on the geometry of the primary visual cortex (V1) to study these difficulties. We have analyzed the specific problems at junctions with a connectivity kernel obtained as the fundamental solution of the Fokker-Planck equation, which is usually used to represent the geometrical structure of multi-orientation cortical connectivity. Using the spectral clustering on a large local affinity matrix constructed by both the connectivity kernel and the feature of intensity, the vessels are identified successfully in a hierarchical topology each representing an individual perceptual unit. version:2
arxiv-1605-06968 | A Riemannian gossip approach to decentralized matrix completion | http://arxiv.org/abs/1605.06968 | id:1605.06968 author:Bamdev Mishra, Hiroyuki Kasai, Atul Saroop category:cs.NA cs.LG math.OC  published:2016-05-23 summary:In this paper, we propose novel gossip algorithms for the low-rank decentralized matrix completion problem. The proposed approach is on the Riemannian Grassmann manifold that allows local matrix completion by different agents while achieving asymptotic consensus on the global low-rank factors. The resulting approach is scalable and parallelizable. Our numerical experiments show the good performance of the proposed algorithms on various benchmarks. version:1
arxiv-1602-03012 | EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos | http://arxiv.org/abs/1602.03012 | id:1602.03012 author:Andru P. Twinanda, Sherif Shehata, Didier Mutter, Jacques Marescaux, Michel de Mathelin, Nicolas Padoy category:cs.CV  published:2016-02-09 summary:Surgical workflow recognition has numerous potential medical applications, such as the automatic indexing of surgical video databases and the optimization of real-time operating room scheduling, among others. As a result, phase recognition has been studied in the context of several kinds of surgeries, such as cataract, neurological, and laparoscopic surgeries. In the literature, two types of features are typically used to perform this task: visual features and tool usage signals. However, the visual features used are mostly handcrafted. Furthermore, the tool usage signals are usually collected via a manual annotation process or by using additional equipment. In this paper, we propose a novel method for phase recognition that uses a convolutional neural network (CNN) to automatically learn features from cholecystectomy videos and that relies uniquely on visual information. In previous studies, it has been shown that the tool signals can provide valuable information in performing the phase recognition task. Thus, we present a novel CNN architecture, called EndoNet, that is designed to carry out the phase recognition and tool presence detection tasks in a multi-task manner. To the best of our knowledge, this is the first work proposing to use a CNN for multiple recognition tasks on laparoscopic videos. Extensive experimental comparisons to other methods show that EndoNet yields state-of-the-art results for both tasks. version:2
arxiv-1507-06617 | Fourier descriptors based on the structure of the human primary visual cortex with applications to object recognition | http://arxiv.org/abs/1507.06617 | id:1507.06617 author:Amine Bohi, Dario Prandi, Vincente Guis, Frédéric Bouchara, Jean-Paul Gauthier category:cs.CV  published:2015-07-23 summary:In this paper we propose a supervised object recognition method using new global features and inspired by the model of the human primary visual cortex V1 as the semidiscrete roto-translation group $SE(2,N) = \mathbb Z_N\rtimes \mathbb R^2$. The proposed technique is based on generalized Fourier descriptors on the latter group, which are invariant to natural geometric transformations (rotations, translations). These descriptors are then used to feed an SVM classifier. We have tested our method against the COIL-100 image database and the ORL face database, and compared it with other techniques based on traditional descriptors, global and local. The obtained results have shown that our approach looks extremely efficient and stable to noise, in presence of which it outperforms the other techniques analyzed in the paper. version:3
arxiv-1605-06955 | Beyond the Low-density Separation Principle: A Novel Approach to Semi-supervised Learning | http://arxiv.org/abs/1605.06955 | id:1605.06955 author:Tomoya Sakai, Marthinus Christoffel du Plessis, Gang Niu, Masashi Sugiyama category:cs.LG  published:2016-05-23 summary:Semi-supervised learning based on the low-density separation principle such as the cluster and manifold assumptions has been extensively studied in the last decades. However, such semi-supervised learning methods do not always perform well due to violation of the cluster and manifold assumptions. In this paper, we propose a novel approach to semi-supervised learning that does not require such restrictive assumptions. Our key idea is to combine learning from positive and negative data (standard supervised learning) and learning from positive and unlabeled data (PU learning), the latter is guaranteed to be able to utilize unlabeled data without the cluster and manifold assumptions. We theoretically and experimentally show the usefulness of our approach. version:1
arxiv-1605-01288 | From exp-concavity to variance control: O(1/n) rates and online-to-batch conversion with high probability | http://arxiv.org/abs/1605.01288 | id:1605.01288 author:Nishant A. Mehta category:cs.LG  published:2016-05-04 summary:We present an algorithm for the statistical learning setting with a bounded exp-concave loss in $d$ dimensions that obtains excess risk $O(d/n)$ with high probability: the dependence on the confidence parameter $\delta$ is polylogarithmic in $1/\delta$. The core technique is to boost the confidence of recent in-expectation $O(d/n)$ excess risk bounds for empirical risk minimization (ERM), without sacrificing the rate, by leveraging a Bernstein condition which holds due to exp-concavity. This Bernstein condition implies that the variance of excess loss random variables are controlled in terms of their excess risk. Using this variance control, we further show that a regret bound for any online learner in this setting translates to a high probability excess risk bound for the corresponding online-to-batch conversion of the online learner. We also show that with probability $1 - \delta$ the standard ERM method obtains excess risk $O(d (\log(n) + \log(1/\delta))/n)$. version:2
arxiv-1605-06931 | Learning the Structure of Nonlinear Dynamical Networks: An Information-Theoretic Perspective | http://arxiv.org/abs/1605.06931 | id:1605.06931 author:Oliver M. Cliff, Mikhail Prokopenko, Robert Fitch category:cs.LG cs.IT math.IT stat.ML  published:2016-05-23 summary:The behaviour of many real systems can be modelled by nonlinear dynamical systems whereby a latent system state is observed through a filter. We are interested in interacting subsystems of this form, which we model as a synchronous graph dynamical systems. Specifically, we study the structure learning problem for dynamical systems coupled via a directed acyclic graph. Unlike established structure learning procedures that find locally maximum posterior probabilities of a network structure containing latent variables, our work shows that we can exploit the properties of certain dynamical systems to compute globally optimal approximations of these distributions. We arrive at this result by use of time delay embedding theorems. Taking an information-theoretic perspective, we show that the log-likelihood has an intuitive interpretation in terms of information transfer. version:1
arxiv-1605-03498 | Deep Neural Networks Under Stress | http://arxiv.org/abs/1605.03498 | id:1605.03498 author:Micael Carvalho, Matthieu Cord, Sandra Avila, Nicolas Thome, Eduardo Valle category:cs.CV cs.AI  published:2016-05-11 summary:In recent years, deep architectures have been used for transfer learning with state-of-the-art performance in many datasets. The properties of their features remain, however, largely unstudied under the transfer perspective. In this work, we present an extensive analysis of the resiliency of feature vectors extracted from deep models, with special focus on the trade-off between performance and compression rate. By introducing perturbations to image descriptions extracted from a deep convolutional neural network, we change their precision and number of dimensions, measuring how it affects the final score. We show that deep features are more robust to these disturbances when compared to classical approaches, achieving a compression rate of 98.4%, while losing only 0.88% of their original score for Pascal VOC 2007. version:2
arxiv-1605-06921 | Generative Choreography using Deep Learning | http://arxiv.org/abs/1605.06921 | id:1605.06921 author:Luka Crnkovic-Friis, Louise Crnkovic-Friis category:cs.AI cs.LG cs.MM cs.NE  published:2016-05-23 summary:Recent advances in deep learning have enabled the extraction of high-level features from raw sensor data which has opened up new possibilities in many different fields, including computer generated choreography. In this paper we present a system chor-rnn for generating novel choreographic material in the nuanced choreographic language and style of an individual choreographer. It also shows promising results in producing a higher level compositional cohesion, rather than just generating sequences of movement. At the core of chor-rnn is a deep recurrent neural network trained on raw motion capture data and that can generate new dance sequences for a solo dancer. Chor-rnn can be used for collaborative human-machine choreography or as a creative catalyst, serving as inspiration for a choreographer. version:1
arxiv-1603-06352 | Online Learning with Low Rank Experts | http://arxiv.org/abs/1603.06352 | id:1603.06352 author:Elad Hazan, Tomer Koren, Roi Livni, Yishay Mansour category:cs.LG  published:2016-03-21 summary:We consider the problem of prediction with expert advice when the losses of the experts have low-dimensional structure: they are restricted to an unknown $d$-dimensional subspace. We devise algorithms with regret bounds that are independent of the number of experts and depend only on the rank $d$. For the stochastic model we show a tight bound of $\Theta(\sqrt{dT})$, and extend it to a setting of an approximate $d$ subspace. For the adversarial model we show an upper bound of $O(d\sqrt{T})$ and a lower bound of $\Omega(\sqrt{dT})$. version:2
arxiv-1605-06900 | Fast Stochastic Methods for Nonsmooth Nonconvex Optimization | http://arxiv.org/abs/1605.06900 | id:1605.06900 author:Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola category:math.OC cs.LG stat.ML  published:2016-05-23 summary:We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonconvex part is smooth and the nonsmooth part is convex. Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we show provably faster convergence than batch proximal gradient descent. Finally, we prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, that subsumes several recent works. This paper builds upon our recent series of papers on fast stochastic methods for smooth nonconvex optimization [22, 23], with a novel analysis for nonconvex and nonsmooth functions. version:1
arxiv-1605-06894 | DLAU: A Scalable Deep Learning Accelerator Unit on FPGA | http://arxiv.org/abs/1605.06894 | id:1605.06894 author:Chao Wang, Qi Yu, Lei Gong, Xi Li, Yuan Xie, Xuehai Zhou category:cs.LG cs.DC cs.NE  published:2016-05-23 summary:As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the low power cost, in this paper we design DLAU, which is a scalable accelerator architecture for large-scale deep learning networks using FPGA as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications. Experimental results on the state-of-the-art Xilinx FPGA board demonstrate that the DLAU accelerator is able to achieve up to 36.1x speedup comparing to the Intel Core2 processors, with the power consumption at 234mW. version:1
arxiv-1603-03130 | Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning | http://arxiv.org/abs/1603.03130 | id:1603.03130 author:Gang Niu, Marthinus Christoffel du Plessis, Tomoya Sakai, Yao Ma, Masashi Sugiyama category:cs.LG stat.ML  published:2016-03-10 summary:In PU learning, a binary classifier is trained from positive (P) and unlabeled (U) data without negative (N) data. Although N data is missing, it sometimes outperforms PN learning (i.e., ordinary supervised learning). Hitherto, neither theoretical nor experimental analysis has been given to explain this phenomenon. In this paper, we theoretically compare PU (and NU) learning against PN learning based on the upper bounds of estimation errors. We find simple conditions when PU and NU learning are likely to outperform PN learning, and we prove that, in terms of the upper bounds, either PU or NU learning (depending on the class-prior probability and the sizes of P and N data) given infinite U data will improve on PN learning. Our theoretical findings well agree with the experimental results on artificial and benchmark data even when the experimental setup does not match the theoretical assumptions exactly. version:2
arxiv-1605-06886 | Stochastic Patching Process | http://arxiv.org/abs/1605.06886 | id:1605.06886 author:Xuhui Fan, Bin Li, Yi Wang, Yang Wang, Fang Chen category:cs.AI stat.ML  published:2016-05-23 summary:Stochastic partition models tailor a product space into a number of rectangular regions such that the data within each region exhibit certain types of homogeneity. Due to constraints of partition strategy, existing models may cause unnecessary dissections in sparse regions when fitting data in dense regions. To alleviate this limitation, we propose a parsimonious partition model, named Stochastic Patching Process (SPP), to deal with multi-dimensional arrays. SPP adopts an "enclosing" strategy to attach rectangular patches to dense regions. SPP is self-consistent such that it can be extended to infinite arrays. We apply SPP to relational modeling and the experimental results validate its merit compared to the state-of-the-arts. version:1
arxiv-1605-06885 | Bridging Category-level and Instance-level Semantic Image Segmentation | http://arxiv.org/abs/1605.06885 | id:1605.06885 author:Zifeng Wu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2016-05-23 summary:We propose an approach to instance-level image segmentation that is built on top of category-level segmentation. Specifically, for each pixel in a semantic category mask, its corresponding instance bounding box is predicted using a deep fully convolutional regression network. Thus it follows a different pipeline to the popular detect-then-segment approaches that first predict instances' bounding boxes, which are the current state-of-the-art in instance segmentation. We show that, by leveraging the strength of our state-of-the-art semantic segmentation models, the proposed method can achieve comparable or even better results to detect-then-segment approaches. We make the following contributions. (i) First, we propose a simple yet effective approach to semantic instance segmentation. (ii) Second, we propose an online bootstrapping method during training, which is critically important for achieving good performance for both semantic category segmentation and instance-level segmentation. (iii) As the performance of semantic category segmentation has a significant impact on the instance-level segmentation, which is the second step of our approach, we train fully convolutional residual networks to achieve the best semantic category segmentation accuracy. On the PASCAL VOC 2012 dataset, we obtain the currently best mean intersection-over-union score of 79.1%. (iv) We also achieve state-of-the-art results for instance-level segmentation. version:1
arxiv-1605-06217 | Localizing by Describing: Attribute-Guided Attention Localization for Fine-Grained Recognition | http://arxiv.org/abs/1605.06217 | id:1605.06217 author:Xiao Liu, Jiang Wang, Shilei Wen, Errui Ding, Yuanqing Lin category:cs.CV  published:2016-05-20 summary:A key challenge in fine-grained recognition is how to find and represent discriminative local regions. Recent attention models are capable of learning discriminative region localizers only from category labels with reinforcement learning. However, not utilizing any explicit part information, they are not able to accurately find multiple distinctive regions. In this work, we introduce an attribute-guided attention localization scheme where the local region localizers are learned under the guidance of part attribute descriptions. By designing a novel reward strategy, we are able to learn to locate regions that are spatially and semantically distinctive with reinforcement learning algorithm. The attribute labeling requirement of the scheme is more amenable than the accurate part location annotation required by traditional part-based fine-grained recognition methods. Experimental results on the CUB-200-2011 dataset demonstrate the superiority of the proposed scheme on both fine-grained recognition and attribute recognition. version:2
arxiv-1605-04131 | Barzilai-Borwein Step Size for Stochastic Gradient Descent | http://arxiv.org/abs/1605.04131 | id:1605.04131 author:Conghui Tan, Shiqian Ma, Yu-Hong Dai, Yuqiu Qian category:math.OC cs.LG stat.ML  published:2016-05-13 summary:One of the major issues in stochastic gradient descent (SGD) methods is how to choose an appropriate step size while running the algorithm. Since the traditional line search technique does not apply for stochastic optimization algorithms, the common practice in SGD is either to use a diminishing step size, or to tune a fixed step size by hand, which can be time consuming in practice. In this paper, we propose to use the Barzilai-Borwein (BB) method to automatically compute step sizes for SGD and its variant: stochastic variance reduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and SVRG-BB. We prove that SVRG-BB converges linearly for strongly convex objective functions. As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result is missing in the literature. Numerical experiments on standard data sets show that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes, and is superior to some advanced SGD variants. version:2
arxiv-1605-06878 | Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition | http://arxiv.org/abs/1605.06878 | id:1605.06878 author:Xiu-Shen Wei, Chen-Wei Xie, Jianxin Wu category:cs.CV  published:2016-05-23 summary:Fine-grained image recognition is a challenging computer vision problem, due to the small inter-class variations caused by highly similar subordinate categories, and the large intra-class variations in poses, scales and rotations. In this paper, we propose a novel end-to-end Mask-CNN model without the fully connected layers for fine-grained recognition. Based on the part annotations of fine-grained images, the proposed model consists of a fully convolutional network to both locate the discriminative parts (e.g., head and torso), and more importantly generate object/part masks for selecting useful and meaningful convolutional descriptors. After that, a four-stream Mask-CNN model is built for aggregating the selected object- and part-level descriptors simultaneously. The proposed Mask-CNN model has the smallest number of parameters, lowest feature dimensionality and highest recognition accuracy when compared with state-of-the-arts fine-grained approaches. version:1
arxiv-1605-06863 | Self-expressive Dictionary Learning for Dynamic 3D Reconstruction | http://arxiv.org/abs/1605.06863 | id:1605.06863 author:Enliang Zheng, Dinghuang Ji, Enrique Dunn, Jan-Michael Frahm category:cs.CV  published:2016-05-22 summary:We target the problem of sparse 3D reconstruction of dynamic objects observed by multiple unsynchronized video cameras with unknown temporal overlap. To this end, we develop a framework to recover the unknown structure without sequencing information across video sequences. Our proposed compressed sensing framework poses the estimation of 3D structure as the problem of dictionary learning, where the dictionary is defined as an aggregation of the temporally varying 3D structures. Given the smooth motion of dynamic objects, we observe any element in the dictionary can be well approximated by a sparse linear combination of other elements in the same dictionary (i. e. self-expression). Moreover, the sparse coefficients describing a locally linear 3D structural interpolation reveal the local sequencing information. Our formulation optimizes a biconvex cost function that leverages a compressed sensing formulation and enforces both structural dependency coherence across video streams, as well as motion smoothness across estimates from common video sources. We further analyze the reconstructability of our approach under different capture scenarios, and its comparison and relation to existing methods. Experimental results on large amounts of synthetic data as well as real imagery demonstrate the effectiveness of our approach. version:1
arxiv-1512-08212 | Improving Facial Analysis and Performance Driven Animation through Disentangling Identity and Expression | http://arxiv.org/abs/1512.08212 | id:1512.08212 author:David Rim, Sina Honari, Md Kamrul Hasan, Chris Pal category:cs.CV  published:2015-12-27 summary:We present techniques for improving performance driven facial animation, emotion recognition, and facial key-point or landmark prediction using learned identity invariant representations. Established approaches to these problems can work well if sufficient examples and labels for a particular identity are available and factors of variation are highly controlled. However, labeled examples of facial expressions, emotions and key-points for new individuals are difficult and costly to obtain. In this paper we improve the ability of techniques to generalize to new and unseen individuals by explicitly modeling previously seen variations related to identity and expression. We use a weakly-supervised approach in which identity labels are used to learn the different factors of variation linked to identity separately from factors related to expression. We show how probabilistic modeling of these sources of variation allows one to learn identity-invariant representations for expressions which can then be used to identity-normalize various procedures for facial expression analysis and animation control. We also show how to extend the widely used techniques of active appearance models and constrained local models through replacing the underlying point distribution models which are typically constructed using principal component analysis with identity-expression factorized representations. We present a wide variety of experiments in which we consistently improve performance on emotion recognition, markerless performance-driven facial animation and facial key-point tracking. version:2
arxiv-1605-06855 | Smart broadcasting: Do you want to be seen? | http://arxiv.org/abs/1605.06855 | id:1605.06855 author:Mohammad Reza Karimi, Erfan Tavakoli, Mehrdad Farajtabar, Le Song, Manuel Gomez-Rodriguez category:cs.SI cs.LG stat.ML  published:2016-05-22 summary:Many users in online social networks are constantly trying to gain attention from their followers by broadcasting posts to them. These broadcasters are likely to gain greater attention if their posts can remain visible for a longer period of time among their followers' most recent feeds. Then when to post? In this paper, we study the problem of smart broadcasting using the framework of temporal point processes, where we model users feeds and posts as discrete events occurring in continuous time. Based on such continuous-time model, then choosing a broadcasting strategy for a user becomes a problem of designing the conditional intensity of her posting events. We derive a novel formula which links this conditional intensity with the visibility of the user in her followers' feeds. Furthermore, by exploiting this formula, we develop an efficient convex optimization framework for the when-to-post problem. Our method can find broadcasting strategies that reach a desired visibility level with provable guarantees. We experimented with data gathered from Twitter, and show that our framework can consistently make broadcasters' post more visible than alternatives. version:1
arxiv-1511-07838 | Dynamic Capacity Networks | http://arxiv.org/abs/1511.07838 | id:1511.07838 author:Amjad Almahairi, Nicolas Ballas, Tim Cooijmans, Yin Zheng, Hugo Larochelle, Aaron Courville category:cs.LG cs.NE  published:2015-11-24 summary:We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity sub-networks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN's output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance. version:7
arxiv-1605-06848 | Nonnegative Matrix Factorization Requires Irrationality | http://arxiv.org/abs/1605.06848 | id:1605.06848 author:Dmitry Chistikov, Stefan Kiefer, Ines Marušić, Mahsa Shirmohammadi, James Worrell category:cs.CC cs.LG math.NA  published:2016-05-22 summary:Nonnegative matrix factorization (NMF) is the problem of decomposing a given nonnegative $n \times m$ matrix $M$ into a product of a nonnegative $n \times d$ matrix $W$ and a nonnegative $d \times m$ matrix $H$. A longstanding open question, posed by Cohen and Rothblum in 1993, is whether a rational matrix $M$ always has an NMF of minimal inner dimension $d$ whose factors $W$ and $H$ are also rational. We answer this question negatively, by exhibiting a matrix for which $W$ and $H$ require irrational entries. version:1
arxiv-1605-06838 | Causality on Longitudinal Data: Stable Specification Search in Constrained Structural Equation Modeling | http://arxiv.org/abs/1605.06838 | id:1605.06838 author:Ridho Rahmadi, Perry Groot, Marieke MHC van Rijn, Jan AJG van den Brand, Marianne Heins, Hans Knoop, Tom Heskes, the Alzheimer's Dise, the MASTERPLAN Study Group, the OPTIMISTIC Consortium category:stat.ML cs.AI  published:2016-05-22 summary:Developing causal models from observational longitudinal studies is an important, ubiquitous problem in many disciplines. In the medical domain, especially in the case of rare diseases, revealing causal relationships from a given data set may lead to improvement of clinical practice, e.g., development of treatment and medication. Many causal discovery methods have been introduced in the past decades. A disadvantage of these causal discovery algorithms, however, is the inherent instability in structure estimation. With finite data samples small changes in the data can lead to completely different optimal structures. The present work presents a new causal discovery algorithm for longitudinal data that is robust for finite data samples. The method works as follows. We model causal models using structural equation models. Models are scored along two objectives: the model fit and the model complexity. Since both objectives are often conflicting we use a multi-objective evolutionary algorithm to search for Pareto optimal models. To handle the instability of small finite data samples, we repeatedly subsample the data and select those substructures (from optimal models) that are both stable and parsimonious which are then used to infer a causal model. In order to validate, we compare our method with the state-of-the-art PC algorithm on a simulated data set with the known ground truth model. Furthermore, we present the results of our discovery algorithm on three real-world longitudinal data sets about chronic fatigue syndrome, Alzheimer disease and chronic kidney disease that have been corroborated by medical experts and literature. version:1
arxiv-1604-02885 | Semantic 3D Reconstruction with Continuous Regularization and Ray Potentials Using a Visibility Consistency Constraint | http://arxiv.org/abs/1604.02885 | id:1604.02885 author:Nikolay Savinov, Christian Haene, Lubor Ladicky, Marc Pollefeys category:cs.CV  published:2016-04-11 summary:We propose an approach for dense semantic 3D reconstruction which uses a data term that is defined as potentials over viewing rays, combined with continuous surface area penalization. Our formulation is a convex relaxation which we augment with a crucial non-convex constraint that ensures exact handling of visibility. To tackle the non-convex minimization problem, we propose a majorize-minimize type strategy which converges to a critical point. We demonstrate the benefits of using the non-convex constraint experimentally. For the geometry-only case, we set a new state of the art on two datasets of the commonly used Middlebury multi-view stereo benchmark. Moreover, our general-purpose formulation directly reconstructs thin objects, which are usually treated with specialized algorithms. A qualitative evaluation on the dense semantic 3D reconstruction task shows that we improve significantly over previous methods. version:2
arxiv-1605-06820 | Automated Resolution Selection for Image Segmentation | http://arxiv.org/abs/1605.06820 | id:1605.06820 author:Fares Al-Qunaieer, Hamid R. Tizhoosh, Shahryar Rahnamayan category:cs.CV  published:2016-05-22 summary:It is well-known in image processing that computational cost increases rapidly with the number and dimensions of the images to be processed. Several fields, such as medical imaging, routinely use numerous very large images, which might also be 3D and/or captured at several frequency bands, all adding to the computational expense. Multiresolution analysis is a method of increasing the efficiency of the segmentation process. One multiresolution approach is the coarse-to-fine segmentation strategy, whereby the segmentation starts at a coarse resolution and is then fine-tuned during subsequent steps. The starting resolution for segmentation is generally selected arbitrarily with no clear selection criteria. The research reported in this paper showed that starting from different resolutions for image segmentation results in different accuracies and computational times, even for images of the same category (depicting similar scenes or objects). An automated method for resolution selection for an input image would thus be beneficial. This paper introduces a framework for the automated selection of the best resolution for image segmentation. We propose a measure for defining the best resolution based on user/system criteria, offering a trade-off between accuracy and computation time. A learning approach is then introduced for the selection of the resolution, whereby extracted image features are mapped to the previously determined best resolution. In the learning process, class (i.e., resolution) distribution is generally imbalanced, making effective learning from the data difficult. Experiments conducted with three datasets using two different segmentation algorithms show that the resolutions selected through learning enable much faster segmentation than the original ones, while retaining at least the original accuracy. version:1
arxiv-1602-01690 | Minimizing the Maximal Loss: How and Why? | http://arxiv.org/abs/1602.01690 | id:1602.01690 author:Shai Shalev-Shwartz, Yonatan Wexler category:cs.LG  published:2016-02-04 summary:A commonly used learning rule is to approximately minimize the \emph{average} loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the \emph{maximal} loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We prove that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers. version:2
arxiv-1605-06799 | Textual Paralanguage and its Implications for Marketing Communications | http://arxiv.org/abs/1605.06799 | id:1605.06799 author:Andrea Webb Luangrath, Joann Peck, Victor A. Barger category:cs.CL cs.SI  published:2016-05-22 summary:Both face-to-face communication and communication in online environments convey information beyond the actual verbal message. In a traditional face-to-face conversation, paralanguage, or the ancillary meaning- and emotion-laden aspects of speech that are not actual verbal prose, gives contextual information that allows interactors to more appropriately understand the message being conveyed. In this paper, we conceptualize textual paralanguage (TPL), which we define as written manifestations of nonverbal audible, tactile, and visual elements that supplement or replace written language and that can be expressed through words, symbols, images, punctuation, demarcations, or any combination of these elements. We develop a typology of textual paralanguage using data from Twitter, Facebook, and Instagram. We present a conceptual framework of antecedents and consequences of brands' use of textual paralanguage. Implications for theory and practice are discussed. version:1
arxiv-1605-06796 | Interpretable Distribution Features with Maximum Testing Power | http://arxiv.org/abs/1605.06796 | id:1605.06796 author:Wittawat Jitkrittum, Zoltan Szabo, Kacper Chwialkowski, Arthur Gretton category:stat.ML cs.LG 46E22  62G10 G.3; I.2.6  published:2016-05-22 summary:Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. An empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results. version:1
arxiv-1605-06792 | Active Nearest-Neighbor Learning in Metric Spaces | http://arxiv.org/abs/1605.06792 | id:1605.06792 author:Aryeh Kontorovich, Sivan Sabato, Ruth Urner category:cs.LG math.ST stat.TH  published:2016-05-22 summary:We propose a pool-based non-parametric active learning algorithm for general metric spaces, called called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. Our algorithm is based on a generalized sample compression scheme and a new label-efficient active model-selection procedure. version:1
