arxiv-1210-5474 | Disentangling Factors of Variation via Generative Entangling | http://arxiv.org/abs/1210.5474 | id:1210.5474 author:Guillaume Desjardins, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG cs.NE  published:2012-10-19 summary:Here we propose a novel model family with the objective of learning to disentangle the factors of variation in data. Our approach is based on the spike-and-slab restricted Boltzmann machine which we generalize to include higher-order interactions among multiple latent variables. Seen from a generative perspective, the multiplicative interactions emulates the entangling of factors of variation. Inference in the model can be seen as disentangling these generative factors. Unlike previous attempts at disentangling latent factors, the proposed model is trained using no supervised information regarding the latent factors. We apply our model to the task of facial expression classification. version:1
arxiv-1212-2508 | Collaborative Ensemble Learning: Combining Collaborative and Content-Based Information Filtering via Hierarchical Bayes | http://arxiv.org/abs/1212.2508 | id:1212.2508 author:Kai Yu, Anton Schwaighofer, Volker Tresp, Wei-Ying Ma, HongJiang Zhang category:cs.LG cs.IR stat.ML  published:2012-10-19 summary:Collaborative filtering (CF) and content-based filtering (CBF) have widely been used in information filtering applications. Both approaches have their strengths and weaknesses which is why researchers have developed hybrid systems. This paper proposes a novel approach to unify CF and CBF in a probabilistic framework, named collaborative ensemble learning. It uses probabilistic SVMs to model each user's profile (as CBF does).At the prediction phase, it combines a society OF users profiles, represented by their respective SVM models, to predict an active users preferences(the CF idea).The combination scheme is embedded in a probabilistic framework and retains an intuitive explanation.Moreover, collaborative ensemble learning does not require a global training stage and thus can incrementally incorporate new data.We report results based on two data sets. For the Reuters-21578 text data set, we simulate user ratings under the assumption that each user is interested in only one category. In the second experiment, we use users' opinions on a set of 642 art images that were collected through a web-based survey. For both data sets, collaborative ensemble achieved excellent performance in terms of recommendation accuracy. version:1
arxiv-1212-2510 | Markov Random Walk Representations with Continuous Distributions | http://arxiv.org/abs/1212.2510 | id:1212.2510 author:Chen-Hsiang Yeang, Martin Szummer category:cs.LG stat.ML  published:2012-10-19 summary:Representations based on random walks can exploit discrete data distributions for clustering and classification. We extend such representations from discrete to continuous distributions. Transition probabilities are now calculated using a diffusion equation with a diffusion coefficient that inversely depends on the data density. We relate this diffusion equation to a path integral and derive the corresponding path probability measure. The framework is useful for incorporating continuous data densities and prior knowledge. version:1
arxiv-1212-2511 | Stochastic complexity of Bayesian networks | http://arxiv.org/abs/1212.2511 | id:1212.2511 author:Keisuke Yamazaki, Sumio Watanbe category:cs.LG stat.ML  published:2012-10-19 summary:Bayesian networks are now being used in enormous fields, for example, diagnosis of a system, data mining, clustering and so on. In spite of their wide range of applications, the statistical properties have not yet been clarified, because the models are nonidentifiable and non-regular. In a Bayesian network, the set of its parameter for a smaller model is an analytic set with singularities in the space of large ones. Because of these singularities, the Fisher information matrices are not positive definite. In other words, the mathematical foundation for learning was not constructed. In recent years, however, we have developed a method to analyze non-regular models using algebraic geometry. This method revealed the relation between the models singularities and its statistical properties. In this paper, applying this method to Bayesian networks with latent variables, we clarify the order of the stochastic complexities.Our result claims that the upper bound of those is smaller than the dimension of the parameter space. This means that the Bayesian generalization error is also far smaller than that of regular model, and that Schwarzs model selection criterion BIC needs to be improved for Bayesian networks. version:1
arxiv-1212-2512 | A Generalized Mean Field Algorithm for Variational Inference in Exponential Families | http://arxiv.org/abs/1212.2512 | id:1212.2512 author:Eric P. Xing, Michael I. Jordan, Stuart Russell category:cs.LG stat.ML  published:2012-10-19 summary:The mean field methods, which entail approximating intractable probability distributions variationally with distributions from a tractable family, enjoy high efficiency, guaranteed convergence, and provide lower bounds on the true likelihood. But due to requirement for model-specific derivation of the optimization equations and unclear inference quality in various models, it is not widely used as a generic approximate inference algorithm. In this paper, we discuss a generalized mean field theory on variational approximation to a broad class of intractable distributions using a rich set of tractable distributions via constrained optimization over distribution spaces. We present a class of generalized mean field (GMF) algorithms for approximate inference in complex exponential family models, which entails limiting the optimization over the class of cluster-factorizable distributions. GMF is a generic method requiring no model-specific derivations. It factors a complex model into a set of disjoint variable clusters, and uses a set of canonical fix-point equations to iteratively update the cluster distributions, and converge to locally optimal cluster marginals that preserve the original dependency structure within each cluster, hence, fully decomposed the overall inference problem. We empirically analyzed the effect of different tractable family (clusters of different granularity) on inference quality, and compared GMF with BP on several canonical models. Possible extension to higher-order MF approximation is also discussed. version:1
arxiv-1212-2513 | Efficient Parametric Projection Pursuit Density Estimation | http://arxiv.org/abs/1212.2513 | id:1212.2513 author:Max Welling, Richard S. Zemel, Geoffrey E. Hinton category:cs.LG stat.ML  published:2012-10-19 summary:Product models of low dimensional experts are a powerful way to avoid the curse of dimensionality. We present the ``under-complete product of experts' (UPoE), where each expert models a one dimensional projection of the data. The UPoE is fully tractable and may be interpreted as a parametric probabilistic model for projection pursuit. Its ML learning rules are identical to the approximate learning rules proposed before for under-complete ICA. We also derive an efficient sequential learning algorithm and discuss its relationship to projection pursuit density estimation and feature induction algorithms for additive random field models. version:1
arxiv-1212-2514 | Boltzmann Machine Learning with the Latent Maximum Entropy Principle | http://arxiv.org/abs/1212.2514 | id:1212.2514 author:Shaojun Wang, Dale Schuurmans, Fuchun Peng, Yunxin Zhao category:cs.LG stat.ML  published:2012-10-19 summary:We present a new statistical learning paradigm for Boltzmann machines based on a new inference principle we have proposed: the latent maximum entropy principle (LME). LME is different both from Jaynes maximum entropy principle and from standard maximum likelihood estimation.We demonstrate the LME principle BY deriving new algorithms for Boltzmann machine parameter estimation, and show how robust and fast new variant of the EM algorithm can be developed.Our experiments show that estimation based on LME generally yields better results than maximum likelihood estimation, particularly when inferring hidden units from small amounts of data. version:1
arxiv-1212-2516 | Learning Measurement Models for Unobserved Variables | http://arxiv.org/abs/1212.2516 | id:1212.2516 author:Ricardo Silva, Richard Scheines, Clark Glymour, Peter L. Spirtes category:cs.LG stat.ML  published:2012-10-19 summary:Observed associations in a database may be due in whole or part to variations in unrecorded (latent) variables. Identifying such variables and their causal relationships with one another is a principal goal in many scientific and practical domains. Previous work shows that, given a partition of observed variables such that members of a class share only a single latent common cause, standard search algorithms for causal Bayes nets can infer structural relations between latent variables. We introduce an algorithm for discovering such partitions when they exist. Uniquely among available procedures, the algorithm is (asymptotically) correct under standard assumptions in causal Bayes net search algorithms, requires no prior knowledge of the number of latent variables, and does not depend on the mathematical form of the relationships among the latent variables. We evaluate the algorithm on a variety of simulated data sets. version:1
arxiv-1212-2517 | Learning Module Networks | http://arxiv.org/abs/1212.2517 | id:1212.2517 author:Eran Segal, Dana Pe'er, Aviv Regev, Daphne Koller, Nir Friedman category:cs.LG cs.CE stat.ML  published:2012-10-19 summary:Methods for learning Bayesian network structure can discover dependency structure between observed variables, and have been shown to be useful in many applications. However, in domains that involve a large number of variables, the space of possible network structures is enormous, making it difficult, for both computational and statistical reasons, to identify a good model. In this paper, we consider a solution to this problem, suitable for domains where many variables have similar behavior. Our method is based on a new class of models, which we call module networks. A module network explicitly represents the notion of a module - a set of variables that have the same parents in the network and share the same conditional probability distribution. We define the semantics of module networks, and describe an algorithm that learns a module network from data. The algorithm learns both the partitioning of the variables into modules and the dependency structure between the variables. We evaluate our algorithm on synthetic data, and on real data in the domains of gene expression and the stock market. Our results show that module networks generalize better than Bayesian networks, and that the learned module network structure reveals regularities that are obscured in learned Bayesian networks. version:1
arxiv-1212-2490 | On the Convergence of Bound Optimization Algorithms | http://arxiv.org/abs/1212.2490 | id:1212.2490 author:Ruslan R Salakhutdinov, Sam T Roweis, Zoubin Ghahramani category:cs.LG stat.ML  published:2012-10-19 summary:Many practitioners who use the EM algorithm complain that it is sometimes slow. When does this happen, and what can be done about it? In this paper, we study the general class of bound optimization algorithms - including Expectation-Maximization, Iterative Scaling and CCCP - and their relationship to direct optimization algorithms such as gradient-based methods for parameter learning. We derive a general relationship between the updates performed by bound optimization methods and those of gradient and second-order methods and identify analytic conditions under which bound optimization algorithms exhibit quasi-Newton behavior, and conditions under which they possess poor, first-order convergence. Based on this analysis, we consider several specific algorithms, interpret and analyze their convergence properties and provide some recipes for preprocessing input to these algorithms to yield faster convergence behavior. We report empirical results supporting our analysis and showing that simple data preprocessing can result in dramatically improved performance of bound optimizers in practice. version:1
arxiv-1212-2491 | Automated Analytic Asymptotic Evaluation of the Marginal Likelihood for Latent Models | http://arxiv.org/abs/1212.2491 | id:1212.2491 author:Dmitry Rusakov, Dan Geiger category:cs.LG stat.ML  published:2012-10-19 summary:We present and implement two algorithms for analytic asymptotic evaluation of the marginal likelihood of data given a Bayesian network with hidden nodes. As shown by previous work, this evaluation is particularly hard for latent Bayesian network models, namely networks that include hidden variables, where asymptotic approximation deviates from the standard BIC score. Our algorithms solve two central difficulties in asymptotic evaluation of marginal likelihood integrals, namely, evaluation of regular dimensionality drop for latent Bayesian network models and computation of non-standard approximation formulas for singular statistics for these models. The presented algorithms are implemented in Matlab and Maple and their usage is demonstrated for marginal likelihood approximations for Bayesian networks with hidden variables. version:1
arxiv-1212-2494 | Learning Generative Models of Similarity Matrices | http://arxiv.org/abs/1212.2494 | id:1212.2494 author:Romer Rosales, Brendan J. Frey category:cs.LG stat.ML  published:2012-10-19 summary:We describe a probabilistic (generative) view of affinity matrices along with inference algorithms for a subclass of problems associated with data clustering. This probabilistic view is helpful in understanding different models and algorithms that are based on affinity functions OF the data. IN particular, we show how(greedy) inference FOR a specific probabilistic model IS equivalent TO the spectral clustering algorithm.It also provides a framework FOR developing new algorithms AND extended models. AS one CASE, we present new generative data clustering models that allow us TO infer the underlying distance measure suitable for the clustering problem at hand. These models seem to perform well in a larger class of problems for which other clustering algorithms (including spectral clustering) usually fail. Experimental evaluation was performed in a variety point data sets, showing excellent performance. version:1
arxiv-1212-2498 | Learning Continuous Time Bayesian Networks | http://arxiv.org/abs/1212.2498 | id:1212.2498 author:Uri Nodelman, Christian R. Shelton, Daphne Koller category:cs.LG stat.ML  published:2012-10-19 summary:Continuous time Bayesian networks (CTBNs) describe structured stochastic processes with finitely many states that evolve over continuous time. A CTBN is a directed (possibly cyclic) dependency graph over a set of variables, each of which represents a finite state continuous time Markov process whose transition model is a function of its parents. We address the problem of learning parameters and structure of a CTBN from fully observed data. We define a conjugate prior for CTBNs, and show how it can be used both for Bayesian parameter estimation and as the basis of a Bayesian score for structure learning. Because acyclicity is not a constraint in CTBNs, we can show that the structure learning problem is significantly easier, both in theory and in practice, than structure learning for dynamic Bayesian networks (DBNs). Furthermore, as CTBNs can tailor the parameters and dependency structure to the different time granularities of the evolution of different variables, they can provide a better fit to continuous-time processes than DBNs with a fixed time granularity. version:1
arxiv-1212-2500 | On Local Optima in Learning Bayesian Networks | http://arxiv.org/abs/1212.2500 | id:1212.2500 author:Jens D. Nielsen, Tomas Kocka, Jose M. Pena category:cs.LG cs.AI stat.ML  published:2012-10-19 summary:This paper proposes and evaluates the k-greedy equivalence search algorithm (KES) for learning Bayesian networks (BNs) from complete data. The main characteristic of KES is that it allows a trade-off between greediness and randomness, thus exploring different good local optima. When greediness is set at maximum, KES corresponds to the greedy equivalence search algorithm (GES). When greediness is kept at minimum, we prove that under mild assumptions KES asymptotically returns any inclusion optimal BN with nonzero probability. Experimental results for both synthetic and real data are reported showing that KES often finds a better local optima than GES. Moreover, we use KES to experimentally confirm that the number of different local optima is often huge. version:1
arxiv-1212-2503 | Practically Perfect | http://arxiv.org/abs/1212.2503 | id:1212.2503 author:Christopher Meek, David Maxwell Chickering category:cs.AI stat.ML  published:2012-10-19 summary:The property of perfectness plays an important role in the theory of Bayesian networks. First, the existence of perfect distributions for arbitrary sets of variables and directed acyclic graphs implies that various methods for reading independence from the structure of the graph (e.g., Pearl, 1988; Lauritzen, Dawid, Larsen & Leimer, 1990) are complete. Second, the asymptotic reliability of various search methods is guaranteed under the assumption that the generating distribution is perfect (e.g., Spirtes, Glymour & Scheines, 2000; Chickering & Meek, 2002). We provide a lower-bound on the probability of sampling a non-perfect distribution when using a fixed number of bits to represent the parameters of the Bayesian network. This bound approaches zero exponentially fast as one increases the number of bits used to represent the parameters. This result implies that perfect distributions with fixed-length representations exist. We also provide a lower-bound on the number of bits needed to guarantee that a distribution sampled from a uniform Dirichlet distribution is perfect with probability greater than 1/2. This result is useful for constructing randomized reductions for hardness proofs. version:1
arxiv-1212-2504 | Efficiently Inducing Features of Conditional Random Fields | http://arxiv.org/abs/1212.2504 | id:1212.2504 author:Andrew McCallum category:cs.LG stat.ML  published:2012-10-19 summary:Conditional Random Fields (CRFs) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. A key advantage of these models is their great flexibility to include a wide array of overlapping, multi-granularity, non-independent features of the input. In face of this freedom, an important question that remains is, what features should be used? This paper presents a feature induction method for CRFs. Founded on the principle of constructing only those feature conjunctions that significantly increase log-likelihood, the approach is based on that of Della Pietra et al [1997], but altered to work with conditional rather than joint probabilities, and with additional modifications for providing tractability specifically for a sequence model. In comparison with traditional approaches, automated feature induction offers both improved accuracy and more than an order of magnitude reduction in feature count; it enables the use of richer, higher-order Markov models, and offers more freedom to liberally guess about which atomic features may be relevant to a task. The induction method applies to linear-chain CRFs, as well as to more arbitrary CRF structures, also known as Relational Markov Networks [Taskar & Koller, 2002]. We present experimental results on a named entity extraction task. version:1
arxiv-1212-2471 | Monte Carlo Matrix Inversion Policy Evaluation | http://arxiv.org/abs/1212.2471 | id:1212.2471 author:Fletcher Lu, Dale Schuurmans category:cs.LG cs.AI cs.NA  published:2012-10-19 summary:In 1950, Forsythe and Leibler (1950) introduced a statistical technique for finding the inverse of a matrix by characterizing the elements of the matrix inverse as expected values of a sequence of random walks. Barto and Duff (1994) subsequently showed relations between this technique and standard dynamic programming and temporal differencing methods. The advantage of the Monte Carlo matrix inversion (MCMI) approach is that it scales better with respect to state-space size than alternative techniques. In this paper, we introduce an algorithm for performing reinforcement learning policy evaluation using MCMI. We demonstrate that MCMI improves on runtime over a maximum likelihood model-based policy evaluation approach and on both runtime and accuracy over the temporal differencing (TD) policy evaluation approach. We further improve on MCMI policy evaluation by adding an importance sampling technique to our algorithm to reduce the variance of our estimator. Lastly, we illustrate techniques for scaling up MCMI to large state spaces in order to perform policy improvement. version:1
arxiv-1212-2472 | Budgeted Learning of Naive-Bayes Classifiers | http://arxiv.org/abs/1212.2472 | id:1212.2472 author:Daniel J. Lizotte, Omid Madani, Russell Greiner category:cs.LG stat.ML  published:2012-10-19 summary:Frequently, acquiring training data has an associated cost. We consider the situation where the learner may purchase data during training, subject TO a budget. IN particular, we examine the CASE WHERE each feature label has an associated cost, AND the total cost OF ALL feature labels acquired during training must NOT exceed the budget.This paper compares methods FOR choosing which feature label TO purchase next, given the budget AND the CURRENT belief state OF naive Bayes model parameters.Whereas active learning has traditionally focused ON myopic(greedy) strategies FOR query selection, this paper presents a tractable method FOR incorporating knowledge OF the budget INTO the decision making process, which improves performance. version:1
arxiv-1212-2474 | Learning Riemannian Metrics | http://arxiv.org/abs/1212.2474 | id:1212.2474 author:Guy Lebanon category:cs.LG stat.ML  published:2012-10-19 summary:We propose a solution to the problem of estimating a Riemannian metric associated with a given differentiable manifold. The metric learning problem is based on minimizing the relative volume of a given set of points. We derive the details for a family of metrics on the multinomial simplex. The resulting metric has applications in text classification and bears some similarity to TFIDF representation of text documents. version:1
arxiv-1212-2475 | Efficient Gradient Estimation for Motor Control Learning | http://arxiv.org/abs/1212.2475 | id:1212.2475 author:Gregory Lawrence, Noah Cowan, Stuart Russell category:cs.LG cs.SY  published:2012-10-19 summary:The task of estimating the gradient of a function in the presence of noise is central to several forms of reinforcement learning, including policy search methods. We present two techniques for reducing gradient estimation errors in the presence of observable input noise applied to the control signal. The first method extends the idea of a reinforcement baseline by fitting a local linear model to the function whose gradient is being estimated; we show how to find the linear model that minimizes the variance of the gradient estimate, and how to estimate the model from data. The second method improves this further by discounting components of the gradient vector that have high variance. These methods are applied to the problem of motor control learning, where actuator noise has a significant influence on behavior. In particular, we apply the techniques to learn locally optimal controllers for a dart-throwing task using a simulated three-link arm; we demonstrate that proposed methods significantly improve the reward function gradient estimate and, consequently, the learning curve, over existing methods. version:1
arxiv-1212-2477 | 1 Billion Pages = 1 Million Dollars? Mining the Web to Play "Who Wants to be a Millionaire?" | http://arxiv.org/abs/1212.2477 | id:1212.2477 author:Shyong, K. Lam, David M Pennock, Dan Cosley, Steve Lawrence category:cs.IR cs.CL  published:2012-10-19 summary:We exploit the redundancy and volume of information on the web to build a computerized player for the ABC TV game show 'Who Wants To Be A Millionaire?' The player consists of a question-answering module and a decision-making module. The question-answering module utilizes question transformation techniques, natural language parsing, multiple information retrieval algorithms, and multiple search engines; results are combined in the spirit of ensemble learning using an adaptive weighting scheme. Empirically, the system correctly answers about 75% of questions from the Millionaire CD-ROM, 3rd edition - general-interest trivia questions often about popular culture and common knowledge. The decision-making module chooses from allowable actions in the game in order to maximize expected risk-adjusted winnings, where the estimated probability of answering correctly is a function of past performance and confidence in in correctly answering the current question. When given a six question head start (i.e., when starting from the $2,000 level), we find that the system performs about as well on average as humans starting at the beginning. Our system demonstrates the potential of simple but well-chosen techniques for mining answers from unstructured information such as the web. version:1
arxiv-1212-2480 | Approximate Inference and Constrained Optimization | http://arxiv.org/abs/1212.2480 | id:1212.2480 author:Tom Heskes, Kees Albers, Hilbert Kappen category:cs.LG cs.AI stat.ML  published:2012-10-19 summary:Loopy and generalized belief propagation are popular algorithms for approximate inference in Markov random fields and Bayesian networks. Fixed points of these algorithms correspond to extrema of the Bethe and Kikuchi free energy. However, belief propagation does not always converge, which explains the need for approaches that explicitly minimize the Kikuchi/Bethe free energy, such as CCCP and UPS. Here we describe a class of algorithms that solves this typically nonconvex constrained minimization of the Kikuchi free energy through a sequence of convex constrained minimizations of upper bounds on the Kikuchi free energy. Intuitively one would expect tighter bounds to lead to faster algorithms, which is indeed convincingly demonstrated in our simulations. Several ideas are applied to obtain tight convex bounds that yield dramatic speed-ups over CCCP. version:1
arxiv-1212-2483 | Sufficient Dimensionality Reduction with Irrelevant Statistics | http://arxiv.org/abs/1212.2483 | id:1212.2483 author:Amir Globerson, Gal Chechik, Naftali Tishby category:cs.LG stat.ML  published:2012-10-19 summary:The problem of finding a reduced dimensionality representation of categorical variables while preserving their most relevant characteristics is fundamental for the analysis of complex data. Specifically, given a co-occurrence matrix of two variables, one often seeks a compact representation of one variable which preserves information about the other variable. We have recently introduced ``Sufficient Dimensionality Reduction' [GT-2003], a method that extracts continuous reduced dimensional features whose measurements (i.e., expectation values) capture maximal mutual information among the variables. However, such measurements often capture information that is irrelevant for a given task. Widely known examples are illumination conditions, which are irrelevant as features for face recognition, writing style which is irrelevant as a feature for content classification, and intonation which is irrelevant as a feature for speech recognition. Such irrelevance cannot be deduced apriori, since it depends on the details of the task, and is thus inherently ill defined in the purely unsupervised case. Separating relevant from irrelevant features can be achieved using additional side data that contains such irrelevant structures. This approach was taken in [CT-2002], extending the information bottleneck method, which uses clustering to compress the data. Here we use this side-information framework to identify features whose measurements are maximally informative for the original data set, but carry as little information as possible on a side data set. In statistical terms this can be understood as extracting statistics which are maximally sufficient for the original dataset, while simultaneously maximally ancillary for the side dataset. We formulate this tradeoff as a constrained optimization problem and characterize its solutions. We then derive a gradient descent algorithm for this problem, which is based on the Generalized Iterative Scaling method for finding maximum entropy distributions. The method is demonstrated on synthetic data, as well as on real face recognition datasets, and is shown to outperform standard methods such as oriented PCA. version:1
arxiv-1212-2487 | Locally Weighted Naive Bayes | http://arxiv.org/abs/1212.2487 | id:1212.2487 author:Eibe Frank, Mark Hall, Bernhard Pfahringer category:cs.LG stat.ML  published:2012-10-19 summary:Despite its simplicity, the naive Bayes classifier has surprised machine learning researchers by exhibiting good performance on a variety of learning problems. Encouraged by these results, researchers have looked to overcome naive Bayes primary weakness - attribute independence - and improve the performance of the algorithm. This paper presents a locally weighted version of naive Bayes that relaxes the independence assumption by learning local models at prediction time. Experimental results show that locally weighted naive Bayes rarely degrades accuracy compared to standard naive Bayes and, in many cases, improves accuracy dramatically. The main advantage of this method compared to other techniques for enhancing naive Bayes is its conceptual and computational simplicity. version:1
arxiv-1212-2488 | A Distance-Based Branch and Bound Feature Selection Algorithm | http://arxiv.org/abs/1212.2488 | id:1212.2488 author:Ari Frank, Dan Geiger, Zohar Yakhini category:cs.LG stat.ML  published:2012-10-19 summary:There is no known efficient method for selecting k Gaussian features from n which achieve the lowest Bayesian classification error. We show an example of how greedy algorithms faced with this task are led to give results that are not optimal. This motivates us to propose a more robust approach. We present a Branch and Bound algorithm for finding a subset of k independent Gaussian features which minimizes the naive Bayesian classification error. Our algorithm uses additive monotonic distance measures to produce bounds for the Bayesian classification error in order to exclude many feature subsets from evaluation, while still returning an optimal solution. We test our method on synthetic data as well as data obtained from gene expression profiling. version:1
arxiv-1212-2460 | The Information Bottleneck EM Algorithm | http://arxiv.org/abs/1212.2460 | id:1212.2460 author:Gal Elidan, Nir Friedman category:cs.LG stat.ML  published:2012-10-19 summary:Learning with hidden variables is a central challenge in probabilistic graphical models that has important implications for many real-life problems. The classical approach is using the Expectation Maximization (EM) algorithm. This algorithm, however, can get trapped in local maxima. In this paper we explore a new approach that is based on the Information Bottleneck principle. In this approach, we view the learning problem as a tradeoff between two information theoretic objectives. The first is to make the hidden variables uninformative about the identity of specific instances. The second is to make the hidden variables informative about the observed attributes. By exploring different tradeoffs between these two objectives, we can gradually converge on a high-scoring solution. As we show, the resulting, Information Bottleneck Expectation Maximization (IB-EM) algorithm, manages to find solutions that are superior to standard EM methods. version:1
arxiv-1212-2462 | A New Algorithm for Maximum Likelihood Estimation in Gaussian Graphical Models for Marginal Independence | http://arxiv.org/abs/1212.2462 | id:1212.2462 author:Mathias Drton, Thomas S. Richardson category:stat.ME cs.LG stat.ML  published:2012-10-19 summary:Graphical models with bi-directed edges (<->) represent marginal independence: the absence of an edge between two vertices indicates that the corresponding variables are marginally independent. In this paper, we consider maximum likelihood estimation in the case of continuous variables with a Gaussian joint distribution, sometimes termed a covariance graph model. We present a new fitting algorithm which exploits standard regression techniques and establish its convergence properties. Moreover, we contrast our procedure to existing estimation methods. version:1
arxiv-1212-2464 | A Robust Independence Test for Constraint-Based Learning of Causal Structure | http://arxiv.org/abs/1212.2464 | id:1212.2464 author:Denver Dash, Marek J. Druzdzel category:cs.AI cs.LG stat.ML  published:2012-10-19 summary:Constraint-based (CB) learning is a formalism for learning a causal network with a database D by performing a series of conditional-independence tests to infer structural information. This paper considers a new test of independence that combines ideas from Bayesian learning, Bayesian network inference, and classical hypothesis testing to produce a more reliable and robust test. The new test can be calculated in the same asymptotic time and space required for the standard tests such as the chi-squared test, but it allows the specification of a prior distribution over parameters and can be used when the database is incomplete. We prove that the test is correct, and we demonstrate empirically that, when used with a CB causal discovery algorithm with noninformative priors, it recovers structural features more reliably and it produces networks with smaller KL-Divergence, especially as the number of nodes increases or the number of records decreases. Another benefit is the dramatic reduction in the probability that a CB algorithm will stall during the search, providing a remedy for an annoying problem plaguing CB learning when the database is small. version:1
arxiv-1212-2466 | On Information Regularization | http://arxiv.org/abs/1212.2466 | id:1212.2466 author:Adrian Corduneanu, Tommi S. Jaakkola category:cs.LG stat.ML  published:2012-10-19 summary:We formulate a principle for classification with the knowledge of the marginal distribution over the data points (unlabeled data). The principle is cast in terms of Tikhonov style regularization where the regularization penalty articulates the way in which the marginal density should constrain otherwise unrestricted conditional distributions. Specifically, the regularization penalty penalizes any information introduced between the examples and labels beyond what is provided by the available labeled examples. The work extends Szummer and Jaakkola's information regularization (NIPS 2002) to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation. We show in addition how the information regularizer can be used as a measure of complexity of the classification task with unlabeled data and prove a relevant sample-complexity bound. We illustrate the regularization principle in practice by restricting the class of conditional distributions to be logistic regression models and constructing the regularization penalty from a finite set of unlabeled examples. version:1
arxiv-1212-2468 | Large-Sample Learning of Bayesian Networks is NP-Hard | http://arxiv.org/abs/1212.2468 | id:1212.2468 author:David Maxwell Chickering, Christopher Meek, David Heckerman category:cs.LG cs.AI stat.ML  published:2012-10-19 summary:In this paper, we provide new complexity results for algorithms that learn discrete-variable Bayesian networks from data. Our results apply whenever the learning algorithm uses a scoring criterion that favors the simplest model able to represent the generative distribution exactly. Our results therefore hold whenever the learning algorithm uses a consistent scoring criterion and is applied to a sufficiently large dataset. We show that identifying high-scoring structures is hard, even when we are given an independence oracle, an inference oracle, and/or an information oracle. Our negative results also apply to the learning of discrete-variable Bayesian networks in which each node has at most k parents, for all k > 3. version:1
arxiv-1212-2470 | Reasoning about Bayesian Network Classifiers | http://arxiv.org/abs/1212.2470 | id:1212.2470 author:Hei Chan, Adnan Darwiche category:cs.LG cs.AI stat.ML  published:2012-10-19 summary:Bayesian network classifiers are used in many fields, and one common class of classifiers are naive Bayes classifiers. In this paper, we introduce an approach for reasoning about Bayesian network classifiers in which we explicitly convert them into Ordered Decision Diagrams (ODDs), which are then used to reason about the properties of these classifiers. Specifically, we present an algorithm for converting any naive Bayes classifier into an ODD, and we show theoretically and experimentally that this algorithm can give us an ODD that is tractable in size even given an intractable number of instances. Since ODDs are tractable representations of classifiers, our algorithm allows us to efficiently test the equivalence of two naive Bayes classifiers and characterize discrepancies between them. We also show a number of additional results including a count of distinct classifiers that can be induced by changing some CPT in a naive Bayes classifier, and the range of allowable changes to a CPT which keeps the current classifier unchanged. version:1
arxiv-1212-2442 | Active Collaborative Filtering | http://arxiv.org/abs/1212.2442 | id:1212.2442 author:Craig Boutilier, Richard S. Zemel, Benjamin Marlin category:cs.IR cs.LG stat.ML  published:2012-10-19 summary:Collaborative filtering (CF) allows the preferences of multiple users to be pooled to make recommendations regarding unseen products. We consider in this paper the problem of online and interactive CF: given the current ratings associated with a user, what queries (new ratings) would most improve the quality of the recommendations made? We cast this terms of expected value of information (EVOI); but the online computational cost of computing optimal queries is prohibitive. We show how offline prototyping and computation of bounds on EVOI can be used to dramatically reduce the required online computation. The framework we develop is general, but we focus on derivations and empirical study in the specific case of the multiple-cause vector quantization model. version:1
arxiv-1212-2447 | Bayesian Hierarchical Mixtures of Experts | http://arxiv.org/abs/1212.2447 | id:1212.2447 author:Christopher M. Bishop, Markus Svensen category:cs.LG stat.ML  published:2012-10-19 summary:The Hierarchical Mixture of Experts (HME) is a well-known tree-based model for regression and classification, based on soft probabilistic splits. In its original formulation it was trained by maximum likelihood, and is therefore prone to over-fitting. Furthermore the maximum likelihood framework offers no natural metric for optimizing the complexity and structure of the tree. Previous attempts to provide a Bayesian treatment of the HME model have relied either on ad-hoc local Gaussian approximations or have dealt with related models representing the joint distribution of both input and output variables. In this paper we describe a fully Bayesian treatment of the HME model based on variational inference. By combining local and global variational methods we obtain a rigourous lower bound on the marginal probability of the data under the model. This bound is optimized during the training phase, and its resulting value can be used for model order selection. We present results using this approach for a data set describing robot arm kinematics. version:1
arxiv-1212-2453 | Web-Based Question Answering: A Decision-Making Perspective | http://arxiv.org/abs/1212.2453 | id:1212.2453 author:David Azari, Eric J. Horvitz, Susan Dumais, Eric Brill category:cs.IR cs.CL  published:2012-10-19 summary:We describe an investigation of the use of probabilistic models and cost-benefit analyses to guide resource-intensive procedures used by a Web-based question answering system. We first provide an overview of research on question-answering systems. Then, we present details on AskMSR, a prototype web-based question answering system. We discuss Bayesian analyses of the quality of answers generated by the system and show how we can endow the system with the ability to make decisions about the number of queries issued to a search engine, given the cost of queries and the expected value of query results in refining an ultimate answer. Finally, we review the results of a set of experiments. version:1
arxiv-1210-5394 | Bayesian Estimation for Continuous-Time Sparse Stochastic Processes | http://arxiv.org/abs/1210.5394 | id:1210.5394 author:Arash Amini, Ulugbek S. Kamilov, Emrah Bostan, Michael Unser category:cs.LG  published:2012-10-19 summary:We consider continuous-time sparse stochastic processes from which we have only a finite number of noisy/noiseless samples. Our goal is to estimate the noiseless samples (denoising) and the signal in-between (interpolation problem). By relying on tools from the theory of splines, we derive the joint a priori distribution of the samples and show how this probability density function can be factorized. The factorization enables us to tractably implement the maximum a posteriori and minimum mean-square error (MMSE) criteria as two statistical approaches for estimating the unknowns. We compare the derived statistical methods with well-known techniques for the recovery of sparse signals, such as the $\ell_1$ norm and Log ($\ell_1$-$\ell_0$ relaxation) regularization methods. The simulation results show that, under certain conditions, the performance of the regularization techniques can be very close to that of the MMSE estimator. version:1
arxiv-1210-5345 | Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions | http://arxiv.org/abs/1210.5345 | id:1210.5345 author:Alexandra Carpentier, Rémi Munos category:stat.ML  published:2012-10-19 summary:We consider the problem of adaptive stratified sampling for Monte Carlo integration of a differentiable function given a finite number of evaluations to the function. We construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy). We prove that the estimate returned by the algorithm is almost similarly accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and provide a finite-sample analysis. version:1
arxiv-1210-5321 | The origin of Mayan languages from Formosan language group of Austronesian | http://arxiv.org/abs/1210.5321 | id:1210.5321 author:Koji Ohnishi category:cs.CL q-bio.PE  published:2012-10-19 summary:Basic body-part names (BBPNs) were defined as body-part names in Swadesh basic 200 words. Non-Mayan cognates of Mayan (MY) BBPNs were extensively searched for, by comparing with non-MY vocabulary, including ca.1300 basic words of 82 AN languages listed by Tryon (1985), etc. Thus found cognates (CGs) in non-MY are listed in Table 1, as classified by language groups to which most similar cognates (MSCs) of MY BBPNs belong. CGs of MY are classified to 23 mutually unrelated CG-items, of which 17.5 CG-items have their MSCs in Austronesian (AN), giving its closest similarity score (CSS), CSS(AN) = 17.5, which consists of 10.33 MSCs in Formosan, 1.83 MSCs in Western Malayo-Polynesian (W.MP), 0.33 in Central MP, 0.0 in SHWNG, and 5.0 in Oceanic [i.e., CSS(FORM)= 10.33, CSS(W.MP) = 1.88, ..., CSS(OC)= 5.0]. These CSSs for language (sub)groups are also listed in the underline portion of every section of (Section1 - Section 6) in Table 1. Chi-squar test (degree of freedom = 1) using [Eq 1] and [Eqs.2] revealed that MSCs of MY BBPNs are distributed in Formosan in significantly higher frequency (P < 0.001) than in other subgroups of AN, as well as than in non-AN languages. MY is thus concluded to have been derived from Formosan of AN. Eskimo shows some BBPN similarities to FORM and MY. version:1
arxiv-1204-2353 | Least Absolute Gradient Selector: Statistical Regression via Pseudo-Hard Thresholding | http://arxiv.org/abs/1204.2353 | id:1204.2353 author:Kun Yang category:stat.ML stat.AP stat.ME  published:2012-04-11 summary:Variable selection in linear models plays a pivotal role in modern statistics. Hard-thresholding methods such as $l_0$ regularization are theoretically ideal but computationally infeasible. In this paper, we propose a new approach, called the LAGS, short for "least absulute gradient selector", to this challenging yet interesting problem by mimicking the discrete selection process of $l_0$ regularization. To estimate $\beta$ under the influence of noise, we consider, nevertheless, the following convex program [\hat{\beta} = \textrm{arg min}\frac{1}{n}\ X^{T}(y - X\beta)\ _1 + \lambda_n\sum_{i = 1}^pw_i(y;X;n) \beta_i ] $\lambda_n > 0$ controls the sparsity and $w_i > 0$ dependent on $y, X$ and $n$ is the weights on different $\beta_i$; $n$ is the sample size. Surprisingly, we shall show in the paper, both geometrically and analytically, that LAGS enjoys two attractive properties: (1) LAGS demonstrates discrete selection behavior and hard thresholding property as $l_0$ regularization by strategically chosen $w_i$, we call this property "pseudo-hard thresholding"; (2) Asymptotically, LAGS is consistent and capable of discovering the true model; nonasymptotically, LAGS is capable of identifying the sparsity in the model and the prediction error of the coefficients is bounded at the noise level up to a logarithmic factor---$\log p$, where $p$ is the number of predictors. Computationally, LAGS can be solved efficiently by convex program routines for its convexity or by simplex algorithm after recasting it into a linear program. The numeric simulation shows that LAGS is superior compared to soft-thresholding methods in terms of mean squared error and parsimony of the model. version:4
arxiv-1210-5196 | Matrix reconstruction with the local max norm | http://arxiv.org/abs/1210.5196 | id:1210.5196 author:Rina Foygel, Nathan Srebro, Ruslan Salakhutdinov category:stat.ML cs.LG  published:2012-10-18 summary:We introduce a new family of matrix norms, the "local max" norms, generalizing existing methods such as the max norm, the trace norm (nuclear norm), and the weighted or smoothed weighted trace norms, which have been extensively used in the literature as regularizers for matrix reconstruction problems. We show that this new family can be used to interpolate between the (weighted or unweighted) trace norm and the more conservative max norm. We test this interpolation on simulated data and on the large-scale Netflix and MovieLens ratings data, and find improved accuracy relative to the existing matrix norms. We also provide theoretical results showing learning guarantees for some of the new norms. version:1
arxiv-1112-4722 | Modeling transition dynamics in MDPs with RKHS embeddings of conditional distributions | http://arxiv.org/abs/1112.4722 | id:1112.4722 author:Steffen Grünewälder, Luca Baldassarre, Massimiliano Pontil, Arthur Gretton, Guy Lever category:cs.LG  published:2011-12-20 summary:We propose a new, nonparametric approach to estimating the value function in reinforcement learning. This approach makes use of a recently developed representation of conditional distributions as functions in a reproducing kernel Hilbert space. Such representations bypass the need for estimating transition probabilities, and apply to any domain on which kernels can be defined. Our approach avoids the need to approximate intractable integrals since expectations are represented as RKHS inner products whose computation has linear complexity in the sample size. Thus, we can efficiently perform value function estimation in a wide variety of settings, including finite state spaces, continuous states spaces, and partially observable tasks where only sensor measurements are available. A second advantage of the approach is that we learn the conditional distribution representation from a training sample, and do not require an exhaustive exploration of the state space. We prove convergence of our approach either to the optimal policy, or to the closest projection of the optimal policy in our model class, under reasonable assumptions. In experiments, we demonstrate the performance of our algorithm on a learning task in a continuous state space (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. We compare with least-squares policy iteration where a Gaussian process is used for value function estimation. Our algorithm achieves better performance in both tasks. version:2
arxiv-1210-5135 | LSBN: A Large-Scale Bayesian Structure Learning Framework for Model Averaging | http://arxiv.org/abs/1210.5135 | id:1210.5135 author:Yang Lu, Mengying Wang, Menglu Li, Qili Zhu, Bo Yuan category:cs.LG stat.ML  published:2012-10-18 summary:The motivation for this paper is to apply Bayesian structure learning using Model Averaging in large-scale networks. Currently, Bayesian model averaging algorithm is applicable to networks with only tens of variables, restrained by its super-exponential complexity. We present a novel framework, called LSBN(Large-Scale Bayesian Network), making it possible to handle networks with infinite size by following the principle of divide-and-conquer. The method of LSBN comprises three steps. In general, LSBN first performs the partition by using a second-order partition strategy, which achieves more robust results. LSBN conducts sampling and structure learning within each overlapping community after the community is isolated from other variables by Markov Blanket. Finally LSBN employs an efficient algorithm, to merge structures of overlapping communities into a whole. In comparison with other four state-of-art large-scale network structure learning algorithms such as ARACNE, PC, Greedy Search and MMHC, LSBN shows comparable results in five common benchmark datasets, evaluated by precision, recall and f-score. What's more, LSBN makes it possible to learn large-scale Bayesian structure by Model Averaging which used to be intractable. In summary, LSBN provides an scalable and parallel framework for the reconstruction of network structures. Besides, the complete information of overlapping communities serves as the byproduct, which could be used to mine meaningful clusters in biological networks, such as protein-protein-interaction network or gene regulatory network, as well as in social network. version:1
arxiv-1210-5128 | A Novel Learning Algorithm for Bayesian Network and Its Efficient Implementation on GPU | http://arxiv.org/abs/1210.5128 | id:1210.5128 author:Yu Wang, Weikang Qian, Shuchang Zhang, Bo Yuan category:cs.DC cs.LG  published:2012-10-18 summary:Computational inference of causal relationships underlying complex networks, such as gene-regulatory pathways, is NP-complete due to its combinatorial nature when permuting all possible interactions. Markov chain Monte Carlo (MCMC) has been introduced to sample only part of the combinations while still guaranteeing convergence and traversability, which therefore becomes widely used. However, MCMC is not able to perform efficiently enough for networks that have more than 15~20 nodes because of the computational complexity. In this paper, we use general purpose processor (GPP) and general purpose graphics processing unit (GPGPU) to implement and accelerate a novel Bayesian network learning algorithm. With a hash-table-based memory-saving strategy and a novel task assigning strategy, we achieve a 10-fold acceleration per iteration than using a serial GPP. Specially, we use a greedy method to search for the best graph from a given order. We incorporate a prior component in the current scoring function, which further facilitates the searching. Overall, we are able to apply this system to networks with more than 60 nodes, allowing inferences and modeling of bigger and more complex networks than current methods. version:1
arxiv-1202-1119 | Cramer Rao-Type Bounds for Sparse Bayesian Learning | http://arxiv.org/abs/1202.1119 | id:1202.1119 author:Ranjitha Prasad, Chandra R. Murthy category:cs.LG stat.ML  published:2012-02-06 summary:In this paper, we derive Hybrid, Bayesian and Marginalized Cram\'{e}r-Rao lower bounds (HCRB, BCRB and MCRB) for the single and multiple measurement vector Sparse Bayesian Learning (SBL) problem of estimating compressible vectors and their prior distribution parameters. We assume the unknown vector to be drawn from a compressible Student-t prior distribution. We derive CRBs that encompass the deterministic or random nature of the unknown parameters of the prior distribution and the regression noise variance. We extend the MCRB to the case where the compressible vector is distributed according to a general compressible prior distribution, of which the generalized Pareto distribution is a special case. We use the derived bounds to uncover the relationship between the compressibility and Mean Square Error (MSE) in the estimates. Further, we illustrate the tightness and utility of the bounds through simulations, by comparing them with the MSE performance of two popular SBL-based estimators. It is found that the MCRB is generally the tightest among the bounds derived and that the MSE performance of the Expectation-Maximization (EM) algorithm coincides with the MCRB for the compressible vector. Through simulations, we demonstrate the dependence of the MSE performance of SBL based estimators on the compressibility of the vector for several values of the number of observations and at different signal powers. version:2
arxiv-1210-4768 | Justice blocks and predictability of US Supreme Court votes | http://arxiv.org/abs/1210.4768 | id:1210.4768 author:Roger Guimera, Marta Sales-Pardo category:physics.soc-ph physics.data-an stat.ML  published:2012-10-17 summary:Successful attempts to predict judges' votes shed light into how legal decisions are made and, ultimately, into the behavior and evolution of the judiciary. Here, we investigate to what extent it is possible to make predictions of a justice's vote based on the other justices' votes in the same case. For our predictions, we use models and methods that have been developed to uncover hidden associations between actors in complex social networks. We show that these methods are more accurate at predicting justice's votes than forecasts made by legal experts and by algorithms that take into consideration the content of the cases. We argue that, within our framework, high predictability is a quantitative proxy for stable justice (and case) blocks, which probably reflect stable a priori attitudes toward the law. We find that U. S. Supreme Court justice votes are more predictable than one would expect from an ideal court composed of perfectly independent justices. Deviations from ideal behavior are most apparent in divided 5-4 decisions, where justice blocks seem to be most stable. Moreover, we find evidence that justice predictability decreased during the 50-year period spanning from the Warren Court to the Rehnquist Court, and that aggregate court predictability has been significantly lower during Democratic presidencies. More broadly, our results show that it is possible to use methods developed for the analysis of complex social networks to quantitatively investigate historical questions related to political decision-making. version:1
arxiv-1210-4762 | Mixture model for designs in high dimensional regression and the LASSO | http://arxiv.org/abs/1210.4762 | id:1210.4762 author:Stéphane Chrétien category:math.ST stat.ML stat.TH  published:2012-10-17 summary:The LASSO is a recent technique for variable selection in the regression model \bean y & = & X\beta +\epsilon, \eean where $X\in \R^{n\times p}$ and $\epsilon$ is a centered gaussian i.i.d. noise vector $\mathcal N(0,\sigma^2I)$. The LASSO has been proved to perform exact support recovery for regression vectors when the design matrix satisfies certain algebraic conditions and $\beta$ is sufficiently sparse. Estimation of the vector $X\beta$ has also extensively been studied for the purpose of prediction under the same algebraic conditions on $X$ and under sufficient sparsity of $\beta$. Among many other, the coherence is an index which can be used to study these nice properties of the LASSO. More precisely, a small coherence implies that most sparse vectors, with less nonzero components than the order $n/\log(p)$, can be recovered with high probability if its nonzero components are larger than the order $\sigma \sqrt{\log(p)}$. However, many matrices occuring in practice do not have a small coherence and thus, most results which have appeared in the litterature cannot be applied. The goal of this paper is to study a model for which precise results can be obtained. In the proposed model, the columns of the design matrix are drawn from a Gaussian mixture model and the coherence condition is imposed on the much smaller matrix whose columns are the mixture's centers, instead of on $X$ itself. Our main theorem states that $X\beta$ is as well estimated as in the case of small coherence up to a correction parametrized by the maximal variance in the mixture model. version:1
arxiv-1210-4695 | Regulating the information in spikes: a useful bias | http://arxiv.org/abs/1210.4695 | id:1210.4695 author:David Balduzzi category:q-bio.NC cs.IT cs.LG math.IT  published:2012-10-17 summary:The bias/variance tradeoff is fundamental to learning: increasing a model's complexity can improve its fit on training data, but potentially worsens performance on future samples. Remarkably, however, the human brain effortlessly handles a wide-range of complex pattern recognition tasks. On the basis of these conflicting observations, it has been argued that useful biases in the form of "generic mechanisms for representation" must be hardwired into cortex (Geman et al). This note describes a useful bias that encourages cooperative learning which is both biologically plausible and rigorously justified. version:1
arxiv-1205-6210 | Learning Dictionaries with Bounded Self-Coherence | http://arxiv.org/abs/1205.6210 | id:1205.6210 author:Christian D. Sigg, Tomas Dikk, Joachim M. Buhmann category:stat.ML cs.LG  published:2012-05-28 summary:Sparse coding in learned dictionaries has been established as a successful approach for signal denoising, source separation and solving inverse problems in general. A dictionary learning method adapts an initial dictionary to a particular signal class by iteratively computing an approximate factorization of a training data matrix into a dictionary and a sparse coding matrix. The learned dictionary is characterized by two properties: the coherence of the dictionary to observations of the signal class, and the self-coherence of the dictionary atoms. A high coherence to the signal class enables the sparse coding of signal observations with a small approximation error, while a low self-coherence of the atoms guarantees atom recovery and a more rapid residual error decay rate for the sparse coding algorithm. The two goals of high signal coherence and low self-coherence are typically in conflict, therefore one seeks a trade-off between them, depending on the application. We present a dictionary learning method with an effective control over the self-coherence of the trained dictionary, enabling a trade-off between maximizing the sparsity of codings and approximating an equiangular tight frame. version:2
arxiv-1210-4657 | Mean-Field Learning: a Survey | http://arxiv.org/abs/1210.4657 | id:1210.4657 author:Hamidou Tembine, Raul Tempone, Pedro Vilanova category:cs.LG cs.GT cs.MA math.DS stat.ML  published:2012-10-17 summary:In this paper we study iterative procedures for stationary equilibria in games with large number of players. Most of learning algorithms for games with continuous action spaces are limited to strict contraction best reply maps in which the Banach-Picard iteration converges with geometrical convergence rate. When the best reply map is not a contraction, Ishikawa-based learning is proposed. The algorithm is shown to behave well for Lipschitz continuous and pseudo-contractive maps. However, the convergence rate is still unsatisfactory. Several acceleration techniques are presented. We explain how cognitive users can improve the convergence rate based only on few number of measurements. The methodology provides nice properties in mean field games where the payoff function depends only on own-action and the mean of the mean-field (first moment mean-field games). A learning framework that exploits the structure of such games, called, mean-field learning, is proposed. The proposed mean-field learning framework is suitable not only for games but also for non-convex global optimization problems. Then, we introduce mean-field learning without feedback and examine the convergence to equilibria in beauty contest games, which have interesting applications in financial markets. Finally, we provide a fully distributed mean-field learning and its speedup versions for satisfactory solution in wireless networks. We illustrate the convergence rate improvement with numerical examples. version:1
arxiv-1003-3967 | Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization | http://arxiv.org/abs/1003.3967 | id:1003.3967 author:Daniel Golovin, Andreas Krause category:cs.LG cs.AI cs.DS I.2.6; F.2.2; G.3  published:2010-03-21 summary:Solving stochastic optimization problems under partial observability, where one needs to adaptively make decisions with uncertain outcomes, is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. In addition to providing performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations. We illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse applications including sensor placement, viral marketing and active learning. Proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations. version:4
arxiv-1210-4601 | A Direct Approach to Multi-class Boosting and Extensions | http://arxiv.org/abs/1210.4601 | id:1210.4601 author:Chunhua Shen, Sakrapee Paisitkriangkrai, Anton van den Hengel category:cs.LG  published:2012-10-17 summary:Boosting methods combine a set of moderately accurate weaklearners to form a highly accurate predictor. Despite the practical importance of multi-class boosting, it has received far less attention than its binary counterpart. In this work, we propose a fully-corrective multi-class boosting formulation which directly solves the multi-class problem without dividing it into multiple binary classification problems. In contrast, most previous multi-class boosting algorithms decompose a multi-boost problem into multiple binary boosting problems. By explicitly deriving the Lagrange dual of the primal optimization problem, we are able to construct a column generation-based fully-corrective approach to boosting which directly optimizes multi-class classification performance. The new approach not only updates all weak learners' coefficients at every iteration, but does so in a manner flexible enough to accommodate various loss functions and regularizations. For example, it enables us to introduce structural sparsity through mixed-norm regularization to promote group sparsity and feature sharing. Boosting with shared features is particularly beneficial in complex prediction problems where features can be expensive to compute. Our experiments on various data sets demonstrate that our direct multi-class boosting generalizes as well as, or better than, a range of competing multi-class boosting methods. The end result is a highly effective and compact ensemble classifier which can be trained in a distributed fashion. version:1
arxiv-0909-0737 | Efficient algorithms for training the parameters of hidden Markov models using stochastic expectation maximization EM training and Viterbi training | http://arxiv.org/abs/0909.0737 | id:0909.0737 author:Tin Yin Lam, Irmtraud M. Meyer category:q-bio.QM cs.LG q-bio.GN  published:2009-09-03 summary:Background: Hidden Markov models are widely employed by numerous bioinformatics programs used today. Applications range widely from comparative gene prediction to time-series analyses of micro-array data. The parameters of the underlying models need to be adjusted for specific data sets, for example the genome of a particular species, in order to maximize the prediction accuracy. Computationally efficient algorithms for parameter training are thus key to maximizing the usability of a wide range of bioinformatics applications. Results: We introduce two computationally efficient training algorithms, one for Viterbi training and one for stochastic expectation maximization (EM) training, which render the memory requirements independent of the sequence length. Unlike the existing algorithms for Viterbi and stochastic EM training which require a two-step procedure, our two new algorithms require only one step and scan the input sequence in only one direction. We also implement these two new algorithms and the already published linear-memory algorithm for EM training into the hidden Markov model compiler HMM-Converter and examine their respective practical merits for three small example models. Conclusions: Bioinformatics applications employing hidden Markov models can use the two algorithms in order to make Viterbi training and stochastic EM training more computationally efficient. Using these algorithms, parameter training can thus be attempted for more complex models and longer training sequences. The two new algorithms have the added advantage of being easier to implement than the corresponding default algorithms for Viterbi training and stochastic EM training. version:2
arxiv-1210-4920 | Factorized Multi-Modal Topic Model | http://arxiv.org/abs/1210.4920 | id:1210.4920 author:Seppo Virtanen, Yangqing Jia, Arto Klami, Trevor Darrell category:cs.LG cs.IR stat.ML  published:2012-10-16 summary:Multi-modal data collections, such as corpora of paired images and text snippets, require analysis methods beyond single-view component and topic models. For continuous observations the current dominant approach is based on extensions of canonical correlation analysis, factorizing the variation into components shared by the different modalities and those private to each of them. For count data, multiple variants of topic models attempting to tie the modalities together have been presented. All of these, however, lack the ability to learn components private to one modality, and consequently will try to force dependencies even between minimally correlating modalities. In this work we combine the two approaches by presenting a novel HDP-based topic model that automatically learns both shared and private topics. The model is shown to be especially useful for querying the contents of one domain given samples of the other. version:1
arxiv-1210-4919 | Latent Dirichlet Allocation Uncovers Spectral Characteristics of Drought Stressed Plants | http://arxiv.org/abs/1210.4919 | id:1210.4919 author:Mirwaes Wahabzada, Kristian Kersting, Christian Bauckhage, Christoph Roemer, Agim Ballvora, Francisco Pinto, Uwe Rascher, Jens Leon, Lutz Ploemer category:cs.LG cs.CE stat.ML  published:2012-10-16 summary:Understanding the adaptation process of plants to drought stress is essential in improving management practices, breeding strategies as well as engineering viable crops for a sustainable agriculture in the coming decades. Hyper-spectral imaging provides a particularly promising approach to gain such understanding since it allows to discover non-destructively spectral characteristics of plants governed primarily by scattering and absorption characteristics of the leaf internal structure and biochemical constituents. Several drought stress indices have been derived using hyper-spectral imaging. However, they are typically based on few hyper-spectral images only, rely on interpretations of experts, and consider few wavelengths only. In this study, we present the first data-driven approach to discovering spectral drought stress indices, treating it as an unsupervised labeling problem at massive scale. To make use of short range dependencies of spectral wavelengths, we develop an online variational Bayes algorithm for latent Dirichlet allocation with convolved Dirichlet regularizer. This approach scales to massive datasets and, hence, provides a more objective complement to plant physiological practices. The spectral topics found conform to plant physiological knowledge and can be computed in a fraction of the time compared to existing LDA approaches. version:1
arxiv-1210-4918 | Dynamic Teaching in Sequential Decision Making Environments | http://arxiv.org/abs/1210.4918 | id:1210.4918 author:Thomas J. Walsh, Sergiu Goschin category:cs.LG cs.AI stat.ML  published:2012-10-16 summary:We describe theoretical bounds and a practical algorithm for teaching a model by demonstration in a sequential decision making environment. Unlike previous efforts that have optimized learners that watch a teacher demonstrate a static policy, we focus on the teacher as a decision maker who can dynamically choose different policies to teach different parts of the environment. We develop several teaching frameworks based on previously defined supervised protocols, such as Teaching Dimension, extending them to handle noise and sequences of inputs encountered in an MDP.We provide theoretical bounds on the learnability of several important model classes in this setting and suggest a practical algorithm for dynamic teaching. version:1
arxiv-1210-4917 | Fast Graph Construction Using Auction Algorithm | http://arxiv.org/abs/1210.4917 | id:1210.4917 author:Jun Wang, Yinglong Xia category:cs.LG stat.ML  published:2012-10-16 summary:In practical machine learning systems, graph based data representation has been widely used in various learning paradigms, ranging from unsupervised clustering to supervised classification. Besides those applications with natural graph or network structure data, such as social network analysis and relational learning, many other applications often involve a critical step in converting data vectors to an adjacency graph. In particular, a sparse subgraph extracted from the original graph is often required due to both theoretic and practical needs. Previous study clearly shows that the performance of different learning algorithms, e.g., clustering and classification, benefits from such sparse subgraphs with balanced node connectivity. However, the existing graph construction methods are either computationally expensive or with unsatisfactory performance. In this paper, we utilize a scalable method called auction algorithm and its parallel extension to recover a sparse yet nearly balanced subgraph with significantly reduced computational cost. Empirical study and comparison with the state-ofart approaches clearly demonstrate the superiority of the proposed method in both efficiency and accuracy. version:1
arxiv-1210-4914 | Latent Structured Ranking | http://arxiv.org/abs/1210.4914 | id:1210.4914 author:Jason Weston, John Blitzer category:cs.LG cs.IR stat.ML  published:2012-10-16 summary:Many latent (factorized) models have been proposed for recommendation tasks like collaborative filtering and for ranking tasks like document or image retrieval and annotation. Common to all those methods is that during inference the items are scored independently by their similarity to the query in the latent embedding space. The structure of the ranked list (i.e. considering the set of items returned as a whole) is not taken into account. This can be a problem because the set of top predictions can be either too diverse (contain results that contradict each other) or are not diverse enough. In this paper we introduce a method for learning latent structured rankings that improves over existing methods by providing the right blend of predictions at the top of the ranked list. Particular emphasis is put on making this method scalable. Empirical results on large scale image annotation and music recommendation tasks show improvements over existing approaches. version:1
arxiv-1210-4913 | An Improved Admissible Heuristic for Learning Optimal Bayesian Networks | http://arxiv.org/abs/1210.4913 | id:1210.4913 author:Changhe Yuan, Brandon Malone category:cs.AI cs.LG stat.ML  published:2012-10-16 summary:Recently two search algorithms, A* and breadth-first branch and bound (BFBnB), were developed based on a simple admissible heuristic for learning Bayesian network structures that optimize a scoring function. The heuristic represents a relaxation of the learning problem such that each variable chooses optimal parents independently. As a result, the heuristic may contain many directed cycles and result in a loose bound. This paper introduces an improved admissible heuristic that tries to avoid directed cycles within small groups of variables. A sparse representation is also introduced to store only the unique optimal parent choices. Empirical results show that the new techniques significantly improved the efficiency and scalability of A* and BFBnB on most of datasets tested in this paper. version:1
arxiv-1210-4910 | New Advances and Theoretical Insights into EDML | http://arxiv.org/abs/1210.4910 | id:1210.4910 author:Khaled S. Refaat, Arthur Choi, Adnan Darwiche category:cs.AI cs.LG stat.ML  published:2012-10-16 summary:EDML is a recently proposed algorithm for learning MAP parameters in Bayesian networks. In this paper, we present a number of new advances and insights on the EDML algorithm. First, we provide the multivalued extension of EDML, originally proposed for Bayesian networks over binary variables. Next, we identify a simplified characterization of EDML that further implies a simple fixed-point algorithm for the convex optimization problem that underlies it. This characterization further reveals a connection between EDML and EM: a fixed point of EDML is a fixed point of EM, and vice versa. We thus identify also a new characterization of EM fixed points, but in the semantics of EDML. Finally, we propose a hybrid EDML/EM algorithm that takes advantage of the improved empirical convergence behavior of EDML, while maintaining the monotonic improvement property of EM. version:1
arxiv-1210-4909 | Active Learning with Distributional Estimates | http://arxiv.org/abs/1210.4909 | id:1210.4909 author:Jens Roeder, Boaz Nadler, Kevin Kunzmann, Fred A. Hamprecht category:cs.LG stat.ML  published:2012-10-16 summary:Active Learning (AL) is increasingly important in a broad range of applications. Two main AL principles to obtain accurate classification with few labeled data are refinement of the current decision boundary and exploration of poorly sampled regions. In this paper we derive a novel AL scheme that balances these two principles in a natural way. In contrast to many AL strategies, which are based on an estimated class conditional probability ^p(y x), a key component of our approach is to view this quantity as a random variable, hence explicitly considering the uncertainty in its estimated value. Our main contribution is a novel mathematical framework for uncertainty-based AL, and a corresponding AL scheme, where the uncertainty in ^p(y x) is modeled by a second-order distribution. On the practical side, we show how to approximate such second-order distributions for kernel density classification. Finally, we find that over a large number of UCI, USPS and Caltech4 datasets, our AL scheme achieves significantly better learning curves than popular AL methods such as uncertainty sampling and error reduction sampling, when all use the same kernel density classifier. version:1
arxiv-1210-4905 | Latent Composite Likelihood Learning for the Structured Canonical Correlation Model | http://arxiv.org/abs/1210.4905 | id:1210.4905 author:Ricardo Silva category:stat.ML cs.LG  published:2012-10-16 summary:Latent variable models are used to estimate variables of interest quantities which are observable only up to some measurement error. In many studies, such variables are known but not precisely quantifiable (such as "job satisfaction" in social sciences and marketing, "analytical ability" in educational testing, or "inflation" in economics). This leads to the development of measurement instruments to record noisy indirect evidence for such unobserved variables such as surveys, tests and price indexes. In such problems, there are postulated latent variables and a given measurement model. At the same time, other unantecipated latent variables can add further unmeasured confounding to the observed variables. The problem is how to deal with unantecipated latents variables. In this paper, we provide a method loosely inspired by canonical correlation that makes use of background information concerning the "known" latent variables. Given a partially specified structure, it provides a structure learning approach to detect "unknown unknowns," the confounding effect of potentially infinitely many other latent variables. This is done without explicitly modeling such extra latent factors. Because of the special structure of the problem, we are able to exploit a new variation of composite likelihood fitting to efficiently learn this structure. Validation is provided with experiments in synthetic data and the analysis of a large survey done with a sample of over 100,000 staff members of the National Health Service of the United Kingdom. version:1
arxiv-1210-4902 | Efficiently Searching for Frustrated Cycles in MAP Inference | http://arxiv.org/abs/1210.4902 | id:1210.4902 author:David Sontag, Do Kook Choe, Yitao Li category:cs.DS cs.LG stat.ML  published:2012-10-16 summary:Dual decomposition provides a tractable framework for designing algorithms for finding the most probable (MAP) configuration in graphical models. However, for many real-world inference problems, the typical decomposition has a large integrality gap, due to frustrated cycles. One way to tighten the relaxation is to introduce additional constraints that explicitly enforce cycle consistency. Earlier work showed that cluster-pursuit algorithms, which iteratively introduce cycle and other higherorder consistency constraints, allows one to exactly solve many hard inference problems. However, these algorithms explicitly enumerate a candidate set of clusters, limiting them to triplets or other short cycles. We solve the search problem for cycle constraints, giving a nearly linear time algorithm for finding the most frustrated cycle of arbitrary length. We show how to use this search algorithm together with the dual decomposition framework and clusterpursuit. The new algorithm exactly solves MAP inference problems arising from relational classification and stereo vision. version:1
arxiv-1210-4899 | Fast Exact Inference for Recursive Cardinality Models | http://arxiv.org/abs/1210.4899 | id:1210.4899 author:Daniel Tarlow, Kevin Swersky, Richard S. Zemel, Ryan Prescott Adams, Brendan J. Frey category:cs.LG stat.ML  published:2012-10-16 summary:Cardinality potentials are a generally useful class of high order potential that affect probabilities based on how many of D binary variables are active. Maximum a posteriori (MAP) inference for cardinality potential models is well-understood, with efficient computations taking O(DlogD) time. Yet efficient marginalization and sampling have not been addressed as thoroughly in the machine learning community. We show that there exists a simple algorithm for computing marginal probabilities and drawing exact joint samples that runs in O(Dlog2 D) time, and we show how to frame the algorithm as efficient belief propagation in a low order tree-structured model that includes additional auxiliary variables. We then develop a new, more general class of models, termed Recursive Cardinality models, which take advantage of this efficiency. Finally, we show how to do efficient exact inference in models composed of a tree structure and a cardinality potential. We explore the expressive power of Recursive Cardinality models and empirically demonstrate their utility. version:1
arxiv-1210-4898 | Value Function Approximation in Noisy Environments Using Locally Smoothed Regularized Approximate Linear Programs | http://arxiv.org/abs/1210.4898 | id:1210.4898 author:Gavin Taylor, Ron Parr category:cs.LG stat.ML  published:2012-10-16 summary:Recently, Petrik et al. demonstrated that L1Regularized Approximate Linear Programming (RALP) could produce value functions and policies which compared favorably to established linear value function approximation techniques like LSPI. RALP's success primarily stems from the ability to solve the feature selection and value function approximation steps simultaneously. RALP's performance guarantees become looser if sampled next states are used. For very noisy domains, RALP requires an accurate model rather than samples, which can be unrealistic in some practical scenarios. In this paper, we demonstrate this weakness, and then introduce Locally Smoothed L1-Regularized Approximate Linear Programming (LS-RALP). We demonstrate that LS-RALP mitigates inaccuracies stemming from noise even without an accurate model. We show that, given some smoothness assumptions, as the number of samples increases, error from noise approaches zero, and provide experimental examples of LS-RALP's success on common reinforcement learning benchmark problems. version:1
arxiv-1210-4896 | Closed-Form Learning of Markov Networks from Dependency Networks | http://arxiv.org/abs/1210.4896 | id:1210.4896 author:Daniel Lowd category:cs.LG cs.AI stat.ML  published:2012-10-16 summary:Markov networks (MNs) are a powerful way to compactly represent a joint probability distribution, but most MN structure learning methods are very slow, due to the high cost of evaluating candidates structures. Dependency networks (DNs) represent a probability distribution as a set of conditional probability distributions. DNs are very fast to learn, but the conditional distributions may be inconsistent with each other and few inference algorithms support DNs. In this paper, we present a closed-form method for converting a DN into an MN, allowing us to enjoy both the efficiency of DN learning and the convenience of the MN representation. When the DN is consistent, this conversion is exact. For inconsistent DNs, we present averaging methods that significantly improve the approximation. In experiments on 12 standard datasets, our methods are orders of magnitude faster than and often more accurate than combining conditional distributions using weight learning. version:1
arxiv-1210-4893 | Sparse Q-learning with Mirror Descent | http://arxiv.org/abs/1210.4893 | id:1210.4893 author:Sridhar Mahadevan, Bo Liu category:cs.LG stat.ML  published:2012-10-16 summary:This paper explores a new framework for reinforcement learning based on online convex optimization, in particular mirror descent and related algorithms. Mirror descent can be viewed as an enhanced gradient method, particularly suited to minimization of convex functions in highdimensional spaces. Unlike traditional gradient methods, mirror descent undertakes gradient updates of weights in both the dual space and primal space, which are linked together using a Legendre transform. Mirror descent can be viewed as a proximal algorithm where the distance generating function used is a Bregman divergence. A new class of proximal-gradient based temporal-difference (TD) methods are presented based on different Bregman divergences, which are more powerful than regular TD learning. Examples of Bregman divergences that are studied include p-norm functions, and Mahalanobis distance based on the covariance of sample gradients. A new family of sparse mirror-descent reinforcement learning methods are proposed, which are able to find sparse fixed points of an l1-regularized Bellman equation at significantly less computational cost than previous methods based on second-order matrix methods. An experimental study of mirror-descent reinforcement learning is presented using discrete and continuous Markov decision processes. version:1
arxiv-1210-4892 | Unsupervised Joint Alignment and Clustering using Bayesian Nonparametrics | http://arxiv.org/abs/1210.4892 | id:1210.4892 author:Marwan A. Mattar, Allen R. Hanson, Erik G. Learned-Miller category:cs.LG stat.ML  published:2012-10-16 summary:Joint alignment of a collection of functions is the process of independently transforming the functions so that they appear more similar to each other. Typically, such unsupervised alignment algorithms fail when presented with complex data sets arising from multiple modalities or make restrictive assumptions about the form of the functions or transformations, limiting their generality. We present a transformed Bayesian infinite mixture model that can simultaneously align and cluster a data set. Our model and associated learning scheme offer two key advantages: the optimal number of clusters is determined in a data-driven fashion through the use of a Dirichlet process prior, and it can accommodate any transformation function parameterized by a continuous parameter vector. As a result, it is applicable to a wide range of data types, and transformation functions. We present positive results on synthetic two-dimensional data, on a set of one-dimensional curves, and on various image data sets, showing large improvements over previous work. We discuss several variations of the model and conclude with directions for future work. version:1
arxiv-1210-4889 | Learning STRIPS Operators from Noisy and Incomplete Observations | http://arxiv.org/abs/1210.4889 | id:1210.4889 author:Kira Mourao, Luke S. Zettlemoyer, Ronald P. A. Petrick, Mark Steedman category:cs.LG cs.AI stat.ML  published:2012-10-16 summary:Agents learning to act autonomously in real-world domains must acquire a model of the dynamics of the domain in which they operate. Learning domain dynamics can be challenging, especially where an agent only has partial access to the world state, and/or noisy external sensors. Even in standard STRIPS domains, existing approaches cannot learn from noisy, incomplete observations typical of real-world domains. We propose a method which learns STRIPS action models in such domains, by decomposing the problem into first learning a transition function between states in the form of a set of classifiers, and then deriving explicit STRIPS rules from the classifiers' parameters. We evaluate our approach on simulated standard planning domains from the International Planning Competition, and show that it learns useful domain descriptions from noisy, incomplete observations. version:1
arxiv-1210-4888 | Local Structure Discovery in Bayesian Networks | http://arxiv.org/abs/1210.4888 | id:1210.4888 author:Teppo Niinimaki, Pekka Parviainen category:cs.LG cs.AI stat.ML  published:2012-10-16 summary:Learning a Bayesian network structure from data is an NP-hard problem and thus exact algorithms are feasible only for small data sets. Therefore, network structures for larger networks are usually learned with various heuristics. Another approach to scaling up the structure learning is local learning. In local learning, the modeler has one or more target variables that are of special interest; he wants to learn the structure near the target variables and is not interested in the rest of the variables. In this paper, we present a score-based local learning algorithm called SLL. We conjecture that our algorithm is theoretically sound in the sense that it is optimal in the limit of large sample size. Empirical results suggest that SLL is competitive when compared to the constraint-based HITON algorithm. We also study the prospects of constructing the network structure for the whole node set based on local results by presenting two algorithms and comparing them to several heuristics. version:1
arxiv-1210-4887 | Hilbert Space Embeddings of POMDPs | http://arxiv.org/abs/1210.4887 | id:1210.4887 author:Yu Nishiyama, Abdeslam Boularias, Arthur Gretton, Kenji Fukumizu category:cs.LG cs.AI stat.ML  published:2012-10-16 summary:A nonparametric approach for policy learning for POMDPs is proposed. The approach represents distributions over the states, observations, and actions as embeddings in feature spaces, which are reproducing kernel Hilbert spaces. Distributions over states given the observations are obtained by applying the kernel Bayes' rule to these distribution embeddings. Policies and value functions are defined on the feature space over states, which leads to a feature space expression for the Bellman equation. Value iteration may then be used to estimate the optimal value function and associated policy. Experimental results confirm that the correct policy is learned using the feature space representation. version:1
arxiv-1210-4884 | A Spectral Algorithm for Latent Junction Trees | http://arxiv.org/abs/1210.4884 | id:1210.4884 author:Ankur P. Parikh, Le Song, Mariya Ishteva, Gabi Teodoru, Eric P. Xing category:cs.LG stat.ML  published:2012-10-16 summary:Latent variable models are an elegant framework for capturing rich probabilistic dependencies in many applications. However, current approaches typically parametrize these models using conditional probability tables, and learning relies predominantly on local search heuristics such as Expectation Maximization. Using tensor algebra, we propose an alternative parameterization of latent variable models (where the model structures are junction trees) that still allows for computation of marginals among observed variables. While this novel representation leads to a moderate increase in the number of parameters for junction trees of low treewidth, it lets us design a local-minimum-free algorithm for learning this parameterization. The main computation of the algorithm involves only tensor operations and SVDs which can be orders of magnitude faster than EM algorithms for large datasets. To our knowledge, this is the first provably consistent parameter learning technique for a large class of low-treewidth latent graphical models beyond trees. We demonstrate the advantages of our method on synthetic and real datasets. version:1
arxiv-1210-4883 | A Model-Based Approach to Rounding in Spectral Clustering | http://arxiv.org/abs/1210.4883 | id:1210.4883 author:Leonard K. M. Poon, April H. Liu, Tengfei Liu, Nevin Lianwen Zhang category:cs.LG cs.NA stat.ML  published:2012-10-16 summary:In spectral clustering, one defines a similarity matrix for a collection of data points, transforms the matrix to get the Laplacian matrix, finds the eigenvectors of the Laplacian matrix, and obtains a partition of the data using the leading eigenvectors. The last step is sometimes referred to as rounding, where one needs to decide how many leading eigenvectors to use, to determine the number of clusters, and to partition the data points. In this paper, we propose a novel method for rounding. The method differs from previous methods in three ways. First, we relax the assumption that the number of clusters equals the number of eigenvectors used. Second, when deciding the number of leading eigenvectors to use, we not only rely on information contained in the leading eigenvectors themselves, but also use subsequent eigenvectors. Third, our method is model-based and solves all the three subproblems of rounding using a class of graphical models called latent tree models. We evaluate our method on both synthetic and real-world data. The results show that our method works correctly in the ideal case where between-clusters similarity is 0, and degrades gracefully as one moves away from the ideal case. version:1
arxiv-1210-4881 | Tightening Fractional Covering Upper Bounds on the Partition Function for High-Order Region Graphs | http://arxiv.org/abs/1210.4881 | id:1210.4881 author:Tamir Hazan, Jian Peng, Amnon Shashua category:cs.LG stat.ML  published:2012-10-16 summary:In this paper we present a new approach for tightening upper bounds on the partition function. Our upper bounds are based on fractional covering bounds on the entropy function, and result in a concave program to compute these bounds and a convex program to tighten them. To solve these programs effectively for general region graphs we utilize the entropy barrier method, thus decomposing the original programs by their dual programs and solve them with dual block optimization scheme. The entropy barrier method provides an elegant framework to generalize the message-passing scheme to high-order region graph, as well as to solve the block dual steps in closed-form. This is a key for computational relevancy for large problems with thousands of regions. version:1
arxiv-1210-4880 | Inferring Strategies from Limited Reconnaissance in Real-time Strategy Games | http://arxiv.org/abs/1210.4880 | id:1210.4880 author:Jesse Hostetler, Ethan W. Dereszynski, Thomas G. Dietterich, Alan Fern category:cs.AI cs.GT cs.LG  published:2012-10-16 summary:In typical real-time strategy (RTS) games, enemy units are visible only when they are within sight range of a friendly unit. Knowledge of an opponent's disposition is limited to what can be observed through scouting. Information is costly, since units dedicated to scouting are unavailable for other purposes, and the enemy will resist scouting attempts. It is important to infer as much as possible about the opponent's current and future strategy from the available observations. We present a dynamic Bayes net model of strategies in the RTS game Starcraft that combines a generative model of how strategies relate to observable quantities with a principled framework for incorporating evidence gained via scouting. We demonstrate the model's ability to infer unobserved aspects of the game from realistic observations. version:1
arxiv-1210-4879 | Causal Discovery of Linear Cyclic Models from Multiple Experimental Data Sets with Overlapping Variables | http://arxiv.org/abs/1210.4879 | id:1210.4879 author:Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer category:stat.ME cs.AI stat.ML  published:2012-10-16 summary:Much of scientific data is collected as randomized experiments intervening on some and observing other variables of interest. Quite often, a given phenomenon is investigated in several studies, and different sets of variables are involved in each study. In this article we consider the problem of integrating such knowledge, inferring as much as possible concerning the underlying causal structure with respect to the union of observed variables from such experimental or passive observational overlapping data sets. We do not assume acyclicity or joint causal sufficiency of the underlying data generating model, but we do restrict the causal relationships to be linear and use only second order statistics of the data. We derive conditions for full model identifiability in the most generic case, and provide novel techniques for incorporating an assumption of faithfulness to aid in inference. In each case we seek to establish what is and what is not determined by the data at hand. version:1
arxiv-1210-4876 | Active Imitation Learning via Reduction to I.I.D. Active Learning | http://arxiv.org/abs/1210.4876 | id:1210.4876 author:Kshitij Judah, Alan Fern, Thomas G. Dietterich category:cs.LG stat.ML  published:2012-10-16 summary:In standard passive imitation learning, the goal is to learn a target policy by passively observing full execution trajectories of it. Unfortunately, generating such trajectories can require substantial expert effort and be impractical in some cases. In this paper, we consider active imitation learning with the goal of reducing this effort by querying the expert about the desired action at individual states, which are selected based on answers to past queries and the learner's interactions with an environment simulator. We introduce a new approach based on reducing active imitation learning to i.i.d. active learning, which can leverage progress in the i.i.d. setting. Our first contribution, is to analyze reductions for both non-stationary and stationary policies, showing that the label complexity (number of queries) of active imitation learning can be substantially less than passive learning. Our second contribution, is to introduce a practical algorithm inspired by the reductions, which is shown to be highly effective in four test domains compared to a number of alternatives. version:1
arxiv-1210-4872 | Nested Dictionary Learning for Hierarchical Organization of Imagery and Text | http://arxiv.org/abs/1210.4872 | id:1210.4872 author:Lingbo Li, XianXing Zhang, Mingyuan Zhou, Lawrence Carin category:cs.LG cs.CV stat.ML  published:2012-10-16 summary:A tree-based dictionary learning model is developed for joint analysis of imagery and associated text. The dictionary learning may be applied directly to the imagery from patches, or to general feature vectors extracted from patches or superpixels (using any existing method for image feature extraction). Each image is associated with a path through the tree (from root to a leaf), and each of the multiple patches in a given image is associated with one node in that path. Nodes near the tree root are shared between multiple paths, representing image characteristics that are common among different types of images. Moving toward the leaves, nodes become specialized, representing details in image classes. If available, words (text) are also jointly modeled, with a path-dependent probability over words. The tree structure is inferred via a nested Dirichlet process, and a retrospective stick-breaking sampler is used to infer the tree depth and width. version:1
arxiv-1210-4871 | Learning Mixtures of Submodular Shells with Application to Document Summarization | http://arxiv.org/abs/1210.4871 | id:1210.4871 author:Hui Lin, Jeff A. Bilmes category:cs.LG cs.CL cs.IR stat.ML  published:2012-10-16 summary:We introduce a method to learn a mixture of submodular "shells" in a large-margin setting. A submodular shell is an abstract submodular function that can be instantiated with a ground set and a set of parameters to produce a submodular function. A mixture of such shells can then also be so instantiated to produce a more complex submodular function. What our algorithm learns are the mixture weights over such shells. We provide a risk bound guarantee when learning in a large-margin structured-prediction setting using a projected subgradient method when only approximate submodular optimization is possible (such as with submodular function maximization). We apply this method to the problem of multi-document summarization and produce the best results reported so far on the widely used NIST DUC-05 through DUC-07 document summarization corpora. version:1
arxiv-1210-4870 | Crowdsourcing Control: Moving Beyond Multiple Choice | http://arxiv.org/abs/1210.4870 | id:1210.4870 author:Christopher H. Lin, Mausam, Daniel Weld category:cs.AI cs.LG  published:2012-10-16 summary:To ensure quality results from crowdsourced tasks, requesters often aggregate worker responses and use one of a plethora of strategies to infer the correct answer from the set of noisy responses. However, all current models assume prior knowledge of all possible outcomes of the task. While not an unreasonable assumption for tasks that can be posited as multiple-choice questions (e.g. n-ary classification), we observe that many tasks do not naturally fit this paradigm, but instead demand a free-response formulation where the outcome space is of infinite size (e.g. audio transcription). We model such tasks with a novel probabilistic graphical model, and design and implement LazySusan, a decision-theoretic controller that dynamically requests responses as necessary in order to infer answers to these tasks. We also design an EM algorithm to jointly learn the parameters of our model while inferring the correct answers to multiple tasks at a time. Live experiments on Amazon Mechanical Turk demonstrate the superiority of LazySusan at solving SAT Math questions, eliminating 83.2% of the error and achieving greater net utility compared to the state-ofthe-art strategy, majority-voting. We also show in live experiments that our EM algorithm outperforms majority-voting on a visualization task that we design. version:1
arxiv-1210-4869 | Response Aware Model-Based Collaborative Filtering | http://arxiv.org/abs/1210.4869 | id:1210.4869 author:Guang Ling, Haiqin Yang, Michael R. Lyu, Irwin King category:cs.LG cs.IR stat.ML  published:2012-10-16 summary:Previous work on recommender systems mainly focus on fitting the ratings provided by users. However, the response patterns, i.e., some items are rated while others not, are generally ignored. We argue that failing to observe such response patterns can lead to biased parameter estimation and sub-optimal model performance. Although several pieces of work have tried to model users' response patterns, they miss the effectiveness and interpretability of the successful matrix factorization collaborative filtering approaches. To bridge the gap, in this paper, we unify explicit response models and PMF to establish the Response Aware Probabilistic Matrix Factorization (RAPMF) framework. We show that RAPMF subsumes PMF as a special case. Empirically we demonstrate the merits of RAPMF from various aspects. version:1
arxiv-1210-4867 | Lifted Relational Variational Inference | http://arxiv.org/abs/1210.4867 | id:1210.4867 author:Jaesik Choi, Eyal Amir category:cs.LG stat.ML  published:2012-10-16 summary:Hybrid continuous-discrete models naturally represent many real-world applications in robotics, finance, and environmental engineering. Inference with large-scale models is challenging because relational structures deteriorate rapidly during inference with observations. The main contribution of this paper is an efficient relational variational inference algorithm that factors largescale probability models into simpler variational models, composed of mixtures of iid (Bernoulli) random variables. The algorithm takes probability relational models of largescale hybrid systems and converts them to a close-to-optimal variational models. Then, it efficiently calculates marginal probabilities on the variational models by using a latent (or lifted) variable elimination or a lifted stochastic sampling. This inference is unique because it maintains the relational structure upon individual observations and during inference steps. version:1
arxiv-1210-4863 | DBN-Based Combinatorial Resampling for Articulated Object Tracking | http://arxiv.org/abs/1210.4863 | id:1210.4863 author:Severine Dubuisson, Christophe Gonzales, Xuan Son NGuyen category:cs.CV  published:2012-10-16 summary:Particle Filter is an effective solution to track objects in video sequences in complex situations. Its key idea is to estimate the density over the possible states of the object using a weighted sample whose elements are called particles. One of its crucial step is a resampling step in which particles are resampled to avoid some degeneracy problem. In this paper, we introduce a new resampling method called Combinatorial Resampling that exploits some features of articulated objects to resample over an implicitly created sample of an exponential size better representing the density to estimate. We prove that it is sound and, through experimentations both on challenging synthetic and real video sequences, we show that it outperforms all classical resampling methods both in terms of the quality of its results and in terms of response times. version:1
arxiv-1210-4862 | Sample-efficient Nonstationary Policy Evaluation for Contextual Bandits | http://arxiv.org/abs/1210.4862 | id:1210.4862 author:Miroslav Dudik, Dumitru Erhan, John Langford, Lihong Li category:cs.LG stat.ML  published:2012-10-16 summary:We present and prove properties of a new offline policy evaluator for an exploration learning setting which is superior to previous evaluators. In particular, it simultaneously and correctly incorporates techniques from importance weighting, doubly robust evaluation, and nonstationary policy evaluation approaches. In addition, our approach allows generating longer histories by careful control of a bias-variance tradeoff, and further decreases variance by incorporating information about randomness of the target policy. Empirical evidence from synthetic and realworld exploration learning problems shows the new evaluator successfully unifies previous approaches and uses information an order of magnitude more efficiently. version:1
arxiv-1210-4860 | Spectral Estimation of Conditional Random Graph Models for Large-Scale Network Data | http://arxiv.org/abs/1210.4860 | id:1210.4860 author:Antonino Freno, Mikaela Keller, Gemma C. Garriga, Marc Tommasi category:cs.SI cs.LG physics.soc-ph stat.ML  published:2012-10-16 summary:Generative models for graphs have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are either only suitable for characterizing some particular network properties (such as degree distribution or clustering coefficient), or they are aimed at estimating joint probability distributions, which is often intractable in large-scale networks. In this paper, we first propose a novel network statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random graph model, switching the focus from the estimation of joint probability distributions to a more tractable conditional estimation setting. After analyzing the dependence structure characterizing Fiedler random graphs, we evaluate them experimentally in edge prediction over several real-world networks, showing that they allow to reach a much higher prediction accuracy than various alternative statistical models. version:1
arxiv-1210-4859 | Mechanism Design for Cost Optimal PAC Learning in the Presence of Strategic Noisy Annotators | http://arxiv.org/abs/1210.4859 | id:1210.4859 author:Dinesh Garg, Sourangshu Bhattacharya, S. Sundararajan, Shirish Shevade category:cs.LG cs.GT stat.ML  published:2012-10-16 summary:We consider the problem of Probably Approximate Correct (PAC) learning of a binary classifier from noisy labeled examples acquired from multiple annotators (each characterized by a respective classification noise rate). First, we consider the complete information scenario, where the learner knows the noise rates of all the annotators. For this scenario, we derive sample complexity bound for the Minimum Disagreement Algorithm (MDA) on the number of labeled examples to be obtained from each annotator. Next, we consider the incomplete information scenario, where each annotator is strategic and holds the respective noise rate as a private information. For this scenario, we design a cost optimal procurement auction mechanism along the lines of Myerson's optimal auction design framework in a non-trivial manner. This mechanism satisfies incentive compatibility property, thereby facilitating the learner to elicit true noise rates of all the annotators. version:1
arxiv-1210-4856 | Exploiting compositionality to explore a large space of model structures | http://arxiv.org/abs/1210.4856 | id:1210.4856 author:Roger Grosse, Ruslan R Salakhutdinov, William T. Freeman, Joshua B. Tenenbaum category:cs.LG stat.ML  published:2012-10-16 summary:The recent proliferation of richly structured probabilistic models raises the question of how to automatically determine an appropriate model for a dataset. We investigate this question for a space of matrix decomposition models which can express a variety of widely used models from unsupervised learning. To enable model selection, we organize these models into a context-free grammar which generates a wide variety of structures through the compositional application of a few simple rules. We use our grammar to generically and efficiently infer latent components and estimate predictive likelihood for nearly 2500 structures using a small toolbox of reusable algorithms. Using a greedy search over our grammar, we automatically choose the decomposition structure from raw data by evaluating only a small fraction of all models. The proposed method typically finds the correct structure for synthetic data and backs off gracefully to simpler models under heavy noise. It learns sensible structures for datasets as diverse as image patches, motion capture, 20 Questions, and U.S. Senate votes, all using exactly the same code. version:1
arxiv-1210-4855 | A Slice Sampler for Restricted Hierarchical Beta Process with Applications to Shared Subspace Learning | http://arxiv.org/abs/1210.4855 | id:1210.4855 author:Sunil Kumar Gupta, Dinh Q. Phung, Svetha Venkatesh category:cs.LG cs.CV stat.ML  published:2012-10-16 summary:Hierarchical beta process has found interesting applications in recent years. In this paper we present a modified hierarchical beta process prior with applications to hierarchical modeling of multiple data sources. The novel use of the prior over a hierarchical factor model allows factors to be shared across different sources. We derive a slice sampler for this model, enabling tractable inference even when the likelihood and the prior over parameters are non-conjugate. This allows the application of the model in much wider contexts without restrictions. We present two different data generative models a linear GaussianGaussian model for real valued data and a linear Poisson-gamma model for count data. Encouraging transfer learning results are shown for two real world applications text modeling and content based image retrieval. version:1
arxiv-1210-4854 | Semantic Understanding of Professional Soccer Commentaries | http://arxiv.org/abs/1210.4854 | id:1210.4854 author:Hannaneh Hajishirzi, Mohammad Rastegari, Ali Farhadi, Jessica K. Hodgins category:cs.CL cs.AI  published:2012-10-16 summary:This paper presents a novel approach to the problem of semantic parsing via learning the correspondences between complex sentences and rich sets of events. Our main intuition is that correct correspondences tend to occur more frequently. Our model benefits from a discriminative notion of similarity to learn the correspondence between sentence and an event and a ranking machinery that scores the popularity of each correspondence. Our method can discover a group of events (called macro-events) that best describes a sentence. We evaluate our method on our novel dataset of professional soccer commentaries. The empirical results show that our method significantly outperforms the state-of-theart. version:1
arxiv-1210-4851 | Learning to Rank With Bregman Divergences and Monotone Retargeting | http://arxiv.org/abs/1210.4851 | id:1210.4851 author:Sreangsu Acharyya, Oluwasanmi Koyejo, Joydeep Ghosh category:cs.LG stat.ML  published:2012-10-16 summary:This paper introduces a novel approach for learning to rank (LETOR) based on the notion of monotone retargeting. It involves minimizing a divergence between all monotonic increasing transformations of the training scores and a parameterized prediction function. The minimization is both over the transformations as well as over the parameters. It is applied to Bregman divergences, a large class of "distance like" functions that were recently shown to be the unique class that is statistically consistent with the normalized discounted gain (NDCG) criterion [19]. The algorithm uses alternating projection style updates, in which one set of simultaneous projections can be computed independent of the Bregman divergence and the other reduces to parameter estimation of a generalized linear model. This results in easily implemented, efficiently parallelizable algorithm for the LETOR task that enjoys global optimum guarantees under mild conditions. We present empirical results on benchmark datasets showing that this approach can outperform the state of the art NDCG consistent techniques. version:1
arxiv-1210-4850 | Markov Determinantal Point Processes | http://arxiv.org/abs/1210.4850 | id:1210.4850 author:Raja Hafiz Affandi, Alex Kulesza, Emily B. Fox category:cs.LG cs.IR stat.ML  published:2012-10-16 summary:A determinantal point process (DPP) is a random process useful for modeling the combinatorial problem of subset selection. In particular, DPPs encourage a random subset Y to contain a diverse set of items selected from a base set Y. For example, we might use a DPP to display a set of news headlines that are relevant to a user's interests while covering a variety of topics. Suppose, however, that we are asked to sequentially select multiple diverse sets of items, for example, displaying new headlines day-by-day. We might want these sets to be diverse not just individually but also through time, offering headlines today that are unlike the ones shown yesterday. In this paper, we construct a Markov DPP (M-DPP) that models a sequence of random sets {Yt}. The proposed M-DPP defines a stationary process that maintains DPP margins. Crucially, the induced union process Zt = Yt u Yt-1 is also marginally DPP-distributed. Jointly, these properties imply that the sequence of random sets are encouraged to be diverse both at a given time step as well as across time steps. We describe an exact, efficient sampling procedure, and a method for incrementally learning a quality measure over items in the base set Y based on external preferences. We apply the M-DPP to the task of sequentially displaying diverse and relevant news articles to a user with topic preferences. version:1
arxiv-1210-4846 | Variational Dual-Tree Framework for Large-Scale Transition Matrix Approximation | http://arxiv.org/abs/1210.4846 | id:1210.4846 author:Saeed Amizadeh, Bo Thiesson, Milos Hauskrecht category:cs.LG stat.ML  published:2012-10-16 summary:In recent years, non-parametric methods utilizing random walks on graphs have been used to solve a wide range of machine learning problems, but in their simplest form they do not scale well due to the quadratic complexity. In this paper, a new dual-tree based variational approach for approximating the transition matrix and efficiently performing the random walk is proposed. The approach exploits a connection between kernel density estimation, mixture modeling, and random walk on graphs in an optimization of the transition matrix for the data graph that ties together edge transitions probabilities that are similar. Compared to the de facto standard approximation method based on k-nearestneighbors, we demonstrate order of magnitudes speedup without sacrificing accuracy for Label Propagation tasks on benchmark data sets in semi-supervised learning. version:1
arxiv-1210-4843 | Deterministic MDPs with Adversarial Rewards and Bandit Feedback | http://arxiv.org/abs/1210.4843 | id:1210.4843 author:Raman Arora, Ofer Dekel, Ambuj Tewari category:cs.GT cs.LG  published:2012-10-16 summary:We consider a Markov decision process with deterministic state transition dynamics, adversarially generated rewards that change arbitrarily from round to round, and a bandit feedback model in which the decision maker only observes the rewards it receives. In this setting, we present a novel and efficient online decision making algorithm named MarcoPolo. Under mild assumptions on the structure of the transition dynamics, we prove that MarcoPolo enjoys a regret of O(T^(3/4)sqrt(log(T))) against the best deterministic policy in hindsight. Specifically, our analysis does not rely on the stringent unichain assumption, which dominates much of the previous work on this topic. version:1
arxiv-1210-4841 | An Efficient Message-Passing Algorithm for the M-Best MAP Problem | http://arxiv.org/abs/1210.4841 | id:1210.4841 author:Dhruv Batra category:cs.AI cs.LG stat.ML  published:2012-10-16 summary:Much effort has been directed at algorithms for obtaining the highest probability configuration in a probabilistic random field model known as the maximum a posteriori (MAP) inference problem. In many situations, one could benefit from having not just a single solution, but the top M most probable solutions known as the M-Best MAP problem. In this paper, we propose an efficient message-passing based algorithm for solving the M-Best MAP problem. Specifically, our algorithm solves the recently proposed Linear Programming (LP) formulation of M-Best MAP [7], while being orders of magnitude faster than a generic LP-solver. Our approach relies on studying a particular partial Lagrangian relaxation of the M-Best MAP LP which exposes a natural combinatorial structure of the problem that we exploit. version:1
arxiv-1210-4839 | Leveraging Side Observations in Stochastic Bandits | http://arxiv.org/abs/1210.4839 | id:1210.4839 author:Stephane Caron, Branislav Kveton, Marc Lelarge, Smriti Bhagat category:cs.LG stat.ML  published:2012-10-16 summary:This paper considers stochastic bandits with side observations, a model that accounts for both the exploration/exploitation dilemma and relationships between arms. In this setting, after pulling an arm i, the decision maker also observes the rewards for some other actions related to i. We will see that this model is suited to content recommendation in social networks, where users' reactions may be endorsed or not by their friends. We provide efficient algorithms based on upper confidence bounds (UCBs) to leverage this additional information and derive new bounds improving on standard regret guarantees. We also evaluate these policies in the context of movie recommendation in social networks: experiments on real datasets show substantial learning rate speedups ranging from 2.2x to 14x on dense networks. version:1
arxiv-1210-4347 | Hilbert Space Embedding for Dirichlet Process Mixtures | http://arxiv.org/abs/1210.4347 | id:1210.4347 author:Krikamol Muandet category:stat.ML cs.LG  published:2012-10-16 summary:This paper proposes a Hilbert space embedding for Dirichlet Process mixture models via a stick-breaking construction of Sethuraman. Although Bayesian nonparametrics offers a powerful approach to construct a prior that avoids the need to specify the model size/complexity explicitly, an exact inference is often intractable. On the other hand, frequentist approaches such as kernel machines, which suffer from the model selection/comparison problems, often benefit from efficient learning algorithms. This paper discusses the possibility to combine the best of both worlds by using the Dirichlet Process mixture model as a case study. version:1
arxiv-1211-1252 | Implementation of Radon Transformation for Electrical Impedance Tomography (EIT) | http://arxiv.org/abs/1211.1252 | id:1211.1252 author:Md. Ali Hossain, Ahsan-Ul-Ambia, Md. Aktaruzzaman, Md. Ahaduzzaman Khan category:cs.CV  published:2012-10-16 summary:Radon Transformation is generally used to construct optical image (like CT image) from the projection data in biomedical imaging. In this paper, the concept of Radon Transformation is implemented to reconstruct Electrical Impedance Topographic Image (conductivity or resistivity distribution) of a circular subject. A parallel resistance model of a subject is proposed for Electrical Impedance Topography(EIT) or Magnetic Induction Tomography(MIT). A circular subject with embedded circular objects is segmented into equal width slices from different angles. For each angle, Conductance and Conductivity of each slice is calculated and stored in an array. A back projection method is used to generate a two-dimensional image from one-dimensional projections. As a back projection method, Inverse Radon Transformation is applied on the calculated conductance and conductivity to reconstruct two dimensional images. These images are compared to the target image. In the time of image reconstruction, different filters are used and these images are compared with each other and target image. version:1
arxiv-1210-4276 | Semi-Supervised Classification Through the Bag-of-Paths Group Betweenness | http://arxiv.org/abs/1210.4276 | id:1210.4276 author:Bertrand Lebichot, Ilkka Kivimäki, Kevin Françoisse, Marco Saerens category:stat.ML cs.LG  published:2012-10-16 summary:This paper introduces a novel, well-founded, betweenness measure, called the Bag-of-Paths (BoP) betweenness, as well as its extension, the BoP group betweenness, to tackle semisupervised classification problems on weighted directed graphs. The objective of semi-supervised classification is to assign a label to unlabeled nodes using the whole topology of the graph and the labeled nodes at our disposal. The BoP betweenness relies on a bag-of-paths framework assigning a Boltzmann distribution on the set of all possible paths through the network such that long (high-cost) paths have a low probability of being picked from the bag, while short (low-cost) paths have a high probability of being picked. Within that context, the BoP betweenness of node j is defined as the sum of the a posteriori probabilities that node j lies in-between two arbitrary nodes i, k, when picking a path starting in i and ending in k. Intuitively, a node typically receives a high betweenness if it has a large probability of appearing on paths connecting two arbitrary nodes of the network. This quantity can be computed in closed form by inverting a n x n matrix where n is the number of nodes. For the group betweenness, the paths are constrained to start and end in nodes within the same class, therefore defining a group betweenness for each class. Unlabeled nodes are then classified according to the class showing the highest group betweenness. Experiments on various real-world data sets show that BoP group betweenness outperforms all the tested state of-the-art methods. The benefit of the BoP betweenness is particularly noticeable when only a few labeled nodes are available. version:1
arxiv-1210-3404 | A polygon-based interpolation operator for super-resolution imaging | http://arxiv.org/abs/1210.3404 | id:1210.3404 author:Stéfan J. van der Walt, B. M. Herbst category:cs.CV  published:2012-10-12 summary:We outline the super-resolution reconstruction problem posed as a maximization of probability. We then introduce an interpolation method based on polygonal pixel overlap, express it as a linear operator, and use it to improve reconstruction. Polygon interpolation outperforms the simpler bilinear interpolation operator and, unlike Gaussian modeling of pixels, requires no parameter estimation. A free software implementation that reproduces the results shown is provided. version:2
arxiv-1210-4184 | The Kernel Pitman-Yor Process | http://arxiv.org/abs/1210.4184 | id:1210.4184 author:Sotirios P. Chatzis, Dimitrios Korkinof, Yiannis Demiris category:cs.LG cs.AI stat.ML  published:2012-10-15 summary:In this work, we propose the kernel Pitman-Yor process (KPYP) for nonparametric clustering of data with general spatial or temporal interdependencies. The KPYP is constructed by first introducing an infinite sequence of random locations. Then, based on the stick-breaking construction of the Pitman-Yor process, we define a predictor-dependent random probability measure by considering that the discount hyperparameters of the Beta-distributed random weights (stick variables) of the process are not uniform among the weights, but controlled by a kernel function expressing the proximity between the location assigned to each weight and the given predictors. version:1
arxiv-1210-4145 | A Biologically Realistic Model of Saccadic Eye Control with Probabilistic Population Codes | http://arxiv.org/abs/1210.4145 | id:1210.4145 author:Sacha Sokoloski category:cs.NE q-bio.NC  published:2012-10-15 summary:The posterior parietal cortex is believed to direct eye movements, especially in regards to target tracking tasks, and a number of debates exist over the precise nature of the computations performed by the parietal cortex, with each side supported by different sets of biological evidence. In this paper I will present my model which navigates a course between some of these debates, towards the end of presenting a model which can explain some of the competing interpretations among the data sets. In particular, rather than assuming that proprioception or efference copies form the key source of information for computing eye position information, I use a biological plausible implementation of a Kalman filter to optimally combine the two signals, and a simple gain control mechanism in order to accommodate the latency of the proprioceptive signal. Fitting within the Bayesian brain hypothesis, the result is a Bayes optimal solution to the eye control problem, with a range of data supporting claims of biological plausibility. version:1
arxiv-1210-4081 | Getting Feasible Variable Estimates From Infeasible Ones: MRF Local Polytope Study | http://arxiv.org/abs/1210.4081 | id:1210.4081 author:Bogdan Savchynskyy, Stefan Schmidt category:cs.NA cs.CV cs.DS cs.LG math.OC  published:2012-10-15 summary:This paper proposes a method for construction of approximate feasible primal solutions from dual ones for large-scale optimization problems possessing certain separability properties. Whereas infeasible primal estimates can typically be produced from (sub-)gradients of the dual function, it is often not easy to project them to the primal feasible set, since the projection itself has a complexity comparable to the complexity of the initial problem. We propose an alternative efficient method to obtain feasibility and show that its properties influencing the convergence to the optimum are similar to the properties of the Euclidean projection. We apply our method to the local polytope relaxation of inference problems for Markov Random Fields and demonstrate its superiority over existing methods. version:1
arxiv-1210-4021 | Local Optima Networks, Landscape Autocorrelation and Heuristic Search Performance | http://arxiv.org/abs/1210.4021 | id:1210.4021 author:Francisco Chicano, Fabio Daolio, Gabriela Ochoa, Sébastien Verel, Marco Tomassini, Enrique Alba category:cs.AI cs.NE  published:2012-10-15 summary:Recent developments in fitness landscape analysis include the study of Local Optima Networks (LON) and applications of the Elementary Landscapes theory. This paper represents a first step at combining these two tools to explore their ability to forecast the performance of search algorithms. We base our analysis on the Quadratic Assignment Problem (QAP) and conduct a large statistical study over 600 generated instances of different types. Our results reveal interesting links between the network measures, the autocorrelation measures and the performance of heuristic search algorithms. version:1
arxiv-1210-4006 | The Perturbed Variation | http://arxiv.org/abs/1210.4006 | id:1210.4006 author:Maayan Harel, Shie Mannor category:cs.LG stat.ML  published:2012-10-15 summary:We introduce a new discrepancy score between two distributions that gives an indication on their similarity. While much research has been done to determine if two samples come from exactly the same distribution, much less research considered the problem of determining if two finite samples come from similar distributions. The new score gives an intuitive interpretation of similarity; it optimally perturbs the distributions so that they best fit each other. The score is defined between distributions, and can be efficiently estimated from samples. We provide convergence bounds of the estimated score, and develop hypothesis testing procedures that test if two data sets come from similar distributions. The statistical power of this procedures is presented in simulations. We also compare the score's capacity to detect similarity with that of other known measures on real data. version:1
arxiv-1210-3865 | Opinion Mining for Relating Subjective Expressions and Annual Earnings in US Financial Statements | http://arxiv.org/abs/1210.3865 | id:1210.3865 author:Chien-Liang Chen, Chao-Lin Liu, Yuan-Chen Chang, Hsiang-Ping Tsai category:cs.CL cs.AI cs.IR q-fin.GN  published:2012-10-15 summary:Financial statements contain quantitative information and manager's subjective evaluation of firm's financial status. Using information released in U.S. 10-K filings. Both qualitative and quantitative appraisals are crucial for quality financial decisions. To extract such opinioned statements from the reports, we built tagging models based on the conditional random field (CRF) techniques, considering a variety of combinations of linguistic factors including morphology, orthography, predicate-argument structure, syntax, and simple semantics. Our results show that the CRF models are reasonably effective to find opinion holders in experiments when we adopted the popular MPQA corpus for training and testing. The contribution of our paper is to identify opinion patterns in multiword expressions (MWEs) forms rather than in single word forms. We find that the managers of corporations attempt to use more optimistic words to obfuscate negative financial performance and to accentuate the positive financial performance. Our results also show that decreasing earnings were often accompanied by ambiguous and mild statements in the reporting year and that increasing earnings were stated in assertive and positive way. version:1
arxiv-1210-3832 | Image Processing using Smooth Ordering of its Patches | http://arxiv.org/abs/1210.3832 | id:1210.3832 author:Idan Ram, Michael Elad, Israel Cohen category:cs.CV  published:2012-10-14 summary:We propose an image processing scheme based on reordering of its patches. For a given corrupted image, we extract all patches with overlaps, refer to these as coordinates in high-dimensional space, and order them such that they are chained in the "shortest possible path", essentially solving the traveling salesman problem. The obtained ordering applied to the corrupted image, implies a permutation of the image pixels to what should be a regular signal. This enables us to obtain good recovery of the clean image by applying relatively simple 1D smoothing operations (such as filtering or interpolation) to the reordered set of pixels. We explore the use of the proposed approach to image denoising and inpainting, and show promising results in both cases. version:1
arxiv-1210-3741 | Online computation of sparse representations of time varying stimuli using a biologically motivated neural network | http://arxiv.org/abs/1210.3741 | id:1210.3741 author:Tao Hu, Dmitri B. Chklovskii category:q-bio.NC cs.NE  published:2012-10-13 summary:Natural stimuli are highly redundant, possessing significant spatial and temporal correlations. While sparse coding has been proposed as an efficient strategy employed by neural systems to encode sensory stimuli, the underlying mechanisms are still not well understood. Most previous approaches model the neural dynamics by the sparse representation dictionary itself and compute the representation coefficients offline. In reality, faced with the challenge of constantly changing stimuli, neurons must compute the sparse representations dynamically in an online fashion. Here, we describe a leaky linearized Bregman iteration (LLBI) algorithm which computes the time varying sparse representations using a biologically motivated network of leaky rectifying neurons. Compared to previous attempt of dynamic sparse coding, LLBI exploits the temporal correlation of stimuli and demonstrate better performance both in representation error and the smoothness of temporal evolution of sparse coefficients. version:1
arxiv-1210-3729 | Inference of Fine-grained Attributes of Bengali Corpus for Stylometry Detection | http://arxiv.org/abs/1210.3729 | id:1210.3729 author:Tanmoy Chakraborty, Sivaji Bandyopadhyay category:cs.CL cs.CV  published:2012-10-13 summary:Stylometry, the science of inferring characteristics of the author from the characteristics of documents written by that author, is a problem with a long history and belongs to the core task of Text categorization that involves authorship identification, plagiarism detection, forensic investigation, computer security, copyright and estate disputes etc. In this work, we present a strategy for stylometry detection of documents written in Bengali. We adopt a set of fine-grained attribute features with a set of lexical markers for the analysis of the text and use three semi-supervised measures for making decisions. Finally, a majority voting approach has been taken for final classification. The system is fully automatic and language-independent. Evaluation results of our attempt for Bengali author's stylometry detection show reasonably promising accuracy in comparison to the baseline model. version:1
arxiv-1210-3718 | On the Role of Contrast and Regularity in Perceptual Boundary Saliency | http://arxiv.org/abs/1210.3718 | id:1210.3718 author:Mariano Tepper, Pablo Musé, Andrés Almansa category:cs.CV stat.AP  published:2012-10-13 summary:Mathematical Morphology proposes to extract shapes from images as connected components of level sets. These methods prove very suitable for shape recognition and analysis. We present a method to select the perceptually significant (i.e., contrasted) level lines (boundaries of level sets), using the Helmholtz principle as first proposed by Desolneux et al. Contrarily to the classical formulation by Desolneux et al. where level lines must be entirely salient, the proposed method allows to detect partially salient level lines, thus resulting in more robust and more stable detections. We then tackle the problem of combining two gestalts as a measure of saliency and propose a method that reinforces detections. Results in natural images show the good performance of the proposed methods. version:1
arxiv-1109-5231 | Noise Tolerance under Risk Minimization | http://arxiv.org/abs/1109.5231 | id:1109.5231 author:Naresh Manwani, P. S. Sastry category:cs.LG  published:2011-09-24 summary:In this paper we explore noise tolerant learning of classifiers. We formulate the problem as follows. We assume that there is an ${\bf unobservable}$ training set which is noise-free. The actual training set given to the learning algorithm is obtained from this ideal data set by corrupting the class label of each example. The probability that the class label of an example is corrupted is a function of the feature vector of the example. This would account for most kinds of noisy data one encounters in practice. We say that a learning method is noise tolerant if the classifiers learnt with the ideal noise-free data and with noisy data, both have the same classification accuracy on the noise-free data. In this paper we analyze the noise tolerance properties of risk minimization (under different loss functions), which is a generic method for learning classifiers. We show that risk minimization under 0-1 loss function has impressive noise tolerance properties and that under squared error loss is tolerant only to uniform noise; risk minimization under other loss functions is not noise tolerant. We conclude the paper with some discussion on implications of these theoretical results. version:4
arxiv-1009-3604 | Geometric Decision Tree | http://arxiv.org/abs/1009.3604 | id:1009.3604 author:Naresh Manwani, P. S. Sastry category:cs.LG  published:2010-09-19 summary:In this paper we present a new algorithm for learning oblique decision trees. Most of the current decision tree algorithms rely on impurity measures to assess the goodness of hyperplanes at each node while learning a decision tree in a top-down fashion. These impurity measures do not properly capture the geometric structures in the data. Motivated by this, our algorithm uses a strategy to assess the hyperplanes in such a way that the geometric structure in the data is taken into account. At each node of the decision tree, we find the clustering hyperplanes for both the classes and use their angle bisectors as the split rule at that node. We show through empirical studies that this idea leads to small decision trees and better performance. We also present some analysis to show that the angle bisectors of clustering hyperplanes that we use as the split rules at each node, are solutions of an interesting optimization problem and hence argue that this is a principled method of learning a decision tree. version:5
arxiv-1108-5244 | Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions | http://arxiv.org/abs/1108.5244 | id:1108.5244 author:Shuichi Kawano category:stat.ML stat.ME  published:2011-08-26 summary:This article addresses the problem of classification method based on both labeled and unlabeled data, where we assume that a density function for labeled data is different from that for unlabeled data. We propose a semi-supervised logistic regression model for classification problem along with the technique of covariate shift adaptation. Unknown parameters involved in proposed models are estimated by regularization with EM algorithm. A crucial issue in the modeling process is the choices of tuning parameters in our semi-supervised logistic models. In order to select the parameters, a model selection criterion is derived from an information-theoretic approach. Some numerical studies show that our modeling procedure performs well in various cases. version:3
arxiv-1210-3652 | A Flexible Mixed Integer Programming framework for Nurse Scheduling | http://arxiv.org/abs/1210.3652 | id:1210.3652 author:Murphy Choy, Michelle Cheong category:cs.DS cs.NE  published:2012-10-12 summary:In this paper, a nurse-scheduling model is developed using mixed integer programming model. It is deployed to a general care ward to replace and automate the current manual approach for scheduling. The developed model differs from other similar studies in that it optimizes both hospitals requirement as well as nurse preferences by allowing flexibility in the transfer of nurses from different duties. The model also incorporated additional policies which are part of the hospitals requirement but not part of the legislations. Hospitals key primary mission is to ensure continuous ward care service with appropriate number of nursing staffs and the right mix of nursing skills. The planning and scheduling is done to avoid additional non essential cost for hospital. Nurses preferences are taken into considerations such as the number of night shift and consecutive rest days. We will also reformulate problems from another paper which considers the penalty objective using the model but without the flexible components. The models are built using AIMMS which solves the problem in very short amount of time. version:1
arxiv-1210-3634 | Quick Summary | http://arxiv.org/abs/1210.3634 | id:1210.3634 author:Robert Wahlstedt category:cs.CL cs.AI  published:2012-10-12 summary:Quick Summary is an innovate implementation of an automatic document summarizer that inputs a document in the English language and evaluates each sentence. The scanner or evaluator determines criteria based on its grammatical structure and place in the paragraph. The program then asks the user to specify the number of sentences the person wishes to highlight. For example should the user ask to have three of the most important sentences, it would highlight the first and most important sentence in green. Commonly this is the sentence containing the conclusion. Then Quick Summary finds the second most important sentence usually called a satellite and highlights it in yellow. This is usually the topic sentence. Then the program finds the third most important sentence and highlights it in red. The implementations of this technology are useful in a society of information overload when a person typically receives 42 emails a day (Microsoft). The paper also is a candid look at difficulty that machine learning has in textural translating. However, it speaks on how to overcome the obstacles that historically prevented progress. This paper proposes mathematical meta-data criteria that justify the place of importance of a sentence. Just as tools for the study of relational symmetry in bio-informatics, this tool seeks to classify words with greater clarity. "Survey Finds Workers Average Only Three Productive Days per Week." Microsoft News Center. Microsoft. Web. 31 Mar. 2012. version:1
arxiv-1210-3448 | Notes on image annotation | http://arxiv.org/abs/1210.3448 | id:1210.3448 author:Adela Barriuso, Antonio Torralba category:cs.CV cs.HC  published:2012-10-12 summary:We are under the illusion that seeing is effortless, but frequently the visual system is lazy and makes us believe that we understand something when in fact we don't. Labeling a picture forces us to become aware of the difficulties underlying scene understanding. Suddenly, the act of seeing is not effortless anymore. We have to make an effort in order to understand parts of the picture that we neglected at first glance. In this report, an expert image annotator relates her experience on segmenting and labeling tens of thousands of images. During this process, the notes she took try to highlight the difficulties encountered, the solutions adopted, and the decisions made in order to get a consistent set of annotations. Those annotations constitute the SUN database. version:1
arxiv-1210-3350 | Enhanced Compressed Sensing Recovery with Level Set Normals | http://arxiv.org/abs/1210.3350 | id:1210.3350 author:Virginia Estellers, Jean-Philippe Thiran, Xavier Bresson category:cs.CV  published:2012-10-11 summary:We propose a compressive sensing algorithm that exploits geometric properties of images to recover images of high quality from few measurements. The image reconstruction is done by iterating the two following steps: 1) estimation of normal vectors of the image level curves and 2) reconstruction of an image fitting the normal vectors, the compressed sensing measurements and the sparsity constraint. The proposed technique can naturally extend to non local operators and graphs to exploit the repetitive nature of textured images in order to recover fine detail structures. In both cases, the problem is reduced to a series of convex minimization problems that can be efficiently solved with a combination of variable splitting and augmented Lagrangian methods, leading to fast and easy-to-code algorithms. Extended experiments show a clear improvement over related state-of-the-art algorithms in the quality of the reconstructed images and the robustness of the proposed method to noise, different kind of images and reduced measurements. version:1
arxiv-1210-3326 | Three dimensional tracking of gold nanoparticles using digital holographic microscopy | http://arxiv.org/abs/1210.3326 | id:1210.3326 author:Frédéric Verpillat, Fadwa Joud, Pierre Desbiolles, Michel Gross category:physics.optics cs.CV  published:2012-10-11 summary:In this paper we present a digital holographic microscope to track gold colloids in three dimensions. We report observations of 100nm gold particles in motion in water. The expected signal and the chosen method of reconstruction are described. We also discuss about how to implement the numerical calculation to reach real-time 3D tracking. version:1
arxiv-1210-3312 | Artex is AnotheR TEXt summarizer | http://arxiv.org/abs/1210.3312 | id:1210.3312 author:Juan-Manuel Torres-Moreno category:cs.IR cs.AI cs.CL  published:2012-10-11 summary:This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-based linguistic post-processing is necessary in order to obtain summaries. Tests over several datasets (coming from Document Understanding Conferences (DUC), Text Analysis Conferences (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown that summarizer achieves interesting results. version:1
arxiv-1210-3288 | Unsupervised Detection and Tracking of Arbitrary Objects with Dependent Dirichlet Process Mixtures | http://arxiv.org/abs/1210.3288 | id:1210.3288 author:Willie Neiswanger, Frank Wood category:stat.ML cs.CV cs.LG  published:2012-10-11 summary:This paper proposes a technique for the unsupervised detection and tracking of arbitrary objects in videos. It is intended to reduce the need for detection and localization methods tailored to specific object types and serve as a general framework applicable to videos with varied objects, backgrounds, and image qualities. The technique uses a dependent Dirichlet process mixture (DDPM) known as the Generalized Polya Urn (GPUDDPM) to model image pixel data that can be easily and efficiently extracted from the regions in a video that represent objects. This paper describes a specific implementation of the model using spatial and color pixel data extracted via frame differencing and gives two algorithms for performing inference in the model to accomplish detection and tracking. This technique is demonstrated on multiple synthetic and benchmark video datasets that illustrate its ability to, without modification, detect and track objects with diverse physical characteristics moving over non-uniform backgrounds and through occlusion. version:1
arxiv-1206-5057 | The Robustness and Super-Robustness of L^p Estimation, when p < 1 | http://arxiv.org/abs/1206.5057 | id:1206.5057 author:Qinghuai Gao category:cs.LG math.ST stat.TH  published:2012-06-22 summary:In robust statistics, the breakdown point of an estimator is the percentage of outliers with which an estimator still generates reliable estimation. The upper bound of breakdown point is 50%, which means it is not possible to generate reliable estimation with more than half outliers. In this paper, it is shown that for majority of experiences, when the outliers exceed 50%, but if they are distributed randomly enough, it is still possible to generate a reliable estimation from minority good observations. The phenomenal of that the breakdown point is larger than 50% is named as super robustness. And, in this paper, a robust estimator is called strict robust if it generates a perfect estimation when all the good observations are perfect. More specifically, the super robustness of the maximum likelihood estimator of the exponential power distribution, or L^p estimation, where p<1, is investigated. This paper starts with proving that L^p (p<1) is a strict robust location estimator. Further, it is proved that L^p (p < 1)has the property of strict super-robustness on translation, rotation, scaling transformation and robustness on Euclidean transform. version:5
arxiv-1108-2836 | Adaptive sequential Monte Carlo by means of mixture of experts | http://arxiv.org/abs/1108.2836 | id:1108.2836 author:J. Cornebise, E. Moulines, J. Olsson category:stat.ME stat.CO stat.ML  published:2011-08-14 summary:Appropriately designing the proposal kernel of particle filters is an issue of significant importance, since a bad choice may lead to deterioration of the particle sample and, consequently, waste of computational power. In this paper we introduce a novel algorithm adaptively approximating the so-called optimal proposal kernel by a mixture of integrated curved exponential distributions with logistic weights. This family of distributions, referred to as mixtures of experts, is broad enough to be used in the presence of multi-modality or strongly skewed distributions. The mixtures are fitted, via online-EM methods, to the optimal kernel through minimisation of the Kullback-Leibler divergence between the auxiliary target and instrumental distributions of the particle filter. At each iteration of the particle filter, the algorithm is required to solve only a single optimisation problem for the whole particle sample, yielding an algorithm with only linear complexity. In addition, we illustrate in a simulation study how the method can be successfully applied to optimal filtering in nonlinear state-space models. version:2
arxiv-1210-3165 | Computationally Efficient Implementation of Convolution-based Locally Adaptive Binarization Techniques | http://arxiv.org/abs/1210.3165 | id:1210.3165 author:Ayatullah Faruk Mollah, Subhadip Basu, Mita Nasipuri category:cs.CV  published:2012-10-11 summary:One of the most important steps of document image processing is binarization. The computational requirements of locally adaptive binarization techniques make them unsuitable for devices with limited computing facilities. In this paper, we have presented a computationally efficient implementation of convolution based locally adaptive binarization techniques keeping the performance comparable to the original implementation. The computational complexity has been reduced from O(W2N2) to O(WN2) where WxW is the window size and NxN is the image size. Experiments over benchmark datasets show that the computation time has been reduced by 5 to 15 times depending on the window size while memory consumption remains the same with respect to the state-of-the-art algorithmic implementation. version:1
arxiv-1210-3139 | A Benchmark to Select Data Mining Based Classification Algorithms For Business Intelligence And Decision Support Systems | http://arxiv.org/abs/1210.3139 | id:1210.3139 author:Pardeep Kumar, Nitin, Vivek Kumar Sehgal, Durg Singh Chauhan category:cs.DB cs.LG  published:2012-10-11 summary:DSS serve the management, operations, and planning levels of an organization and help to make decisions, which may be rapidly changing and not easily specified in advance. Data mining has a vital role to extract important information to help in decision making of a decision support system. Integration of data mining and decision support systems (DSS) can lead to the improved performance and can enable the tackling of new types of problems. Artificial Intelligence methods are improving the quality of decision support, and have become embedded in many applications ranges from ant locking automobile brakes to these days interactive search engines. It provides various machine learning techniques to support data mining. The classification is one of the main and valuable tasks of data mining. Several types of classification algorithms have been suggested, tested and compared to determine the future trends based on unseen data. There has been no single algorithm found to be superior over all others for all data sets. The objective of this paper is to compare various classification algorithms that have been frequently used in data mining for decision support systems. Three decision trees based algorithms, one artificial neural network, one statistical, one support vector machines with and without ada boost and one clustering algorithm are tested and compared on four data sets from different domains in terms of predictive accuracy, error rate, classification index, comprehensibility and training time. Experimental results demonstrate that Genetic Algorithm (GA) and support vector machines based algorithms are better in terms of predictive accuracy. SVM without adaboost shall be the first choice in context of speed and predictive accuracy. Adaboost improves the accuracy of SVM but on the cost of large training time. version:1
arxiv-1209-5833 | Locality-Sensitive Hashing with Margin Based Feature Selection | http://arxiv.org/abs/1209.5833 | id:1209.5833 author:Makiko Konoshima, Yui Noma category:cs.LG cs.IR  published:2012-09-26 summary:We propose a learning method with feature selection for Locality-Sensitive Hashing. Locality-Sensitive Hashing converts feature vectors into bit arrays. These bit arrays can be used to perform similarity searches and personal authentication. The proposed method uses bit arrays longer than those used in the end for similarity and other searches and by learning selects the bits that will be used. We demonstrated this method can effectively perform optimization for cases such as fingerprint images with a large number of labels and extremely few data that share the same labels, as well as verifying that it is also effective for natural images, handwritten digits, and speech features. version:2
arxiv-1209-4893 | On the Sensitivity of Shape Fitting Problems | http://arxiv.org/abs/1209.4893 | id:1209.4893 author:Kasturi Varadarajan, Xin Xiao category:cs.CG cs.LG  published:2012-09-21 summary:In this article, we study shape fitting problems, $\epsilon$-coresets, and total sensitivity. We focus on the $(j,k)$-projective clustering problems, including $k$-median/$k$-means, $k$-line clustering, $j$-subspace approximation, and the integer $(j,k)$-projective clustering problem. We derive upper bounds of total sensitivities for these problems, and obtain $\epsilon$-coresets using these upper bounds. Using a dimension-reduction type argument, we are able to greatly simplify earlier results on total sensitivity for the $k$-median/$k$-means clustering problems, and obtain positively-weighted $\epsilon$-coresets for several variants of the $(j,k)$-projective clustering problem. We also extend an earlier result on $\epsilon$-coresets for the integer $(j,k)$-projective clustering problem in fixed dimension to the case of high dimension. version:2
arxiv-1210-3039 | Sequential Convex Programming Methods for A Class of Structured Nonlinear Programming | http://arxiv.org/abs/1210.3039 | id:1210.3039 author:Zhaosong Lu category:math.OC cs.NA stat.CO stat.ML  published:2012-10-10 summary:In this paper we study a broad class of structured nonlinear programming (SNLP) problems. In particular, we first establish the first-order optimality conditions for them. Then we propose sequential convex programming (SCP) methods for solving them in which each iteration is obtained by solving a convex programming problem exactly or inexactly. Under some suitable assumptions, we establish that any accumulation point of the sequence generated by the methods is a KKT point of the SNLP problems. In addition, we propose a variant of the exact SCP method for SNLP in which nonmonotone scheme and "local" Lipschitz constants of the associated functions are used. And a similar convergence result as mentioned above is established. version:1
arxiv-1210-2877 | Efficient Solution to the 3D Problem of Automatic Wall Paintings Reassembly | http://arxiv.org/abs/1210.2877 | id:1210.2877 author:Constantin Papaodysseus, Dimitris Arabadjis, Michalis Exarhos, Panayiotis Rousopoulos, Solomon Zannos, Michail Panagopoulos, Lena Papazoglou-Manioudaki category:cs.CV math.DG  published:2012-10-10 summary:This paper introduces a new approach for the automated reconstruction - reassembly of fragmented objects having one surface near to plane, on the basis of the 3D representation of their constituent fragments. The whole process starts by 3D scanning of the available fragments. The obtained representations are properly processed so that they can be tested for possible matches. Next, four novel criteria are introduced, that lead to the determination of pairs of matching fragments. These criteria have been chosen so as the whole process imitates the instinctive reassembling method dedicated scholars apply. The first criterion exploits the volume of the gap between two properly placed fragments. The second one considers the fragments' overlapping in each possible matching position. Criteria 3,4 employ principles from calculus of variations to obtain bounds for the area and the mean curvature of the contact surfaces and the length of contact curves, which must hold if the two fragments match. The method has been applied, with great success, both in the reconstruction of objects artificially broken by the authors and, most importantly, in the virtual reassembling of parts of wall paintings belonging to the Mycenaic civilization (c. 1300 B.C.), excavated in a highly fragmented condition in Tyrins, Greece. version:1
arxiv-1210-4502 | Comparing several heuristics for a packing problem | http://arxiv.org/abs/1210.4502 | id:1210.4502 author:Camelia-M. Pintea, Cristian Pascan, Mara Hajdu-Macelaru category:cs.NE  published:2012-10-10 summary:Packing problems are in general NP-hard, even for simple cases. Since now there are no highly efficient algorithms available for solving packing problems. The two-dimensional bin packing problem is about packing all given rectangular items, into a minimum size rectangular bin, without overlapping. The restriction is that the items cannot be rotated. The current paper is comparing a greedy algorithm with a hybrid genetic algorithm in order to see which technique is better for the given problem. The algorithms are tested on different sizes data. version:1
arxiv-1210-2838 | Kinects and Human Kinetics: A New Approach for Studying Crowd Behavior | http://arxiv.org/abs/1210.2838 | id:1210.2838 author:Stefan Seer, Norbert Brändle, Carlo Ratti category:cs.CV physics.soc-ph  published:2012-10-10 summary:Modeling crowd behavior relies on accurate data of pedestrian movements at a high level of detail. Imaging sensors such as cameras provide a good basis for capturing such detailed pedestrian motion data. However, currently available computer vision technologies, when applied to conventional video footage, still cannot automatically unveil accurate motions of groups of people or crowds from the image sequences. We present a novel data collection approach for studying crowd behavior which uses the increasingly popular low-cost sensor Microsoft Kinect. The Kinect captures both standard camera data and a three-dimensional depth map. Our human detection and tracking algorithm is based on agglomerative clustering of depth data captured from an elevated view - in contrast to the lateral view used for gesture recognition in Kinect gaming applications. Our approach transforms local Kinect 3D data to a common world coordinate system in order to stitch together human trajectories from multiple Kinects, which allows for a scalable and flexible capturing area. At a testbed with real-world pedestrian traffic we demonstrate that our approach can provide accurate trajectories from three Kinects with a Pedestrian Detection Rate of up to 94% and a Multiple Object Tracking Precision of 4 cm. Using a comprehensive dataset of 2240 captured human trajectories we calibrate three variations of the Social Force model. The results of our model validations indicate their particular ability to reproduce the observed crowd behavior in microscopic simulations. version:1
arxiv-1210-2826 | An anisotropy preserving metric for DTI processing | http://arxiv.org/abs/1210.2826 | id:1210.2826 author:Anne Collard, Silvère Bonnabel, Christophe Phillips, Rodolphe Sepulchre category:cs.CV math.DG  published:2012-10-10 summary:Statistical analysis of Diffusion Tensor Imaging (DTI) data requires a computational framework that is both numerically tractable (to account for the high dimensional nature of the data) and geometric (to account for the nonlinear nature of diffusion tensors). Building upon earlier studies that have shown that a Riemannian framework is appropriate to address these challenges, the present paper proposes a novel metric and an accompanying computational framework for DTI data processing. The proposed metric retains the geometry and the computational tractability of earlier methods grounded in the affine invariant metric. In addition, and in contrast to earlier methods, it provides an interpolation method which preserves anisotropy, a central information carried by diffusion tensor data. version:1
arxiv-1202-3772 | Rank/Norm Regularization with Closed-Form Solutions: Application to Subspace Clustering | http://arxiv.org/abs/1202.3772 | id:1202.3772 author:Yao-Liang Yu, Dale Schuurmans category:cs.LG cs.NA stat.ML  published:2012-02-14 summary:When data is sampled from an unknown subspace, principal component analysis (PCA) provides an effective way to estimate the subspace and hence reduce the dimension of the data. At the heart of PCA is the Eckart-Young-Mirsky theorem, which characterizes the best rank k approximation of a matrix. In this paper, we prove a generalization of the Eckart-Young-Mirsky theorem under all unitarily invariant norms. Using this result, we obtain closed-form solutions for a set of rank/norm regularized problems, and derive closed-form solutions for a general class of subspace clustering problems (where data is modelled by unions of unknown subspaces). From these results we obtain new theoretical insights and promising experimental results. version:2
arxiv-1210-2646 | A General Methodology for the Determination of 2D Bodies Elastic Deformation Invariants. Application to the Automatic Identification of Parasites | http://arxiv.org/abs/1210.2646 | id:1210.2646 author:Dimitris Arabadjis, Panayiotis Rousopoulos, Constantin Papaodysseus, Michalis Panagopoulos, Panayiota Loumou, Georgios Theodoropoulos category:cs.CV cs.AI  published:2012-10-09 summary:A novel methodology is introduced here that exploits 2D images of arbitrary elastic body deformation instances, so as to quantify mechano-elastic characteristics that are deformation invariant. Determination of such characteristics allows for developing methods offering an image of the undeformed body. General assumptions about the mechano-elastic properties of the bodies are stated, which lead to two different approaches for obtaining bodies' deformation invariants. One was developed to spot deformed body's neutral line and its cross sections, while the other solves deformation PDEs by performing a set of equivalent image operations on the deformed body images. Both these processes may furnish a body undeformed version from its deformed image. This was confirmed by obtaining the undeformed shape of deformed parasites, cells (protozoa), fibers and human lips. In addition, the method has been applied to the important problem of parasite automatic classification from their microscopic images. To achieve this, we first apply the previous method to straighten the highly deformed parasites and then we apply a dedicated curve classification method to the straightened parasite contours. It is demonstrated that essentially different deformations of the same parasite give rise to practically the same undeformed shape, thus confirming the consistency of the introduced methodology. Finally, the developed pattern recognition method classifies the unwrapped parasites into 6 families, with an accuracy rate of 97.6 %. version:1
arxiv-1210-2640 | Multi-view constrained clustering with an incomplete mapping between views | http://arxiv.org/abs/1210.2640 | id:1210.2640 author:Eric Eaton, Marie desJardins, Sara Jacob category:cs.LG cs.AI  published:2012-10-09 summary:Multi-view learning algorithms typically assume a complete bipartite mapping between the different views in order to exchange information during the learning process. However, many applications provide only a partial mapping between the views, creating a challenge for current methods. To address this problem, we propose a multi-view algorithm based on constrained clustering that can operate with an incomplete mapping. Given a set of pairwise constraints in each view, our approach propagates these constraints using a local similarity measure to those instances that can be mapped to the other views, allowing the propagated constraints to be transferred across views via the partial mapping. It uses co-EM to iteratively estimate the propagation within each view based on the current clustering model, transfer the constraints across views, and then update the clustering model. By alternating the learning process between views, this approach produces a unified clustering model that is consistent with all views. We show that this approach significantly improves clustering performance over several other methods for transferring constraints and allows multi-view clustering to be reliably applied when given a limited mapping between the views. Our evaluation reveals that the propagated constraints have high precision with respect to the true clusters in the data, explaining their benefit to clustering performance in both single- and multi-view learning scenarios. version:1
arxiv-1210-2629 | Optimization in Differentiable Manifolds in Order to Determine the Method of Construction of Prehistoric Wall-Paintings | http://arxiv.org/abs/1210.2629 | id:1210.2629 author:Dimitris Arabadjis, Panayiotis Rousopoulos, Constantin Papaodysseus, Michalis Exarhos, Michalis Panagopoulos, Lena Papazoglou-Manioudaki category:cs.CV cs.AI cs.CG  published:2012-10-09 summary:In this paper a general methodology is introduced for the determination of potential prototype curves used for the drawing of prehistoric wall-paintings. The approach includes a) preprocessing of the wall-paintings contours to properly partition them, according to their curvature, b) choice of prototype curves families, c) analysis and optimization in 4-manifold for a first estimation of the form of these prototypes, d) clustering of the contour parts and the prototypes, to determine a minimal number of potential guides, e) further optimization in 4-manifold, applied to each cluster separately, in order to determine the exact functional form of the potential guides, together with the corresponding drawn contour parts. The introduced methodology simultaneously deals with two problems: a) the arbitrariness in data-points orientation and b) the determination of one proper form for a prototype curve that optimally fits the corresponding contour data. Arbitrariness in orientation has been dealt with a novel curvature based error, while the proper forms of curve prototypes have been exhaustively determined by embedding curvature deformations of the prototypes into 4-manifolds. Application of this methodology to celebrated wall-paintings excavated at Tyrins, Greece and the Greek island of Thera, manifests it is highly probable that these wall-paintings had been drawn by means of geometric guides that correspond to linear spirals and hyperbolae. These geometric forms fit the drawings' lines with an exceptionally low average error, less than 0.39mm. Hence, the approach suggests the existence of accurate realizations of complicated geometric entities, more than 1000 years before their axiomatic formulation in Classical Ages. version:1
arxiv-1210-2503 | Gaussian process modelling of multiple short time series | http://arxiv.org/abs/1210.2503 | id:1210.2503 author:Hande Topa, Antti Honkela category:stat.ML q-bio.QM stat.ME  published:2012-10-09 summary:We present techniques for effective Gaussian process (GP) modelling of multiple short time series. These problems are common when applying GP models independently to each gene in a gene expression time series data set. Such sets typically contain very few time points. Naive application of common GP modelling techniques can lead to severe over-fitting or under-fitting in a significant fraction of the fitted models, depending on the details of the data set. We propose avoiding over-fitting by constraining the GP length-scale to values that focus most of the energy spectrum to frequencies below the Nyquist frequency corresponding to the sampling frequency in the data set. Under-fitting can be avoided by more informative priors on observation noise. Combining these methods allows applying GP methods reliably automatically to large numbers of independent instances of short time series. This is illustrated with experiments with both synthetic data and real gene expression data. version:1
arxiv-1207-0578 | Parameterized Runtime Analyses of Evolutionary Algorithms for the Euclidean Traveling Salesperson Problem | http://arxiv.org/abs/1207.0578 | id:1207.0578 author:Andrew M. Sutton, Frank Neumann category:cs.NE cs.DS  published:2012-07-03 summary:Parameterized runtime analysis seeks to understand the influence of problem structure on algorithmic runtime. In this paper, we contribute to the theoretical understanding of evolutionary algorithms and carry out a parameterized analysis of evolutionary algorithms for the Euclidean traveling salesperson problem (Euclidean TSP). We investigate the structural properties in TSP instances that influence the optimization process of evolutionary algorithms and use this information to bound the runtime of simple evolutionary algorithms. Our analysis studies the runtime in dependence of the number of inner points $k$ and shows that $(\mu + \lambda)$ evolutionary algorithms solve the Euclidean TSP in expected time $O((\mu/\lambda) \cdot n^3\gamma(\epsilon) + n\gamma(\epsilon) + (\mu/\lambda) \cdot n^{4k}(2k-1)!)$ where $\gamma$ is a function of the minimum angle $\epsilon$ between any three points. Finally, our analysis provides insights into designing a mutation operator that improves the upper bound on expected runtime. We show that a mixed mutation strategy that incorporates both 2-opt moves and permutation jumps results in an upper bound of $O((\mu/\lambda) \cdot n^3\gamma(\epsilon) + n\gamma(\epsilon) + (\mu/\lambda) \cdot n^{2k}(k-1)!)$ for the $(\mu+\lambda)$ EA. version:2
arxiv-1210-2474 | Level Set Estimation from Compressive Measurements using Box Constrained Total Variation Regularization | http://arxiv.org/abs/1210.2474 | id:1210.2474 author:Akshay Soni, Jarvis Haupt category:cs.CV stat.AP stat.ML  published:2012-10-09 summary:Estimating the level set of a signal from measurements is a task that arises in a variety of fields, including medical imaging, astronomy, and digital elevation mapping. Motivated by scenarios where accurate and complete measurements of the signal may not available, we examine here a simple procedure for estimating the level set of a signal from highly incomplete measurements, which may additionally be corrupted by additive noise. The proposed procedure is based on box-constrained Total Variation (TV) regularization. We demonstrate the performance of our approach, relative to existing state-of-the-art techniques for level set estimation from compressive measurements, via several simulation examples. version:1
arxiv-1210-2440 | Group Model Selection Using Marginal Correlations: The Good, the Bad and the Ugly | http://arxiv.org/abs/1210.2440 | id:1210.2440 author:Waheed U. Bajwa, Dustin G. Mixon category:math.ST cs.IT math.IT stat.ML stat.TH  published:2012-10-08 summary:Group model selection is the problem of determining a small subset of groups of predictors (e.g., the expression data of genes) that are responsible for majority of the variation in a response variable (e.g., the malignancy of a tumor). This paper focuses on group model selection in high-dimensional linear models, in which the number of predictors far exceeds the number of samples of the response variable. Existing works on high-dimensional group model selection either require the number of samples of the response variable to be significantly larger than the total number of predictors contributing to the response or impose restrictive statistical priors on the predictors and/or nonzero regression coefficients. This paper provides comprehensive understanding of a low-complexity approach to group model selection that avoids some of these limitations. The proposed approach, termed Group Thresholding (GroTh), is based on thresholding of marginal correlations of groups of predictors with the response variable and is reminiscent of existing thresholding-based approaches in the literature. The most important contribution of the paper in this regard is relating the performance of GroTh to a polynomial-time verifiable property of the predictors for the general case of arbitrary (random or deterministic) predictors and arbitrary nonzero regression coefficients. version:1
arxiv-1210-2429 | Mining Permission Request Patterns from Android and Facebook Applications (extended author version) | http://arxiv.org/abs/1210.2429 | id:1210.2429 author:Mario Frank, Ben Dong, Adrienne Porter Felt, Dawn Song category:cs.CR cs.AI stat.ML  published:2012-10-08 summary:Android and Facebook provide third-party applications with access to users' private data and the ability to perform potentially sensitive operations (e.g., post to a user's wall or place phone calls). As a security measure, these platforms restrict applications' privileges with permission systems: users must approve the permissions requested by applications before the applications can make privacy- or security-relevant API calls. However, recent studies have shown that users often do not understand permission requests and lack a notion of typicality of requests. As a first step towards simplifying permission systems, we cluster a corpus of 188,389 Android applications and 27,029 Facebook applications to find patterns in permission requests. Using a method for Boolean matrix factorization for finding overlapping clusters, we find that Facebook permission requests follow a clear structure that exhibits high stability when fitted with only five clusters, whereas Android applications demonstrate more complex permission requests. We also find that low-reputation applications often deviate from the permission request patterns that we identified for high-reputation applications suggesting that permission request patterns are indicative for user satisfaction or application quality. version:1
arxiv-1207-6231 | Touchalytics: On the Applicability of Touchscreen Input as a Behavioral Biometric for Continuous Authentication | http://arxiv.org/abs/1207.6231 | id:1207.6231 author:Mario Frank, Ralf Biedert, Eugene Ma, Ivan Martinovic, Dawn Song category:cs.CR cs.LG  published:2012-07-26 summary:We investigate whether a classifier can continuously authenticate users based on the way they interact with the touchscreen of a smart phone. We propose a set of 30 behavioral touch features that can be extracted from raw touchscreen logs and demonstrate that different users populate distinct subspaces of this feature space. In a systematic experiment designed to test how this behavioral pattern exhibits consistency over time, we collected touch data from users interacting with a smart phone using basic navigation maneuvers, i.e., up-down and left-right scrolling. We propose a classification framework that learns the touch behavior of a user during an enrollment phase and is able to accept or reject the current user by monitoring interaction with the touch screen. The classifier achieves a median equal error rate of 0% for intra-session authentication, 2%-3% for inter-session authentication and below 4% when the authentication test was carried out one week after the enrollment phase. While our experimental findings disqualify this method as a standalone authentication mechanism for long-term authentication, it could be implemented as a means to extend screen-lock time or as a part of a multi-modal biometric authentication system. version:2
arxiv-1210-2388 | Video De-fencing | http://arxiv.org/abs/1210.2388 | id:1210.2388 author:Yadong Mu, Wei Liu, Shuicheng Yan category:cs.CV cs.MM  published:2012-10-08 summary:This paper describes and provides an initial solution to a novel video editing task, i.e., video de-fencing. It targets automatic restoration of the video clips that are corrupted by fence-like occlusions during capture. Our key observation lies in the visual parallax between fences and background scenes, which is caused by the fact that the former are typically closer to the camera. Unlike in traditional image inpainting, fence-occluded pixels in the videos tend to appear later in the temporal dimension and are therefore recoverable via optimized pixel selection from relevant frames. To eventually produce fence-free videos, major challenges include cross-frame sub-pixel image alignment under diverse scene depth, and "correct" pixel selection that is robust to dominating fence pixels. Several novel tools are developed in this paper, including soft fence detection, weighted truncated optical flow method and robust temporal median filter. The proposed algorithm is validated on several real-world video clips with fences. version:1
arxiv-1210-2381 | The Power of Linear Reconstruction Attacks | http://arxiv.org/abs/1210.2381 | id:1210.2381 author:Shiva Prasad Kasiviswanathan, Mark Rudelson, Adam Smith category:cs.DS cs.CR cs.LG math.PR  published:2012-10-08 summary:We consider the power of linear reconstruction attacks in statistical data privacy, showing that they can be applied to a much wider range of settings than previously understood. Linear attacks have been studied before (Dinur and Nissim PODS'03, Dwork, McSherry and Talwar STOC'07, Kasiviswanathan, Rudelson, Smith and Ullman STOC'10, De TCC'12, Muthukrishnan and Nikolov STOC'12) but have so far been applied only in settings with releases that are obviously linear. Consider a database curator who manages a database of sensitive information but wants to release statistics about how a sensitive attribute (say, disease) in the database relates to some nonsensitive attributes (e.g., postal code, age, gender, etc). We show one can mount linear reconstruction attacks based on any release that gives: a) the fraction of records that satisfy a given non-degenerate boolean function. Such releases include contingency tables (previously studied by Kasiviswanathan et al., STOC'10) as well as more complex outputs like the error rate of classifiers such as decision trees; b) any one of a large class of M-estimators (that is, the output of empirical risk minimization algorithms), including the standard estimators for linear and logistic regression. We make two contributions: first, we show how these types of releases can be transformed into a linear format, making them amenable to existing polynomial-time reconstruction algorithms. This is already perhaps surprising, since many of the above releases (like M-estimators) are obtained by solving highly nonlinear formulations. Second, we show how to analyze the resulting attacks under various distributional assumptions on the data. Specifically, we consider a setting in which the same statistic (either a) or b) above) is released about how the sensitive attribute relates to all subsets of size k (out of a total of d) nonsensitive boolean attributes. version:1
arxiv-1206-1971 | A Connectionist Network Approach to Find Numerical Solutions of Diophantine Equations | http://arxiv.org/abs/1206.1971 | id:1206.1971 author:Siby Abraham, Sugata Sanyal, Mukund Sanglikar category:cs.NE  published:2012-06-09 summary:The paper introduces a connectionist network approach to find numerical solutions of Diophantine equations as an attempt to address the famous Hilbert's tenth problem. The proposed methodology uses a three layer feed forward neural network with back propagation as sequential learning procedure to find numerical solutions of a class of Diophantine equations. It uses a dynamically constructed network architecture where number of nodes in the input layer is chosen based on the number of variables in the equation. The powers of the given Diophantine equation are taken as input to the input layer. The training of the network starts with initial random integral weights. The weights are updated based on the back propagation of the error values at the output layer. The optimization of weights is augmented by adding a momentum factor into the network. The optimized weights of the connection between the input layer and the hidden layer are taken as numerical solution of the given Diophantine equation. The procedure is validated using different Diophantine Equations of different number of variables and different powers. version:2
arxiv-1210-2294 | Modeling Weather Conditions Consequences on Road Trafficking Behaviors | http://arxiv.org/abs/1210.2294 | id:1210.2294 author:Guillaume Allain, Fabrice Gamboa, Philippe Goudal, Jean-Noël Kien, Jean-Michel Loubes category:stat.ME stat.AP stat.ML  published:2012-10-08 summary:We provide a model to understand how adverse weather conditions modify traffic flow dynamic. We first prove that the microscopic Free Flow Speed of the vehicles is changed and then provide a rule to model this change. For this, we consider a thresholded linear model, corresponding to an application of a MARS model to road trafficking. This model adapts itself locally to the whole road network and provides accurate unbiased forecasted speed using live or short term forecasted weather data information. version:1
arxiv-1210-2289 | A Fast Distributed Proximal-Gradient Method | http://arxiv.org/abs/1210.2289 | id:1210.2289 author:Annie I. Chen, Asuman Ozdaglar category:cs.DC cs.LG stat.ML  published:2012-10-08 summary:We present a distributed proximal-gradient method for optimizing the average of convex functions, each of which is the private local objective of an agent in a network with time-varying topology. The local objectives have distinct differentiable components, but they share a common nondifferentiable component, which has a favorable structure suitable for effective computation of the proximal operator. In our method, each agent iteratively updates its estimate of the global minimum by optimizing its local objective function, and exchanging estimates with others via communication in the network. Using Nesterov-type acceleration techniques and multiple communication steps per iteration, we show that this method converges at the rate 1/k (where k is the number of communication rounds between the agents), which is faster than the convergence rate of the existing distributed methods for solving this problem. The superior convergence rate of our method is also verified by numerical experiments. version:1
arxiv-1210-0310 | Intra-Retinal Layer Segmentation of 3D Optical Coherence Tomography Using Coarse Grained Diffusion Map | http://arxiv.org/abs/1210.0310 | id:1210.0310 author:Raheleh Kafieh, Hossein Rabbani, Michael D. Abramoff, Milan Sonka category:cs.CV  published:2012-10-01 summary:Optical coherence tomography (OCT) is a powerful and noninvasive method for retinal imaging. In this paper, we introduce a fast segmentation method based on a new variant of spectral graph theory named diffusion maps. The research is performed on spectral domain (SD) OCT images depicting macular and optic nerve head appearance. The presented approach does not require edge-based image information and relies on regional image texture. Consequently, the proposed method demonstrates robustness in situations of low image contrast or poor layer-to-layer image gradients. Diffusion mapping is applied to 2D and 3D OCT datasets composed of two steps, one for partitioning the data into important and less important sections, and another one for localization of internal layers.In the first step, the pixels/voxels are grouped in rectangular/cubic sets to form a graph node.The weights of a graph are calculated based on geometric distances between pixels/voxels and differences of their mean intensity.The first diffusion map clusters the data into three parts, the second of which is the area of interest. The other two sections are eliminated from the remaining calculations. In the second step, the remaining area is subjected to another diffusion map assessment and the internal layers are localized based on their textural similarities.The proposed method was tested on 23 datasets from two patient groups (glaucoma and normals). The mean unsigned border positioning errors(mean - SD) was 8.52 - 3.13 and 7.56 - 2.95 micrometer for the 2D and 3D methods, respectively. version:2
arxiv-1210-2162 | Semisupervised Classifier Evaluation and Recalibration | http://arxiv.org/abs/1210.2162 | id:1210.2162 author:Peter Welinder, Max Welling, Pietro Perona category:cs.LG cs.CV  published:2012-10-08 summary:How many labeled examples are needed to estimate a classifier's performance on a new dataset? We study the case where data is plentiful, but labels are expensive. We show that by making a few reasonable assumptions on the structure of the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels. Our approach, which we call Semisupervised Performance Evaluation (SPE), is based on a generative model for the classifier's confidence scores. In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by re-estimating the class-conditional confidence distributions. version:1
arxiv-1210-4481 | Epitome for Automatic Image Colorization | http://arxiv.org/abs/1210.4481 | id:1210.4481 author:Yingzhen Yang, Xinqi Chu, Tian-Tsong Ng, Alex Yong-Sang Chia, Shuicheng Yan, Thomas S. Huang category:cs.CV cs.LG cs.MM  published:2012-10-08 summary:Image colorization adds color to grayscale images. It not only increases the visual appeal of grayscale images, but also enriches the information contained in scientific images that lack color information. Most existing methods of colorization require laborious user interaction for scribbles or image segmentation. To eliminate the need for human labor, we develop an automatic image colorization method using epitome. Built upon a generative graphical model, epitome is a condensed image appearance and shape model which also proves to be an effective summary of color information for the colorization task. We train the epitome from the reference images and perform inference in the epitome to colorize grayscale images, rendering better colorization results than previous method in our experiments. version:1
arxiv-1202-4050 | On the Sample Complexity of Predictive Sparse Coding | http://arxiv.org/abs/1202.4050 | id:1202.4050 author:Nishant A. Mehta, Alexander G. Gray category:cs.LG stat.ML  published:2012-02-18 summary:The goal of predictive sparse coding is to learn a representation of examples as sparse linear combinations of elements from a dictionary, such that a learned hypothesis linear in the new representation performs well on a predictive task. Predictive sparse coding algorithms recently have demonstrated impressive performance on a variety of supervised tasks, but their generalization properties have not been studied. We establish the first generalization error bounds for predictive sparse coding, covering two settings: 1) the overcomplete setting, where the number of features k exceeds the original dimensionality d; and 2) the high or infinite-dimensional setting, where only dimension-free bounds are useful. Both learning bounds intimately depend on stability properties of the learned sparse encoder, as measured on the training sample. Consequently, we first present a fundamental stability result for the LASSO, a result characterizing the stability of the sparse codes with respect to perturbations to the dictionary. In the overcomplete setting, we present an estimation error bound that decays as \tilde{O}(sqrt(d k/m)) with respect to d and k. In the high or infinite-dimensional setting, we show a dimension-free bound that is \tilde{O}(sqrt(k^2 s / m)) with respect to k and s, where s is an upper bound on the number of non-zeros in the sparse code for any training data point. version:2
arxiv-1210-2077 | Sparsity by Worst-Case Quadratic Penalties | http://arxiv.org/abs/1210.2077 | id:1210.2077 author:Yves Grandvalet, Julien Chiquet, Christophe Ambroise category:stat.ML stat.CO  published:2012-10-07 summary:This paper proposes a new robust regression interpretation of sparse penalties such as the elastic net and the group-lasso. Beyond providing a new viewpoint on these penalization schemes, our approach results in a unified optimization strategy. Our evaluation experiments demonstrate that this strategy, implemented on the elastic net, is computationally extremely efficient for small to medium size problems. Our accompanying software solves problems at machine precision in the time required to get a rough estimate with competing state-of-the-art algorithms. version:1
arxiv-1210-1983 | Reply to Comments on Neuroelectrodynamics: Where are the Real Conceptual Pitfalls? | http://arxiv.org/abs/1210.1983 | id:1210.1983 author:Dorian Aur category:cs.NE nlin.AO physics.bio-ph q-bio.NC  published:2012-10-06 summary:The fundamental, powerful process of computation in the brain has been widely misunderstood. The paper [1] associates the general failure to build intelligent thinking machines with current reductionist principles of temporal coding and advocates for a change in paradigm regarding the brain analogy. Since fragments of information are stored in proteins which can shift between several structures to perform their function, the biological substrate is actively involved in physical computation. The intrinsic nonlinear dynamics of action potentials and synaptic activities maintain physical interactions within and between neurons in the brain. During these events the required information is exchanged between molecular structures (proteins) which store fragments of information and the generated electric flux which carries and integrates information in the brain. The entire process of physical interaction explains how the brain actively creates or experiences meaning. This process of interaction during an action potential generation can be simply seen as the moment when the neuron solves a many-body problem. A neuroelectrodynamic theory shows that the neuron solves equations rather than exclusively computes functions. With the main focus on temporal patterns, the spike timing dogma (STD) has neglected important forms of computation which do occur inside neurons. In addition, artificial neural models have missed the most important part since the real super-computing power of the brain has its origins in computations that occur within neurons. version:1
arxiv-1210-1960 | Feature Selection via L1-Penalized Squared-Loss Mutual Information | http://arxiv.org/abs/1210.1960 | id:1210.1960 author:Wittawat Jitkrittum, Hirotaka Hachiya, Masashi Sugiyama category:stat.ML cs.LG  published:2012-10-06 summary:Feature selection is a technique to screen out less important features. Many existing supervised feature selection algorithms use redundancy and relevancy as the main criteria to select features. However, feature interaction, potentially a key characteristic in real-world problems, has not received much attention. As an attempt to take feature interaction into account, we propose L1-LSMI, an L1-regularization based algorithm that maximizes a squared-loss variant of mutual information between selected features and outputs. Numerical results show that L1-LSMI performs well in handling redundancy, detecting non-linear dependency, and considering feature interaction. version:1
arxiv-1210-1916 | A comparative study on face recognition techniques and neural network | http://arxiv.org/abs/1210.1916 | id:1210.1916 author:Meftah Ur Rahman category:cs.CV  published:2012-10-06 summary:In modern times, face recognition has become one of the key aspects of computer vision. There are at least two reasons for this trend; the first is the commercial and law enforcement applications, and the second is the availability of feasible technologies after years of research. Due to the very nature of the problem, computer scientists, neuro-scientists and psychologists all share a keen interest in this field. In plain words, it is a computer application for automatically identifying a person from a still image or video frame. One of the ways to accomplish this is by comparing selected features from the image and a facial database. There are hundreds if not thousand factors associated with this. In this paper some of the most common techniques available including applications of neural network in facial recognition are studied and compared with respect to their performance. version:1
arxiv-1207-3554 | Designing various component analysis at will | http://arxiv.org/abs/1207.3554 | id:1207.3554 author:Akisato Kimura, Masashi Sugiyama, Sakano Hitoshi, Hirokazu Kameoka category:cs.CV cs.NA stat.ME stat.ML  published:2012-07-16 summary:This paper provides a generic framework of component analysis (CA) methods introducing a new expression for scatter matrices and Gram matrices, called Generalized Pairwise Expression (GPE). This expression is quite compact but highly powerful: The framework includes not only (1) the standard CA methods but also (2) several regularization techniques, (3) weighted extensions, (4) some clustering methods, and (5) their semi-supervised extensions. This paper also presents quite a simple methodology for designing a desired CA method from the proposed framework: Adopting the known GPEs as templates, and generating a new method by combining these templates appropriately. version:2
arxiv-1111-6085 | Automatic Relevance Determination in Nonnegative Matrix Factorization with the β-Divergence | http://arxiv.org/abs/1111.6085 | id:1111.6085 author:Vincent Y. F. Tan, Cédric Févotte category:stat.ML stat.ME  published:2011-11-25 summary:This paper addresses the estimation of the latent dimensionality in nonnegative matrix factorization (NMF) with the \beta-divergence. The \beta-divergence is a family of cost functions that includes the squared Euclidean distance, Kullback-Leibler and Itakura-Saito divergences as special cases. Learning the model order is important as it is necessary to strike the right balance between data fidelity and overfitting. We propose a Bayesian model based on automatic relevance determination in which the columns of the dictionary matrix and the rows of the activation matrix are tied together through a common scale parameter in their prior. A family of majorization-minimization algorithms is proposed for maximum a posteriori (MAP) estimation. A subset of scale parameters is driven to a small lower bound in the course of inference, with the effect of pruning the corresponding spurious components. We demonstrate the efficacy and robustness of our algorithms by performing extensive experiments on synthetic data, the swimmer dataset, a music decomposition example and a stock price prediction task. version:3
arxiv-1205-2172 | Modularity-Based Clustering for Network-Constrained Trajectories | http://arxiv.org/abs/1205.2172 | id:1205.2172 author:Mohamed Khalil El Mahrsi, Fabrice Rossi category:stat.ML cs.LG physics.data-an  published:2012-05-10 summary:We present a novel clustering approach for moving object trajectories that are constrained by an underlying road network. The approach builds a similarity graph based on these trajectories then uses modularity-optimization hiearchical graph clustering to regroup trajectories with similar profiles. Our experimental study shows the superiority of the proposed approach over classic hierarchical clustering and gives a brief insight to visualization of the clustering results. version:2
arxiv-1210-1530 | A network of spiking neurons for computing sparse representations in an energy efficient way | http://arxiv.org/abs/1210.1530 | id:1210.1530 author:Tao Hu, Alexander Genkin, Dmitri B. Chklovskii category:cs.NE q-bio.NC  published:2012-10-04 summary:Computing sparse redundant representations is an important problem both in applied mathematics and neuroscience. In many applications, this problem must be solved in an energy efficient way. Here, we propose a hybrid distributed algorithm (HDA), which solves this problem on a network of simple nodes communicating via low-bandwidth channels. HDA nodes perform both gradient-descent-like steps on analog internal variables and coordinate-descent-like steps via quantized external variables communicated to each other. Interestingly, such operation is equivalent to a network of integrate-and-fire neurons, suggesting that HDA may serve as a model of neural computation. We show that the numerical performance of HDA is on par with existing algorithms. In the asymptotic regime the representation error of HDA decays with time, t, as 1/t. HDA is stable against time-varying noise, specifically, the representation error decays as 1/sqrt(t) for Gaussian white noise. version:1
arxiv-1210-1461 | A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound | http://arxiv.org/abs/1210.1461 | id:1210.1461 author:Shusen Wang, Zhihua Zhang, Jian Li category:cs.LG cs.DM stat.ML  published:2012-10-04 summary:The CUR matrix decomposition is an important extension of Nystr\"{o}m approximation to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR algorithm with an expected relative-error bound. The proposed algorithm has the advantages over the existing relative-error CUR algorithms that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory. Finally, experiments on several real-world datasets demonstrate significant improvement over the existing relative-error algorithms. version:1
arxiv-1107-3090 | On the Computational Complexity of Stochastic Controller Optimization in POMDPs | http://arxiv.org/abs/1107.3090 | id:1107.3090 author:Nikos Vlassis, Michael L. Littman, David Barber category:cs.CC cs.LG cs.SY math.OC F.2.1  published:2011-07-15 summary:We show that the problem of finding an optimal stochastic 'blind' controller in a Markov decision process is an NP-hard problem. The corresponding decision problem is NP-hard, in PSPACE, and SQRT-SUM-hard, hence placing it in NP would imply breakthroughs in long-standing open problems in computer science. Our result establishes that the more general problem of stochastic controller optimization in POMDPs is also NP-hard. Nonetheless, we outline a special case that is convex and admits efficient global solutions. version:2
arxiv-1210-1317 | Learning Heterogeneous Similarity Measures for Hybrid-Recommendations in Meta-Mining | http://arxiv.org/abs/1210.1317 | id:1210.1317 author:Phong Nguyen, Jun Wang, Melanie Hilario, Alexandros Kalousis category:cs.LG cs.AI  published:2012-10-04 summary:The notion of meta-mining has appeared recently and extends the traditional meta-learning in two ways. First it does not learn meta-models that provide support only for the learning algorithm selection task but ones that support the whole data-mining process. In addition it abandons the so called black-box approach to algorithm description followed in meta-learning. Now in addition to the datasets, algorithms also have descriptors, workflows as well. For the latter two these descriptions are semantic, describing properties of the algorithms. With the availability of descriptors both for datasets and data mining workflows the traditional modelling techniques followed in meta-learning, typically based on classification and regression algorithms, are no longer appropriate. Instead we are faced with a problem the nature of which is much more similar to the problems that appear in recommendation systems. The most important meta-mining requirements are that suggestions should use only datasets and workflows descriptors and the cold-start problem, e.g. providing workflow suggestions for new datasets. In this paper we take a different view on the meta-mining modelling problem and treat it as a recommender problem. In order to account for the meta-mining specificities we derive a novel metric-based-learning recommender approach. Our method learns two homogeneous metrics, one in the dataset and one in the workflow space, and a heterogeneous one in the dataset-workflow space. All learned metrics reflect similarities established from the dataset-workflow preference matrix. We demonstrate our method on meta-mining over biological (microarray datasets) problems. The application of our method is not limited to the meta-mining problem, its formulations is general enough so that it can be applied on problems with similar requirements. version:1
arxiv-1205-2265 | Efficient Constrained Regret Minimization | http://arxiv.org/abs/1205.2265 | id:1205.2265 author:Mehrdad Mahdavi, Tianbao Yang, Rong Jin category:cs.LG  published:2012-05-08 summary:Online learning constitutes a mathematical and compelling framework to analyze sequential decision making problems in adversarial environments. The learner repeatedly chooses an action, the environment responds with an outcome, and then the learner receives a reward for the played action. The goal of the learner is to maximize his total reward. However, there are situations in which, in addition to maximizing the cumulative reward, there are some additional constraints on the sequence of decisions that must be satisfied on average by the learner. In this paper we study an extension to the online learning where the learner aims to maximize the total reward given that some additional constraints need to be satisfied. By leveraging on the theory of Lagrangian method in constrained optimization, we propose Lagrangian exponentially weighted average (LEWA) algorithm, which is a primal-dual variant of the well known exponentially weighted average algorithm, to efficiently solve constrained online decision making problems. Using novel theoretical analysis, we establish the regret and the violation of the constraint bounds in full information and bandit feedback models. version:2
arxiv-1210-1258 | Unfolding Latent Tree Structures using 4th Order Tensors | http://arxiv.org/abs/1210.1258 | id:1210.1258 author:Mariya Ishteva, Haesun Park, Le Song category:cs.LG stat.ML  published:2012-10-03 summary:Discovering the latent structure from many observed variables is an important yet challenging learning task. Existing approaches for discovering latent structures often require the unknown number of hidden states as an input. In this paper, we propose a quartet based approach which is \emph{agnostic} to this number. The key contribution is a novel rank characterization of the tensor associated with the marginal distribution of a quartet. This characterization allows us to design a \emph{nuclear norm} based test for resolving quartet relations. We then use the quartet test as a subroutine in a divide-and-conquer algorithm for recovering the latent tree structure. Under mild conditions, the algorithm is consistent and its error probability decays exponentially with increasing sample size. We demonstrate that the proposed approach compares favorably to alternatives. In a real world stock dataset, it also discovers meaningful groupings of variables, and produces a model that fits the data better. version:1
arxiv-1210-1230 | Evaluating Discussion Boards on BlackBoard as a Collaborative Learning Tool A Students Survey and Reflections | http://arxiv.org/abs/1210.1230 | id:1210.1230 author:AbdelHameed A. Badawy, Michelle M. Hugue category:cs.CV cs.CY  published:2012-10-03 summary:In this paper, we investigate how the students think of their experience in a junior level course that has a blackboard course presence where the students use the discussion boards extensively. A survey is set up through blackboard as a voluntary quiz and the student who participated were given a freebie point. The results and the participation were very interesting in terms of the feedback we got via open comments from the students as well as the statistics we gathered from the answers to the questions. The students have shown understanding and willingness to participate in pedagogy-enhancing endeavors. version:1
arxiv-1210-1190 | Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization | http://arxiv.org/abs/1210.1190 | id:1210.1190 author:Abhishek Kumar, Vikas Sindhwani, Prabhanjan Kambadur category:stat.ML cs.LG  published:2012-10-03 summary:The separability assumption (Donoho & Stodden, 2003; Arora et al., 2012) turns non-negative matrix factorization (NMF) into a tractable problem. Recently, a new class of provably-correct NMF algorithms have emerged under this assumption. In this paper, we reformulate the separable NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. From this geometric perspective, we derive new separable NMF algorithms that are highly scalable and empirically noise robust, and have several other favorable properties in relation to existing methods. A parallel implementation of our algorithm demonstrates high scalability on shared- and distributed-memory machines. version:1
arxiv-1210-1121 | Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations | http://arxiv.org/abs/1210.1121 | id:1210.1121 author:Krishnakumar Balasubramanian, Kai Yu, Guy Lebanon category:stat.ML cs.LG  published:2012-10-03 summary:We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via non-parametric kernel smoothing. We provide generalization bounds for dictionary learning using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach could be used for improving semi-supervised sparse coding. version:1
arxiv-1210-1104 | Sensory Anticipation of Optical Flow in Mobile Robotics | http://arxiv.org/abs/1210.1104 | id:1210.1104 author:Arturo Ribes, Jesús Cerquides, Yiannis Demiris, Ramón López de Mántaras category:cs.RO cs.LG  published:2012-10-03 summary:In order to anticipate dangerous events, like a collision, an agent needs to make long-term predictions. However, those are challenging due to uncertainties in internal and external variables and environment dynamics. A sensorimotor model is acquired online by the mobile robot using a state-of-the-art method that learns the optical flow distribution in images, both in space and time. The learnt model is used to anticipate the optical flow up to a given time horizon and to predict an imminent collision by using reinforcement learning. We demonstrate that multi-modal predictions reduce to simpler distributions once actions are taken into account. version:1
arxiv-1210-1048 | Predicting human preferences using the block structure of complex social networks | http://arxiv.org/abs/1210.1048 | id:1210.1048 author:Roger Guimera, Alejandro Llorente, Esteban Moro, Marta Sales-Pardo category:physics.soc-ph cs.SI physics.data-an stat.ML  published:2012-10-03 summary:With ever-increasing available data, predicting individuals' preferences and helping them locate the most relevant information has become a pressing need. Understanding and predicting preferences is also important from a fundamental point of view, as part of what has been called a "new" computational social science. Here, we propose a novel approach based on stochastic block models, which have been developed by sociologists as plausible models of complex networks of social interactions. Our model is in the spirit of predicting individuals' preferences based on the preferences of others but, rather than fitting a particular model, we rely on a Bayesian approach that samples over the ensemble of all possible models. We show that our approach is considerably more accurate than leading recommender algorithms, with major relative improvements between 38% and 99% over industry-level algorithms. Besides, our approach sheds light on decision-making processes by identifying groups of individuals that have consistently similar preferences, and enabling the analysis of the characteristics of those groups. version:1
arxiv-1210-1033 | Robust Degraded Face Recognition Using Enhanced Local Frequency Descriptor and Multi-scale Competition | http://arxiv.org/abs/1210.1033 | id:1210.1033 author:Guangling Sun, Guoqing Li, Xinpeng Zhang category:cs.CV  published:2012-10-03 summary:Recognizing degraded faces from low resolution and blurred images are common yet challenging task. Local Frequency Descriptor (LFD) has been proved to be effective for this task yet it is extracted from a spatial neighborhood of a pixel of a frequency plane independently regardless of correlations between frequencies. In addition, it uses a fixed window size named single scale of short-term Frequency transform (STFT). To explore the frequency correlations and preserve low resolution and blur insensitive simultaneously, we propose Enhanced LFD in which information in space and frequency is jointly utilized so as to be more descriptive and discriminative than LFD. The multi-scale competition strategy that extracts multiple descriptors corresponding to multiple window sizes of STFT and take one corresponding to maximum confidence as the final recognition result. The experiments conducted on Yale and FERET databases demonstrate that promising results have been achieved by the proposed Enhanced LFD and multi-scale competition strategy. version:1
arxiv-1210-1029 | Blurred Image Classification based on Adaptive Dictionary | http://arxiv.org/abs/1210.1029 | id:1210.1029 author:Guangling Sun, Guoqing Li, Jie Yin category:cs.CV  published:2012-10-03 summary:Two types of framework for blurred image classification based on adaptive dictionary are proposed. Given a blurred image, instead of image deblurring, the semantic category of the image is determined by blur insensitive sparse coefficients calculated depending on an adaptive dictionary. The dictionary is adaptive to the Point Spread Function (PSF) estimated from input blurred image. The PSF is assumed to be space invariant and inferred separately in one framework or updated combining with sparse coefficients calculation in an alternative and iterative algorithm in the other framework. The experiment has evaluated three types of blur, naming defocus blur, simple motion blur and camera shake blur. The experiment results confirm the effectiveness of the proposed frameworks. version:1
arxiv-1210-0999 | Logical segmentation for article extraction in digitized old newspapers | http://arxiv.org/abs/1210.0999 | id:1210.0999 author:Thomas Palfray, David Hébert, Stéphane Nicolas, Pierrick Tranouez, Thierry Paquet category:cs.IR cs.CV cs.DL  published:2012-10-03 summary:Newspapers are documents made of news item and informative articles. They are not meant to be red iteratively: the reader can pick his items in any order he fancies. Ignoring this structural property, most digitized newspaper archives only offer access by issue or at best by page to their content. We have built a digitization workflow that automatically extracts newspaper articles from images, which allows indexing and retrieval of information at the article level. Our back-end system extracts the logical structure of the page to produce the informative units: the articles. Each image is labelled at the pixel level, through a machine learning based method, then the page logical structure is constructed up from there by the detection of structuring entities such as horizontal and vertical separators, titles and text lines. This logical structure is stored in a METS wrapper associated to the ALTO file produced by the system including the OCRed text. Our front-end system provides a web high definition visualisation of images, textual indexing and retrieval facilities, searching and reading at the article level. Articles transcriptions can be collaboratively corrected, which as a consequence allows for better indexing. We are currently testing our system on the archives of the Journal de Rouen, one of France eldest local newspaper. These 250 years of publication amount to 300 000 pages of very variable image quality and layout complexity. Test year 1808 can be consulted at plair.univ-rouen.fr. version:1
arxiv-1210-0386 | Combined Descriptors in Spatial Pyramid Domain for Image Classification | http://arxiv.org/abs/1210.0386 | id:1210.0386 author:Junlin Hu, Ping Guo category:cs.CV I.4.9; I.5.4  published:2012-10-01 summary:Recently spatial pyramid matching (SPM) with scale invariant feature transform (SIFT) descriptor has been successfully used in image classification. Unfortunately, the codebook generation and feature quantization procedures using SIFT feature have the high complexity both in time and space. To address this problem, in this paper, we propose an approach which combines local binary patterns (LBP) and three-patch local binary patterns (TPLBP) in spatial pyramid domain. The proposed method does not need to learn the codebook and feature quantization processing, hence it becomes very efficient. Experiments on two popular benchmark datasets demonstrate that the proposed method always significantly outperforms the very popular SPM based SIFT descriptor method both in time and classification accuracy. version:3
arxiv-1210-0954 | Learning from Collective Intelligence in Groups | http://arxiv.org/abs/1210.0954 | id:1210.0954 author:Guo-Jun Qi, Charu Aggarwal, Pierre Moulin, Thomas Huang category:cs.SI cs.LG  published:2012-10-03 summary:Collective intelligence, which aggregates the shared information from large crowds, is often negatively impacted by unreliable information sources with the low quality data. This becomes a barrier to the effective use of collective intelligence in a variety of applications. In order to address this issue, we propose a probabilistic model to jointly assess the reliability of sources and find the true data. We observe that different sources are often not independent of each other. Instead, sources are prone to be mutually influenced, which makes them dependent when sharing information with each other. High dependency between sources makes collective intelligence vulnerable to the overuse of redundant (and possibly incorrect) information from the dependent sources. Thus, we reveal the latent group structure among dependent sources, and aggregate the information at the group level rather than from individual sources directly. This can prevent the collective intelligence from being inappropriately dominated by dependent sources. We will also explicitly reveal the reliability of groups, and minimize the negative impacts of unreliable groups. Experimental results on real-world data sets show the effectiveness of the proposed approach with respect to existing algorithms. version:1
arxiv-1210-0880 | Schrödinger Diffusion for Shape Analysis with Texture | http://arxiv.org/abs/1210.0880 | id:1210.0880 author:Jose A. Iglesias, Ron Kimmel category:cs.CV cs.CG cs.GR math.AP 68U05  35K08  published:2012-10-02 summary:In recent years, quantities derived from the heat equation have become popular in shape processing and analysis of triangulated surfaces. Such measures are often robust with respect to different kinds of perturbations, including near-isometries, topological noise and partialities. Here, we propose to exploit the semigroup of a Schr\"{o}dinger operator in order to deal with texture data, while maintaining the desirable properties of the heat kernel. We define a family of Schr\"{o}dinger diffusion distances analogous to the ones associated to the heat kernels, and show that they are continuous under perturbations of the data. As an application, we introduce a method for retrieval of textured shapes through comparison of Schr\"{o}dinger diffusion distance histograms with the earth's mover distance, and present some numerical experiments showing superior performance compared to an analogous method that ignores the texture. version:1
arxiv-1210-0866 | Classification of Hepatic Lesions using the Matching Metric | http://arxiv.org/abs/1210.0866 | id:1210.0866 author:Aaron Adcock, Daniel Rubin, Gunnar Carlsson category:cs.CV cs.CG math.AT  published:2012-10-02 summary:In this paper we present a methodology of classifying hepatic (liver) lesions using multidimensional persistent homology, the matching metric (also called the bottleneck distance), and a support vector machine. We present our classification results on a dataset of 132 lesions that have been outlined and annotated by radiologists. We find that topological features are useful in the classification of hepatic lesions. We also find that two-dimensional persistent homology outperforms one-dimensional persistent homology in this application. version:1
arxiv-1210-0864 | Learning mixtures of structured distributions over discrete domains | http://arxiv.org/abs/1210.0864 | id:1210.0864 author:Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun category:cs.LG cs.DS math.ST stat.TH  published:2012-10-02 summary:Let $\mathfrak{C}$ be a class of probability distributions over the discrete domain $[n] = \{1,...,n\}.$ We show that if $\mathfrak{C}$ satisfies a rather general condition -- essentially, that each distribution in $\mathfrak{C}$ can be well-approximated by a variable-width histogram with few bins -- then there is a highly efficient (both in terms of running time and sample complexity) algorithm that can learn any mixture of $k$ unknown distributions from $\mathfrak{C}.$ We analyze several natural types of distributions over $[n]$, including log-concave, monotone hazard rate and unimodal distributions, and show that they have the required structural property of being well-approximated by a histogram with few bins. Applying our general algorithm, we obtain near-optimally efficient algorithms for all these mixture learning problems. version:1
arxiv-1210-0852 | Detecting multiword phrases in mathematical text corpora | http://arxiv.org/abs/1210.0852 | id:1210.0852 author:Winfried Gödert category:cs.CL cs.IR 68P20 H.3.1; I.7.3  published:2012-10-02 summary:We present an approach for detecting multiword phrases in mathematical text corpora. The method used is based on characteristic features of mathematical terminology. It makes use of a software tool named Lingo which allows to identify words by means of previously defined dictionaries for specific word classes as adjectives, personal names or nouns. The detection of multiword groups is done algorithmically. Possible advantages of the method for indexing and information retrieval and conclusions for applying dictionary-based methods of automatic indexing instead of stemming procedures are discussed. version:1
arxiv-1210-0848 | Enhancing Twitter Data Analysis with Simple Semantic Filtering: Example in Tracking Influenza-Like Illnesses | http://arxiv.org/abs/1210.0848 | id:1210.0848 author:Son Doan, Lucila Ohno-Machado, Nigel Collier category:cs.SI cs.CL physics.soc-ph  published:2012-10-02 summary:Systems that exploit publicly available user generated content such as Twitter messages have been successful in tracking seasonal influenza. We developed a novel filtering method for Influenza-Like-Illnesses (ILI)-related messages using 587 million messages from Twitter micro-blogs. We first filtered messages based on syndrome keywords from the BioCaster Ontology, an extant knowledge model of laymen's terms. We then filtered the messages according to semantic features such as negation, hashtags, emoticons, humor and geography. The data covered 36 weeks for the US 2009 influenza season from 30th August 2009 to 8th May 2010. Results showed that our system achieved the highest Pearson correlation coefficient of 98.46% (p-value<2.2e-16), an improvement of 3.98% over the previous state-of-the-art method. The results indicate that simple NLP-based enhancements to existing approaches to mine Twitter data can increase the value of this inexpensive resource. version:1
arxiv-1210-0829 | A Survey of Multibiometric Systems | http://arxiv.org/abs/1210.0829 | id:1210.0829 author:Harbi AlMahafzah, Maen Zaid AlRwashdeh category:cs.CV  published:2012-10-02 summary:Most biometric systems deployed in real-world applications are unimodal. Using unimodal biometric systems have to contend with a variety of problems such as: Noise in sensed data; Intra-class variations; Inter-class similarities; Non-universality; Spoof attacks. These problems have addressed by using multibiometric systems, which expected to be more reliable due to the presence of multiple, independent pieces of evidence. version:1
arxiv-1210-0824 | Distributed High Dimensional Information Theoretical Image Registration via Random Projections | http://arxiv.org/abs/1210.0824 | id:1210.0824 author:Zoltan Szabo, Andras Lorincz category:cs.IT cs.LG math.IT stat.ML E.4; H.1.1; G.1.0  published:2012-10-02 summary:Information theoretical measures, such as entropy, mutual information, and various divergences, exhibit robust characteristics in image registration applications. However, the estimation of these quantities is computationally intensive in high dimensions. On the other hand, consistent estimation from pairwise distances of the sample points is possible, which suits random projection (RP) based low dimensional embeddings. We adapt the RP technique to this task by means of a simple ensemble method. To the best of our knowledge, this is the first distributed, RP based information theoretical image registration approach. The efficiency of the method is demonstrated through numerical examples. version:1
arxiv-1210-0822 | Discrete geodesic calculus in the space of viscous fluidic objects | http://arxiv.org/abs/1210.0822 | id:1210.0822 author:Martin Rumpf, Benedikt Wirth category:math.NA cs.CV  published:2012-10-02 summary:Based on a local approximation of the Riemannian distance on a manifold by a computationally cheap dissimilarity measure, a time discrete geodesic calculus is developed, and applications to shape space are explored. The dissimilarity measure is derived from a deformation energy whose Hessian reproduces the underlying Riemannian metric, and it is used to define length and energy of discrete paths in shape space. The notion of discrete geodesics defined as energy minimizing paths gives rise to a discrete logarithmic map, a variational definition of a discrete exponential map, and a time discrete parallel transport. This new concept is applied to a shape space in which shapes are considered as boundary contours of physical objects consisting of viscous material. The flexibility and computational efficiency of the approach is demonstrated for topology preserving shape morphing, the representation of paths in shape space via local shape variations as path generators, shape extrapolation via discrete geodesic flow, and the transfer of geometric features. version:1
arxiv-1210-0818 | Multibiometric: Feature Level Fusion Using FKP Multi-Instance biometric | http://arxiv.org/abs/1210.0818 | id:1210.0818 author:Harbi AlMahafzah, Mohammad Imran, H. S. Sheshadri category:cs.CV  published:2012-10-02 summary:This paper proposed the use of multi-instance feature level fusion as a means to improve the performance of Finger Knuckle Print (FKP) verification. A log-Gabor filter has been used to extract the image local orientation information, and represent the FKP features. Experiments are performed using the FKP database, which consists of 7,920 images. Results indicate that the multi-instance verification approach outperforms higher performance than using any single instance. The influence on biometric performance using feature level fusion under different fusion rules have been demonstrated in this paper. version:1
arxiv-1210-0794 | A Semantic Approach for Automatic Structuring and Analysis of Software Process Patterns | http://arxiv.org/abs/1210.0794 | id:1210.0794 author:Nahla Jlaiel, Khouloud Madhbouh, Mohamed Ben Ahmed category:cs.AI cs.CL  published:2012-10-02 summary:The main contribution of this paper, is to propose a novel semantic approach based on a Natural Language Processing technique in order to ensure a semantic unification of unstructured process patterns which are expressed not only in different formats but also, in different forms. This approach is implemented using the GATE text engineering framework and then evaluated leading up to high-quality results motivating us to continue in this direction. version:1
arxiv-1210-0762 | Graph-Based Approaches to Clustering Network-Constrained Trajectory Data | http://arxiv.org/abs/1210.0762 | id:1210.0762 author:Mohamed Khalil El Mahrsi, Fabrice Rossi category:cs.LG stat.ML  published:2012-10-02 summary:Even though clustering trajectory data attracted considerable attention in the last few years, most of prior work assumed that moving objects can move freely in an euclidean space and did not consider the eventual presence of an underlying road network and its influence on evaluating the similarity between trajectories. In this paper, we present two approaches to clustering network-constrained trajectory data. The first approach discovers clusters of trajectories that traveled along the same parts of the road network. The second approach is segment-oriented and aims to group together road segments based on trajectories that they have in common. Both approaches use a graph model to depict the interactions between observations w.r.t. their similarity and cluster this similarity graph using a community detection algorithm. We also present experimental results obtained on synthetic data to showcase our propositions. version:1
arxiv-1210-0758 | A fast compression-based similarity measure with applications to content-based image retrieval | http://arxiv.org/abs/1210.0758 | id:1210.0758 author:Daniele Cerra, Mihai Datcu category:stat.ML cs.IR cs.LG  published:2012-10-02 summary:Compression-based similarity measures are effectively employed in applications on diverse data types with a basically parameter-free approach. Nevertheless, there are problems in applying these techniques to medium-to-large datasets which have been seldom addressed. This paper proposes a similarity measure based on compression with dictionaries, the Fast Compression Distance (FCD), which reduces the complexity of these methods, without degradations in performance. On its basis a content-based color image retrieval system is defined, which can be compared to state-of-the-art methods based on invariant color features. Through the FCD a better understanding of compression-based techniques is achieved, by performing experiments on datasets which are larger than the ones analyzed so far in literature. version:1
arxiv-1210-0754 | Invariance of visual operations at the level of receptive fields | http://arxiv.org/abs/1210.0754 | id:1210.0754 author:Tony Lindeberg category:q-bio.NC cs.CV  published:2012-10-02 summary:Receptive field profiles registered by cell recordings have shown that mammalian vision has developed receptive fields tuned to different sizes and orientations in the image domain as well as to different image velocities in space-time. This article presents a theoretical model by which families of idealized receptive field profiles can be derived mathematically from a small set of basic assumptions that correspond to structural properties of the environment. The article also presents a theory for how basic invariance properties to variations in scale, viewing direction and relative motion can be obtained from the output of such receptive fields, using complementary selection mechanisms that operate over the output of families of receptive fields tuned to different parameters. Thereby, the theory shows how basic invariance properties of a visual system can be obtained already at the level of receptive fields, and we can explain the different shapes of receptive field profiles found in biological vision from a requirement that the visual system should be invariant to the natural types of image transformations that occur in its environment. version:1
arxiv-1210-0734 | Evaluation of linear classifiers on articles containing pharmacokinetic evidence of drug-drug interactions | http://arxiv.org/abs/1210.0734 | id:1210.0734 author:Artemy Kolchinsky, Anália Lourenço, Lang Li, Luis M. Rocha category:stat.ML cs.LG q-bio.QM H.2.8; H.3.1; J.3  published:2012-10-02 summary:Background. Drug-drug interaction (DDI) is a major cause of morbidity and mortality. [...] Biomedical literature mining can aid DDI research by extracting relevant DDI signals from either the published literature or large clinical databases. However, though drug interaction is an ideal area for translational research, the inclusion of literature mining methodologies in DDI workflows is still very preliminary. One area that can benefit from literature mining is the automatic identification of a large number of potential DDIs, whose pharmacological mechanisms and clinical significance can then be studied via in vitro pharmacology and in populo pharmaco-epidemiology. Experiments. We implemented a set of classifiers for identifying published articles relevant to experimental pharmacokinetic DDI evidence. These documents are important for identifying causal mechanisms behind putative drug-drug interactions, an important step in the extraction of large numbers of potential DDIs. We evaluate performance of several linear classifiers on PubMed abstracts, under different feature transformation and dimensionality reduction methods. In addition, we investigate the performance benefits of including various publicly-available named entity recognition features, as well as a set of internally-developed pharmacokinetic dictionaries. Results. We found that several classifiers performed well in distinguishing relevant and irrelevant abstracts. We found that the combination of unigram and bigram textual features gave better performance than unigram features alone, and also that normalization transforms that adjusted for feature frequency and document length improved classification. For some classifiers, such as linear discriminant analysis (LDA), proper dimensionality reduction had a large impact on performance. Finally, the inclusion of NER features and dictionaries was found not to help classification. version:1
arxiv-1210-0699 | TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data Classification | http://arxiv.org/abs/1210.0699 | id:1210.0699 author:Xavier Bresson, Ruiliang Zhang category:cs.LG  published:2012-10-02 summary:We introduce semi-supervised data classification algorithms based on total variation (TV), Reproducing Kernel Hilbert Space (RKHS), support vector machine (SVM), Cheeger cut, labeled and unlabeled data points. We design binary and multi-class semi-supervised classification algorithms. We compare the TV-based classification algorithms with the related Laplacian-based algorithms, and show that TV classification perform significantly better when the number of labeled data is small. version:1
arxiv-1210-0685 | Local stability and robustness of sparse dictionary learning in the presence of noise | http://arxiv.org/abs/1210.0685 | id:1210.0685 author:Rodolphe Jenatton, Rémi Gribonval, Francis Bach category:stat.ML cs.LG  published:2012-10-02 summary:A popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. While this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. In particular, sparse coding, or sparse dictionary learning, relies on a non-convex procedure whose local minima have not been fully analyzed yet. In this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. Our study takes into account the case of over-complete dictionaries and noisy signals, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. The analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations. version:1
arxiv-1210-0564 | Super-resolution using Sparse Representations over Learned Dictionaries: Reconstruction of Brain Structure using Electron Microscopy | http://arxiv.org/abs/1210.0564 | id:1210.0564 author:Tao Hu, Juan Nunez-Iglesias, Shiv Vitaladevuni, Lou Scheffer, Shan Xu, Mehdi Bolorizadeh, Harald Hess, Richard Fetter, Dmitri Chklovskii category:cs.CV q-bio.NC stat.ML  published:2012-10-01 summary:A central problem in neuroscience is reconstructing neuronal circuits on the synapse level. Due to a wide range of scales in brain architecture such reconstruction requires imaging that is both high-resolution and high-throughput. Existing electron microscopy (EM) techniques possess required resolution in the lateral plane and either high-throughput or high depth resolution but not both. Here, we exploit recent advances in unsupervised learning and signal processing to obtain high depth-resolution EM images computationally without sacrificing throughput. First, we show that the brain tissue can be represented as a sparse linear combination of localized basis functions that are learned using high-resolution datasets. We then develop compressive sensing-inspired techniques that can reconstruct the brain tissue from very few (typically 5) tomographic views of each section. This enables tracing of neuronal processes and, hence, high throughput reconstruction of neural circuits on the level of individual synapses. version:1
arxiv-1210-0563 | Sparse LMS via Online Linearized Bregman Iteration | http://arxiv.org/abs/1210.0563 | id:1210.0563 author:Tao Hu, Dmitri B. Chklovskii category:cs.IT cs.LG math.IT stat.ML  published:2012-10-01 summary:We propose a version of least-mean-square (LMS) algorithm for sparse system identification. Our algorithm called online linearized Bregman iteration (OLBI) is derived from minimizing the cumulative prediction error squared along with an l1-l2 norm regularizer. By systematically treating the non-differentiable regularizer we arrive at a simple two-step iteration. We demonstrate that OLBI is bias free and compare its operation with existing sparse LMS algorithms by rederiving them in the online convex optimization framework. We perform convergence analysis of OLBI for white input signals and derive theoretical expressions for both the steady state and instantaneous mean square deviations (MSD). We demonstrate numerically that OLBI improves the performance of LMS type algorithms for signals generated from sparse tap weights. version:1
arxiv-1210-0477 | Think Locally, Act Globally: Perfectly Balanced Graph Partitioning | http://arxiv.org/abs/1210.0477 | id:1210.0477 author:Peter Sanders, Christian Schulz category:cs.DS cs.DC cs.NE  published:2012-10-01 summary:We present a novel local improvement scheme for the perfectly balanced graph partitioning problem. This scheme encodes local searches that are not restricted to a balance constraint into a model allowing us to find combinations of these searches maintaining balance by applying a negative cycle detection algorithm. We combine this technique with an algorithm to balance unbalanced solutions and integrate it into a parallel multi-level evolutionary algorithm, KaFFPaE, to tackle the problem. Overall, we obtain a system that is fast on the one hand and on the other hand is able to improve or reproduce most of the best known perfectly balanced partitioning results ever reported in the literature. version:1
arxiv-1210-0473 | Memory Constraint Online Multitask Classification | http://arxiv.org/abs/1210.0473 | id:1210.0473 author:Giovanni Cavallanti, Nicolò Cesa-Bianchi category:cs.LG  published:2012-10-01 summary:We investigate online kernel algorithms which simultaneously process multiple classification tasks while a fixed constraint is imposed on the size of their active sets. We focus in particular on the design of algorithms that can efficiently deal with problems where the number of tasks is extremely high and the task data are large scale. Two new projection-based algorithms are introduced to efficiently tackle those issues while presenting different trade offs on how the available memory is managed with respect to the prior information about the learning tasks. Theoretically sound budget algorithms are devised by coupling the Randomized Budget Perceptron and the Forgetron algorithms with the multitask kernel. We show how the two seemingly contrasting properties of learning from multiple tasks and keeping a constant memory footprint can be balanced, and how the sharing of the available space among different tasks is automatically taken care of. We propose and discuss new insights on the multitask kernel. Experiments show that online kernel multitask algorithms running on a budget can efficiently tackle real world learning problems involving multiple tasks. version:1
arxiv-1205-4481 | Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure | http://arxiv.org/abs/1205.4481 | id:1205.4481 author:Hua Ouyang, Alexander Gray category:cs.LG stat.CO stat.ML  published:2012-05-21 summary:In this work we consider the stochastic minimization of nonsmooth convex loss functions, a central problem in machine learning. We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs. It is the first stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions (with strong convexity). The fast rates are confirmed by empirical comparisons, in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD. version:4
arxiv-1210-0347 | Enhanced Techniques for PDF Image Segmentation and Text Extraction | http://arxiv.org/abs/1210.0347 | id:1210.0347 author:D. Sasirekha, E. Chandra category:cs.CV  published:2012-10-01 summary:Extracting text objects from the PDF images is a challenging problem. The text data present in the PDF images contain certain useful information for automatic annotation, indexing etc. However variations of the text due to differences in text style, font, size, orientation, alignment as well as complex structure make the problem of automatic text extraction extremely difficult and challenging job. This paper presents two techniques under block-based classification. After a brief introduction of the classification methods, two methods were enhanced and results were evaluated. The performance metrics for segmentation and time consumption are tested for both the models. version:1
arxiv-1102-0059 | Statistical methods for tissue array images - algorithmic scoring and co-training | http://arxiv.org/abs/1102.0059 | id:1102.0059 author:Donghui Yan, Pei Wang, Michael Linden, Beatrice Knudsen, Timothy Randolph category:stat.ME cs.CE cs.CV cs.LG q-bio.QM  published:2011-02-01 summary:Recent advances in tissue microarray technology have allowed immunohistochemistry to become a powerful medium-to-high throughput analysis tool, particularly for the validation of diagnostic and prognostic biomarkers. However, as study size grows, the manual evaluation of these assays becomes a prohibitive limitation; it vastly reduces throughput and greatly increases variability and expense. We propose an algorithm - Tissue Array Co-Occurrence Matrix Analysis (TACOMA) - for quantifying cellular phenotypes based on textural regularity summarized by local inter-pixel relationships. The algorithm can be easily trained for any staining pattern, is absent of sensitive tuning parameters and has the ability to report salient pixels in an image that contribute to its score. Pathologists' input via informative training patches is an important aspect of the algorithm that allows the training for any specific marker or cell type. With co-training, the error rate of TACOMA can be reduced substantially for a very small training sample (e.g., with size 30). We give theoretical insights into the success of co-training via thinning of the feature set in a high-dimensional setting when there is "sufficient" redundancy among the features. TACOMA is flexible, transparent and provides a scoring process that can be evaluated with clarity and confidence. In a study based on an estrogen receptor (ER) marker, we show that TACOMA is comparable to, or outperforms, pathologists' performance in terms of accuracy and repeatability. version:2
arxiv-1207-3598 | Learning to rank from medical imaging data | http://arxiv.org/abs/1207.3598 | id:1207.3598 author:Fabian Pedregosa, Alexandre Gramfort, Gaël Varoquaux, Elodie Cauvet, Christophe Pallier, Bertrand Thirion category:cs.LG cs.CV  published:2012-07-16 summary:Medical images can be used to predict a clinical score coding for the severity of a disease, a pain level or the complexity of a cognitive task. In all these cases, the predicted variable has a natural order. While a standard classifier discards this information, we would like to take it into account in order to improve prediction performance. A standard linear regression does model such information, however the linearity assumption is likely not be satisfied when predicting from pixel intensities in an image. In this paper we address these modeling challenges with a supervised learning procedure where the model aims to order or rank images. We use a linear model for its robustness in high dimension and its possible interpretation. We show on simulations and two fMRI datasets that this approach is able to predict the correct ordering on pairs of images, yielding higher prediction accuracy than standard regression and multiclass classification techniques. version:2
arxiv-1210-0193 | On The Convergence of a Nash Seeking Algorithm with Stochastic State Dependent Payoff | http://arxiv.org/abs/1210.0193 | id:1210.0193 author:A. F. Hanif, H. Tembine, M. Assaad, D. Zeghlache category:math.OC math.DS math.NA stat.ML  published:2012-09-30 summary:Distributed strategic learning has been getting attention in recent years. As systems become distributed finding Nash equilibria in a distributed fashion is becoming more important for various applications. In this paper, we develop a distributed strategic learning framework for seeking Nash equilibria under stochastic state-dependent payoff functions. We extend the work of Krstic et.al. in [1] to the case of stochastic state dependent payoff functions. We develop an iterative distributed algorithm for Nash seeking and examine its convergence to a limiting trajectory defined by an Ordinary Differential Equation (ODE). We show convergence of our proposed algorithm for vanishing step size and provide an error bound for fixed step size. Finally, we conduct a stability analysis and apply the proposed scheme in a generic wireless networks. We also present numerical results which corroborate our claim. version:1
arxiv-1109-6090 | Robust Parametric Classification and Variable Selection by a Minimum Distance Criterion | http://arxiv.org/abs/1109.6090 | id:1109.6090 author:Eric C. Chi, David W. Scott category:stat.ME stat.CO stat.ML  published:2011-09-28 summary:We investigate a robust penalized logistic regression algorithm based on a minimum distance criterion. Influential outliers are often associated with the explosion of parameter vector estimates, but in the context of standard logistic regression, the bias due to outliers always causes the parameter vector to implode, that is shrink towards the zero vector. Thus, using LASSO-like penalties to perform variable selection in the presence of outliers can result in missed detections of relevant covariates. We show that by choosing a minimum distance criterion together with an Elastic Net penalty, we can simultaneously find a parsimonious model and avoid estimation implosion even in the presence of many outliers in the important small $n$ large $p$ situation. Implementation using an MM algorithm is described and performance evaluated. version:3
arxiv-1210-0153 | A Low Cost Vision Based Hybrid Fiducial Mark Tracking Technique for Mobile Industrial Robots | http://arxiv.org/abs/1210.0153 | id:1210.0153 author:Mohammed Y Aalsalem, Wazir Zada Khan, Quratul Ain Arshad category:cs.CV cs.RO  published:2012-09-29 summary:The field of robotic vision is developing rapidly. Robots can react intelligently and provide assistance to user activities through sentient computing. Since industrial applications pose complex requirements that cannot be handled by humans, an efficient low cost and robust technique is required for the tracking of mobile industrial robots. The existing sensor based techniques for mobile robot tracking are expensive and complex to deploy, configure and maintain. Also some of them demand dedicated and often expensive hardware. This paper presents a low cost vision based technique called Hybrid Fiducial Mark Tracking (HFMT) technique for tracking mobile industrial robot. HFMT technique requires off-the-shelf hardware (CCD cameras) and printable 2-D circular marks used as fiducials for tracking a mobile industrial robot on a pre-defined path. This proposed technique allows the robot to track on a predefined path by using fiducials for the detection of Right and Left turns on the path and White Strip for tracking the path. The HFMT technique is implemented and tested on an indoor mobile robot at our laboratory. Experimental results from robot navigating in real environments have confirmed that our approach is simple and robust and can be adopted in any hostile industrial environment where humans are unable to work. version:1
arxiv-1210-0118 | Self-Delimiting Neural Networks | http://arxiv.org/abs/1210.0118 | id:1210.0118 author:Juergen Schmidhuber category:cs.NE  published:2012-09-29 summary:Self-delimiting (SLIM) programs are a central concept of theoretical computer science, particularly algorithmic information & probability theory, and asymptotically optimal program search (AOPS). To apply AOPS to (possibly recurrent) neural networks (NNs), I introduce SLIM NNs. Neurons of a typical SLIM NN have threshold activation functions. During a computational episode, activations are spreading from input neurons through the SLIM NN until the computation activates a special halt neuron. Weights of the NN's used connections define its program. Halting programs form a prefix code. The reset of the initial NN state does not cost more than the latest program execution. Since prefixes of SLIM programs influence their suffixes (weight changes occurring early in an episode influence which weights are considered later), SLIM NN learning algorithms (LAs) should execute weight changes online during activation spreading. This can be achieved by applying AOPS to growing SLIM NNs. To efficiently teach a SLIM NN to solve many tasks, such as correctly classifying many different patterns, or solving many different robot control tasks, each connection keeps a list of tasks it is used for. The lists may be efficiently updated during training. To evaluate the overall effect of currently tested weight changes, a SLIM NN LA needs to re-test performance only on the efficiently computable union of tasks potentially affected by the current weight changes. Future SLIM NNs will be implemented on 3-dimensional brain-like multi-processor hardware. Their LAs will minimize task-specific total wire length of used connections, to encourage efficient solutions of subtasks by subsets of neurons that are physically close. The novel class of SLIM NN LAs is currently being probed in ongoing experiments to be reported in separate papers. version:1
arxiv-1207-3169 | The law of brevity in macaque vocal communication is not an artifact of analyzing mean call durations | http://arxiv.org/abs/1207.3169 | id:1207.3169 author:Stuart Semple, Minna J. Hsu, Govindasamy Agoramoorthy, Ramon Ferrer-i-Cancho category:q-bio.NC cs.CL physics.data-an  published:2012-07-13 summary:Words follow the law of brevity, i.e. more frequent words tend to be shorter. From a statistical point of view, this qualitative definition of the law states that word length and word frequency are negatively correlated. Here the recent finding of patterning consistent with the law of brevity in Formosan macaque vocal communication (Semple et al., 2010) is revisited. It is shown that the negative correlation between mean duration and frequency of use in the vocalizations of Formosan macaques is not an artifact of the use of a mean duration for each call type instead of the customary 'word' length of studies of the law in human language. The key point demonstrated is that the total duration of calls of a particular type increases with the number of calls of that type. The finding of the law of brevity in the vocalizations of these macaques therefore defies a trivial explanation. version:2
arxiv-1210-0077 | Optimistic Agents are Asymptotically Optimal | http://arxiv.org/abs/1210.0077 | id:1210.0077 author:Peter Sunehag, Marcus Hutter category:cs.AI cs.LG  published:2012-09-29 summary:We use optimism to introduce generic asymptotically optimal reinforcement learning agents. They achieve, with an arbitrary finite or compact class of environments, asymptotically optimal behavior. Furthermore, in the finite deterministic case we provide finite error bounds. version:1
arxiv-1210-0066 | Iterative Reweighted Minimization Methods for $l_p$ Regularized Unconstrained Nonlinear Programming | http://arxiv.org/abs/1210.0066 | id:1210.0066 author:Zhaosong Lu category:math.OC cs.LG stat.CO stat.ML  published:2012-09-29 summary:In this paper we study general $l_p$ regularized unconstrained minimization problems. In particular, we derive lower bounds for nonzero entries of first- and second-order stationary points, and hence also of local minimizers of the $l_p$ minimization problems. We extend some existing iterative reweighted $l_1$ (IRL1) and $l_2$ (IRL2) minimization methods to solve these problems and proposed new variants for them in which each subproblem has a closed form solution. Also, we provide a unified convergence analysis for these methods. In addition, we propose a novel Lipschitz continuous $\epsilon$-approximation to $\ x\ ^p_p$. Using this result, we develop new IRL1 methods for the $l_p$ minimization problems and showed that any accumulation point of the sequence generated by these methods is a first-order stationary point, provided that the approximation parameter $\epsilon$ is below a computable threshold value. This is a remarkable result since all existing iterative reweighted minimization methods require that $\epsilon$ be dynamically updated and approach zero. Our computational results demonstrate that the new IRL1 method is generally more stable than the existing IRL1 methods [21,18] in terms of objective function value and CPU time. version:1
arxiv-1210-0528 | Band Selection and Classification of Hyperspectral Images using Mutual Information: An algorithm based on minimizing the error probability using the inequality of Fano | http://arxiv.org/abs/1210.0528 | id:1210.0528 author:Elkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV 68U10  68R05 I.4.7; I.4.8; I.4.9  published:2012-09-28 summary:Hyperspectral image is a substitution of more than a hundred images, called bands, of the same region. They are taken at juxtaposed frequencies. The reference image of the region is called Ground Truth map (GT). the problematic is how to find the good bands to classify the pixels of regions; because the bands can be not only redundant, but a source of confusion, and decreasing so the accuracy of classification. Some methods use Mutual Information (MI) and threshold, to select relevant bands. Recently there's an algorithm selection based on mutual information, using bandwidth rejection and a threshold to control and eliminate redundancy. The band top ranking the MI is selected, and if its neighbors have sensibly the same MI with the GT, they will be considered redundant and so discarded. This is the most inconvenient of this method, because this avoids the advantage of hyperspectral images: some precious information can be discarded. In this paper we'll make difference between useful and useless redundancy. A band contains useful redundancy if it contributes to decreasing error probability. According to this scheme, we introduce new algorithm using also mutual information, but it retains only the bands minimizing the error probability of classification. To control redundancy, we introduce a complementary threshold. So the good band candidate must contribute to decrease the last error probability augmented by the threshold. This process is a wrapper strategy; it gets high performance of classification accuracy but it is expensive than filter strategy. version:1
arxiv-1210-0052 | Dimensionality Reduction and Classification feature using Mutual Information applied to Hyperspectral Images : A Filter strategy based algorithm | http://arxiv.org/abs/1210.0052 | id:1210.0052 author:ELkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV 68U10  68R05 I.4.7; I.4.8; I.4.9  published:2012-09-28 summary:Hyperspectral images (HIS) classification is a high technical remote sensing tool. The goal is to reproduce a thematic map that will be compared with a reference ground truth map (GT), constructed by expecting the region. The HIS contains more than a hundred bidirectional measures, called bands (or simply images), of the same region. They are taken at juxtaposed frequencies. Unfortunately, some bands contain redundant information, others are affected by the noise, and the high dimensionality of features made the accuracy of classification lower. The problematic is how to find the good bands to classify the pixels of regions. Some methods use Mutual Information (MI) and threshold, to select relevant bands, without treatment of redundancy. Others control and eliminate redundancy by selecting the band top ranking the MI, and if its neighbors have sensibly the same MI with the GT, they will be considered redundant and so discarded. This is the most inconvenient of this method, because this avoids the advantage of hyperspectral images: some precious information can be discarded. In this paper we'll accept the useful redundancy. A band contains useful redundancy if it contributes to produce an estimated reference map that has higher MI with the GT.nTo control redundancy, we introduce a complementary threshold added to last value of MI. This process is a Filter strategy; it gets a better performance of classification accuracy and not expensive, but less preferment than Wrapper strategy. version:1
arxiv-1210-0026 | Coupled quasi-harmonic bases | http://arxiv.org/abs/1210.0026 | id:1210.0026 author:A. Kovnatsky, M. M. Bronstein, A. M. Bronstein, K. Glashoff, R. Kimmel category:cs.CV cs.GR  published:2012-09-28 summary:The use of Laplacian eigenbases has been shown to be fruitful in many computer graphics applications. Today, state-of-the-art approaches to shape analysis, synthesis, and correspondence rely on these natural harmonic bases that allow using classical tools from harmonic analysis on manifolds. However, many applications involving multiple shapes are obstacled by the fact that Laplacian eigenbases computed independently on different shapes are often incompatible with each other. In this paper, we propose the construction of common approximate eigenbases for multiple shapes using approximate joint diagonalization algorithms. We illustrate the benefits of the proposed approach on tasks from shape editing, pose transfer, correspondence, and similarity. version:1
arxiv-1204-3198 | The failure of the law of brevity in two New World primates. Statistical caveats | http://arxiv.org/abs/1204.3198 | id:1204.3198 author:Ramon Ferrer-i-Cancho, Antoni Hernández-Fernández category:q-bio.NC cs.CL  published:2012-04-14 summary:Parallels of Zipf's law of brevity, the tendency of more frequent words to be shorter, have been found in bottlenose dolphins and Formosan macaques. Although these findings suggest that behavioral repertoires are shaped by a general principle of compression, common marmosets and golden-backed uakaris do not exhibit the law. However, we argue that the law may be impossible or difficult to detect statistically in a given species if the repertoire is too small, a problem that could be affecting golden backed uakaris, and show that the law is present in a subset of the repertoire of common marmosets. We suggest that the visibility of the law will depend on the subset of the repertoire under consideration or the repertoire size. version:2
arxiv-1209-6560 | Sparse Modeling of Intrinsic Correspondences | http://arxiv.org/abs/1209.6560 | id:1209.6560 author:J. Pokrass, A. M. Bronstein, M. M. Bronstein, P. Sprechmann, G. Sapiro category:cs.GR cs.CG cs.CV  published:2012-09-28 summary:We present a novel sparse modeling approach to non-rigid shape matching using only the ability to detect repeatable regions. As the input to our algorithm, we are given only two sets of regions in two shapes; no descriptors are provided so the correspondence between the regions is not know, nor we know how many regions correspond in the two shapes. We show that even with such scarce information, it is possible to establish very accurate correspondence between the shapes by using methods from the field of sparse modeling, being this, the first non-trivial use of sparse models in shape correspondence. We formulate the problem of permuted sparse coding, in which we solve simultaneously for an unknown permutation ordering the regions on two shapes and for an unknown correspondence in functional representation. We also propose a robust variant capable of handling incomplete matches. Numerically, the problem is solved efficiently by alternating the solution of a linear assignment and a sparse coding problem. The proposed methods are evaluated qualitatively and quantitatively on standard benchmarks containing both synthetic and scanned objects. version:1
arxiv-1209-5375 | Improving accuracy and power with transfer learning using a meta-analytic database | http://arxiv.org/abs/1209.5375 | id:1209.5375 author:Yannick Schwartz, Gaël Varoquaux, Christophe Pallier, Philippe Pinel, Jean-Baptiste Poline, Bertrand Thirion category:stat.ML  published:2012-09-24 summary:Typical cohorts in brain imaging studies are not large enough for systematic testing of all the information contained in the images. To build testable working hypotheses, investigators thus rely on analysis of previous work, sometimes formalized in a so-called meta-analysis. In brain imaging, this approach underlies the specification of regions of interest (ROIs) that are usually selected on the basis of the coordinates of previously detected effects. In this paper, we propose to use a database of images, rather than coordinates, and frame the problem as transfer learning: learning a discriminant model on a reference task to apply it to a different but related new task. To facilitate statistical analysis of small cohorts, we use a sparse discriminant model that selects predictive voxels on the reference task and thus provides a principled procedure to define ROIs. The benefits of our approach are twofold. First it uses the reference database for prediction, i.e. to provide potential biomarkers in a clinical setting. Second it increases statistical power on the new task. We demonstrate on a set of 18 pairs of functional MRI experimental conditions that our approach gives good prediction. In addition, on a specific transfer situation involving different scanners at different locations, we show that voxel selection based on transfer learning leads to higher detection power on small cohorts. version:2
arxiv-1209-6525 | A Complete System for Candidate Polyps Detection in Virtual Colonoscopy | http://arxiv.org/abs/1209.6525 | id:1209.6525 author:Marcelo Fiori, Pablo Musé, Guillermo Sapiro category:cs.CV cs.LG  published:2012-09-28 summary:Computer tomographic colonography, combined with computer-aided detection, is a promising emerging technique for colonic polyp analysis. We present a complete pipeline for polyp detection, starting with a simple colon segmentation technique that enhances polyps, followed by an adaptive-scale candidate polyp delineation and classification based on new texture and geometric features that consider both the information in the candidate polyp location and its immediate surrounding area. The proposed system is tested with ground truth data, including flat and small polyps which are hard to detect even with optical colonoscopy. For polyps larger than 6mm in size we achieve 100% sensitivity with just 0.9 false positives per case, and for polyps larger than 3mm in size we achieve 93% sensitivity with 2.8 false positives per case. version:1
arxiv-0909-1373 | Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eQTL mapping | http://arxiv.org/abs/0909.1373 | id:0909.1373 author:Seyoung Kim, Eric P. Xing category:stat.ML q-bio.GN q-bio.QM stat.AP stat.ME  published:2009-09-08 summary:We consider the problem of estimating a sparse multi-response regression function, with an application to expression quantitative trait locus (eQTL) mapping, where the goal is to discover genetic variations that influence gene-expression levels. In particular, we investigate a shrinkage technique capable of capturing a given hierarchical structure over the responses, such as a hierarchical clustering tree with leaf nodes for responses and internal nodes for clusters of related responses at multiple granularity, and we seek to leverage this structure to recover covariates relevant to each hierarchically-defined cluster of responses. We propose a tree-guided group lasso, or tree lasso, for estimating such structured sparsity under multi-response regression by employing a novel penalty function constructed from the tree. We describe a systematic weighting scheme for the overlapping groups in the tree-penalty such that each regression coefficient is penalized in a balanced manner despite the inhomogeneous multiplicity of group memberships of the regression coefficients due to overlaps among groups. For efficient optimization, we employ a smoothing proximal gradient method that was originally developed for a general class of structured-sparsity-inducing penalties. Using simulated and yeast data sets, we demonstrate that our method shows a superior performance in terms of both prediction errors and recovery of true sparsity patterns, compared to other methods for learning a multivariate-response regression. version:3
arxiv-1207-7167 | Predicate Generation for Learning-Based Quantifier-Free Loop Invariant Inference | http://arxiv.org/abs/1207.7167 | id:1207.7167 author:Wonchan Lee, Yungbum Jung, Bow-yaw Wang, Kwangkuen Yi category:cs.LO cs.LG F.3.1  published:2012-07-31 summary:We address the predicate generation problem in the context of loop invariant inference. Motivated by the interpolation-based abstraction refinement technique, we apply the interpolation theorem to synthesize predicates implicitly implied by program texts. Our technique is able to improve the effectiveness and efficiency of the learning-based loop invariant inference algorithm in [14]. We report experiment results of examples from Linux, SPEC2000, and Tar utility. version:2
arxiv-1209-6419 | Partial Gaussian Graphical Model Estimation | http://arxiv.org/abs/1209.6419 | id:1209.6419 author:Xiao-Tong Yuan, Tong Zhang category:cs.LG cs.IT math.IT stat.ML  published:2012-09-28 summary:This paper studies the partial estimation of Gaussian graphical models from high-dimensional empirical observations. We derive a convex formulation for this problem using $\ell_1$-regularized maximum-likelihood estimation, which can be solved via a block coordinate descent algorithm. Statistical estimation performance can be established for our method. The proposed approach has competitive empirical performance compared to existing methods, as demonstrated by various experiments on synthetic and real datasets. version:1
arxiv-1209-6409 | A Deterministic Analysis of an Online Convex Mixture of Expert Algorithms | http://arxiv.org/abs/1209.6409 | id:1209.6409 author:Mehmet A. Donmez, Sait Tunc, Suleyman S. Kozat category:cs.LG  published:2012-09-28 summary:We analyze an online learning algorithm that adaptively combines outputs of two constituent algorithms (or the experts) running in parallel to model an unknown desired signal. This online learning algorithm is shown to achieve (and in some cases outperform) the mean-square error (MSE) performance of the best constituent algorithm in the mixture in the steady-state. However, the MSE analysis of this algorithm in the literature uses approximations and relies on statistical models on the underlying signals and systems. Hence, such an analysis may not be useful or valid for signals generated by various real life systems that show high degrees of nonstationarity, limit cycles and, in many cases, that are even chaotic. In this paper, we produce results in an individual sequence manner. In particular, we relate the time-accumulated squared estimation error of this online algorithm at any time over any interval to the time accumulated squared estimation error of the optimal convex mixture of the constituent algorithms directly tuned to the underlying signal in a deterministic sense without any statistical assumptions. In this sense, our analysis provides the transient, steady-state and tracking behavior of this algorithm in a strong sense without any approximations in the derivations or statistical assumptions on the underlying signals such that our results are guaranteed to hold. We illustrate the introduced results through examples. version:1
arxiv-1209-6393 | Learning Robust Low-Rank Representations | http://arxiv.org/abs/1209.6393 | id:1209.6393 author:Pablo Sprechmann, Alex M. Bronstein, Guillermo Sapiro category:cs.LG math.OC  published:2012-09-27 summary:In this paper we present a comprehensive framework for learning robust low-rank representations by combining and extending recent ideas for learning fast sparse coding regressors with structured non-convex optimization techniques. This approach connects robust principal component analysis (RPCA) with dictionary learning techniques and allows its approximation via trainable encoders. We propose an efficient feed-forward architecture derived from an optimization algorithm designed to exactly solve robust low dimensional projections. This architecture, in combination with different training objective functions, allows the regressors to be used as online approximants of the exact offline RPCA problem or as RPCA-based neural networks. Simple modifications of these encoders can handle challenging extensions, such as the inclusion of geometric data transformations. We present several examples with real data from image, audio, and video processing. When used to approximate RPCA, our basic implementation shows several orders of magnitude speedup compared to the exact solvers with almost no performance degradation. We show the strength of the inclusion of learning to the RPCA approach on a music source separation application, where the encoders outperform the exact RPCA algorithms, which are already reported to produce state-of-the-art results on a benchmark database. Our preliminary implementation on an iPad shows faster-than-real-time performance with minimal latency. version:1
arxiv-1111-6082 | Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints | http://arxiv.org/abs/1111.6082 | id:1111.6082 author:Mehrdad Mahdavi, Rong Jin, Tianbao Yang category:cs.LG  published:2011-11-25 summary:In this paper we propose a framework for solving constrained online convex optimization problem. Our motivation stems from the observation that most algorithms proposed for online convex optimization require a projection onto the convex set $\mathcal{K}$ from which the decisions are made. While for simple shapes (e.g. Euclidean ball) the projection is straightforward, for arbitrary complex sets this is the main computational challenge and may be inefficient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring decisions belong to $\mathcal{K}$ for all rounds, we only require that the constraints which define the set $\mathcal{K}$ be satisfied in the long run. We show that our framework can be utilized to solve a relaxed version of online learning with side constraints addressed in \cite{DBLP:conf/colt/MannorT06} and \cite{DBLP:conf/aaai/KvetonYTM08}. By turning the problem into an online convex-concave optimization problem, we propose an efficient algorithm which achieves $\tilde{\mathcal{O}}(\sqrt{T})$ regret bound and $\tilde{\mathcal{O}}(T^{3/4})$ bound for the violation of constraints. Then we modify the algorithm in order to guarantee that the constraints are satisfied in the long run. This gain is achieved at the price of getting $\tilde{\mathcal{O}}(T^{3/4})$ regret bound. Our second algorithm is based on the Mirror Prox method \citep{nemirovski-2005-prox} to solve variational inequalities which achieves $\tilde{\mathcal{\mathcal{O}}}(T^{2/3})$ bound for both regret and the violation of constraints when the domain $\K$ can be described by a finite number of linear constraints. Finally, we extend the result to the setting where we only have partial access to the convex set $\mathcal{K}$ and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our first algorithm. version:3
arxiv-1209-6342 | Sparse Ising Models with Covariates | http://arxiv.org/abs/1209.6342 | id:1209.6342 author:Jie Cheng, Elizaveta Levina, Pei Wang, Ji Zhu category:stat.ML cs.LG  published:2012-09-27 summary:There has been a lot of work fitting Ising models to multivariate binary data in order to understand the conditional dependency relationships between the variables. However, additional covariates are frequently recorded together with the binary data, and may influence the dependence relationships. Motivated by such a dataset on genomic instability collected from tumor samples of several types, we propose a sparse covariate dependent Ising model to study both the conditional dependency within the binary data and its relationship with the additional covariates. This results in subject-specific Ising models, where the subject's covariates influence the strength of association between the genes. As in all exploratory data analysis, interpretability of results is important, and we use L1 penalties to induce sparsity in the fitted graphs and in the number of selected covariates. Two algorithms to fit the model are proposed and compared on a set of simulated data, and asymptotic results are established. The results on the tumor dataset and their biological significance are discussed in detail. version:1
arxiv-1202-3323 | Mirror Descent Meets Fixed Share (and feels no regret) | http://arxiv.org/abs/1202.3323 | id:1202.3323 author:Nicolò Cesa-Bianchi, Pierre Gaillard, Gabor Lugosi, Gilles Stoltz category:cs.LG stat.ML  published:2012-02-15 summary:Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters. version:2
arxiv-1209-6329 | More Is Better: Large Scale Partially-supervised Sentiment Classification - Appendix | http://arxiv.org/abs/1209.6329 | id:1209.6329 author:Yoav Haimovitch, Koby Crammer, Shie Mannor category:cs.LG  published:2012-09-27 summary:We describe a bootstrapping algorithm to learn from partially labeled data, and the results of an empirical study for using it to improve performance of sentiment classification using up to 15 million unlabeled Amazon product reviews. Our experiments cover semi-supervised learning, domain adaptation and weakly supervised learning. In some cases our methods were able to reduce test error by more than half using such large amount of data. NOTICE: This is only the supplementary material. version:1
arxiv-1209-6204 | Reclassification formula that provides to surpass K-means method | http://arxiv.org/abs/1209.6204 | id:1209.6204 author:M. Kharinov category:cs.CV cs.DS  published:2012-09-27 summary:The paper presents a formula for the reclassification of multidimensional data points (columns of real numbers, "objects", "vectors", etc.). This formula describes the change in the total squared error caused by reclassification of data points from one cluster into another and prompts the way to calculate the sequence of optimal partitions, which are characterized by a minimum value of the total squared error E (weighted sum of within-class variance, within-cluster sum of squares WCSS etc.), i.e. the sum of squared distances from each data point to its cluster center. At that source data points are treated with repetitions allowed, and resulting clusters from different partitions, in general case, overlap each other. The final partitions are characterized by "equilibrium" stability with respect to the reclassification of the data points, where the term "stability" means that any prescribed reclassification of data points does not increase the total squared error E. It is important that conventional K-means method, in general case, provides generation of instable partitions with overstated values of the total squared error E. The proposed method, based on the formula of reclassification, is more efficient than K-means method owing to converting of any partition into stable one, as well as involving into the process of reclassification of certain sets of data points, in contrast to the classification of individual data points according to K-means method. version:1
arxiv-1209-6190 | Noise Influence on the Fuzzy-Linguistic Partitioning of Iris Code Space | http://arxiv.org/abs/1209.6190 | id:1209.6190 author:Iulia M. Motoc, Cristina M. Noaica, Robert Badea, Claudiu G. Ghica category:cs.CV 68U10 I.5  published:2012-09-27 summary:This paper analyses the set of iris codes stored or used in an iris recognition system as an f-granular space. The f-granulation is given by identifying in the iris code space the extensions of the fuzzy concepts wolves, goats, lambs and sheep (previously introduced by Doddington as 'animals' of the biometric menagerie) - which together form a partitioning of the iris code space. The main question here is how objective (stable / stationary) this partitioning is when the iris segments are subject to noisy acquisition. In order to prove that the f-granulation of iris code space with respect to the fuzzy concepts that define the biometric menagerie is unstable in noisy conditions (is sensitive to noise), three types of noise (localvar, motion blur, salt and pepper) have been alternatively added to the iris segments extracted from University of Bath Iris Image Database. The results of 180 exhaustive (all-to-all) iris recognition tests are presented and commented here. version:1
arxiv-1209-6189 | The Biometric Menagerie - A Fuzzy and Inconsistent Concept | http://arxiv.org/abs/1209.6189 | id:1209.6189 author:Nicolaie Popescu-Bodorin, Valentina E. Balas, Iulia M. Motoc category:cs.CV 68U10 I.5  published:2012-09-27 summary:This paper proves that in iris recognition, the concepts of sheep, goats, lambs and wolves - as proposed by Doddington and Yager in the so-called Biometric Menagerie, are at most fuzzy and at least not quite well defined. They depend not only on the users or on their biometric templates, but also on the parameters that calibrate the iris recognition system. This paper shows that, in the case of iris recognition, the extensions of these concepts have very unsharp and unstable (non-stationary) boundaries. The membership of a user to these categories is more often expressed as a degree (as a fuzzy value) rather than as a crisp value. Moreover, they are defined by fuzzy Sugeno rules instead of classical (crisp) definitions. For these reasons, we said that the Biometric Menagerie proposed by Doddington and Yager could be at most a fuzzy concept of biometry, but even this status is conditioned by improving its definition. All of these facts are confirmed experimentally in a series of 12 exhaustive iris recognition tests undertaken for University of Bath Iris Image Database while using three different iris code dimensions (256x16, 128x8 and 64x4), two different iris texture encoders (Log-Gabor and Haar-Hilbert) and two different types of safety models. version:1
arxiv-1203-0145 | The Horse Raced Past: Gardenpath Processing in Dynamical Systems | http://arxiv.org/abs/1203.0145 | id:1203.0145 author:Peter beim Graben category:cs.CL  published:2012-03-01 summary:I pinpoint an interesting similarity between a recent account to rational parsing and the treatment of sequential decisions problems in a dynamical systems approach. I argue that expectation-driven search heuristics aiming at fast computation resembles a high-risk decision strategy in favor of large transition velocities. Hale's rational parser, combining generalized left-corner parsing with informed $\mathrm{A}^*$ search to resolve processing conflicts, explains gardenpath effects in natural sentence processing by misleading estimates of future processing costs that are to be minimized. On the other hand, minimizing the duration of cognitive computations in time-continuous dynamical systems can be described by combining vector space representations of cognitive states by means of filler/role decompositions and subsequent tensor product representations with the paradigm of stable heteroclinic sequences. Maximizing transition velocities according to a high-risk decision strategy could account for a fast race even between states that are apparently remote in representation space. version:2
arxiv-1209-6151 | Face Alignment Using Active Shape Model And Support Vector Machine | http://arxiv.org/abs/1209.6151 | id:1209.6151 author:Thai Hoang Le, Truong Nhat Vo category:cs.CV  published:2012-09-27 summary:The Active Shape Model (ASM) is one of the most popular local texture models for face alignment. It applies in many fields such as locating facial features in the image, face synthesis, etc. However, the experimental results show that the accuracy of the classical ASM for some applications is not high. This paper suggests some improvements on the classical ASM to increase the performance of the model in the application: face alignment. Four of our major improvements include: i) building a model combining Sobel filter and the 2-D profile in searching face in image; ii) applying Canny algorithm for the enhancement edge on image; iii) Support Vector Machine (SVM) is used to classify landmarks on face, in order to determine exactly location of these landmarks support for ASM; iv)automatically adjust 2-D profile in the multi-level model based on the size of the input image. The experimental results on Caltech face database and Technical University of Denmark database (imm_face) show that our proposed improvement leads to far better performance. version:1
arxiv-1209-6070 | Movie Popularity Classification based on Inherent Movie Attributes using C4.5,PART and Correlation Coefficient | http://arxiv.org/abs/1209.6070 | id:1209.6070 author:Khalid Ibnal Asad, Tanvir Ahmed, Md. Saiedur Rahman category:cs.LG cs.DB cs.IR H.2.8  published:2012-09-26 summary:Abundance of movie data across the internet makes it an obvious candidate for machine learning and knowledge discovery. But most researches are directed towards bi-polar classification of movie or generation of a movie recommendation system based on reviews given by viewers on various internet sites. Classification of movie popularity based solely on attributes of a movie i.e. actor, actress, director rating, language, country and budget etc. has been less highlighted due to large number of attributes that are associated with each movie and their differences in dimensions. In this paper, we propose classification scheme of pre-release movie popularity based on inherent attributes using C4.5 and PART classifier algorithm and define the relation between attributes of post release movies using correlation coefficient. version:1
arxiv-1209-6037 | Reproduction of Images by Gamut Mapping and Creation of New Test Charts in Prepress Process | http://arxiv.org/abs/1209.6037 | id:1209.6037 author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV  published:2012-09-26 summary:With the advent of digital images the problem of keeping picture visualization uniformity arises because each printing or scanning device has its own color chart. So, universal color profiles are made by ICC to bring uniformity in various types of devices. Keeping that color profile in mind various new color charts are created and calibrated with the help of standard IT8 test charts available in the market. The main objective to color reproduction is to produce the identical picture at device output. For that principles for gamut mapping has been designed version:1
arxiv-1209-6004 | The Issue-Adjusted Ideal Point Model | http://arxiv.org/abs/1209.6004 | id:1209.6004 author:Sean M. Gerrish, David M. Blei category:stat.ML cs.LG stat.AP  published:2012-09-26 summary:We develop a model of issue-specific voting behavior. This model can be used to explore lawmakers' personal voting patterns of voting by issue area, providing an exploratory window into how the language of the law is correlated with political support. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout prediction performance and the model's utility in interpreting an inherently multi-dimensional space. version:1
arxiv-1209-6001 | Bayesian Mixture Models for Frequent Itemset Discovery | http://arxiv.org/abs/1209.6001 | id:1209.6001 author:Ruefei He, Jonathan Shapiro category:cs.LG cs.IR stat.ML H.2.8; H.3.3; I.2.6  published:2012-09-26 summary:In binary-transaction data-mining, traditional frequent itemset mining often produces results which are not straightforward to interpret. To overcome this problem, probability models are often used to produce more compact and conclusive results, albeit with some loss of accuracy. Bayesian statistics have been widely used in the development of probability models in machine learning in recent years and these methods have many advantages, including their abilities to avoid overfitting. In this paper, we develop two Bayesian mixture models with the Dirichlet distribution prior and the Dirichlet process (DP) prior to improve the previous non-Bayesian mixture model developed for transaction dataset mining. We implement the inference of both mixture models using two methods: a collapsed Gibbs sampling scheme and a variational approximation algorithm. Experiments in several benchmark problems have shown that both mixture models achieve better performance than a non-Bayesian mixture model. The variational algorithm is the faster of the two approaches while the Gibbs sampling method achieves a more accurate results. The Dirichlet process mixture model can automatically grow to a proper complexity for a better approximation. Once the model is built, it can be very fast to query and run analysis on (typically 10 times faster than Eclat, as we will show in the experiment section). However, these approaches also show that mixture models underestimate the probabilities of frequent itemsets. Consequently, these models have a higher sensitivity but a lower specificity. version:1
arxiv-1209-5991 | Subset Selection for Gaussian Markov Random Fields | http://arxiv.org/abs/1209.5991 | id:1209.5991 author:Satyaki Mahalanabis, Daniel Stefankovic category:cs.LG stat.ML 68Q32  published:2012-09-26 summary:Given a Gaussian Markov random field, we consider the problem of selecting a subset of variables to observe which minimizes the total expected squared prediction error of the unobserved variables. We first show that finding an exact solution is NP-hard even for a restricted class of Gaussian Markov random fields, called Gaussian free fields, which arise in semi-supervised learning and computer vision. We then give a simple greedy approximation algorithm for Gaussian free fields on arbitrary graphs. Finally, we give a message passing algorithm for general Gaussian Markov random fields on bounded tree-width graphs. version:1
arxiv-1209-5982 | PlaceRaider: Virtual Theft in Physical Spaces with Smartphones | http://arxiv.org/abs/1209.5982 | id:1209.5982 author:Robert Templeman, Zahid Rahman, David Crandall, Apu Kapadia category:cs.CR cs.CV  published:2012-09-26 summary:As smartphones become more pervasive, they are increasingly targeted by malware. At the same time, each new generation of smartphone features increasingly powerful onboard sensor suites. A new strain of sensor malware has been developing that leverages these sensors to steal information from the physical environment (e.g., researchers have recently demonstrated how malware can listen for spoken credit card numbers through the microphone, or feel keystroke vibrations using the accelerometer). Yet the possibilities of what malware can see through a camera have been understudied. This paper introduces a novel visual malware called PlaceRaider, which allows remote attackers to engage in remote reconnaissance and what we call virtual theft. Through completely opportunistic use of the camera on the phone and other sensors, PlaceRaider constructs rich, three dimensional models of indoor environments. Remote burglars can thus download the physical space, study the environment carefully, and steal virtual objects from the environment (such as financial documents, information on computer monitors, and personally identifiable information). Through two human subject studies we demonstrate the effectiveness of using mobile devices as powerful surveillance and virtual theft platforms, and we suggest several possible defenses against visual malware. version:1
arxiv-1206-1275 | Alternating Direction Methods for Latent Variable Gaussian Graphical Model Selection | http://arxiv.org/abs/1206.1275 | id:1206.1275 author:Shiqian Ma, Lingzhou Xue, Hui Zou category:math.OC stat.ML  published:2012-06-06 summary:Chandrasekaran, Parrilo and Willsky (2010) proposed a convex optimization problem to characterize graphical model selection in the presence of unobserved variables. This convex optimization problem aims to estimate an inverse covariance matrix that can be decomposed into a sparse matrix minus a low-rank matrix from sample data. Solving this convex optimization problem is very challenging, especially for large problems. In this paper, we propose two alternating direction methods for solving this problem. The first method is to apply the classical alternating direction method of multipliers to solve the problem as a consensus problem. The second method is a proximal gradient based alternating direction method of multipliers. Our methods exploit and take advantage of the special structure of the problem and thus can solve large problems very efficiently. Global convergence result is established for the proposed methods. Numerical results on both synthetic data and gene expression data show that our methods usually solve problems with one million variables in one to two minutes, and are usually five to thirty five times faster than a state-of-the-art Newton-CG proximal point algorithm. version:2
arxiv-1209-5477 | Optimal Weighting of Multi-View Data with Low Dimensional Hidden States | http://arxiv.org/abs/1209.5477 | id:1209.5477 author:Yichao Lu, Dean P. Foster category:stat.ML cs.LG  published:2012-09-25 summary:In Natural Language Processing (NLP) tasks, data often has the following two properties: First, data can be chopped into multi-views which has been successfully used for dimension reduction purposes. For example, in topic classification, every paper can be chopped into the title, the main text and the references. However, it is common that some of the views are less noisier than other views for supervised learning problems. Second, unlabeled data are easy to obtain while labeled data are relatively rare. For example, articles occurred on New York Times in recent 10 years are easy to grab but having them classified as 'Politics', 'Finance' or 'Sports' need human labor. Hence less noisy features are preferred before running supervised learning methods. In this paper we propose an unsupervised algorithm which optimally weights features from different views when these views are generated from a low dimensional hidden state, which occurs in widely used models like Mixture Gaussian Model, Hidden Markov Model (HMM) and Latent Dirichlet Allocation (LDA). version:2
arxiv-1209-5826 | Refinability of splines from lattice Voronoi cells | http://arxiv.org/abs/1209.5826 | id:1209.5826 author:Jorg Peters category:math.NA cs.CV 41A15  65D07  published:2012-09-26 summary:Splines can be constructed by convolving the indicator function of the Voronoi cell of a lattice. This paper presents simple criteria that imply that only a small subset of such spline families can be refined: essentially the well-known box splines and tensor-product splines. Among the many non-refinable constructions are hex-splines and their generalization to non-Cartesian lattices. An example shows how non-refinable splines can exhibit increased approximation error upon refinement of the lattice. version:1
arxiv-1209-6238 | Natural Language Processing - A Survey | http://arxiv.org/abs/1209.6238 | id:1209.6238 author:Kevin Mote category:cs.CL  published:2012-09-25 summary:The utility and power of Natural Language Processing (NLP) seems destined to change our technological society in profound and fundamental ways. However there are, to date, few accessible descriptions of the science of NLP that have been written for a popular audience, or even for an audience of intelligent, but uninitiated scientists. This paper aims to provide just such an overview. In short, the objective of this article is to describe the purpose, procedures and practical applications of NLP in a clear, balanced, and readable way. We will examine the most recent literature describing the methods and processes of NLP, analyze some of the challenges that researchers are faced with, and briefly survey some of the current and future applications of this science to IT research in general. version:1
arxiv-1209-5756 | Environmental Sounds Spectrogram Classification using Log-Gabor Filters and Multiclass Support Vector Machines | http://arxiv.org/abs/1209.5756 | id:1209.5756 author:Sameh Souli, Zied Lachiri category:cs.CV  published:2012-09-25 summary:This paper presents novel approaches for efficient feature extraction using environmental sound magnitude spectrogram. We propose approach based on the visual domain. This approach included three methods. The first method is based on extraction for each spectrogram a single log-Gabor filter followed by mutual information procedure. In the second method, the spectrogram is passed by the same steps of the first method but with an averaged bank of 12 log-Gabor filter. The third method consists of spectrogram segmentation into three patches, and after that for each spectrogram patch we applied the second method. The classification results prove that the second method is the most efficient in our environmental sound classification system. version:1
arxiv-1209-5601 | Feature selection with test cost constraint | http://arxiv.org/abs/1209.5601 | id:1209.5601 author:Fan Min, Qinghua Hu, William Zhu category:cs.AI cs.LG  published:2012-09-25 summary:Feature selection is an important preprocessing step in machine learning and data mining. In real-world applications, costs, including money, time and other resources, are required to acquire the features. In some cases, there is a test cost constraint due to limited resources. We shall deliberately select an informative and cheap feature subset for classification. This paper proposes the feature selection with test cost constraint problem for this issue. The new problem has a simple form while described as a constraint satisfaction problem (CSP). Backtracking is a general algorithm for CSP, and it is efficient in solving the new problem on medium-sized data. As the backtracking algorithm is not scalable to large datasets, a heuristic algorithm is also developed. Experimental results show that the heuristic algorithm can find the optimal solution in most cases. We also redefine some existing feature selection problems in rough sets, especially in decision-theoretic rough sets, from the viewpoint of CSP. These new definitions provide insight to some new research directions. version:1
arxiv-1209-5561 | Supervised Blockmodelling | http://arxiv.org/abs/1209.5561 | id:1209.5561 author:Leto Peel category:cs.LG cs.SI stat.ML  published:2012-09-25 summary:Collective classification models attempt to improve classification performance by taking into account the class labels of related instances. However, they tend not to learn patterns of interactions between classes and/or make the assumption that instances of the same class link to each other (assortativity assumption). Blockmodels provide a solution to these issues, being capable of modelling assortative and disassortative interactions, and learning the pattern of interactions in the form of a summary network. The Supervised Blockmodel provides good classification performance using link structure alone, whilst simultaneously providing an interpretable summary of network interactions to allow a better understanding of the data. This work explores three variants of supervised blockmodels of varying complexity and tests them on four structurally different real world networks. version:1
arxiv-1209-5549 | Towards a learning-theoretic analysis of spike-timing dependent plasticity | http://arxiv.org/abs/1209.5549 | id:1209.5549 author:David Balduzzi, Michel Besserve category:q-bio.NC cs.LG stat.ML  published:2012-09-25 summary:This paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning. We introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (STDP) and (ii) is amenable to theoretical analysis. We show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights. Moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength. Finally, based on our analysis, we propose a regularized version of STDP, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli. version:1
arxiv-1109-3714 | High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity | http://arxiv.org/abs/1109.3714 | id:1109.3714 author:Po-Ling Loh, Martin J. Wainwright category:math.ST cs.IT math.IT stat.ML stat.TH  published:2011-09-16 summary:Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependence, as well. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently nonconvex, and it is difficult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing nonconvex programs, we are able to both analyze the statistical error associated with any global optimum, and more surprisingly, to prove that a simple algorithm based on projected gradient descent will converge in polynomial time to a small neighborhood of the set of all global minimizers. On the statistical side, we provide nonasymptotic bounds that hold with high probability for the cases of noisy, missing and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm is guaranteed to converge at a geometric rate to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing close agreement with the predicted scalings. version:4
arxiv-1209-5494 | Segmentation of Breast Regions in Mammogram Based on Density: A Review | http://arxiv.org/abs/1209.5494 | id:1209.5494 author:Nafiza Saidin, Harsa Amylia Mat Sakim, Umi Kalthum Ngah, Ibrahim Lutfi Shuaib category:cs.CV  published:2012-09-25 summary:The focus of this paper is to review approaches for segmentation of breast regions in mammograms according to breast density. Studies based on density have been undertaken because of the relationship between breast cancer and density. Breast cancer usually occurs in the fibroglandular area of breast tissue, which appears bright on mammograms and is described as breast density. Most of the studies are focused on the classification methods for glandular tissue detection. Others highlighted on the segmentation methods for fibroglandular tissue, while few researchers performed segmentation of the breast anatomical regions based on density. There have also been works on the segmentation of other specific parts of breast regions such as either detection of nipple position, skin-air interface or pectoral muscles. The problems on the evaluation performance of the segmentation results in relation to ground truth are also discussed in this paper. version:1
arxiv-1203-0117 | Learning a Common Substructure of Multiple Graphical Gaussian Models | http://arxiv.org/abs/1203.0117 | id:1203.0117 author:Satoshi Hara, Takashi Washio category:stat.ML  published:2012-03-01 summary:Properties of data are frequently seen to vary depending on the sampled situations, which usually changes along a time evolution or owing to environmental effects. One way to analyze such data is to find invariances, or representative features kept constant over changes. The aim of this paper is to identify one such feature, namely interactions or dependencies among variables that are common across multiple datasets collected under different conditions. To that end, we propose a common substructure learning (CSSL) framework based on a graphical Gaussian model. We further present a simple learning algorithm based on the Dual Augmented Lagrangian and the Alternating Direction Method of Multipliers. We confirm the performance of CSSL over other existing techniques in finding unchanging dependency structures in multiple datasets through numerical simulations on synthetic data and through a real world application to anomaly detection in automobile sensors. version:3
arxiv-1209-5417 | Model based neuro-fuzzy ASR on Texas processor | http://arxiv.org/abs/1209.5417 | id:1209.5417 author:Hesam Ekhtiyar, Mehdi Sheida, Somaye Sobati Moghadam category:cs.CV  published:2012-09-24 summary:In this paper an algorithm for recognizing speech has been proposed. The recognized speech is used to execute related commands which use the MFCC and two kind of classifiers, first one uses MLP and second one uses fuzzy inference system as a classifier. The experimental results demonstrate the high gain and efficiency of the proposed algorithm. We have implemented this system based on graphical design and tested on a fix point digital signal processor (DSP) of 600 MHz, with reference DM6437-EVM of Texas instrument. version:1
arxiv-1209-0127 | Autoregressive short-term prediction of turning points using support vector regression | http://arxiv.org/abs/1209.0127 | id:1209.0127 author:Ran El-Yaniv, Alexandra Faynburd category:cs.LG cs.CE cs.NE  published:2012-09-01 summary:This work is concerned with autoregressive prediction of turning points in financial price sequences. Such turning points are critical local extrema points along a series, which mark the start of new swings. Predicting the future time of such turning points or even their early or late identification slightly before or after the fact has useful applications in economics and finance. Building on recently proposed neural network model for turning point prediction, we propose and study a new autoregressive model for predicting turning points of small swings. Our method relies on a known turning point indicator, a Fourier enriched representation of price histories, and support vector regression. We empirically examine the performance of the proposed method over a long history of the Dow Jones Industrial average. Our study shows that the proposed method is superior to the previous neural network model, in terms of trading performance of a simple trading application and also exhibits a quantifiable advantage over the buy-and-hold benchmark. version:2
arxiv-1209-2673 | Conditional validity of inductive conformal predictors | http://arxiv.org/abs/1209.2673 | id:1209.2673 author:Vladimir Vovk category:cs.LG 68T05  62G15  published:2012-09-12 summary:Conformal predictors are set predictors that are automatically valid in the sense of having coverage probability equal to or exceeding a given confidence level. Inductive conformal predictors are a computationally efficient version of conformal predictors satisfying the same property of validity. However, inductive conformal predictors have been only known to control unconditional coverage probability. This paper explores various versions of conditional validity and various ways to achieve them using inductive conformal predictors and their modifications. version:2
arxiv-1209-5339 | Developing Improved Greedy Crossover to Solve Symmetric Traveling Salesman Problem | http://arxiv.org/abs/1209.5339 | id:1209.5339 author:Hassan Ismkhan, Kamran Zamanifar category:cs.NE  published:2012-09-24 summary:The Traveling Salesman Problem (TSP) is one of the most famous optimization problems. Greedy crossover designed by Greffenstette et al, can be used while Symmetric TSP (STSP) is resolved by Genetic Algorithm (GA). Researchers have proposed several versions of greedy crossover. Here we propose improved version of it. We compare our greedy crossover with some of recent crossovers, we use our greedy crossover and some recent crossovers in GA then compare crossovers on speed and accuracy. version:1
arxiv-1209-5335 | BPRS: Belief Propagation Based Iterative Recommender System | http://arxiv.org/abs/1209.5335 | id:1209.5335 author:Erman Ayday, Arash Einolghozati, Faramarz Fekri category:cs.LG  published:2012-09-24 summary:In this paper we introduce the first application of the Belief Propagation (BP) algorithm in the design of recommender systems. We formulate the recommendation problem as an inference problem and aim to compute the marginal probability distributions of the variables which represent the ratings to be predicted. However, computing these marginal probability functions is computationally prohibitive for large-scale systems. Therefore, we utilize the BP algorithm to efficiently compute these functions. Recommendations for each active user are then iteratively computed by probabilistic message passing. As opposed to the previous recommender algorithms, BPRS does not require solving the recommendation problem for all the users if it wishes to update the recommendations for only a single active. Further, BPRS computes the recommendations for each user with linear complexity and without requiring a training period. Via computer simulations (using the 100K MovieLens dataset), we verify that BPRS iteratively reduces the error in the predicted ratings of the users until it converges. Finally, we confirm that BPRS is comparable to the state of art methods such as Correlation-based neighborhood model (CorNgbr) and Singular Value Decomposition (SVD) in terms of rating and precision accuracy. Therefore, we believe that the BP-based recommendation algorithm is a new promising approach which offers a significant advantage on scalability while providing competitive accuracy for the recommender systems. version:1
arxiv-1209-5260 | Towards Large-scale and Ultrahigh Dimensional Feature Selection via Feature Generation | http://arxiv.org/abs/1209.5260 | id:1209.5260 author:Mingkui Tan, Ivor W. Tsang, Li Wang category:cs.LG  published:2012-09-24 summary:In many real-world applications such as text mining, it is desirable to select the most relevant features or variables to improve the generalization ability, or to provide a better interpretation of the prediction models. {In this paper, a novel adaptive feature scaling (AFS) scheme is proposed by introducing a feature scaling {vector $\d \in [0, 1]^m$} to alleviate the bias problem brought by the scaling bias of the diverse features.} By reformulating the resultant AFS model to semi-infinite programming problem, a novel feature generating method is presented to identify the most relevant features for classification problems. In contrast to the traditional feature selection methods, the new formulation has the advantage of solving extremely high-dimensional and large-scale problems. With an exact solution to the worst-case analysis in the identification of relevant features, the proposed feature generating scheme converges globally. More importantly, the proposed scheme facilitates the group selection with or without special structures. Comprehensive experiments on a wide range of synthetic and real-world datasets demonstrate that the proposed method {achieves} better or competitive performance compared with the existing methods on (group) feature selection in terms of generalization performance and training efficiency. The C++ and MATLAB implementations of our algorithm can be available at \emph{http://c2inet.sce.ntu.edu.sg/Mingkui/robust-FGM.rar}. version:1
arxiv-1209-5251 | On Move Pattern Trends in a Large Go Games Corpus | http://arxiv.org/abs/1209.5251 | id:1209.5251 author:Petr Baudiš, Josef Moudřík category:cs.AI cs.LG  published:2012-09-24 summary:We process a large corpus of game records of the board game of Go and propose a way of extracting summary information on played moves. We then apply several basic data-mining methods on the summary information to identify the most differentiating features within the summary information, and discuss their correspondence with traditional Go knowledge. We show statistically significant mappings of the features to player attributes such as playing strength or informally perceived "playing style" (e.g. territoriality or aggressivity), describe accurate classifiers for these attributes, and propose applications including seeding real-work ranks of internet players, aiding in Go study and tuning of Go-playing programs, or contribution to Go-theoretical discussion on the scope of "playing style". version:1
arxiv-1209-5245 | Spike Timing Dependent Competitive Learning in Recurrent Self Organizing Pulsed Neural Networks Case Study: Phoneme and Word Recognition | http://arxiv.org/abs/1209.5245 | id:1209.5245 author:Tarek Behi, Najet Arous, Noureddine Ellouze category:cs.CV cs.AI q-bio.NC  published:2012-09-24 summary:Synaptic plasticity seems to be a capital aspect of the dynamics of neural networks. It is about the physiological modifications of the synapse, which have like consequence a variation of the value of the synaptic weight. The information encoding is based on the precise timing of single spike events that is based on the relative timing of the pre- and post-synaptic spikes, local synapse competitions within a single neuron and global competition via lateral connections. In order to classify temporal sequences, we present in this paper how to use a local hebbian learning, spike-timing dependent plasticity for unsupervised competitive learning, preserving self-organizing maps of spiking neurons. In fact we present three variants of self-organizing maps (SOM) with spike-timing dependent Hebbian learning rule, the Leaky Integrators Neurons (LIN), the Spiking_SOM and the recurrent Spiking_SOM (RSSOM) models. The case study of the proposed SOM variants is phoneme classification and word recognition in continuous speech and speaker independent. version:1
arxiv-1209-5111 | Making a Science of Model Search | http://arxiv.org/abs/1209.5111 | id:1209.5111 author:J. Bergstra, D. Yamins, D. D. Cox category:cs.CV cs.NE  published:2012-09-23 summary:Many computer vision algorithms depend on a variety of parameter choices and settings that are typically hand-tuned in the course of evaluating the algorithm. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to evaluating a method's full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it can be difficult to determine whether a given technique is genuinely better, or simply better tuned. In this work, we propose a meta-modeling approach to support automated hyper parameter optimization, with the goal of providing practical tools to replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from parameters that govern not only how individual processing steps are applied, but even which processing steps are included. A hyper parameter optimization algorithm transforms this graph into a program for optimizing that performance metric. Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single algorithm. More broadly, we argue that the formalization of a meta-model supports more objective, reproducible, and quantitative evaluation of computer vision algorithms, and that it can serve as a valuable tool for guiding algorithm development. version:1
arxiv-1209-5041 | An Implementation of Computer Graphics as Prepress Image Enhancement Process | http://arxiv.org/abs/1209.5041 | id:1209.5041 author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV  published:2012-09-23 summary:The production of a printed product involves three stages: prepress, the printing process (press) itself, and finishing (post press). There are various types of equipments (printers, scanners) and various qualities image are present in the market. These give different color rendering each time during reproduction. So, a color key tool has been developed keeping Color Management Scheme (CMS) in mind so that during reproduction no color rendering takes place irrespective of use of any device and resolution level has also been improved. version:1
arxiv-1209-5040 | Image Classification and Optimized Image Reproduction | http://arxiv.org/abs/1209.5040 | id:1209.5040 author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV  published:2012-09-23 summary:By taking into account the properties and limitations of the human visual system, images can be more efficiently compressed, colors more accurately reproduced, prints better rendered. To show all these advantages in this paper new adapted color charts have been created based on technical and visual image category analysis. A number of tests have been carried out using extreme images with their key information strictly in dark and light areas. It was shown that the image categorization using the adapted color charts improves the analysis of relevant image information with regard to both the image gradation and the detail reproduction. The images with key information in hi-key areas were also test printed using the adapted color charts. version:1
arxiv-1209-5039 | Creation of Digital Test Form for Prepress Department | http://arxiv.org/abs/1209.5039 | id:1209.5039 author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV  published:2012-09-23 summary:The main problem in colour management in prepress department is lack of availability of literature on colour management and knowledge gap between prepress department and press department. So a digital test from has been created by Adobe Photoshop to analyse the ICC profile and to create a new profile and this analysed data is used to study about various grey scale of RGB and CMYK images. That helps in conversion of image from RGB to CMYK in prepress department. version:1
arxiv-1209-5038 | Fast Randomized Model Generation for Shapelet-Based Time Series Classification | http://arxiv.org/abs/1209.5038 | id:1209.5038 author:Daniel Gordon, Danny Hendler, Lior Rokach category:cs.LG  published:2012-09-23 summary:Time series classification is a field which has drawn much attention over the past decade. A new approach for classification of time series uses classification trees based on shapelets. A shapelet is a subsequence extracted from one of the time series in the dataset. A disadvantage of this approach is the time required for building the shapelet-based classification tree. The search for the best shapelet requires examining all subsequences of all lengths from all time series in the training set. A key goal of this work was to find an evaluation order of the shapelets space which enables fast convergence to an accurate model. The comparative analysis we conducted clearly indicates that a random evaluation order yields the best results. Our empirical analysis of the distribution of high-quality shapelets within the shapelets space provides insights into why randomized shapelets sampling is superior to alternative evaluation orders. We present an algorithm for randomized model generation for shapelet-based classification that converges extremely quickly to a model with surprisingly high accuracy after evaluating only an exceedingly small fraction of the shapelets space. version:1
arxiv-1109-2553 | Nominal Association Vector and Matrix | http://arxiv.org/abs/1109.2553 | id:1109.2553 author:Wenxue Huang, Yong Shi, Xiaogang Wang category:stat.ML  published:2011-09-12 summary:When response variables are nominal and populations are cross-classified with respect to multiple polytomies, questions often arise about the degree of association of the responses with explanatory variables. When populations are known, we introduce a nominal association vector and matrix to evaluate the dependence of a response variable with an explanatory variable. These measures provide detailed evaluations of nominal associations at both local and global levels. We also define a general class of global association measures which embraces the well known association measure by Goodman-Kruskal (1954). The proposed association matrix also gives rise to the expected generalized confusion matrix in classification. The hierarchy of equivalence relations defined by the association vector and matrix are also shown. version:2
arxiv-1209-5019 | A Bayesian Nonparametric Approach to Image Super-resolution | http://arxiv.org/abs/1209.5019 | id:1209.5019 author:Gungor Polatkan, Mingyuan Zhou, Lawrence Carin, David Blei, Ingrid Daubechies category:cs.LG stat.ML  published:2012-09-22 summary:Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data. We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior. However, this algorithm is not feasible for large-scale data. To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries in a fraction of the time needed by the Gibbs sampler. version:1
arxiv-1110-4347 | Is the k-NN classifier in high dimensions affected by the curse of dimensionality? | http://arxiv.org/abs/1110.4347 | id:1110.4347 author:Vladimir Pestov category:stat.ML 62H30  68H05 I.2.6  published:2011-10-19 summary:There is an increasing body of evidence suggesting that exact nearest neighbour search in high-dimensional spaces is affected by the curse of dimensionality at a fundamental level. Does it necessarily mean that the same is true for k nearest neighbours based learning algorithms such as the k-NN classifier? We analyse this question at a number of levels and show that the answer is different at each of them. As our first main observation, we show the consistency of a k approximate nearest neighbour classifier. However, the performance of the classifier in very high dimensions is provably unstable. As our second main observation, we point out that the existing model for statistical learning is oblivious of dimension of the domain and so every learning problem admits a universally consistent deterministic reduction to the one-dimensional case by means of a Borel isomorphism. version:3
arxiv-1207-5554 | Bellman Error Based Feature Generation using Random Projections on Sparse Spaces | http://arxiv.org/abs/1207.5554 | id:1207.5554 author:Mahdi Milani Fard, Yuri Grinberg, Amir-massoud Farahmand, Joelle Pineau, Doina Precup category:cs.LG stat.ML  published:2012-07-23 summary:We address the problem of automatic generation of features for value function approximation. Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error. Empirical results demonstrate the strength of this method. version:3
arxiv-1209-4887 | A Note on the SPICE Method | http://arxiv.org/abs/1209.4887 | id:1209.4887 author:Cristian R. Rojas, Dimitrios Katselis, Håkan Hjalmarsson category:stat.ML cs.SY  published:2012-09-21 summary:In this article, we analyze the SPICE method developed in [1], and establish its connections with other standard sparse estimation methods such as the Lasso and the LAD-Lasso. This result positions SPICE as a computationally efficient technique for the calculation of Lasso-type estimators. Conversely, this connection is very useful for establishing the asymptotic properties of SPICE under several problem scenarios and for suggesting suitable modifications in cases where the naive version of SPICE would not work. version:1
arxiv-1209-4120 | Scaling Multidimensional Inference for Structured Gaussian Processes | http://arxiv.org/abs/1209.4120 | id:1209.4120 author:Elad Gilboa, Yunus Saatçi, John P. Cunningham category:stat.ML  published:2012-09-18 summary:Exact Gaussian Process (GP) regression has O(N^3) runtime for data size N, making it intractable for large N. Many algorithms for improving GP scaling approximate the covariance with lower rank matrices. Other work has exploited structure inherent in particular covariance functions, including GPs with implied Markov structure, and equispaced inputs (both enable O(N) runtime). However, these GP advances have not been extended to the multidimensional input setting, despite the preponderance of multidimensional applications. This paper introduces and tests novel extensions of structured GPs to multidimensional inputs. We present new methods for additive GPs, showing a novel connection between the classic backfitting method and the Bayesian framework. To achieve optimal accuracy-complexity tradeoff, we extend this model with a novel variant of projection pursuit regression. Our primary result -- projection pursuit Gaussian Process Regression -- shows orders of magnitude speedup while preserving high accuracy. The natural second and third steps include non-Gaussian observations and higher dimensional equispaced grid methods. We introduce novel techniques to address both of these necessary directions. We thoroughly illustrate the power of these three advances on several datasets, achieving close performance to the naive Full GP at orders of magnitude less cost. version:2
arxiv-1201-5338 | On Constrained Spectral Clustering and Its Applications | http://arxiv.org/abs/1201.5338 | id:1201.5338 author:Xiang Wang, Buyue Qian, Ian Davidson category:cs.LG stat.ML H.2.8  published:2012-01-25 summary:Constrained clustering has been well-studied for algorithms such as $K$-means and hierarchical clustering. However, how to satisfy many constraints in these algorithmic settings has been shown to be intractable. One alternative to encode many constraints is to use spectral clustering, which remains a developing area. In this paper, we propose a flexible framework for constrained spectral clustering. In contrast to some previous efforts that implicitly encode Must-Link and Cannot-Link constraints by modifying the graph Laplacian or constraining the underlying eigenspace, we present a more natural and principled formulation, which explicitly encodes the constraints as part of a constrained optimization problem. Our method offers several practical advantages: it can encode the degree of belief in Must-Link and Cannot-Link constraints; it guarantees to lower-bound how well the given constraints are satisfied using a user-specified threshold; it can be solved deterministically in polynomial time through generalized eigendecomposition. Furthermore, by inheriting the objective function from spectral clustering and encoding the constraints explicitly, much of the existing analysis of unconstrained spectral clustering techniques remains valid for our formulation. We validate the effectiveness of our approach by empirical results on both artificial and real datasets. We also demonstrate an innovative use of encoding large number of constraints: transfer learning via constraints. version:2
arxiv-1209-4551 | Probabilistic Auto-Associative Models and Semi-Linear PCA | http://arxiv.org/abs/1209.4551 | id:1209.4551 author:Serge Iovleff category:stat.AP stat.ML  published:2012-09-20 summary:Auto-Associative models cover a large class of methods used in data analysis. In this paper, we describe the generals properties of these models when the projection component is linear and we propose and test an easy to implement Probabilistic Semi-Linear Auto- Associative model in a Gaussian setting. We show it is a generalization of the PCA model to the semi-linear case. Numerical experiments on simulated datasets and a real astronomical application highlight the interest of this approach version:1
arxiv-1209-4855 | The Future of Neural Networks | http://arxiv.org/abs/1209.4855 | id:1209.4855 author:Sachin Lakra, T. V. Prasad, G. Ramakrishna category:cs.NE  published:2012-09-20 summary:The paper describes some recent developments in neural networks and discusses the applicability of neural networks in the development of a machine that mimics the human brain. The paper mentions a new architecture, the pulsed neural network that is being considered as the next generation of neural networks. The paper also explores the use of memristors in the development of a brain-like computer called the MoNETA. A new model, multi/infinite dimensional neural networks, are a recent development in the area of advanced neural networks. The paper concludes that the need of neural networks in the development of human-like technology is essential and may be non-expendable for it. version:1
arxiv-1209-4895 | A Neuro-Fuzzy Technique for Implementing the Half-Adder Circuit Using the CANFIS Model | http://arxiv.org/abs/1209.4895 | id:1209.4895 author:Sachin Lakra, T. V. Prasad, Deepak Sharma, Shree Harsh Atrey, Anubhav Sharma category:cs.NE  published:2012-09-20 summary:A Neural Network, in general, is not considered to be a good solver of mathematical and binary arithmetic problems. However, networks have been developed for such problems as the XOR circuit. This paper presents a technique for the implementation of the Half-adder circuit using the CoActive Neuro-Fuzzy Inference System (CANFIS) Model and attempts to solve the problem using the NeuroSolutions 5 Simulator. The paper gives the experimental results along with the interpretations and possible applications of the technique. version:1
arxiv-1209-4471 | Stemmer for Serbian language | http://arxiv.org/abs/1209.4471 | id:1209.4471 author:Nikola Milošević category:cs.CL cs.IR  published:2012-09-20 summary:In linguistic morphology and information retrieval, stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form; generally a written word form. In this work is presented suffix stripping stemmer for Serbian language, one of the highly inflectional languages. version:1
arxiv-1209-4420 | An Efficient Color Face Verification Based on 2-Directional 2-Dimensional Feature Extraction | http://arxiv.org/abs/1209.4420 | id:1209.4420 author:Lan-Ting LI category:cs.CV  published:2012-09-20 summary:A novel and uniform framework for face verification is presented in this paper. First of all, a 2-directional 2-dimensional feature extraction method is adopted to extract client-specific template - 2D discrimant projection matrix. Then the face skin color information is utilized as an additive feature to enhance decision making strategy that makes use of not only 2D grey feature but also 2D skin color feature. A fusion decision of both is applied to experiment the performance on the XM2VTS database according to Lausanne protocol. Experimental results show that the framework achieves high verification accuracy and verification speed. version:1
arxiv-1209-4419 | Head Frontal-View Identification Using Extended LLE | http://arxiv.org/abs/1209.4419 | id:1209.4419 author:Chao Wang category:cs.CV  published:2012-09-20 summary:Automatic head frontal-view identification is challenging due to appearance variations caused by pose changes, especially without any training samples. In this paper, we present an unsupervised algorithm for identifying frontal view among multiple facial images under various yaw poses (derived from the same person). Our approach is based on Locally Linear Embedding (LLE), with the assumption that with yaw pose being the only variable, the facial images should lie in a smooth and low dimensional manifold. We horizontally flip the facial images and present two K-nearest neighbor protocols for the original images and the flipped images, respectively. In the proposed extended LLE, for any facial image (original or flipped one), we search (1) the Ko nearest neighbors among the original facial images and (2) the Kf nearest neighbors among the flipped facial images to construct the same neighborhood graph. The extended LLE eliminates the differences (because of background, face position and scale in the whole image and some asymmetry of left-right face) between the original facial image and the flipped facial image at the same yaw pose so that the flipped facial images can be used effectively. Our approach does not need any training samples as prior information. The experimental results show that the frontal view of head can be identified reliably around the lowest point of the pose manifold for multiple facial images, especially the cropped facial images (little background and centered face). version:1
arxiv-1205-4450 | Spectral Graph Cut from a Filtering Point of View | http://arxiv.org/abs/1205.4450 | id:1205.4450 author:Chengxi Ye, Yuxu Lin, Mingli Song, Chun Chen, David W. Jacobs category:cs.CV  published:2012-05-20 summary:We analyze spectral graph theory based image segmentation algorithms and show there is a natural connection with edge preserving filtering. Based on this connection we show that the normalized cut algorithm is equivalent to repeated application of bilateral filtering. Then, using this interpretation we present and implement a fast normalized cut algorithm. Experiments show that our implementation can solve the original optimization problem with a 10x-100x speedup. Furthermore, we show this connection makes possible a new model for segmentation called conditioned normalized cut that easily incorporates image patches in color and demonstrate how this problem can be solved with edge preserving filtering. version:2
arxiv-1209-4317 | Image Super-Resolution via Sparse Bayesian Modeling of Natural Images | http://arxiv.org/abs/1209.4317 | id:1209.4317 author:Haichao Zhang, David Wipf, Yanning Zhang category:cs.CV  published:2012-09-19 summary:Image super-resolution (SR) is one of the long-standing and active topics in image processing community. A large body of works for image super resolution formulate the problem with Bayesian modeling techniques and then obtain its Maximum-A-Posteriori (MAP) solution, which actually boils down to a regularized regression task over separable regularization term. Although straightforward, this approach cannot exploit the full potential offered by the probabilistic modeling, as only the posterior mode is sought. Also, the separable property of the regularization term can not capture any correlations between the sparse coefficients, which sacrifices much on its modeling accuracy. We propose a Bayesian image SR algorithm via sparse modeling of natural images. The sparsity property of the latent high resolution image is exploited by introducing latent variables into the high-order Markov Random Field (MRF) which capture the content adaptive variance by pixel-wise adaptation. The high-resolution image is estimated via Empirical Bayesian estimation scheme, which is substantially faster than our previous approach based on Markov Chain Monte Carlo sampling [1]. It is shown that the actual cost function for the proposed approach actually incorporates a non-factorial regularization term over the sparse coefficients. Experimental results indicate that the proposed method can generate competitive or better results than \emph{state-of-the-art} SR algorithms. version:1
arxiv-1209-4280 | Alpha/Beta Divergences and Tweedie Models | http://arxiv.org/abs/1209.4280 | id:1209.4280 author:Y. Kenan Yilmaz, A. Taylan Cemgil category:stat.ML cs.IT math.IT math.ST stat.TH  published:2012-09-19 summary:We describe the underlying probabilistic interpretation of alpha and beta divergences. We first show that beta divergences are inherently tied to Tweedie distributions, a particular type of exponential family, known as exponential dispersion models. Starting from the variance function of a Tweedie model, we outline how to get alpha and beta divergences as special cases of Csisz\'ar's $f$ and Bregman divergences. This result directly generalizes the well-known relationship between the Gaussian distribution and least squares estimation to Tweedie models and beta divergence minimization. version:1
arxiv-1209-4233 | Writing Reusable Digital Geometry Algorithms in a Generic Image Processing Framework | http://arxiv.org/abs/1209.4233 | id:1209.4233 author:Roland Levillain, Thierry Géraud, Laurent Najman category:cs.MS cs.CV  published:2012-09-18 summary:Digital Geometry software should reflect the generality of the underlying mathe- matics: mapping the latter to the former requires genericity. By designing generic solutions, one can effectively reuse digital geometry data structures and algorithms. We propose an image processing framework focused on the Generic Programming paradigm in which an algorithm on the paper can be turned into a single code, written once and usable with various input types. This approach enables users to design and implement new methods at a lower cost, try cross-domain experiments and help generalize results version:1
arxiv-1209-2352 | Feature Specific Sentiment Analysis for Product Reviews | http://arxiv.org/abs/1209.2352 | id:1209.2352 author:Subhabrata Mukherjee, Pushpak Bhattacharyya category:cs.IR cs.CL  published:2012-09-11 summary:In this paper, we present a novel approach to identify feature specific expressions of opinion in product reviews with different features and mixed emotions. The objective is realized by identifying a set of potential features in the review and extracting opinion expressions about those features by exploiting their associations. Capitalizing on the view that more closely associated words come together to express an opinion about a certain feature, dependency parsing is used to identify relations between the opinion expressions. The system learns the set of significant relations to be used by dependency parsing and a threshold parameter which allows us to merge closely associated opinion expressions. The data requirement is minimal as this is a one time learning of the domain independent parameters. The associations are represented in the form of a graph which is partitioned to finally retrieve the opinion expression describing the user specified feature. We show that the system achieves a high accuracy across all domains and performs at par with state-of-the-art systems despite its data limitations. version:2
arxiv-1209-2493 | WikiSent : Weakly Supervised Sentiment Analysis Through Extractive Summarization With Wikipedia | http://arxiv.org/abs/1209.2493 | id:1209.2493 author:Subhabrata Mukherjee, Pushpak Bhattacharyya category:cs.IR cs.CL  published:2012-09-12 summary:This paper describes a weakly supervised system for sentiment analysis in the movie review domain. The objective is to classify a movie review into a polarity class, positive or negative, based on those sentences bearing opinion on the movie alone. The irrelevant text, not directly related to the reviewer opinion on the movie, is left out of analysis. Wikipedia incorporates the world knowledge of movie-specific features in the system which is used to obtain an extractive summary of the review, consisting of the reviewer's opinions about the specific aspects of the movie. This filters out the concepts which are irrelevant or objective with respect to the given movie. The proposed system, WikiSent, does not require any labeled data for training. The only weak supervision arises out of the usage of resources like WordNet, Part-of-Speech Tagger and Sentiment Lexicons by virtue of their construction. WikiSent achieves a considerable accuracy improvement over the baseline and has a better or comparable accuracy to the existing semi-supervised and unsupervised systems in the domain, on the same dataset. We also perform a general movie review trend analysis using WikiSent to find the trend in movie-making and the public acceptance in terms of movie genre, year of release and polarity. version:2
arxiv-1209-2495 | TwiSent: A Multistage System for Analyzing Sentiment in Twitter | http://arxiv.org/abs/1209.2495 | id:1209.2495 author:Subhabrata Mukherjee, Akshat Malu, A. R. Balamurali, Pushpak Bhattacharyya category:cs.IR cs.CL  published:2012-09-12 summary:In this paper, we present TwiSent, a sentiment analysis system for Twitter. Based on the topic searched, TwiSent collects tweets pertaining to it and categorizes them into the different polarity classes positive, negative and objective. However, analyzing micro-blog posts have many inherent challenges compared to the other text genres. Through TwiSent, we address the problems of 1) Spams pertaining to sentiment analysis in Twitter, 2) Structural anomalies in the text in the form of incorrect spellings, nonstandard abbreviations, slangs etc., 3) Entity specificity in the context of the topic searched and 4) Pragmatics embedded in text. The system performance is evaluated on manually annotated gold standard data and on an automatically annotated tweet set based on hashtags. It is a common practise to show the efficacy of a supervised system on an automatically annotated dataset. However, we show that such a system achieves lesser classification accurcy when tested on generic twitter dataset. We also show that our system performs much better than an existing system. version:2
arxiv-1209-2341 | Leveraging Sentiment to Compute Word Similarity | http://arxiv.org/abs/1209.2341 | id:1209.2341 author:A. R. Balamurali, Subhabrata Mukherjee, Akshat Malu, Pushpak Bhattacharyya category:cs.IR cs.CL  published:2012-09-11 summary:In this paper, we introduce a new WordNet based similarity metric, SenSim, which incorporates sentiment content (i.e., degree of positive or negative sentiment) of the words being compared to measure the similarity between them. The proposed metric is based on the hypothesis that knowing the sentiment is beneficial in measuring the similarity. To verify this hypothesis, we measure and compare the annotator agreement for 2 annotation strategies: 1) sentiment information of a pair of words is considered while annotating and 2) sentiment information of a pair of words is not considered while annotating. Inter-annotator correlation scores show that the agreement is better when the two annotators consider sentiment information while assigning a similarity score to a pair of words. We use this hypothesis to measure the similarity between a pair of words. Specifically, we represent each word as a vector containing sentiment scores of all the content words in the WordNet gloss of the sense of that word. These sentiment scores are derived from a sentiment lexicon. We then measure the cosine similarity between the two vectors. We perform both intrinsic and extrinsic evaluation of SenSim and compare the performance with other widely usedWordNet similarity metrics. version:2
arxiv-1207-1655 | Robust Online Hamiltonian Learning | http://arxiv.org/abs/1207.1655 | id:1207.1655 author:Christopher E. Granade, Christopher Ferrie, Nathan Wiebe, D. G. Cory category:quant-ph cs.LG  published:2012-07-06 summary:In this work we combine two distinct machine learning methodologies, sequential Monte Carlo and Bayesian experimental design, and apply them to the problem of inferring the dynamical parameters of a quantum system. We design the algorithm with practicality in mind by including parameters that control trade-offs between the requirements on computational and experimental resources. The algorithm can be implemented online (during experimental data collection), avoiding the need for storage and post-processing. Most importantly, our algorithm is capable of learning Hamiltonian parameters even when the parameters change from experiment-to-experiment, and also when additional noise processes are present and unknown. The algorithm also numerically estimates the Cramer-Rao lower bound, certifying its own performance. version:2
arxiv-1209-3761 | Generalized Canonical Correlation Analysis for Disparate Data Fusion | http://arxiv.org/abs/1209.3761 | id:1209.3761 author:Ming Sun, Carey E. Priebe, Minh Tang category:stat.ML cs.LG  published:2012-09-17 summary:Manifold matching works to identify embeddings of multiple disparate data spaces into the same low-dimensional space, where joint inference can be pursued. It is an enabling methodology for fusion and inference from multiple and massive disparate data sources. In this paper we focus on a method called Canonical Correlation Analysis (CCA) and its generalization Generalized Canonical Correlation Analysis (GCCA), which belong to the more general Reduced Rank Regression (RRR) framework. We present an efficiency investigation of CCA and GCCA under different training conditions for a particular text document classification task. version:1
arxiv-1209-3694 | Submodularity in Batch Active Learning and Survey Problems on Gaussian Random Fields | http://arxiv.org/abs/1209.3694 | id:1209.3694 author:Yifei Ma, Roman Garnett, Jeff Schneider category:cs.LG cs.AI cs.DS  published:2012-09-17 summary:Many real-world datasets can be represented in the form of a graph whose edge weights designate similarities between instances. A discrete Gaussian random field (GRF) model is a finite-dimensional Gaussian process (GP) whose prior covariance is the inverse of a graph Laplacian. Minimizing the trace of the predictive covariance Sigma (V-optimality) on GRFs has proven successful in batch active learning classification problems with budget constraints. However, its worst-case bound has been missing. We show that the V-optimality on GRFs as a function of the batch query set is submodular and hence its greedy selection algorithm guarantees an (1-1/e) approximation ratio. Moreover, GRF models have the absence-of-suppressor (AofS) condition. For active survey problems, we propose a similar survey criterion which minimizes 1'(Sigma)1. In practice, V-optimality criterion performs better than GPs with mutual information gain criteria and allows nonuniform costs for different nodes. version:1
arxiv-1108-5838 | Off-grid Direction of Arrival Estimation Using Sparse Bayesian Inference | http://arxiv.org/abs/1108.5838 | id:1108.5838 author:Zai Yang, Lihua Xie, Cishen Zhang category:stat.AP cs.IT math.IT stat.ML  published:2011-08-30 summary:Direction of arrival (DOA) estimation is a classical problem in signal processing with many practical applications. Its research has recently been advanced owing to the development of methods based on sparse signal reconstruction. While these methods have shown advantages over conventional ones, there are still difficulties in practical situations where true DOAs are not on the discretized sampling grid. To deal with such an off-grid DOA estimation problem, this paper studies an off-grid model that takes into account effects of the off-grid DOAs and has a smaller modeling error. An iterative algorithm is developed based on the off-grid model from a Bayesian perspective while joint sparsity among different snapshots is exploited by assuming a Laplace prior for signals at all snapshots. The new approach applies to both single snapshot and multi-snapshot cases. Numerical simulations show that the proposed algorithm has improved accuracy in terms of mean squared estimation error. The algorithm can maintain high estimation accuracy even under a very coarse sampling grid. version:4
arxiv-1109-0258 | Nonconvex proximal splitting: batch and incremental algorithms | http://arxiv.org/abs/1109.0258 | id:1109.0258 author:Suvrit Sra category:math.OC stat.ML  published:2011-09-01 summary:Within the unmanageably large class of nonconvex optimization, we consider the rich subclass of nonsmooth problems that have composite objectives---this already includes the extensively studied convex, composite objective problems as a special case. For this subclass, we introduce a powerful, new framework that permits asymptotically non-vanishing perturbations. In particular, we develop perturbation-based batch and incremental (online like) nonconvex proximal splitting algorithms. To our knowledge, this is the first time that such perturbation-based nonconvex splitting algorithms are being proposed and analyzed. While the main contribution of the paper is the theoretical framework, we complement our results by presenting some empirical results on matrix factorization. version:2
arxiv-1206-5533 | Practical recommendations for gradient-based training of deep architectures | http://arxiv.org/abs/1206.5533 | id:1206.5533 author:Yoshua Bengio category:cs.LG  published:2012-06-24 summary:Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures. version:2
arxiv-1208-5333 | A hybrid ACO approach to the Matrix Bandwidth Minimization Problem | http://arxiv.org/abs/1208.5333 | id:1208.5333 author:Camelia-M. Pintea, Camelia Chira, Gloria-C. Crisan category:cs.AI cs.NE 68T20  published:2012-08-27 summary:The evolution of the human society raises more and more difficult endeavors. For some of the real-life problems, the computing time-restriction enhances their complexity. The Matrix Bandwidth Minimization Problem (MBMP) seeks for a simultaneous permutation of the rows and the columns of a square matrix in order to keep its nonzero entries close to the main diagonal. The MBMP is a highly investigated P-complete problem, as it has broad applications in industry, logistics, artificial intelligence or information recovery. This paper describes a new attempt to use the Ant Colony Optimization framework in tackling MBMP. The introduced model is based on the hybridization of the Ant Colony System technique with new local search mechanisms. Computational experiments confirm a good performance of the proposed algorithm for the considered set of MBMP instances. version:2
arxiv-1207-4676 | Proceedings of the 29th International Conference on Machine Learning (ICML-12) | http://arxiv.org/abs/1207.4676 | id:1207.4676 author:John Langford, Joelle Pineau category:cs.LG stat.ML  published:2012-07-19 summary:This is an index to the papers that appear in the Proceedings of the 29th International Conference on Machine Learning (ICML-12). The conference was held in Edinburgh, Scotland, June 27th - July 3rd, 2012. version:2
arxiv-1209-3433 | A Hajj And Umrah Location Classification System For Video Crowded Scenes | http://arxiv.org/abs/1209.3433 | id:1209.3433 author:Hossam M. Zawbaa, Salah A. Aly, Adnan A. Gutub category:cs.CV cs.CY cs.LG  published:2012-09-15 summary:In this paper, a new automatic system for classifying ritual locations in diverse Hajj and Umrah video scenes is investigated. This challenging subject has mostly been ignored in the past due to several problems one of which is the lack of realistic annotated video datasets. HUER Dataset is defined to model six different Hajj and Umrah ritual locations[26]. The proposed Hajj and Umrah ritual location classifying system consists of four main phases: Preprocessing, segmentation, feature extraction, and location classification phases. The shot boundary detection and background/foregroud segmentation algorithms are applied to prepare the input video scenes into the KNN, ANN, and SVM classifiers. The system improves the state of art results on Hajj and Umrah location classifications, and successfully recognizes the six Hajj rituals with more than 90% accuracy. The various demonstrated experiments show the promising results. version:1
arxiv-1108-3605 | Hierarchical Object Parsing from Structured Noisy Point Clouds | http://arxiv.org/abs/1108.3605 | id:1108.3605 author:Adrian Barbu category:cs.CV  published:2011-08-18 summary:Object parsing and segmentation from point clouds are challenging tasks because the relevant data is available only as thin structures along object boundaries or other features, and is corrupted by large amounts of noise. To handle this kind of data, flexible shape models are desired that can accurately follow the object boundaries. Popular models such as Active Shape and Active Appearance models lack the necessary flexibility for this task, while recent approaches such as the Recursive Compositional Models make model simplifications in order to obtain computational guarantees. This paper investigates a hierarchical Bayesian model of shape and appearance in a generative setting. The input data is explained by an object parsing layer, which is a deformation of a hidden PCA shape model with Gaussian prior. The paper also introduces a novel efficient inference algorithm that uses informed data-driven proposals to initialize local searches for the hidden variables. Applied to the problem of object parsing from structured point clouds such as edge detection images, the proposed approach obtains state of the art parsing errors on two standard datasets without using any intensity information. version:2
arxiv-1209-3353 | Further Optimal Regret Bounds for Thompson Sampling | http://arxiv.org/abs/1209.3353 | id:1209.3353 author:Shipra Agrawal, Navin Goyal category:cs.LG cs.DS stat.ML 68W40  68Q25 F.2.0  published:2012-09-15 summary:Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that simultaneously proves both the optimal problem-dependent bound of $(1+\epsilon)\sum_i \frac{\ln T}{\Delta_i}+O(\frac{N}{\epsilon^2})$ and the first near-optimal problem-independent bound of $O(\sqrt{NT\ln T})$ on the expected regret of this algorithm. Our near-optimal problem-independent bound solves a COLT 2012 open problem of Chapelle and Li. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are conceptually simple, easily extend to distributions other than the Beta distribution, and also extend to the more general contextual bandits setting [Manuscript, Agrawal and Goyal, 2012]. version:1
arxiv-1209-3230 | Link Prediction in Graphs with Autoregressive Features | http://arxiv.org/abs/1209.3230 | id:1209.3230 author:Emile Richard, Stephane Gaiffas, Nicolas Vayatis category:stat.ML  published:2012-09-14 summary:In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices which takes into account both sparsity and low rank properties of the matrices. Oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. The estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm. version:1
arxiv-1209-1360 | Multiclass Learning with Simplex Coding | http://arxiv.org/abs/1209.1360 | id:1209.1360 author:Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, Jean-Jacques Slotine category:stat.ML cs.LG  published:2012-09-06 summary:In this paper we discuss a novel framework for multiclass learning, defined by a suitable coding/decoding strategy, namely the simplex coding, that allows to generalize to multiple classes a relaxation approach commonly used in binary classification. In this framework, a relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized method with training/tuning complexity which is independent to the number of classes. Tools from convex analysis are introduced that can be used beyond the scope of this paper. version:2
arxiv-1209-3150 | Agent-based Exploration of Wirings of Biological Neural Networks: Position Paper | http://arxiv.org/abs/1209.3150 | id:1209.3150 author:Önder Gürcan, Oğuz Dikenelli, Kemal S. Türker category:cs.NE q-bio.NC  published:2012-09-14 summary:The understanding of human central nervous system depends on knowledge of its wiring. However, there are still gaps in our understanding of its wiring due to technical difficulties. While some information is coming out from human experiments, medical research is lacking of simulation models to put current findings together to obtain the global picture and to predict hypotheses to lead future experiments. Agent-based modeling and simulation (ABMS) is a strong candidate for the simulation model. In this position paper, we discuss the current status of "neural wiring" and "ABMS in biological systems". In particular, we discuss that the ABMS context provides features required for exploration of biological neural wiring. version:1
arxiv-1209-3129 | Analog readout for optical reservoir computers | http://arxiv.org/abs/1209.3129 | id:1209.3129 author:Anteo Smerieri, François Duport, Yvan Paquot, Benjamin Schrauwen, Marc Haelterman, Serge Massar category:cs.ET cs.LG cs.NE physics.optics  published:2012-09-14 summary:Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers. version:1
arxiv-1209-3126 | Beyond Stemming and Lemmatization: Ultra-stemming to Improve Automatic Text Summarization | http://arxiv.org/abs/1209.3126 | id:1209.3126 author:Juan-Manuel Torres-Moreno category:cs.IR cs.CL  published:2012-09-14 summary:In Automatic Text Summarization, preprocessing is an important phase to reduce the space of textual representation. Classically, stemming and lemmatization have been widely used for normalizing words. However, even using normalization on large texts, the curse of dimensionality can disturb the performance of summarizers. This paper describes a new method for normalization of words to further reduce the space of representation. We propose to reduce each word to its initial letters, as a form of Ultra-stemming. The results show that Ultra-stemming not only preserve the content of summaries produced by this representation, but often the performances of the systems can be dramatically improved. Summaries on trilingual corpora were evaluated automatically with Fresa. Results confirm an increase in the performance, regardless of summarizer system used. version:1
arxiv-1209-3113 | Detection and Classification of Viewer Age Range Smart Signs at TV Broadcast | http://arxiv.org/abs/1209.3113 | id:1209.3113 author:Baran Tander, Atilla Özmen, Murat Başkan category:cs.CV  published:2012-09-14 summary:In this paper, the identification and classification of Viewer Age Range Smart Signs, designed by the Radio and Television Supreme Council of Turkey, to give age range information for the TV viewers, are realized. Therefore, the automatic detection at the broadcast will be possible, enabling the manufacturing of TV receivers which are sensible to these signs. The most important step at this process is the pattern recognition. Since the symbols that must be identified are circular, various circle detection techniques can be employed. In our study, first, two different circle segmentation methods for still images are analyzed, their advantages and drawbacks are discussed. A popular neural network structure called Multilayer Perceptron is employed for the classification. Afterwards, the same procedures are carried out for streaming video. All of the steps depicted above are realized on a standard PC. version:1
arxiv-1209-3079 | Signal Recovery in Unions of Subspaces with Applications to Compressive Imaging | http://arxiv.org/abs/1209.3079 | id:1209.3079 author:Nikhil Rao, Benjamin Recht, Robert Nowak category:stat.ML math.OC  published:2012-09-14 summary:In applications ranging from communications to genetics, signals can be modeled as lying in a union of subspaces. Under this model, signal coefficients that lie in certain subspaces are active or inactive together. The potential subspaces are known in advance, but the particular set of subspaces that are active (i.e., in the signal support) must be learned from measurements. We show that exploiting knowledge of subspaces can further reduce the number of measurements required for exact signal recovery, and derive universal bounds for the number of measurements needed. The bound is universal in the sense that it only depends on the number of subspaces under consideration, and their orientation relative to each other. The particulars of the subspaces (e.g., compositions, dimensions, extents, overlaps, etc.) does not affect the results we obtain. In the process, we derive sample complexity bounds for the special case of the group lasso with overlapping groups (the latent group lasso), which is used in a variety of applications. Finally, we also show that wavelet transform coefficients of images can be modeled as lying in groups, and hence can be efficiently recovered using group lasso methods. version:1
arxiv-1209-3056 | Parametric Local Metric Learning for Nearest Neighbor Classification | http://arxiv.org/abs/1209.3056 | id:1209.3056 author:Jun Wang, Adam Woznica, Alexandros Kalousis category:cs.LG  published:2012-09-13 summary:We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this "independence" approach delivers an increased flexibility its downside is the considerable risk of overfitting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space. We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several large-scale classification problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a significant manner. version:1
arxiv-1209-2910 | Community Detection in the Labelled Stochastic Block Model | http://arxiv.org/abs/1209.2910 | id:1209.2910 author:Simon Heimlicher, Marc Lelarge, Laurent Massoulié category:cs.SI cs.LG math.PR physics.soc-ph  published:2012-09-13 summary:We consider the problem of community detection from observed interactions between individuals, in the context where multiple types of interaction are possible. We use labelled stochastic block models to represent the observed data, where labels correspond to interaction types. Focusing on a two-community scenario, we conjecture a threshold for the problem of reconstructing the hidden communities in a way that is correlated with the true partition. To substantiate the conjecture, we prove that the given threshold correctly identifies a transition on the behaviour of belief propagation from insensitive to sensitive. We further prove that the same threshold corresponds to the transition in a related inference problem on a tree model from infeasible to feasible. Finally, numerical results using belief propagation for community detection give further support to the conjecture. version:1
arxiv-1209-2903 | A Novel Approach of Harris Corner Detection of Noisy Images using Adaptive Wavelet Thresholding Technique | http://arxiv.org/abs/1209.2903 | id:1209.2903 author:Nilanjan Dey, Pradipti Nandi, Nilanjana Barman category:cs.CV  published:2012-09-13 summary:In this paper we propose a method of corner detection for obtaining features which is required to track and recognize objects within a noisy image. Corner detection of noisy images is a challenging task in image processing. Natural images often get corrupted by noise during acquisition and transmission. Though Corner detection of these noisy images does not provide desired results, hence de-noising is required. Adaptive wavelet thresholding approach is applied for the same. version:1
arxiv-1205-6352 | Generalized sequential tree-reweighted message passing | http://arxiv.org/abs/1205.6352 | id:1205.6352 author:Vladimir Kolmogorov, Thomas Schoenemann category:cs.CV  published:2012-05-29 summary:This paper addresses the problem of approximate MAP-MRF inference in general graphical models. Following [36], we consider a family of linear programming relaxations of the problem where each relaxation is specified by a set of nested pairs of factors for which the marginalization constraint needs to be enforced. We develop a generalization of the TRW-S algorithm [9] for this problem, where we use a decomposition into junction chains, monotonic w.r.t. some ordering on the nodes. This generalizes the monotonic chains in [9] in a natural way. We also show how to deal with nested factors in an efficient way. Experiments show an improvement over min-sum diffusion, MPLP and subgradient ascent algorithms on a number of computer vision and natural language processing problems. version:4
arxiv-1209-2816 | Hirarchical Digital Image Inpainting Using Wavelets | http://arxiv.org/abs/1209.2816 | id:1209.2816 author:S. Padmavathi, B. Priyalakshmi. Dr. K. P. Soman category:cs.CV  published:2012-09-13 summary:Inpainting is the technique of reconstructing unknown or damaged portions of an image in a visually plausible way. Inpainting algorithm automatically fills the damaged region in an image using the information available in undamaged region. Propagation of structure and texture information becomes a challenge as the size of damaged area increases. In this paper, a hierarchical inpainting algorithm using wavelets is proposed. The hierarchical method tries to keep the mask size smaller while wavelets help in handling the high pass structure information and low pass texture information separately. The performance of the proposed algorithm is tested using different factors. The results of our algorithm are compared with existing methods such as interpolation, diffusion and exemplar techniques. version:1
arxiv-1209-2790 | Improving Energy Efficiency in Femtocell Networks: A Hierarchical Reinforcement Learning Framework | http://arxiv.org/abs/1209.2790 | id:1209.2790 author:Xianfu Chen, Honggang Zhang, Tao Chen, Mika Lasanen category:cs.LG  published:2012-09-13 summary:This paper investigates energy efficiency for two-tier femtocell networks through combining game theory and stochastic learning. With the Stackelberg game formulation, a hierarchical reinforcement learning framework is applied to study the joint average utility maximization of macrocells and femtocells subject to the minimum signal-to-interference-plus-noise-ratio requirements. The macrocells behave as the leaders and the femtocells are followers during the learning procedure. At each time step, the leaders commit to dynamic strategies based on the best responses of the followers, while the followers compete against each other with no further information but the leaders' strategy information. In this paper, we propose two learning algorithms to schedule each cell's stochastic power levels, leading by the macrocells. Numerical experiments are presented to validate the proposed studies and show that the two learning algorithms substantially improve the energy efficiency of the femtocell networks. version:1
arxiv-1209-2784 | Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL | http://arxiv.org/abs/1209.2784 | id:1209.2784 author:Nishant A. Mehta, Dongryeol Lee, Alexander G. Gray category:cs.LG stat.ML  published:2012-09-13 summary:Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks. We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging. version:1
arxiv-1205-2584 | Low Complexity Damped Gauss-Newton Algorithms for CANDECOMP/PARAFAC | http://arxiv.org/abs/1205.2584 | id:1205.2584 author:Anh Huy Phan, Petr Tichavský, Andrzej Cichocki category:cs.NA cs.LG math.OC  published:2012-05-11 summary:The damped Gauss-Newton (dGN) algorithm for CANDECOMP/PARAFAC (CP) decomposition can handle the challenges of collinearity of factors and different magnitudes of factors; nevertheless, for factorization of an $N$-D tensor of size $I_1\times I_N$ with rank $R$, the algorithm is computationally demanding due to construction of large approximate Hessian of size $(RT \times RT)$ and its inversion where $T = \sum_n I_n$. In this paper, we propose a fast implementation of the dGN algorithm which is based on novel expressions of the inverse approximate Hessian in block form. The new implementation has lower computational complexity, besides computation of the gradient (this part is common to both methods), requiring the inversion of a matrix of size $NR^2\times NR^2$, which is much smaller than the whole approximate Hessian, if $T \gg NR$. In addition, the implementation has lower memory requirements, because neither the Hessian nor its inverse never need to be stored in their entirety. A variant of the algorithm working with complex valued data is proposed as well. Complexity and performance of the proposed algorithm is compared with those of dGN and ALS with line search on examples of difficult benchmark tensors. version:2
arxiv-1209-2759 | Multi-track Map Matching | http://arxiv.org/abs/1209.2759 | id:1209.2759 author:Adel Javanmard, Maya Haridasan, Li Zhang category:cs.LG cs.DS stat.AP  published:2012-09-13 summary:We study algorithms for matching user tracks, consisting of time-ordered location points, to paths in the road network. Previous work has focused on the scenario where the location data is linearly ordered and consists of fairly dense and regular samples. In this work, we consider the \emph{multi-track map matching}, where the location data comes from different trips on the same route, each with very sparse samples. This captures the realistic scenario where users repeatedly travel on regular routes and samples are sparsely collected, either due to energy consumption constraints or because samples are only collected when the user actively uses a service. In the multi-track problem, the total set of combined locations is only partially ordered, rather than globally ordered as required by previous map-matching algorithms. We propose two methods, the iterative projection scheme and the graph Laplacian scheme, to solve the multi-track problem by using a single-track map-matching subroutine. We also propose a boosting technique which may be applied to either approach to improve the accuracy of the estimated paths. In addition, in order to deal with variable sampling rates in single-track map matching, we propose a method based on a particular regularized cost function that can be adapted for different sampling rates and measurement errors. We evaluate the effectiveness of our techniques for reconstructing tracks under several different configurations of sampling error and sampling rate. version:1
