arxiv-1603-01185 | Evolving Boolean Regulatory Networks with Variable Gene Expression Times | http://arxiv.org/abs/1603.01185 | id:1603.01185 author:Larry Bull category:q-bio.BM cs.NE q-bio.MN  published:2016-03-02 summary:The time taken for gene expression varies not least because proteins vary in length considerably. This paper uses an abstract, tuneable Boolean regulatory network model to explore gene expression time variation. In particular, it is shown how non-uniform expression times can emerge under certain conditions through simulated evolution. That is, gene expression time variance appears beneficial in the shaping of the dynamical behaviour of the regulatory network without explicit consideration of protein function. version:2
arxiv-1305-4778 | Zero-sum repeated games: Counterexamples to the existence of the asymptotic value and the conjecture $\operatorname{maxmin}=\operatorname{lim}v_n$ | http://arxiv.org/abs/1305.4778 | id:1305.4778 author:Bruno Ziliotto category:math.OC cs.LG  published:2013-05-21 summary:Mertens [In Proceedings of the International Congress of Mathematicians (Berkeley, Calif., 1986) (1987) 1528-1577 Amer. Math. Soc.] proposed two general conjectures about repeated games: the first one is that, in any two-person zero-sum repeated game, the asymptotic value exists, and the second one is that, when Player 1 is more informed than Player 2, in the long run Player 1 is able to guarantee the asymptotic value. We disprove these two long-standing conjectures by providing an example of a zero-sum repeated game with public signals and perfect observation of the actions, where the value of the $\lambda$-discounted game does not converge when $\lambda$ goes to 0. The aforementioned example involves seven states, two actions and two signals for each player. Remarkably, players observe the payoffs, and play in turn. version:4
arxiv-1603-04628 | Accelerating a hybrid continuum-atomistic fluidic model with on-the-fly machine learning | http://arxiv.org/abs/1603.04628 | id:1603.04628 author:David Stephenson, James R Kermode, Duncan A Lockerby category:physics.flu-dyn cond-mat.mes-hall stat.ML  published:2016-03-15 summary:We present a hybrid continuum-atomistic scheme which combines molecular dynamics (MD) simulations with on-the-fly machine learning techniques for the accurate and efficient prediction of multiscale fluidic systems. By using a Gaussian process as a surrogate model for the computationally expensive MD simulations, we use Bayesian inference to predict the system behaviour at the atomistic scale, purely by consideration of the macroscopic inputs and outputs. Whenever the uncertainty of this prediction is greater than a predetermined acceptable threshold, a new MD simulation is performed to continually augment the database, which is never required to be complete. This provides a substantial enhancement to the current generation of hybrid methods, which often require many similar atomistic simulations to be performed, discarding information after it is used once. We apply our hybrid scheme to nano-confined unsteady flow through a high-aspect-ratio converging-diverging channel, and make comparisons between the new scheme and full MD simulations for a range of uncertainty thresholds and initial databases. For low thresholds, our hybrid solution is highly accurate\,---\,within the thermal noise of a full MD simulation. As the uncertainty threshold is raised, the accuracy of our scheme decreases and the computational speed-up increases (relative to a full MD simulation), enabling the compromise between precision and efficiency to be tuned. The speed-up of our hybrid solution ranges from an order of magnitude, with no initial database, to cases where an extensive initial database ensures no new MD simulations are required. version:1
arxiv-1603-04619 | Image Co-localization by Mimicking a Good Detector's Confidence Score Distribution | http://arxiv.org/abs/1603.04619 | id:1603.04619 author:Yao Li, Linqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2016-03-15 summary:Given a set of images containing objects from the same category, the task of image co-localization is to identify and localize each instance. This paper shows that this problem can be solved by a simple but intriguing idea, that is, a common object detector can be learnt by making its detection confidence scores distributed like those of a strongly supervised detector. More specifically, we observe that given a set of object proposals extracted from an image that contains the object of interest, an accurate strongly supervised object detector should give high scores to only a small minority of proposals, and low scores to most of them. Thus, we devise an entropy-based objective function to enforce the above property when learning the common object detector. Once the detector is learnt, we resort to a segmentation approach to refine the localization. We show that despite its simplicity, our approach outperforms state-of-the-art methods. version:1
arxiv-1603-04614 | Scalable Image Retrieval by Sparse Product Quantization | http://arxiv.org/abs/1603.04614 | id:1603.04614 author:Qingqun Ning, Jianke Zhu, Zhiyuan Zhong, Steven C. H. Hoi, Chun Chen category:cs.CV  published:2016-03-15 summary:Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional feature indexing and retrieval is the crux of large-scale image retrieval. A recent promising technique is Product Quantization, which attempts to index high-dimensional image features by decomposing the feature space into a Cartesian product of low dimensional subspaces and quantizing each of them separately. Despite the promising results reported, their quantization approach follows the typical hard assignment of traditional quantization methods, which may result in large quantization errors and thus inferior search performance. Unlike the existing approaches, in this paper, we propose a novel approach called Sparse Product Quantization (SPQ) to encoding the high-dimensional feature vectors into sparse representation. We optimize the sparse representations of the feature vectors by minimizing their quantization errors, making the resulting representation is essentially close to the original data in practice. Experiments show that the proposed SPQ technique is not only able to compress data, but also an effective encoding technique. We obtain state-of-the-art results for ANN search on four public image datasets and the promising results of content-based image retrieval further validate the efficacy of our proposed method. version:1
arxiv-1603-04588 | Classification with Repulsion Tensors: A Case Study on Face Recognition | http://arxiv.org/abs/1603.04588 | id:1603.04588 author:Hawren Fang category:cs.CV I.5.2; I.4.10  published:2016-03-15 summary:We consider dimensionality reduction methods for face recognition in a supervised setting, using an image-as-matrix representation. A common procedure is to project image matrices into a smaller space in which the recognition is performed. These methods are often called "two-dimensional" in the literature and there exist counterparts that use an image-as-vector representation. When two face images are close to each other in the input space they may remain close after projection - but this is not desirable in the situation when these two images are from different classes, and this often affects the recognition performance. We extend a previously developed `repulsion Laplacean' technique based on adding terms to the objective function with the goal or creation a repulsion energy between such images in the projected space. This scheme, which relies on a repulsion graph, is generic and can be incorporated into various two-dimensional methods. It can be regarded as a multilinear generalization of the repulsion strategy by Kokiopoulou and Saad [Pattern Recog., 42 (2009), pp. 2392--2402]. Experimental results demonstrate that the proposed methodology offers significant recognition improvement relative to the underlying two-dimensional methods. version:1
arxiv-1509-06824 | Model-based Reinforcement Learning with Parametrized Physical Models and Optimism-Driven Exploration | http://arxiv.org/abs/1509.06824 | id:1509.06824 author:Christopher Xie, Sachin Patil, Teodor Moldovan, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO  published:2015-09-23 summary:In this paper, we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control. We use a feature-based representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure, and the features are identified from a high-level specification of the robot's morphology, consisting of the number and connectivity structure of its links. Model predictive control is then used to choose the actions under an optimistic model of the dynamics, which produces an efficient and goal-directed exploration strategy. We present real time experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems. Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods. To evaluate our approach on a realistic robotic control task, we also demonstrate real time control of a simulated 7 degree of freedom arm. version:2
arxiv-1603-04572 | On the exact recovery of sparse signals via conic relaxations | http://arxiv.org/abs/1603.04572 | id:1603.04572 author:Hongbo Dong category:stat.ML math.OC 90C22  90C25  90C90  published:2016-03-15 summary:In this note we compare two recently proposed semidefinite relaxations for the sparse linear regression problem by Pilanci, Wainwright and El Ghaoui (Sparse learning via boolean relaxations, 2015) and Dong, Chen and Linderoth (Relaxation vs. Regularization A conic optimization perspective of statistical variable selection, 2015). We focus on the cardinality constrained formulation, and prove that the relaxation proposed by Dong, etc. is theoretically no weaker than the one proposed by Pilanci, etc. Therefore any sufficient condition of exact recovery derived by Pilanci can be readily applied to the other relaxation, including their results on high probability recovery for Gaussian ensemble. Finally we provide empirical evidence that the relaxation by Dong, etc. requires much fewer observations to guarantee the recovery of true support. version:1
arxiv-1509-06841 | One-Shot Learning of Manipulation Skills with Online Dynamics Adaptation and Neural Network Priors | http://arxiv.org/abs/1509.06841 | id:1509.06841 author:Justin Fu, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO  published:2015-09-23 summary:One of the key challenges in applying reinforcement learning to complex robotic control tasks is the need to gather large amounts of experience in order to find an effective policy for the task at hand. Model-based reinforcement learning can achieve good sample efficiency, but requires the ability to learn a model of the dynamics that is good enough to learn an effective policy. In this work, we develop a modelbased reinforcement learning algorithm that combines prior knowledge from previous tasks with online adaptation of the dynamics model. These two ingredients enable highly sample efficient learning even in regimes where estimating the true dynamics is very difficult, since the online model adaptation allows the method to locally compensate for unmodeled variation in the dynamics. We encode the prior experience into a neural network dynamics model, and adapt it online by progressively refitting a local linear model of the dynamics. Our experimental results show that this approach can be used to solve a variety of complex robotic manipulation tasks in just a single attempt, using prior data from other manipulation behaviors. version:2
arxiv-1602-03570 | Optimized Kernel-based Projection Space of Riemannian Manifolds | http://arxiv.org/abs/1602.03570 | id:1602.03570 author:Azadeh Alavi, Vishal M Patel, Rama Chellappa category:cs.CV  published:2016-02-10 summary:It is proven that encoding images and videos through Symmetric Positive Definite (SPD) matrices, and considering the Riemannian geometry of the resulting space, can lead to increased classification performance. Taking into account manifold geometry is typically done via embedding the manifolds in tangent spaces, or Reproducing Kernel Hilbert Spaces (RKHS). Recently, it was shown that embedding such manifolds into a Random Projection Spaces (RPS), rather than RKHS or tangent space, leads to higher classification and clustering performance. However, based on structure and dimensionality of the randomly generated hyperplanes, the classification performance over RPS may vary significantly. In addition, fine-tuning RPS is data expensive (as it requires validation-data), time consuming, and resource demanding. In this paper, we introduce an approach to learn an optimized kernel-based projection (with fixed dimensionality), by employing the concept of subspace clustering. As such, we encode the association of data points to the underlying subspace of each point, to generate meaningful hyperplanes. Further, we adopt the concept of dictionary learning and sparse coding, and discriminative analysis, for the optimized kernel-based projection space (OPS) on SPD manifolds. We validate our algorithm on several classification tasks. The experiment results also demonstrate that the proposed method outperforms state-of-the-art methods on such manifolds. version:3
arxiv-1603-04553 | Unsupervised Ranking Model for Entity Coreference Resolution | http://arxiv.org/abs/1603.04553 | id:1603.04553 author:Xuezhe Ma, Zhengzhong Liu, Eduard Hovy category:cs.CL cs.LG  published:2016-03-15 summary:Coreference resolution is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community. In this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables. Our unsupervised system achieves 58.44% F1 score of the CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), outperforming the Stanford deterministic system (Lee et al., 2013) by 3.01%. version:1
arxiv-1603-04550 | Effective Computer Model For Recognizing Nationality From Frontal Image | http://arxiv.org/abs/1603.04550 | id:1603.04550 author:Bat-Erdene Batsukh, Ganbat Tsend category:cs.CV  published:2016-03-15 summary:We are introducing new effective computer model for extracting nationality from frontal image candidate using face part color, size and distances based on deep research. Determining face part size, color, and distances is depending on a variety of factors including image quality, lighting condition, rotation angle, occlusion and facial emotion. Therefore, first we need to detect a face on the image then convert an image into the real input. After that, we can determine image candidate gender, face shape, key points and face parts. Finally, we will return the result, based on the comparison of sizes and distances with the sample measurement table database. While we were measuring samples, there were big differences between images by their gender and face shapes. Input images must be the frontal face image that has smooth lighting and does not have any rotation angle. The model can be used in military, police, defense, healthcare, and technology sectors. Finally, Computer can distinguish nationality from the face image. version:1
arxiv-1603-04549 | Know Your Customer: Multi-armed Bandits with Capacity Constraints | http://arxiv.org/abs/1603.04549 | id:1603.04549 author:Ramesh Johari, Vijay Kamble, Yash Kanoria category:cs.LG cs.DS stat.ME stat.ML  published:2016-03-15 summary:A wide range of resource allocation and platform operation settings exhibit the following two simultaneous challenges: (1) service resources are capacity constrained; and (2) clients' preferences are not perfectly known. To study this pair of challenges, we consider a service system with heterogeneous servers and clients. Server types are known and there is fixed capacity of servers of each type. Clients arrive over time, with types initially unknown and drawn from some distribution. Each client sequentially brings $N$ jobs before leaving. The system operator assigns each job to some server type, resulting in a payoff whose distribution depends on the client and server types. Our main contribution is a complete characterization of the structure of the optimal policy for maximization of the rate of payoff accumulation. Such a policy must balance three goals: (i) earning immediate payoffs; (ii) learning client types to increase future payoffs; and (iii) satisfying the capacity constraints. We construct a policy that has provably optimal regret (to leading order as $N$ grows large). Our policy has an appealingly simple three-phase structure: a short type-"guessing" phase, a type-"confirmation" phase that balances payoffs with learning, and finally an "exploitation" phase that focuses on payoffs. Crucially, our approach employs the shadow prices of the capacity constraints in the assignment problem with known types as "externality prices" on the servers' capacity. version:1
arxiv-1601-04155 | Brain-Inspired Deep Networks for Image Aesthetics Assessment | http://arxiv.org/abs/1601.04155 | id:1601.04155 author:Zhangyang Wang, Shiyu Chang, Florin Dolcos, Diane Beck, Ding Liu, Thomas S. Huang category:cs.CV cs.LG cs.NE  published:2016-01-16 summary:Image aesthetics assessment has been challenging due to its subjective nature. Inspired by the scientific advances in the human visual perception and neuroaesthetics, we design Brain-Inspired Deep Networks (BDN) for this task. BDN first learns attributes through the parallel supervised pathways, on a variety of selected feature dimensions. A high-level synthesis network is trained to associate and transform those attributes into the overall aesthetics rating. We then extend BDN to predicting the distribution of human ratings, since aesthetics ratings are often subjective. Another highlight is our first-of-its-kind study of label-preserving transformations in the context of aesthetics assessment, which leads to an effective data augmentation approach. Experimental results on the AVA dataset show that our biological inspired and task-specific BDN model gains significantly performance improvement, compared to other state-of-the-art models with the same or higher parameter capacity. version:2
arxiv-1412-5059 | Estimation of Large Covariance and Precision Matrices from Temporally Dependent Observations | http://arxiv.org/abs/1412.5059 | id:1412.5059 author:Hai Shu, Bin Nan category:math.ST stat.ML stat.TH  published:2014-12-16 summary:We consider the estimation of large covariance and precision matrices from high-dimensional sub-Gaussian observations with slowly decaying temporal dependence that is bounded by certain polynomial decay rate. The temporal dependence is allowed to be long-range so with longer memory than those considered in the current literature. The rates of convergence are obtained for the generalized thresholding estimation of covariance and correlation matrices, and for the constrained $\ell_1$ minimization and the $\ell_1$ penalized likelihood estimation of precision matrix. Properties of sparsistency and sign-consistency are also established. A gap-block cross-validation method is proposed for the tuning parameter selection, which performs well in simulations. As our motivating example, we study the brain functional connectivity using resting-state fMRI time series data with long-range temporal dependence. version:4
arxiv-1603-00110 | Robust Multi-body Feature Tracker: A Segmentation-free Approach | http://arxiv.org/abs/1603.00110 | id:1603.00110 author:Pan Ji, Hongdong Li, Mathieu Salzmann, Yiran Zhong category:cs.CV  published:2016-03-01 summary:Feature tracking is a fundamental problem in computer vision, with applications in many computer vision tasks, such as visual SLAM and action recognition. This paper introduces a novel multi-body feature tracker that exploits a multi-body rigidity assumption to improve tracking robustness under a general perspective camera model. A conventional approach to addressing this problem would consist of alternating between solving two subtasks: motion segmentation and feature tracking under rigidity constraints for each segment. This approach, however, requires knowing the number of motions, as well as assigning points to motion groups, which is typically sensitive to the motion estimates. By contrast, here, we introduce a segmentation-free solution to multi-body feature tracking that bypasses the motion assignment step and reduces to solving a series of subproblems with closed-form solutions. Our experiments demonstrate the benefits of our approach in terms of tracking accuracy and robustness to noise. version:2
arxiv-1603-04535 | Domain Adaptation via Maximum Independence of Domain Features | http://arxiv.org/abs/1603.04535 | id:1603.04535 author:Ke Yan, Lu Kou, David Zhang category:cs.CV cs.AI cs.LG  published:2016-03-15 summary:When the distributions of the source and the target domains are different, domain adaptation techniques are needed. For example, in the field of sensors and measurement, discrete and continuous distributional change often exist in data because of instrumental variation and time-varying sensor drift. In this paper, we propose maximum independence domain adaptation (MIDA) to address this problem. Domain features are first defined to describe the background information of a sample, such as the device label and acquisition time. Then, MIDA learns features which have maximal independence with the domain features, so as to reduce the inter-domain discrepancy in distributions. A feature augmentation strategy is designed so that the learned projection is background-specific. Semi-supervised MIDA (SMIDA) extends MIDA by exploiting the label information. The proposed methods can handle not only discrete domains in traditional domain adaptation problems but also continuous distributional change such as the time-varying drift. In addition, they are naturally applicable in supervised/semi-supervised/unsupervised classification or regression problems with multiple domains. This flexibility brings potential for a wide range of applications. The effectiveness of our approaches is verified by experiments on synthetic datasets and four real-world ones on sensors, measurement, and computer vision. version:1
arxiv-1603-04132 | A Novel Method for Extrinsic Calibration of a 2-D Laser-Rangefinder and a Camera | http://arxiv.org/abs/1603.04132 | id:1603.04132 author:Wenbo Dong, Volkan Isler category:cs.CV cs.RO  published:2016-03-14 summary:We present a novel solution for extrinsically calibrating a camera and a Laser Rangefinder (LRF) by computing the transformation between the camera frame and the LRF frame. Our method is applicable for LRFs which measure only a single plane. It does not rely on observing the laser plane in the camera image. Instead, we show that point-to-plane constraints from a single observation of a V-shaped calibration pattern composed of two non-coplanar triangles suffice to uniquely constrain the transformation. Next, we present a method to obtain a solution using point-to-plane constraints from single or multiple observations. Along the way, we also show that previous solutions, in contrast to our method, have inherent ambiguities and therefore must rely on a good initial estimate. Real and synthetic experiments validate our method and show that it achieves better accuracy than previous methods. version:2
arxiv-1603-04530 | Object Contour Detection with a Fully Convolutional Encoder-Decoder Network | http://arxiv.org/abs/1603.04530 | id:1603.04530 author:Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, Ming-Hsuan Yang category:cs.CV cs.LG  published:2016-03-15 summary:We develop a deep learning algorithm for contour detection with a fully convolutional encoder-decoder network. Different from previous low-level edge detection, our algorithm focuses on detecting higher-level object contours. Our network is trained end-to-end on PASCAL VOC with refined ground truth from inaccurate polygon annotations, yielding much higher precision in object contour detection than previous methods. We find that the learned model generalizes well to unseen object classes from the same super-categories on MS COCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning. By combining with the multiscale combinatorial grouping algorithm, our method can generate high-quality segmented object proposals, which significantly advance the state-of-the-art on PASCAL VOC (improving average recall from 0.62 to 0.67) with a relatively small amount of candidates ($\sim$1660 per image). version:1
arxiv-1603-04146 | Saliency Detection for Improving Object Proposals | http://arxiv.org/abs/1603.04146 | id:1603.04146 author:Shuhan Chen, Jindong Li, Xuelong Hu, Ping Zhou category:cs.CV  published:2016-03-14 summary:Object proposals greatly benefit object detection task in recent state-of-the-art works, such as R-CNN [2]. However, the existing object proposals usually have low localization accuracy at high intersection over union threshold. To address it, we apply saliency detection to each bounding box to improve their quality in this paper. We first present a geodesic saliency detection method in contour, which is designed to find closed contours. Then, we apply it to each candidate box with multi-sizes, and refined boxes can be easily produced in the obtained saliency maps which are further used to calculate saliency scores for proposal ranking. Experiments on PASCAL VOC 2007 test dataset demonstrate the proposed refinement approach can greatly improve existing models. version:2
arxiv-1511-04960 | Sample and Filter: Nonparametric Scene Parsing via Efficient Filtering | http://arxiv.org/abs/1511.04960 | id:1511.04960 author:Mohammad Najafi, Sarah Taghavi Namin, Mathieu Salzmann, Lars Petersson category:cs.CV  published:2015-11-16 summary:Scene parsing has attracted a lot of attention in computer vision. While parametric models have proven effective for this task, they cannot easily incorporate new training data. By contrast, nonparametric approaches, which bypass any learning phase and directly transfer the labels from the training data to the query images, can readily exploit new labeled samples as they become available. Unfortunately, because of the computational cost of their label transfer procedures, state-of-the-art nonparametric methods typically filter out most training images to only keep a few relevant ones to label the query. As such, these methods throw away many images that still contain valuable information and generally obtain an unbalanced set of labeled samples. In this paper, we introduce a nonparametric approach to scene parsing that follows a sample-and-filter strategy. More specifically, we propose to sample labeled superpixels according to an image similarity score, which allows us to obtain a balanced set of samples. We then formulate label transfer as an efficient filtering procedure, which lets us exploit more labeled samples than existing techniques. Our experiments evidence the benefits of our approach over state-of-the-art nonparametric methods on two benchmark datasets. version:2
arxiv-1603-04513 | Multichannel Variable-Size Convolution for Sentence Classification | http://arxiv.org/abs/1603.04513 | id:1603.04513 author:Wenpeng Yin, Hinrich Schütze category:cs.CL  published:2016-03-15 summary:We propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification. version:1
arxiv-1511-08228 | Neural GPUs Learn Algorithms | http://arxiv.org/abs/1511.08228 | id:1511.08228 author:Łukasz Kaiser, Ilya Sutskever category:cs.LG cs.NE  published:2015-11-25 summary:Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization. version:3
arxiv-1603-04506 | Conformal Predictors for Compound Activity Prediction | http://arxiv.org/abs/1603.04506 | id:1603.04506 author:Paolo Toccacheli, Ilia Nouretdinov, Alexander Gammerman category:cs.LG  published:2016-03-14 summary:The paper presents an application of Conformal Predictors to a chemoinformatics problem of identifying activities of chemical compounds. The paper addresses some specific challenges of this domain: a large number of compounds (training examples), high-dimensionality of feature space, sparseness and a strong class imbalance. A variant of conformal predictors called Inductive Mondrian Conformal Predictor is applied to deal with these challenges. Results are presented for several non-conformity measures (NCM) extracted from underlying algorithms and different kernels. A number of performance measures are used in order to demonstrate the flexibility of Inductive Mondrian Conformal Predictors in dealing with such a complex set of data. Keywords: Conformal Prediction, Confidence Estimation, Chemoinformatics, Non-Conformity Measure. version:1
arxiv-1508-04395 | End-to-End Attention-based Large Vocabulary Speech Recognition | http://arxiv.org/abs/1508.04395 | id:1508.04395 author:Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, Yoshua Bengio category:cs.CL cs.AI cs.LG cs.NE  published:2015-08-18 summary:Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches. version:2
arxiv-1502-04938 | A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena | http://arxiv.org/abs/1502.04938 | id:1502.04938 author:Arianna Bisazza, Marcello Federico category:cs.CL  published:2015-02-17 summary:Word reordering is one of the most difficult aspects of statistical machine translation (SMT), and an important factor of its quality and efficiency. Despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. Instead, the choice of the optimal approach for a new translation task still seems to be mostly driven by empirical trials. To orientate the reader in this vast and complex research area, we present a comprehensive survey of word reordering viewed as a statistical modeling challenge and as a natural language phenomenon. The survey describes in detail how word reordering is modeled within different string-based and tree-based SMT frameworks and as a stand-alone task, including systematic overviews of the literature in advanced reordering modeling. We then question why some approaches are more successful than others in different language pairs. We argue that, besides measuring the amount of reordering, it is important to understand which kinds of reordering occur in a given language pair. To this end, we conduct a qualitative analysis of word reordering phenomena in a diverse sample of language pairs, based on a large collection of linguistic knowledge. Empirical results in the SMT literature are shown to support the hypothesis that a few linguistic facts can be very useful to anticipate the reordering characteristics of a language pair and to select the SMT framework that best suits them. version:2
arxiv-1603-02644 | Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling | http://arxiv.org/abs/1603.02644 | id:1603.02644 author:Christophe Dupuy, Francis Bach category:cs.LG stat.ML  published:2016-03-08 summary:We study parameter inference in large-scale latent variable models. We first propose an unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. We then propose a novel inference method for the frequentist estimation of parameters, that adapts MCMC methods to online inference of latent variable models with the proper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we provide an extensive set of experiments and comparisons with existing work, where our new approach outperforms all previously proposed methods. In particular, using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods. Moreover, Bayesian inference through variational methods perform poorly, sometimes leading to worse fits with latent variables of higher dimensionality. version:2
arxiv-1603-04416 | Criteria of efficiency for conformal prediction | http://arxiv.org/abs/1603.04416 | id:1603.04416 author:Vladimir Vovk, Valentina Fedorova, Ilia Nouretdinov, Alex Gammerman category:cs.LG 68T05 I.2.6  published:2016-03-14 summary:We study optimal conformity measures for various criteria of efficiency in an idealized setting. This leads to an important class of criteria of efficiency that we call probabilistic; it turns out that the most standard criteria of efficiency used in literature on conformal prediction are not probabilistic. version:1
arxiv-1506-04573 | A New PAC-Bayesian Perspective on Domain Adaptation | http://arxiv.org/abs/1506.04573 | id:1506.04573 author:Pascal Germain, Amaury Habrard, François Laviolette, Emilie Morvant category:stat.ML cs.LG  published:2015-06-15 summary:We study the issue of PAC-Bayesian domain adaptation: We want to learn, from a source domain, a majority vote model dedicated to a target one. Our theoretical contribution brings a new perspective by deriving an upper-bound on the target risk where the distributions' divergence---expressed as a ratio---controls the trade-off between a source error measure and the target voters' disagreement. Our bound suggests that one has to focus on regions where the source data is informative.From this result, we derive a PAC-Bayesian generalization bound, and specialize it to linear classifiers. Then, we infer a learning algorithmand perform experiments on real data. version:3
arxiv-1507-05899 | Sparsity in Multivariate Extremes with Applications to Anomaly Detection | http://arxiv.org/abs/1507.05899 | id:1507.05899 author:Nicolas Goix, Anne Sabourin, Stéphan Clémençon category:stat.ML  published:2015-07-21 summary:Capturing the dependence structure of multivariate extreme events is a major concern in many fields involving the management of risks stemming from multiple sources, e.g. portfolio monitoring, insurance, environmental risk management and anomaly detection. One convenient (non-parametric) characterization of extremal dependence in the framework of multivariate Extreme Value Theory (EVT) is the angular measure, which provides direct information about the probable 'directions' of extremes, that is, the relative contribution of each feature/coordinate of the 'largest' observations. Modeling the angular measure in high dimensional problems is a major challenge for the multivariate analysis of rare events. The present paper proposes a novel methodology aiming at exhibiting a sparsity pattern within the dependence structure of extremes. This is done by estimating the amount of mass spread by the angular measure on representative sets of directions, corresponding to specific sub-cones of $R^d\_+$. This dimension reduction technique paves the way towards scaling up existing multivariate EVT methods. Beyond a non-asymptotic study providing a theoretical validity framework for our method, we propose as a direct application a --first-- anomaly detection algorithm based on multivariate EVT. This algorithm builds a sparse 'normal profile' of extreme behaviours, to be confronted with new (possibly abnormal) extreme observations. Illustrative experimental results provide strong empirical evidence of the relevance of our approach. version:2
arxiv-1504-08291 | Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy? | http://arxiv.org/abs/1504.08291 | id:1504.08291 author:Raja Giryes, Guillermo Sapiro, Alex M. Bronstein category:cs.NE cs.LG stat.ML 62M45 I.5.1  published:2015-04-30 summary:Three important properties of a classification machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks. version:5
arxiv-1603-04392 | Rapid building detection using machine learning | http://arxiv.org/abs/1603.04392 | id:1603.04392 author:Joseph Paul Cohen, Wei Ding, Caitlin Kuhlman, Aijun Chen, Liping Di category:cs.CV  published:2016-03-14 summary:This work describes algorithms for performing discrete object detection, specifically in the case of buildings, where usually only low quality RGB-only geospatial reflective imagery is available. We utilize new candidate search and feature extraction techniques to reduce the problem to a machine learning (ML) classification task. Here we can harness the complex patterns of contrast features contained in training data to establish a model of buildings. We avoid costly sliding windows to generate candidates; instead we innovatively stitch together well known image processing techniques to produce candidates for building detection that cover 80-85% of buildings. Reducing the number of possible candidates is important due to the scale of the problem. Each candidate is subjected to classification which, although linear, costs time and prohibits large scale evaluation. We propose a candidate alignment algorithm to boost classification performance to 80-90% precision with a linear time algorithm and show it has negligible cost. Also, we propose a new concept called a Permutable Haar Mesh (PHM) which we use to form and traverse a search space to recover candidate buildings which were lost in the initial preprocessing phase. version:1
arxiv-1603-04381 | A Ranking Approach to Global Optimization | http://arxiv.org/abs/1603.04381 | id:1603.04381 author:Cédric Malherbe, Emile Contal, Nicolas Vayatis category:stat.ML  published:2016-03-14 summary:In this paper, we consider the problem of maximizing an unknown function f over a compact and convex set using as few observations f(x) as possible. We observe that the optimization of the function f essentially relies on learning the induced bipartite ranking rule of f. Based on this idea, we relate global optimization to bipartite ranking which allows to address problems with high dimensional input space, as well as cases of functions with weak regularity properties. The paper introduces novel meta-algorithms for global optimization which rely on the choice of any bipartite ranking method. Theoretical properties are provided as well as convergence guarantees and equivalences between various optimization methods are obtained as a by-product. Eventually, numerical evidence is given to show that the main algorithm of the paper which adapts empirically to the underlying ranking structure essentially outperforms existing state-of-the-art global optimization algorithms in typical benchmarks. version:1
arxiv-1502-02377 | Sparse Coding with Earth Mover's Distance for Multi-Instance Histogram Representation | http://arxiv.org/abs/1502.02377 | id:1502.02377 author:Mohua Zhang, Jianhua Peng, Xuejie Liu, Jim Jing-Yan Wang category:cs.LG stat.ML  published:2015-02-09 summary:Sparse coding (Sc) has been studied very well as a powerful data representation method. It attempts to represent the feature vector of a data sample by reconstructing it as the sparse linear combination of some basic elements, and a $L_2$ norm distance function is usually used as the loss function for the reconstruction error. In this paper, we investigate using Sc as the representation method within multi-instance learning framework, where a sample is given as a bag of instances, and further represented as a histogram of the quantized instances. We argue that for the data type of histogram, using $L_2$ norm distance is not suitable, and propose to use the earth mover's distance (EMD) instead of $L_2$ norm distance as a measure of the reconstruction error. By minimizing the EMD between the histogram of a sample and the its reconstruction from some basic histograms, a novel sparse coding method is developed, which is refereed as SC-EMD. We evaluate its performances as a histogram representation method in tow multi-instance learning problems --- abnormal image detection in wireless capsule endoscopy videos, and protein binding site retrieval. The encouraging results demonstrate the advantages of the new method over the traditional method using $L_2$ norm distance. version:2
arxiv-1511-06382 | Iterative Refinement of Approximate Posterior for Training Directed Belief Networks | http://arxiv.org/abs/1511.06382 | id:1511.06382 author:R Devon Hjelm, Kyunghyun Cho, Junyoung Chung, Russ Salakhutdinov, Vince Calhoun, Nebojsa Jojic category:cs.LG stat.ML  published:2015-11-19 summary:Recent advances in variational inference that make use of an inference or recognition network for training and evaluating deep directed graphical models have advanced well beyond traditional variational inference and Markov chain Monte Carlo methods. These techniques offer higher flexibility with simpler and faster inference; yet training and evaluation still remains a challenge. We propose a method for improving the per-example approximate posterior by iterative refinement, which can provide notable gains in maximizing the variational lower bound of the log likelihood and works with both continuous and discrete latent variables. We evaluate our approach as a method of training and evaluating directed graphical models. We show that, when used for training, iterative refinement improves the variational lower bound and can also improve the log-likelihood over related methods. We also show that iterative refinement can be used to get a better estimate of the log-likelihood in any directed model trained with mean-field inference. version:4
arxiv-1511-04156 | Neuroprosthetic decoder training as imitation learning | http://arxiv.org/abs/1511.04156 | id:1511.04156 author:Josh Merel, David Carlson, Liam Paninski, John P. Cunningham category:stat.ML cs.LG q-bio.NC  published:2015-11-13 summary:Neuroprosthetic brain-computer interfaces function via an algorithm which decodes neural activity of the user into movements of an end effector, such as a cursor or robotic arm. In practice, the decoder is often learned by updating its parameters while the user performs a task. When the user's intention is not directly observable, recent methods have demonstrated value in training the decoder against a surrogate for the user's intended movement. We describe how training a decoder in this way is a novel variant of an imitation learning problem, where an oracle or expert is employed for supervised training in lieu of direct observations, which are not available. Specifically, we describe how a generic imitation learning meta-algorithm, dataset aggregation (DAgger, [1]), can be adapted to train a generic brain-computer interface. By deriving existing learning algorithms for brain-computer interfaces in this framework, we provide a novel analysis of regret (an important metric of learning efficacy) for brain-computer interfaces. This analysis allows us to characterize the space of algorithmic variants and bounds on their regret rates. Existing approaches for decoder learning have been performed in the cursor control setting, but the available design principles for these decoders are such that it has been impossible to scale them to naturalistic settings. Leveraging our findings, we then offer an algorithm that combines imitation learning with optimal control, which should allow for training of arbitrary effectors for which optimal control can generate goal-oriented control. We demonstrate this novel and general BCI algorithm with simulated neuroprosthetic control of a 26 degree-of-freedom model of an arm, a sophisticated and realistic end effector. version:2
arxiv-1603-04327 | Automatic Discrimination of Color Retinal Images using the Bag of Words Approach | http://arxiv.org/abs/1603.04327 | id:1603.04327 author:Ibrahim Sadek category:cs.CV  published:2016-03-14 summary:Diabetic retinopathy (DR) and age related macular degeneration (ARMD) are among the major causes of visual impairment worldwide. DR is mainly characterized by red spots, namely microaneurysms and bright lesions, specifically exudates whereas ARMD is mainly identified by tiny yellow or white deposits called drusen. Since exudates might be the only manifestation of the early diabetic retinopathy, there is an increase demand for automatic retinopathy diagnosis. Exudates and drusen may share similar appearances, thus discriminating between them is of interest to enhance screening performance. In this research, we investigative the role of bag of words approach in the automatic diagnosis of retinopathy diabetes. We proposed to use a single based and multiple based methods for the construction of the visual dictionary by combining the histogram of word occurrences from each dictionary and building a single histogram. The introduced approach is evaluated for automatic diagnosis of normal and abnormal color fundus images with bright lesions. This approach has been implemented on 430 fundus images, including six publicly available datasets, in addition to one local dataset. The mean accuracies reported are 97.2% and 99.77% for single based and multiple based dictionaries respectively. version:1
arxiv-1603-04319 | Learning Network of Multivariate Hawkes Processes: A Time Series Approach | http://arxiv.org/abs/1603.04319 | id:1603.04319 author:Jalal Etesami, Negar Kiyavash, Kun Zhang, Kushagra Singhal category:cs.LG cs.AI stat.ML  published:2016-03-14 summary:Learning the influence structure of multiple time series data is of great interest to many disciplines. This paper studies the problem of recovering the causal structure in network of multivariate linear Hawkes processes. In such processes, the occurrence of an event in one process affects the probability of occurrence of new events in some other processes. Thus, a natural notion of causality exists between such processes captured by the support of the excitation matrix. We show that the resulting causal influence network is equivalent to the Directed Information graph (DIG) of the processes, which encodes the causal factorization of the joint distribution of the processes. Furthermore, we present an algorithm for learning the support of excitation matrix (or equivalently the DIG). The performance of the algorithm is evaluated on synthesized multivariate Hawkes networks as well as a stock market and MemeTracker real-world dataset. version:1
arxiv-1603-04308 | Diversity in Object Proposals | http://arxiv.org/abs/1603.04308 | id:1603.04308 author:Anton Winschel, Rainer Lienhart, Christian Eggert category:cs.CV  published:2016-03-14 summary:Current top performing object recognition systems build on object proposals as a preprocessing step. Object proposal algorithms are designed to generate candidate regions for generic objects, yet current approaches are limited in capturing the vast variety of object characteristics. In this paper we analyze the error modes of the state-of-the-art Selective Search object proposal algorithm and suggest extensions to broaden its feature diversity in order to mitigate its error modes. We devise an edge grouping algorithm for handling objects without clear boundaries. To further enhance diversity, we incorporate the Edge Boxes proposal algorithm, which is based on fundamentally different principles than Selective Search. The combination of segmentations and edges provides rich image information and feature diversity which is essential for obtaining high quality object proposals for generic objects. For a preset amount of object proposals we achieve considerably better results by using our combination of different strategies than using any single strategy alone. version:1
arxiv-1603-04283 | Universal probability-free conformal prediction | http://arxiv.org/abs/1603.04283 | id:1603.04283 author:Vladimir Vovk, Dusko Pavlovic category:cs.LG 68T05 I.2.6  published:2016-03-14 summary:We construct a universal prediction system in the spirit of Popper's falsifiability and Kolmogorov complexity. This prediction system does not depend on any statistical assumptions, but under the IID assumption it dominates, although in a rather weak sense, conformal prediction. version:1
arxiv-1603-04265 | Dynamic Scene Deblurring using a Locally Adaptive Linear Blur Model | http://arxiv.org/abs/1603.04265 | id:1603.04265 author:Tae Hyun Kim, Seungjun Nah, Kyoung Mu Lee category:cs.CV  published:2016-03-14 summary:State-of-the-art video deblurring methods cannot handle blurry videos recorded in dynamic scenes, since they are built under a strong assumption that the captured scenes are static. Contrary to the existing methods, we propose a video deblurring algorithm that can deal with general blurs inherent in dynamic scenes. To handle general and locally varying blurs caused by various sources, such as moving objects, camera shake, depth variation, and defocus, we estimate pixel-wise non-uniform blur kernels. We infer bidirectional optical flows to handle motion blurs, and also estimate Gaussian blur maps to remove optical blur from defocus in our new blur model. Therefore, we propose a single energy model that jointly estimates optical flows, defocus blur maps and latent frames. We also provide a framework and efficient solvers to minimize the proposed energy model. By optimizing the energy model, we achieve significant improvements in removing general blurs, estimating optical flows, and extending depth-of-field in blurry frames. Moreover, in this work, to evaluate the performance of non-uniform deblurring methods objectively, we have constructed a new realistic dataset with ground truths. In addition, extensive experimental on publicly available challenging video data demonstrate that the proposed method produces qualitatively superior performance than the state-of-the-art methods which often fail in either deblurring or optical flow estimation. version:1
arxiv-1511-04508 | Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks | http://arxiv.org/abs/1511.04508 | id:1511.04508 author:Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami category:cs.CR cs.LG cs.NE stat.ML  published:2015-11-14 summary:Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10^30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested. version:2
arxiv-1603-04245 | A Variational Perspective on Accelerated Methods in Optimization | http://arxiv.org/abs/1603.04245 | id:1603.04245 author:Andre Wibisono, Ashia C. Wilson, Michael I. Jordan category:math.OC cs.LG stat.ML  published:2016-03-14 summary:Accelerated gradient methods play a central role in optimization, achieving optimal rates in many settings. While many generalizations and extensions of Nesterov's original acceleration method have been proposed, it is not yet clear what is the natural scope of the acceleration concept. In this paper, we study accelerated methods from a continuous-time perspective. We show that there is a Lagrangian functional that we call the \emph{Bregman Lagrangian} which generates a large class of accelerated methods in continuous time, including (but not limited to) accelerated gradient descent, its non-Euclidean extension, and accelerated higher-order gradient methods. We show that the continuous-time limit of all of these methods correspond to traveling the same curve in spacetime at different speeds. From this perspective, Nesterov's technique and many of its generalizations can be viewed as a systematic way to go from the continuous-time curves generated by the Bregman Lagrangian to a family of discrete-time accelerated algorithms. version:1
arxiv-1603-04236 | Interactive Tools and Tasks for the Hebrew Bible | http://arxiv.org/abs/1603.04236 | id:1603.04236 author:Nicolai Winther-Nielsen category:cs.CL  published:2016-03-14 summary:Ancient texts can support intertextuality in different ways through digital tools for databases and for tasks that scholars and students do, when they interac twith the texts in new ways. This contribution explores how the corpus of the Hebrew Bible created and maintained by the Eep Talstra Center for Bible and Computer has potential for redefining the way we learn from our ancient texts as modern knowledge workers. It first describes how the corpus was used for development of Bible Online Learner as a persuasive technology to enhance language learning with, in and around a database that drives interactive tasks for learners. The achievements obtained through so far are very promising, and it can help us explore textual criticism as another target for interactive study of the Hebrew Bible through corpus-technology. Because textual criticism is an increasingly specialized area of research which depends on digital resources. The commercial solution from Logos Bible Software offers advanced scholarly resources from the German Bible Society as a model for how affluent Western scholars can use technology for the Hebrew corpus. The achievements in corpus-driven learning and the potential of commercial resources can help us suggest new tasks in textual criticism based on online applications which use corpora for a new kind of textual corpus criticism. Some promising tools for text categorization, analysis of translation shifts and interpretation are recommended as potential models for the future. The main goal in the future must be more open global access for the new tools. version:1
arxiv-1603-04203 | Graph Based Sinogram Denoising for Tomographic Reconstructions | http://arxiv.org/abs/1603.04203 | id:1603.04203 author:Faisal Mahmood, Nauman Shahid, Pierre Vandergheynst, Ulf Skoglund category:cs.CV  published:2016-03-14 summary:Limited data and low dose constraints are common problems in a variety of tomographic reconstruction paradigms which lead to noisy and incomplete data. Over the past few years sinogram denoising has become an essential pre-processing step for low dose Computed Tomographic (CT) reconstructions. We propose a novel sinogram denoising algorithm inspired by the modern field of signal processing on graphs. Graph based methods often perform better than standard filtering operations since they can exploit the signal structure. This makes the sinogram an ideal candidate for graph based denoising since it generally has a piecewise smooth structure. We test our method with a variety of phantoms and different reconstruction methods. Our numerical study shows that the proposed algorithm improves the performance of analytical filtered back-projection (FBP) and iterative methods ART (Kaczmarz) and SIRT (Cimmino).We observed that graph denoised sinogram always minimizes the error measure and improves the accuracy of the solution as compared to regular reconstructions. version:1
arxiv-1603-04190 | Online Isotonic Regression | http://arxiv.org/abs/1603.04190 | id:1603.04190 author:Wojciech Kotłowski, Wouter M. Koolen, Alan Malek category:cs.LG stat.ML  published:2016-03-14 summary:We consider the online version of the isotonic regression problem. Given a set of linearly ordered points (e.g., on the real line), the learner must predict labels sequentially at adversarially chosen positions and is evaluated by her total squared loss compared against the best isotonic (non-decreasing) function in hindsight. We survey several standard online learning algorithms and show that none of them achieve the optimal regret exponent; in fact, most of them (including Online Gradient Descent, Follow the Leader and Exponential Weights) incur linear regret. We then prove that the Exponential Weights algorithm played over a covering net of isotonic functions has regret is bounded by $O(T^{1/3} \log^{2/3}(T))$ and present a matching $\Omega(T^{1/3})$ lower bound on regret. We also provide a computationally efficient version of this algorithm. We also analyze the noise-free case, in which the revealed labels are isotonic, and show that the bound can be improved to $O(\log T)$ or even to $O(1)$ (when the labels are revealed in the isotonic order). Finally, we extend the analysis beyond squared loss and give bounds for log-loss and absolute loss. version:1
arxiv-1604-00970 | Extended Object Tracking: Introduction, Overview and Applications | http://arxiv.org/abs/1604.00970 | id:1604.00970 author:Karl Granstrom, Marcus Baum category:cs.CV cs.SY  published:2016-03-14 summary:This article provides an elaborate overview of current research in extended object tracking. We provide a clear definition of an extended object and discuss its delimitation to other object types and sensor models. Next, different shape models and possibilities to model the number of measurements are extensively discussed. Subsequently, we give a tutorial introduction to two basic and well used extended object tracking methods -- the random matrix approach and random hypersurface approach. The next part treats approaches for tracking multiple extended objects and elaborates how the large number of feasible association hypotheses can be tackled using both Random Finite Set (RFS) and Non-RFS multi-object trackers. The article concludes with a summary of current applications, where three example applications involving Lidar, RGB, and RGB-D sensors are highlighted. version:1
arxiv-1501-03766 | Correlations between the Hurst exponent and the maximal Lyapunov exponent for some low-dimensional discrete conservative dynamical systems | http://arxiv.org/abs/1501.03766 | id:1501.03766 author:Mariusz Tarnopolski category:nlin.CD math.DS physics.data-an stat.ML  published:2015-01-15 summary:The Chirikov standard map and the 2D Froeschl\'e map are investigated. A few thousand values of the Hurst exponent (HE) and the maximal Lyapunov exponent (mLE) are plotted in a mixed space of the nonlinear parameter versus the initial condition. Both characteristic exponents reveal remarkably similar structures in this mixed space. Moreover, a tight correlation between the HEs and mLEs for the two maps was found: $\rho=0.83$ and $\rho=0.75$ for the Chirikov and Froeschl\'e maps, respectively, where $\rho$ is the Spearman rank. Based on this relation, a machine learning (ML) procedure, using the nearest neighbour algorithm, was performed to reproduce the HE distributions based on the mLE distributions. A few thousand HE and mLE values from the mixed spaces were used for training, and then using $2-2.4\times 10^5$ mLEs, the HEs were retrieved. The ML procedure allowed to reproduce the structure of the mixed spaces in great detail. The HE is proposed as an informative parameter in the area of chaotic control, as it provides expectations about the general trend in a time series. version:3
arxiv-1503-00488 | Graphical Representation for Heterogeneous Face Recognition | http://arxiv.org/abs/1503.00488 | id:1503.00488 author:Chunlei Peng, Xinbo Gao, Nannan Wang, Jie Li category:cs.CV  published:2015-03-02 summary:Heterogeneous face recognition (HFR) refers to matching face images acquired from different sources (i.e., different sensors or different wavelengths) for identification. HFR plays an important role in both biometrics research and industry. In spite of promising progresses achieved in recent years, HFR is still a challenging problem due to the difficulty to represent two heterogeneous images in a homogeneous manner. Existing HFR methods either represent an image ignoring the spatial information, or rely on a transformation procedure which complicates the recognition task. Considering these problems, we propose a novel graphical representation based HFR method (G-HFR) in this paper. Markov networks are employed to represent heterogeneous image patches separately, which takes the spatial compatibility between neighboring image patches into consideration. A coupled representation similarity metric (CRSM) is designed to measure the similarity between obtained graphical representations. Extensive experiments conducted on multiple HFR scenarios (viewed sketch, forensic sketch, near infrared image, and thermal infrared image) show that the proposed method outperforms state-of-the-art methods. version:3
arxiv-1603-04153 | Top-$K$ Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal | http://arxiv.org/abs/1603.04153 | id:1603.04153 author:Minje Jang, Sunghyun Kim, Changho Suh, Sewoong Oh category:cs.LG cs.IT cs.SI math.IT stat.ML  published:2016-03-14 summary:We explore the top-$K$ rank aggregation problem. Suppose a collection of items is compared in pairs repeatedly, and we aim to recover a consistent ordering that focuses on the top-$K$ ranked items based on partially revealed preference information. We investigate the Bradley-Terry-Luce model in which one ranks items according to their perceived utilities modeled as noisy observations of their underlying true utilities. Our main contributions are two-fold. First, in a general comparison model where item pairs to compare are given a priori, we attain an upper and lower bound on the sample size for reliable recovery of the top-$K$ ranked items. Second, more importantly, extending the result to a random comparison model where item pairs to compare are chosen independently with some probability, we show that in slightly restricted regimes, the gap between the derived bounds reduces to a constant factor, hence reveals that a spectral method can achieve the minimax optimality on the (order-wise) sample size required for top-$K$ ranking. That is to say, we demonstrate a spectral method alone to be sufficient to achieve the optimality and advantageous in terms of computational complexity, as it does not require an additional stage of maximum likelihood estimation that a state-of-the-art scheme employs to achieve the optimality. We corroborate our main results by numerical experiments. version:1
arxiv-1603-04150 | Regression-based Hypergraph Learning for Image Clustering and Classification | http://arxiv.org/abs/1603.04150 | id:1603.04150 author:Sheng Huang, Dan Yang, Bo Liu, Xiaohong Zhang category:cs.CV  published:2016-03-14 summary:Inspired by the recently remarkable successes of Sparse Representation (SR), Collaborative Representation (CR) and sparse graph, we present a novel hypergraph model named Regression-based Hypergraph (RH) which utilizes the regression models to construct the high quality hypergraphs. Moreover, we plug RH into two conventional hypergraph learning frameworks, namely hypergraph spectral clustering and hypergraph transduction, to present Regression-based Hypergraph Spectral Clustering (RHSC) and Regression-based Hypergraph Transduction (RHT) models for addressing the image clustering and classification issues. Sparse Representation and Collaborative Representation are employed to instantiate two RH instances and their RHSC and RHT algorithms. The experimental results on six popular image databases demonstrate that the proposed RH learning algorithms achieve promising image clustering and classification performances, and also validate that RH can inherit the desirable properties from both hypergraph models and regression models. version:1
arxiv-1603-04139 | SSSC-AM: A Unified Framework for Video Co-Segmentation by Structured Sparse Subspace Clustering with Appearance and Motion Features | http://arxiv.org/abs/1603.04139 | id:1603.04139 author:Junlin Yao, Frank Nielsen category:cs.CV  published:2016-03-14 summary:Video co-segmentation refers to the task of jointly segmenting common objects appearing in a given group of videos. In practice, high-dimensional data such as videos can be conceptually thought as being drawn from a union of subspaces corresponding to categories rather than from a smooth manifold. Therefore, segmenting data into respective subspaces --- subspace clustering --- finds widespread applications in computer vision, including co-segmentation. State-of-the-art methods via subspace clustering seek to solve the problem in two steps: First, an affinity matrix is built from data, with appearance features or motion patterns. Second, the data are segmented by applying spectral clustering to the affinity matrix. However, this process is insufficient to obtain an optimal solution since it does not take into account the {\em interdependence} of the affinity matrix with the segmentation. In this work, we present a novel unified video co-segmentation framework inspired by the recent Structured Sparse Subspace Clustering ($\mathrm{S^{3}C}$) based on the {\em self-expressiveness} model. Our method yields more consistent segmentation results. In order to improve the detectability of motion features with missing trajectories due to occlusion or tracked points moving out of frames, we add an extra-dimensional signature to the motion trajectories. Moreover, we reformulate the $\mathrm{S^{3}C}$ algorithm by adding the affine subspace constraint in order to make it more suitable to segment rigid motions lying in affine subspaces of dimension at most $3$. Experiments on MOViCS dataset demonstrate the effectiveness of our approaches and its robustness to heavy noise. version:1
arxiv-1603-04134 | RISAS: A Novel Rotation, Illumination, Scale Invariant Appearance and Shape Feature | http://arxiv.org/abs/1603.04134 | id:1603.04134 author:Xiaoyang Li, Kanzhi Wu, Yong Liu, Ravindra Ranasinghe, Gamini Dissanayake, Rong Xiong category:cs.RO cs.CV  published:2016-03-14 summary:In this paper, we present a novel RGB-D feature, RISAS, which is robust to Rotation, Illumination and Scale variations through fusing Appearance and Shape information. We propose a keypoint detector which is able to extract information rich regions in both appearance and shape using a novel 3D information representation method in combination with grayscale information. We extend our recent work on Local Ordinal Intensity and Normal Descriptor(LOIND), to further significantly improve its illumination, scale and rotation invariance using 1) a precise neighbourhood region selection method and 2) a more robust dominant orientation estimation. We also present a dataset for evaluation of RGB-D features, together with comprehensive experiments to illustrate the effectiveness of the proposed RGB-D feature when compared to SIFT, C-SHOT and LOIND. We also show the use of RISAS for point cloud alignment associated with many robotics applications and demonstrate its effectiveness in a poorly illuminated environment when compared with SIFT and ORB. version:1
arxiv-1604-02416 | How deep is knowledge tracing? | http://arxiv.org/abs/1604.02416 | id:1604.02416 author:Mohammad Khajah, Robert V. Lindsey, Michael C. Mozer category:cs.AI cs.NE  published:2016-03-14 summary:In theoretical cognitive science, there is a tension between highly structured models whose parameters have a direct psychological interpretation and highly complex, general-purpose models whose parameters and representations are difficult to interpret. The former typically provide more insight into cognition but the latter often perform better. This tension has recently surfaced in the realm of educational data mining, where a deep learning approach to predicting students' performance as they work through a series of exercises---termed deep knowledge tracing or DKT---has demonstrated a stunning performance advantage over the mainstay of the field, Bayesian knowledge tracing or BKT. In this article, we attempt to understand the basis for DKT's advantage by considering the sources of statistical regularity in the data that DKT can leverage but which BKT cannot. We hypothesize four forms of regularity that BKT fails to exploit: recency effects, the contextualized trial sequence, inter-skill similarity, and individual variation in ability. We demonstrate that when BKT is extended to allow it more flexibility in modeling statistical regularities---using extensions previously proposed in the literature---BKT achieves a level of performance indistinguishable from that of DKT. We argue that while DKT is a powerful, useful, general-purpose framework for modeling student learning, its gains do not come from the discovery of novel representations---the fundamental advantage of deep learning. To answer the question posed in our title, knowledge tracing may be a domain that does not require `depth'; shallow models like BKT can perform just as well and offer us greater interpretability and explanatory power. version:1
arxiv-1509-05142 | Fast Gaussian Process Regression for Big Data | http://arxiv.org/abs/1509.05142 | id:1509.05142 author:Sourish Das, Sasanka Roy, Rajiv Sambasivan category:cs.LG stat.ML  published:2015-09-17 summary:Gaussian Processes are widely used for regression tasks. A known limitation in the application of Gaussian Processes to regression tasks is that the computation of the solution requires performing a matrix inversion. The solution also requires the storage of a large matrix in memory. These factors restrict the application of Gaussian Process regression to small and moderate size data sets. We present an algorithm based on empirically determined subset selection that works well on both real world and synthetic datasets. We also compare the performance of this algorithm with two other methods that are used to apply Gaussian Processes Regression on large datasets. On the synthetic and real world datasets used in this study, the algorithm demonstrated sub-linear time and space complexity. The accuracy obtained with this algorithm on the datasets used for this study is comparable to what is achieved with the two other methods commonly used to apply Gaussian Processes to large datasets. version:4
arxiv-1603-04119 | Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains | http://arxiv.org/abs/1603.04119 | id:1603.04119 author:David Abel, Alekh Agarwal, Fernando Diaz, Akshay Krishnamurthy, Robert E. Schapire category:cs.AI cs.LG stat.ML  published:2016-03-14 summary:High-dimensional observations and complex real-world dynamics present major challenges in reinforcement learning for both function approximation and exploration. We address both of these challenges with two complementary techniques: First, we develop a gradient-boosting style, non-parametric function approximator for learning on $Q$-function residuals. And second, we propose an exploration strategy inspired by the principles of state abstraction and information acquisition under uncertainty. We demonstrate the empirical effectiveness of these techniques, first, as a preliminary check, on two standard tasks (Blackjack and $n$-Chain), and then on two much larger and more realistic tasks with high-dimensional observation spaces. Specifically, we introduce two benchmarks built within the game Minecraft where the observations are pixel arrays of the agent's visual field. A combination of our two algorithmic techniques performs competitively on the standard reinforcement-learning tasks while consistently and substantially outperforming baselines on the two tasks with high-dimensional observation spaces. The new function approximator, exploration strategy, and evaluation benchmarks are each of independent interest in the pursuit of reinforcement-learning methods that scale to real-world domains. version:1
arxiv-1603-04118 | Bandit Approaches to Preference Learning Problems with Multiple Populations | http://arxiv.org/abs/1603.04118 | id:1603.04118 author:Aniruddha Bhargava, Ravi Ganti, Robert Nowak category:stat.ML cs.AI cs.LG  published:2016-03-14 summary:In this paper we study an extension of the stochastic multi-armed bandit (MAB) framework, where in each round a player can play multiple actions and receive a stochastic reward which depends on the actions played. This problem is motivated by applications in recommendation problems where there are multiple populations of users and hence no single choice might be good for the entire population. We specifically look at bandit problems where we are allowed to make two choices in each round. We provide algorithms for this problem in both the noiseless and noisy case. Our algorithms are computationally efficient and have provable sample complexity guarantees. In the process of establishing sample complexity guarantees for our algorithms, we establish new results regarding the Nystr{\"o}m method which can be of independent interest. We supplement our theoretical results with experimental comparisons. version:1
arxiv-1603-04117 | Multi-modal Tracking for Object based SLAM | http://arxiv.org/abs/1603.04117 | id:1603.04117 author:Prateek Singhal, Ruffin White, Henrik Christensen category:cs.CV  published:2016-03-14 summary:We present an on-line 3D visual object tracking framework for monocular cameras by incorporating spatial knowledge and uncertainty from semantic mapping along with high frequency measurements from visual odometry. Using a combination of vision and odometry that are tightly integrated we can increase the overall performance of object based tracking for semantic mapping. We present a framework for integration of the two data-sources into a coherent framework through information based fusion/arbitration. We demonstrate the framework in the context of OmniMapper[1] and present results on 6 challenging sequences over multiple objects compared to data obtained from a motion capture systems. We are able to achieve a mean error of 0.23m for per frame tracking showing 9% relative error less than state of the art tracker. version:1
arxiv-1603-04116 | Learning Binary Codes and Binary Weights for Efficient Classification | http://arxiv.org/abs/1603.04116 | id:1603.04116 author:Fumin Shen, Yadong Mu, Wei Liu, Yang Yang, Heng Tao Shen category:cs.CV  published:2016-03-14 summary:This paper proposes a generic formulation that significantly expedites the training and deployment of image classification models, particularly under the scenarios of many image categories and high feature dimensions. As a defining property, our method represents both the images and learned classifiers using binary hash codes, which are simultaneously learned from the training data. Classifying an image thereby reduces to computing the Hamming distance between the binary codes of the image and classifiers and selecting the class with minimal Hamming distance. Conventionally, compact hash codes are primarily used for accelerating image search. Our work is first of its kind to represent classifiers using binary codes. Specifically, we formulate multi-class image classification as an optimization problem over binary variables. The optimization alternatively proceeds over the binary classifiers and image hash codes. Profiting from the special property of binary codes, we show that the sub-problems can be efficiently solved through either a binary quadratic program (BQP) or linear program. In particular, for attacking the BQP problem, we propose a novel bit-flipping procedure which enjoys high efficacy and local optimality guarantee. Our formulation supports a large family of empirical loss functions and is here instantiated by exponential / hinge losses. Comprehensive evaluations are conducted on several representative image benchmarks. The experiments consistently observe reduced complexities of model training and deployment, without sacrifice of accuracies. version:1
arxiv-1603-04080 | A Stochastic Approach to STDP | http://arxiv.org/abs/1603.04080 | id:1603.04080 author:Runchun Wang, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, André van Schaik category:cs.NE  published:2016-03-13 summary:We present a digital implementation of the Spike Timing Dependent Plasticity (STDP) learning rule. The proposed digital implementation consists of an exponential decay generator array and a STDP adaptor array. On the arrival of a pre- and post-synaptic spike, the STDP adaptor will send a digital spike to the decay generator. The decay generator will then generate an exponential decay, which will be used by the STDP adaptor to perform the weight adaption. The exponential decay, which is computational expensive, is efficiently implemented by using a novel stochastic approach, which we analyse and characterise here. We use a time multiplexing approach to achieve 8192 (8k) virtual STDP adaptors and decay generators with only one physical implementation of each. We have validated our stochastic STDP approach with measurement results of a balanced excitation/inhibition experiment. Our stochastic approach is ideal for implementing the STDP learning rule in large-scale spiking neural networks running in real time. version:1
arxiv-1602-07188 | Exploring the Neural Algorithm of Artistic Style | http://arxiv.org/abs/1602.07188 | id:1602.07188 author:Yaroslav Nikulin, Roman Novak category:cs.CV  published:2016-02-23 summary:We explore the method of style transfer presented in the article "A Neural Algorithm of Artistic Style" by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge (arXiv:1508.06576). We first demonstrate the power of the suggested style space on a few examples. We then vary different hyper-parameters and program properties that were not discussed in the original paper, among which are the recognition network used, starting point of the gradient descent and different ways to partition style and content layers. We also give a brief comparison of some of the existing algorithm implementations and deep learning frameworks used. To study the style space further we attempt to generate synthetic images by maximizing a single entry in one of the Gram matrices $\mathcal{G}_l$ and some interesting results are observed. Next, we try to mimic the sparsity and intensity distribution of Gram matrices obtained from a real painting and generate more complex textures. Finally, we propose two new style representations built on top of network's features and discuss how one could be used to achieve local and potentially content-aware style transfer. version:2
arxiv-1603-04064 | A Grothendieck-type inequality for local maxima | http://arxiv.org/abs/1603.04064 | id:1603.04064 author:Andrea Montanari category:math.OC stat.ML  published:2016-03-13 summary:A large number of problems in optimization, machine learning, signal processing can be effectively addressed by suitable semidefinite programming (SDP) relaxations. Unfortunately, generic SDP solvers hardly scale beyond instances with a few hundreds variables (in the underlying combinatorial problem). On the other hand, it has been observed empirically that an effective strategy amounts to introducing a (non-convex) rank constraint, and solving the resulting smooth optimization problem by ascent methods. This non-convex problem has --generically-- a large number of local maxima, and the reason for this success is therefore unclear. This paper provides rigorous support for this approach. For the problem of maximizing a linear functional over the elliptope, we prove that all local maxima are within a small gap from the SDP optimum. In several problems of interest, arbitrarily small relative error can be achieved by taking the rank constraint $k$ to be of order one, independently of the problem size. version:1
arxiv-1603-02729 | Revisiting Active Perception | http://arxiv.org/abs/1603.02729 | id:1603.02729 author:Ruzena Bajcsy, Yiannis Aloimonos, John K. Tsotsos category:cs.CV cs.RO  published:2016-03-08 summary:Despite the recent successes in robotics, artificial intelligence and computer vision, a complete artificial agent necessarily must include active perception. A multitude of ideas and methods for how to accomplish this have already appeared in the past, their broader utility perhaps impeded by insufficient computational power or costly hardware. The history of these ideas, perhaps selective due to our perspectives, is presented with the goal of organizing the past literature and highlighting the seminal contributions. We argue that those contributions are as relevant today as they were decades ago and, with the state of modern computational tools, are poised to find new life in the robotic perception systems of the next decade. version:2
arxiv-1602-03418 | Triplet Similarity Embedding for Face Verification | http://arxiv.org/abs/1602.03418 | id:1602.03418 author:Swami Sankaranarayanan, Azadeh Alavi, Rama Chellappa category:cs.CV  published:2016-02-10 summary:In this work, we present an unconstrained face verification algorithm and evaluate it on the recently released IJB-A dataset that aims to push the boundaries of face verification methods. The proposed algorithm couples a deep CNN-based approach with a low-dimensional discriminative embedding learnt using triplet similarity constraints in a large margin fashion. Aside from yielding performance improvement, this embedding provides significant advantages in terms of memory and post-processing operations like hashing and visualization. Experiments on the IJB-A dataset show that the proposed algorithm outperforms state of the art methods in verification and identification metrics, while requiring less training time. version:2
arxiv-1601-06239 | Divide and Conquer Local Average Regression | http://arxiv.org/abs/1601.06239 | id:1601.06239 author:Xiangyu Chang, Shaobo Lin, Yao Wang category:cs.LG math.ST stat.TH  published:2016-01-23 summary:The divide and conquer strategy, which breaks a massive data set into a se- ries of manageable data blocks, and then combines the independent results of data blocks to obtain a final decision, has been recognized as a state-of-the-art method to overcome challenges of massive data analysis. In this paper, we merge the divide and conquer strategy with local average regression methods to infer the regressive relationship of input-output pairs from a massive data set. After theoretically analyzing the pros and cons, we find that although the divide and conquer local average regression can reach the optimal learning rate, the restric- tion to the number of data blocks is a bit strong, which makes it only feasible for small number of data blocks. We then propose two variants to lessen (or remove) this restriction. Our results show that these variants can achieve the optimal learning rate with much milder restriction (or without such restriction). Extensive experimental studies are carried out to verify our theoretical assertions. version:2
arxiv-1603-04046 | Image and Depth from a Single Defocused Image Using Coded Aperture Photography | http://arxiv.org/abs/1603.04046 | id:1603.04046 author:Mina Masoudifar, Hamid Reza Pourreza category:cs.CV  published:2016-03-13 summary:Depth from defocus and defocus deblurring from a single image are two challenging problems that are derived from the finite depth of field in conventional cameras. Coded aperture imaging is one of the techniques that is used for improving the results of these two problems. Up to now, different methods have been proposed for improving the results of either defocus deblurring or depth estimation. In this paper, a multi-objective function is proposed for evaluating and designing aperture patterns with the aim of improving the results of both depth from defocus and defocus deblurring. Pattern evaluation is performed by considering the scene illumination condition and camera system specification. Based on the proposed criteria, a single asymmetric pattern is designed that is used for restoring a sharp image and a depth map from a single input. Since the designed pattern is asymmetric, defocus objects on the two sides of the focal plane can be distinguished. Depth estimation is performed by using a new algorithm, which is based on image quality assessment criteria and can distinguish between blurred objects lying in front or behind the focal plane. Extensive simulations as well as experiments on a variety of real scenes are conducted to compare our aperture with previously proposed ones. version:1
arxiv-1603-04042 | Deep Interactive Object Selection | http://arxiv.org/abs/1603.04042 | id:1603.04042 author:Ning Xu, Brian Price, Scott Cohen, Jimei Yang, Thomas Huang category:cs.CV  published:2016-03-13 summary:Interactive object selection is a very important research problem and has many applications. Previous algorithms require substantial user interactions to estimate the foreground and background distributions. In this paper, we present a novel deep learning based algorithm which has a much better understanding of objectness and thus can reduce user interactions to just a few clicks. Our algorithm transforms user provided positive and negative clicks into two Euclidean distance maps which are then concatenated with the RGB channels of images to compose (image, user interactions) pairs. We generate many of such pairs by combining several random sampling strategies to model user click patterns and use them to fine tune deep Fully Convolutional Networks (FCNs). Finally the output probability maps of our FCN 8s model is integrated with graph cut optimization to refine the boundary segments. Our model is trained on the PASCAL segmentation dataset and evaluated on other datasets with different object classes. Experimental results on both seen and unseen objects clearly demonstrate that our algorithm has a good generalization ability and is superior to all existing interactive object selection approaches. version:1
arxiv-1603-04037 | Pose for Action - Action for Pose | http://arxiv.org/abs/1603.04037 | id:1603.04037 author:Umar Iqbal, Martin Garbade, Juergen Gall category:cs.CV  published:2016-03-13 summary:In this work we propose to utilize information about human actions to improve pose estimation in monocular videos. To this end, we present a pictorial structure model that exploits high-level information about activities to incorporate higher-order part dependencies by modeling action specific appearance models and pose priors. However, instead of using an additional expensive action recognition framework, the action priors are efficiently estimated by our pose estimation framework. This is achieved by starting with a uniform action prior and updating the action prior during pose estimation. We also show that learning the right amount of appearance sharing among action classes improves the pose estimation. Our proposed model achieves state-of-the-art performance on two challenging datasets for pose estimation and action recognition with over 80,000 test images. version:1
arxiv-1603-04026 | A comprehensive study of sparse codes on abnormality detection | http://arxiv.org/abs/1603.04026 | id:1603.04026 author:Huamin Ren, Hong Pan, Søren Ingvor Olsen, Thomas B. Moeslund category:cs.CV  published:2016-03-13 summary:Sparse representation has been applied successfully in abnormal event detection, in which the baseline is to learn a dictionary accompanied by sparse codes. While much emphasis is put on discriminative dictionary construction, there are no comparative studies of sparse codes regarding abnormality detection. We comprehensively study two types of sparse codes solutions - greedy algorithms and convex L1-norm solutions - and their impact on abnormality detection performance. We also propose our framework of combining sparse codes with different detection methods. Our comparative experiments are carried out from various angles to better understand the applicability of sparse codes, including computation time, reconstruction error, sparsity, detection accuracy, and their performance combining various detection methods. Experiments show that combining OMP codes with maximum coordinate detection could achieve state-of-the-art performance on the UCSD dataset [14]. version:1
arxiv-1510-02706 | Conditional Risk Minimization for Stochastic Processes | http://arxiv.org/abs/1510.02706 | id:1510.02706 author:Alexander Zimin, Christoph H. Lampert category:stat.ML cs.LG  published:2015-10-09 summary:We study the task of learning from non-i.i.d. data. In particular, we aim at learning predictors that minimize the conditional risk for a stochastic process, i.e. the expected loss of the predictor on the next point conditioned on the set of training samples observed so far. For non-i.i.d. data, the training set contains information about the upcoming samples, so learning with respect to the conditional distribution can be expected to yield better predictors than one obtains from the classical setting of minimizing the marginal risk. Our main contribution is a practical estimator for the conditional risk based on the theory of non-parametric time-series prediction, and a finite sample concentration bound that establishes uniform convergence of the estimator to the true conditional risk under certain regularity assumptions on the process. version:2
arxiv-1505-05800 | Complexity Theoretic Limitations on Learning Halfspaces | http://arxiv.org/abs/1505.05800 | id:1505.05800 author:Amit Daniely category:cs.CC cs.LG  published:2015-05-21 summary:We study the problem of agnostically learning halfspaces which is defined by a fixed but unknown distribution $\mathcal{D}$ on $\mathbb{Q}^n\times \{\pm 1\}$. We define $\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D})$ as the least error of a halfspace classifier for $\mathcal{D}$. A learner who can access $\mathcal{D}$ has to return a hypothesis whose error is small compared to $\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D})$. Using the recently developed method of the author, Linial and Shalev-Shwartz we prove hardness of learning results under a natural assumption on the complexity of refuting random $K$-$\mathrm{XOR}$ formulas. We show that no efficient learning algorithm has non-trivial worst-case performance even under the guarantees that $\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D}) \le \eta$ for arbitrarily small constant $\eta>0$, and that $\mathcal{D}$ is supported in $\{\pm 1\}^n\times \{\pm 1\}$. Namely, even under these favorable conditions its error must be $\ge \frac{1}{2}-\frac{1}{n^c}$ for every $c>0$. In particular, no efficient algorithm can achieve a constant approximation ratio. Under a stronger version of the assumption (where $K$ can be poly-logarithmic in $n$), we can take $\eta = 2^{-\log^{1-\nu}(n)}$ for arbitrarily small $\nu>0$. Interestingly, this is even stronger than the best known lower bounds (Arora et. al. 1993, Feldamn et. al. 2006, Guruswami and Raghavendra 2006) for the case that the learner is restricted to return a halfspace classifier (i.e. proper learning). version:2
arxiv-1603-04002 | Fast Learning from Distributed Datasets without Entity Matching | http://arxiv.org/abs/1603.04002 | id:1603.04002 author:Giorgio Patrini, Richard Nock, Stephen Hardy, Tiberio Caetano category:cs.LG cs.DC I.2.6  published:2016-03-13 summary:Consider the following data fusion scenario: two datasets/peers contain the same real-world entities described using partially shared features, e.g. banking and insurance company records of the same customer base. Our goal is to learn a classifier in the cross product space of the two domains, in the hard case in which no shared ID is available -- e.g. due to anonymization. Traditionally, the problem is approached by first addressing entity matching and subsequently learning the classifier in a standard manner. We present an end-to-end solution which bypasses matching entities, based on the recently introduced concept of Rademacher observations (rados). Informally, we replace the minimisation of a loss over examples, which requires to solve entity resolution, by the equivalent minimisation of a (different) loss over rados. Among others, key properties we show are (i) a potentially huge subset of these rados does not require to perform entity matching, and (ii) the algorithm that provably minimizes the rado loss over these rados has time and space complexities smaller than the algorithm minimizing the equivalent example loss. Last, we relax a key assumption of the model, that the data is vertically partitioned among peers --- in this case, we would not even know the existence of a solution to entity resolution. In this more general setting, experiments validate the possibility of significantly beating even the optimal peer in hindsight. version:1
arxiv-1603-04000 | Learning Typographic Style | http://arxiv.org/abs/1603.04000 | id:1603.04000 author:Shumeet Baluja category:cs.CV cs.LG cs.NE  published:2016-03-13 summary:Typography is a ubiquitous art form that affects our understanding, perception, and trust in what we read. Thousands of different font-faces have been created with enormous variations in the characters. In this paper, we learn the style of a font by analyzing a small subset of only four letters. From these four letters, we learn two tasks. The first is a discrimination task: given the four letters and a new candidate letter, does the new letter belong to the same font? Second, given the four basis letters, can we generate all of the other letters with the same characteristics as those in the basis set? We use deep neural networks to address both tasks, quantitatively and qualitatively measure the results in a variety of novel manners, and present a thorough investigation of the weaknesses and strengths of the approach. version:1
arxiv-1603-03984 | An efficient Exact-PGA algorithm for constant curvature manifolds | http://arxiv.org/abs/1603.03984 | id:1603.03984 author:Rudrasis Chakraborty, Dohyung Seo, Baba C. Vemuri category:cs.CV  published:2016-03-13 summary:Manifold-valued datasets are widely encountered in many computer vision tasks. A non-linear analog of the PCA, called the Principal Geodesic Analysis (PGA) suited for data lying on Riemannian manifolds was reported in literature a decade ago. Since the objective function in PGA is highly non-linear and hard to solve efficiently in general, researchers have proposed a linear approximation. Though this linear approximation is easy to compute, it lacks accuracy especially when the data exhibits a large variance. Recently, an alternative called exact PGA was proposed which tries to solve the optimization without any linearization. For general Riemannian manifolds, though it gives better accuracy than the original (linearized) PGA, for data that exhibit large variance, the optimization is not computationally efficient. In this paper, we propose an efficient exact PGA for constant curvature Riemannian manifolds (CCM-EPGA). CCM-EPGA differs significantly from existing PGA algorithms in two aspects, (i) the distance between a given manifold-valued data point and the principal submanifold is computed analytically and thus no optimization is required as in existing methods. (ii) Unlike the existing PGA algorithms, the descent into codimension-1 submanifolds does not require any optimization but is accomplished through the use of the Rimeannian inverse Exponential map and the parallel transport operations. We present theoretical and experimental results for constant curvature Riemannian manifolds depicting favorable performance of CCM-EPGA compared to existing PGA algorithms. We also present data reconstruction from principal components and directions which has not been presented in literature in this setting. version:1
arxiv-1603-03980 | On Learning High Dimensional Structured Single Index Models | http://arxiv.org/abs/1603.03980 | id:1603.03980 author:Nikhil Rao, Ravi Ganti, Laura Balzano, Rebecca Willett, Robert Nowak category:stat.ML cs.AI cs.LG  published:2016-03-13 summary:Single Index Models (SIMs) are simple yet flexible semi-parametric models for classification and regression, where response variables are modeled as a nonlinear, monotonic function of a linear combination of features. Estimation in this context requires learning both the feature weights and the nonlinear function that relates features to observations. While methods have been described to learn SIMs in the low dimensional regime, a method that can efficiently learn SIMs in high dimensions, and under general structural assumptions, has not been forthcoming. In this paper, we propose computationally efficient algorithms for SIM inference in high dimensions using atomic norm regularization. This general approach to imposing structure in high-dimensional modeling specializes to sparsity, group sparsity, and low-rank assumptions among others. We also provide a scalable, stochastic version of the method. Experiments show that the method we propose enjoys superior predictive performance when compared to generalized linear models such as logistic regression, on several real-world datasets. version:1
arxiv-1603-03977 | Privacy-preserving Analysis of Correlated Data | http://arxiv.org/abs/1603.03977 | id:1603.03977 author:Yizhen Wang, Shuang Song, Kamalika Chaudhuri category:cs.LG cs.CR stat.ML  published:2016-03-13 summary:Many modern machine learning applications involve sensitive correlated data, such private information on users connected together in a social network, and measurements of physical activity of a single user across time. However, the current standard of privacy in machine learning, differential privacy, cannot adequately address privacy issues in this kind of data. This work looks at a recent generalization of differential privacy, called Pufferfish, that can be used to address privacy in correlated data. The main challenge in applying Pufferfish to correlated data problems is the lack of suitable mechanisms. In this paper, we provide a general mechanism, called the Wasserstein Mechanism, which applies to any Pufferfish framework. Since the Wasserstein Mechanism may be computationally inefficient, we provide an additional mechanism, called Markov Quilt Mechanism, that applies to some practical cases such as physical activity measurements across time, and is computationally efficient. version:1
arxiv-1603-03972 | Laplacian Eigenmaps from Sparse, Noisy Similarity Measurements | http://arxiv.org/abs/1603.03972 | id:1603.03972 author:Keith Levin, Vince Lyzinski category:stat.ML  published:2016-03-12 summary:Manifold learning and dimensionality reduction techniques are ubiquitous in science and engineering, but can be computationally expensive procedures when applied to large data sets or when similarities are expensive to compute. To date, little work has been done to investigate the tradeoff between computational resources and the quality of learned representations. We present both theoretical and experimental explorations of this question. In particular, we consider Laplacian eigenmaps embeddings based on a kernel matrix, and explore how the embeddings behave when this kernel matrix is corrupted by occlusion and noise. Our main theoretical result shows that under modest noise and occlusion assumptions, we can (with high probability) recover a good approximation to the Laplacian eigenmaps embedding based on the uncorrupted kernel matrix. Our results also show how regularization can aid this approximation. Experimentally, we explore the effects of noise and occlusion on Laplacian eigenmaps embeddings of two real-world data sets, one from speech processing and one from neuroscience, as well as a synthetic data set. version:1
arxiv-1603-03968 | Temporally Robust Global Motion Compensation by Keypoint-based Congealing | http://arxiv.org/abs/1603.03968 | id:1603.03968 author:S. Morteza Safdarnejad, Yousef Atoum, Xiaoming Liu category:cs.CV  published:2016-03-12 summary:Global motion compensation (GMC) removes the impact of camera motion and creates a video in which the background appears static over the progression of time. Various vision problems, such as human activity recognition, background reconstruction, and multi-object tracking can benefit from GMC. Existing GMC algorithms rely on sequentially processing consecutive frames, by estimating the transformation mapping the two frames, and obtaining a composite transformation to a global motion compensated coordinate. Sequential GMC suffers from temporal drift of frames from the accurate global coordinate, due to either error accumulation or sporadic failures of motion estimation at a few frames. We propose a temporally robust global motion compensation (TRGMC) algorithm which performs accurate and stable GMC, despite complicated and long-term camera motion. TRGMC densely connects pairs of frames, by matching local keypoints of each frame. A joint alignment of these frames is formulated as a novel keypoint-based congealing problem, where the transformation of each frame is updated iteratively, such that the spatial coordinates for the start and end points of matched keypoints are identical. Experimental results demonstrate that TRGMC has superior performance in a wide range of scenarios. version:1
arxiv-1603-03925 | Image Captioning with Semantic Attention | http://arxiv.org/abs/1603.03925 | id:1603.03925 author:Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo category:cs.CV  published:2016-03-12 summary:Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics. version:1
arxiv-1603-03876 | Variational Neural Discourse Relation Recognizer | http://arxiv.org/abs/1603.03876 | id:1603.03876 author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL  published:2016-03-12 summary:Implicit discourse relation recognition is a crucial component for automatic discourse-level analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VIRILE. VIRILE establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce a neural discourse relation model to approximate the posterior of the latent variable, and employ this approximated posterior to optimize a reparameterized variational lower bound. This allows VIRILE to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VIRILE can achieve competitive results against state-of-the-art baselines. version:1
arxiv-1410-8229 | Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse Recovery and the Grouping Effect | http://arxiv.org/abs/1410.8229 | id:1410.8229 author:Mehmet Eren Ahsen, Mathukumalli Vidyasagar category:stat.ML 90C25  published:2014-10-30 summary:In this paper we introduce a new optimization formulation for sparse regression and compressed sensing, called CLOT (Combined L-One and Two), wherein the regularizer is a convex combination of the $\ell_1$- and $\ell_2$-norms. This formulation differs from the Elastic Net (EN) formulation, in which the regularizer is a convex combination of the $\ell_1$- and $\ell_2$-norm squared. This seemingly simple modification has fairly significant consequences. In particular, it is shown in this paper that the EN formulation \textit{does not achieve} robust recovery of sparse vectors in the context of compressed sensing, whereas the new CLOT formulation does so. Also, like EN but unlike LASSO, the CLOT formulation achieves the grouping effect, wherein coefficients of highly correlated columns of the measurement (or design) matrix are assigned roughly comparable values. It is noteworthy that LASSO does not have the grouping effect and EN (as shown here) does not achieve robust sparse recovery. Therefore the CLOT formulation combines the best features of both LASSO (robust sparse recovery) and EN (grouping effect). The CLOT formulation is a special case of another one called SGL (Sparse Group LASSO) which was introduced into the literature previously, but without any analysis of either the grouping effect or robust sparse recovery. It is shown here that SGL achieves robust sparse recovery, and also achieves a version of the grouping effect in that coefficients of highly correlated columns of the measurement (or design) matrix are assigned roughly comparable values, \textit{if the columns belong to the same group}. version:2
arxiv-1603-03875 | Towards Building an RGBD-M Scanner | http://arxiv.org/abs/1603.03875 | id:1603.03875 author:Zhe Wu, Sai-Kit Yeung, Ping Tan category:cs.CV  published:2016-03-12 summary:We present a portable device to capture both shape and reflectance of an indoor scene. Consisting of a Kinect, an IR camera and several IR LEDs, our device allows the user to acquire data in a similar way as he/she scans with a single Kinect. Scene geometry is reconstructed by KinectFusion. To estimate reflectance from incomplete and noisy observations, 3D vertices of the same material are identified by our material segmentation propagation algorithm. Then BRDF observations at these vertices are merged into a more complete and accurate BRDF for the material. Effectiveness of our device is demonstrated by quality results on real-world scenes. version:1
arxiv-1603-03873 | Neural Discourse Relation Recognition with Semantic Memory | http://arxiv.org/abs/1603.03873 | id:1603.03873 author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL  published:2016-03-12 summary:Humans comprehend the meanings and relations of discourses heavily relying on their semantic memory that encodes general knowledge about concepts and facts. Inspired by this, we propose a neural recognizer for implicit discourse relation analysis, which builds upon a semantic memory that stores knowledge in a distributed fashion. We refer to this recognizer as SeMDER. Starting from word embeddings of discourse arguments, SeMDER employs a shallow encoder to generate a distributed surface representation for a discourse. A semantic encoder with attention to the semantic memory matrix is further established over surface representations. It is able to retrieve a deep semantic meaning representation for the discourse from the memory. Using the surface and semantic representations as input, SeMDER finally predicts implicit discourse relations via a neural recognizer. Experiments on the benchmark data set show that SeMDER benefits from the semantic memory and achieves substantial improvements of 2.56\% on average over current state-of-the-art baselines in terms of F1-score. version:1
arxiv-1602-04984 | Deconvolutional Feature Stacking for Weakly-Supervised Semantic Segmentation | http://arxiv.org/abs/1602.04984 | id:1602.04984 author:Hyo-Eun Kim, Sangheum Hwang category:cs.CV  published:2016-02-16 summary:A weakly-supervised semantic segmentation framework with a tied deconvolutional neural network is presented. Each deconvolution layer in the framework consists of unpooling and deconvolution operations. 'Unpooling' upsamples the input feature map based on unpooling switches defined by corresponding convolution layer's pooling operation. 'Deconvolution' convolves the input unpooled features by using convolutional weights tied with the corresponding convolution layer's convolution operation. The unpooling-deconvolution combination helps to eliminate less discriminative features in a feature extraction stage, since output features of the deconvolution layer are reconstructed from the most discriminative unpooled features instead of the raw one. This results in reduction of false positives in a pixel-level inference stage. All the feature maps restored from the entire deconvolution layers can constitute a rich discriminative feature set according to different abstraction levels. Those features are stacked to be selectively used for generating class-specific activation maps. Under the weak supervision (image-level labels), the proposed framework shows promising results on lesion segmentation in medical images (chest X-rays) and achieves state-of-the-art performance on the PASCAL VOC segmentation dataset in the same experimental condition. version:3
arxiv-1603-03856 | Real-time 3D scene description using Spheres, Cones and Cylinders | http://arxiv.org/abs/1603.03856 | id:1603.03856 author:Kristiyan Georgiev, Motaz Al-Hami, Rolf Lakaemper category:cs.CV  published:2016-03-12 summary:The paper describes a novel real-time algorithm for finding 3D geometric primitives (cylinders, cones and spheres) from 3D range data. In its core, it performs a fast model fitting with a model update in constant time (O(1)) for each new data point added to the model. We use a three stage approach.The first step inspects 1.5D sub spaces, to find ellipses. The next stage uses these ellipses as input by examining their neighborhood structure to form sets of candidates for the 3D geometric primitives. Finally, candidate ellipses are fitted to the geometric primitives. The complexity for point processing is O(n); additional time of lower order is needed for working on significantly smaller amount of mid-level objects. This allows the approach to process 30 frames per second on Kinect depth data, which suggests this approach as a pre-processing step for 3D real-time higher level tasks in robotics, like tracking or feature based mapping. version:1
arxiv-1603-03827 | Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks | http://arxiv.org/abs/1603.03827 | id:1603.03827 author:Ji Young Lee, Franck Dernoncourt category:cs.CL cs.AI cs.LG cs.NE stat.ML  published:2016-03-12 summary:Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction. version:1
arxiv-1603-03185 | Personalized Speech recognition on mobile devices | http://arxiv.org/abs/1603.03185 | id:1603.03185 author:Ian McGraw, Rohit Prabhavalkar, Raziel Alvarez, Montse Gonzalez Arenas, Kanishka Rao, David Rybach, Ouais Alsharif, Hasim Sak, Alexander Gruenstein, Francoise Beaufays, Carolina Parada category:cs.CL cs.LG cs.SD  published:2016-03-10 summary:We describe a large vocabulary speech recognition system that is accurate, has low latency, and yet has a small enough memory and computational footprint to run faster than real-time on a Nexus 5 Android smartphone. We employ a quantized Long Short-Term Memory (LSTM) acoustic model trained with connectionist temporal classification (CTC) to directly predict phoneme targets, and further reduce its memory footprint using an SVD-based compression scheme. Additionally, we minimize our memory footprint by using a single language model for both dictation and voice command domains, constructed using Bayesian interpolation. Finally, in order to properly handle device-specific information, such as proper names and other context-dependent information, we inject vocabulary items into the decoder graph and bias the language model on-the-fly. Our system achieves 13.5% word error rate on an open-ended dictation task, running with a median speed that is seven times faster than real-time. version:2
arxiv-1603-03805 | Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow | http://arxiv.org/abs/1603.03805 | id:1603.03805 author:Huishuai Zhang, Yuejie Chi, Yingbin Liang category:stat.ML  published:2016-03-11 summary:Solving systems of quadratic equations is a central problem in machine learning and signal processing. One important example is phase retrieval, which aims to recover a signal from only magnitudes of its linear measurements. This paper focuses on the situation when the measurements are corrupted by arbitrary outliers, for which the recently developed non-convex gradient descent Wirtinger flow (WF) and truncated Wirtinger flow (TWF) algorithms likely fail. We develop a novel median-TWF algorithm that exploits robustness of sample median to resist arbitrary outliers in the initialization and the gradient update in each iteration. We show that such a non-convex algorithm provably recovers the signal from a near-optimal number of measurements composed of i.i.d. Gaussian entries, up to a logarithmic factor, even when a constant portion of the measurements are corrupted by arbitrary outliers. We further show that median-TWF is also robust when measurements are corrupted by both arbitrary outliers and bounded noise. Our analysis of performance guarantee is accomplished by development of non-trivial concentration measures of median-related quantities, which may be of independent interest. We further provide numerical experiments to demonstrate the effectiveness of the approach. version:1
arxiv-1603-03795 | Demonstrating the Feasibility of Automatic Game Balancing | http://arxiv.org/abs/1603.03795 | id:1603.03795 author:Vanessa Volz, Günter Rudolph, Boris Naujoks category:cs.HC cs.AI cs.NE  published:2016-03-11 summary:Game balancing is an important part of the (computer) game design process, in which designers adapt a game prototype so that the resulting gameplay is as entertaining as possible. In industry, the evaluation of a game is often based on costly playtests with human players. It suggests itself to automate this process using surrogate models for the prediction of gameplay and outcome. In this paper, the feasibility of automatic balancing using simulation- and deck-based objectives is investigated for the card game top trumps. Additionally, the necessity of a multi-objective approach is asserted by a comparison with the only known (single-objective) method. We apply a multi-objective evolutionary algorithm to obtain decks that optimise objectives, e.g. win rate and average number of tricks, developed to express the fairness and the excitement of a game of top trumps. The results are compared with decks from published top trumps decks using simulation-based objectives. The possibility to generate decks better or at least as good as decks from published top trumps decks in terms of these objectives is demonstrated. Our results indicate that automatic balancing with the presented approach is feasible even for more complex games such as real-time strategy games. version:1
arxiv-1603-03793 | Training with Exploration Improves a Greedy Stack-LSTM Parser | http://arxiv.org/abs/1603.03793 | id:1603.03793 author:Miguel Ballesteros, Yoav Goldberg, Chris Dyer, Noah A. Smith category:cs.CL  published:2016-03-11 summary:We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles(Goldberg and Nivre, 2013) instead of cross-entropy minimization. This form of training, which accounts for model predictions at training time rather than assuming an error-free action history, improves parsing accuracies for both English and Chinese, obtaining very strong results for both languages. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural-network. version:1
arxiv-1603-03788 | A Primer on the Signature Method in Machine Learning | http://arxiv.org/abs/1603.03788 | id:1603.03788 author:Ilya Chevyrev, Andrey Kormilitzin category:stat.ML cs.LG stat.ME  published:2016-03-11 summary:In these notes, we wish to provide an introduction to the signature method, focusing on its basic theoretical properties and recent numerical applications. The notes are split into two parts. The first part focuses on the definition and fundamental properties of the signature of a path, or the path signature. We have aimed for a minimalistic approach, assuming only familiarity with classical real analysis and integration theory, and supplementing theory with straightforward examples. We have chosen to focus in detail on the principle properties of the signature which we believe are fundamental to understanding its role in applications. We also present an informal discussion on some of its deeper properties and briefly mention the role of the signature in rough paths theory, which we hope could serve as a light introduction to rough paths for the interested reader. The second part of these notes discusses practical applications of the path signature to the area of machine learning. The signature approach represents a non-parametric way for extraction of characteristic features from data. The data are converted into a multi-dimensional path by means of various embedding algorithms and then processed for computation of individual terms of the signature which summarise certain information contained in the data. The signature thus transforms raw data into a set of features which are used in machine learning tasks. We will review current progress in applications of signatures to machine learning problems. version:1
arxiv-1603-03784 | Towards using social media to identify individuals at risk for preventable chronic illness | http://arxiv.org/abs/1603.03784 | id:1603.03784 author:Dane Bell, Daniel Fried, Luwen Huangfu, Mihai Surdeanu, Stephen Kobourov category:cs.CL cs.CY cs.SI  published:2016-03-11 summary:We describe a strategy for the acquisition of training data necessary to build a social-media-driven early detection system for individuals at risk for (preventable) type 2 diabetes mellitus (T2DM). The strategy uses a game-like quiz with data and questions acquired semi-automatically from Twitter. The questions are designed to inspire participant engagement and collect relevant data to train a public-health model applied to individuals. Prior systems designed to use social media such as Twitter to predict obesity (a risk factor for T2DM) operate on entire communities such as states, counties, or cities, based on statistics gathered by government agencies. Because there is considerable variation among individuals within these groups, training data on the individual level would be more effective, but this data is difficult to acquire. The approach proposed here aims to address this issue. Our strategy has two steps. First, we trained a random forest classifier on data gathered from (public) Twitter statuses and state-level statistics with state-of-the-art accuracy. We then converted this classifier into a 20-questions-style quiz and made it available online. In doing so, we achieved high engagement with individuals that took the quiz, while also building a training set of voluntarily supplied individual-level data for future classification. version:1
arxiv-1603-03783 | Region Graph Based Method for Multi-Object Detection and Tracking using Depth Cameras | http://arxiv.org/abs/1603.03783 | id:1603.03783 author:Sachin Mehta, Balakrishnan Prabhakaran category:cs.CV  published:2016-03-11 summary:In this paper, we propose a multi-object detection and tracking method using depth cameras. Depth maps are very noisy and obscure in object detection. We first propose a region-based method to suppress high magnitude noise which cannot be filtered using spatial filters. Second, the proposed method detect Region of Interests by temporal learning which are then tracked using weighted graph-based approach. We demonstrate the performance of the proposed method on standard depth camera datasets with and without object occlusions. Experimental results show that the proposed method is able to suppress high magnitude noise in depth maps and detect/track the objects (with and without occlusion). version:1
arxiv-1603-03758 | An investigation of coreference phenomena in the biomedical domain | http://arxiv.org/abs/1603.03758 | id:1603.03758 author:Dane Bell, Gus Hahn-Powell, Marco A. Valenzuela-Escárcega, Mihai Surdeanu category:cs.CL  published:2016-03-11 summary:We describe challenges and advantages unique to coreference resolution in the biomedical domain, and a sieve-based architecture that leverages domain knowledge for both entity and event coreference resolution. Domain-general coreference resolution algorithms perform poorly on biomedical documents, because the cues they rely on such as gender are largely absent in this domain, and because they do not encode domain-specific knowledge such as the number and type of participants required in chemical reactions. Moreover, it is difficult to directly encode this knowledge into most coreference resolution algorithms because they are not rule-based. Our rule-based architecture uses sequentially applied hand-designed "sieves", with the output of each sieve informing and constraining subsequent sieves. This architecture provides a 3.2% increase in throughput to our Reach event extraction system with precision parallel to that of the stricter system that relies solely on syntactic patterns for extraction. version:1
arxiv-1603-03724 | Efficient Clustering of Correlated Variables and Variable Selection in High-Dimensional Linear Models | http://arxiv.org/abs/1603.03724 | id:1603.03724 author:Niharika Gauraha, Swapan K. Parui category:stat.ML cs.LG  published:2016-03-11 summary:In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variable selection in high dimensional sparse regression models with strongly correlated variables. To handle correlated variables, the concept of clustering or grouping variables and then pursuing model fitting is widely accepted. When the dimension is very high, finding an appropriate group structure is as difficult as the original problem. The ACL is a three-stage procedure where, at the first stage, we use the Lasso(or its adaptive or thresholded version) to do initial selection, then we also include those variables which are not selected by the Lasso but are strongly correlated with the variables selected by the Lasso. At the second stage we cluster the variables based on the reduced set of predictors and in the third stage we perform sparse estimation such as Lasso on cluster representatives or the group Lasso based on the structures generated by clustering procedure. We show that our procedure is consistent and efficient in finding true underlying population group structure(under assumption of irrepresentable and beta-min conditions). We also study the group selection consistency of our method and we support the theory using simulated and pseudo-real dataset examples. version:1
arxiv-1603-03714 | Distribution Free Learning with Local Queries | http://arxiv.org/abs/1603.03714 | id:1603.03714 author:Galit Bary-Weisberg, Amit Daniely, Shai Shalev-Shwartz category:cs.LG  published:2016-03-11 summary:The model of learning with \emph{local membership queries} interpolates between the PAC model and the membership queries model by allowing the learner to query the label of any example that is similar to an example in the training set. This model, recently proposed and studied by Awasthi, Feldman and Kanade, aims to facilitate practical use of membership queries. We continue this line of work, proving both positive and negative results in the {\em distribution free} setting. We restrict to the boolean cube $\{-1, 1\}^n$, and say that a query is $q$-local if it is of a hamming distance $\le q$ from some training example. On the positive side, we show that $1$-local queries already give an additional strength, and allow to learn a certain type of DNF formulas. On the negative side, we show that even $\left(n^{0.99}\right)$-local queries cannot help to learn various classes including Automata, DNFs and more. Likewise, $q$-local queries for any constant $q$ cannot help to learn Juntas, Decision Trees, Sparse Polynomials and more. Moreover, for these classes, an algorithm that uses $\left(\log^{0.99}(n)\right)$-local queries would lead to a breakthrough in the best known running times. version:1
arxiv-1603-03713 | Cost-sensitive Learning for Bidding in Online Advertising Auctions | http://arxiv.org/abs/1603.03713 | id:1603.03713 author:Flavian Vasile, Damien Lefortier category:cs.LG  published:2016-03-11 summary:One of the most challenging problems in computational advertising is the prediction of ad click and conversion rates for bidding in online advertising auctions. State-of- the-art prediction methods include using the maximum entropy framework (also known as logistic regression) and log linear models. However, one unaddressed problem in the previous approaches is the existence of highly non-uniform misprediction costs. In this paper, we present our approach for making cost-sensitive predictions for bidding in online advertising auctions. We show that one can get significant lifts in offline and online performance by using a simple modification of the logistic loss function. version:1
arxiv-1603-03703 | Searching for Topological Symmetry in Data Haystack | http://arxiv.org/abs/1603.03703 | id:1603.03703 author:Kallol Roy, Anh Tong, Jaesik Choi category:cs.LG  published:2016-03-11 summary:Finding interesting symmetrical topological structures in high-dimensional systems is an important problem in statistical machine learning. Limited amount of available high-dimensional data and its sensitivity to noise pose computational challenges to find symmetry. Our paper presents a new method to find local symmetries in a low-dimensional 2-D grid structure which is embedded in high-dimensional structure. To compute the symmetry in a grid structure, we introduce three legal grid moves (i) Commutation (ii) Cyclic Permutation (iii) Stabilization on sets of local grid squares, grid blocks. The three grid moves are legal transformations as they preserve the statistical distribution of hamming distances in each grid block. We propose and coin the term of grid symmetry of data on the 2-D data grid as the invariance of statistical distributions of hamming distance are preserved after a sequence of grid moves. We have computed and analyzed the grid symmetry of data on multivariate Gaussian distributions and Gamma distributions with noise. version:1
arxiv-1603-03685 | Determination of the edge of criticality in echo state networks through Fisher information maximization | http://arxiv.org/abs/1603.03685 | id:1603.03685 author:Lorenzo Livi, Filippo Maria Bianchi, Cesare Alippi category:physics.data-an cs.LG cs.NE  published:2016-03-11 summary:It is a widely accepted fact that the computational capability of recurrent neural networks is maximized on the so-called "edge of criticality". Once in this configuration, the network performs efficiently on a specific application both in terms of (i) low prediction error and (ii) high short-term memory capacity. Since the behavior of recurrent networks is strongly influenced by the particular input signal driving the dynamics, a universal, application-independent method for determining the edge of criticality is still missing. In this paper, we propose a theoretically motivated method based on Fisher information for determining the edge of criticality in recurrent neural networks. It is proven that Fisher information is maximized for (finite-size) systems operating in such critical regions. However, Fisher information is notoriously difficult to compute and either requires the probability density function or the conditional dependence of the system states with respect to the model parameters. The paper exploits a recently-developed non-parametric estimator of the Fisher information matrix and provides a method to determine the critical region of echo state networks, a particular class of recurrent networks. The considered control parameters, which indirectly affect the echo state network performance, are suitably controlled to identify a collection of network configurations lying on the edge of criticality and, as such, maximizing Fisher information and computational performance. version:1
arxiv-1603-03669 | Learning Gaze Transitions from Depth to Improve Video Saliency Estimation | http://arxiv.org/abs/1603.03669 | id:1603.03669 author:G. Leifman, D. Rudoy, T. Swedish, E. Bayro-Corrochano, R. Raskar category:cs.CV  published:2016-03-11 summary:In this paper we introduce a novel Depth-Aware Video Saliency approach to predict human focus of attention when viewing RGBD videos on regular 2D screens. We train a generative convolutional neural network which predicts a saliency map for a frame, given the fixation map of the previous frame. Saliency estimation in this scenario is highly important since in the near future 3D video content will be easily acquired and yet hard to display. This can be explained, on the one hand, by the dramatic improvement of 3D-capable acquisition equipment. On the other hand, despite the considerable progress in 3D display technologies, most of the 3D displays are still expensive and require wearing special glasses. To evaluate the performance of our approach, we present a new comprehensive database of eye-fixation ground-truth for RGBD videos. Our experiments indicate that integrating depth into video saliency calculation is beneficial. We demonstrate that our approach outperforms state-of-the-art methods for video saliency, achieving 15% relative improvement. version:1
arxiv-1603-03657 | Efficient forward propagation of time-sequences in convolutional neural networks using Deep Shifting | http://arxiv.org/abs/1603.03657 | id:1603.03657 author:Koen Groenland, Sander Bohte category:cs.LG cs.CV cs.NE  published:2016-03-11 summary:When a Convolutional Neural Network is used for on-the-fly evaluation of continuously updating time-sequences, many redundant convolution operations are performed. We propose the method of Deep Shifting, which remembers previously calculated results of convolution operations in order to minimize the number of calculations. The reduction in complexity is at least a constant and in the best case quadratic. We demonstrate that this method does indeed save significant computation time in a practical implementation, especially when the networks receives a large number of time-frames. version:1
arxiv-1603-03627 | Learning from Imbalanced Multiclass Sequential Data Streams Using Dynamically Weighted Conditional Random Fields | http://arxiv.org/abs/1603.03627 | id:1603.03627 author:Roberto L. Shinmoto Torres, Damith C. Ranasinghe, Qinfeng Shi, Anton van den Hengel category:cs.LG  published:2016-03-11 summary:The present study introduces a method for improving the classification performance of imbalanced multiclass data streams from wireless body worn sensors. Data imbalance is an inherent problem in activity recognition caused by the irregular time distribution of activities, which are sequential and dependent on previous movements. We use conditional random fields (CRF), a graphical model for structured classification, to take advantage of dependencies between activities in a sequence. However, CRFs do not consider the negative effects of class imbalance during training. We propose a class-wise dynamically weighted CRF (dWCRF) where weights are automatically determined during training by maximizing the expected overall F-score. Our results based on three case studies from a healthcare application using a batteryless body worn sensor, demonstrate that our method, in general, improves overall and minority class F-score when compared to other CRF based classifiers and achieves similar or better overall and class-wise performance when compared to SVM based classifiers under conditions of limited training data. We also confirm the performance of our approach using an additional battery powered body worn sensor dataset, achieving similar results in cases of high class imbalance. version:1
arxiv-1408-5286 | Designing labeled graph classifiers by exploiting the Rényi entropy of the dissimilarity representation | http://arxiv.org/abs/1408.5286 | id:1408.5286 author:Lorenzo Livi category:cs.CV cs.IT math.IT stat.ML I.2.6; H.1.1; E.4  published:2014-08-22 summary:Representing patterns by complex relational structures, such as labeled graphs, is becoming an increasingly common practice in the broad field of computational intelligence. Accordingly, a wide repertoire of pattern recognition tools, such as classifiers and knowledge discovery procedures, are nowadays available and tested for various labeled graph data types. However, the design of effective learning and mining procedures operating in the space of labeled graphs is still a challenging problem, especially from the computational complexity viewpoint. In this paper, we present a major improvement of a general-purpose graph classification system, which is conceived on an interplay among dissimilarity representation, clustering, information-theoretic techniques, and evolutionary optimization. The improvement focuses on a specific key subroutine of the system that performs the compression of the input data. We prove different theorems which are fundamental to the setting of such a compression operation. We demonstrate the effectiveness of the resulting classifier by benchmarking the developed variants on well-known datasets of labeled graphs, considering as distinct performance indicators the classification accuracy, the computing time, and the parsimony in terms of structural complexity of the synthesized classification model. Overall, the results show state-of-the-art standards in terms of test set accuracy, while achieving considerable reductions for what concerns both the effective computing time and model complexity. version:4
arxiv-1601-06719 | Relief R-CNN : Utilizing Convolutional Feature Interrelationship for Object Detection | http://arxiv.org/abs/1601.06719 | id:1601.06719 author:Guiying Li, Junlong Liu, Chunhui Jiang, Liangpeng Zhang, Ke Tang, Yufeng Liu category:cs.CV  published:2016-01-25 summary:The state-of-the-art object detection pipeline needs a set of object location hypotheses followed by a deep CNN classifier. Previous research on this paradigm, usually extracts features from the same image for the two steps separately, which is time consuming and is hard to optimize. This work shows that the high-level patterns of feature values in deep convolutional feature map contain plenty of useful spatial information and proposes a new deep learning approach to object detection, namely Relief R-CNN ($R^2$-CNN). By extracting positions of objects from these high-level patterns, $R^2$-CNN generates region proposals and performs deep classification simultaneously using the same forward CNN features, unifying the formerly separated object detection process. In this way, $R^2$-CNN does not involve additional information extraction process for region proposal generation, considerably reducing the total computation costs. In addition, a recursive fine-tune technique is also developed for refining coarse proposals. Empirical results showed that our $R^2$-CNN had a very high speed and a reasonable detection rate even in the presence of limit on the proposal number, indicating that the region proposals generated by our $R^2$-CNN are in excellent quality. version:2
arxiv-1603-03768 | A Recursive Born Approach to Nonlinear Inverse Scattering | http://arxiv.org/abs/1603.03768 | id:1603.03768 author:Ulugbek S. Kamilov, Dehong Liu, Hassan Mansour, Petros T. Boufounos category:cs.LG physics.optics  published:2016-03-11 summary:The Iterative Born Approximation (IBA) is a well-known method for describing waves scattered by semi-transparent objects. In this paper, we present a novel nonlinear inverse scattering method that combines IBA with an edge-preserving total variation (TV) regularizer. The proposed method is obtained by relating iterations of IBA to layers of a feedforward neural network and developing a corresponding error backpropagation algorithm for efficiently estimating the permittivity of the object. Simulations illustrate that, by accounting for multiple scattering, the method successfully recovers the permittivity distribution where the traditional linear inverse scattering fails. version:1
arxiv-1603-03610 | A short proof that $O_2$ is an MCFL | http://arxiv.org/abs/1603.03610 | id:1603.03610 author:Mark-Jan Nederhof category:cs.FL cs.CL 68T50 I.2.7; F.4.2  published:2016-03-11 summary:We present a new proof that $O_2$ is a multiple context-free language. It contrasts with a recent proof by Salvati (2015) in its avoidance of concepts that seem specific to two-dimensional geometry, such as the complex exponential function. Our simple proof creates realistic prospects of widening the results to higher dimensions. This finding is of central importance to the relation between extreme free word order and classes of grammars used to describe the syntax of natural language. version:1
arxiv-1603-01520 | Optimized Polynomial Evaluation with Semantic Annotations | http://arxiv.org/abs/1603.01520 | id:1603.01520 author:Daniel Rubio Bonilla, Colin W. Glass, Jan Kuper category:cs.PL cs.CL B.1.4  published:2016-03-04 summary:In this paper we discuss how semantic annotations can be used to introduce mathematical algorithmic information of the underlying imperative code to enable compilers to produce code transformations that will enable better performance. By using this approaches not only good performance is achieved, but also better programmability, maintainability and portability across different hardware architectures. To exemplify this we will use polynomial equations of different degrees. version:3
arxiv-1507-04888 | Maximum Entropy Deep Inverse Reinforcement Learning | http://arxiv.org/abs/1507.04888 | id:1507.04888 author:Markus Wulfmeier, Peter Ondruska, Ingmar Posner category:cs.LG  published:2015-07-17 summary:This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations. version:3
arxiv-1603-03590 | Fast Optical Flow using Dense Inverse Search | http://arxiv.org/abs/1603.03590 | id:1603.03590 author:Till Kroeger, Radu Timofte, Dengxin Dai, Luc Van Gool category:cs.CV cs.RO  published:2016-03-11 summary:Most recent works in optical flow extraction focus on the accuracy and neglect the time complexity. However, in real-life visual applications, such as tracking, activity detection and recognition, the time complexity is critical. We propose a solution with very low time complexity and competitive accuracy for the computation of dense optical flow. It consists of three parts: 1) inverse search for patch correspondences; 2) dense displacement field creation through patch aggregation along multiple scales; 3) variational refinement. At the core of our Dense Inverse Search-based method (DIS) is the efficient search of correspondences inspired by the inverse compositional image alignment proposed by Baker and Matthews in 2001. DIS is competitive on standard optical flow benchmarks with large displacements. DIS runs at 300Hz up to 600Hz on a single CPU core, reaching the temporal resolution of human's biological vision system. It is order(s) of magnitude faster than state-of-the-art methods in the same range of accuracy, making DIS ideal for visual applications. version:1
arxiv-1506-04908 | Learning with Clustering Penalties | http://arxiv.org/abs/1506.04908 | id:1506.04908 author:Vincent Roulet, Fajwel Fogel, Alexandre d'Aspremont, Francis Bach category:cs.LG 68T05  91C20  published:2015-06-16 summary:We study supervised learning problems using clustering penalties to impose structure on either features, tasks or samples, seeking to help both prediction and interpretation. This arises naturally in problems involving dimensionality reduction, transfer learning or regression clustering. We derive a unified optimization formulation handling these three settings and produce algorithms whose core iteration complexity amounts to a k-means clustering step, which can be approximated efficiently. We test the robustness of our methods on artificial data sets as well as real data extracted from movie reviews and a corpus of text documents. version:2
arxiv-1603-03304 | Template Matching on the Roto-Translation Group | http://arxiv.org/abs/1603.03304 | id:1603.03304 author:Erik J. Bekkers, Marco Loog, Bart M. ter Haar Romeny, Remco Duits category:cs.CV math.GR  published:2016-03-10 summary:We propose a template matching method for the detection of 2D image objects that are characterized by orientation patterns. Our method is based on data representations via orientation scores, which are functions on the space of positions and orientations, and which are obtained via a wavelet-type transform. This new representation allows us to detect orientation patterns in an intuitive and direct way, namely via cross-correlations. Additionally, we propose a generalized linear regression framework for the construction of suitable templates using smoothing splines. Here, it is important to recognize a curved geometry on the position-orientation domain, which we identify with the Lie group SE(2): the roto-translation group. Templates are then optimized in a B-spline basis, and smoothness is defined with respect to the curved geometry. We achieve state-of-the-art results on two important detection problems in retinal imaging: detection of the optic nerve head (99.83\% success rate on 1737 images) and detection of the fovea (99.32\% success rate on 1616 images). The high performance is due to inclusion of both intensity and orientation features with effective geometric priors in the template matching. Moreover, our method is fast due to a cross-correlation based matching approach. version:2
arxiv-1603-03541 | Watch-n-Patch: Unsupervised Learning of Actions and Relations | http://arxiv.org/abs/1603.03541 | id:1603.03541 author:Chenxia Wu, Jiemi Zhang, Ozan Sener, Bart Selman, Silvio Savarese, Ashutosh Saxena category:cs.CV cs.LG cs.RO  published:2016-03-11 summary:There is a large variation in the activities that humans perform in their everyday lives. We consider modeling these composite human activities which comprises multiple basic level actions in a completely unsupervised setting. Our model learns high-level co-occurrence and temporal relations between the actions. We consider the video as a sequence of short-term action clips, which contains human-words and object-words. An activity is about a set of action-topics and object-topics indicating which actions are present and which objects are interacting with. We then propose a new probabilistic model relating the words and the topics. It allows us to model long-range action relations that commonly exist in the composite activities, which is challenging in previous works. We apply our model to the unsupervised action segmentation and clustering, and to a novel application that detects forgotten actions, which we call action patching. For evaluation, we contribute a new challenging RGB-D activity video dataset recorded by the new Kinect v2, which contains several human daily activities as compositions of multiple actions interacting with different objects. Moreover, we develop a robotic system that watches people and reminds people by applying our action patching algorithm. Our robotic setup can be easily deployed on any assistive robot. version:1
arxiv-1603-03515 | Dimension Coupling: Optimal Active Learning of Halfspaces via Query Synthesis | http://arxiv.org/abs/1603.03515 | id:1603.03515 author:Lin Chen, Hamed Hassani, Amin Karbasi category:cs.AI cs.IT cs.LG math.IT  published:2016-03-11 summary:In this paper, we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries. This problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to elicit information (e.g., automated science) or to evade detection with minimal cost (e.g., adversarial reverse engineering). We develop a general framework, called dimension coupling (DC), that 1) reduces a d-dimensional learning problem to d-1 low-dimensional sub-problems, 2) solves each sub-problem efficiently, and 3) appropriately aggregates the results and outputs a linear classifier. We consider the three most common scenarios in the literature: idealized noise-free, independent noise realizations, and agnostic settings. We show that the DC framework avoids the curse of dimensionality: its computational complexity in all three cases scales linearly with the dimension. Moreover, in the noiseless and noisy cases, we show that the query complexity of DC is near optimal (within a constant factor of the optimum algorithm). We also develop an agnostic variant of DC for which we provide strong theoretical guarantees. To further support our theoretical analysis, we compare the performance of DC with the existing work in all three settings. We observe that DC consistently outperforms the prior arts in terms of query complexity while often running orders of magnitude faster. version:1
arxiv-1512-02895 | Embedding Label Structures for Fine-Grained Feature Representation | http://arxiv.org/abs/1512.02895 | id:1512.02895 author:Xiaofan Zhang, Feng Zhou, Yuanqing Lin, Shaoting Zhang category:cs.CV  published:2015-12-09 summary:Recent algorithms in convolutional neural networks (CNN) considerably advance the fine-grained image classification, which aims to differentiate subtle differences among subordinate classes. However, previous studies have rarely focused on learning a fined-grained and structured feature representation that is able to locate similar images at different levels of relevance, e.g., discovering cars from the same make or the same model, both of which require high precision. In this paper, we propose two main contributions to tackle this problem. 1) A multi-task learning framework is designed to effectively learn fine-grained feature representations by jointly optimizing both classification and similarity constraints. 2) To model the multi-level relevance, label structures such as hierarchy or shared attributes are seamlessly embedded into the framework by generalizing the triplet loss. Extensive and thorough experiments have been conducted on three fine-grained datasets, i.e., the Stanford car, the car-333, and the food datasets, which contain either hierarchical labels or shared attributes. Our proposed method has achieved very competitive performance, i.e., among state-of-the-art classification accuracy. More importantly, it significantly outperforms previous fine-grained feature representations for image retrieval at different levels of relevance. version:2
arxiv-1511-07938 | Temporal Convolutional Neural Networks for Diagnosis from Lab Tests | http://arxiv.org/abs/1511.07938 | id:1511.07938 author:Narges Razavian, David Sontag category:cs.LG  published:2015-11-25 summary:Early diagnosis of treatable diseases is essential for improving healthcare, and many diseases' onsets are predictable from annual lab tests and their temporal trends. We introduce a multi-resolution convolutional neural network for early detection of multiple diseases from irregularly measured sparse lab values. Our novel architecture takes as input both an imputed version of the data and a binary observation matrix. For imputing the temporal sparse observations, we develop a flexible, fast to train method for differentiable multivariate kernel regression. Our experiments on data from 298K individuals over 8 years, 18 common lab measurements, and 171 diseases show that the temporal signatures learned via convolution are significantly more predictive than baselines commonly used for early disease diagnosis. version:4
arxiv-1603-03417 | Texture Networks: Feed-forward Synthesis of Textures and Stylized Images | http://arxiv.org/abs/1603.03417 | id:1603.03417 author:Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky category:cs.CV  published:2016-03-10 summary:Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys~et~al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions. version:1
arxiv-1603-02905 | Lexical bundles in computational linguistics academic literature | http://arxiv.org/abs/1603.02905 | id:1603.02905 author:Adel Rahimi category:cs.CL  published:2016-03-09 summary:In this study we analyzed a corpus of 8 million words academic literature from Computational lingustics' academic literature. the lexical bundles from this corpus are categorized based on structures and functions. version:2
arxiv-1506-00053 | Efficient Bayesian experimentation using an expected information gain lower bound | http://arxiv.org/abs/1506.00053 | id:1506.00053 author:Panagiotis Tsilifis, Roger G. Ghanem, Paris Hajali category:stat.ML physics.geo-ph stat.CO stat.ME  published:2015-05-30 summary:Experimental design is crucial for inference where limitations in the data collection procedure are present due to cost or other restrictions. Optimal experimental designs determine parameters that in some appropriate sense make the data the most informative possible. In a Bayesian setting this is translated to updating to the best possible posterior. Information theoretic arguments have led to the formation of the expected information gain as a design criterion. This can be evaluated mainly by Monte Carlo sampling and maximized by using stochastic approximation methods, both known for being computationally expensive tasks. We propose a framework where a lower bound of the expected information gain is used as an alternative design criterion. In addition to alleviating the computational burden, this also addresses issues concerning estimation bias. The problem of permeability inference in a large contaminated area is used to demonstrate the validity of our approach where we employ the massively parallel version of the multiphase multicomponent simulator TOUGH2 to simulate contaminant transport and a Polynomial Chaos approximation of the forward model that further accelerates the objective function evaluations. The proposed methodology is demonstrated to a setting where field measurements are available. version:2
arxiv-1406-5370 | Spectral Ranking using Seriation | http://arxiv.org/abs/1406.5370 | id:1406.5370 author:Fajwel Fogel, Alexandre d'Aspremont, Milan Vojnovic category:cs.LG cs.AI stat.ML 62F07  06A07  90C27  published:2014-06-20 summary:We describe a seriation algorithm for ranking a set of items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than classical scoring methods. Finally, we bound the ranking error when only a random subset of the comparions are observed. An additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods. version:4
arxiv-1603-03336 | Scalable Linear Causal Inference for Irregularly Sampled Time Series with Long Range Dependencies | http://arxiv.org/abs/1603.03336 | id:1603.03336 author:Francois W. Belletti, Evan R. Sparks, Michael J. Franklin, Alexandre M. Bayen, Joseph E. Gonzalez category:cs.LG stat.ME  published:2016-03-10 summary:Linear causal analysis is central to a wide range of important application spanning finance, the physical sciences, and engineering. Much of the existing literature in linear causal analysis operates in the time domain. Unfortunately, the direct application of time domain linear causal analysis to many real-world time series presents three critical challenges: irregular temporal sampling, long range dependencies, and scale. Moreover, real-world data is often collected at irregular time intervals across vast arrays of decentralized sensors and with long range dependencies which make naive time domain correlation estimators spurious. In this paper we present a frequency domain based estimation framework which naturally handles irregularly sampled data and long range dependencies while enabled memory and communication efficient distributed processing of time series data. By operating in the frequency domain we eliminate the need to interpolate and help mitigate the effects of long range dependencies. We implement and evaluate our new work-flow in the distributed setting using Apache Spark and demonstrate on both Monte Carlo simulations and high-frequency financial trading that we can accurately recover causal structure at scale. version:1
arxiv-1510-07867 | Some like it hot - visual guidance for preference prediction | http://arxiv.org/abs/1510.07867 | id:1510.07867 author:Rasmus Rothe, Radu Timofte, Luc Van Gool category:cs.CV  published:2015-10-27 summary:For people first impressions of someone are of determining importance. They are hard to alter through further information. This begs the question if a computer can reach the same judgement. Earlier research has already pointed out that age, gender, and average attractiveness can be estimated with reasonable precision. We improve the state-of-the-art, but also predict - based on someone's known preferences - how much that particular person is attracted to a novel face. Our computational pipeline comprises a face detector, convolutional neural networks for the extraction of deep features, standard support vector regression for gender, age and facial beauty, and - as the main novelties - visual regularized collaborative filtering to infer inter-person preferences as well as a novel regression technique for handling visual queries without rating history. We validate the method using a very large dataset from a dating site as well as images from celebrities. Our experiments yield convincing results, i.e. we predict 76% of the ratings correctly solely based on an image, and reveal some sociologically relevant conclusions. We also validate our collaborative filtering solution on the standard MovieLens rating dataset, augmented with movie posters, to predict an individual's movie rating. We demonstrate our algorithms on howhot.io which went viral around the Internet with more than 50 million pictures evaluated in the first month. version:2
arxiv-1503-06058 | Sequential Monte Carlo Methods for System Identification | http://arxiv.org/abs/1503.06058 | id:1503.06058 author:Thomas B. Schön, Fredrik Lindsten, Johan Dahlin, Johan Wågberg, Christian A. Naesseth, Andreas Svensson, Liang Dai category:stat.CO math.OC stat.ML  published:2015-03-20 summary:One of the key challenges in identifying nonlinear and possibly non-Gaussian state space models (SSMs) is the intractability of estimating the system state. Sequential Monte Carlo (SMC) methods, such as the particle filter (introduced more than two decades ago), provide numerical solutions to the nonlinear state estimation problems arising in SSMs. When combined with additional identification techniques, these algorithms provide solid solutions to the nonlinear system identification problem. We describe two general strategies for creating such combinations and discuss why SMC is a natural tool for implementing these strategies. version:3
arxiv-1603-03281 | An Innovative Imputation and Classification Approach for Accurate Disease Prediction | http://arxiv.org/abs/1603.03281 | id:1603.03281 author:Yelipe UshaRani, P. Sammulal category:cs.DB cs.IR cs.LG  published:2016-03-10 summary:Imputation of missing attribute values in medical datasets for extracting hidden knowledge from medical datasets is an interesting research topic of interest which is very challenging. One cannot eliminate missing values in medical records. The reason may be because some tests may not been conducted as they are cost effective, values missed when conducting clinical trials, values may not have been recorded to name some of the reasons. Data mining researchers have been proposing various approaches to find and impute missing values to increase classification accuracies so that disease may be predicted accurately. In this paper, we propose a novel imputation approach for imputation of missing values and performing classification after fixing missing values. The approach is based on clustering concept and aims at dimensionality reduction of the records. The case study discussed shows that missing values can be fixed and imputed efficiently by achieving dimensionality reduction. The importance of proposed approach for classification is visible in the case study which assigns single class label in contrary to multi-label assignment if dimensionality reduction is not performed. version:1
arxiv-1602-02505 | Binarized Neural Networks | http://arxiv.org/abs/1602.02505 | id:1602.02505 author:Itay Hubara, Daniel Soudry, Ran El Yaniv category:cs.LG cs.NE  published:2016-02-08 summary:We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time and when computing the parameters' gradient at train-time. We conduct two sets of experiments, each based on a different framework, namely Torch7 and Theano, where we train BNNs on MNIST, CIFAR-10 and SVHN, and achieve nearly state-of-the-art results. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which might lead to a great increase in power-efficiency. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available. version:3
arxiv-1603-03234 | Instance-Aware Hashing for Multi-Label Image Retrieval | http://arxiv.org/abs/1603.03234 | id:1603.03234 author:Hanjiang Lai, Pan Yan, Xiangbo Shu, Yunchao Wei, Shuicheng Yan category:cs.CV  published:2016-03-10 summary:Similarity-preserving hashing is a commonly used method for nearest neighbour search in large-scale image retrieval. For image retrieval, deep-networks-based hashing methods are appealing since they can simultaneously learn effective image representations and compact hash codes. This paper focuses on deep-networks-based hashing for multi-label images, each of which may contain objects of multiple categories. In most existing hashing methods, each image is represented by one piece of hash code, which is referred to as semantic hashing. This setting may be suboptimal for multi-label image retrieval. To solve this problem, we propose a deep architecture that learns \textbf{instance-aware} image representations for multi-label image data, which are organized in multiple groups, with each group containing the features for one category. The instance-aware representations not only bring advantages to semantic hashing, but also can be used in category-aware hashing, in which an image is represented by multiple pieces of hash codes and each piece of code corresponds to a category. Extensive evaluations conducted on several benchmark datasets demonstrate that, for both semantic hashing and category-aware hashing, the proposed method shows substantial improvement over the state-of-the-art supervised and unsupervised hashing methods. version:1
arxiv-1602-03619 | Optimality of Belief Propagation for Crowdsourced Classification | http://arxiv.org/abs/1602.03619 | id:1602.03619 author:Jungseul Ok, Sewoong Oh, Jinwoo Shin, Yung Yi category:cs.LG stat.ML  published:2016-02-11 summary:Crowdsourcing systems are popular for solving large-scale labelling tasks with low-paid (or even non-paid) workers. We study the problem of recovering the true labels from the possibly erroneous crowdsourced labels under the popular Dawid-Skene model. To address this inference problem, several algorithms have recently been proposed, but the best known guarantee is still significantly larger than the fundamental limit. We close this gap under a simple but canonical scenario where each worker is assigned at most two tasks. In particular, we introduce a tighter lower bound on the fundamental limit and prove that Belief Propagation (BP) exactly matches this lower bound. The guaranteed optimality of BP is the strongest in the sense that it is information-theoretically impossible for any other algorithm to correctly label a larger fraction of the tasks. In the general setting, when more than two tasks are assigned to each worker, we establish the dominance result on BP that it outperforms all existing algorithms with provable guarantees. Experimental results suggest that BP is close to optimal for all regimes considered, while all other algorithms show suboptimal performances in certain regimes. version:2
arxiv-1408-1000 | Estimating Renyi Entropy of Discrete Distributions | http://arxiv.org/abs/1408.1000 | id:1408.1000 author:Jayadev Acharya, Alon Orlitsky, Ananda Theertha Suresh, Himanshu Tyagi category:cs.IT cs.DS cs.LG math.IT  published:2014-08-02 summary:It was recently shown that estimating the Shannon entropy $H({\rm p})$ of a discrete $k$-symbol distribution ${\rm p}$ requires $\Theta(k/\log k)$ samples, a number that grows near-linearly in the support size. In many applications $H({\rm p})$ can be replaced by the more general R\'enyi entropy of order $\alpha$, $H_\alpha({\rm p})$. We determine the number of samples needed to estimate $H_\alpha({\rm p})$ for all $\alpha$, showing that $\alpha < 1$ requires a super-linear, roughly $k^{1/\alpha}$ samples, noninteger $\alpha>1$ requires a near-linear $k$ samples, but, perhaps surprisingly, integer $\alpha>1$ requires only $\Theta(k^{1-1/\alpha})$ samples. Furthermore, developing on a recently established connection between polynomial approximation and estimation of additive functions of the form $\sum_{x} f({\rm p}_x)$, we reduce the sample complexity for noninteger values of $\alpha$ by a factor of $\log k$ compared to the empirical estimator. The estimators achieving these bounds are simple and run in time linear in the number of samples. Our lower bounds provide explicit constructions of distributions with different R\'enyi entropies that are hard to distinguish. version:3
arxiv-1603-03158 | Scenario Submodular Cover | http://arxiv.org/abs/1603.03158 | id:1603.03158 author:Nathaniel Grammel, Lisa Hellerstein, Devorah Kletenik, Patrick Lin category:cs.DS cs.LG  published:2016-03-10 summary:Many problems in Machine Learning can be modeled as submodular optimization problems. Recent work has focused on stochastic or adaptive versions of these problems. We consider the Scenario Submodular Cover problem, which is a counterpart to the Stochastic Submodular Cover problem studied by Golovin and Krause. In Scenario Submodular Cover, the goal is to produce a cover with minimum expected cost, where the expectation is with respect to an empirical joint distribution, given as input by a weighted sample of realizations. In contrast, in Stochastic Submodular Cover, the variables of the input distribution are assumed to be independent, and the distribution of each variable is given as input. Building on algorithms developed by Cicalese et al. and Golovin and Krause for related problems, we give two approximation algorithms for Scenario Submodular Cover over discrete distributions. The first achieves an approximation factor of O(log Qm), where m is the size of the sample and Q is the goal utility. The second, simpler algorithm achieves an approximation bound of O(log QW), where Q is the goal utility and W is the sum of the integer weights. (Both bounds assume an integer-valued utility function.) Our results yield approximation bounds for other problems involving non-independent distributions that are explicitly specified by their support. version:1
arxiv-1603-03149 | Real time error detection in metal arc welding process using Artificial Neural Netwroks | http://arxiv.org/abs/1603.03149 | id:1603.03149 author:Prashant Sharma, Shaju K. Albert, S. Rajeswari category:cs.NE  published:2016-03-10 summary:Quality assurance in production line demands reliable weld joints. Human made errors is a major cause of faulty production. Promptly Identifying errors in the weld while welding is in progress will decrease the post inspection cost spent on the welding process. Electrical parameters generated during welding, could able to characterize the process efficiently. Parameter values are collected using high speed data acquisition system. Time series analysis tasks such as filtering, pattern recognition etc. are performed over the collected data. Filtering removes the unwanted noisy signal components and pattern recognition task segregate error patterns in the time series based upon similarity, which is performed by Self Organized mapping clustering algorithm. Welder quality is thus compared by detecting and counting number of error patterns appeared in his parametric time series. Moreover, Self Organized mapping algorithm provides the database in which patterns are segregated into two classes either desirable or undesirable. Database thus generated is used to train the classification algorithms, and thereby automating the real time error detection task. Multi Layer Perceptron and Radial basis function are the two classification algorithms used, and their performance has been compared based on metrics such as specificity, sensitivity, accuracy and time required in training. version:1
arxiv-1503-03512 | Is language evolution grinding to a halt?: Exploring the life and death of words in English fiction | http://arxiv.org/abs/1503.03512 | id:1503.03512 author:Eitan Adam Pechenick, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL cs.IT math.IT physics.soc-ph stat.AP  published:2015-03-11 summary:The Google Books corpus, derived from millions of books in a range of major languages, would seem to offer many possibilities for research into cultural, social, and linguistic evolution. In a previous work, we found that the 2009 and 2012 versions of the unfiltered English data set as well as the 2009 version of the English Fiction data set are all heavily saturated with scientific and medical literature, rendering them unsuitable for rigorous analysis [Pechenick, Danforth and Dodds, PLoS ONE, 10, e0137041, 2015]. By contrast, the 2012 version of English Fiction appeared to be uncompromised, and we use this data set to explore language dynamics for English from 1820--2000. We critique an earlier method for measuring birth and death rates of words, and provide a robust, principled approach to examining the volume of word flux across various relative frequency usage thresholds. We use the contributions to the Jensen-Shannon divergence of words crossing thresholds between consecutive decades to illuminate the major driving factors behind the flux. We find that while individual word usage may vary greatly, the overall statistical structure of the language appears to remain fairly stable. We also find indications that scholarly works about fiction are strongly represented in the 2012 English Fiction corpus, and suggest that a future revision of the corpus should attempt to separate critical works from fiction itself. version:3
arxiv-1512-07372 | Multi-centrality Graph Spectral Decompositions and their Application to Cyber Intrusion Detection | http://arxiv.org/abs/1512.07372 | id:1512.07372 author:Pin-Yu Chen, Sutanay Choudhury, Alfred O. Hero category:cs.SI cs.CR stat.ML  published:2015-12-23 summary:Many modern datasets can be represented as graphs and hence spectral decompositions such as graph principal component analysis (PCA) can be useful. Distinct from previous graph decomposition approaches based on subspace projection of a single topological feature, e.g., the Fiedler vector of centered graph adjacency matrix (graph Laplacian), we propose spectral decomposition approaches to graph PCA and graph dictionary learning that integrate multiple features, including graph walk statistics, centrality measures and graph distances to reference nodes. In this paper we propose a new PCA method for single graph analysis, called multi-centrality graph PCA (MC-GPCA), and a new dictionary learning method for ensembles of graphs, called multi-centrality graph dictionary learning (MC-GDL), both based on spectral decomposition of multi-centrality matrices. As an application to cyber intrusion detection, MC-GPCA can be an effective indicator of anomalous connectivity pattern and MC-GDL can provide discriminative basis for attack classification. version:2
arxiv-1603-03030 | Global and Local Uncertainty Principles for Signals on Graphs | http://arxiv.org/abs/1603.03030 | id:1603.03030 author:Nathanael Perraudin, Benjamin Ricaud, David Shuman, Pierre Vandergheynst category:stat.ML math-ph math.MP  published:2016-03-10 summary:Uncertainty principles such as Heisenberg's provide limits on the time-frequency concentration of a signal, and constitute an important theoretical tool for designing and evaluating linear signal transforms. Generalizations of such principles to the graph setting can inform dictionary design for graph signals, lead to algorithms for reconstructing missing information from graph signals via sparse representations, and yield new graph analysis tools. While previous work has focused on generalizing notions of spreads of a graph signal in the vertex and graph spectral domains, our approach is to generalize the methods of Lieb in order to develop uncertainty principles that provide limits on the concentration of the analysis coefficients of any graph signal under a dictionary transform whose atoms are jointly localized in the vertex and graph spectral domains. One challenge we highlight is that due to the inhomogeneity of the underlying graph data domain, the local structure in a single small region of the graph can drastically affect the uncertainty bounds for signals concentrated in different regions of the graph, limiting the information provided by global uncertainty principles. Accordingly, we suggest a new way to incorporate a notion of locality, and develop local uncertainty principles that bound the concentration of the analysis coefficients of each atom of a localized graph spectral filter frame in terms of quantities that depend on the local structure of the graph around the center vertex of the given atom. Finally, we demonstrate how our proposed local uncertainty measures can improve the random sampling of graph signals. version:1
arxiv-1603-03112 | Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre) | http://arxiv.org/abs/1603.03112 | id:1603.03112 author:Lifu Huang, Jonathan May, Xiaoman Pan, Heng Ji category:cs.CL cs.AI  published:2016-03-10 summary:Recent research has shown great progress on fine-grained entity typing. Most existing methods require pre-defining a set of types and training a multi-class classifier from a large labeled data set based on multi-level linguistic features. They are thus limited to certain domains, genres and languages. In this paper, we propose a novel unsupervised entity typing framework by combining symbolic and distributional semantics. We start from learning general embeddings for each entity mention, compose the embeddings of specific contexts using linguistic structures, link the mention to knowledge bases and learn its related knowledge representations. Then we develop a novel joint hierarchical clustering and linking algorithm to type all mentions using these representations. This framework doesn't rely on any annotated data, predefined typing schema, or hand-crafted features, therefore it can be quickly adapted to a new domain, genre and language. Furthermore, it has great flexibility at incorporating linguistic structures (e.g., Abstract Meaning Representation (AMR), dependency relations) to improve specific context representation. Experiments on genres (news and discussion forum) show comparable performance with state-of-the-art supervised typing systems trained from a large amount of labeled data. Results on various languages (English, Chinese, Japanese, Hausa, and Yoruba) and domains (general and biomedical) demonstrate the portability of our framework. version:1
arxiv-1603-03101 | Recursive Recurrent Nets with Attention Modeling for OCR in the Wild | http://arxiv.org/abs/1603.03101 | id:1603.03101 author:Chen-Yu Lee, Simon Osindero category:cs.CV  published:2016-03-09 summary:We present recursive recurrent neural networks with attention modeling (R$^2$AM) for lexicon-free optical character recognition in natural scene images. The primary advantages of the proposed method are: (1) use of recursive convolutional neural networks (CNNs), which allow for parametrically efficient and effective image feature extraction; (2) an implicitly learned character-level language model, embodied in a recurrent neural network which avoids the need to use N-grams; and (3) the use of a soft-attention mechanism, allowing the model to selectively exploit image features in a coordinated way, and allowing for end-to-end training within a standard backpropagation framework. We validate our method with state-of-the-art performance on challenging benchmark datasets: Street View Text, IIIT5k, ICDAR and Synth90k. version:1
arxiv-1603-03089 | Blind Source Separation: Fundamentals and Recent Advances (A Tutorial Overview Presented at SBrT-2001) | http://arxiv.org/abs/1603.03089 | id:1603.03089 author:Eleftherios Kofidis category:stat.ML cs.IT math.IT  published:2016-03-09 summary:Blind source separation (BSS), i.e., the decoupling of unknown signals that have been mixed in an unknown way, has been a topic of great interest in the signal processing community for the last decade, covering a wide range of applications in such diverse fields as digital communications, pattern recognition, biomedical engineering, and financial data analysis, among others. This course aims at an introduction to the BSS problem via an exposition of well-known and established as well as some more recent approaches to its solution. A unified way is followed in presenting the various results so as to more easily bring out their similarities/differences and emphasize their relative advantages/disadvantages. Only a representative sample of the existing knowledge on BSS will be included in this course. The interested readers are encouraged to consult the list of bibliographical references for more details on this exciting and always active research topic. version:1
arxiv-1601-03466 | Dynamic Privacy For Distributed Machine Learning Over Network | http://arxiv.org/abs/1601.03466 | id:1601.03466 author:Tao Zhang, Quanyan Zhu category:cs.LG  published:2016-01-14 summary:Privacy-preserving distributed machine learning becomes increasingly important due to the recent rapid growth of data. This paper focuses on a class of regularized empirical risk minimization (ERM) machine learning problems, and develops two methods to provide differential privacy to distributed learning algorithms over a network. We first decentralize the learning algorithm using the alternating direction method of multipliers (ADMM), and propose the methods of dual variable perturbation and primal variable perturbation to provide dynamic differential privacy. The two mechanisms lead to algorithms that can provide privacy guarantees under mild conditions of the convexity and differentiability of the loss function and the regularizer. We study the performance of the algorithms, and show that the dual variable perturbation outperforms its primal counterpart. To design an optimal privacy mechanisms, we analyze the fundamental tradeoff between privacy and accuracy, and provide guidelines to choose privacy parameters. Numerical experiments using customer information database are performed to corroborate the results on privacy and utility tradeoffs and design. version:3
arxiv-1511-02300 | Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images | http://arxiv.org/abs/1511.02300 | id:1511.02300 author:Shuran Song, Jianxiong Xiao category:cs.CV  published:2015-11-07 summary:We focus on the task of amodal 3D object detection in RGB-D images, which aims to produce a 3D bounding box of an object in metric form at its full extent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a 3D volumetric scene from a RGB-D image as input and outputs 3D object bounding boxes. In our approach, we propose the first 3D Region Proposal Network (RPN) to learn objectness from geometric shapes and the first joint Object Recognition Network (ORN) to extract geometric features in 3D and color features in 2D. In particular, we handle objects of various sizes by training an amodal RPN at two different scales and an ORN to regress 3D bounding boxes. Experiments show that our algorithm outperforms the state-of-the-art by 13.8 in mAP and is 200x faster than the original Sliding Shapes. All source code and pre-trained models will be available at GitHub. version:2
arxiv-1603-00806 | Hybrid Collaborative Filtering with Neural Networks | http://arxiv.org/abs/1603.00806 | id:1603.00806 author:Florian Strub, Jeremie Mary, Romaric Gaudel category:cs.IR cs.AI cs.NE  published:2016-03-02 summary:Collaborative Filtering aims at exploiting the feedback of users to provide personalised recommendations. Such algorithms look for latent variables in a large sparse matrix of ratings. They can be enhanced by adding side information to tackle the well-known cold start problem. While Neu-ral Networks have tremendous success in image and speech recognition, they have received less attention in Collaborative Filtering. This is all the more surprising that Neural Networks are able to discover latent variables in large and heterogeneous datasets. In this paper, we introduce a Collaborative Filtering Neural network architecture aka CFN which computes a non-linear Matrix Factorization from sparse rating inputs and side information. We show experimentally on the MovieLens and Douban dataset that CFN outper-forms the state of the art and benefits from side information. We provide an implementation of the algorithm as a reusable plugin for Torch, a popular Neural Network framework. version:2
arxiv-1506-01273 | Summarization of Films and Documentaries Based on Subtitles and Scripts | http://arxiv.org/abs/1506.01273 | id:1506.01273 author:Marta Aparício, Paulo Figueiredo, Francisco Raposo, David Martins de Matos, Ricardo Ribeiro, Luís Marujo category:cs.CL cs.AI cs.IR I.2.7  published:2015-06-03 summary:We assess the performance of generic text summarization algorithms applied to films and documentaries, using the well-known behavior of summarization of news articles as reference. We use three datasets: (i) news articles, (ii) film scripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics are used for comparing generated summaries against news abstracts, plot summaries, and synopses. We show that the best performing algorithms are LSA, for news articles and documentaries, and LexRank and Support Sets, for films. Despite the different nature of films and documentaries, their relative behavior is in accordance with that obtained for news articles. version:3
arxiv-1503-06666 | Using Generic Summarization to Improve Music Information Retrieval Tasks | http://arxiv.org/abs/1503.06666 | id:1503.06666 author:Francisco Raposo, Ricardo Ribeiro, David Martins de Matos category:cs.IR cs.LG cs.SD H.5.5  published:2015-03-23 summary:In order to satisfy processing time constraints, many MIR tasks process only a segment of the whole music signal. This practice may lead to decreasing performance, since the most important information for the tasks may not be in those processed segments. In this paper, we leverage generic summarization algorithms, previously applied to text and speech summarization, to summarize items in music datasets. These algorithms build summaries, that are both concise and diverse, by selecting appropriate segments from the input signal which makes them good candidates to summarize music as well. We evaluate the summarization process on binary and multiclass music genre classification tasks, by comparing the performance obtained using summarized datasets against the performances obtained using continuous segments (which is the traditional method used for addressing the previously mentioned time constraints) and full songs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA, MMR, and a Support Sets-based Centrality model improve classification performance when compared to selected 30-second baselines. We also show that summarized datasets lead to a classification performance whose difference is not statistically significant from using full songs. Furthermore, we make an argument stating the advantages of sharing summarized datasets for future MIR research. version:3
arxiv-1506-06646 | Nonparametric Bayesian Double Articulation Analyzer for Direct Language Acquisition from Continuous Speech Signals | http://arxiv.org/abs/1506.06646 | id:1506.06646 author:Tadahiro Taniguchi, Ryo Nakashima, Shogo Nagasaka category:cs.AI cs.CL cs.LG stat.ML  published:2015-06-22 summary:Human infants can discover words directly from unsegmented speech signals without any explicitly labeled data. In this paper, we develop a novel machine learning method called nonparametric Bayesian double articulation analyzer (NPB-DAA) that can directly acquire language and acoustic models from observed continuous speech signals. For this purpose, we propose an integrative generative model that combines a language model and an acoustic model into a single generative model called the "hierarchical Dirichlet process hidden language model" (HDP-HLM). The HDP-HLM is obtained by extending the hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by Johnson et al. An inference procedure for the HDP-HLM is derived using the blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure enables the simultaneous and direct inference of language and acoustic models from continuous speech signals. Based on the HDP-HLM and its inference procedure, we developed a novel double articulation analyzer. By assuming HDP-HLM as a generative model of observed time series data, and by inferring latent variables of the model, the method can analyze latent double articulation structure, i.e., hierarchically organized latent words and phonemes, of the data in an unsupervised manner. The novel unsupervised double articulation analyzer is called NPB-DAA. The NPB-DAA can automatically estimate double articulation structure embedded in speech signals. We also carried out two evaluation experiments using synthetic data and actual human continuous speech signals representing Japanese vowel sequences. In the word acquisition and phoneme categorization tasks, the NPB-DAA outperformed a conventional double articulation analyzer (DAA) and baseline automatic speech recognition system whose acoustic model was trained in a supervised manner. version:2
arxiv-1504-07947 | Patch-based Convolutional Neural Network for Whole Slide Tissue Image Classification | http://arxiv.org/abs/1504.07947 | id:1504.07947 author:Le Hou, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, James E. Davis, Joel H. Saltz category:cs.CV J.3; I.4; I.5  published:2015-04-29 summary:Convolutional Neural Networks (CNN) are state-of-the-art models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN. version:5
arxiv-1507-04201 | Minimum Density Hyperplanes | http://arxiv.org/abs/1507.04201 | id:1507.04201 author:Nicos Pavlidis, David Hofmeyr, Sotiris Tasoulis category:stat.ML cs.LG 62H30  68T10 I.5.0; I.5.3; G.3  published:2015-07-15 summary:Associating distinct groups of objects (clusters) with contiguous regions of high probability density (high-density clusters), is central to many statistical and machine learning approaches to the classification of unlabelled data. We propose a novel hyperplane classifier for clustering and semi-supervised classification which is motivated by this objective. The proposed minimum density hyperplane minimises the integral of the empirical probability density function along it, thereby avoiding intersection with high density clusters. We show that the minimum density and the maximum margin hyperplanes are asymptotically equivalent, thus linking this approach to maximum margin clustering and semi-supervised support vector classifiers. We propose a projection pursuit formulation of the associated optimisation problem which allows us to find minimum density hyperplanes efficiently in practice, and evaluate its performance on a range of benchmark datasets. The proposed approach is found to be very competitive with state of the art methods for clustering and semi-supervised classification. version:2
arxiv-1603-02845 | Unsupervised word segmentation and lexicon discovery using acoustic word embeddings | http://arxiv.org/abs/1603.02845 | id:1603.02845 author:Herman Kamper, Aren Jansen, Sharon Goldwater category:cs.CL  published:2016-03-09 summary:In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modelling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation. We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size. version:1
arxiv-1603-02844 | Fast Training of Triplet-based Deep Binary Embedding Networks | http://arxiv.org/abs/1603.02844 | id:1603.02844 author:Bohan Zhuang, Guosheng Lin, Chunhua Shen, Ian Reid category:cs.CV  published:2016-03-09 summary:In this paper, we aim to learn a mapping (or embedding) from images to a compact binary space in which Hamming distances correspond to a ranking measure for the image retrieval task. We make use of a triplet loss because this has been shown to be most effective for ranking problems. However, training in previous works can be prohibitively expensive due to the fact that optimization is directly performed on the triplet space, where the number of possible triplets for training is cubic in the number of training examples. To address this issue, we propose to formulate high-order binary codes learning as a multi-label classification problem by explicitly separating learning into two interleaved stages. To solve the first stage, we design a large-scale high-order binary codes inference algorithm to reduce the high-order objective to a standard binary quadratic problem such that graph cuts can be used to efficiently infer the binary code which serve as the label of each training datum. In the second stage we propose to map the original image to compact binary codes via carefully designed deep convolutional neural networks (CNNs) and the hashing function fitting can be solved by training binary CNN classifiers. An incremental/interleaved optimization strategy is proffered to ensure that these two steps are interactive with each other during training for better accuracy. We conduct experiments on several benchmark datasets, which demonstrate both improved training time (by as much as two orders of magnitude) as well as producing state-of-the-art hashing for various retrieval tasks. version:1
arxiv-1603-02443 | Note on the equivalence of hierarchical variational models and auxiliary deep generative models | http://arxiv.org/abs/1603.02443 | id:1603.02443 author:Niko Brümmer category:stat.ML  published:2016-03-08 summary:This note compares two recently published machine learning methods for constructing flexible, but tractable families of variational hidden-variable posteriors. The first method, called "hierarchical variational models" enriches the inference model with an extra variable, while the other, called "auxiliary deep generative models", enriches the generative model instead. We conclude that the two methods are mathematically equivalent. version:2
arxiv-1603-02839 | Starting Small -- Learning with Adaptive Sample Sizes | http://arxiv.org/abs/1603.02839 | id:1603.02839 author:Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann category:cs.LG  published:2016-03-09 summary:For many machine learning problems, data is abundant and it may be prohibitive to make multiple passes through the full training set. In this context, we investigate strategies for dynamically increasing the effective sample size, when using iterative methods such as stochastic gradient descent. Our interest is motivated by the rise of variance-reduced methods, which achieve linear convergence rates that scale favorably for smaller sample sizes. Exploiting this feature, we show -- theoretically and empirically -- how to obtain significant speed-ups with a novel algorithm that reaches statistical accuracy on an $n$-sample in $2n$, instead of $n \log n$ steps. version:1
arxiv-1603-02836 | Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training | http://arxiv.org/abs/1603.02836 | id:1603.02836 author:Anirban Santara, Debapriya Maji, DP Tejas, Pabitra Mitra, Arobinda Gupta category:cs.LG  published:2016-03-09 summary:Deep neural networks are capable of modelling highly non-linear functions by capturing different levels of abstraction of data hierarchically. While training deep networks, first the system is initialized near a good optimum by greedy layer-wise unsupervised pre-training. However, with burgeoning data and increasing dimensions of the architecture, the time complexity of this approach becomes enormous. Also, greedy pre-training of the layers often turns detrimental by over-training a layer causing it to lose harmony with the rest of the network. In this paper a synchronized parallel algorithm for pre-training deep networks on multi-core machines has been proposed. Different layers are trained by parallel threads running on different cores with regular synchronization. Thus the pre-training process becomes faster and chances of over-training are reduced. This is experimentally validated using a stacked autoencoder for dimensionality reduction of MNIST handwritten digit database. The proposed algorithm achieved 26\% speed-up compared to greedy layer-wise pre-training for achieving the same reconstruction accuracy substantiating its potential as an alternative. version:1
arxiv-1603-02814 | Image Captioning and Visual Question Answering Based on Attributes and Their Related External Knowledge | http://arxiv.org/abs/1603.02814 | id:1603.02814 author:Qi Wu, Chunhua Shen, Anton van den Hengel, Peng Wang, Anthony Dick category:cs.CV  published:2016-03-09 summary:Much recent progress in Vision-to-Language problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. In this paper we first propose a method of incorporating high-level concepts into the successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering. We further show that the same mechanism can be used to incorporate external knowledge, which is critically important for answering high level visual questions. Specifically, we design a visual question answering model that combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. It particularly allows questions to be asked about the contents of an image, even when the image itself does not contain a complete answer. Our final model achieves the best reported results on both image captioning and visual question answering on several benchmark datasets. version:1
arxiv-1603-02806 | Optimized Kernel Entropy Components | http://arxiv.org/abs/1603.02806 | id:1603.02806 author:Emma Izquierdo-Verdiguier, Valero Laparra, Robert Jenssen, Luis Gómez-Chova, Gustau Camps-Valls category:stat.ML cs.LG  published:2016-03-09 summary:This work addresses two main issues of the standard Kernel Entropy Component Analysis (KECA) algorithm: the optimization of the kernel decomposition and the optimization of the Gaussian kernel parameter. KECA roughly reduces to a sorting of the importance of kernel eigenvectors by entropy instead of by variance as in Kernel Principal Components Analysis. In this work, we propose an extension of the KECA method, named Optimized KECA (OKECA), that directly extracts the optimal features retaining most of the data entropy by means of compacting the information in very few features (often in just one or two). The proposed method produces features which have higher expressive power. In particular, it is based on the Independent Component Analysis (ICA) framework, and introduces an extra rotation to the eigen-decomposition, which is optimized via gradient ascent search. This maximum entropy preservation suggests that OKECA features are more efficient than KECA features for density estimation. In addition, a critical issue in both methods is the selection of the kernel parameter since it critically affects the resulting performance. Here we analyze the most common kernel length-scale selection criteria. Results of both methods are illustrated in different synthetic and real problems. Results show that 1) OKECA returns projections with more expressive power than KECA, 2) the most successful rule for estimating the kernel parameter is based on maximum likelihood, and 3) OKECA is more robust to the selection of the length-scale parameter in kernel density estimation. version:1
arxiv-1602-04601 | Selective Inference Approach for Statistically Sound Predictive Pattern Mining | http://arxiv.org/abs/1602.04601 | id:1602.04601 author:Shinya Suzumura, Kazuya Nakagawa, Mahito Sugiyama, Koji Tsuda, Ichiro Takeuchi category:stat.ML  published:2016-02-15 summary:Discovering statistically significant patterns from databases is an important challenging problem. The main obstacle of this problem is in the difficulty of taking into account the selection bias, i.e., the bias arising from the fact that patterns are selected from extremely large number of candidates in databases. In this paper, we introduce a new approach for predictive pattern mining problems that can address the selection bias issue. Our approach is built on a recently popularized statistical inference framework called selective inference. In selective inference, statistical inferences (such as statistical hypothesis testing) are conducted based on sampling distributions conditional on a selection event. If the selection event is characterized in a tractable way, statistical inferences can be made without minding selection bias issue. However, in pattern mining problems, it is difficult to characterize the entire selection process of mining algorithms. Our main contribution in this paper is to solve this challenging problem for a class of predictive pattern mining problems by introducing a novel algorithmic framework. We demonstrate that our approach is useful for finding statistically significant patterns from databases. version:2
arxiv-1603-02782 | Bipartite Correlation Clustering -- Maximizing Agreements | http://arxiv.org/abs/1603.02782 | id:1603.02782 author:Megasthenis Asteris, Anastasios Kyrillidis, Dimitris Papailiopoulos, Alexandros G. Dimakis category:cs.DS stat.ML  published:2016-03-09 summary:In Bipartite Correlation Clustering (BCC) we are given a complete bipartite graph $G$ with `+' and `-' edges, and we seek a vertex clustering that maximizes the number of agreements: the number of all `+' edges within clusters plus all `-' edges cut across clusters. BCC is known to be NP-hard. We present a novel approximation algorithm for $k$-BCC, a variant of BCC with an upper bound $k$ on the number of clusters. Our algorithm outputs a $k$-clustering that provably achieves a number of agreements within a multiplicative ${(1-\delta)}$-factor from the optimal, for any desired accuracy $\delta$. It relies on solving a combinatorially constrained bilinear maximization on the bi-adjacency matrix of $G$. It runs in time exponential in $k$ and $\delta^{-1}$, but linear in the size of the input. Further, we show that, in the (unconstrained) BCC setting, an ${(1-\delta)}$-approximation can be achieved by $O(\delta^{-1})$ clusters regardless of the size of the graph. In turn, our $k$-BCC algorithm implies an Efficient PTAS for the BCC objective of maximizing agreements. version:1
arxiv-1603-02776 | Implicit Discourse Relation Classification via Multi-Task Neural Networks | http://arxiv.org/abs/1603.02776 | id:1603.02776 author:Yang Liu, Sujian Li, Xiaodong Zhang, Zhifang Sui category:cs.CL cs.AI cs.NE  published:2016-03-09 summary:Without discourse connectives, classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. Previous research usually makes use of one kind of discourse framework such as PDTB or RST to improve the classification performance on discourse relations. Actually, under different discourse annotation frameworks, there exist multiple corpora which have internal connections. To exploit the combination of different discourse corpora, we design related discourse classification tasks specific to a corpus, and propose a novel Convolutional Neural Network embedded multi-task learning system to synthesize these tasks by learning both unique and shared representations for each task. The experimental results on the PDTB implicit discourse relation classification task demonstrate that our model achieves significant gains over baseline systems. version:1
arxiv-1601-06207 | Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least Squares Problem | http://arxiv.org/abs/1601.06207 | id:1601.06207 author:Alican Nalci, Igor Fedorov, Bhaskar D. Rao category:cs.LG stat.ML  published:2016-01-22 summary:In this paper we introduce a hierarchical Bayesian framework to obtain sparse and non-negative solutions to the sparse non-negative least squares problem (S-NNLS). We introduce a new family of scale mixtures, the Rectified Gaussian Scale Mixture (R-GSM), to model the sparsity enforcing prior distribution for the signal of interest. One advantage of the R-GSM prior is that through proper choice of the mixing density it encompasses a wide variety of heavy tailed distributions, such as the rectified Laplacian and rectified Student's t distributions. Similar to the Gaussian Scale Mixture (GSM) approach, a Type II Expectation-Maximization framework is developed to estimate the hyper-parameters and obtain a point estimate of the parameter of interest. In the proposed method, called rectified Sparse Bayesian Learning (R-SBL), we provide two ways to perform the Expectation step; Markov-Chain Monte-Carlo (MCMC) simulations and a simple yet effective diagonal approximation approach (DA). Through numerical experiments we show that R-SBL outperforms existing S-NNLS solvers in terms of both signal and support recovery and that the proposed DA approach admits both computational efficiency and numerical accuracy. version:3
arxiv-1603-02763 | megaman: Manifold Learning with Millions of points | http://arxiv.org/abs/1603.02763 | id:1603.02763 author:James McQueen, Marina Meila, Jacob VanderPlas, Zhongyue Zhang category:cs.LG cs.CG stat.ML  published:2016-03-09 summary:Manifold Learning is a class of algorithms seeking a low-dimensional non-linear representation of high-dimensional data. Thus manifold learning algorithms are, at least in theory, most applicable to high-dimensional data and sample sizes to enable accurate estimation of the manifold. Despite this, most existing manifold learning implementations are not particularly scalable. Here we present a Python package that implements a variety of manifold learning algorithms in a modular and scalable fashion, using fast approximate neighbors searches and fast sparse eigendecompositions. The package incorporates theoretical advances in manifold learning, such as the unbiased Laplacian estimator and the estimation of the embedding distortion by the Riemannian metric method. In benchmarks, even on a single-core desktop computer, our code embeds millions of data points in minutes, and takes just 200 minutes to embed the main sample of galaxy spectra from the Sloan Digital Sky Survey --- consisting of 0.6 million samples in 3750-dimensions --- a task which has not previously been possible. version:1
arxiv-1603-02743 | Computing AIC for black-box models using Generalised Degrees of Freedom: a comparison with cross-validation | http://arxiv.org/abs/1603.02743 | id:1603.02743 author:Severin Hauenstein, Carsten F. Dormann, Simon N Wood category:stat.ML  published:2016-03-09 summary:Generalised Degrees of Freedom (GDF), as defined by Ye (1998 JASA 93:120-131), represent the sensitivity of model fits to perturbations of the data. As such they can be computed for any statistical model, making it possible, in principle, to derive the number of parameters in machine-learning approaches. Defined originally for normally distributed data only, we here investigate the potential of this approach for Bernoulli-data. GDF-values for models of simulated and real data are compared to model complexity-estimates from cross-validation. Similarly, we computed GDF-based AICc for randomForest, neural networks and boosted regression trees and demonstrated its similarity to cross-validation. GDF-estimates for binary data were unstable and inconsistently sensitive to the number of data points perturbed simultaneously, while at the same time being extremely computer-intensive in their calculation. Repeated 10-fold cross-validation was more robust, based on fewer assumptions and faster to compute. Our findings suggest that the GDF-approach does not readily transfer to Bernoulli data and a wider range of regression approaches. version:1
arxiv-1603-00929 | A Kernel Test for Three-Variable Interactions with Random Processes | http://arxiv.org/abs/1603.00929 | id:1603.00929 author:Paul K. Rubenstein, Kacper P. Chwialkowski, Arthur Gretton category:stat.ML 62G10  published:2016-03-02 summary:We apply a wild bootstrap method to the Lancaster three-variable interaction measure in order to detect factorisation of the joint distribution on three variables forming a stationary random process, for which the existing permutation bootstrap method fails. As in the i.i.d. case, the Lancaster test is found to outperform existing tests in cases for which two independent variables individually have a weak influence on a third, but that when considered jointly the influence is strong. The main contributions of this paper are twofold: first, we prove that the Lancaster statistic satisfies the conditions required to estimate the quantiles of the null distribution using the wild bootstrap; second, the manner in which this is proved is novel, simpler than existing methods, and can further be applied to other statistics. version:2
arxiv-1603-02736 | Discriminative models for robust image classification | http://arxiv.org/abs/1603.02736 | id:1603.02736 author:Umamahesh Srinivas category:stat.ML cs.CV  published:2016-03-08 summary:A variety of real-world tasks involve the classification of images into pre-determined categories. Designing image classification algorithms that exhibit robustness to acquisition noise and image distortions, particularly when the available training data are insufficient to learn accurate models, is a significant challenge. This dissertation explores the development of discriminative models for robust image classification that exploit underlying signal structure, via probabilistic graphical models and sparse signal representations. Probabilistic graphical models are widely used in many applications to approximate high-dimensional data in a reduced complexity set-up. Learning graphical structures to approximate probability distributions is an area of active research. Recent work has focused on learning graphs in a discriminative manner with the goal of minimizing classification error. In the first part of the dissertation, we develop a discriminative learning framework that exploits the complementary yet correlated information offered by multiple representations (or projections) of a given signal/image. Specifically, we propose a discriminative tree-based scheme for feature fusion by explicitly learning the conditional correlations among such multiple projections in an iterative manner. Experiments reveal the robustness of the resulting graphical model classifier to training insufficiency. version:1
arxiv-1602-00991 | Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks | http://arxiv.org/abs/1602.00991 | id:1602.00991 author:Peter Ondruska, Ingmar Posner category:cs.LG cs.AI cs.CV cs.NE cs.RO  published:2016-02-02 summary:This paper presents to the best of our knowledge the first end-to-end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or system identification in the form of plant or sensor models. Specifically, our system accepts a stream of raw sensor data at one end and, in real-time, produces an estimate of the entire environment state at the output including even occluded objects. We achieve this by framing the problem as a deep learning task and exploit sequence models in the form of recurrent neural networks to learn a mapping from sensor measurements to object tracks. In particular, we propose a learning method based on a form of input dropout which allows learning in an unsupervised manner, only based on raw, occluded sensor data without access to ground-truth annotations. We demonstrate our approach using a synthetic dataset designed to mimic the task of tracking objects in 2D laser data -- as commonly encountered in robotics applications -- and show that it learns to track many dynamic objects despite occlusions and the presence of sensor noise. version:2
arxiv-1603-00930 | Super Mario as a String: Platformer Level Generation Via LSTMs | http://arxiv.org/abs/1603.00930 | id:1603.00930 author:Adam Summerville, Michael Mateas category:cs.NE cs.LG  published:2016-03-02 summary:The procedural generation of video game levels has existed for at least 30 years, but only recently have machine learning approaches been used to generate levels without specifying the rules for generation. A number of these have looked at platformer levels as a sequence of characters and performed generation using Markov chains. In this paper we examine the use of Long Short-Term Memory recurrent neural networks (LSTMs) for the purpose of generating levels trained from a corpus of Super Mario Brothers levels. We analyze a number of different data representations and how the generated levels fit into the space of human authored Super Mario Brothers levels. version:2
arxiv-1504-08342 | Parsing Linear Context-Free Rewriting Systems with Fast Matrix Multiplication | http://arxiv.org/abs/1504.08342 | id:1504.08342 author:Shay B. Cohen, Daniel Gildea category:cs.CL cs.FL  published:2015-04-30 summary:We describe a matrix multiplication recognition algorithm for a subset of binary linear context-free rewriting systems (LCFRS) with running time $O(n^{\omega d})$ where $M(m) = O(m^{\omega})$ is the running time for $m \times m$ matrix multiplication and $d$ is the "contact rank" of the LCFRS -- the maximal number of combination and non-combination points that appear in the grammar rules. We also show that this algorithm can be used as a subroutine to get a recognition algorithm for general binary LCFRS with running time $O(n^{\omega d + 1})$. The currently best known $\omega$ is smaller than $2.38$. Our result provides another proof for the best known result for parsing mildly context sensitive formalisms such as combinatory categorial grammars, head grammars, linear indexed grammars, and tree adjoining grammars, which can be parsed in time $O(n^{4.76})$. It also shows that inversion transduction grammars can be parsed in time $O(n^{5.76})$. In addition, binary LCFRS subsumes many other formalisms and types of grammars, for some of which we also improve the asymptotic complexity of parsing. version:3
arxiv-1603-02649 | A regularization-based approach for unsupervised image segmentation | http://arxiv.org/abs/1603.02649 | id:1603.02649 author:Aleksandar Dimitriev, Matej Kristan category:cs.CV  published:2016-03-08 summary:We propose a novel unsupervised image segmentation algorithm, which aims to segment an image into several coherent parts. It requires no user input, no supervised learning phase and assumes an unknown number of segments. It achieves this by first over-segmenting the image into several hundred superpixels. These are iteratively joined on the basis of a discriminative classifier trained on color and texture information obtained from each superpixel. The output of the classifier is regularized by a Markov random field that lends more influence to neighbouring superpixels that are more similar. In each iteration, similar superpixels fall under the same label, until only a few coherent regions remain in the image. The algorithm was tested on a standard evaluation data set, where it performs on par with state-of-the-art algorithms in term of precision and greatly outperforms the state of the art by reducing the oversegmentation of the object of interest. version:1
arxiv-1603-02638 | Small ensembles of kriging models for optimization | http://arxiv.org/abs/1603.02638 | id:1603.02638 author:Hossein Mohammadi, Rodolphe Le Riche, Eric Touboul category:math.OC cs.LG stat.ML  published:2016-03-08 summary:The Efficient Global Optimization (EGO) algorithm uses a conditional Gaus-sian Process (GP) to approximate an objective function known at a finite number of observation points and sequentially adds new points which maximize the Expected Improvement criterion according to the GP. The important factor that controls the efficiency of EGO is the GP covariance function (or kernel) which should be chosen according to the objective function. Traditionally, a pa-rameterized family of covariance functions is considered whose parameters are learned through statistical procedures such as maximum likelihood or cross-validation. However, it may be questioned whether statistical procedures for learning covariance functions are the most efficient for optimization as they target a global agreement between the GP and the observations which is not the ultimate goal of optimization. Furthermore, statistical learning procedures are computationally expensive. The main alternative to the statistical learning of the GP is self-adaptation, where the algorithm tunes the kernel parameters based on their contribution to objective function improvement. After questioning the possibility of self-adaptation for kriging based optimizers, this paper proposes a novel approach for tuning the length-scale of the GP in EGO: At each iteration, a small ensemble of kriging models structured by their length-scales is created. All of the models contribute to an iterate in an EGO-like fashion. Then, the set of models is densified around the model whose length-scale yielded the best iterate and further points are produced. Numerical experiments are provided which motivate the use of many length-scales. The tested implementation does not perform better than the classical EGO algorithm in a sequential context but show the potential of the approach for parallel implementations. version:1
arxiv-1603-02636 | DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range Data | http://arxiv.org/abs/1603.02636 | id:1603.02636 author:Lucas Beyer, Alexander Hermans, Bastian Leibe category:cs.RO cs.CV cs.LG cs.NE  published:2016-03-08 summary:We introduce the DROW detector, a deep learning based detector for 2D range data. Laser scanners are lighting invariant, provide accurate range data, and typically cover a large field of view, making them interesting sensors for robotics applications. So far, research on detection in laser range data has been dominated by handcrafted features and boosted classifiers, potentially losing performance due to suboptimal design choices. We propose a Convolutional Neural Network (CNN) based detector for this task. We show how to effectively apply CNNs for detection in 2D range data, and propose a depth preprocessing step and voting scheme that significantly improve CNN performance. We demonstrate our approach on wheelchairs and walkers, obtaining state of the art detection results. Apart from the training data, none of our design choices limits the detector to these two classes, though. We provide a ROS node for our detector and release our dataset containing 464k laser scans, out of which 24k were annotated for training. version:1
arxiv-1603-02617 | Iterative Hough Forest with Histogram of Control Points for 6 DoF Object Registration from Depth Images | http://arxiv.org/abs/1603.02617 | id:1603.02617 author:Caner Sahin, Rigas Kouskouridas, Tae-Kyun Kim category:cs.CV cs.RO  published:2016-03-08 summary:State-of-the-art techniques for 6D object pose recovery depend on occlusion-free point clouds to accurately register objects in the 3D space. To reduce this dependency, we introduce a novel architecture called Iterative Hough forest with Histogram of Control Points that is capable of estimating occluded and cluttered objects' 6D pose given a candidate 2D bounding box. Our iterative Hough forest is learnt using patches extracted only from the positive samples. These patches are represented with Histogram of Control Points (HoCP), a scale-variant implicit volumetric description, which we derive from recently introduced Implicit B-Splines (IBS). The rich discriminative information provided by this scale-variance is leveraged during inference, where the initial pose estimation of the object is iteratively refined based on more discriminative control points by using our iterative Hough forest. We conduct experiments on several test objects of a publicly available dataset to test our architecture and to compare with the state-of-the-art. version:1
arxiv-1603-02604 | Observing Trends in Automated Multilingual Media Analysis | http://arxiv.org/abs/1603.02604 | id:1603.02604 author:Ralf Steinberger, Aldo Podavini, Alexandra Balahur, Guillaume Jacquet, Hristo Tanev, Jens Linge, Martin Atkinson, Michele Chinosi, Vanni Zavarella, Yaniv Steiner, Erik van der Goot category:cs.CL  published:2016-03-08 summary:Any large organisation, be it public or private, monitors the media for information to keep abreast of developments in their field of interest, and usually also to become aware of positive or negative opinions expressed towards them. At least for the written media, computer programs have become very efficient at helping the human analysts significantly in their monitoring task by gathering media reports, analysing them, detecting trends and - in some cases - even to issue early warnings or to make predictions of likely future developments. We present here trend recognition-related functionality of the Europe Media Monitor (EMM) system, which was developed by the European Commission's Joint Research Centre (JRC) for public administrations in the European Union (EU) and beyond. EMM performs large-scale media analysis in up to seventy languages and recognises various types of trends, some of them combining information from news articles written in different languages and from social media posts. EMM also lets users explore the huge amount of multilingual media data through interactive maps and graphs, allowing them to examine the data from various view points and according to multiple criteria. A lot of EMM's functionality is accessibly freely over the internet or via apps for hand-held devices. version:1
arxiv-1603-02597 | Prediction of Infinite Words with Automata | http://arxiv.org/abs/1603.02597 | id:1603.02597 author:Tim Smith category:cs.FL cs.LG  published:2016-03-08 summary:In the classic problem of sequence prediction, a predictor receives a sequence of values from an emitter and tries to guess the next value before it appears. The predictor masters the emitter if there is a point after which all of the predictor's guesses are correct. In this paper we consider the case in which the predictor is an automaton and the emitted values are drawn from a finite set; i.e., the emitted sequence is an infinite word. We examine the predictive capabilities of finite automata, pushdown automata, stack automata (a generalization of pushdown automata), and multihead finite automata. We relate our predicting automata to purely periodic words, ultimately periodic words, and multilinear words, describing novel prediction algorithms for mastering these sequences. version:1
arxiv-1603-02578 | Batched Lazy Decision Trees | http://arxiv.org/abs/1603.02578 | id:1603.02578 author:Mathieu Guillame-Bert, Artur Dubrawski category:cs.LG  published:2016-03-08 summary:We introduce a batched lazy algorithm for supervised classification using decision trees. It avoids unnecessary visits to irrelevant nodes when it is used to make predictions with either eagerly or lazily trained decision trees. A set of experiments demonstrate that the proposed algorithm can outperform both the conventional and lazy decision tree algorithms in terms of computation time as well as memory consumption, without compromising accuracy. version:1
arxiv-1603-01670 | Network Morphism | http://arxiv.org/abs/1603.01670 | id:1603.01670 author:Tao Wei, Changhu Wang, Yong Rui, Chang Wen Chen category:cs.LG cs.CV cs.NE  published:2016-03-05 summary:We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as \emph{network morphism} in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme. version:2
arxiv-1603-02532 | On the inconsistency of $\ell_1$-penalised sparse precision matrix estimation | http://arxiv.org/abs/1603.02532 | id:1603.02532 author:Otte Heinävaara, Janne Leppä-aho, Jukka Corander, Antti Honkela category:cs.LG stat.CO stat.ML  published:2016-03-08 summary:Various $\ell_1$-penalised estimation methods such as graphical lasso and CLIME are widely used for sparse precision matrix estimation. Many of these methods have been shown to be consistent under various quantitative assumptions about the underlying true covariance matrix. Intuitively, these conditions are related to situations where the penalty term will dominate the optimisation. In this paper, we explore the consistency of $\ell_1$-based methods for a class of sparse latent variable -like models, which are strongly motivated by several types of applications. We show that all $\ell_1$-based methods fail dramatically for models with nearly linear dependencies between the variables. We also study the consistency on models derived from real gene expression data and note that the assumptions needed for consistency never hold even for modest sized gene networks and $\ell_1$-based methods also become unreliable in practice for larger networks. version:1
arxiv-1601-05585 | Generalized optimal sub-pattern assignment metric | http://arxiv.org/abs/1601.05585 | id:1601.05585 author:Abu Sajana Rahmathullah, Ángel F. García-Fernández, Lennart Svensson category:cs.SY cs.CV  published:2016-01-21 summary:In this paper, we present the generalized optimal sub-pattern assignment (GOSPA) metric on the space of sets of targets. This metric is a generalized version of the unnormalized optimal sub-pattern assignment (OSPA) metric. The difference between unnormalized OSPA and GOSPA is that, in the proposed metric, we can choose a range of values for the cardinality mismatch penalty for a given cut-off distance c. We argue that in multiple target tracking, we should select the cardinality mismatch of GOSPA in a specific way, which is different from OSPA. In this case, the metric can be viewed as sum of target localization error and error due to missed and false targets. We also extend the GOSPA metric to the space of random finite sets, and show that both mean GOSPA and root mean squared GOSPA are metrics, which are useful for performance evaluation. version:2
arxiv-1603-02494 | A Bayesian non-parametric method for clustering high-dimensional binary data | http://arxiv.org/abs/1603.02494 | id:1603.02494 author:Tapesh Santra category:stat.AP cs.LG stat.ML  published:2016-03-08 summary:In many real life problems, objects are described by large number of binary features. For instance, documents are characterized by presence or absence of certain keywords; cancer patients are characterized by presence or absence of certain mutations etc. In such cases, grouping together similar objects/profiles based on such high dimensional binary features is desirable, but challenging. Here, I present a Bayesian non parametric algorithm for clustering high dimensional binary data. It uses a Dirichlet Process (DP) mixture model and simulated annealing to not only cluster binary data, but also find optimal number of clusters in the data. The performance of the algorithm was evaluated and compared with other algorithms using simulated datasets. It outperformed all other clustering methods that were tested in the simulation studies. It was also used to cluster real datasets arising from document analysis, handwritten image analysis and cancer research. It successfully divided a set of documents based on their topics, hand written images based on different styles of writing digits and identified tissue and mutation specificity of chemotherapy treatments. version:1
arxiv-1603-02488 | Extracting Arabic Relations from the Web | http://arxiv.org/abs/1603.02488 | id:1603.02488 author:Shimaa M. Abd El-salam, Enas M. F. El Houby, A. K. Al Sammak, T. A. El-Shishtawy category:cs.CL  published:2016-03-08 summary:The goal of this research is to extract a large list or table from named entities and relations in a specific domain. A small set of a handful of instance relations is required as input from the user. The system exploits summaries from Google search engine as a source text. These instances are used to extract patterns. The output is a set of new entities and their relations. The results from four experiments show that precision and recall varies according to relation type. Precision ranges from 0.61 to 0.75 while recall ranges from 0.71 to 0.83. The best result is obtained for (player, club) relationship, 0.72 and 0.83 for precision and recall respectively. version:1
arxiv-1509-01007 | Encoding Prior Knowledge with Eigenword Embeddings | http://arxiv.org/abs/1509.01007 | id:1509.01007 author:Dominique Osborne, Shashi Narayan, Shay B. Cohen category:cs.CL  published:2015-09-03 summary:Canonical correlation analysis (CCA) is a method for reducing the dimension of data represented using two views. It has been previously used to derive word embeddings, where one view indicates a word, and the other view indicates its context. We describe a way to incorporate prior knowledge into CCA, give a theoretical justification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets. version:2
arxiv-1603-02466 | A non-extensive entropy feature and its application to texture classification | http://arxiv.org/abs/1603.02466 | id:1603.02466 author:Seba Susan, Madasu Hanmandlu category:cs.CV  published:2016-03-08 summary:This paper proposes a new probabilistic non-extensive entropy feature for texture characterization, based on a Gaussian information measure. The highlights of the new entropy are that it is bounded by finite limits and that it is non additive in nature. The non additive property of the proposed entropy makes it useful for the representation of information content in the non-extensive systems containing some degree of regularity or correlation. The effectiveness of the proposed entropy in representing the correlated random variables is demonstrated by applying it for the texture classification problem since textures found in nature are random and at the same time contain some degree of correlation or regularity at some scale. The gray level co-occurrence probabilities (GLCP) are used for computing the entropy function. The experimental results indicate high degree of the classification accuracy. The performance of the new entropy function is found superior to other forms of entropy such as Shannon, Renyi, Tsallis and Pal and Pal entropies on comparison. Using the feature based polar interaction maps (FBIM) the proposed entropy is shown to be the best measure among the entropies compared for representing the correlated textures. version:1
arxiv-1506-05967 | Doubly Decomposing Nonparametric Tensor Regression | http://arxiv.org/abs/1506.05967 | id:1506.05967 author:Masaaki Imaizumi, Kohei Hayashi category:stat.ML  published:2015-06-19 summary:Nonparametric extension of tensor regression is proposed. Nonlinearity in a high-dimensional tensor space is broken into simple local functions by incorporating low-rank tensor decomposition. Compared to naive nonparametric approaches, our formulation considerably improves the convergence rate of estimation while maintaining consistency with the same function class under specific conditions. To estimate local functions, we develop a Bayesian estimator with the Gaussian process prior. Experimental results show its theoretical properties and high performance in terms of predicting a summary statistic of a real complex network. version:3
arxiv-1603-02447 | A hybrid approach based segmentation technique for brain tumor in MRI Images | http://arxiv.org/abs/1603.02447 | id:1603.02447 author:D. Anithadevi, K. Perumal category:cs.CV  published:2016-03-08 summary:Automatic image segmentation becomes very crucial for tumor detection in medical image processing.In general, manual and semi automatic segmentation techniques require more time and knowledge. However these drawbacks had overcome by automatic segmentation still there needs to develop more appropriate techniques for medical image segmentation. Therefore, we proposed hybrid approach based image segmentation using the combined features of region growing and threshold based segmentation techniques. It is followed by pre-processing stage to provide an accurate brain tumor extraction by the help of Magnetic Resonance Imaging (MRI). If the tumor has holes, the region growing segmentation algorithm cannot reveal but the proposed hybrid segmentation technique can be achieved and the result as well improved. Hence the result used to made assessment with the various performance measures as DICE, JACCARD similarity, accuracy, sensitivity and specificity. These similarity measures have been extensively used for evaluation with the ground truth of each processed image and its results are compared and analyzed. version:1
arxiv-1603-02434 | Effective Mean-Field Inference Method for Nonnegative Boltzmann Machines | http://arxiv.org/abs/1603.02434 | id:1603.02434 author:Muneki Yasuda category:stat.ML cond-mat.dis-nn cond-mat.stat-mech  published:2016-03-08 summary:Nonnegative Boltzmann machines (NNBMs) are recurrent probabilistic neural network models that can describe multi-modal nonnegative data. NNBMs form rectified Gaussian distributions that appear in biological neural network models, positive matrix factorization, nonnegative matrix factorization, and so on. In this paper, an effective inference method for NNBMs is proposed that uses the mean-field method, referred to as the Thouless--Anderson--Palmer equation, and the diagonal consistency method, which was recently proposed. version:1
arxiv-1602-08191 | DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and Caffe Compatibility | http://arxiv.org/abs/1602.08191 | id:1602.08191 author:Hanjoo Kim, Jaehong Park, Jaehee Jang, Sungroh Yoon category:cs.LG  published:2016-02-26 summary:The increasing complexity of deep neural networks (DNNs) has made it challenging to exploit existing large-scale data processing pipelines for handling massive data and parameters involved in DNN training. Distributed computing platforms and GPGPU-based acceleration provide a mainstream solution to this computational challenge. In this paper, we propose DeepSpark, a distributed and parallel deep learning framework that simultaneously exploits Apache Spark for large-scale distributed data management and Caffe for GPU-based acceleration. DeepSpark directly accepts Caffe input specifications, providing seamless compatibility with existing designs and network structures. To support parallel operations, DeepSpark automatically distributes workloads and parameters to Caffe-running nodes using Spark and iteratively aggregates training results by a novel lock-free asynchronous variant of the popular elastic averaging stochastic gradient descent (SGD) update scheme, effectively complementing the synchronized processing capabilities of Spark. DeepSpark is an on-going project, and the current release is available at http://deepspark.snu.ac.kr. version:2
arxiv-1603-02412 | Stochastic dual averaging methods using variance reduction techniques for regularized empirical risk minimization problems | http://arxiv.org/abs/1603.02412 | id:1603.02412 author:Tomoya Murata, Taiji Suzuki category:math.OC cs.LG stat.ML  published:2016-03-08 summary:We consider a composite convex minimization problem associated with regularized empirical risk minimization, which often arises in machine learning. We propose two new stochastic gradient methods that are based on stochastic dual averaging method with variance reduction. Our methods generate a sparser solution than the existing methods because we do not need to take the average of the history of the solutions. This is favorable in terms of both interpretability and generalization. Moreover, our methods have theoretical support for both a strongly and a non-strongly convex regularizer and achieve the best known convergence rates among existing nonaccelerated stochastic gradient methods. version:1
arxiv-1603-02977 | Frequency estimation in three-phase power systems with harmonic contamination: A multistage quaternion Kalman filtering approach | http://arxiv.org/abs/1603.02977 | id:1603.02977 author:Sayed Pouria Talebi, Danilo P. Mandic category:stat.ML stat.AP  published:2016-03-08 summary:Motivated by the need for accurate frequency information, a novel algorithm for estimating the fundamental frequency and its rate of change in three-phase power systems is developed. This is achieved through two stages of Kalman filtering. In the first stage a quaternion extended Kalman filter, which provides a unified framework for joint modeling of voltage measurements from all the phases, is used to estimate the instantaneous phase increment of the three-phase voltages. The phase increment estimates are then used as observations of the extended Kalman filter in the second stage that accounts for the dynamic behavior of the system frequency and simultaneously estimates the fundamental frequency and its rate of change. The framework is then extended to account for the presence of harmonics. Finally, the concept is validated through simulation on both synthetic and real-world data. version:1
arxiv-1503-03467 | Multigrid with rough coefficients and Multiresolution operator decomposition from Hierarchical Information Games | http://arxiv.org/abs/1503.03467 | id:1503.03467 author:Houman Owhadi category:math.NA cs.AI math.ST stat.ML stat.TH  published:2015-03-11 summary:We introduce a near-linear complexity (geometric and meshless/algebraic) multigrid/multiresolution method for PDEs with rough ($L^\infty$) coefficients with rigorous a-priori accuracy and performance estimates. The method is discovered through a decision/game theory formulation of the problems of (1) identifying restriction and interpolation operators (2) recovering a signal from incomplete measurements based on norm constraints on its image under a linear operator (3) gambling on the value of the solution of the PDE based on a hierarchy of nested measurements of its solution or source term. The resulting elementary gambles form a hierarchy of (deterministic) basis functions of $H^1_0(\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands with respect to the scalar product induced by the energy norm of the PDE (2) enable sparse compression of the solution space in $H^1_0(\Omega)$ (3) induce an orthogonal multiresolution operator decomposition. The operating diagram of the multigrid method is that of an inverted pyramid in which gamblets are computed locally (by virtue of their exponential decay), hierarchically (from fine to coarse scales) and the PDE is decomposed into a hierarchy of independent linear systems with uniformly bounded condition numbers. The resulting algorithm is parallelizable both in space (via localization) and in bandwith/subscale (subscales can be computed independently from each other). Although the method is deterministic it has a natural Bayesian interpretation under the measure of probability emerging (as a mixed strategy) from the information game formulation and multiresolution approximations form a martingale with respect to the filtration induced by the hierarchy of nested measurements. version:4
arxiv-1312-6947 | Formal Ontology Learning on Factual IS-A Corpus in English using Description Logics | http://arxiv.org/abs/1312.6947 | id:1312.6947 author:Sourish Dasgupta, Ankur Padia, Kushal Shah, Prasenjit Majumder category:cs.CL cs.AI  published:2013-12-25 summary:Ontology Learning (OL) is the computational task of generating a knowledge base in the form of an ontology given an unstructured corpus whose content is in natural language (NL). Several works can be found in this area most of which are limited to statistical and lexico-syntactic pattern matching based techniques Light-Weight OL. These techniques do not lead to very accurate learning mostly because of several linguistic nuances in NL. Formal OL is an alternative (less explored) methodology were deep linguistics analysis is made using theory and tools found in computational linguistics to generate formal axioms and definitions instead simply inducing a taxonomy. In this paper we propose "Description Logic (DL)" based formal OL framework for learning factual IS-A type sentences in English. We claim that semantic construction of IS-A sentences is non trivial. Hence, we also claim that such sentences requires special studies in the context of OL before any truly formal OL can be proposed. We introduce a learner tool, called DLOL_IS-A, that generated such ontologies in the owl format. We have adopted "Gold Standard" based OL evaluation on IS-A rich WCL v.1.1 dataset and our own Community representative IS-A dataset. We observed significant improvement of DLOL_IS-A when compared to the light-weight OL tool Text2Onto and formal OL tool FRED. version:2
arxiv-1509-02314 | A Scalable and Extensible Framework for Superposition-Structured Models | http://arxiv.org/abs/1509.02314 | id:1509.02314 author:Shenjian Zhao, Cong Xie, Zhihua Zhang category:cs.NA math.OC stat.ML  published:2015-09-08 summary:In many learning tasks, structural models usually lead to better interpretability and higher generalization performance. In recent years, however, the simple structural models such as lasso are frequently proved to be insufficient. Accordingly, there has been a lot of work on "superposition-structured" models where multiple structural constraints are imposed. To efficiently solve these "superposition-structured" statistical models, we develop a framework based on a proximal Newton-type method. Employing the smoothed conic dual approach with the LBFGS updating formula, we propose a scalable and extensible proximal quasi-Newton (SEP-QN) framework. Empirical analysis on various datasets shows that our framework is potentially powerful, and achieves super-linear convergence rate for optimizing some popular "superposition-structured" statistical models such as the fused sparse group lasso. version:2
arxiv-1504-04914 | Negatively Correlated Search | http://arxiv.org/abs/1504.04914 | id:1504.04914 author:Ke Tang, Peng Yang, Xin Yao category:cs.NE cs.AI  published:2015-04-20 summary:Evolutionary Algorithms (EAs) have been shown to be powerful tools for complex optimization problems, which are ubiquitous in both communication and big data analytics. This paper presents a new EA, namely Negatively Correlated Search (NCS), which maintains multiple individual search processes in parallel and models the search behaviors of individual search processes as probability distributions. NCS explicitly promotes negatively correlated search behaviors by encouraging differences among the probability distributions (search behaviors). By this means, individual search processes share information and cooperate with each other to search diverse regions of a search space, which makes NCS a promising method for non-convex optimization. The cooperation scheme of NCS could also be regarded as a novel diversity preservation scheme that, different from other existing schemes, directly promotes diversity at the level of search behaviors rather than merely trying to maintain diversity among candidate solutions. Empirical studies showed that NCS is competitive to well-established search methods in the sense that NCS achieved the best overall performance on 20 multimodal (non-convex) continuous optimization problems. The advantages of NCS over state-of-the-art approaches are also demonstrated with a case study on the synthesis of unequally spaced linear antenna arrays. version:2
arxiv-1511-02476 | Statistical physics of inference: Thresholds and algorithms | http://arxiv.org/abs/1511.02476 | id:1511.02476 author:Lenka Zdeborová, Florent Krzakala category:cond-mat.stat-mech cs.DS stat.ML  published:2015-11-08 summary:Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic. version:3
arxiv-1603-02345 | Hand Segmentation for Hand-Object Interaction from Depth map | http://arxiv.org/abs/1603.02345 | id:1603.02345 author:Byeongkeun Kang, Kar-Han Tan, Hung-Shuo Tai, Daniel Tretter, Truong Q. Nguyen category:cs.CV  published:2016-03-08 summary:Hand-object interaction is important for many applications such as augmented reality, medical application, and human-robot interaction. Hand segmentation is a necessary pre-process to estimate hand pose and to recognize hand gesture or object in interaction. However, current hand segmentation method for hand-object interaction is based on color information which is not robust to objects with skin color, skin pigment difference, and light condition variations. Therefore, we propose the first hand segmentation method for hand-object interaction using depth map. This is challenging because of only small depth difference between hand and object during interaction. The proposed method includes two-stage randomized decision forest (RDF) with validation process, bilateral filtering, decision adjustment, and post-processing. We demonstrate the effectiveness of the proposed method by testing for five objects. The proposed method achieves the average F1 score of 0.8826 using different model for each object and 0.8645 using a global model for entire objects. Also, the method takes only about 10ms to process each frame. We believe that this is the state-of-the-art hand segmentation algorithm using depth map for hand-object interaction. version:1
arxiv-1601-06892 | ReconNet: Non-Iterative Reconstruction of Images from Compressively Sensed Random Measurements | http://arxiv.org/abs/1601.06892 | id:1601.06892 author:Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Kerviche, Amit Ashok category:cs.CV  published:2016-01-26 summary:The goal of this paper is to present a non-iterative and more importantly an extremely fast algorithm to reconstruct images from compressively sensed (CS) random measurements. To this end, we propose a novel convolutional neural network (CNN) architecture which takes in CS measurements of an image as input and outputs an intermediate reconstruction. We call this network, ReconNet. The intermediate reconstruction is fed into an off-the-shelf denoiser to obtain the final reconstructed image. On a standard dataset of images we show significant improvements in reconstruction results (both in terms of PSNR and time complexity) over state-of-the-art iterative CS reconstruction algorithms at various measurement rates. Further, through qualitative experiments on real data collected using our block single pixel camera (SPC), we show that our network is highly robust to sensor noise and can recover visually better quality images than competitive algorithms at extremely low sensing rates of 0.1 and 0.04. To demonstrate that our algorithm can recover semantically informative images even at a low measurement rate of 0.01, we present a very robust proof of concept real-time visual tracking application. version:2
arxiv-1601-00863 | Coordinate Friendly Structures, Algorithms and Applications | http://arxiv.org/abs/1601.00863 | id:1601.00863 author:Zhimin Peng, Tianyu Wu, Yangyang Xu, Ming Yan, Wotao Yin category:math.OC cs.CE cs.DC math.NA stat.ML  published:2016-01-05 summary:This paper focuses on coordinate update methods, which are useful for solving problems involving large or high-dimensional datasets. They decompose a problem into simple subproblems, where each updates one, or a small block of, variables while fixing others. These methods can deal with linear and nonlinear mappings, smooth and nonsmooth functions, as well as convex and nonconvex problems. In addition, they are easy to parallelize. The great performance of coordinate update methods depends on solving simple subproblems. To derive simple subproblems for several new classes of applications, this paper systematically studies coordinate friendly operators that perform low-cost coordinate updates. Based on the discovered coordinate friendly operators, as well as operator splitting techniques, we obtain new coordinate update algorithms for a variety of problems in machine learning, image processing, as well as sub-areas of optimization. Several problems are treated with coordinate update for the first time in history. The obtained algorithms are scalable to large instances through parallel and even asynchronous computing. We present numerical examples to illustrate how effective these algorithms are. version:2
arxiv-1505-03924 | $k$-center Clustering under Perturbation Resilience | http://arxiv.org/abs/1505.03924 | id:1505.03924 author:Maria-Florina Balcan, Nika Haghtalab, Colin White category:cs.DS cs.LG  published:2015-05-14 summary:The $k$-center problem is a canonical and long-studied facility location and clustering problem with many applications in both its symmetric and asymmetric forms. Both versions of the problem have tight approximation factors on worst case instances: a $2$-approximation for symmetric $k$-center and an $O(\log^*(k))$-approximation for the asymmetric version. In this work, we go beyond the worst case and provide strong positive results both for the asymmetric and symmetric $k$-center problems under a very natural input stability (promise) condition called $\alpha$-perturbation resilience (Bilu & Linial 2012) , which states that the optimal solution does not change under any $\alpha$-factor perturbation to the input distances. We show that by assuming 2-perturbation resilience, the exact solution for the asymmetric $k$-center problem can be found in polynomial time. To our knowledge, this is the first problem that is hard to approximate to any constant factor in the worst case, yet can be optimally solved in polynomial time under perturbation resilience for a constant value of $\alpha$. Furthermore, we prove our result is tight by showing symmetric $k$-center under $(2-\epsilon)$-perturbation resilience is hard unless $NP=RP$. This is the first tight result for any problem under perturbation resilience, i.e., this is the first time the exact value of $\alpha$ for which the problem switches from being NP-hard to efficiently computable has been found. Our results illustrate a surprising relationship between symmetric and asymmetric $k$-center instances under perturbation resilience. Unlike approximation ratio, for which symmetric $k$-center is easily solved to a factor of $2$ but asymmetric $k$-center cannot be approximated to any constant factor, both symmetric and asymmetric $k$-center can be solved optimally under resilience to 2-perturbations. version:3
arxiv-1603-02253 | Blur Robust Optical Flow using Motion Channel | http://arxiv.org/abs/1603.02253 | id:1603.02253 author:Wenbin Li, Yang Chen, JeeHang Lee, Gang Ren, Darren Cosker category:cs.CV  published:2016-03-07 summary:It is hard to estimate optical flow given a realworld video sequence with camera shake and other motion blur. In this paper, we first investigate the blur parameterization for video footage using near linear motion elements. we then combine a commercial 3D pose sensor with an RGB camera, in order to film video footage of interest together with the camera motion. We illustrates that this additional camera motion/trajectory channel can be embedded into a hybrid framework by interleaving an iterative blind deconvolution and warping based optical flow scheme. Our method yields improved accuracy within three other state-of-the-art baselines given our proposed ground truth blurry sequences; and several other realworld sequences filmed by our imaging system. version:1
arxiv-1603-02252 | Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences | http://arxiv.org/abs/1603.02252 | id:1603.02252 author:Wenbin Li, Darren Cosker, Matthew Brown category:cs.CV  published:2016-03-07 summary:It is hard to densely track a nonrigid object in long term, which is a fundamental research issue in the computer vision community. This task often relies on estimating pairwise correspondences between images over time where the error is accumulated and leads to a drift issue. In this paper, we introduce a novel optimization framework with an Anchor Patch constraint. It is supposed to significantly reduce overall errors given long sequences containing non-rigidly deformable objects. Our framework can be applied to any dense tracking algorithm, e.g. optical flow. We demonstrate the success of our approach by showing significant error reduction on 6 popular optical flow algorithms applied to a range of real-world nonrigid benchmarks. We also provide quantitative analysis of our approach given synthetic occlusions and image noise. version:1
arxiv-1603-02250 | Online Sparse Linear Regression | http://arxiv.org/abs/1603.02250 | id:1603.02250 author:Dean Foster, Satyen Kale, Howard Karloff category:cs.LG  published:2016-03-07 summary:We consider the online sparse linear regression problem, which is the problem of sequentially making predictions observing only a limited number of features in each round, to minimize regret with respect to the best sparse linear regressor, where prediction accuracy is measured by square loss. We give an inefficient algorithm that obtains regret bounded by $\tilde{O}(\sqrt{T})$ after $T$ prediction rounds. We complement this result by showing that no algorithm running in polynomial time per iteration can achieve regret bounded by $O(T^{1-\delta})$ for any constant $\delta > 0$ unless $\text{NP} \subseteq \text{BPP}$. This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension. version:1
arxiv-1508-03606 | Hierarchical Models as Marginals of Hierarchical Models | http://arxiv.org/abs/1508.03606 | id:1508.03606 author:Guido Montufar, Johannes Rauh category:math.PR cs.LG cs.NE math.ST stat.TH  published:2015-08-14 summary:We investigate the representation of hierarchical models in terms of marginals of other hierarchical models with smaller interactions. We focus on binary variables and marginals of pairwise interaction models whose hidden variables are conditionally independent given the visible variables. In this case the problem is equivalent to the representation of linear subspaces of polynomials by feedforward neural networks with soft-plus computational units. We show that every hidden variable can freely model multiple interactions among the visible variables, which allows us to generalize and improve previous results. In particular, we show that a restricted Boltzmann machine with less than $[ 2(\log(v)+1) / (v+1) ] 2^v-1$ hidden binary variables can approximate every distribution of $v$ visible binary variables arbitrarily well, compared to $2^{v-1}-1$ from the best previously known result. version:2
arxiv-1603-02211 | Authenticating users through their arm movement patterns | http://arxiv.org/abs/1603.02211 | id:1603.02211 author:Rajesh Kumar, Vir V Phoha, Rahul Raina category:cs.CV cs.CR K.6.5  published:2016-03-07 summary:In this paper, we propose four continuous authentication designs by using the characteristics of arm movements while individuals walk. The first design uses acceleration of arms captured by a smartwatch's accelerometer sensor, the second design uses the rotation of arms captured by a smartwatch's gyroscope sensor, third uses the fusion of both acceleration and rotation at the feature-level and fourth uses the fusion at score-level. Each of these designs is implemented by using four classifiers, namely, k nearest neighbors (k-NN) with Euclidean distance, Logistic Regression, Multilayer Perceptrons, and Random Forest resulting in a total of sixteen authentication mechanisms. These authentication mechanisms are tested under three different environments, namely an intra-session, inter-session on a dataset of 40 users and an inter-phase on a dataset of 12 users. The sessions of data collection were separated by at least ten minutes, whereas the phases of data collection were separated by at least three months. Under the intra-session environment, all of the twelve authentication mechanisms achieve a mean dynamic false accept rate (DFAR) of 0% and dynamic false reject rate (DFRR) of 0%. For the inter-session environment, feature level fusion-based design with classifier k-NN achieves the best error rates that are a mean DFAR of 2.2% and DFRR of 4.2%. The DFAR and DFRR increased from 5.68% and 4.23% to 15.03% and 14.62% respectively when feature level fusion-based design with classifier k-NN was tested under the inter-phase environment on a dataset of 12 users. version:1
arxiv-1603-02200 | Elastic Functional Coding of Riemannian Trajectories | http://arxiv.org/abs/1603.02200 | id:1603.02200 author:Rushil Anirudh, Pavan Turaga, Jingyong Su, Anuj Srivastava category:cs.CV math.DG  published:2016-03-07 summary:Visual observations of dynamic phenomena, such as human actions, are often represented as sequences of smoothly-varying features . In cases where the feature spaces can be structured as Riemannian manifolds, the corresponding representations become trajectories on manifolds. Analysis of these trajectories is challenging due to non-linearity of underlying spaces and high-dimensionality of trajectories. In vision problems, given the nature of physical systems involved, these phenomena are better characterized on a low-dimensional manifold compared to the space of Riemannian trajectories. For instance, if one does not impose physical constraints of the human body, in data involving human action analysis, the resulting representation space will have highly redundant features. Learning an effective, low-dimensional embedding for action representations will have a huge impact in the areas of search and retrieval, visualization, learning, and recognition. The difficulty lies in inherent non-linearity of the domain and temporal variability of actions that can distort any traditional metric between trajectories. To overcome these issues, we use the framework based on transported square-root velocity fields (TSRVF); this framework has several desirable properties, including a rate-invariant metric and vector space representations. We propose to learn an embedding such that each action trajectory is mapped to a single point in a low-dimensional Euclidean space, and the trajectories that differ only in temporal rates map to the same point. We utilize the TSRVF representation, and accompanying statistical summaries of Riemannian trajectories, to extend existing coding methods such as PCA, KSVD and Label Consistent KSVD to Riemannian trajectories or more generally to Riemannian functions. version:1
arxiv-1505-06236 | A Bottom-up Approach for Pancreas Segmentation using Cascaded Superpixels and (Deep) Image Patch Labeling | http://arxiv.org/abs/1505.06236 | id:1505.06236 author:Amal Farag, Le Lu, Holger R. Roth, Jiamin Liu, Evrim Turkbey, Ronald M. Summers category:cs.CV  published:2015-05-22 summary:Robust automated organ segmentation is a prerequisite for computer-aided diagnosis (CAD), quantitative imaging analysis and surgical assistance. For high-variability organs such as the pancreas, previous approaches report undesirably low accuracies. We present a bottom-up approach for pancreas segmentation in abdominal CT scans that is based on a hierarchy of information propagation by classifying image patches at different resolutions; and cascading superpixels. There are four stages: 1) decomposing CT slice images as a set of disjoint boundary-preserving superpixels; 2) computing pancreas class probability maps via dense patch labeling; 3) classifying superpixels by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. The dense image patch labeling are conducted by: efficient random forest classifier on image histogram, location and texture features; and more expensive (but with better specificity) deep convolutional neural network classification on larger image windows (with more spatial contexts). Evaluation of the approach is performed on a database of 80 manually segmented CT volumes in six-fold cross-validation (CV). Our achieved results are comparable, or better than the state-of-the-art methods (evaluated by "leave-one-patient-out"), with Dice 70.7% and Jaccard 57.9%. The computational efficiency has been drastically improved in the order of 6~8 minutes, comparing with others of ~10 hours per case. Finally, we implement a multi-atlas label fusion (MALF) approach for pancreas segmentation using the same datasets. Under six-fold CV, our bottom-up segmentation method significantly outperforms its MALF counterpart: (70.7 +/- 13.0%) versus (52.5 +/- 20.8%) in Dice. Deep CNN patch labeling confidences offer more numerical stability, reflected by smaller standard deviations. version:2
arxiv-1603-02185 | Distributed Multi-Task Learning with Shared Representation | http://arxiv.org/abs/1603.02185 | id:1603.02185 author:Jialei Wang, Mladen Kolar, Nathan Srebro category:cs.LG stat.ML  published:2016-03-07 summary:We study the problem of distributed multi-task learning with shared representation, where each machine aims to learn a separate, but related, task in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix has low rank. We consider a setting where each task is handled by a different machine, with samples for the task available locally on the machine, and study communication-efficient methods for exploiting the shared structure. version:1
arxiv-1506-02178 | Capturing Hands in Action using Discriminative Salient Points and Physics Simulation | http://arxiv.org/abs/1506.02178 | id:1506.02178 author:Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo Aponte, Marc Pollefeys, Juergen Gall category:cs.CV  published:2015-06-06 summary:Hand motion capture is a popular research field, recently gaining more attention due to the ubiquity of RGB-D sensors. However, even most recent approaches focus on the case of a single isolated hand. In this work, we focus on hands that interact with other hands or objects and present a framework that successfully captures motion in such interaction scenarios for both rigid and articulated objects. Our framework combines a generative model with discriminatively trained salient points to achieve a low tracking error and with collision detection and physics simulation to achieve physically plausible estimates even in case of occlusions and missing visual data. Since all components are unified in a single objective function which is almost everywhere differentiable, it can be optimized with standard optimization techniques. Our approach works for monocular RGB-D sequences as well as setups with multiple synchronized RGB cameras. For a qualitative and quantitative evaluation, we captured 29 sequences with a large variety of interactions and up to 150 degrees of freedom. version:4
arxiv-1511-06341 | Communicating Semantics: Reference by Description | http://arxiv.org/abs/1511.06341 | id:1511.06341 author:Ramanathan V Guha, Vineet Gupta category:cs.CL  published:2015-11-19 summary:Messages often refer to entities such as people, places and events. Correct identification of the intended reference is an essential part of communication. Lack of shared unique names often complicates entity reference. Shared knowledge can be used to construct uniquely identifying descriptive references for entities with ambiguous names. We introduce a mathematical model for `Reference by Description', derive results on the conditions under which, with high probability, programs can construct unambiguous references to most entities in the domain of discourse and provide empirical validation of these results. version:4
arxiv-1603-02139 | Learning a Discriminative Null Space for Person Re-identification | http://arxiv.org/abs/1603.02139 | id:1603.02139 author:Li Zhang, Tao Xiang, Shaogang Gong category:cs.CV  published:2016-03-07 summary:Most existing person re-identification (re-id) methods focus on learning the optimal distance metrics across camera views. Typically a person's appearance is represented using features of thousands of dimensions, whilst only hundreds of training samples are available due to the difficulties in collecting matched training images. With the number of training samples much smaller than the feature dimension, the existing methods thus face the classic small sample size (SSS) problem and have to resort to dimensionality reduction techniques and/or matrix regularisation, which lead to loss of discriminative power. In this work, we propose to overcome the SSS problem in re-id distance metric learning by matching people in a discriminative null space of the training data. In this null space, images of the same person are collapsed into a single point thus minimising the within-class scatter to the extreme and maximising the relative between-class separation simultaneously. Importantly, it has a fixed dimension, a closed-form solution and is very efficient to compute. Extensive experiments carried out on five person re-identification benchmarks including VIPeR, PRID2011, CUHK01, CUHK03 and Market1501 show that such a simple approach beats the state-of-the-art alternatives, often by a big margin. version:1
arxiv-1602-06818 | Graph Regularized Low Rank Representation for Aerosol Optical Depth Retrieval | http://arxiv.org/abs/1602.06818 | id:1602.06818 author:Yubao Sun, Renlong Hang, Qingshan Liu, Fuping Zhu, Hucheng Pei category:cs.LG  published:2016-02-22 summary:In this paper, we propose a novel data-driven regression model for aerosol optical depth (AOD) retrieval. First, we adopt a low rank representation (LRR) model to learn a powerful representation of the spectral response. Then, graph regularization is incorporated into the LRR model to capture the local structure information and the nonlinear property of the remote-sensing data. Since it is easy to acquire the rich satellite-retrieval results, we use them as a baseline to construct the graph. Finally, the learned feature representation is feeded into support vector machine (SVM) to retrieve AOD. Experiments are conducted on two widely used data sets acquired by different sensors, and the experimental results show that the proposed method can achieve superior performance compared to the physical models and other state-of-the-art empirical models. version:2
arxiv-1510-06786 | Freshman or Fresher? Quantifying the Geographic Variation of Internet Language | http://arxiv.org/abs/1510.06786 | id:1510.06786 author:Vivek Kulkarni, Bryan Perozzi, Steven Skiena category:cs.CL cs.IR cs.LG  published:2015-10-22 summary:We present a new computational technique to detect and analyze statistically significant geographic variation in language. Our meta-analysis approach captures statistical properties of word usage across geographical regions and uses statistical methods to identify significant changes specific to regions. While previous approaches have primarily focused on lexical variation between regions, our method identifies words that demonstrate semantic and syntactic variation as well. We extend recently developed techniques for neural language models to learn word representations which capture differing semantics across geographical regions. In order to quantify this variation and ensure robust detection of true regional differences, we formulate a null model to determine whether observed changes are statistically significant. Our method is the first such approach to explicitly account for random variation due to chance while detecting regional variation in word meaning. To validate our model, we study and analyze two different massive online data sets: millions of tweets from Twitter spanning not only four different countries but also fifty states, as well as millions of phrases contained in the Google Book Ngrams. Our analysis reveals interesting facets of language change at multiple scales of geographic resolution -- from neighboring states to distant continents. Finally, using our model, we propose a measure of semantic distance between languages. Our analysis of British and American English over a period of 100 years reveals that semantic variation between these dialects is shrinking. version:2
arxiv-1502-05803 | Visual object tracking performance measures revisited | http://arxiv.org/abs/1502.05803 | id:1502.05803 author:Luka Čehovin, Aleš Leonardis, Matej Kristan category:cs.CV  published:2015-02-20 summary:The problem of visual tracking evaluation is sporting a large variety of performance measures, and largely suffers from lack of consensus about which measures should be used in experiments. This makes the cross-paper tracker comparison difficult. Furthermore, as some measures may be less effective than others, the tracking results may be skewed or biased towards particular tracking aspects. In this paper we revisit the popular performance measures and tracker performance visualizations and analyze them theoretically and experimentally. We show that several measures are equivalent from the point of information they provide for tracker comparison and, crucially, that some are more brittle than the others. Based on our analysis we narrow down the set of potential measures to only two complementary ones, describing accuracy and robustness, thus pushing towards homogenization of the tracker evaluation methodology. These two measures can be intuitively interpreted and visualized and have been employed by the recent Visual Object Tracking (VOT) challenges as the foundation for the evaluation methodology. version:3
arxiv-1603-02078 | A Learning-based Frame Pooling Model For Event Detection | http://arxiv.org/abs/1603.02078 | id:1603.02078 author:Jiang Liu, Chenqiang Gao, Lan Wang, Deyu Meng category:cs.CV  published:2016-03-07 summary:Detecting complex events in a large video collection crawled from video websites is a challenging task. When applying directly good image-based feature representation, e.g., HOG, SIFT, to videos, we have to face the problem of how to pool multiple frame feature representations into one feature representation. In this paper, we propose a novel learning-based frame pooling method. We formulate the pooling weight learning as an optimization problem and thus our method can automatically learn the best pooling weight configuration for each specific event category. Experimental results conducted on TRECVID MED 2011 reveal that our method outperforms the commonly used average pooling and max pooling strategies on both high-level and low-level 2D image features. version:1
arxiv-1506-04557 | Learning Deep Generative Models with Doubly Stochastic MCMC | http://arxiv.org/abs/1506.04557 | id:1506.04557 author:Chao Du, Jun Zhu, Bo Zhang category:cs.LG  published:2015-06-15 summary:We present doubly stochastic gradient MCMC, a simple and generic method for (approximate) Bayesian inference of deep generative models (DGMs) in a collapsed continuous parameter space. At each MCMC sampling step, the algorithm randomly draws a mini-batch of data samples to estimate the gradient of log-posterior and further estimates the intractable expectation over hidden variables via a neural adaptive importance sampler, where the proposal distribution is parameterized by a deep neural network and learnt jointly. We demonstrate the effectiveness on learning various DGMs in a wide range of tasks, including density estimation, data generation and missing data imputation. Our method outperforms many state-of-the-art competitors. version:4
arxiv-1603-02074 | Optimal dictionary for least squares representation | http://arxiv.org/abs/1603.02074 | id:1603.02074 author:Mohammed Rayyan Sheriff, Debasish Chatterjee category:cs.LG math.OC stat.ML  published:2016-03-07 summary:Dictionary Learning problems are concerned with finding a collection of vectors usually referred to as the dictionary, such that the representation of random vectors with a given distribution using this dictionary is optimal. Most of the recent research in dictionary learning is focused on developing dictionaries which offer sparse representation, i.e., optimal representation in the $\ell_0$ sense. We consider the problem of finding an optimal dictionary with which representation of samples of a random vector on an average is optimal. Optimality of representation is defined in the sense of attaining minimal average $\ell_2$-norm of the coefficient vector used to represent the random vector. With the help of recent results related to rank-one decompositions of real symmetric positive semi-definite matrices, an explicit solution for an $\ell_2$-optimal dictionary is obtained. version:1
arxiv-1603-02041 | Learning Shared Representations in Multi-task Reinforcement Learning | http://arxiv.org/abs/1603.02041 | id:1603.02041 author:Diana Borsa, Thore Graepel, John Shawe-Taylor category:cs.AI cs.LG  published:2016-03-07 summary:We investigate a paradigm in multi-task reinforcement learning (MT-RL) in which an agent is placed in an environment and needs to learn to perform a series of tasks, within this space. Since the environment does not change, there is potentially a lot of common ground amongst tasks and learning to solve them individually seems extremely wasteful. In this paper, we explicitly model and learn this shared structure as it arises in the state-action value space. We will show how one can jointly learn optimal value-functions by modifying the popular Value-Iteration and Policy-Iteration procedures to accommodate this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization. this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization. version:1
arxiv-1603-02038 | Unscented Bayesian Optimization for Safe Robot Grasping | http://arxiv.org/abs/1603.02038 | id:1603.02038 author:José Nogueira, Ruben Martinez-Cantin, Alexandre Bernardino, Lorenzo Jamone category:cs.RO cs.AI cs.LG cs.SY  published:2016-03-07 summary:We address the robot grasp optimization problem of unknown objects considering uncertainty in the input space. Grasping unknown objects can be achieved by using a trial and error exploration strategy. Bayesian optimization is a sample efficient optimization algorithm that is especially suitable for this setups as it actively reduces the number of trials for learning about the function to optimize. In fact, this active object exploration is the same strategy that infants do to learn optimal grasps. One problem that arises while learning grasping policies is that some configurations of grasp parameters may be very sensitive to error in the relative pose between the object and robot end-effector. We call these configurations unsafe because small errors during grasp execution may turn good grasps into bad grasps. Therefore, to reduce the risk of grasp failure, grasps should be planned in safe areas. We propose a new algorithm, Unscented Bayesian optimization that is able to perform sample efficient optimization while taking into consideration input noise to find safe optima. The contribution of Unscented Bayesian optimization is twofold as if provides a new decision process that drives exploration to safe regions and a new selection procedure that chooses the optimal in terms of its safety without extra analysis or computational cost. Both contributions are rooted on the strong theory behind the unscented transformation, a popular nonlinear approximation method. We show its advantages with respect to the classical Bayesian optimization both in synthetic problems and in realistic robot grasp simulations. The results highlights that our method achieves optimal and robust grasping policies after few trials while the selected grasps remain in safe regions. version:1
arxiv-1603-02028 | Adaptive Visualisation System for Construction Building Information Models Using Saliency | http://arxiv.org/abs/1603.02028 | id:1603.02028 author:Hugo Martin, Sylvain Chevallier, Eric Monacelli category:cs.CV cs.AI  published:2016-03-07 summary:Building Information Modeling (BIM) is a recent construction process based on a 3D model, containing every component related to the building achievement. Architects, structure engineers, method engineers, and others participant to the building process work on this model through the design-to-construction cycle. The high complexity and the large amount of information included in these models raise several issues, delaying its wide adoption in the industrial world. One of the most important is the visualization: professionals have difficulties to find out the relevant information for their job. Actual solutions suffer from two limitations: the BIM models information are processed manually and insignificant information are simply hidden, leading to inconsistencies in the building model. This paper describes a system relying on an ontological representation of the building information to label automatically the building elements. Depending on the user's department, the visualization is modified according to these labels by automatically adjusting the colors and image properties based on a saliency model. The proposed saliency model incorporates several adaptations to fit the specificities of architectural images. version:1
arxiv-1603-02010 | Differentially Private Policy Evaluation | http://arxiv.org/abs/1603.02010 | id:1603.02010 author:Borja Balle, Maziar Gomrokchi, Doina Precup category:cs.LG stat.ML  published:2016-03-07 summary:We present the first differentially private algorithms for reinforcement learning, which apply to the task of evaluating a fixed policy. We establish two approaches for achieving differential privacy, provide a theoretical analysis of the privacy and utility of the two algorithms, and show promising results on simple empirical examples. version:1
arxiv-1603-02008 | Position paper: Towards an observer-oriented theory of shape comparison | http://arxiv.org/abs/1603.02008 | id:1603.02008 author:Patrizio Frosini category:cs.CG cs.CV math.AT I.4.7; I.5.1  published:2016-03-07 summary:In this position paper we suggest a possible metric approach to shape comparison that is based on a mathematical formalization of the concept of observer, seen as a collection of suitable operators acting on a metric space of functions. These functions represent the set of data that are accessible to the observer, while the operators describe the way the observer elaborates the data and enclose the invariance that he/she associates with them. We expose this model and illustrate some theoretical reasons that justify its possible use for shape comparison. version:1
arxiv-1603-02003 | From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators | http://arxiv.org/abs/1603.02003 | id:1603.02003 author:Paul Upchurch, Noah Snavely, Kavita Bala category:cs.CV  published:2016-03-07 summary:We propose a new neural network architecture for solving single-image analogies - the generation of an entire set of stylistically similar images from just a single input image. Solving this problem requires separating image style from content. Our network is a modified variational autoencoder (VAE) that supports supervised training of single-image analogies and in-network evaluation of outputs with a structured similarity objective that captures pixel covariances. On the challenging task of generating a 62-letter font from a single example letter we produce images with 22.4% lower dissimilarity to the ground truth than state-of-the-art. version:1
arxiv-1603-01987 | A matter of words: NLP for quality evaluation of Wikipedia medical articles | http://arxiv.org/abs/1603.01987 | id:1603.01987 author:Vittoria Cozza, Marinella Petrocchi, Angelo Spognardi category:cs.IR cs.CL  published:2016-03-07 summary:Automatic quality evaluation of Web information is a task with many fields of applications and of great relevance, especially in critical domains like the medical one. We move from the intuition that the quality of content of medical Web documents is affected by features related with the specific domain. First, the usage of a specific vocabulary (Domain Informativeness); then, the adoption of specific codes (like those used in the infoboxes of Wikipedia articles) and the type of document (e.g., historical and technical ones). In this paper, we propose to leverage specific domain features to improve the results of the evaluation of Wikipedia medical articles. In particular, we evaluate the articles adopting an "actionable" model, whose features are related to the content of the articles, so that the model can also directly suggest strategies for improving a given article quality. We rely on Natural Language Processing (NLP) and dictionaries-based techniques in order to extract the bio-medical concepts in a text. We prove the effectiveness of our approach by classifying the medical articles of the Wikipedia Medicine Portal, which have been previously manually labeled by the Wiki Project team. The results of our experiments confirm that, by considering domain-oriented features, it is possible to obtain sensible improvements with respect to existing solutions, mainly for those articles that other approaches have less correctly classified. Other than being interesting by their own, the results call for further research in the area of domain specific features suitable for Web data quality assessment. version:1
arxiv-1510-03608 | Deep convolutional neural networks for pedestrian detection | http://arxiv.org/abs/1510.03608 | id:1510.03608 author:Denis Tomè, Federico Monti, Luca Baroffio, Luca Bondi, Marco Tagliasacchi, Stefano Tubaro category:cs.CV  published:2015-10-13 summary:Pedestrian detection is a popular research topic due to its paramount importance for a number of applications, especially in the fields of automotive, surveillance and robotics. Despite the significant improvements, pedestrian detection is still an open challenge that calls for more and more accurate algorithms. In the last few years, deep learning and in particular convolutional neural networks emerged as the state of the art in terms of accuracy for a number of computer vision tasks such as image classification, object detection and segmentation, often outperforming the previous gold standards by a large margin. In this paper, we propose a pedestrian detection system based on deep learning, adapting a general-purpose convolutional network to the task at hand. By thoroughly analyzing and optimizing each step of the detection pipeline we propose an architecture that outperforms traditional methods, achieving a task accuracy close to that of state-of-the-art approaches, while requiring a low computational time. Finally, we tested the system on an NVIDIA Jetson TK1, a 192-core platform that is envisioned to be a forerunner computational brain of future self-driving cars. version:5
arxiv-1311-6531 | Brains and pseudorandom generators | http://arxiv.org/abs/1311.6531 | id:1311.6531 author:Vašek Chvátal, Mark Goldsmith, Nan Yang category:math.DS cs.CR cs.NE math.NA  published:2013-11-26 summary:In a pioneering classic, Warren McCulloch and Walter Pitts proposed a model of the central nervous system; motivated by EEG recordings of normal brain activity, Chv\' atal and Goldsmith asked whether or not this model can be engineered to provide pseudorandom number generators. We supply evidence suggesting that the answer is negative. version:3
arxiv-1603-01976 | Deep Contrast Learning for Salient Object Detection | http://arxiv.org/abs/1603.01976 | id:1603.01976 author:Guanbin Li, Yizhou Yu category:cs.CV  published:2016-03-07 summary:Salient object detection has recently witnessed substantial progress due to powerful features extracted using deep convolutional neural networks (CNNs). However, existing CNN-based methods operate at the patch level instead of the pixel level. Resulting saliency maps are typically blurry, especially near the boundary of salient objects. Furthermore, image patches are treated as independent samples even when they are overlapping, giving rise to significant redundancy in computation and storage. In this CVPR 2016 paper, we propose an end-to-end deep contrast network to overcome the aforementioned limitations. Our deep network consists of two complementary components, a pixel-level fully convolutional stream and a segment-wise spatial pooling stream. The first stream directly produces a saliency map with pixel-level accuracy from an input image. The second stream extracts segment-wise features very efficiently, and better models saliency discontinuities along object boundaries. Finally, a fully connected CRF model can be optionally incorporated to improve spatial coherence and contour localization in the fused result from these two streams. Experimental results demonstrate that our deep model significantly improves the state of the art. version:1
arxiv-1511-01214 | Quantifying the information of the prior and likelihood in parametric Bayesian modeling | http://arxiv.org/abs/1511.01214 | id:1511.01214 author:Giri Gopalan category:stat.ML cs.IT math.IT stat.AP stat.ME  published:2015-11-04 summary:I suggest using a pair of metrics to quantify the information of the prior and likelihood functions within a parametric Bayesian model, one of which is closely related to the reference priors of Berger and Bernardo (Bernardo 1979, Berger and Bernardo 2009) and information measure introduced by Lindley (Lindley 1956). A Monte Carlo algorithm to estimate these metrics is developed and their properties are explored via a combination of theoretical results, simulations, and applications to public medical data sets. This combination of theoretical, empirical, and computational support provides evidence that these metrics may be useful diagnostic tools when performing a Bayesian analysis. version:7
arxiv-1511-02258 | Efficient Multiscale Gaussian Process Regression using Hierarchical Clustering | http://arxiv.org/abs/1511.02258 | id:1511.02258 author:Z. Zhang, K. Duraisamy, N. A. Gumerov category:cs.LG stat.ML  published:2015-11-06 summary:Standard Gaussian Process (GP) regression, a powerful machine learning tool, is computationally expensive when it is applied to large datasets, and potentially inaccurate when data points are sparsely distributed in a high-dimensional feature space. To address these challenges, a new multiscale, sparsified GP algorithm is formulated, with the goal of application to large scientific computing datasets. In this approach, the data is partitioned into clusters and the cluster centers are used to define a reduced training set, resulting in an improvement over standard GPs in terms of training and evaluation costs. Further, a hierarchical technique is used to adaptively map the local covariance representation to the underlying sparsity of the feature space, leading to improved prediction accuracy when the data distribution is highly non-uniform. A theoretical investigation of the computational complexity of the algorithm is presented. The efficacy of this method is then demonstrated on smooth and discontinuous analytical functions and on data from a direct numerical simulation of turbulent combustion. version:2
arxiv-1603-01901 | Confidence-Constrained Maximum Entropy Framework for Learning from Multi-Instance Data | http://arxiv.org/abs/1603.01901 | id:1603.01901 author:Behrouz Behmardi, Forrest Briggs, Xiaoli Z. Fern, Raviv Raich category:cs.LG cs.IT math.IT stat.ML  published:2016-03-07 summary:Multi-instance data, in which each object (bag) contains a collection of instances, are widespread in machine learning, computer vision, bioinformatics, signal processing, and social sciences. We present a maximum entropy (ME) framework for learning from multi-instance data. In this approach each bag is represented as a distribution using the principle of ME. We introduce the concept of confidence-constrained ME (CME) to simultaneously learn the structure of distribution space and infer each distribution. The shared structure underlying each density is used to learn from instances inside each bag. The proposed CME is free of tuning parameters. We devise a fast optimization algorithm capable of handling large scale multi-instance data. In the experimental section, we evaluate the performance of the proposed approach in terms of exact rank recovery in the space of distributions and compare it with the regularized ME approach. Moreover, we compare the performance of CME with Multi-Instance Learning (MIL) state-of-the-art algorithms and show a comparable performance in terms of accuracy with reduced computational complexity. version:1
arxiv-1412-4098 | Manifold Matching using Shortest-Path Distance and Joint Neighborhood Selection | http://arxiv.org/abs/1412.4098 | id:1412.4098 author:Cencheng Shen, Joshua T. Vogelstein, Carey E. Priebe category:stat.ML  published:2014-12-12 summary:We propose a nonlinear manifold matching algorithm to match multiple data sets using shortest-path distance and joint neighborhood selection. Based on the correspondence information, a neighborhood graph is jointly constructed; then the shortest-path distance within each data set is computed from the joint neighborhood graph, followed by embedding into and matching in a common low-dimensional Euclidean space. Our approach exhibits superior and robust performance for matching data from disparate sources, compared to algorithms that do not use shortest-path distance or joint neighborhood selection. version:3
arxiv-1603-01882 | Composing inference algorithms as program transformations | http://arxiv.org/abs/1603.01882 | id:1603.01882 author:Robert Zinkov, Chung-chieh Shan category:stat.ML cs.AI stat.CO stat.ME  published:2016-03-06 summary:Probabilistic inference procedures are usually coded painstakingly from scratch, for each target model and each inference algorithm. We reduce this coding effort by generating inference procedures from models automatically. We make this code generation modular by decomposing inference algorithms into reusable program transformations. These source-to-source transformations perform exact inference as well as generate probabilistic programs that compute expectations, densities, and MCMC samples. The resulting inference procedures run in time comparable to that of handwritten procedures. version:1
arxiv-1410-1103 | Online Ranking with Top-1 Feedback | http://arxiv.org/abs/1410.1103 | id:1410.1103 author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG  published:2014-10-05 summary:We consider a setting where a system learns to rank a fixed set of $m$ items. The goal is produce good item rankings for users with diverse interests who interact online with the system for $T$ rounds. We consider a novel top-$1$ feedback model: at the end of each round, the relevance score for only the top ranked object is revealed. However, the performance of the system is judged on the entire ranked list. We provide a comprehensive set of results regarding learnability under this challenging setting. For PairwiseLoss and DCG, two popular ranking measures, we prove that the minimax regret is $\Theta(T^{2/3})$. Moreover, the minimax regret is achievable using an efficient strategy that only spends $O(m \log m)$ time per round. The same efficient strategy achieves $O(T^{2/3})$ regret for Precision@$k$. Surprisingly, we show that for normalized versions of these ranking measures, i.e., AUC, NDCG \& MAP, no online ranking algorithm can have sublinear regret. version:3
arxiv-1603-01870 | Personalized Advertisement Recommendation: A Ranking Approach to Address the Ubiquitous Click Sparsity Problem | http://arxiv.org/abs/1603.01870 | id:1603.01870 author:Sougata Chaudhuri, Georgios Theocharous, Mohammad Ghavamzadeh category:cs.LG cs.IR  published:2016-03-06 summary:We study the problem of personalized advertisement recommendation (PAR), which consist of a user visiting a system (website) and the system displaying one of $K$ ads to the user. The system uses an internal ad recommendation policy to map the user's profile (context) to one of the ads. The user either clicks or ignores the ad and correspondingly, the system updates its recommendation policy. PAR problem is usually tackled by scalable \emph{contextual bandit} algorithms, where the policies are generally based on classifiers. A practical problem in PAR is extreme click sparsity, due to very few users actually clicking on ads. We systematically study the drawback of using contextual bandit algorithms based on classifier-based policies, in face of extreme click sparsity. We then suggest an alternate policy, based on rankers, learnt by optimizing the Area Under the Curve (AUC) ranking loss, which can significantly alleviate the problem of click sparsity. We conduct extensive experiments on public datasets, as well as three industry proprietary datasets, to illustrate the improvement in click-through-rate (CTR) obtained by using the ranker-based policy over classifier-based policies. version:1
arxiv-1603-01864 | General Participative Media Single Image Restoration | http://arxiv.org/abs/1603.01864 | id:1603.01864 author:Felipe Codevilla, Joel D. O. Gaya, Amanda C. Duarte, Silvia Botelho category:cs.CV  published:2016-03-06 summary:This paper describes a method to restore degraded images captured in general participative media --- fog, turbid water, sand storm, etc. To obtain generality, we, first, propose a novel interpretation of the participative media image formation by considering the color variation of the media. Second, we introduce that joining different image priors is an effective alternative for image restoration. The proposed method contains a Composite Prior supported by statistics collected on both haze-free and degraded participative environment images. The key of the method is joining two complementary measures --- local contrast and color. The results presented for a variety of underwater and haze images demonstrate the power of the method. Moreover, we showed the potential of our method using a special dataset for which a reference haze-free image is available for comparison. version:1
arxiv-1603-01860 | Generalization error bounds for learning to rank: Does the length of document lists matter? | http://arxiv.org/abs/1603.01860 | id:1603.01860 author:Ambuj Tewari, Sougata Chaudhuri category:cs.LG  published:2016-03-06 summary:We consider the generalization ability of algorithms for learning to rank at a query level, a problem also called subset ranking. Existing generalization error bounds necessarily degrade as the size of the document list associated with a query increases. We show that such a degradation is not intrinsic to the problem. For several loss functions, including the cross-entropy loss used in the well known ListNet method, there is \emph{no} degradation in generalization ability as document lists become longer. We also provide novel generalization error bounds under $\ell_1$ regularization and faster convergence rates if the loss function is smooth. version:1
arxiv-1603-01855 | Online Learning to Rank with Feedback at the Top | http://arxiv.org/abs/1603.01855 | id:1603.01855 author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG  published:2016-03-06 summary:We consider an online learning to rank setting in which, at each round, an oblivious adversary generates a list of $m$ documents, pertaining to a query, and the learner produces scores to rank the documents. The adversary then generates a relevance vector and the learner updates its ranker according to the feedback received. We consider the setting where the feedback is restricted to be the relevance levels of only the top $k$ documents in the ranked list for $k \ll m$. However, the performance of learner is judged based on the unrevealed full relevance vectors, using an appropriate learning to rank loss function. We develop efficient algorithms for well known losses in the pointwise, pairwise and listwise families. We also prove that no online algorithm can have sublinear regret, with top-1 feedback, for any loss that is calibrated with respect to NDCG. We apply our algorithms on benchmark datasets demonstrating efficient online learning of a ranking function from highly restricted feedback. version:1
arxiv-1603-01842 | Proximal groupoid patterns In digital images | http://arxiv.org/abs/1603.01842 | id:1603.01842 author:Enoch A-iyeh, James F. Peters category:cs.CV 54E40  published:2016-03-06 summary:The focus of this article is on the detection and classification of patterns based on groupoids. The approach hinges on descriptive proximity of points in a set based on the neighborliness property. This approach lends support to image analysis and understanding and in studying nearness of image segments. A practical application of the approach is in terms of the analysis of natural images for pattern identification and classification. version:1
arxiv-1603-01840 | Hierarchical Decision Making In Electricity Grid Management | http://arxiv.org/abs/1603.01840 | id:1603.01840 author:Gal Dalal, Elad Gilboa, Shie Mannor category:cs.AI cs.LG stat.AP  published:2016-03-06 summary:The power grid is a complex and vital system that necessitates careful reliability management. Managing the grid is a difficult problem with multiple time scales of decision making and stochastic behavior due to renewable energy generations, variable demand and unplanned outages. Solving this problem in the face of uncertainty requires a new methodology with tractable algorithms. In this work, we introduce a new model for hierarchical decision making in complex systems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a level of abstraction, for real-time power grid reliability. We devise an algorithm that alternates between slow time-scale policy improvement, and fast time-scale value function approximation. We compare our results to prevailing heuristics, and show the strength of our method. version:1
arxiv-1603-01833 | Semi-Automatic Data Annotation, POS Tagging and Mildly Context-Sensitive Disambiguation: the eXtended Revised AraMorph (XRAM) | http://arxiv.org/abs/1603.01833 | id:1603.01833 author:Giuliano Lancioni, Valeria Pettinari, Laura Garofalo, Marta Campanelli, Ivana Pepe, Simona Olivieri, Ilaria Cicola category:cs.CL cs.IR  published:2016-03-06 summary:An extended, revised form of Tim Buckwalter's Arabic lexical and morphological resource AraMorph, eXtended Revised AraMorph (henceforth XRAM), is presented which addresses a number of weaknesses and inconsistencies of the original model by allowing a wider coverage of real-world Classical and contemporary (both formal and informal) Arabic texts. Building upon previous research, XRAM enhancements include (i) flag-selectable usage markers, (ii) probabilistic mildly context-sensitive POS tagging, filtering, disambiguation and ranking of alternative morphological analyses, (iii) semi-automatic increment of lexical coverage through extraction of lexical and morphological information from existing lexical resources. Testing of XRAM through a front-end Python module showed a remarkable success level. version:1
arxiv-1509-03611 | A Parallel Corpus of Translationese | http://arxiv.org/abs/1509.03611 | id:1509.03611 author:Ella Rabinovich, Shuly Wintner, Ofek Luis Lewinsohn category:cs.CL  published:2015-09-11 summary:We describe a set of bilingual English--French and English--German parallel corpora in which the direction of translation is accurately and reliably annotated. The corpora are diverse, consisting of parliamentary proceedings, literary works, transcriptions of TED talks and political commentary. They will be instrumental for research of translationese and its applications to (human and machine) translation; specifically, they can be used for the task of translationese identification, a research direction that enjoys a growing interest in recent years. To validate the quality and reliability of the corpora, we replicated previous results of supervised and unsupervised identification of translationese, and further extended the experiments to additional datasets and languages. version:2
arxiv-1602-06516 | Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques | http://arxiv.org/abs/1602.06516 | id:1602.06516 author:Debarghya Ghoshdastidar, Ambedkar Dukkipati category:cs.LG stat.ML  published:2016-02-21 summary:Graph partitioning plays a central role in machine learning, and the development of graph partitioning algorithms is still an active area of research. The immense demand for such algorithms arises due to the abundance of applications that involve pairwise interactions or similarities among entities. Recent studies in computer vision and databases systems have emphasized on the necessity of considering multi-way interactions, and has led to the study of a more general problem in the form of hypergraph partitioning. This paper focuses on the problem of partitioning uniform hypergraphs, which arises in applications such as subspace clustering, motion segmentation etc. We show that uniform hypergraph partitioning is equivalent to a tensor trace maximization problem, and hence, a tensor based method is a natural answer to this problem. We also propose a tensor spectral method that extends the widely known spectral clustering algorithm to the case of uniform hypergraphs. While the theoretical guarantees of spectral clustering have been extensively studied, very little is known about the statistical properties of tensor based methods. To this end, we prove the consistency of the proposed algorithm under a planted partition model. The computational complexity of tensorial approaches has resulted in the use of various tensor sampling strategies. We present the first theoretical study on the effect of sampling in tensor based hypergraph partitioning. Our result justifies the empirical success of iterative sampling techniques often used in practice. We also present an iteratively sampled variant of the proposed algorithm for the purpose of subspace clustering, and demonstrate the performance of this method on a benchmark problem. version:2
arxiv-1603-01801 | Variational methods for Conditional Multimodal Learning: Generating Human Faces from Attributes | http://arxiv.org/abs/1603.01801 | id:1603.01801 author:Gaurav Pandey, Ambedkar Dukkipati category:cs.CV cs.LG stat.ML  published:2016-03-06 summary:Prior to this decade, the field of computer vision was primarily focused around hand-crafted feature extraction methods used in conjunction with discriminative models for specific tasks such as object recognition, detection/localization, tracking etc. A generative image understanding was neither within reach nor the prime concern of the period. In this paper, we address the following problem: Given a description of a human face, can we generate the image corresponding to it? We frame this problem as a conditional modality learning problem and use variational methods for maximizing the corresponding conditional log-likelihood. The resultant deep model, which we refer to as conditional multimodal autoencoder (CMMA), forces the latent representation obtained from the attributes alone to be 'close' to the joint representation obtained from both face and attributes. We show that the faces generated from attributes using the proposed model, are qualitatively and quantitatively more representative of the attributes from which they were generated, than those obtained by other deep generative models. We also propose a secondary task, whereby the existing faces are modified by modifying the corresponding attributes. We observe that the modifications in face introduced by the proposed model are representative of the corresponding modifications in attributes. Hence, our proposed method solves the above mentioned problem. version:1
arxiv-1603-01772 | Fast calculation of correlations in recognition systems | http://arxiv.org/abs/1603.01772 | id:1603.01772 author:Pavel Dourbal, Mikhail Pekker category:cs.CV  published:2016-03-06 summary:Computationally efficient classification system architecture is proposed. It utilizes fast tensor-vector multiplication algorithm to apply linear operators upon input signals . The approach is applicable to wide variety of recognition system architectures ranging from single stage matched filter bank classifiers to complex neural networks with unlimited number of hidden layers. version:1
arxiv-1603-01768 | Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks | http://arxiv.org/abs/1603.01768 | id:1603.01768 author:Alex J. Champandard category:cs.CV  published:2016-03-05 summary:Convolutional neural networks (CNNs) have proven highly effective at image synthesis and style transfer. For most users, however, using them as tools can be a challenging task due to their unpredictable behavior that goes against common intuitions. This paper introduces a novel concept to augment such generative architectures with semantic annotations, either by manually authoring pixel labels or using existing solutions for semantic segmentation. The result is a content-aware generative algorithm that offers meaningful control over the outcome. Thus, we increase the quality of images generated by avoiding common glitches, make the results look significantly more plausible, and extend the functional range of these algorithms---whether for portraits or landscapes, etc. Applications include semantic style transfer and turning doodles with few colors into masterful paintings! version:1
arxiv-1506-07285 | Ask Me Anything: Dynamic Memory Networks for Natural Language Processing | http://arxiv.org/abs/1506.07285 | id:1506.07285 author:Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher category:cs.CL cs.LG cs.NE  published:2015-06-24 summary:Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets. version:5
arxiv-1508-07630 | Feature Selection via Binary Simultaneous Perturbation Stochastic Approximation | http://arxiv.org/abs/1508.07630 | id:1508.07630 author:Vural Aksakalli, Milad Malekipirbazari category:stat.ML cs.LG  published:2015-08-30 summary:Feature selection (FS) has become an indispensable task in dealing with today's highly complex pattern recognition problems with massive number of features. In this study, we propose a new wrapper approach for FS based on binary simultaneous perturbation stochastic approximation (BSPSA). This pseudo-gradient descent stochastic algorithm starts with an initial feature vector and moves toward the optimal feature vector via successive iterations. In each iteration, the current feature vector's individual components are perturbed simultaneously by random offsets from a qualified probability distribution. We present computational experiments on datasets with numbers of features ranging from a few dozens to thousands using three widely-used classifiers as wrappers: nearest neighbor, decision tree, and linear support vector machine. We compare our methodology against the full set of features as well as a binary genetic algorithm and sequential FS methods using cross-validated classification error rate and AUC as the performance criteria. Our results indicate that features selected by BSPSA compare favorably to alternative methods in general and BSPSA can yield superior feature sets for datasets with tens of thousands of features by examining an extremely small fraction of the solution space. We are not aware of any other wrapper FS methods that are computationally feasible with good convergence properties for such large datasets. version:3
arxiv-1603-02626 | UTA-poly and UTA-splines: additive value functions with polynomial marginals | http://arxiv.org/abs/1603.02626 | id:1603.02626 author:Olivier Sobrie, Nicolas Gillis, Vincent Mousseau, Marc Pirlot category:math.OC cs.AI cs.LG  published:2016-03-05 summary:Additive utility function models are widely used in multiple criteria decision analysis. In such models, a numerical value is associated to each alternative involved in the decision problem. It is computed by aggregating the scores of the alternative on the different criteria of the decision problem. The score of an alternative is determined by a marginal value function that evolves monotonically as a function of the performance of the alternative on this criterion. Determining the shape of the marginals is not easy for a decision maker. It is easier for him/her to make statements such as "alternative $a$ is preferred to $b$". In order to help the decision maker, UTA disaggregation procedures use linear programming to approximate the marginals by piecewise linear functions based only on such statements. In this paper, we propose to infer polynomials and splines instead of piecewise linear functions for the marginals. In this aim, we use semidefinite programming instead of linear programming. We illustrate this new elicitation method and present some experimental results. version:1
arxiv-1603-01739 | Grading of Mammalian Cumulus Oocyte Complexes using Machine Learning for in Vitro Embryo Culture | http://arxiv.org/abs/1603.01739 | id:1603.01739 author:Viswanath P Sudarshan, Tobias Weiser, Phalgun Chintala, Subhamoy Mandal, Rahul Dutta category:cs.CV physics.med-ph  published:2016-03-05 summary:Visual observation of Cumulus Oocyte Complexes provides only limited information about its functional competence, whereas the molecular evaluations methods are cumbersome or costly. Image analysis of mammalian oocytes can provide attractive alternative to address this challenge. However, it is complex, given the huge number of oocytes under inspection and the subjective nature of the features inspected for identification. Supervised machine learning methods like random forest with annotations from expert biologists can make the analysis task standardized and reduces inter-subject variability. We present a semi-automatic framework for predicting the class an oocyte belongs to, based on multi-object parametric segmentation on the acquired microscopic image followed by a feature based classification using random forests. version:1
arxiv-1603-01716 | Classifier ensemble creation via false labelling | http://arxiv.org/abs/1603.01716 | id:1603.01716 author:Bálint Antal category:cs.LG I.2  I.5.2  published:2016-03-05 summary:In this paper, a novel approach to classifier ensemble creation is presented. While other ensemble creation techniques are based on careful selection of existing classifiers or preprocessing of the data, the presented approach automatically creates an optimal labelling for a number of classifiers, which are then assigned to the original data instances and fed to classifiers. The approach has been evaluated on high-dimensional biomedical datasets. The results show that the approach outperformed individual approaches in all cases. version:1
arxiv-1603-01700 | High-Dimensional Metrics in R | http://arxiv.org/abs/1603.01700 | id:1603.01700 author:Victor Chernozhukov, Chris Hansen, Martin Spindler category:stat.ML stat.ME  published:2016-03-05 summary:The package High-dimensional Metrics (\Rpackage{hdm}) is an evolving collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. It focuses on providing semi-parametrically efficient estimators, confidence intervals, and significance testing for low-dimensional subcomponents of the high-dimensional parameter vector. This vignette offers a brief introduction and a tutorial to the implemented methods. \R and the package \Rpackage{hdm} are open-source software projects and can be freely downloaded from CRAN: \texttt{http://cran.r-project.org}. version:1
arxiv-1603-01696 | A Feature Learning and Object Recognition Framework for Underwater Fish Images | http://arxiv.org/abs/1603.01696 | id:1603.01696 author:Meng-Che Chuang, Jenq-Neng Hwang, Kresimir Williams category:cs.CV  published:2016-03-05 summary:Live fish recognition is one of the most crucial elements of fisheries survey applications where vast amount of data are rapidly acquired. Different from general scenarios, challenges to underwater image recognition are posted by poor image quality, uncontrolled objects and environment, as well as difficulty in acquiring representative samples. Also, most existing feature extraction techniques are hindered from automation due to involving human supervision. Toward this end, we propose an underwater fish recognition framework that consists of a fully unsupervised feature learning technique and an error-resilient classifier. Object parts are initialized based on saliency and relaxation labeling to match object parts correctly. A non-rigid part model is then learned based on fitness, separation and discrimination criteria. For the classifier, an unsupervised clustering approach generates a binary class hierarchy, where each node is a classifier. To exploit information from ambiguous images, the notion of partial classification is introduced to assign coarse labels by optimizing the "benefit" of indecision made by the classifier. Experiments show that the proposed framework achieves high accuracy on both public and self-collected underwater fish images with high uncertainty and class imbalance. version:1
arxiv-1603-01695 | Underwater Fish Tracking for Moving Cameras based on Deformable Multiple Kernels | http://arxiv.org/abs/1603.01695 | id:1603.01695 author:Meng-Che Chuang, Jenq-Neng Hwang, Jian-Hui Ye, Shih-Chia Huang, Kresimir Williams category:cs.CV  published:2016-03-05 summary:Fishery surveys that call for the use of single or multiple underwater cameras have been an emerging technology as a non-extractive mean to estimate the abundance of fish stocks. Tracking live fish in an open aquatic environment posts challenges that are different from general pedestrian or vehicle tracking in surveillance applications. In many rough habitats fish are monitored by cameras installed on moving platforms, where tracking is even more challenging due to inapplicability of background models. In this paper, a novel tracking algorithm based on the deformable multiple kernels (DMK) is proposed to address these challenges. Inspired by the deformable part model (DPM) technique, a set of kernels is defined to represent the holistic object and several parts that are arranged in a deformable configuration. Color histogram, texture histogram and the histogram of oriented gradients (HOG) are extracted and serve as object features. Kernel motion is efficiently estimated by the mean-shift algorithm on color and texture features to realize tracking. Furthermore, the HOG-feature deformation costs are adopted as soft constraints on kernel positions to maintain the part configuration. Experimental results on practical video set from underwater moving cameras show the reliable performance of the proposed method with much less computational cost comparing with state-of-the-art techniques. version:1
arxiv-1603-01684 | Saliency Detection combining Multi-layer Integration algorithm with background prior and energy function | http://arxiv.org/abs/1603.01684 | id:1603.01684 author:Hanling Zhang, Chenxing Xia category:cs.CV  published:2016-03-05 summary:In this paper, we propose an improved mechanism for saliency detection. Firstly,based on a neoteric background prior selecting four corners of an image as background,we use color and spatial contrast with each superpixel to obtain a salinecy map(CBP). Inspired by reverse-measurement methods to improve the accuracy of measurement in Engineering,we employ the Objectness labels as foreground prior based on part of information of CBP to construct a map(OFP).Further,an original energy function is applied to optimize both of them respectively and a single-layer saliency map(SLP)is formed by merging the above twos.Finally,to deal with the scale problem,we obtain our multi-layer map(MLP) by presenting an integration algorithm to take advantage of multiple saliency maps. Quantitative and qualitative experiments on three datasets demonstrate that our method performs favorably against the state-of-the-art algorithm. version:1
arxiv-1603-01681 | A single-phase, proximal path-following framework | http://arxiv.org/abs/1603.01681 | id:1603.01681 author:Quoc Tran-Dinh, Anastasios Kyrillidis, Volkan Cevher category:math.OC cs.IT math.IT stat.ML 90C06  90C25  90-08  published:2016-03-05 summary:We propose a new proximal, path-following framework for a class of---possibly non-smooth---constrained convex problems. We consider settings where the non-smooth part is endowed with a proximity operator, and the constraint set is equipped with a self-concordant barrier. Our main contribution is a new re-parametrization of the optimality condition of the barrier problem, that allows us to process the objective function with its proximal operator within a new path following scheme. In particular, our approach relies on the following two main ideas. First, we re-parameterize the optimality condition as an auxiliary problem, such that a "good" initial point is available. Second, we combine the proximal operator of the objective and path-following ideas to design a single phase, proximal, path-following algorithm. Our method has several advantages. First, it allows handling non-smooth objectives via proximal operators, this avoids lifting the problem dimension via slack variables and additional constraints. Second, it consists of only a \emph{single phase} as compared to a two-phase algorithm in [43] In this work, we show how to overcome this difficulty in the proximal setting and prove that our scheme has the same $\mathcal{O}(\sqrt{\nu}\log(1/\varepsilon))$ worst-case iteration-complexity with standard approaches [30, 33], but our method can handle nonsmooth objectives, where $\nu$ is the barrier parameter and $\varepsilon$ is a desired accuracy. Finally, our framework allows errors in the calculation of proximal-Newton search directions, without sacrificing the worst-case iteration complexity. We demonstrate the merits of our algorithm via three numerical examples, where proximal operators play a key role to improve the performance over off-the-shelf interior-point solvers. version:1
arxiv-1603-01648 | Getting More Out Of Syntax with PropS | http://arxiv.org/abs/1603.01648 | id:1603.01648 author:Gabriel Stanovsky, Jessica Ficler, Ido Dagan, Yoav Goldberg category:cs.CL  published:2016-03-04 summary:Semantic NLP applications often rely on dependency trees to recognize major elements of the proposition structure of sentences. Yet, while much semantic structure is indeed expressed by syntax, many phenomena are not easily read out of dependency trees, often leading to further ad-hoc heuristic post-processing or to information loss. To directly address the needs of semantic applications, we present PropS -- an output representation designed to explicitly and uniformly express much of the proposition structure which is implied from syntax, and an associated tool for extracting it from dependency trees. version:1
arxiv-1601-07996 | Feature Selection: A Data Perspective | http://arxiv.org/abs/1601.07996 | id:1601.07996 author:Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P. Trevino, Jiliang Tang, Huan Liu category:cs.LG  published:2016-01-29 summary:Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems. The objectives of feature selection include: building simpler and more comprehensible models, improving data mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities of feature selection algorithms. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the big data age, we revisit feature selection research from a data perspective, and review representative feature selection algorithms for generic data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for generic data, we generally categorize them into four groups: similarity based, information theoretical based, sparse learning based and statistical based methods. Finally, to facilitate and promote the research in this community, we also present a open-source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). At the end of this survey, we also have a discussion about some open problems and challenges that need to be paid more attention in future research. version:3
arxiv-1603-01633 | Depth Superresolution using Motion Adaptive Regularization | http://arxiv.org/abs/1603.01633 | id:1603.01633 author:Ulugbek S. Kamilov, Petros T. Boufounos category:cs.CV  published:2016-03-04 summary:Spatial resolution of depth sensors is often significantly lower compared to that of conventional optical cameras. Recent work has explored the idea of improving the resolution of depth using higher resolution intensity as a side information. In this paper, we demonstrate that further incorporating temporal information in videos can significantly improve the results. In particular, we propose a novel approach that improves depth resolution, exploiting the space-time redundancy in the depth and intensity using motion-adaptive low-rank regularization. Experiments confirm that the proposed approach substantially improves the quality of the estimated high-resolution depth. Our approach can be a first component in systems using vision techniques that rely on high resolution depth information. version:1
arxiv-1511-08198 | Towards Universal Paraphrastic Sentence Embeddings | http://arxiv.org/abs/1511.08198 | id:1511.08198 author:John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu category:cs.CL cs.LG  published:2015-11-25 summary:We consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database (Ganitkevitch et al., 2013). We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training data and from a wide range of other domains. We find that the most complex architectures, such as long short-term memory (LSTM) recurrent neural networks, perform best on the in-domain data. However, in out-of-domain scenarios, simple architectures such as word averaging vastly outperform LSTMs. Our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efficient and easy to use. In order to better understand how these architectures compare, we conduct further experiments on three supervised NLP tasks: sentence similarity, entailment, and sentiment classification. We again find that the word averaging models perform well for sentence similarity and entailment, outperforming LSTMs. However, on sentiment classification, we find that the LSTM performs very strongly-even recording new state-of-the-art performance on the Stanford Sentiment Treebank. We then demonstrate how to combine our pretrained sentence embeddings with these supervised tasks, using them both as a prior and as a black box feature extractor. This leads to performance rivaling the state of the art on the SICK similarity and entailment tasks. We release all of our resources to the research community with the hope that they can serve as the new baseline for further work on universal sentence embeddings. version:3
arxiv-1601-00670 | Variational Inference: A Review for Statisticians | http://arxiv.org/abs/1601.00670 | id:1601.00670 author:David M. Blei, Alp Kucukelbir, Jon D. McAuliffe category:stat.CO cs.LG stat.ML  published:2016-01-04 summary:One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation about the posterior. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in myriad applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this widely-used class of algorithms. version:2
arxiv-1511-05122 | Adversarial Manipulation of Deep Representations | http://arxiv.org/abs/1511.05122 | id:1511.05122 author:Sara Sabour, Yanshuai Cao, Fartash Faghri, David J. Fleet category:cs.CV cs.LG cs.NE  published:2015-11-16 summary:We show that the representation of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels, while we concentrate on the internal layers of DNN representations. In this way our new class of adversarial images differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, one from a different class, bearing little if any apparent similarity to the input; they appear generic and consistent with the space of natural images. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves. version:9
arxiv-1603-01597 | Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning | http://arxiv.org/abs/1603.01597 | id:1603.01597 author:Mike Kestemont, Jeroen De Gussem category:cs.CL cs.LG stat.ML  published:2016-03-04 summary:In this paper we consider two sequence tagging tasks for medieval Latin: part-of-speech tagging and lemmatization. These are both basic, yet foundational preprocessing steps in applications such as text re-use detection. Nevertheless, they are generally complicated by the considerable orthographic variation which is typical of medieval Latin. In Digital Classics, these tasks are traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion. For example, a lexicon is used to generate all the potential lemma-tag pairs for a token, and next, a context-aware PoS-tagger is used to select the most appropriate tag-lemma pair. Apart from the problems with out-of-lexicon items, error percolation is a major downside of such approaches. In this paper we explore the possibility to elegantly solve these tasks using a single, integrated approach. For this, we make use of a layered neural network architecture from the field of deep representation learning. version:1
arxiv-1603-01595 | Sentiment Analysis in Scholarly Book Reviews | http://arxiv.org/abs/1603.01595 | id:1603.01595 author:Hussam Hamdan, Patrice Bellot, Frederic Bechet category:cs.CL cs.AI  published:2016-03-04 summary:So far different studies have tackled the sentiment analysis in several domains such as restaurant and movie reviews. But, this problem has not been studied in scholarly book reviews which is different in terms of review style and size. In this paper, we propose to combine different features in order to be presented to a supervised classifiers which extract the opinion target expressions and detect their polarities in scholarly book reviews. We construct a labeled corpus for training and evaluating our methods in French book reviews. We also evaluate them on English restaurant reviews in order to measure their robustness across the domains and languages. The evaluation shows that our methods are enough robust for English restaurant reviews and French book reviews. version:1
arxiv-1508-02479 | Normalized Hierarchical SVM | http://arxiv.org/abs/1508.02479 | id:1508.02479 author:Heejin Choi, Yutaka Sasaki, Nathan Srebro category:cs.LG  published:2015-08-11 summary:We present improved methods of using structured SVMs in a large-scale hierarchical classification problem, that is when labels are leaves, or sets of leaves, in a tree or a DAG. We examine the need to normalize both the regularization and the margin and show how doing so significantly improves performance, including allowing achieving state-of-the-art results where unnormalized structured SVMs do not perform better than flat models. We also describe a further extension of hierarchical SVMs that highlight the connection between hierarchical SVMs and matrix factorization models. version:2
arxiv-1603-01566 | X-rank and identifiability for a polynomial decomposition model | http://arxiv.org/abs/1603.01566 | id:1603.01566 author:Pierre Comon, Yang Qi, Konstantin Usevich category:cs.IT math.IT math.NA stat.ML  published:2016-03-04 summary:In this paper, we study a polynomial decomposition model that arises in problems of system identification, signal processing and machine learning. We show that this decomposition is a special case of the X-rank decomposition --- a powerful novel concept in algebraic geometry that generalizes the tensor CP decomposition. We prove new results on generic/maximal rank and on identifiability of the polynomial decomposition model. In the paper, we try to make results and basic tools accessible for a general audience (assuming no knowledge of algebraic geometry or its prerequisites). version:1
arxiv-1511-05552 | Recurrent Neural Networks Hardware Implementation on FPGA | http://arxiv.org/abs/1511.05552 | id:1511.05552 author:Andre Xian Ming Chang, Berin Martini, Eugenio Culurciello category:cs.NE  published:2015-11-17 summary:Recurrent Neural Networks (RNNs) have the ability to retain memory and learn data sequences. Due to the recurrent nature of RNNs, it is sometimes hard to parallelize all its computations on conventional hardware. CPUs do not currently offer large parallelism, while GPUs offer limited parallelism due to sequential components of RNN models. In this paper we present a hardware implementation of Long-Short Term Memory (LSTM) recurrent network on the programmable logic Zynq 7020 FPGA from Xilinx. We implemented a RNN with $2$ layers and $128$ hidden units in hardware and it has been tested using a character level language model. The implementation is more than $21\times$ faster than the ARM CPU embedded on the Zynq 7020 FPGA. This work can potentially evolve to a RNN co-processor for future mobile devices. version:4
arxiv-1603-01547 | Text Understanding with the Attention Sum Reader Network | http://arxiv.org/abs/1603.01547 | id:1603.01547 author:Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, Jan Kleindienst category:cs.CL  published:2016-03-04 summary:Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Our model outperforms models previously proposed for these tasks by a large margin. version:1
arxiv-1603-01541 | Parallel Texts in the Hebrew Bible, New Methods and Visualizations | http://arxiv.org/abs/1603.01541 | id:1603.01541 author:Martijn Naaijer, Dirk Roorda category:cs.CL  published:2016-03-04 summary:In this article we develop an algorithm to detect parallel texts in the Masoretic Text of the Hebrew Bible. The results are presented online and chapters in the Hebrew Bible containing parallel passages can be inspected synoptically. Differences between parallel passages are highlighted. In a similar way the MT of Isaiah is presented synoptically with 1QIsaa. We also investigate how one can investigate the degree of similarity between parallel passages with the help of a case study of 2 Kings 19-25 and its parallels in Isaiah, Jeremiah and 2 Chronicles. version:1
arxiv-1506-08316 | Keypoint Encoding for Improved Feature Extraction from Compressed Video at Low Bitrates | http://arxiv.org/abs/1506.08316 | id:1506.08316 author:Jianshu Chao, Eckehard Steinbach category:cs.MM cs.CV cs.IR  published:2015-06-27 summary:In many mobile visual analysis applications, compressed video is transmitted over a communication network and analyzed by a server. Typical processing steps performed at the server include keypoint detection, descriptor calculation, and feature matching. Video compression has been shown to have an adverse effect on feature-matching performance. The negative impact of compression can be reduced by using the keypoints extracted from the uncompressed video to calculate descriptors from the compressed video. Based on this observation, we propose to provide these keypoints to the server as side information and to extract only the descriptors from the compressed video. First, we introduce four different frame types for keypoint encoding to address different types of changes in video content. These frame types represent a new scene, the same scene, a slowly changing scene, or a rapidly moving scene and are determined by comparing features between successive video frames. Then, we propose Intra, Skip and Inter modes of encoding the keypoints for different frame types. For example, keypoints for new scenes are encoded using the Intra mode, and keypoints for unchanged scenes are skipped. As a result, the bitrate of the side information related to keypoint encoding is significantly reduced. Finally, we present pairwise matching and image retrieval experiments conducted to evaluate the performance of the proposed approach using the Stanford mobile augmented reality dataset and 720p format videos. The results show that the proposed approach offers significantly improved feature matching and image retrieval performance at a given bitrate. version:2
arxiv-1511-05065 | Proposal Flow | http://arxiv.org/abs/1511.05065 | id:1511.05065 author:Bumsub Ham, Minsu Cho, Cordelia Schmid, Jean Ponce category:cs.CV  published:2015-11-16 summary:Finding image correspondences remains a challenging problem in the presence of intra-class variations and large changes in scene layout, typical in scene flow computation. We introduce a novel approach to this problem, dubbed proposal flow, that establishes reliable correspondences using object proposals. Unlike prevailing scene flow approaches that operate on pixels or regularly sampled local regions, proposal flow benefits from the characteristics of modern object proposals, that exhibit high repeatability at multiple scales, and can take advantage of both local and geometric consistency constraints among proposals. We also show that proposal flow can effectively be transformed into a conventional dense flow field. We introduce a new dataset that can be used to evaluate both general scene flow techniques and region-based approaches such as proposal flow. We use this benchmark to compare different matching algorithms, object proposals, and region features within proposal flow with the state of the art in scene flow. This comparison, along with experiments on standard datasets, demonstrates that proposal flow significantly outperforms existing scene flow methods in various settings. version:2
arxiv-1603-01514 | A Bayesian Model of Multilingual Unsupervised Semantic Role Induction | http://arxiv.org/abs/1603.01514 | id:1603.01514 author:Nikhil Garg, James Henderson category:cs.CL  published:2016-03-04 summary:We propose a Bayesian model of unsupervised semantic role induction in multiple languages, and use it to explore the usefulness of parallel corpora for this task. Our joint Bayesian model consists of individual models for each language plus additional latent variables that capture alignments between roles across languages. Because it is a generative Bayesian model, we can do evaluations in a variety of scenarios just by varying the inference procedure, without changing the model, thereby comparing the scenarios directly. We compare using only monolingual data, using a parallel corpus, using a parallel corpus with annotations in the other language, and using small amounts of annotation in the target language. We find that the biggest impact of adding a parallel corpus to training is actually the increase in mono-lingual data, with the alignments to another language resulting in small improvements, even with labeled data for the other language. version:1
arxiv-1508-03929 | Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition | http://arxiv.org/abs/1508.03929 | id:1508.03929 author:Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh, Timothée Masquelier category:cs.CV q-bio.NC  published:2015-08-17 summary:Deep convolutional neural networks (DCNNs) have attracted much attention recently, and have shown to be able to recognize thousands of object categories in natural image databases. Their architecture is somewhat similar to that of the human visual system: both use restricted receptive fields, and a hierarchy of layers which progressively extract more and more abstracted features. Yet it is unknown whether DCNNs match human performance at the task of view-invariant object recognition, whether they make similar errors and use similar representations for this task, and whether the answers depend on the magnitude of the viewpoint variations. To investigate these issues, we benchmarked eight state-of-the-art DCNNs, the HMAX model, and a baseline shallow model and compared their results to those of humans with backward masking. Unlike in all previous DCNN studies, we carefully controlled the magnitude of the viewpoint variations to demonstrate that shallow nets can outperform deep nets and humans when variations are weak. When facing larger variations, however, more layers were needed to match human performance and error distributions, and to have representations that are consistent with human behavior. A very deep net with 18 layers even outperformed humans at the highest variation level, using the most human-like representations. version:3
arxiv-1509-02805 | Clustering by Hierarchical Nearest Neighbor Descent (H-NND) | http://arxiv.org/abs/1509.02805 | id:1509.02805 author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG stat.ME  published:2015-09-09 summary:Previously in 2014, we proposed the Nearest Descent (ND) method, capable of generating an efficient Graph, called the in-tree (IT). Due to some beautiful and effective features, this IT structure proves well suited for data clustering. Although there exist some redundant edges in IT, they usually have salient features and thus it is not hard to remove them. Subsequently, in order to prevent the seemingly redundant edges from occurring, we proposed the Nearest Neighbor Descent (NND) by adding the "Neighborhood" constraint on ND. Consequently, clusters automatically emerged, without the additional requirement of removing the redundant edges. However, NND proved still not perfect, since it brought in a new yet worse problem, the "over-partitioning" problem. Now, in this paper, we propose a method, called the Hierarchical Nearest Neighbor Descent (H-NND), which overcomes the over-partitioning problem of NND via using the hierarchical strategy. Specifically, H-NND uses ND to effectively merge the over-segmented sub-graphs or clusters that NND produces. Like ND, H-NND also generates the IT structure, in which the redundant edges once again appear. This seemingly comes back to the situation that ND faces. However, compared with ND, the redundant edges in the IT structure generated by H-NND generally become more salient, thus being much easier and more reliable to be identified even by the simplest edge-removing method which takes the edge length as the only measure. In other words, the IT structure constructed by H-NND becomes more fitted for data clustering. We prove this on several clustering datasets of varying shapes, dimensions and attributes. Besides, compared with ND, H-NND generally takes less computation time to construct the IT data structure for the input data. version:3
arxiv-1603-01489 | Performance Localisation | http://arxiv.org/abs/1603.01489 | id:1603.01489 author:Brendan Cody-Kenny, Stephen Barrett category:cs.SE cs.NE cs.PF  published:2016-03-04 summary:Profiling is a prominent technique for finding the location of performance "bottlenecks" in code. Profiling can be performed by adding code to a program which increments a counter for each line of code each time it is executed. Any lines of code which have a large execution count relative to other lines in the program can be considered a bottleneck. Though code profiling can determine the location of a performance issue or bottleneck, we posit that the code change required to improve performance may not always be found at the same location. Developers must frequently trace back through a program to understand what code is contributing to a bottleneck. We seek to highlight code which is likely causing or has the most effect on the overall execution cost of a program. In this document we compare different methods for localising potential performance improvements. version:1
arxiv-1504-06103 | Online Adaptive Hidden Markov Model for Multi-Tracker Fusion | http://arxiv.org/abs/1504.06103 | id:1504.06103 author:Tomas Vojir, Jiri Matas, Jana Noskova category:cs.CV  published:2015-04-23 summary:In this paper, we propose a novel method for visual object tracking called HMMTxD. The method fuses observations from complementary out-of-the box trackers and a detector by utilizing a hidden Markov model whose latent states correspond to a binary vector expressing the failure of individual trackers. The Markov model is trained in an unsupervised way, relying on an online learned detector to provide a source of tracker-independent information for a modified Baum- Welch algorithm that updates the model w.r.t. the partially annotated data. We show the effectiveness of the proposed method on combination of two and three tracking algorithms. The performance of HMMTxD is evaluated on two standard benchmarks (CVPR2013 and VOT) and on a rich collection of 77 publicly available sequences. The HMMTxD outperforms the state-of-the-art, often significantly, on all datasets in almost all criteria. version:2
arxiv-1511-06433 | Blending LSTMs into CNNs | http://arxiv.org/abs/1511.06433 | id:1511.06433 author:Krzysztof J. Geras, Abdel-rahman Mohamed, Rich Caruana, Gregor Urban, Shengjie Wang, Ozlem Aslan, Matthai Philipose, Matthew Richardson, Charles Sutton category:cs.LG  published:2015-11-19 summary:We consider whether deep convolutional networks (CNNs) can represent decision functions with similar accuracy as recurrent networks such as LSTMs. First, we show that a deep CNN with an architecture inspired by the models recently introduced in image recognition can yield better accuracy than previous convolutional and LSTM networks on the standard 309h Switchboard automatic speech recognition task. Then we show that even more accurate CNNs can be trained under the guidance of LSTMs using a variant of model compression, which we call model blending because the teacher and student models are similar in complexity but different in inductive bias. Blending further improves the accuracy of our CNN, yielding a computationally efficient model of accuracy higher than any of the other individual models. Examining the effect of "dark knowledge" in this model compression task, we find that less than 1% of the highest probability labels are needed for accurate model compression. version:2
arxiv-1511-05897 | Censoring Representations with an Adversary | http://arxiv.org/abs/1511.05897 | id:1511.05897 author:Harrison Edwards, Amos Storkey category:cs.LG cs.AI stat.ML  published:2015-11-18 summary:In practice, there are often explicit constraints on what representations or decisions are acceptable in an application of machine learning. For example it may be a legal requirement that a decision must not favour a particular group. Alternatively it can be that that representation of data must not have identifying information. We address these two related issues by learning flexible representations that minimize the capability of an adversarial critic. This adversary is trying to predict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the representation about the sensitive variable. We demonstrate this adversarial approach on two problems: making decisions free from discrimination and removing private information from images. We formulate the adversarial model as a minimax problem, and optimize that minimax objective using a stochastic gradient alternate min-max optimizer. We demonstrate the ability to provide discriminant free representations for standard test problems, and compare with previous state of the art methods for fairness, showing statistically significant improvement across most cases. The flexibility of this method is shown via a novel problem: removing annotations from images, from unaligned training examples of annotated and unannotated images, and with no a priori knowledge of the form of annotation provided to the model. version:3
arxiv-1602-06920 | Implicit LOD for processing, visualisation and classification in Point Cloud Servers | http://arxiv.org/abs/1602.06920 | id:1602.06920 author:Rémi Cura, Julien Perret, Nicolas Paparoditis category:cs.CG cs.CV cs.SE  published:2016-02-22 summary:We propose a new paradigm to effortlessly get a portable geometric Level Of Details (LOD) for a point cloud inside a Point Cloud Server. The point cloud is divided into groups of points (patch), then each patch is reordered (MidOc ordering) so that reading points following this order provides more and more details on the patch. This LOD have then multiple applications: point cloud size reduction for visualisation (point cloud streaming) or speeding of slow algorithm, fast density peak detection and correction as well as safeguard for methods that may be sensible to density variations. The LOD method also embeds information about the sensed object geometric nature, and thus can be used as a crude multi-scale dimensionality descriptor, enabling fast classification and on-the-fly filtering for basic classes. version:2
arxiv-1603-01032 | Right Ideals of a Ring and Sublanguages of Science | http://arxiv.org/abs/1603.01032 | id:1603.01032 author:Javier Arias Navarro category:cs.CL  published:2016-03-03 summary:Among Zellig Harris's numerous contributions to linguistics his theory of the sublanguages of science probably ranks among the most underrated. However, not only has this theory led to some exhaustive and meaningful applications in the study of the grammar of immunology language and its changes over time, but it also illustrates the nature of mathematical relations between chunks or subsets of a grammar and the language as a whole. This becomes most clear when dealing with the connection between metalanguage and language, as well as when reflecting on operators. This paper tries to justify the claim that the sublanguages of science stand in a particular algebraic relation to the rest of the language they are embedded in, namely, that of right ideals in a ring. version:2
arxiv-1603-01417 | Dynamic Memory Networks for Visual and Textual Question Answering | http://arxiv.org/abs/1603.01417 | id:1603.01417 author:Caiming Xiong, Stephen Merity, Richard Socher category:cs.NE cs.CL cs.CV  published:2016-03-04 summary:Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \babi-10k text question-answering dataset without supporting fact supervision. version:1
arxiv-1603-01376 | Lasso estimation for GEFCom2014 probabilistic electric load forecasting | http://arxiv.org/abs/1603.01376 | id:1603.01376 author:Florian Ziel, Bidong Liu category:stat.AP stat.ML G.3; I.5  published:2016-03-04 summary:We present a methodology for probabilistic load forecasting that is based on lasso (least absolute shrinkage and selection operator) estimation. The model considered can be regarded as a bivariate time-varying threshold autoregressive(AR) process for the hourly electric load and temperature. The joint modeling approach incorporates the temperature effects directly, and reflects daily, weekly, and annual seasonal patterns and public holiday effects. We provide two empirical studies, one based on the probabilistic load forecasting track of the Global Energy Forecasting Competition 2014 (GEFCom2014-L), and the other based on another recent probabilistic load forecasting competition that follows a setup similar to that of GEFCom2014-L. In both empirical case studies, the proposed methodology outperforms two multiple linear regression based benchmarks from among the top eight entries to GEFCom2014-L. version:1
arxiv-1603-01374 | A Unified View of Localized Kernel Learning | http://arxiv.org/abs/1603.01374 | id:1603.01374 author:John Moeller, Sarathkrishna Swaminathan, Suresh Venkatasubramanian category:cs.LG stat.ML  published:2016-03-04 summary:Multiple Kernel Learning, or MKL, extends (kernelized) SVM by attempting to learn not only a classifier/regressor but also the best kernel for the training task, usually from a combination of existing kernel functions. Most MKL methods seek the combined kernel that performs best over every training example, sacrificing performance in some areas to seek a global optimum. Localized kernel learning (LKL) overcomes this limitation by allowing the training algorithm to match a component kernel to the examples that can exploit it best. Several approaches to the localized kernel learning problem have been explored in the last several years. We unify many of these approaches under one simple system and design a new algorithm with improved performance. We also develop enhanced versions of existing algorithms, with an eye on scalability and performance. version:1
arxiv-1602-04915 | Gradient Descent Converges to Minimizers | http://arxiv.org/abs/1602.04915 | id:1602.04915 author:Jason D. Lee, Max Simchowitz, Michael I. Jordan, Benjamin Recht category:stat.ML cs.LG math.OC  published:2016-02-16 summary:We show that gradient descent converges to a local minimizer, almost surely with random initialization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory. version:2
arxiv-1603-01359 | Learning deep representation of multityped objects and tasks | http://arxiv.org/abs/1603.01359 | id:1603.01359 author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.CV cs.LG  published:2016-03-04 summary:We introduce a deep multitask architecture to integrate multityped representations of multimodal objects. This multitype exposition is less abstract than the multimodal characterization, but more machine-friendly, and thus is more precise to model. For example, an image can be described by multiple visual views, which can be in the forms of bag-of-words (counts) or color/texture histograms (real-valued). At the same time, the image may have several social tags, which are best described using a sparse binary vector. Our deep model takes as input multiple type-specific features, narrows the cross-modality semantic gaps, learns cross-type correlation, and produces a high-level homogeneous representation. At the same time, the model supports heterogeneously typed tasks. We demonstrate the capacity of the model on two applications: social image retrieval and multiple concept prediction. The deep architecture produces more compact representation, naturally integrates multiviews and multimodalities, exploits better side information, and most importantly, performs competitively against baselines. version:1
arxiv-1603-01333 | Joint Learning Templates and Slots for Event Schema Induction | http://arxiv.org/abs/1603.01333 | id:1603.01333 author:Lei Sha, Sujian Li, Baobao Chang, Zhifang Sui category:cs.CL  published:2016-03-04 summary:Automatic event schema induction (AESI) means to extract meta-event from raw text, in other words, to find out what types (templates) of event may exist in the raw text and what roles (slots) may exist in each event type. In this paper, we propose a joint entity-driven model to learn templates and slots simultaneously based on the constraints of templates and slots in the same sentence. In addition, the entities' semantic information is also considered for the inner connectivity of the entities. We borrow the normalized cut criteria in image segmentation to divide the entities into more accurate template clusters and slot clusters. The experiment shows that our model gains a relatively higher result than previous work. version:1
arxiv-1508-00451 | Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines | http://arxiv.org/abs/1508.00451 | id:1508.00451 author:Rein Houthooft, Filip De Turck category:stat.ML cs.CV cs.LG cs.NE  published:2015-08-03 summary:Tackling pattern recognition problems in areas such as computer vision, bioinformatics, speech or text recognition is often done best by taking into account task-specific statistical relations between output variables. In structured prediction, this internal structure is used to predict multiple outputs simultaneously, leading to more accurate and coherent predictions. Structural support vector machines (SSVMs) are nonprobabilistic models that optimize a joint input-output function through margin-based learning. Because SSVMs generally disregard the interplay between unary and interaction factors during the training phase, final parameters are suboptimal. Moreover, its factors are often restricted to linear combinations of input features, limiting its generalization power. To improve prediction accuracy, this paper proposes: (i) Joint inference and learning by integration of back-propagation and loss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM factors to neural networks that form highly nonlinear functions of input features. Image segmentation benchmark results demonstrate improvements over conventional SSVM training methods in terms of accuracy, highlighting the feasibility of end-to-end SSVM training with neural factors. version:4
arxiv-1603-01250 | Decision Forests, Convolutional Networks and the Models in-Between | http://arxiv.org/abs/1603.01250 | id:1603.01250 author:Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie Shotton, Matthew Brown, Antonio Criminisi category:cs.CV cs.AI  published:2016-03-03 summary:This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters. version:1
arxiv-1511-06396 | Multilingual Relation Extraction using Compositional Universal Schema | http://arxiv.org/abs/1511.06396 | id:1511.06396 author:Patrick Verga, David Belanger, Emma Strubell, Benjamin Roth, Andrew McCallum category:cs.CL cs.LG  published:2015-11-19 summary:Universal schema builds a knowledge base (KB) of entities and relations by jointly embedding all relation types from input KBs as well as textual patterns expressing relations from raw text. In most previous applications of universal schema, each textual pattern is represented as a single embedding, preventing generalization to unseen patterns. Recent work employs a neural network to capture patterns' compositional semantics, providing generalization to all possible input text. In response, this paper introduces significant further improvements to the coverage and flexibility of universal schema relation extraction: predictions for entities unseen in training and multilingual transfer learning to domains with no annotation. We evaluate our model through extensive experiments on the English and Spanish TAC KBP benchmark, outperforming the top system from TAC 2013 slot-filling using no handwritten patterns or additional annotation. We also consider a multilingual setting in which English training data entities overlap with the seed KB, but Spanish text does not. Despite having no annotation for Spanish data, we train an accurate predictor, with additional improvements obtained by tying word embeddings across languages. Furthermore, we find that multilingual training improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in a variety of languages and domains. version:2
arxiv-1602-02191 | Convex Relaxation Regression: Black-Box Optimization of Smooth Functions by Learning Their Convex Envelopes | http://arxiv.org/abs/1602.02191 | id:1602.02191 author:Mohammad Gheshlaghi Azar, Eva Dyer, Konrad Kording category:stat.ML cs.LG  published:2016-02-05 summary:Finding efficient and provable methods to solve non-convex optimization problems is an outstanding challenge in machine learning and optimization theory. A popular approach used to tackle non-convex problems is to use convex relaxation techniques to find a convex surrogate for the problem. Unfortunately, convex relaxations typically must be found on a problem-by-problem basis. Thus, providing a general-purpose strategy to estimate a convex relaxation would have a wide reaching impact. Here, we introduce Convex Relaxation Regression (CoRR), an approach for learning convex relaxations for a class of smooth functions. The main idea behind our approach is to estimate the convex envelope of a function $f$ by evaluating $f$ at a set of $T$ random points and then fitting a convex function to these function evaluations. We prove that with probability greater than $1-\delta$, the solution of our algorithm converges to the global optimizer of $f$ with error $\mathcal{O} \Big( \big(\frac{\log(1/\delta) }{T} \big)^{\alpha} \Big)$ for some $\alpha> 0$. Our approach enables the use of convex optimization tools to solve a class of non-convex optimization problems. version:3
arxiv-1603-01232 | Multi-domain Neural Network Language Generation for Spoken Dialogue Systems | http://arxiv.org/abs/1603.01232 | id:1603.01232 author:Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, David Vandyke, Steve Young category:cs.CL  published:2016-03-03 summary:Moving from limited-domain natural language generation (NLG) to open domain is difficult because the number of semantic input combinations grows exponentially with the number of domains. Therefore, it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation. In this paper, we propose a procedure to train multi-domain, Recurrent Neural Network-based (RNN) language generators via multiple adaptation steps. In this procedure, a model is first trained on counterfeited data synthesised from an out-of-domain dataset, and then fine tuned on a small set of in-domain utterances with a discriminative objective function. Corpus-based evaluation results show that the proposed procedure can achieve competitive performance in terms of BLEU score and slot error rate while significantly reducing the data needed to train generators in new, unseen domains. In subjective testing, human judges confirm that the procedure greatly improves generator performance when only a small amount of data is available in the domain. version:1
arxiv-1509-04648 | Comparative Design Space Exploration of Dense and Semi-Dense SLAM | http://arxiv.org/abs/1509.04648 | id:1509.04648 author:M. Zeeshan Zia, Luigi Nardi, Andrew Jack, Emanuele Vespa, Bruno Bodin, Paul H. J. Kelly, Andrew J. Davison category:cs.RO cs.CV  published:2015-09-15 summary:SLAM has matured significantly over the past few years, and is beginning to appear in serious commercial products. While new SLAM systems are being proposed at every conference, evaluation is often restricted to qualitative visualizations or accuracy estimation against a ground truth. This is due to the lack of benchmarking methodologies which can holistically and quantitatively evaluate these systems. Further investigation at the level of individual kernels and parameter spaces of SLAM pipelines is non-existent, which is absolutely essential for systems research and integration. We extend the recently introduced SLAMBench framework to allow comparing two state-of-the-art SLAM pipelines, namely KinectFusion and LSD-SLAM, along the metrics of accuracy, energy consumption, and processing frame rate on two different hardware platforms, namely a desktop and an embedded device. We also analyze the pipelines at the level of individual kernels and explore their algorithmic and hardware design spaces for the first time, yielding valuable insights. version:3
arxiv-1603-02695 | Rank Aggregation for Course Sequence Discovery | http://arxiv.org/abs/1603.02695 | id:1603.02695 author:Mihai Cucuringu, Charlie Marshak, Dillon Montag, Puck Rombach category:cs.LG  published:2016-03-03 summary:In this work, we adapt the rank aggregation framework for the discovery of optimal course sequences at the university level. Each student provides a partial ranking of the courses taken throughout his or her undergraduate career. We compute pairwise rank comparisons between courses based on the order students typically take them, aggregate the results over the entire student population, and then obtain a proxy for the rank offset between pairs of courses. We extract a global ranking of the courses via several state-of-the art algorithms for ranking with pairwise noisy information, including SerialRank, Rank Centrality, and the recent SyncRank based on the group synchronization problem. We test this application of rank aggregation on 15 years of student data from the Department of Mathematics at the University of California, Los Angeles (UCLA). Furthermore, we experiment with the above approach on different subsets of the student population conditioned on final GPA, and highlight several differences in the obtained rankings that uncover hidden pre-requisites in the Mathematics curriculum. version:1
arxiv-1603-01140 | Overdispersed Black-Box Variational Inference | http://arxiv.org/abs/1603.01140 | id:1603.01140 author:Francisco J. R. Ruiz, Michalis K. Titsias, David M. Blei category:stat.ML  published:2016-03-03 summary:We introduce overdispersed black-box variational inference, a method to reduce the variance of the Monte Carlo estimator of the gradient in black-box variational inference. Instead of taking samples from the variational distribution, we use importance sampling to take samples from an overdispersed distribution in the same exponential family as the variational approximation. Our approach is general since it can be readily applied to any exponential family distribution, which is the typical choice for the variational approximation. We run experiments on two non-conjugate probabilistic models to show that our method effectively reduces the variance, and the overhead introduced by the computation of the proposal parameters and the importance weights is negligible. We find that our overdispersed importance sampling scheme provides lower variance than black-box variational inference, even when the latter uses twice the number of samples. This results in faster convergence of the black-box inference procedure. version:1
arxiv-1603-01121 | Deep Reinforcement Learning from Self-Play in Imperfect-Information Games | http://arxiv.org/abs/1603.01121 | id:1603.01121 author:Johannes Heinrich, David Silver category:cs.LG cs.AI cs.GT  published:2016-03-03 summary:Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without any prior knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a competitive strategy that approached the performance of human experts and state-of-the-art methods. version:1
arxiv-1603-01096 | Elastic Net Hypergraph Learning for Image Clustering and Semi-supervised Classification | http://arxiv.org/abs/1603.01096 | id:1603.01096 author:Qingshan Liu, Yubao Sun, Cantian Wang, Tongliang Liu, Dacheng Tao category:cs.CV  published:2016-03-03 summary:Graph model is emerging as a very effective tool for learning the complex structures and relationships hidden in data. Generally, the critical purpose of graph-oriented learning algorithms is to construct an informative graph for image clustering and classification tasks. In addition to the classical $K$-nearest-neighbor and $r$-neighborhood methods for graph construction, $l_1$-graph and its variants are emerging methods for finding the neighboring samples of a center datum, where the corresponding ingoing edge weights are simultaneously derived by the sparse reconstruction coefficients of the remaining samples. However, the pair-wise links of $l_1$-graph are not capable of capturing the high order relationships between the center datum and its prominent data in sparse reconstruction. Meanwhile, from the perspective of variable selection, the $l_1$ norm sparse constraint, regarded as a LASSO model, tends to select only one datum from a group of data that are highly correlated and ignore the others. To simultaneously cope with these drawbacks, we propose a new elastic net hypergraph learning model, which consists of two steps. In the first step, the Robust Matrix Elastic Net model is constructed to find the canonically related samples in a somewhat greedy way, achieving the grouping effect by adding the $l_2$ penalty to the $l_1$ constraint. In the second step, hypergraph is used to represent the high order relationships between each datum and its prominent samples by regarding them as a hyperedge. Subsequently, hypergraph Laplacian matrix is constructed for further analysis. New hypergraph learning algorithms, including unsupervised clustering and multi-class semi-supervised classification, are then derived. Extensive experiments on face and handwriting databases demonstrate the effectiveness of the proposed method. version:1
arxiv-1512-04280 | Small-footprint Deep Neural Networks with Highway Connections for Speech Recognition | http://arxiv.org/abs/1512.04280 | id:1512.04280 author:Liang Lu, Steve Renals category:cs.CL cs.LG cs.NE  published:2015-12-14 summary:For speech recognition, deep neural networks (DNNs) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models, DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g., mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are {\it thinner} and {\it deeper}, and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 70 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy. version:2
arxiv-1603-01068 | Camera identification with deep convolutional networks | http://arxiv.org/abs/1603.01068 | id:1603.01068 author:Luca Baroffio, Luca Bondi, Paolo Bestagini, Stefano Tubaro category:cs.CV cs.MM  published:2016-03-03 summary:The possibility of detecting which camera has been used to shoot a specific picture is of paramount importance for many forensics tasks. This is extremely useful for copyright infringement cases, ownership attribution, as well as for detecting the authors of distributed illicit material (e.g., pedo-pornographic shots). Due to its importance, the forensics community has developed a series of robust detectors that exploit characteristic traces left by each camera on the acquired images during the acquisition pipeline. These traces are reverse-engineered in order to attribute a picture to a camera. In this paper, we investigate an alternative approach to solve camera identification problem. Indeed, we propose a data-driven algorithm based on convolutional neural networks, which learns features characterizing each camera directly from the acquired pictures. The proposed approach is tested on both instance-attribution and model-attribution, providing an accuracy greater than 94% in discriminating 27 camera models. version:1
arxiv-1603-01067 | Modeling the Sequence of Brain Volumes by Local Mesh Models for Brain Decoding | http://arxiv.org/abs/1603.01067 | id:1603.01067 author:Itir Onal, Mete Ozay, Eda Mizrak, Ilke Oztekin, Fatos T. Yarman Vural category:cs.LG cs.AI cs.CV  published:2016-03-03 summary:We represent the sequence of fMRI (Functional Magnetic Resonance Imaging) brain volumes recorded during a cognitive stimulus by a graph which consists of a set of local meshes. The corresponding cognitive process, encoded in the brain, is then represented by these meshes each of which is estimated assuming a linear relationship among the voxel time series in a predefined locality. First, we define the concept of locality in two neighborhood systems, namely, the spatial and functional neighborhoods. Then, we construct spatially and functionally local meshes around each voxel, called seed voxel, by connecting it either to its spatial or functional p-nearest neighbors. The mesh formed around a voxel is a directed sub-graph with a star topology, where the direction of the edges is taken towards the seed voxel at the center of the mesh. We represent the time series recorded at each seed voxel in terms of linear combination of the time series of its p-nearest neighbors in the mesh. The relationships between a seed voxel and its neighbors are represented by the edge weights of each mesh, and are estimated by solving a linear regression equation. The estimated mesh edge weights lead to a better representation of information in the brain for encoding and decoding of the cognitive tasks. We test our model on a visual object recognition and emotional memory retrieval experiments using Support Vector Machines that are trained using the mesh edge weights as features. In the experimental analysis, we observe that the edge weights of the spatial and functional meshes perform better than the state-of-the-art brain decoding models. version:1
arxiv-1511-06219 | Knowledge Base Population using Semantic Label Propagation | http://arxiv.org/abs/1511.06219 | id:1511.06219 author:Lucas Sterckx, Thomas Demeester, Johannes Deleu, Chris Develder category:cs.CL cs.LG  published:2015-11-19 summary:A crucial aspect of a knowledge base population system that extracts new facts from text corpora, is the generation of training data for its relation extractors. In this paper, we present a method that maximizes the effectiveness of newly trained relation extractors at a minimal annotation cost. Manual labeling can be significantly reduced by Distant Supervision, which is a method to construct training data automatically by aligning a large text corpus with an existing knowledge base of known facts. For example, all sentences mentioning both 'Barack Obama' and 'US' may serve as positive training instances for the relation born_in(subject,object). However, distant supervision typically results in a highly noisy training set: many training sentences do not really express the intended relation. We propose to combine distant supervision with minimal manual supervision in a technique called feature labeling, to eliminate noise from the large and noisy initial training set, resulting in a significant increase of precision. We further improve on this approach by introducing the Semantic Label Propagation method, which uses the similarity between low-dimensional representations of candidate training instances, to extend the training set in order to increase recall while maintaining high precision. Our proposed strategy for generating training data is studied and evaluated on an established test collection designed for knowledge base population tasks. The experimental results show that the Semantic Label Propagation strategy leads to substantial performance gains when compared to existing approaches, while requiring an almost negligible manual annotation effort. version:2
arxiv-1603-01056 | A novel and automatic pectoral muscle identification algorithm for mediolateral oblique (MLO) view mammograms using ImageJ | http://arxiv.org/abs/1603.01056 | id:1603.01056 author:Chao Wang category:cs.CV  published:2016-03-03 summary:Pectoral muscle identification is often required for breast cancer risk analysis, such as estimating breast density. Traditional methods are overwhelmingly based on manual visual assessment or straight line fitting for the pectoral muscle boundary, which are inefficient and inaccurate since pectoral muscle in mammograms can have curved boundaries. This paper proposes a novel and automatic pectoral muscle identification algorithm for MLO view mammograms. It is suitable for both scanned film and full field digital mammograms. This algorithm is demonstrated using a public domain software ImageJ. A validation of this algorithm has been performed using real-world data and it shows promising result. version:1
arxiv-1603-01046 | Photographic dataset: random peppercorns | http://arxiv.org/abs/1603.01046 | id:1603.01046 author:Teemu Helenius, Samuli Siltanen category:physics.data-an cs.CV  published:2016-03-03 summary:This is a photographic dataset collected for testing image processing algorithms. The idea is to have sets of different but statistically similar images. In this work the images show randomly distributed peppercorns. The dataset is made available at www.fips.fi/photographic_dataset.php . version:1
arxiv-1603-01029 | Whitening-Free Least-Squares Non-Gaussian Component Analysis | http://arxiv.org/abs/1603.01029 | id:1603.01029 author:Hiroaki Shiino, Hiroaki Sasaki, Gang Niu, Masashi Sugiyama category:stat.ML  published:2016-03-03 summary:Non-Gaussian component analysis (NGCA) is an unsupervised linear dimension reduction method that extracts low-dimensional non-Gaussian "signals" from high-dimensional data contaminated with Gaussian noise. NGCA can be regarded as a generalization of projection pursuit (PP) and independent component analysis (ICA) to multi-dimensional and dependent non-Gaussian components. Indeed, seminal approaches to NGCA are based on PP and ICA. Recently, a novel NGCA approach called least-squares NGCA (LSNGCA) has been developed, which gives a solution analytically through least-squares estimation of log-density gradients and eigendecomposition. However, since pre-whitening of data is involved in LSNGCA, it performs unreliably when the data covariance matrix is ill-conditioned, which is often the case in high-dimensional data analysis. In this paper, we propose a whitening-free LSNGCA method and experimentally demonstrate its superiority. version:1
arxiv-1603-00993 | Self-localization from Images with Small Overlap | http://arxiv.org/abs/1603.00993 | id:1603.00993 author:Tanaka Kanji category:cs.CV  published:2016-03-03 summary:With the recent success of visual features from deep convolutional neural networks (DCNN) in visual robot self-localization, it has become important and practical to address more general self-localization scenarios. In this paper, we address the scenario of self-localization from images with small overlap. We explicitly introduce a localization difficulty index as a decreasing function of view overlap between query and relevant database images and investigate performance versus difficulty for challenging cross-view self-localization tasks. We then reformulate the self-localization as a scalable bag-of-visual-features (BoVF) scene retrieval and present an efficient solution called PCA-NBNN, aiming to facilitate fast and yet discriminative correspondence between partially overlapping images. The proposed approach adopts recent findings in discriminativity preserving encoding of DCNN features using principal component analysis (PCA) and cross-domain scene matching using naive Bayes nearest neighbor distance metric (NBNN). We experimentally demonstrate that the proposed PCA-NBNN framework frequently achieves comparable results to previous DCNN features and that the BoVF model is significantly more efficient. We further address an important alternative scenario of "self-localization from images with NO overlap" and report the result. version:1
arxiv-1411-2158 | Covariate-assisted spectral clustering | http://arxiv.org/abs/1411.2158 | id:1411.2158 author:Norbert Binkiewicz, Joshua T. Vogelstein, Karl Rohe category:stat.ML cs.LG math.ST stat.ME stat.TH  published:2014-11-08 summary:Biological and social systems consist of myriad interacting units. The interactions can be represented in the form of a graph or network. Measurements of these graphs can reveal the underlying structure of these interactions, which provides insight into the systems that generated the graphs. Moreover, in applications such as connectomics, social networks, and genomics, graph data are accompanied by contextualizing measures on each node. We utilize these node covariates to help uncover latent communities in a graph, using a modification of spectral clustering. Statistical guarantees are provided under a joint mixture model that we call the node-contextualized stochastic blockmodel, including a bound on the mis-clustering rate. For most simulated conditions, covariate-assisted spectral clustering yields results superior to regularized spectral clustering without node covariates and to an adaptation of canonical correlation analysis. We apply our clustering method to large brain graphs derived from diffusion MRI data, using the node locations or neurological region membership as covariates. In both cases, covariate-assisted spectral clustering yields clusters that are easier to interpret neurologically. version:4
arxiv-1405-5576 | Sparse Precision Matrix Selection for Fitting Gaussian Random Field Models to Large Data Sets | http://arxiv.org/abs/1405.5576 | id:1405.5576 author:Sam Davanloo Tajbakhsh, Necdet Serhat Aybat, Enrique Del Castillo category:stat.ML stat.CO  published:2014-05-21 summary:Iterative methods for fitting a Gaussian Random Field (GRF) model to spatial data via maximum likelihood (ML) require $\mathcal{O}(n^3)$ floating point operations per iteration, where $n$ denotes the number of data locations. For large data sets, the $\mathcal{O}(n^3)$ complexity per iteration together with the non-convexity of the ML problem render traditional ML methods inefficient for GRF fitting. The problem is even more aggravated for anisotropic GRFs where the number of covariance function parameters increases with the process domain dimension. In this paper, we propose a new two-step GRF estimation procedure when the process is second-order stationary. First, a \emph{convex} likelihood problem regularized with a weighted $\ell_1$-norm, utilizing the available distance information between observation locations, is solved to fit a sparse \emph{{precision} (inverse covariance) matrix to the observed data using the Alternating Direction Method of Multipliers. Second, the parameters of the GRF spatial covariance function are estimated by solving a least squares problem. Theoretical error bounds for the proposed estimator are provided; moreover, convergence of the estimator is shown as the number of samples per location increases. The proposed method is numerically compared with state-of-the-art methods for big $n$. Data segmentation schemes are implemented to handle large data sets. version:4
arxiv-1603-00961 | Interactive and Scale Invariant Segmentation of the Rectum/Sigmoid via User-Defined Templates | http://arxiv.org/abs/1603.00961 | id:1603.00961 author:Tobias Lüddemann, Jan Egger category:cs.CV cs.GR  published:2016-03-03 summary:Among all types of cancer, gynecological malignancies belong to the 4th most frequent type of cancer among women. Besides chemotherapy and external beam radiation, brachytherapy is the standard procedure for the treatment of these malignancies. In the progress of treatment planning, localization of the tumor as the target volume and adjacent organs of risks by segmentation is crucial to accomplish an optimal radiation distribution to the tumor while simultaneously preserving healthy tissue. Segmentation is performed manually and represents a time-consuming task in clinical daily routine. This study focuses on the segmentation of the rectum/sigmoid colon as an Organ-At-Risk in gynecological brachytherapy. The proposed segmentation method uses an interactive, graph-based segmentation scheme with a user-defined template. The scheme creates a directed two dimensional graph, followed by the minimal cost closed set computation on the graph, resulting in an outlining of the rectum. The graphs outline is dynamically adapted to the last calculated cut. Evaluation was performed by comparing manual segmentations of the rectum/sigmoid colon to results achieved with the proposed method. The comparison of the algorithmic to manual results yielded to a Dice Similarity Coefficient value of 83.85+/-4.08%, in comparison to 83.97+/-8.08% for the comparison of two manual segmentations of the same physician. Utilizing the proposed methodology resulted in a median time of 128 seconds per dataset, compared to 300 seconds needed for pure manual segmentation. version:1
arxiv-1603-00960 | Cellular Automata Segmentation of the Boundary between the Compacta of Vertebral Bodies and Surrounding Structures | http://arxiv.org/abs/1603.00960 | id:1603.00960 author:Jan Egger, Christopher Nimsky category:cs.CV cs.CG cs.GR  published:2016-03-03 summary:Due to the aging population, spinal diseases get more and more common nowadays; e.g., lifetime risk of osteoporotic fracture is 40% for white women and 13% for white men in the United States. Thus the numbers of surgical spinal procedures are also increasing with the aging population and precise diagnosis plays a vital role in reducing complication and recurrence of symptoms. Spinal imaging of vertebral column is a tedious process subjected to interpretation errors. In this contribution, we aim to reduce time and error for vertebral interpretation by applying and studying the GrowCut-algorithm for boundary segmentation between vertebral body compacta and surrounding structures. GrowCut is a competitive region growing algorithm using cellular automata. For our study, vertebral T2-weighted Magnetic Resonance Imaging (MRI) scans were first manually outlined by neurosurgeons. Then, the vertebral bodies were segmented in the medical images by a GrowCut-trained physician using the semi-automated GrowCut-algorithm. Afterwards, results of both segmentation processes were compared using the Dice Similarity Coefficient (DSC) and the Hausdorff Distance (HD) which yielded to a DSC of 82.99+/-5.03% and a HD of 18.91+/-7.2 voxel, respectively. In addition, the times have been measured during the manual and the GrowCut segmentations, showing that a GrowCut-segmentation - with an average time of less than six minutes (5.77+/-0.73) - is significantly shorter than a pure manual outlining. version:1
arxiv-1603-00952 | Sparse model selection in the highly under-sampled regime | http://arxiv.org/abs/1603.00952 | id:1603.00952 author:Nicola Bulso, Matteo Marsili, Yasser Roudi category:stat.ML cond-mat.dis-nn  published:2016-03-03 summary:We propose a method for recovering the structure of a sparse undirected graphical model when very few samples are available. The method decides about the presence or absence of bonds between pairs of variable by considering one pair at a time and using a closed form formula, analytically derived by calculating the posterior probability for every possible model explaining a two body system using Jeffreys prior. The approach does not rely on the optimisation of any cost functions and consequently is much faster than existing algorithms. Despite this time and computational advantage, numerical results show that for several sparse topologies the algorithm is comparable to the best existing algorithms, and is more accurate in the presence of hidden variables. We apply this approach to the analysis of US stock market data and to neural data, in order to show its efficiency in recovering robust statistical dependencies in real data with non stationary correlations in time and space. version:1
arxiv-1511-07428 | Estimating the number of unseen species: A bird in the hand is worth $\log n $ in the bush | http://arxiv.org/abs/1511.07428 | id:1511.07428 author:Alon Orlitsky, Ananda Theertha Suresh, Yihong Wu category:math.ST stat.ML stat.TH  published:2015-11-23 summary:Estimating the number of unseen species is an important problem in many scientific endeavors. Its most popular formulation, introduced by Fisher, uses $n$ samples to predict the number $U$ of hitherto unseen species that would be observed if $t\cdot n$ new samples were collected. Of considerable interest is the largest ratio $t$ between the number of new and existing samples for which $U$ can be accurately predicted. In seminal works, Good and Toulmin constructed an intriguing estimator that predicts $U$ for all $t\le 1$, thereby showing that the number of species can be estimated for a population twice as large as that observed. Subsequently Efron and Thisted obtained a modified estimator that empirically predicts $U$ even for some $t>1$, but without provable guarantees. We derive a class of estimators that $\textit{provably}$ predict $U$ not just for constant $t>1$, but all the way up to $t$ proportional to $\log n$. This shows that the number of species can be estimated for a population $\log n$ times larger than that observed, a factor that grows arbitrarily large as $n$ increases. We also show that this range is the best possible and that the estimators' mean-square error is optimal up to constants for any $t$. Our approach yields the first provable guarantee for the Efron-Thisted estimator and, in addition, a variant which achieves stronger theoretical and experimental performance than existing methodologies on a variety of synthetic and real datasets. The estimators we derive are simple linear estimators that are computable in time proportional to $n$. The performance guarantees hold uniformly for all distributions, and apply to all four standard sampling models commonly used across various scientific disciplines: multinomial, Poisson, hypergeometric, and Bernoulli product. version:3
arxiv-1602-05931 | RandomOut: Using a convolutional gradient norm to win The Filter Lottery | http://arxiv.org/abs/1602.05931 | id:1602.05931 author:Joseph Paul Cohen, Henry Z. Lo, Wei Ding category:cs.CV  published:2016-02-18 summary:Convolutional neural networks are sensitive to the random initialization of filters. We call this The Filter Lottery (TFL) because the random numbers used to initialize the network determine if you will "win" and converge to a satisfactory local minimum. This issue forces networks to contain more filters (be wider) to achieve higher accuracy because they have better odds of being transformed into highly discriminative features at the risk of introducing redundant features. To deal with this, we propose to evaluate and replace specific convolutional filters that have little impact on the prediction. We use the gradient norm to evaluate the impact of a filter on error, and re-initialize filters when the gradient norm of its weights falls below a specific threshold. This consistently improves accuracy across two datasets by up to 1.8%. Our scheme RandomOut allows us to increase the number of filters explored without increasing the size of the network. This yields more compact networks which can train and predict with less computation, thus allowing more powerful CNNs to run on mobile devices. version:2
