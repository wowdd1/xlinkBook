arxiv-1611-05009 | OctNet: Learning Deep 3D Representations at High Resolutions | http://arxiv.org/abs/1611.05009 | id:1611.05009 author:Gernot Riegler, Ali Osman Ulusoy, Andreas Geiger category:cs.CV  published:2016-11-15 summary:We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling. version:2
arxiv-1611-05113 | Efficient Diffusion on Region Manifolds: Recovering Small Objects with Compact CNN Representations | http://arxiv.org/abs/1611.05113 | id:1611.05113 author:Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, Ondrej Chum category:cs.CV  published:2016-11-16 summary:Query expansion is a popular method to improve the quality of image retrieval with both conventional and CNN representations. It has been so far limited to global image similarity. This work focuses on diffusion, a mechanism that captures the image manifold in the feature space. The diffusion is carried out on descriptors of overlapping image regions rather than on a global image descriptor like in previous approaches. An efficient off-line stage allows optional reduction in the number of stored regions. In the on-line stage, the proposed handling of unseen queries in the indexing stage removes additional computation to adjust the precomputed data. A novel way to perform diffusion through a sparse linear system solver yields practical query times well below one second. Experimentally, we observe a significant boost in performance of image retrieval with compact CNN descriptors on standard benchmarks, especially when the query object covers only a small part of the image. Small objects have been a common failure case of CNN-based retrieval. version:1
arxiv-1611-05109 | Low-rank Bilinear Pooling for Fine-Grained Classification | http://arxiv.org/abs/1611.05109 | id:1611.05109 author:Shu Kong, Charless Fowlkes category:cs.CV  published:2016-11-16 summary:Pooling second-order local feature statistics to form a high-dimensional bilinear feature has been shown to achieve state-of-the-art performance on a variety of fine-grained classification tasks. To address the computational demands of high feature dimensionality, we propose to represent the covariance features as a matrix and apply a low-rank bilinear classifier. The resulting classifier can be evaluated without explicitly computing the bilinear feature map which allows for a large reduction in the compute time as well as decreasing the effective number of parameters to be learned. To further compress the model, we propose classifier co-decomposition that factorizes the collection of bilinear classifiers into a common factor and compact per-class terms. The co-decomposition idea can be deployed through two convolutional layers and trained in an end-to-end architecture. We suggest a simple yet effective initialization that avoids explicitly first training and factorizing the larger bilinear classifiers. Through extensive experiments, we show that our model achieves state-of-the-art performance on several public datasets for fine-grained classification trained with only category labels. Importantly, our final model is an order of magnitude smaller than the recently proposed compact bilinear model, and three orders smaller than the standard bilinear CNN model. version:1
arxiv-1611-05104 | A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs | http://arxiv.org/abs/1611.05104 | id:1611.05104 author:Shayne Longpre, Sabeek Pradhan, Caiming Xiong, Richard Socher category:cs.CL cs.AI  published:2016-11-16 summary:LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model. version:1
arxiv-1611-05095 | Learning Dexterous Manipulation Policies from Experience and Imitation | http://arxiv.org/abs/1611.05095 | id:1611.05095 author:Vikash Kumar, Abhishek Gupta, Emanuel Todorov, Sergey Levine category:cs.LG cs.RO cs.SY  published:2016-11-15 summary:We explore learning-based approaches for feedback control of a dexterous five-finger hand performing non-prehensile manipulation. First, we learn local controllers that are able to perform the task starting at a predefined initial state. These controllers are constructed using trajectory optimization with respect to locally-linear time-varying models learned directly from sensor data. In some cases, we initialize the optimizer with human demonstrations collected via teleoperation in a virtual environment. We demonstrate that such controllers can perform the task robustly, both in simulation and on the physical platform, for a limited range of initial conditions around the trained starting state. We then consider two interpolation methods for generalizing to a wider range of initial conditions: deep learning, and nearest neighbors. We find that nearest neighbors achieve higher performance. Nevertheless, the neural network has its advantages: it uses only tactile and proprioceptive feedback but no visual feedback about the object (i.e. it performs the task blind) and learns a time-invariant policy. In contrast, the nearest neighbors method switches between time-varying local controllers based on the proximity of initial object states sensed via motion capture. While both generalization methods leave room for improvement, our work shows that (i) local trajectory-based controllers for complex non-prehensile manipulation tasks can be constructed from surprisingly small amounts of training data, and (ii) collections of such controllers can be interpolated to form more global controllers. Results are summarized in the supplementary video: https://youtu.be/E0wmO6deqjo version:1
arxiv-1611-05088 | Learning a Deep Embedding Model for Zero-Shot Learning | http://arxiv.org/abs/1611.05088 | id:1611.05088 author:Li Zhang, Tao Xiang, Shaogang Gong category:cs.CV  published:2016-11-15 summary:Zero-shot learning (ZSL) models rely on learning a joint embedding space where both textual/semantic description of object classes and visual representation of object images can be projected to for nearest neighbour search. Despite the success of deep neural networks that learn an end-to-end model between text and images in other vision problems such as image captioning, very few deep ZSL model exists and they show little advantage over ZSL models that utilise deep feature representations but do not learn an end-to-end embedding. In this paper we argue that the key to make deep ZSL models succeed is to choose the right embedding space. Instead of embedding into a semantic space or an intermediate space, we propose to use the visual space as the embedding space. This is because that in this space, the subsequent nearest neighbour search would suffer much less from the hubness problem and thus become more effective. This model design also provides a natural mechanism for multiple semantic modalities (e.g., attributes and sentence descriptions) to be fused and optimised jointly in an end-to-end manner. Extensive experiments on four benchmarks show that our model significantly outperforms the existing models. version:1
arxiv-1611-05083 | Probabilistic Failure Analysis in Model Validation & Verification | http://arxiv.org/abs/1611.05083 | id:1611.05083 author:Ning Ge, Marc Panten, Xavier Cr√©gut category:cs.SE cs.LG  published:2016-11-15 summary:Automated fault localization is an important issue in model validation and verification. It helps the end users in analyzing the origin of failure. In this work, we show the early experiments with probabilistic analysis approaches in fault localization. Inspired by the Kullback-Leibler Divergence from Bayesian probabilistic theory, we propose a suspiciousness factor to compute the fault contribution for the transitions in the reachability graph of model checking, using which to rank the potential faulty transitions. To automatically locate design faults in the simulation model of detailed design, we propose to use the statistical model Hidden Markov Model (HMM), which provides statistically identical information to component's real behavior. The core of this method is a fault localization algorithm that gives out the set of suspicious ranked faulty components and a backward algorithm that computes the matching degree between the HMM and the simulation model to evaluate the confidence degree of the localization conclusion. version:1
arxiv-1611-05053 | Learning Detailed Face Reconstruction from a Single Image | http://arxiv.org/abs/1611.05053 | id:1611.05053 author:Elad Richardson, Matan Sela, Roy Or-El, Ron Kimmel category:cs.CV  published:2016-11-15 summary:Reconstructing the detailed geometric structure of a face from a given image is a key to many computer vision and graphics applications, such as motion capture and reenactment. The reconstruction task is challenging as human faces vary extensively when considering expressions, poses, textures, and intrinsic geometry. While many approaches tackle this complexity by using additional data to reconstruct the face of a single subject, extracting facial surface from a single image remains a difficult problem. As a result, single-image based methods can usually provide only a rough estimate of the facial geometry. In contrast, we propose to leverage the power of convolutional neural networks to produce a highly detailed face reconstruction from a single image. For this purpose, we introduce an end-to-end CNN framework which derives the shape in a coarse-to-fine fashion. The proposed architecture is composed of two main blocks, a network that recovers the coarse facial geometry (CoarseNet), followed by a CNN that refines the facial features of that geometry (FineNet). The proposed networks are connected by a novel layer which renders a depth image given a mesh in 3D. Unlike object recognition and detection problems, there are no suitable datasets for training CNNs to perform face geometry reconstruction. Therefore, our training regime begins with a supervised phase, based on synthetic images, followed by an unsupervised phase that uses only unconstrained facial images. The accuracy and robustness of the proposed model is demonstrated by both qualitative and quantitative evaluation tests. version:1
arxiv-1611-05013 | PixelVAE: A Latent Variable Model for Natural Images | http://arxiv.org/abs/1611.05013 | id:1611.05013 author:Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, Aaron Courville category:cs.LG  published:2016-11-15 summary:Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64x64 ImageNet, and high-quality samples on the LSUN bedrooms dataset. version:1
arxiv-1611-05010 | Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm | http://arxiv.org/abs/1611.05010 | id:1611.05010 author:Kejun Huang, Xiao Fu, Nicholas D. Sidiropoulos category:stat.ML cs.CL cs.IR cs.SI  published:2016-11-15 summary:In topic modeling, many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words -- i.e., words that only appear (with positive probability) in one topic. Follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption. Reliable estimates of higher-order statistics are hard to obtain, however, and the identification of topics under those models hinges on uncorrelatedness of the topics, which can be unrealistic. This paper revisits topic modeling based on second-order moments, and proposes an anchor-free topic mining framework. The proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption, thereby exhibiting much better robustness in practice. The associated algorithm only involves one eigen-decomposition and a few small linear programs. This makes it easy to implement and scale up to very large problem instances. Experiments using the TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence, similarity count, and clustering accuracy metrics) compared to the prior art. version:1
arxiv-1611-05008 | Hybrid Light Field Imaging for Improved Spatial Resolution and Depth Range | http://arxiv.org/abs/1611.05008 | id:1611.05008 author:M. Zeshan Alam, Bahadir K. Gunturk category:cs.CV  published:2016-11-15 summary:Light field imaging involves capturing both angular and spatial distribution of light; it enables new capabilities, such as post-capture digital refocusing, camera aperture adjustment, perspective shift, and depth estimation. Micro-lens array (MLA) based light field cameras provide a cost-effective approach to light field imaging. There are two main limitations of MLA-based light field cameras: low spatial resolution and narrow baseline. While low spatial resolution limits the general purpose use and applicability of light field cameras, narrow baseline limits the depth estimation range and accuracy. In this paper, we present a hybrid stereo imaging system that includes a light field camera and a regular camera. The hybrid system addresses both spatial resolution and narrow baseline issues of the MLA-based light field cameras while preserving light field imaging capabilities. version:1
arxiv-1611-02639 | Gradients of Counterfactuals | http://arxiv.org/abs/1611.02639 | id:1611.02639 author:Mukund Sundararajan, Ankur Taly, Qiqi Yan category:cs.LG cs.CV  published:2016-11-08 summary:Gradients have been used to quantify feature importance in machine learning models. Unfortunately, in nonlinear deep networks, not only individual neurons but also the whole network can saturate, and as a result an important input feature can have a tiny gradient. We study various networks, and observe that this phenomena is indeed widespread, across many inputs. We propose to examine interior gradients, which are gradients of counterfactual inputs constructed by scaling down the original input. We apply our method to the GoogleNet architecture for object recognition in images, as well as a ligand-based virtual screening network with categorical features and an LSTM based language model for the Penn Treebank dataset. We visualize how interior gradients better capture feature importance. Furthermore, interior gradients are applicable to a wide variety of deep networks, and have the attribution property that the feature importance scores sum to the the prediction score. Best of all, interior gradients can be computed just as easily as gradients. In contrast, previous methods are complex to implement, which hinders practical adoption. version:2
arxiv-1611-05003 | Light Field Stitching for Extended Synthetic Aperture | http://arxiv.org/abs/1611.05003 | id:1611.05003 author:M. Umair Mukati, Bahadir K. Gunturk category:cs.CV  published:2016-11-15 summary:Through capturing spatial and angular radiance distribution, light field cameras introduce new capabilities that are not possible with conventional cameras. So far in the light field imaging literature, the focus has been on the theory and applications of single light field capture. By combining multiple light fields, it is possible to obtain new capabilities and enhancements, and even exceed physical limitations, such as spatial resolution and aperture size of the imaging device. In this paper, we present an algorithm to register and stitch multiple light fields. We utilize the regularity of the spatial and angular sampling in light field data, and extend some techniques developed for stereo vision systems to light field data. Such an extension is not straightforward for a micro-lens array (MLA) based light field camera due to extremely small baseline and low spatial resolution. By merging multiple light fields captured by an MLA based camera, we obtain larger synthetic aperture, which results in improvements in light field capabilities, such as increased depth estimation range/accuracy and wider perspective shift range. version:1
arxiv-1611-04994 | One-to-Many Network for Visually Pleasing Compression Artifacts Reduction | http://arxiv.org/abs/1611.04994 | id:1611.04994 author:Jun Guo, Hongyang Chao category:cs.CV  published:2016-11-15 summary:We consider the compression artifacts reduction problem, where a compressed image is transformed into an artifact-free image. Recent approaches for this problem typically train a one-to-one mapping using a per-pixel $L_2$ loss between the outputs and the ground-truths. We point out that these approaches used to produce overly smooth results, and PSNR doesn't reflect their real performance. In this paper, we propose a one-to-many network, which measures output quality using a perceptual loss, a naturalness loss, and a JPEG loss. We also avoid grid-like artifacts during deconvolution using a "shift-and-average" strategy. Extensive experimental results demonstrate the dramatic visual improvement of our approach over the state of the arts. version:1
arxiv-1611-04982 | Oracle Complexity of Second-Order Methods for Finite-Sum Problems | http://arxiv.org/abs/1611.04982 | id:1611.04982 author:Yossi Arjevani, Ohad Shamir category:math.OC cs.LG stat.ML  published:2016-11-15 summary:Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in \emph{second-order} methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer -- perhaps surprisingly -- is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result. version:1
arxiv-1611-04967 | Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box Models | http://arxiv.org/abs/1611.04967 | id:1611.04967 author:Julius Adebayo, Lalana Kagal category:cs.LG stat.ML  published:2016-11-15 summary:Predictive models are increasingly deployed for the purpose of determining access to services such as credit, insurance, and employment. Despite potential gains in productivity and efficiency, several potential problems have yet to be addressed, particularly the potential for unintentional discrimination. We present an iterative procedure, based on orthogonal projection of input attributes, for enabling interpretability of black-box predictive models. Through our iterative procedure, one can quantify the relative dependence of a black-box model on its input attributes.The relative significance of the inputs to a predictive model can then be used to assess the fairness (or discriminatory extent) of such a model. version:1
arxiv-1611-00094 | Learning recurrent representations for hierarchical behavior modeling | http://arxiv.org/abs/1611.00094 | id:1611.00094 author:Eyrun Eyjolfsdottir, Kristin Branson, Yisong Yue, Pietro Perona category:cs.AI cs.CV  published:2016-11-01 summary:We propose a framework for detecting action patterns from motion sequences and modeling the sensory-motor relationship of animals, using a generative recurrent neural network. The network has a discriminative part (classifying actions) and a generative part (predicting motion), whose recurrent cells are laterally connected, allowing higher levels of the network to represent high level phenomena. We test our framework on two types of data, fruit fly behavior and online handwriting. Our results show that 1) taking advantage of unlabeled sequences, by predicting future motion, significantly improves action detection performance when training labels are scarce, 2) the network learns to represent high level phenomena such as writer identity and fly gender, without supervision, and 3) simulated motion trajectories, generated by treating motion prediction as input to the network, look realistic and may be used to qualitatively evaluate whether the model has learnt generative control rules. version:3
arxiv-1611-04953 | End-to-End Neural Sentence Ordering Using Pointer Network | http://arxiv.org/abs/1611.04953 | id:1611.04953 author:Jingjing Gong, Xinchi Chen, Xipeng Qiu, Xuanjing Huang category:cs.CL  published:2016-11-15 summary:Sentence ordering is one of important tasks in NLP. Previous works mainly focused on improving its performance by using pair-wise strategy. However, it is nontrivial for pair-wise models to incorporate the contextual sentence information. In addition, error prorogation could be introduced by using the pipeline strategy in pair-wise models. In this paper, we propose an end-to-end neural approach to address the sentence ordering problem, which uses the pointer network (Ptr-Net) to alleviate the error propagation problem and utilize the whole contextual information. Experimental results show the effectiveness of the proposed model. Source codes and dataset of this paper are available. version:1
arxiv-1611-04928 | Neural Machine Translation with Pivot Languages | http://arxiv.org/abs/1611.04928 | id:1611.04928 author:Yong Cheng, Yang Liu, Qian Yang, Maosong Sun, Wei Xu category:cs.CL  published:2016-11-15 summary:Neural machine translation systems typically rely on the size of parallel corpora. Nevertheless, high-quality parallel corpora are scarce resources for specific language pairs and domains. For a source-to-target language pair with a small parallel corpus, we introduce the pivot language to "bridge" source language and target language under the existence of large source-to-pivot and pivot-to-target parallel corpora. We propose three kinds of connection terms to jointly train source-to-pivot and pivot-to-target translation models in order to enhance the interaction between two sets of model parameters. Experiments on German-French and Spanish-French translation tasks with English as the pivot language show that our joint training approach improves the translation quality significantly than independent training on source-to-pivot, pivot-to-target and source-to-target directions. version:1
arxiv-1611-04924 | Robust Semi-Supervised Graph Classifier Learning with Negative Edge Weights | http://arxiv.org/abs/1611.04924 | id:1611.04924 author:Gene Cheung, Weng-Tai Su, Yu Mao, Chia-Wen Lin category:cs.LG  published:2016-11-15 summary:In a semi-supervised learning scenario, (possibly noisy) partially observed labels are used as input to train a classifier, in order to assign labels to unclassified samples. In this paper, we study this classifier learning problem from a graph signal processing (GSP) perspective. Specifically, by viewing a binary classifier as a piecewise constant graph-signal in a high-dimensional feature space, we cast classifier learning as a signal restoration problem via a classical maximum a posteriori (MAP) formulation. Unlike previous graph-signal restoration works, we consider in addition edges with negative weights that signify anti-correlation between samples. One unfortunate consequence is that the graph Laplacian matrix $\mathbf{L}$ can be indefinite, and previously proposed graph-signal smoothness prior $\mathbf{x}^T \mathbf{L} \mathbf{x}$ for candidate signal $\mathbf{x}$ can lead to pathological solutions. In response, we derive an optimal perturbation matrix $\boldsymbol{\Delta}$ - based on a fast lower-bound computation of the minimum eigenvalue of $\mathbf{L}$ via a novel application of the Haynsworth inertia additivity formula---so that $\mathbf{L} + \boldsymbol{\Delta}$ is positive semi-definite, resulting in a stable signal prior. Further, instead of forcing a hard binary decision for each sample, we define the notion of generalized smoothness on graph that promotes ambiguity in the classifier signal. Finally, we propose an algorithm based on iterative reweighted least squares (IRLS) that solves the posed MAP problem efficiently. Extensive simulation results show that our proposed algorithm outperforms both SVM variants and graph-based classifiers using positive-edge graphs noticeably. version:1
arxiv-1611-04920 | Unsupervised Learning with Truncated Gaussian Graphical Models | http://arxiv.org/abs/1611.04920 | id:1611.04920 author:Qinliang Su, Xuejun Liao, Chunyuan Li, Zhe Gan, Lawrence Carin category:stat.ML cs.LG  published:2016-11-15 summary:Gaussian graphical models (GGMs) are widely used for statistical modeling, because of ease of inference and the ubiquitous use of the normal distribution in practical approximations. However, they are also known for their limited modeling abilities, due to the Gaussian assumption. In this paper, we introduce a novel variant of GGMs, which relaxes the Gaussian restriction and yet admits efficient inference. Specifically, we impose a bipartite structure on the GGM and govern the hidden variables by truncated normal distributions. The nonlinearity of the model is revealed by its connection to rectified linear unit (ReLU) neural networks. Meanwhile, thanks to the bipartite structure and appealing properties of truncated normals, we are able to train the models efficiently using contrastive divergence. We consider three output constructs, accounting for real-valued, binary and count data. We further extend the model to deep structures and show that deep models can be used for unsupervised pre-training of rectifier neural networks. Extensive experimental results are provided to validate the proposed models and demonstrate their superiority over competing models. version:1
arxiv-1611-04905 | CIFAR-10: KNN-based Ensemble of Classifiers | http://arxiv.org/abs/1611.04905 | id:1611.04905 author:Yehya Abouelnaga, Ola S. Ali, Hager Rady, Mohamed Moustafa category:cs.CV  published:2016-11-15 summary:In this paper, we study the performance of different classifiers on the CIFAR-10 dataset, and build an ensemble of classifiers to reach a better performance. We show that, on CIFAR-10, K-Nearest Neighbors (KNN) and Convolutional Neural Network (CNN), on some classes, are mutually exclusive, thus yield in higher accuracy when combined. We reduce KNN overfitting using Principal Component Analysis (PCA), and ensemble it with a CNN to increase its accuracy. Our approach improves our best CNN model from 93.33% to 94.03%. version:1
arxiv-1611-04899 | Diversity encouraged learning of unsupervised LSTM ensemble for neural activity video prediction | http://arxiv.org/abs/1611.04899 | id:1611.04899 author:Yilin Song, Jonathan Viventi, Yao Wang category:cs.CV  published:2016-11-15 summary:Being able to predict the neural signal in the near future from the current and previous observations has the potential to enable real-time responsive brain stimulation to suppress seizures. We have investigated how to use an auto-encoder model consisting of LSTM cells for such prediction. Recog- nizing that there exist multiple activity pattern clusters, we have further explored to train an ensemble of LSTM mod- els so that each model can specialize in modeling certain neural activities, without explicitly clustering the training data. We train the ensemble using an ensemble-awareness loss, which jointly solves the model assignment problem and the error minimization problem. During training, for each training sequence, only the model that has the lowest recon- struction and prediction error is updated. Intrinsically such a loss function enables each LTSM model to be adapted to a subset of the training sequences that share similar dynamic behavior. We demonstrate this can be trained in an end- to-end manner and achieve significant accuracy in neural activity prediction. version:1
arxiv-1611-04887 | Interpreting the Syntactic and Social Elements of the Tweet Representations via Elementary Property Prediction Tasks | http://arxiv.org/abs/1611.04887 | id:1611.04887 author:J Ganesh, Manish Gupta, Vasudeva Varma category:cs.CL cs.SI  published:2016-11-15 summary:Research in social media analysis is experiencing a recent surge with a large number of works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis, semantic textual similarity computation, hashtag prediction and so on. Although the performance of the representation learning models are better than the traditional baselines for the tasks, little is known about the core properties of a tweet encoded within the representations. Understanding these core properties would empower us in making generalizable conclusions about the quality of representations. Our work presented here constitutes the first step in opening the black-box of vector embedding for social media posts, with emphasis on tweets in particular. In order to understand the core properties encoded in a tweet representation, we evaluate the representations to estimate the extent to which it can model each of those properties such as tweet length, presence of words, hashtags, mentions, capitalization, and so on. This is done with the help of multiple classifiers which take the representation as input. Essentially, each classifier evaluates one of the syntactic or social properties which are arguably salient for a tweet. This is also the first holistic study on extensively analysing the ability to encode these properties for a wide variety of tweet representation models including the traditional unsupervised methods (BOW, LDA), unsupervised representation learning methods (Siamese CBOW, Tweet2Vec) as well as supervised methods (CNN, BLSTM). version:1
arxiv-1611-04870 | Constrained Low-Rank Learning Using Least Squares-Based Regularization | http://arxiv.org/abs/1611.04870 | id:1611.04870 author:Ping Li, Jun Yu, Meng Wang, Luming Zhang, Deng Cai, Xuelong Li category:cs.CV cs.LG  published:2016-11-15 summary:Low-rank learning has attracted much attention recently due to its efficacy in a rich variety of real-world tasks, e.g., subspace segmentation and image categorization. Most low-rank methods are incapable of capturing low-dimensional subspace for supervised learning tasks, e.g., classification and regression. This paper aims to learn both the discriminant low-rank representation (LRR) and the robust projecting subspace in a supervised manner. To achieve this goal, we cast the problem into a constrained rank minimization framework by adopting the least squares regularization. Naturally, the data label structure tends to resemble that of the corresponding low-dimensional representation, which is derived from the robust subspace projection of clean data by low-rank learning. Moreover, the low-dimensional representation of original data can be paired with some informative structure by imposing an appropriate constraint, e.g., Laplacian regularizer. Therefore, we propose a novel constrained LRR method. The objective function is formulated as a constrained nuclear norm minimization problem, which can be solved by the inexact augmented Lagrange multiplier algorithm. Extensive experiments on image classification, human pose estimation, and robust face recovery have confirmed the superiority of our method. version:1
arxiv-1611-04358 | Character-level Convolutional Network for Text Classification Applied to Chinese Corpus | http://arxiv.org/abs/1611.04358 | id:1611.04358 author:Weijie Huang, Jun Wang category:cs.CL  published:2016-11-14 summary:This article provides an interesting exploration of character-level convolutional neural network solving Chinese corpus text classification problem. We constructed a large-scale Chinese language dataset, and the result shows that character-level convolutional neural network works better on Chinese corpus than its corresponding pinyin format dataset. This is the first time that character-level convolutional neural network applied to text classification problem. version:2
arxiv-1611-04850 | Scale-constrained Unsupervised Evaluation Method for Multi-scale Image Segmentation | http://arxiv.org/abs/1611.04850 | id:1611.04850 author:Yuhang Lu, Youchuan Wan, Gang Li category:cs.CV  published:2016-11-15 summary:Unsupervised evaluation of segmentation quality is a crucial step in image segmentation applications. Previous unsupervised evaluation methods usually lacked the adaptability to multi-scale segmentation. A scale-constrained evaluation method that evaluates segmentation quality according to the specified target scale is proposed in this paper. First, regional saliency and merging cost are employed to describe intra-region homogeneity and inter-region heterogeneity, respectively. Subsequently, both of them are standardized into equivalent spectral distances of a predefined region. Finally, by analyzing the relationship between image characteristics and segmentation quality, we establish the evaluation model. Experimental results show that the proposed method outperforms four commonly used unsupervised methods in multi-scale evaluation tasks. version:1
arxiv-1611-04849 | Deeply supervised salient object detection with short connections | http://arxiv.org/abs/1611.04849 | id:1611.04849 author:Qibin Hou, Ming-Ming Cheng, Xiao-Wei Hu, Ali Borji, Zhuowen Tu, Philip Torr category:cs.CV  published:2016-11-15 summary:Recent progress on saliency detection is substantial, benefiting mostly from the explosive development of Convolutional Neural Networks (CNNs). Semantic segmentation and saliency detection algorithms developed lately have been mostly based on Fully Convolutional Neural Networks (FCNs). There is still a large room for improvement over the generic FCN models that do not explicitly deal with the scale-space problem. Holistically-Nested Edge Detector (HED) provides a skip-layer structure with deep supervision for edge and boundary detection, but the performance gain of HED on salience detection is not obvious. In this paper, we propose a new method for saliency detection by introducing short connections to the skip-layer structures within the HED architecture. Our framework provides rich multi-scale feature maps at each layer, a property that is critically needed to perform segment detection. Our method produces state-of-the-art results on 5 widely tested salient object detection benchmarks, with advantages in terms of efficiency (0.15 seconds per image), effectiveness, and simplicity over the existing algorithms. version:1
arxiv-1611-04842 | The Role of Word Length in Semantic Topology | http://arxiv.org/abs/1611.04842 | id:1611.04842 author:Francesco Fumarola category:q-bio.NC cs.CL 91E10  published:2016-11-15 summary:A topological argument is presented concering the structure of semantic space, based on the negative correlation between polysemy and word length. The resulting graph structure is applied to the modeling of free-recall experiments, resulting in predictions on the comparative values of recall probabilities. Associative recall is found to favor longer words whereas sequential recall is found to favor shorter words. Data from the PEERS experiments of Lohnas et al. (2015) and Healey and Kahana (2016) confirm both predictons, with correlation coefficients $r_{seq}= -0.17$ and $r_{ass}= +0.17$. The argument is then applied to predicting global properties of list recall, which leads to a novel explanation for the word-length effect based on the optimization of retrieval strategies. version:1
arxiv-1611-04835 | Multilinear Low-Rank Tensors on Graphs & Applications | http://arxiv.org/abs/1611.04835 | id:1611.04835 author:Nauman Shahid, Francesco Grassi, Pierre Vandergheynst category:cs.CV cs.LG stat.ML  published:2016-11-15 summary:We propose a new framework for the analysis of low-rank tensors which lies at the intersection of spectral graph theory and signal processing. As a first step, we present a new graph based low-rank decomposition which approximates the classical low-rank SVD for matrices and multi-linear SVD for tensors. Then, building on this novel decomposition we construct a general class of convex optimization problems for approximately solving low-rank tensor inverse problems, such as tensor Robust PCA. The whole framework is named as 'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis shows: 1) MLRTG stands on the notion of approximate stationarity of multi-dimensional signals on graphs and 2) the approximation error depends on the eigen gaps of the graphs. We demonstrate applications for a wide variety of 4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance videos and hyperspectral images. Generalization of the tensor concepts to non-euclidean domain, orders of magnitude speed-up, low-memory requirement and significantly enhanced performance at low SNR are the key aspects of our framework. version:1
arxiv-1611-04831 | The Power of Normalization: Faster Evasion of Saddle Points | http://arxiv.org/abs/1611.04831 | id:1611.04831 author:Kfir Y. Levy category:cs.LG  published:2016-11-15 summary:A commonly used heuristic in non-convex optimization is Normalized Gradient Descent (NGD) - a variant of gradient descent in which only the direction of the gradient is taken into account and its magnitude ignored. We analyze this heuristic and show that with carefully chosen parameters and noise injection, this method can provably evade saddle points. We establish the convergence of NGD to a local minimum, and demonstrate rates which improve upon the fastest known first order algorithm due to Ge e al. (2015). The effectiveness of our method is demonstrated via an application to the problem of online tensor decomposition; a task for which saddle point evasion is known to result in convergence to global minima. version:1
arxiv-1611-04822 | SimDoc: Topic Sequence Alignment based Document Similarity Framework | http://arxiv.org/abs/1611.04822 | id:1611.04822 author:Gaurav Maheshwari, Priyansh Trivedi, Harshita Sahijwani, Kunal Jha, Sourish Dasgupta, Jens Lehmann category:cs.CL  published:2016-11-15 summary:Document similarity is the problem of formally representing textual documents and then proposing a similarity measure that can be used to compute the linguistic similarity between two documents. Accurate document similarity computation improves many enterprise relevant tasks such as document clustering, text mining, and question-answering. Most contemporary techniques employ bag-of-words (BoW) based document representation models. In this paper, we show that a document's thematic flow, which is often disregarded by bag-of-word techniques, is pivotal in estimating their semantic similarity. In this direction, we propose a novel semantic document similarity framework, called SimDoc. We model documents as topic-sequences, where topics represent latent generative clusters of relative words. We then use a sequence alignment algorithm, that has been adapted from the Smith-Waterman gene-sequencing algorithm, to estimate their semantic similarity. For similarity computation at a finer granularity, we tune the alignment algorithm by integrating it with a word embedding matrix based topic-to-topic similarity measure. A document level similarity score is then computed by again using the sequence alignment algorithm over all sentence pairs. In our experiments, we see that SimDoc outperforms many contemporary bag-of-words techniques in accurately computing document similarity, and on practical applications such as document clustering. version:1
arxiv-1611-01547 | Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations | http://arxiv.org/abs/1611.01547 | id:1611.01547 author:Philip Blair, Yuval Merhav, Joel Barry category:cs.CL cs.LG  published:2016-11-04 summary:We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of "outlier" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis. version:2
arxiv-1611-04798 | Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder | http://arxiv.org/abs/1611.04798 | id:1611.04798 author:Thanh-Le Ha, Jan Niehues, Alexander Waibel category:cs.CL  published:2016-11-15 summary:In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach. We are then able to employ attention-based NMT for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages. version:1
arxiv-1611-04790 | Improved Particle Filters for Vehicle Localisation | http://arxiv.org/abs/1611.04790 | id:1611.04790 author:Kira Kempinska, John Shawe-Taylor category:stat.ML  published:2016-11-15 summary:The ability to track a moving vehicle is of crucial importance in numerous applications. The task has often been approached by the importance sampling technique of particle filters due to its ability to model non-linear and non-Gaussian dynamics, of which a vehicle travelling on a road network is a good example. Particle filters perform poorly when observations are highly informative. In this paper, we address this problem by proposing particle filters that sample around the most recent observation. The proposal leads to an order of magnitude improvement in accuracy and efficiency over conventional particle filters, especially when observations are infrequent but low-noise. version:1
arxiv-1611-04786 | AdversariaLib: An Open-source Library for the Security Evaluation of Machine Learning Algorithms Under Attack | http://arxiv.org/abs/1611.04786 | id:1611.04786 author:Igino Corona, Battista Biggio, Davide Maiorca category:cs.CR cs.LG  published:2016-11-15 summary:We present AdversariaLib, an open-source python library for the security evaluation of machine learning (ML) against carefully-targeted attacks. It supports the implementation of several attacks proposed thus far in the literature of adversarial learning, allows for the evaluation of a wide range of ML algorithms, runs on multiple platforms, and has multi-processing enabled. The library has a modular architecture that makes it easy to use and to extend by implementing novel attacks and countermeasures. It relies on other widely-used open-source ML libraries, including scikit-learn and FANN. Classification algorithms are implemented and optimized in C/C++, allowing for a fast evaluation of the simulated attacks. The package is distributed under the GNU General Public License v3, and it is available for download at http://sourceforge.net/projects/adversarialib. version:1
arxiv-1611-04783 | Comparison of Brain Networks with Unknown Correspondences | http://arxiv.org/abs/1611.04783 | id:1611.04783 author:Sofia Ira Ktena, Sarah Parisot, Jonathan Passerat-Palmbach, Daniel Rueckert category:q-bio.NC cs.NE  published:2016-11-15 summary:Graph theory has drawn a lot of attention in the field of Neuroscience during the last decade, mainly due to the abundance of tools that it provides to explore the interactions of elements in a complex network like the brain. The local and global organization of a brain network can shed light on mechanisms of complex cognitive functions, while disruptions within the network can be linked to neurodevelopmental disorders. In this effort, the construction of a representative brain network for each individual is critical for further analysis. Additionally, graph comparison is an essential step for inference and classification analyses on brain graphs. In this work we explore a method based on graph edit distance for evaluating graph similarity, when correspondences between network elements are unknown due to different underlying subdivisions of the brain. We test this method on 30 unrelated subjects as well as 40 twin pairs and show that this method can accurately reflect the higher similarity between two related networks compared to unrelated ones, while identifying node correspondences. version:1
arxiv-1611-04782 | Feature Extraction and Soft Computing Methods for Aerospace Structure Defect Classification | http://arxiv.org/abs/1611.04782 | id:1611.04782 author:Gianni D'Angelo, Salvatore Rampone category:cs.CV  published:2016-11-15 summary:This study concerns the effectiveness of several techniques and methods of signals processing and data interpretation for the diagnosis of aerospace structure defects. This is done by applying different known feature extraction methods, in addition to a new CBIR-based one; and some soft computing techniques including a recent HPC parallel implementation of the U-BRAIN learning algorithm on Non Destructive Testing data. The performance of the resulting detection systems are measured in terms of Accuracy, Sensitivity, Specificity, and Precision. Their effectiveness is evaluated by the Matthews correlation, the Area Under Curve (AUC), and the F-Measure. Several experiments are performed on a standard dataset of eddy current signal samples for aircraft structures. Our experimental results evidence that the key to a successful defect classifier is the feature extraction method - namely the novel CBIR-based one outperforms all the competitors - and they illustrate the greater effectiveness of the U-BRAIN algorithm and the MLP neural network among the soft computing methods in this kind of application. Keywords- Non-destructive testing (NDT); Soft Computing; Feature Extraction; Classification Algorithms; Content-Based Image Retrieval (CBIR); Eddy Currents (EC). version:1
arxiv-1611-04767 | Prediction of Seasonal Temperature Using Soft Computing Techniques: Application in Benevento (Southern Italy) Area | http://arxiv.org/abs/1611.04767 | id:1611.04767 author:Salvatore Rampone, Alessio Valente category:cs.NE physics.ao-ph  published:2016-11-15 summary:In this work two soft computing methods, Artificial Neural Networks and Genetic Programming, are proposed in order to forecast the mean temperature that will occur in future seasons. The area in which the soft computing techniques were applied is that of the surroundings of the town of Benevento, in the south of Italy, having geographic coordinates (lat. 41{\deg}07'50"N; long.14{\deg}47'13"E). This area is not affected by maritime influences as well as by winds coming from the west. The methods are fed by data recorded in the meteorological stations of Benevento and Castelvenere, located in the hilly area, which characterizes the territory surrounding this city, at 144 m a.s.l. Both the applied methods show low error rates, while the Genetic Programming offers an explicit rule representation (a formula) explaining the prevision. Keywords Seasonal Temperature Forecasting; Soft Computing; Artificial Neural Networks; Genetic Programming; Southern Italy. version:1
arxiv-1611-04766 | Differentiable Genetic Programming | http://arxiv.org/abs/1611.04766 | id:1611.04766 author:Dario Izzo, Francesco Biscani, Alessio Mereta category:cs.NE  published:2016-11-15 summary:We introduce the use of high order automatic differentiation, implemented via the algebra of truncated Taylor polynomials, in genetic programming. Using the Cartesian Genetic Programming encoding we obtain a high-order Taylor representation of the program output that is then used to back-propagate errors during learning. The resulting machine learning framework is called differentiable Cartesian Genetic Programming (dCGP). In the context of symbolic regression, dCGP offers a new approach to the long unsolved problem of constant representation in GP expressions. On several problems of increasing complexity we find that dCGP is able to find the exact form of the symbolic expression as well as the constants values. We also demonstrate the use of dCGP to solve a large class of differential equations and to find prime integrals of dynamical systems, presenting, in both cases, results that confirm the efficacy of our approach. version:1
arxiv-1611-04741 | A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference | http://arxiv.org/abs/1611.04741 | id:1611.04741 author:Biswajit Paria, K. M. Annervaz, Ambedkar Dukkipati, Ankush Chatterjee, Sanjay Podder category:cs.CL  published:2016-11-15 summary:In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory (LSTM), attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of natural language inference. The model is end-to-end differentiable enabling training by stochastic gradient descent. On Stanford Natural Language Inference(SNLI) dataset, the proposed model achieves better accuracy numbers than all published models in literature. version:1
arxiv-1611-04215 | Convolutional Regression for Visual Tracking | http://arxiv.org/abs/1611.04215 | id:1611.04215 author:Kai Chen, Wenbing Tao category:cs.CV  published:2016-11-14 summary:Recently, discriminatively learned correlation filters (DCF) has drawn much attention in visual object tracking community. The success of DCF is potentially attributed to the fact that a large amount of samples are utilized to train the ridge regression model and predict the location of object. To solve the regression problem in an efficient way, these samples are all generated by circularly shifting from a search patch. However, these synthetic samples also induce some negative effects which weaken the robustness of DCF based trackers. In this paper, we propose a Convolutional Regression framework for visual tracking (CRT). Instead of learning the linear regression model in a closed form, we try to solve the regression problem by optimizing a one-channel-output convolution layer with Gradient Descent (GD). In particular, the receptive field size of the convolution layer is set to the size of object. Contrary to DCF, it is possible to incorporate all "real" samples clipped from the whole image. A critical issue of the GD approach is that most of the convolutional samples are negative and the contribution of positive samples will be suppressed. To address this problem, we propose a novel Automatic Hard Negative Mining method to eliminate easy negatives and enhance positives. Extensive experiments are conducted on a widely-used benchmark with 100 sequences. The results show that the proposed algorithm achieves outstanding performance and outperforms almost all the existing DCF based algorithms. version:2
arxiv-1611-04717 | #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning | http://arxiv.org/abs/1611.04717 | id:1611.04717 author:Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel category:cs.AI cs.LG  published:2016-11-15 summary:Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration. version:1
arxiv-1611-04709 | Recoverability of Joint Distribution from Missing Data | http://arxiv.org/abs/1611.04709 | id:1611.04709 author:Jin Tian category:stat.ML cs.AI  published:2016-11-15 summary:A probabilistic query may not be estimable from observed data corrupted by missing values if the data are not missing at random (MAR). It is therefore of theoretical interest and practical importance to determine in principle whether a probabilistic query is estimable from missing data or not when the data are not MAR. We present an algorithm that systematically determines whether the joint probability is estimable from observed data with missing values, assuming that the data-generation model is represented as a Bayesian network containing unobserved latent variables that not only encodes the dependencies among the variables but also explicitly portrays the mechanisms responsible for the missingness process. The result significantly advances the existing work. version:1
arxiv-1611-04208 | Joint mean and covariance estimation with unreplicated matrix-variate data | http://arxiv.org/abs/1611.04208 | id:1611.04208 author:Michael Hornstein, Roger Fan, Kerby Shedden, Shuheng Zhou category:stat.ML  published:2016-11-13 summary:It has been proposed that complex populations, such as those that arise in genomics studies, may exhibit dependencies among the observations as well as among the variables. This gives rise to the challenging problem of analyzing unreplicated high-dimensional data with unknown mean and dependence structures. Matrix-variate approaches that impose various forms of (inverse) covariance sparsity allow flexible dependence structures to be estimated, but cannot directly be applied when the mean and dependences are estimated jointly. We present a practical method utilizing penalized (inverse) covariance estimation and generalized least squares to address this challenge. The advantages of our approaches are: (i) dependence graphs and covariance structures can be estimated in the presence of unknown mean structure, (ii) the mean structure becomes more efficiently estimated when accounting for the dependence structure(s); and (iii) inferences about the mean parameters become correctly calibrated. We establish consistency and obtain rates of convergence for estimating the mean parameters and covariance matrices. We use simulation studies and analysis of genomic data from a twin study of ulcerative colitis to illustrate the statistical convergence and the performance of the procedures in practical settings. Furthermore, several lines of evidence show that the test statistics for differential gene expression produced by our method are correctly calibrated. version:2
arxiv-1611-05317 | A Learning Scheme for Microgrid Islanding and Reconnection | http://arxiv.org/abs/1611.05317 | id:1611.05317 author:Carter Lassetter, Eduardo Cotilla-Sanchez, Jinsub Kim category:cs.LG cs.SY  published:2016-11-15 summary:This paper introduces a robust learning scheme that can dynamically predict the stability of the reconnection of sub-networks to a main grid. As the future electrical power systems tend towards smarter and greener technology, the deployment of self sufficient networks, or microgrids, becomes more likely. Microgrids may operate on their own or synchronized with the main grid, thus control methods need to take into account islanding and reconnecting said networks. The ability to optimally and safely reconnect a portion of the grid is not well understood and, as of now, limited to raw synchronization between interconnection points. A support vector machine (SVM) leveraging real-time data from phasor measurement units (PMUs) is proposed to predict in real time whether the reconnection of a sub-network to the main grid would lead to stability or instability. A dynamics simulator fed with pre-acquired system parameters is used to create training data for the SVM in various operating states. The classifier was tested on a variety of cases and operating points to ensure diversity. Accuracies of approximately 90% were observed throughout most conditions when making dynamic predictions of a given network. version:1
arxiv-1611-00456 | Measuring Asymmetric Opinions on Online Social Interrelationship with Language and Network Features | http://arxiv.org/abs/1611.00456 | id:1611.00456 author:Bo Wang, Yanshu Yu, Yuan Wang category:cs.SI cs.CL  published:2016-11-02 summary:Instead of studying the properties of social relationship from an objective view, in this paper, we focus on individuals' subjective and asymmetric opinions on their interrelationships. Inspired by the theories from sociolinguistics, we investigate two individuals' opinions on their interrelationship with their interactive language features. Eliminating the difference of personal language style, we clarify that the asymmetry of interactive language feature values can indicate individuals' asymmetric opinions on their interrelationship. We also discuss how the degree of opinions' asymmetry is related to the individuals' personality traits. Furthermore, to measure the individuals' asymmetric opinions on interrelationship concretely, we develop a novel model synthetizing interactive language and social network features. The experimental results with Enron email dataset provide multiple evidences of the asymmetric opinions on interrelationship, and also verify the effectiveness of the proposed model in measuring the degree of opinions' asymmetry. version:2
arxiv-1611-04701 | Errors-in-variables models with dependent measurements | http://arxiv.org/abs/1611.04701 | id:1611.04701 author:Mark Rudelson, Shuheng Zhou category:stat.ML math.ST stat.TH  published:2016-11-15 summary:Suppose that we observe $y \in \mathbb{R}^n$ and $X \in \mathbb{R}^{n \times m}$ in the following errors-in-variables model: \begin{eqnarray*} y & = & X_0 \beta^* + \epsilon X & = & X_0 + W \end{eqnarray*} where $X_0$ is a $n \times m$ design matrix with independent subgaussian row vectors, $\epsilon \in \R^n$ is a noise vector and $W$ is a mean zero $n \times m$ random noise matrix with independent subgaussian column vectors, independent of $X_0$ and $\epsilon$. This model is significantly different from those analyzed in the literature in the sense that we allow the measurement error for each covariate to be a dependent vector across its $n$ observations. Such error structures appear in the science literature when modeling the trial-to-trial fluctuations in response strength shared across a set of neurons. We establish consistency in estimating $\beta^*$ and obtain the rates of convergence in the $\ell_q$ norm, where $q = 1, 2$ for the Lasso-type estimator, and for $q \in [1, 2]$ for a Dantzig-type conic programming estimator. We show error bounds which approach that of the regular Lasso and the Dantzig selector in case the errors in $W$ are tending to 0. We analyze the convergence rates of the gradient descent methods for solving the nonconvex programs and show that the composite gradient descent algorithm is guaranteed to converge at a geometric rate to a neighborhood of the global minimizers: the size of the neighborhood is bounded by the statistical error in the $\ell_2$ norm. Our analysis reveals interesting connections between compuational and statistical efficiency and the concentration of measure phenomenon in random matrix theory. We provide simulation evidence illuminating the theoretical predictions. version:1
arxiv-1611-04687 | Intrinsic Geometric Information Transfer Learning on Multiple Graph-Structured Datasets | http://arxiv.org/abs/1611.04687 | id:1611.04687 author:Jaekoo Lee, Hyunjae Kim, Jongsun Lee, Sungroh Yoon category:cs.NE  published:2016-11-15 summary:Graphs provide a powerful means for representing complex interactions between entities. Recently, deep learning approaches are emerging for representing and modeling graph-structured data, although the conventional deep learning methods (such as convolutional neural networks and recurrent neural networks) have mainly focused on grid-structured inputs (image and audio). Leveraged by the capability of representation learning, deep learning based techniques are reporting promising results for graph applications by detecting structural characteristics of graphs in an automated fashion. In this paper, we attempt to advance deep learning for graph-structured data by incorporating another component, transfer learning. By transferring the intrinsic geometric information learned in the source domain, our approach can help us to construct a model for a new but related task in the target domain without collecting new data and without training a new model from scratch. We thoroughly test our approach with large-scale real corpora and confirm the effectiveness of the proposed transfer learning framework for deep learning on graphs. According to our experiments, transfer learning is most effective when the source and target domains bear a high level of structural similarity in their graph representations. version:1
arxiv-1611-04686 | Robust Matrix Regression | http://arxiv.org/abs/1611.04686 | id:1611.04686 author:Hang Zhang, Fengyuan Zhu, Shixin Li category:cs.LG  published:2016-11-15 summary:Modern technologies are producing datasets with complex intrinsic structures, and they can be naturally represented as matrices instead of vectors. To preserve the latent data structures during processing, modern regression approaches incorporate the low-rank property to the model and achieve satisfactory performance for certain applications. These approaches all assume that both predictors and labels for each pair of data within the training set are accurate. However, in real-world applications, it is common to see the training data contaminated by noises, which can affect the robustness of these matrix regression methods. In this paper, we address this issue by introducing a novel robust matrix regression method. We also derive efficient proximal algorithms for model training. To evaluate the performance of our methods, we apply it to real world applications with comparative studies. Our method achieves the state-of-the-art performance, which shows the effectiveness and the practical value of our method. version:1
arxiv-1611-04684 | Knowledge Enhanced Hybrid Neural Network for Text Matching | http://arxiv.org/abs/1611.04684 | id:1611.04684 author:Yu Wu, Wei Wu, Zhoujun Li, Ming Zhou category:cs.CL  published:2016-11-15 summary:Long text brings a big challenge to semantic matching due to their complicated semantic and syntactic structures. To tackle the challenge, we consider using prior knowledge to help identify useful information and filter out noise to matching in long text. To this end, we propose a knowledge enhanced hybrid neural network (KEHNN). The model fuses prior knowledge into word representations by knowledge gates and establishes three matching channels with words, sequential structures of sentences given by Gated Recurrent Units (GRU), and knowledge enhanced representations. The three channels are processed by a convolutional neural network to generate high level features for matching, and the features are synthesized as a matching score by a multilayer perceptron. The model extends the existing methods by conducting matching on words, local structures of sentences, and global context of sentences. Evaluation results from extensive experiments on public data sets for question answering and conversation show that KEHNN can significantly outperform the-state-of-the-art matching models and particularly improve the performance on pairs with long text. version:1
arxiv-1611-04035 | Entropic Causal Inference | http://arxiv.org/abs/1611.04035 | id:1611.04035 author:Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath, Babak Hassibi category:cs.AI cs.IT math.IT stat.ML  published:2016-11-12 summary:We consider the problem of identifying the causal direction between two discrete random variables using observational data. Unlike previous work, we keep the most general functional model but make an assumption on the unobserved exogenous variable: Inspired by Occam's razor, we assume that the exogenous variable is simple in the true causal direction. We quantify simplicity using R\'enyi entropy. Our main result is that, under natural assumptions, if the exogenous variable has low $H_0$ entropy (cardinality) in the true direction, it must have high $H_0$ entropy in the wrong direction. We establish several algorithmic hardness results about estimating the minimum entropy exogenous variable. We show that the problem of finding the exogenous variable with minimum entropy is equivalent to the problem of finding minimum joint entropy given $n$ marginal distributions, also known as minimum entropy coupling problem. We propose an efficient greedy algorithm for the minimum entropy coupling problem, that for $n=2$ provably finds a local optimum. This gives a greedy algorithm for finding the exogenous variable with minimum $H_1$ (Shannon Entropy). Our greedy entropy-based causal inference algorithm has similar performance to the state of the art additive noise models in real datasets. One advantage of our approach is that we make no use of the values of random variables but only their distributions. Our method can therefore be used for causal inference for both ordinal and also categorical data, unlike additive noise models. version:2
arxiv-1611-04666 | A Generic Coordinate Descent Framework for Learning from Implicit Feedback | http://arxiv.org/abs/1611.04666 | id:1611.04666 author:Immanuel Bayer, Xiangnan He, Bhargav Kanagal, Steffen Rendle category:cs.IR cs.LG  published:2016-11-15 summary:In recent years, interest in recommender research has shifted from explicit feedback towards implicit feedback data. A diversity of complex models has been proposed for a wide variety of applications. Despite this, learning from implicit feedback is still computationally challenging. So far, most work relies on stochastic gradient descent (SGD) solvers which are easy to derive, but in practice challenging to apply, especially for tasks with many items. For the simple matrix factorization model, an efficient coordinate descent (CD) solver has been previously proposed. However, efficient CD approaches have not been derived for more complex models. In this paper, we provide a new framework for deriving efficient CD algorithms for complex recommender models. We identify and introduce the property of k-separable models. We show that k-separability is a sufficient property to allow efficient optimization of implicit recommender problems with CD. We illustrate this framework on a variety of state-of-the-art models including factorization machines and Tucker decomposition. To summarize, our work provides the theory and building blocks to derive efficient implicit CD algorithms for complex recommender models. version:1
arxiv-1611-04655 | Motion Estimated-Compensated Reconstruction with Preserved-Features in Free-Breathing Cardiac MRI | http://arxiv.org/abs/1611.04655 | id:1611.04655 author:Aurelien Bustin, Anne Menini, Martin A. Janich, Darius Burschka, Jacques Felblinger, Anja C. S. Brau, Freddy Odille category:cs.CV physics.med-ph  published:2016-11-15 summary:To develop an efficient motion-compensated reconstruction technique for free-breathing cardiac magnetic resonance imaging (MRI) that allows high-quality images to be reconstructed from multiple undersampled single-shot acquisitions. The proposed method is a joint image reconstruction and motion correction method consisting of several steps, including a non-rigid motion extraction and a motion-compensated reconstruction. The reconstruction includes a denoising with the Beltrami regularization, which offers an ideal compromise between feature preservation and staircasing reduction. Results were assessed in simulation, phantom and volunteer experiments. The proposed joint image reconstruction and motion correction method exhibits visible quality improvement over previous methods while reconstructing sharper edges. Moreover, when the acceleration factor increases, standard methods show blurry results while the proposed method preserves image quality. The method was applied to free-breathing single-shot cardiac MRI, successfully achieving high image quality and higher spatial resolution than conventional segmented methods, with the potential to offer high-quality delayed enhancement scans in challenging patients. version:1
arxiv-1611-04642 | Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory | http://arxiv.org/abs/1611.04642 | id:1611.04642 author:Yelong Shen, Po-Sen Huang, Ming-Wei Chang, Jianfeng Gao category:cs.AI cs.CL cs.LG  published:2016-11-14 summary:Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory. Unlike previous work, IRNs use training data to learn to perform multi-step inference through the shared memory, which is also jointly updated during training. While the inference procedure is not operating on top of observed instances for IRNs, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%. version:1
arxiv-1611-02261 | Memory-augmented Attention Modelling for Videos | http://arxiv.org/abs/1611.02261 | id:1611.02261 author:Rasool Fakoor, Abdel-rahman Mohamed, Margaret Mitchell, Sing Bing Kang, Pushmeet Kohli category:cs.CV cs.LG cs.NE  published:2016-11-07 summary:Recent works on neural architectures have demonstrated the utility of attention mechanisms for a wide variety of tasks. Attention models used for problems such as image captioning typically depend on the image under consideration, as well as the previous sequence of words that come before the word currently being generated. While these types of models have produced impressive results, they are not able to model the higher-order interactions involved in problems such as video description/captioning, where the relationship between parts of the video and the concepts being depicted is complex. Motivated by these observations, we propose a novel memory-based attention model for video description. Our model utilizes memories of past attention when reasoning about where to attend to in the current time step, similar to the central executive system proposed in human cognition. This allows the model to not only reason about local attention more effectively, it allows it to consider the entire sequence of video frames while generating each word. Evaluation on the challenging and popular MSVD and Charades datasets show that the proposed architecture outperforms all previously proposed methods and leads to a new state of the art results in the video description. version:2
arxiv-1611-04636 | When Saliency Meets Sentiment: Understanding How Image Content Invokes Emotion and Sentiment | http://arxiv.org/abs/1611.04636 | id:1611.04636 author:Honglin Zheng, Tianlang Chen, Jiebo Luo category:cs.AI cs.CV  published:2016-11-14 summary:Sentiment analysis is crucial for extracting social signals from social media content. Due to the prevalence of images in social media, image sentiment analysis is receiving increasing attention in recent years. However, most existing systems are black-boxes that do not provide insight on how image content invokes sentiment and emotion in the viewers. Psychological studies have confirmed that salient objects in an image often invoke emotions. In this work, we investigate more fine-grained and more comprehensive interaction between visual saliency and visual sentiment. In particular, we partition images in several primary scene-type dimensions, including: open-closed, natural-manmade, indoor-outdoor, and face-noface. Using state of the art saliency detection algorithm and sentiment classification algorithm, we examine how the sentiment of the salient region(s) in an image relates to the overall sentiment of the image. The experiments on a representative image emotion dataset have shown interesting correlation between saliency and sentiment in different scene types and in turn shed light on the mechanism of visual sentiment evocation. version:1
arxiv-1611-04581 | How to scale distributed deep learning? | http://arxiv.org/abs/1611.04581 | id:1611.04581 author:Peter H. Jin, Qiaochu Yuan, Forrest Iandola, Kurt Keutzer category:cs.LG  published:2016-11-14 summary:Training time on large datasets for deep neural networks is the principal workflow bottleneck in a number of important applications of deep learning, such as object classification and detection in automatic driver assistance systems (ADAS). To minimize training time, the training of a deep neural network must be scaled beyond a single machine to as many machines as possible by distributing the optimization method used for training. While a number of approaches have been proposed for distributed stochastic gradient descent (SGD), at the current time synchronous approaches to distributed SGD appear to be showing the greatest performance at large scale. Synchronous scaling of SGD suffers from the need to synchronize all processors on each gradient step and is not resilient in the face of failing or lagging processors. In asynchronous approaches using parameter servers, training is slowed by contention to the parameter server. In this paper we compare the convergence of synchronous and asynchronous SGD for training a modern ResNet network architecture on the ImageNet classification problem. We also propose an asynchronous method, gossiping SGD, that aims to retain the positive features of both systems by replacing the all-reduce collective operation of synchronous training with a gossip aggregation algorithm. We find, perhaps counterintuitively, that asynchronous SGD, including both elastic averaging and gossiping, converges faster at fewer nodes (up to about 32 nodes), whereas synchronous SGD scales better to more nodes (up to about 100 nodes). version:1
arxiv-1611-04578 | Earliness-Aware Deep Convolutional Networks for Early Time Series Classification | http://arxiv.org/abs/1611.04578 | id:1611.04578 author:Wenlin Wang, Changyou Chen, Wenqi Wang, Piyush Rai, Lawrence Carin category:cs.LG  published:2016-11-14 summary:We present Earliness-Aware Deep Convolutional Networks (EA-ConvNets), an end-to-end deep learning framework, for early classification of time series data. Unlike most existing methods for early classification of time series data, that are designed to solve this problem under the assumption of the availability of a good set of pre-defined (often hand-crafted) features, our framework can jointly perform feature learning (by learning a deep hierarchy of \emph{shapelets} capturing the salient characteristics in each time series), along with a dynamic truncation model to help our deep feature learning architecture focus on the early parts of each time series. Consequently, our framework is able to make highly reliable early predictions, outperforming various state-of-the-art methods for early time series classification, while also being competitive when compared to the state-of-the-art time series classification algorithms that work with \emph{fully observed} time series data. To the best of our knowledge, the proposed framework is the first to perform data-driven (deep) feature learning in the context of early classification of time series data. We perform a comprehensive set of experiments, on several benchmark data sets, which demonstrate that our method yields significantly better predictions than various state-of-the-art methods designed for early time series classification. In addition to obtaining high accuracies, our experiments also show that the learned deep shapelets based features are also highly interpretable and can help gain better understanding of the underlying characteristics of time series data. version:1
arxiv-1611-04837 | Lost in Space: Geolocation in Event Data | http://arxiv.org/abs/1611.04837 | id:1611.04837 author:Sophie J. Lee, Howard Liu, Michael D. Ward category:cs.CL  published:2016-11-14 summary:Extracting the "correct" location information from text data, i.e., determining the place of event, has long been a goal for automated text processing. To approximate human-like coding schema, we introduce a supervised machine learning algorithm that classifies each location word to be either correct or incorrect. We use news articles collected from around the world (Integrated Crisis Early Warning System [ICEWS] data and Open Event Data Alliance [OEDA] data) to test our algorithm that consists of two stages. In the feature selection stage, we extract contextual information from texts, namely, the N-gram patterns for location words, the frequency of mention, and the context of the sentences containing location words. In the classification stage, we use three classifiers to estimate the model parameters in the training set and then to predict whether a location word in the test set news articles is the place of the event. The validation results show that our algorithm improves the accuracy rate of the current geolocation methods of dictionary approach by as much as 25%. version:1
arxiv-1611-04561 | Splitting matters: how monotone transformation of predictor variables may improve the predictions of decision tree models | http://arxiv.org/abs/1611.04561 | id:1611.04561 author:Tal Galili, Isaac Meilijson category:stat.ML cs.LG  published:2016-11-14 summary:It is widely believed that the prediction accuracy of decision tree models is invariant under any strictly monotone transformation of the individual predictor variables. However, this statement may be false when predicting new observations with values that were not seen in the training-set and are close to the location of the split point of a tree rule. The sensitivity of the prediction error to the split point interpolation is high when the split point of the tree is estimated based on very few observations, reaching 9% misclassification error when only 10 observations are used for constructing a split, and shrinking to 1% when relying on 100 observations. This study compares the performance of alternative methods for split point interpolation and concludes that the best choice is taking the mid-point between the two closest points to the split point of the tree. Furthermore, if the (continuous) distribution of the predictor variable is known, then using its probability integral for transforming the variable ("quantile transformation") will reduce the model's interpolation error by up to about a half on average. Accordingly, this study provides guidelines for both developers and users of decision tree models (including bagging and random forest). version:1
arxiv-1611-04558 | Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation | http://arxiv.org/abs/1611.04558 | id:1611.04558 author:Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi√©gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean category:cs.CL cs.AI  published:2016-11-14 summary:We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\rightarrow$French and surpasses state-of-the-art results for English$\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\rightarrow$English and German$\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages. version:1
arxiv-1611-04535 | Learning the best algorithm for max-cut, clustering, and other partitioning problems | http://arxiv.org/abs/1611.04535 | id:1611.04535 author:Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, Colin White category:cs.DS cs.AI cs.LG  published:2016-11-14 summary:Many data analysis problems are NP-hard, a reality that has motivated researchers to develop a wealth of approximation algorithms and heuristics over the past few decades. Max-cut, clustering, and many other widely applicable partitioning problems fall into this camp. We focus on two common classes of algorithms that are used for clustering and partitioning problems, including SDP rounding algorithms and agglomerative clustering algorithms with dynamic programming. The best algorithm to use typically depends on the specific application or distribution over instances. A worst-case analysis is often used to compare algorithms, but worst-case instances may not be present in the particular problem domain, and thus may be misleading when determining which algorithm to apply. Therefore, it is necessary to develop optimization methods which return the algorithms and parameters best suited for typical inputs from the application at hand. We address this problem for integer quadratic programming and clustering within a learning-theoretic framework where the learning algorithm is given samples from an application-specific distribution over problem instances and learns an algorithm which performs well over the distribution. We provide sample complexity guarantees and computationally efficient algorithms which optimize an algorithm family's parameters to best suit typical inputs from a particular application. We analyze these algorithms using the learning-theoretic notion of pseudo-dimension. Along with upper bounds, we provide the first pseudo-dimension lower bounds in this line of work, which require an involved analysis of each algorithm family's performance on carefully constructed problem instances. Our bounds are tight and therefore nail down the intrinsic complexity of the algorithm classes we analyze, which are significantly more complex than classes commonly used in learning theory. version:1
arxiv-1611-04534 | 3-D Convolutional Neural Networks for Glioblastoma Segmentation | http://arxiv.org/abs/1611.04534 | id:1611.04534 author:Darvin Yi, Mu Zhou, Zhao Chen, Olivier Gevaert category:cs.CV  published:2016-11-14 summary:Convolutional Neural Networks (CNN) have emerged as powerful tools for learning discriminative image features. In this paper, we propose a framework of 3-D fully CNN models for Glioblastoma segmentation from multi-modality MRI data. By generalizing CNN models to true 3-D convolutions in learning 3-D tumor MRI data, the proposed approach utilizes a unique network architecture to decouple image pixels. Specifically, we design a convolutional layer with pre-defined Difference- of-Gaussian (DoG) filters to perform true 3-D convolution incorporating local neighborhood information at each pixel. We then use three trained convolutional layers that act to decouple voxels from the initial 3-D convolution. The proposed framework allows identification of high-level tumor structures on MRI. We evaluate segmentation performance on the BRATS segmentation dataset with 274 tumor samples. Extensive experimental results demonstrate encouraging performance of the proposed approach comparing to the state-of-the-art methods. Our data-driven approach achieves a median Dice score accuracy of 89% in whole tumor glioblastoma segmentation, revealing a generalized low-bias possibility to learn from medium-size MRI datasets. version:1
arxiv-1611-04528 | Benchmarking Quantum Hardware for Training of Fully Visible Boltzmann Machines | http://arxiv.org/abs/1611.04528 | id:1611.04528 author:Dmytro Korenkevych, Yanbo Xue, Zhengbing Bian, Fabian Chudak, William G. Macready, Jason Rolfe, Evgeny Andriyash category:quant-ph cs.LG stat.ML  published:2016-11-14 summary:Quantum annealing (QA) is a hardware-based heuristic optimization and sampling method applicable to discrete undirected graphical models. While similar to simulated annealing, QA relies on quantum, rather than thermal, effects to explore complex search spaces. For many classes of problems, QA is known to offer computational advantages over simulated annealing. Here we report on the ability of recent QA hardware to accelerate training of fully visible Boltzmann machines. We characterize the sampling distribution of QA hardware, and show that in many cases, the quantum distributions differ significantly from classical Boltzmann distributions. In spite of this difference, training (which seeks to match data and model statistics) using standard classical gradient updates is still effective. We investigate the use of QA for seeding Markov chains as an alternative to contrastive divergence (CD) and persistent contrastive divergence (PCD). Using $k=50$ Gibbs steps, we show that for problems with high-energy barriers between modes, QA-based seeds can improve upon chains with CD and PCD initializations. For these hard problems, QA gradient estimates are more accurate, and allow for faster learning. Furthermore, and interestingly, even the case of raw QA samples (that is, $k=0$) achieved similar improvements. We argue that this relates to the fact that we are training a quantum rather than classical Boltzmann distribution in this case. The learned parameters give rise to hardware QA distributions closely approximating classical Boltzmann distributions that are hard to train with CD/PCD. version:1
arxiv-1611-04520 | Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes | http://arxiv.org/abs/1611.04520 | id:1611.04520 author:Mengye Ren, Renjie Liao, Raquel Urtasun, Fabian H. Sinz, Richard S. Zemel category:cs.LG stat.ML  published:2016-11-14 summary:Normalization techniques have only recently begun to be exploited in supervised learning tasks. Batch normalization exploits mini-batch statistics to normalize the activations. This was shown to speed up training and result in better models. However its success has been very limited when dealing with recurrent neural networks. On the other hand, layer normalization normalizes the activations across all activities within a layer. This was shown to work well in the recurrent setting. In this paper we propose a unified view of normalization techniques, as forms of divisive normalization, which includes layer and batch normalization as special cases. Our second contribution is the finding that a small modification to these normalization schemes, in conjunction with a sparse regularizer on the activations, leads to significant benefits over standard normalization techniques. We demonstrate the effectiveness of our unified divisive normalization framework in the context of convolutional neural nets and recurrent neural networks, showing improvements over baselines in image classification, language modeling as well as super-resolution. version:1
arxiv-1611-04519 | Fast Task-Specific Target Detection via Graph Based Constraints Representation and Checking | http://arxiv.org/abs/1611.04519 | id:1611.04519 author:Went Luan, Yezhou Yang, Cornelia Fermuller, John S. Baras category:cs.CV  published:2016-11-14 summary:In this work, we present a fast target detection framework for real-world robotics applications. Considering that an intelligent agent attends to a task-specific object target during execution, our goal is to detect the object efficiently. We propose the concept of early recognition, which influences the candidate proposal process to achieve fast and reliable detection performance. To check the target constraints efficiently, we put forward a novel policy to generate a sub-optimal checking order, and prove that it has bounded time cost compared to the optimal checking sequence, which is not achievable in polynomial time. Experiments on two different scenarios: 1) rigid object and 2) non-rigid body part detection validate our pipeline. To show that our method is widely applicable, we further present a human-robot interaction system based on our non-rigid body part detection. version:1
arxiv-1611-04503 | Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot | http://arxiv.org/abs/1611.04503 | id:1611.04503 author:Hideki Nakayama, Noriki Nishida category:cs.CL cs.AI cs.CV cs.MM  published:2016-11-14 summary:We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the "pivot", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best. version:1
arxiv-1611-04500 | Deep Learning with Sets and Point Clouds | http://arxiv.org/abs/1611.04500 | id:1611.04500 author:Siamak Ravanbakhsh, Jeff Schneider, Barnabas Poczos category:stat.ML cs.LG cs.NE  published:2016-11-14 summary:We study a simple notion of structural invariance that readily suggests a parameter-sharing scheme in deep neural networks. In particular, we define structure as a collection of relations, and derive graph convolution and recurrent neural networks as special cases. We study composition of basic structures in defining models that are invariant to more complex "product" structures such as graph of graphs, sets of images or sequence of sets. For demonstration, our experimental results are focused on the setting where the discrete structure of interest is a set. We present results on several novel and non-trivial problems on sets, including semi-supervised learning using clustering information, point-cloud classification and set outlier detection. version:1
arxiv-1611-04499 | Post Training in Deep Learning with Last Kernel | http://arxiv.org/abs/1611.04499 | id:1611.04499 author:Thomas Moreau, Julien Audiffren category:stat.ML cs.LG  published:2016-11-14 summary:One of the main challenges of deep learning methods is the choice of an appropriate training strategy. In particular, additional steps, such as unsupervised pre-training, have been shown to greatly improve the performances of deep structures. In this paper, we introduce a new training step,the post-training, which takes place after the training and where only specific layers are trained. In particular, we focus on the particular case -- named Last Kernel -- where only the last layer is trained. This step aims to find the optimal use of data representation learned during the other phases of the training. We show that Last Kernel can be effortlessly added to most learning strategies, is computationally inexpensive, does not cause overfitting and often produces significant improvement. Additionally, we show that with commonly used losses and activation functions, Last Kernel solves a convex closed optimization problems, offering rapid convergence -- or even closed-form solutions. version:1
arxiv-1611-04496 | Multi-view Recurrent Neural Acoustic Word Embeddings | http://arxiv.org/abs/1611.04496 | id:1611.04496 author:Wanjia He, Weiran Wang, Karen Livescu category:cs.CL  published:2016-11-14 summary:Recent work has begun exploring neural acoustic word embeddings--fixed-dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity. version:1
arxiv-1611-04491 | Ranking medical jargon in electronic health record notes by adapted distant supervision | http://arxiv.org/abs/1611.04491 | id:1611.04491 author:Jinying Chen, Abhyuday N. Jagannatha, Samah J. Jarad, Hong Yu category:cs.CL I.2.7  published:2016-11-14 summary:Objective: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, medical jargon, which abounds in EHR notes, has been shown to be a barrier for patient EHR comprehension. Existing knowledge bases that link medical jargon to lay terms or definitions play an important role in alleviating this problem but have low coverage of medical jargon in EHRs. We developed a data-driven approach that mines EHRs to identify and rank medical jargon based on its importance to patients, to support the building of EHR-centric lay language resources. Methods: We developed an innovative adapted distant supervision (ADS) model based on support vector machines to rank medical jargon from EHRs. For distant supervision, we utilized the open-access, collaborative consumer health vocabulary, a large, publicly available resource that links lay terms to medical jargon. We explored both knowledge-based features from the Unified Medical Language System and distributed word representations learned from unlabeled large corpora. We evaluated the ADS model using physician-identified important medical terms. Results: Our ADS model significantly surpassed two state-of-the-art automatic term recognition methods, TF*IDF and C-Value, yielding 0.810 ROC-AUC versus 0.710 and 0.667, respectively. Our model identified 10K important medical jargon terms after ranking over 100K candidate terms mined from over 7,500 EHR narratives. Conclusion: Our work is an important step towards enriching lexical resources that link medical jargon to lay terms/definitions to support patient EHR comprehension. The identified medical jargon terms and their rankings are available upon request. version:1
arxiv-1611-01116 | Binary Paragraph Vectors | http://arxiv.org/abs/1611.01116 | id:1611.01116 author:Karol Grzegorczyk, Marcin Kurdziel category:cs.CL  published:2016-11-03 summary:Recently Le & Mikolov described two log-linear models, called Paragraph Vector, that can be used to learn state-of-the-art distributed representations of documents. Inspired by this work, we present Binary Paragraph Vectors: simple neural networks that learn short binary codes for fast information retrieval. We show that binary paragraph vectors outperform autoencoder-based binary codes, despite using fewer bits. We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus. Results from these experiments indicate that Binary Paragraph Vectors can capture semantics relevant for various domain-specific documents. Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations. This model can be used to rapidly retrieve a short list of highly relevant documents from a large document collection. version:2
arxiv-1611-04482 | Practical Secure Aggregation for Federated Learning on User-Held Data | http://arxiv.org/abs/1611.04482 | id:1611.04482 author:Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, Karn Seth category:cs.CR stat.ML  published:2016-11-14 summary:Secure Aggregation protocols allow a collection of mutually distrust parties, each holding a private value, to collaboratively compute the sum of those values without revealing the values themselves. We consider training a deep neural network in the Federated Learning model, using distributed stochastic gradient descent across user-held training data on mobile devices, wherein Secure Aggregation protects each user's model gradient. We design a novel, communication-efficient Secure Aggregation protocol for high-dimensional data that tolerates up to 1/3 users failing to complete the protocol. For 16-bit input values, our protocol offers 1.73x communication expansion for $2^{10}$ users and $2^{20}$-dimensional vectors, and 1.98x expansion for $2^{14}$ users and $2^{24}$ dimensional vectors. version:1
arxiv-1611-04481 | Can fully convolutional networks perform well for general image restoration problems? | http://arxiv.org/abs/1611.04481 | id:1611.04481 author:Subhajit Chaudhury, Hiya Roy category:cs.CV  published:2016-11-14 summary:We present a fully convolutional network(FCN) based approach for color image restoration. Fully convolutional networks have recently shown remarkable performance for high level vision problems like semantic segmentation. In this paper, we investigate if fully convolutional networks can show promising performance for low level problems like image restoration as well. We propose a FCN model, that learns a direct end-to-end mapping between the corrupted images as input and the desired clean images as output. Experimental results show that our FCN model outperforms traditional sparse coding based methods and demonstrates competitive performance compared to the state-of-the-art methods for image denoising. We further show that our proposed model can solve the difficult problem of blind image inpainting and can produce reconstructed images of impressive visual quality. version:1
arxiv-1611-01843 | Learning to Perform Physics Experiments via Deep Reinforcement Learning | http://arxiv.org/abs/1611.01843 | id:1611.01843 author:Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, Nando de Freitas category:stat.ML cs.AI cs.CV cs.LG cs.NE physics.soc-ph  published:2016-11-06 summary:When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations. version:2
arxiv-1611-04416 | On numerical approximation schemes for expectation propagation | http://arxiv.org/abs/1611.04416 | id:1611.04416 author:Alexis Roche category:stat.CO cs.LG stat.ML  published:2016-11-14 summary:Several numerical approximation strategies for the expectation-propagation algorithm are studied in the context of large-scale learning: the Laplace method, a faster variant of it, Gaussian quadrature, and a deterministic version of variational sampling (i.e., combining quadrature with variational approximation). Experiments in training linear binary classifiers show that the expectation-propagation algorithm converges best using variational sampling, while it also converges well using Laplace-style methods with smooth factors but tends to be unstable with non-differentiable ones. Gaussian quadrature yields unstable behavior or convergence to a sub-optimal solution in most experiments. version:1
arxiv-1611-04413 | Automatic discovery of discriminative parts as a quadratic assignment problem | http://arxiv.org/abs/1611.04413 | id:1611.04413 author:Ronan Sicre, Julien Rabin, Yannis Avrithis, Teddy Furon, Frederic Jurie category:cs.CV  published:2016-11-14 summary:Part-based image classification consists in representing categories by small sets of discriminative parts upon which a representation of the images is built. This paper addresses the question of how to automatically learn such parts from a set of labeled training images. The training of parts is cast as a quadratic assignment problem in which optimal correspondences between image regions and parts are automatically learned. The paper analyses different assignment strategies and thoroughly evaluates them on two public datasets: Willow actions and MIT 67 scenes. State-of-the art results are obtained on these datasets. version:1
arxiv-1611-04399 | Joint Graph Decomposition and Node Labeling by Local Search | http://arxiv.org/abs/1611.04399 | id:1611.04399 author:Evgeny Levinkov, Siyu Tang, Eldar Insafutdinov, Bjoern Andres category:cs.CV cs.DM  published:2016-11-14 summary:We state a combinatorial optimization problem whose feasible solutions define both a decomposition and a node labeling of a given graph. This problem offers a common mathematical abstraction of seemingly unrelated computer vision tasks, including instance-separating semantic segmentation, articulated human body pose estimation and multiple object tracking. Conceptually, the problem we propose generalizes the unconstrained integer quadratic program and the minimum cost lifted multicut problem, both of which are NP-hard. In order to find feasible solutions efficiently, we define a local search algorithm that converges monotonously to a local optimum, offering a feasible solution at any time. To demonstrate the effectiveness of this algorithm in solving computer vision tasks, we report running times and competitive solutions for two above-mentioned applications. version:1
arxiv-1611-01408 | Nonnegative Matrix Underapproximation for Robust Multiple Model Fitting | http://arxiv.org/abs/1611.01408 | id:1611.01408 author:Mariano Tepper, Guillermo Sapiro category:cs.CV  published:2016-11-04 summary:In this work, we introduce a highly efficient algorithm to address the nonnegative matrix underapproximation (NMU) problem, i.e., nonnegative matrix factorization (NMF) with an additional underapproximation constraint. NMU results are interesting as, compared to traditional NMF, they present additional sparsity and part-based behavior, explaining unique data features. To show an example of these features in practice, we first present an application to the analysis of climate data. We then present an NMU-based algorithm to robustly fit multiple parametric models to a dataset. The proposed approach delivers state-of-the-art results for the estimation of multiple fundamental matrices and homographies, outperforming other alternatives in the literature and exemplifying the use of efficient NMU computations. version:2
arxiv-1611-00847 | Deep Convolutional Neural Network Design Patterns | http://arxiv.org/abs/1611.00847 | id:1611.00847 author:Leslie N. Smith, Nicholay Topin category:cs.LG cs.CV cs.NE  published:2016-11-02 summary:Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work. version:3
arxiv-1611-04361 | Attending to Characters in Neural Sequence Labeling Models | http://arxiv.org/abs/1611.04361 | id:1611.04361 author:Marek Rei, Gamal K. O. Crichton, Sampo Pyysalo category:cs.CL cs.LG cs.NE I.5.1; I.2.6; I.2.7  published:2016-11-14 summary:Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters. version:1
arxiv-1611-04357 | Selfie Detection by Synergy-Constraint Based Convolutional Neural Network | http://arxiv.org/abs/1611.04357 | id:1611.04357 author:Yashas Annadani, Vijayakrishna Naganoor, Akshay Kumar Jagadish, Krishnan Chemmangat category:cs.CV  published:2016-11-14 summary:Categorisation of huge amount of data on the multimedia platform is a crucial task. In this work, we propose a novel approach to address the subtle problem of selfie detection for image database segregation on the web, given rapid rise in number of selfies clicked. A Convolutional Neural Network (CNN) is modeled to learn a synergy feature in the common subspace of head and shoulder orientation, derived from Local Binary Pattern (LBP) and Histogram of Oriented Gradients (HOG) features respectively. This synergy was captured by projecting the aforementioned features using Canonical Correlation Analysis (CCA). We show that the resulting network's convolutional activations in the neighbourhood of spatial keypoints captured by SIFT are discriminative for selfie-detection. In general, proposed approach aids in capturing intricacies present in the image data and has the potential for usage in other subtle image analysis scenarios apart from just selfie detection. We investigate and analyse the performance of popular CNN architectures (GoogleNet, AlexNet), used for other image classification tasks, when subjected to the task of detecting the selfies on the multimedia platform. The results of the proposed approach are compared with these popular architectures on a dataset of ninety thousand images comprising of roughly equal number of selfies and non-selfies. Experimental results on this dataset shows the effectiveness of the proposed approach. version:1
arxiv-1611-04353 | Herding Generalizes Diverse M -Best Solutions | http://arxiv.org/abs/1611.04353 | id:1611.04353 author:Ece Ozkan, Gemma Roig, Orcun Goksel, Xavier Boix category:cs.CV  published:2016-11-14 summary:We show that the algorithm to extract diverse M -solutions from a Conditional Random Field (called divMbest [1]) takes exactly the form of a Herding procedure [2], i.e. a deterministic dynamical system that produces a sequence of hypotheses that respect a set of observed moment constraints. This generalization enables us to invoke properties of Herding that show that divMbest enforces implausible constraints which may yield wrong assumptions for some problem settings. Our experiments in semantic segmentation demonstrate that seeing divMbest as an instance of Herding leads to better alternatives for the implausible constraints of divMbest. version:1
arxiv-1611-04841 | Quantitative Entropy Study of Language Complexity | http://arxiv.org/abs/1611.04841 | id:1611.04841 author:R. R. Xie, W. B. Deng, D. J. Wang, L. P. Csernai category:cs.CL physics.soc-ph  published:2016-11-14 summary:We study the entropy of Chinese and English texts, based on characters in case of Chinese texts and based on words for both languages. Significant differences are found between the languages and between different personal styles of debating partners. The entropy analysis points in the direction of lower entropy, that is of higher complexity. Such a text analysis would be applied for individuals of different styles, a single individual at different age, as well as different groups of the population. version:1
arxiv-1611-04326 | `Who would have thought of that!': A Hierarchical Topic Model for Extraction of Sarcasm-prevalent Topics and Sarcasm Detection | http://arxiv.org/abs/1611.04326 | id:1611.04326 author:Aditya Joshi, Prayas Jain, Pushpak Bhattacharyya, Mark Carman category:cs.CL  published:2016-11-14 summary:Topic Models have been reported to be beneficial for aspect-based sentiment analysis. This paper reports a simple topic model for sarcasm detection, a first, to the best of our knowledge. Designed on the basis of the intuition that sarcastic tweets are likely to have a mixture of words of both sentiments as against tweets with literal sentiment (either positive or negative), our hierarchical topic model discovers sarcasm-prevalent topics and topic-level sentiment. Using a dataset of tweets labeled using hashtags, the model estimates topic-level, and sentiment-level distributions. Our evaluation shows that topics such as `work', `gun laws', `weather' are sarcasm-prevalent topics. Our model is also able to discover the mixture of sentiment-bearing words that exist in a text of a given sentiment-related label. Finally, we apply our model to predict sarcasm in tweets. We outperform two prior work based on statistical classifiers with specific features, by around 25\%. version:1
arxiv-1611-04298 | A DNN Framework For Text Image Rectification From Planar Transformations | http://arxiv.org/abs/1611.04298 | id:1611.04298 author:Chengzhe Yan, Jie Hu, Changshui Zhang category:cs.CV  published:2016-11-14 summary:In this paper, a novel neural network architecture is proposed attempting to rectify text images with mild assumptions. A new dataset of text images is collected to verify our model and open to public. We explored the capability of deep neural network in learning geometric transformation and found the model could segment the text image without explicit supervised segmentation information. Experiments show the architecture proposed can restore planar transformations with wonderful robustness and effectiveness. version:1
arxiv-1611-04281 | Statistical mechanics of the inverse Ising problem and the optimal objective function | http://arxiv.org/abs/1611.04281 | id:1611.04281 author:Johannes Berg category:cond-mat.dis-nn q-bio.QM stat.ML  published:2016-11-14 summary:The inverse Ising problem seeks to reconstruct the parameters of an Ising Hamiltonian on the basis of spin configurations sampled from the Boltzmann measure. Recently, strategies to solve the inverse Ising problem based on convex optimisation have proven to be very successful. These approaches maximize particular objective functions with respect to the model parameters. Examples are the pseudolikelihood method and interaction screening. In this paper, we establish a link between approaches to the inverse Ising problem based on convex optimisation and the statistical physics of disordered systems. We characterize the performance of an arbitrary objective function and calculate the objective function which optimally reconstructs the model parameters. We evaluate the optimal objective function within a replica-symmetric ansatz and compare the results of the optimal objective function with other reconstruction methods. Apart from giving a theoretical underpinning to solving the inverse Ising problem by convex optimisation, the optimal method outperforms state-of-the art methods, in some regimes by a significant margin. version:1
arxiv-1611-04273 | On the Quantitative Analysis of Decoder-Based Generative Models | http://arxiv.org/abs/1611.04273 | id:1611.04273 author:Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, Roger Grosse category:cs.LG  published:2016-11-14 summary:The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution. version:1
arxiv-1611-04251 | Baseline CNN structure analysis for facial expression recognition | http://arxiv.org/abs/1611.04251 | id:1611.04251 author:Minchul Shin, Munsang Kim, Dong-Soo Kwon category:cs.CV  published:2016-11-14 summary:We present a baseline convolutional neural network (CNN) structure and image preprocessing methodology to improve facial expression recognition algorithm using CNN. To analyze the most efficient network structure, we investigated four network structures that are known to show good performance in facial expression recognition. Moreover, we also investigated the effect of input image preprocessing methods. Five types of data input (raw, histogram equalization, isotropic smoothing, diffusion-based normalization, difference of Gaussian) were tested, and the accuracy was compared. We trained 20 different CNN models (4 networks x 5 data input types) and verified the performance of each network with test images from five different databases. The experiment result showed that a three-layer structure consisting of a simple convolutional and a max pooling layer with histogram equalization image input was the most efficient. We describe the detailed training procedure and analyze the result of the test accuracy based on considerable observation. version:1
arxiv-1611-04246 | Multi-Shot Mining Semantic Part Concepts in CNNs | http://arxiv.org/abs/1611.04246 | id:1611.04246 author:Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu category:cs.CV  published:2016-11-14 summary:This paper proposes a new learning strategy that incrementally embeds new object-part concepts into a pre-trained convolutional neural network (CNN), in order to 1) explore explicit semantics for the CNN units and 2) gradually transfer the pre-trained CNN into a "white-box" model for hierarchical object understanding. Given part annotations on a very small number (e.g. 3--12) of objects, our method mines certain units from the pre-trained CNN and associate them with different part concepts. We use a four-layer And-Or graph to organize the CNN units, which clarifies their internal semantic hierarchy. Our method is guided by a small number of part annotations, and it achieves superior part-localization performance (about 28%--107% improvement in part center prediction). version:1
arxiv-1611-04244 | Classify or Select: Neural Architectures for Extractive Document Summarization | http://arxiv.org/abs/1611.04244 | id:1611.04244 author:Ramesh Nallapati, Bowen Zhou, Mingbo Ma category:cs.CL  published:2016-11-14 summary:We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the final summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy. We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence. version:1
arxiv-1611-04234 | F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media | http://arxiv.org/abs/1611.04234 | id:1611.04234 author:Hangfeng He, Xu Sun category:cs.CL  published:2016-11-14 summary:We focus on named entity recognition (NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a semi-supervised learning model based on B-LSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between label accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of F-score driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields 7.44\% improvement over previous state-of-the-art result. version:1
arxiv-1611-04233 | A New Recurrent Neural CRF for Learning Non-linear Edge Features | http://arxiv.org/abs/1611.04233 | id:1611.04233 author:Shuming Ma, Xu Sun category:cs.CL  published:2016-11-14 summary:Conditional Random Field (CRF) and recurrent neural models have achieved success in structured prediction. More recently, there is a marriage of CRF and recurrent neural models, so that we can gain from both non-linear dense features and globally normalized CRF objective. These recurrent neural CRF models mainly focus on encode node features in CRF undirected graphs. However, edge features prove important to CRF in structured prediction. In this work, we introduce a new recurrent neural CRF model, which learns non-linear edge features, and thus makes non-linear features encoded completely. We compare our model with different neural models in well-known structured prediction tasks. Experiments show that our model outperforms state-of-the-art methods in NP chunking, shallow parsing, Chinese word segmentation and POS tagging. version:1
arxiv-1611-04231 | Identity Matters in Deep Learning | http://arxiv.org/abs/1611.04231 | id:1611.04231 author:Moritz Hardt, Tengyu Ma category:cs.LG cs.NE stat.ML  published:2016-11-14 summary:An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as \emph{batch normalization}, but was also key to the immense success of \emph{residual networks}. In this work, we put the principle of \emph{identity parameterization} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks. version:1
arxiv-1611-04230 | SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents | http://arxiv.org/abs/1611.04230 | id:1611.04230 author:Ramesh Nallapati, Feifei Zhai, Bowen Zhou category:cs.CL  published:2016-11-14 summary:We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels. version:1
arxiv-1611-04228 | Learning Sparse, Distributed Representations using the Hebbian Principle | http://arxiv.org/abs/1611.04228 | id:1611.04228 author:Aseem Wadhwa, Upamanyu Madhow category:cs.LG  published:2016-11-14 summary:The "fire together, wire together" Hebbian model is a central principle for learning in neuroscience, but surprisingly, it has found limited applicability in modern machine learning. In this paper, we take a first step towards bridging this gap, by developing flavors of competitive Hebbian learning which produce sparse, distributed neural codes using online adaptation with minimal tuning. We propose an unsupervised algorithm, termed Adaptive Hebbian Learning (AHL). We illustrate the distributed nature of the learned representations via output entropy computations for synthetic data, and demonstrate superior performance, compared to standard alternatives such as autoencoders, in training a deep convolutional net on standard image datasets. version:1
arxiv-1610-09447 | Asynchronous Stochastic Block Coordinate Descent with Variance Reduction | http://arxiv.org/abs/1610.09447 | id:1610.09447 author:Bin Gu, Zhouyuan Huo, Heng Huang category:cs.LG  published:2016-10-29 summary:Asynchronous parallel implementations for stochastic optimization have received huge successes in theory and practice recently. Asynchronous implementations with lock-free are more efficient than the one with writing or reading lock. In this paper, we focus on a composite objective function consisting of a smooth convex function $f$ and a block separable convex function, which widely exists in machine learning and computer vision. We propose an asynchronous stochastic block coordinate descent algorithm with the accelerated technology of variance reduction (AsySBCDVR), which are with lock-free in the implementation and analysis. AsySBCDVR is particularly important because it can scale well with the sample size and dimension simultaneously. We prove that AsySBCDVR achieves a linear convergence rate when the function $f$ is with the optimal strong convexity property, and a sublinear rate when $f$ is with the general convexity. More importantly, a near-linear speedup on a parallel system with shared memory can be obtained. version:3
arxiv-1611-04218 | Preference Completion from Partial Rankings | http://arxiv.org/abs/1611.04218 | id:1611.04218 author:Suriya Gunasekar, Oluwasanmi Koyejo, Joydeep Ghosh category:stat.ML cs.LG  published:2016-11-14 summary:We propose a novel and efficient algorithm for the collaborative preference completion problem, which involves jointly estimating individualized rankings for a set of entities over a shared set of items, based on a limited number of observed affinity values. Our approach exploits the observation that while preferences are often recorded as numerical scores, the predictive quantity of interest is the underlying rankings. Thus, attempts to closely match the recorded scores may lead to overfitting and impair generalization performance. Instead, we propose an estimator that directly fits the underlying preference order, combined with nuclear norm constraints to encourage low--rank parameters. Besides (approximate) correctness of the ranking order, the proposed estimator makes no generative assumption on the numerical scores of the observations. One consequence is that the proposed estimator can fit any consistent partial ranking over a subset of the items represented as a directed acyclic graph (DAG), generalizing standard techniques that can only fit preference scores. Despite this generality, for supervision representing total or blockwise total orders, the computational complexity of our algorithm is within a $\log$ factor of the standard algorithms for nuclear norm regularization based estimates for matrix completion. We further show promising empirical results for a novel and challenging application of collaboratively ranking of the associations between brain--regions and cognitive neuroscience terms. version:1
arxiv-1611-01838 | Entropy-SGD: Biasing Gradient Descent Into Wide Valleys | http://arxiv.org/abs/1611.01838 | id:1611.01838 author:Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun category:cs.LG  published:2016-11-06 summary:This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape at solutions found by gradient descent. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local entropy based objective that favors well-generalizable solutions lying in the flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Our algorithm resembles two nested loops of SGD, where we use Langevin dynamics to compute the gradient of local entropy at each update of the weights. We prove that incorporating local entropy into the objective function results in a smoother energy landscape and use uniform stability to show improved generalization bounds over SGD. Our experiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training. version:2
arxiv-1611-02447 | Action Recognition Based on Joint Trajectory Maps Using Convolutional Neural Networks | http://arxiv.org/abs/1611.02447 | id:1611.02447 author:Pichao Wang, Zhaoyang Li, Yonghong Hou, Wanqing Li category:cs.CV  published:2016-11-08 summary:Recently, Convolutional Neural Networks (ConvNets) have shown promising performances in many computer vision tasks, especially image-based recognition. How to effectively use ConvNets for video-based recognition is still an open problem. In this paper, we propose a compact, effective yet simple method to encode spatio-temporal information carried in $3D$ skeleton sequences into multiple $2D$ images, referred to as Joint Trajectory Maps (JTM), and ConvNets are adopted to exploit the discriminative features for real-time human action recognition. The proposed method has been evaluated on three public benchmarks, i.e., MSRC-12 Kinect gesture dataset (MSRC-12), G3D dataset and UTD multimodal human action dataset (UTD-MHAD) and achieved the state-of-the-art results. version:2
arxiv-1611-04201 | (CAD)$^2$RL: Real Single-Image Flight without a Single Real Image | http://arxiv.org/abs/1611.04201 | id:1611.04201 author:Fereshteh Sadeghi, Sergey Levine category:cs.LG cs.CV cs.RO  published:2016-11-13 summary:We propose (CAD)$^2$RL, a flight controller for Collision Avoidance via Deep Reinforcement Learning that can be used to perform collision-free flight in the real world although it is trained entirely in a 3D CAD model simulator. Our method uses only single RGB images from a monocular camera mounted on the robot as the input and is specialized for indoor hallway following and obstacle avoidance. In contrast to most indoor navigation techniques that aim to directly reconstruct the 3D geometry of the environment, our approach directly predicts the probability of collision given the current monocular image and a candidate action. To obtain accurate predictions, we develop a deep reinforcement learning algorithm for learning indoor navigation, which uses the actual performance of the current policy to construct accurate supervision. The collision prediction model is represented by a deep convolutional neural network that directly processes raw image inputs. Our collision avoidance system is entirely trained in simulation and thus addresses the high sample complexity of deep reinforcement learning and avoids the dangers of trial-and-error learning in the real world. By highly randomizing the rendering settings for our simulated training set, we show that we can train a collision predictor that generalizes to new environments with substantially different appearance from the training scenarios. Finally, we evaluate our method in the real world by controlling a real quadrotor flying through real hallways. We demonstrate that our method can perform real-world collision avoidance and hallway following after being trained exclusively on synthetic images, without ever having seen a single real image at the training time. For supplementary video see: https://fsadeghi.github.io/CAD2RL version:1
arxiv-1611-04199 | Realistic risk-mitigating recommendations via inverse classification | http://arxiv.org/abs/1611.04199 | id:1611.04199 author:Michael T. Lash, W. Nick Street category:cs.LG stat.ML  published:2016-11-13 summary:Inverse classification, the process of making meaningful perturbations to a test point such that it is more likely to have a desired classification, has previously been addressed using data from a single static point in time. Such an approach yields inflated probability estimates, stemming from an implicitly made assumption that recommendations are implemented instantaneously. We propose using longitudinal data to alleviate such issues in two ways. First, we use past outcome probabilities as features in the present. Use of such past probabilities ties historical behavior to the present, allowing for more information to be taken into account when making initial probability estimates and subsequently performing inverse classification. Secondly, following inverse classification application, optimized instances' unchangeable features (e.g.,~age) are updated using values from the next longitudinal time period. Optimized test instance probabilities are then reassessed. Updating the unchangeable features in this manner reflects the notion that improvements in outcome likelihood, which result from following the inverse classification recommendations, do not materialize instantaneously. As our experiments demonstrate, more realistic estimates of probability can be obtained by factoring in such considerations. version:1
arxiv-1611-04149 | Accelerated Variance Reduced Block Coordinate Descent | http://arxiv.org/abs/1611.04149 | id:1611.04149 author:Zebang Shen, Hui Qian, Chao Zhang, Tengfei Zhou category:stat.ML cs.LG  published:2016-11-13 summary:Algorithms with fast convergence, small number of data access, and low per-iteration complexity are particularly favorable in the big data era, due to the demand for obtaining \emph{highly accurate solutions} to problems with \emph{a large number of samples} in \emph{ultra-high} dimensional space. Existing algorithms lack at least one of these qualities, and thus are inefficient in handling such big data challenge. In this paper, we propose a method enjoying all these merits with an accelerated convergence rate $O(\frac{1}{k^2})$. Empirical studies on large scale datasets with more than one million features are conducted to show the effectiveness of our methods in practice. version:1
arxiv-1611-04144 | Semi-Dense 3D Semantic Mapping from Monocular SLAM | http://arxiv.org/abs/1611.04144 | id:1611.04144 author:Xuanpeng Li, Rachid Belaroussi category:cs.CV  published:2016-11-13 summary:The bundle of geometry and appearance in computer vision has proven to be a promising solution for robots across a wide variety of applications. Stereo cameras and RGB-D sensors are widely used to realise fast 3D reconstruction and trajectory tracking in a dense way. However, they lack flexibility of seamless switch between different scaled environments, i.e., indoor and outdoor scenes. In addition, semantic information are still hard to acquire in a 3D mapping. We address this challenge by combining the state-of-art deep learning method and semi-dense Simultaneous Localisation and Mapping (SLAM) based on video stream from a monocular camera. In our approach, 2D semantic information are transferred to 3D mapping via correspondence between connective Keyframes with spatial consistency. There is no need to obtain a semantic segmentation for each frame in a sequence, so that it could achieve a reasonable computation time. We evaluate our method on indoor/outdoor datasets and lead to an improvement in the 2D semantic labelling over baseline single frame predictions. version:1
arxiv-1611-04138 | Hand Gesture Recognition for Contactless Device Control in Operating Rooms | http://arxiv.org/abs/1611.04138 | id:1611.04138 author:Ebrahim Nasr-Esfahani, Nader Karimi, S. M. Reza Soroushmehr, M. Hossein Jafari, M. Amin Khorsandi, Shadrokh Samavi, Kayvan Najarian category:cs.CV  published:2016-11-13 summary:Hand gesture is one of the most important means of touchless communication between human and machines. There is a great interest for commanding electronic equipment in surgery rooms by hand gesture for reducing the time of surgery and the potential for infection. There are challenges in implementation of a hand gesture recognition system. It has to fulfill requirements such as high accuracy and fast response. In this paper we introduce a system of hand gesture recognition based on a deep learning approach. Deep learning is known as an accurate detection model, but its high complexity prevents it from being fabricated as an embedded system. To cope with this problem, we applied some changes in the structure of our work to achieve low complexity. As a result, the proposed method could be implemented on a naive embedded system. Our experiments show that the proposed system results in higher accuracy while having less complexity in comparison with the existing comparable methods. version:1
arxiv-1611-04135 | Automated Inference on Criminality using Face Images | http://arxiv.org/abs/1611.04135 | id:1611.04135 author:Xiaolin Wu, Xi Zhang category:cs.CV  published:2016-11-13 summary:We study, for the first time, automated inference on criminality based solely on still face images. Via supervised machine learning, we build four classifiers (logistic regression, KNN, SVM, CNN) using facial images of 1856 real persons controlled for race, gender, age and facial expressions, nearly half of whom were convicted criminals, for discriminating between criminals and non-criminals. All four classifiers perform consistently well and produce evidence for the validity of automated face-induced inference on criminality, despite the historical controversy surrounding the topic. Also, we find some discriminating structural features for predicting criminality, such as lip curvature, eye inner corner distance, and the so-called nose-mouth angle. Above all, the most important discovery of this research is that criminal and non-criminal face images populate two quite distinctive manifolds. The variation among criminal faces is significantly greater than that of the non-criminal faces. The two manifolds consisting of criminal and non-criminal faces appear to be concentric, with the non-criminal manifold lying in the kernel with a smaller span, exhibiting a law of normality for faces of non-criminals. In other words, the faces of general law-biding public have a greater degree of resemblance compared with the faces of criminals, or criminals have a higher degree of dissimilarity in facial appearance than normal people. version:1
arxiv-1611-04125 | Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion | http://arxiv.org/abs/1611.04125 | id:1611.04125 author:Xu Han, Zhiyuan Liu, Maosong Sun category:cs.CL  published:2016-11-13 summary:Joint representation learning of text and knowledge within a unified semantic space enables us to perform knowledge graph completion more accurately. In this work, we propose a novel framework to embed words, entities and relations into the same continuous vector space. In this model, both entity and relation embeddings are learned by taking knowledge graph and plain text into consideration. In experiments, we evaluate the joint learning model on three tasks including entity prediction, relation prediction and relation classification from text. The experiment results show that our model can significantly and consistently improve the performance on the three tasks as compared with other baselines. version:1
arxiv-1611-04122 | Cross-lingual Dataless Classification for Languages with Small Wikipedia Presence | http://arxiv.org/abs/1611.04122 | id:1611.04122 author:Yangqiu Song, Stephen Mayhew, Dan Roth category:cs.CL  published:2016-11-13 summary:This paper presents an approach to classify documents in any language into an English topical label space, without any text categorization training data. The approach, Cross-Lingual Dataless Document Classification (CLDDC) relies on mapping the English labels or short category description into a Wikipedia-based semantic representation, and on the use of the target language Wikipedia. Consequently, performance could suffer when Wikipedia in the target language is small. In this paper, we focus on languages with small Wikipedias, (Small-Wikipedia languages, SWLs). We use a word-level dictionary to convert documents in a SWL to a large-Wikipedia language (LWLs), and then perform CLDDC based on the LWL's Wikipedia. This approach can be applied to thousands of languages, which can be contrasted with machine translation, which is a supervision heavy approach and can be done for about 100 languages. We also develop a ranking algorithm that makes use of language similarity metrics to automatically select a good LWL, and show that this significantly improves classification of SWLs' documents, performing comparably to the best bridge possible. version:1
arxiv-1611-04088 | Batched Gaussian Process Bandit Optimization via Determinantal Point Processes | http://arxiv.org/abs/1611.04088 | id:1611.04088 author:Tarun Kathuria, Amit Deshpande, Pushmeet Kohli category:cs.LG  published:2016-11-13 summary:Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function requires training a model which may involve days or even weeks of computation. Most methods for this so-called "Bayesian optimization" only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods. version:1
arxiv-1611-02401 | Divide and Conquer with Neural Networks | http://arxiv.org/abs/1611.02401 | id:1611.02401 author:Alex Nowak, Joan Bruna category:cs.LG stat.ML  published:2016-11-08 summary:We consider the learning of algorithmic tasks by mere observation of input-output pairs. Rather than studying this as a black-box discrete regression problem with no assumption whatsoever on the input-output mapping, we concentrate on tasks that are amenable to the principle of \emph{divide and conquer}, and study what are its implications in terms of learning. This principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to \emph{split} a given input into two disjoint sets, and how to \emph{merge} two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner. As a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls. version:3
arxiv-1611-04076 | Multi-class Generative Adversarial Networks with the L2 Loss Function | http://arxiv.org/abs/1611.04076 | id:1611.04076 author:Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang category:cs.CV  published:2016-11-13 summary:Generative adversarial networks (GANs) have achieved huge success in unsupervised learning. Most of GANs treat the discriminator as a classifier with the binary sigmoid cross entropy loss function. However, we find that the sigmoid cross entropy loss function will sometimes lead to the saturation problem in GANs learning. In this work, we propose to adopt the L2 loss function for the discriminator. The properties of the L2 loss function can improve the stabilization of GANs learning. With the usage of the L2 loss function, we propose the multi-class generative adversarial networks for the purpose of image generation with multiple classes. We evaluate the multi-class GANs on a handwritten Chinese characters dataset with 3740 classes. The experiments demonstrate that the multi-class GANs can generate elegant images on datasets with a large number of classes. Comparison experiments between the L2 loss function and the sigmoid cross entropy loss function are also conducted and the results demonstrate the stabilization of the L2 loss function. version:1
arxiv-1611-04069 | Low-rank and Adaptive Sparse Signal (LASSI) Models for Highly Accelerated Dynamic Imaging | http://arxiv.org/abs/1611.04069 | id:1611.04069 author:Saiprasad Ravishankar, Brian E. Moore, Raj Rao Nadakuditi, Jeffrey A. Fessler category:stat.ML cs.LG  published:2016-11-13 summary:Sparsity-based approaches have been popular in many applications in image processing and imaging. Compressed sensing exploits the sparsity of images in a transform domain or dictionary to improve image recovery from undersampled measurements. In the context of inverse problems in dynamic imaging, recent research has demonstrated the promise of sparsity and low-rank techniques. For example, the patches of the underlying data are modeled as sparse in an adaptive dictionary domain, and the resulting image and dictionary estimation from undersampled measurements is called dictionary-blind compressed sensing, or the dynamic image sequence is modeled as a sum of low-rank and sparse (in some transform domain) components (L+S model) that are estimated from limited measurements. In this work, we investigate a data-adaptive extension of the L+S model, dubbed LASSI, where the temporal image sequence is decomposed into a low-rank component and a component whose spatiotemporal (3D) patches are sparse in some adaptive dictionary domain. We investigate various formulations and efficient methods for jointly estimating the underlying dynamic signal components and the spatiotemporal dictionary from limited measurements. We also obtain efficient sparsity penalized dictionary-blind compressed sensing methods as special cases of our LASSI algorithms. Our numerical experiments demonstrate the promising performance of LASSI schemes for dynamic magnetic resonance image reconstruction from limited k-t space data compared to recent methods such as k-t SLR and L+S, and compared to the proposed dictionary-blind compressed sensing method. version:1
arxiv-1611-04067 | Error Metrics for Learning Reliable Manifolds from Streaming Data | http://arxiv.org/abs/1611.04067 | id:1611.04067 author:Frank Schoeneman, Suchismit Mahapatra, Varun Chandola, Nils Napp, Jaroslaw Zola category:stat.ML  published:2016-11-13 summary:Spectral dimensionality reduction is frequently used to identify low-dimensional structure in high-dimensional data. However, learning manifolds, especially from the streaming data, is computationally and memory expensive. In this paper, we argue that a stable manifold can be learned using only a fraction of the stream, and the remaining stream can be mapped to the manifold in a significantly less costly manner. Identifying the transition point at which the manifold is stable is the key step. We present error metrics that allow us to identify the transition point for a given stream by quantitatively assessing the quality of a manifold learned using Isomap. We further propose an efficient mapping algorithm, called S-Isomap, that can be used to map new samples onto the stable manifold. We describe experiments on a variety of data sets that show that the proposed approach is computationally efficient without sacrificing accuracy. version:1
arxiv-1611-04052 | Semi-automatic Simultaneous Interpreting Quality Evaluation | http://arxiv.org/abs/1611.04052 | id:1611.04052 author:Xiaojun Zhang category:cs.CL  published:2016-11-12 summary:Increasing interpreting needs a more objective and automatic measurement. We hold a basic idea that 'translating means translating meaning' in that we can assessment interpretation quality by comparing the meaning of the interpreting output with the source input. That is, a translation unit of a 'chunk' named Frame which comes from frame semantics and its components named Frame Elements (FEs) which comes from Frame Net are proposed to explore their matching rate between target and source texts. A case study in this paper verifies the usability of semi-automatic graded semantic-scoring measurement for human simultaneous interpreting and shows how to use frame and FE matches to score. Experiments results show that the semantic-scoring metrics have a significantly correlation coefficient with human judgment. version:1
arxiv-1611-04051 | GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution | http://arxiv.org/abs/1611.04051 | id:1611.04051 author:Matt J. Kusner, Jos√© Miguel Hern√°ndez-Lobato category:stat.ML cs.LG  published:2016-11-12 summary:Generative Adversarial Networks (GAN) have limitations when the goal is to generate sequences of discrete elements. The reason for this is that samples from a distribution on discrete objects such as the multinomial are not differentiable with respect to the distribution parameters. This problem can be avoided by using the Gumbel-softmax distribution, which is a continuous approximation to a multinomial distribution parameterized in terms of the softmax function. In this work, we evaluate the performance of GANs based on recurrent neural networks with Gumbel-softmax output distributions in the task of generating sequences of discrete elements. version:1
arxiv-1611-04049 | Prognostics of Surgical Site Infections using Dynamic Health Data | http://arxiv.org/abs/1611.04049 | id:1611.04049 author:Chuyang Ke, Yan Jin, Heather Evans, Bill Lober, Xiaoning Qian, Ji Liu, Shuai Huang category:cs.LG  published:2016-11-12 summary:Surgical Site Infection (SSI) is a national priority in healthcare research. Much research attention has been attracted to develop better SSI risk prediction models. However, most of the existing SSI risk prediction models are built on static risk factors such as comorbidities and operative factors. In this paper, we investigate the use of the dynamic wound data for SSI risk prediction. There have been emerging mobile health (mHealth) tools that can closely monitor the patients and generate continuous measurements of many wound-related variables and other evolving clinical variables. Since existing prediction models of SSI have quite limited capacity to utilize the evolving clinical data, we develop the corresponding solution to equip these mHealth tools with decision-making capabilities for SSI prediction with a seamless assembly of several machine learning models to tackle the analytic challenges arising from the spatial-temporal data. The basic idea is to exploit the low-rank property of the spatial-temporal data via the bilinear formulation, and further enhance it with automatic missing data imputation by the matrix completion technique. We derive efficient optimization algorithms to implement these models and demonstrate the superior performances of our new predictive model on a real-world dataset of SSI, compared to a range of state-of-the-art methods. version:1
arxiv-1611-04033 | 1.5 billion words Arabic Corpus | http://arxiv.org/abs/1611.04033 | id:1611.04033 author:Ibrahim Abu El-khair category:cs.CL cs.DL cs.IR  published:2016-11-12 summary:This study is an attempt to build a contemporary linguistic corpus for Arabic language. The corpus produced, is a text corpus includes more than five million newspaper articles. It contains over a billion and a half words in total, out of which, there is about three million unique words. The data were collected from newspaper articles in ten major news sources from eight Arabic countries, over a period of fourteen years. The corpus was encoded with two types of encoding, namely: UTF-8, and Windows CP-1256. Also it was marked with two mark-up languages, namely: SGML, and XML. version:1
arxiv-1611-04021 | Leveraging Video Descriptions to Learn Video Question Answering | http://arxiv.org/abs/1611.04021 | id:1611.04021 author:Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan Carlos Niebles, Min Sun category:cs.CV cs.AI cs.MM  published:2016-11-12 summary:We propose a scalable approach to learn video-based question answering (QA): answer a "free-form natural language question" about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines. version:1
arxiv-1611-04010 | Multi-Language Identification Using Convolutional Recurrent Neural Network | http://arxiv.org/abs/1611.04010 | id:1611.04010 author:Vrishabh Ajay Lakhani, Rohan Mahadev category:cs.CL  published:2016-11-12 summary:Language Identification, being an important aspect of Automatic Speaker Recognition has had many changes and new approaches to ameliorate performance over the last decade. We compare the performance of using audio spectrum in the log scale and using Polyphonic sound sequences from raw audio samples to train the neural network and to classify speech as either English or Spanish. To achieve this, we use the novel approach of using a Convolutional Recurrent Neural Network using Long Short Term Memory (LSTM) or a Gated Recurrent Unit (GRU) for forward propagation of the neural network. Our hypothesis is that the performance of using polyphonic sound sequence as features and both LSTM and GRU as the gating mechanisms for the neural network outperform the traditional MFCC features using a unidirectional Deep Neural Network. version:1
arxiv-1611-03193 | Mahalanobis Distance for Class Averaging of Cryo-EM Images | http://arxiv.org/abs/1611.03193 | id:1611.03193 author:Tejal Bhamre, Zhizhen Zhao, Amit Singer category:stat.AP cs.CV q-bio.BM stat.ML  published:2016-11-10 summary:Single particle reconstruction (SPR) from cryo-electron microscopy (EM) is a technique in which the 3D structure of a molecule needs to be determined from its contrast transfer function (CTF) affected, noisy 2D projection images taken at unknown viewing directions. One of the main challenges in cryo-EM is the typically low signal to noise ratio (SNR) of the acquired images. 2D classification of images, followed by class averaging, improves the SNR of the resulting averages, and is used for selecting particles from micrographs and for inspecting the particle images. We introduce a new affinity measure, akin to the Mahalanobis distance, to compare cryo-EM images belonging to different defocus groups. The new similarity measure is employed to detect similar images, thereby leading to an improved algorithm for class averaging. We evaluate the performance of the proposed class averaging procedure on synthetic datasets, obtaining state of the art classification. version:2
arxiv-1611-03999 | Optimized clothes segmentation to boost gender classification in unconstrained scenarios | http://arxiv.org/abs/1611.03999 | id:1611.03999 author:D. Freire-Obreg√≥n, M. Castrill√≥n-Santana, J. Lorenzo-Navarro category:cs.CV  published:2016-11-12 summary:Several applications require demographic information of ordinary people in unconstrained scenarios. This is not a trivial task due to significant human appearance variations. In this work, we introduce trixels for clustering image regions, enumerating their advantages compared to superpixels. The classical GrabCut algorithm is later modified to segment trixels instead of pixels in an unsupervised context. Combining with face detection lead us to a clothes segmentation approach close to real time. The study uses the challenging Pascal VOC dataset for segmentation evaluation experiments. A final experiment analyzes the fusion of clothes features with state-of-the-art gender classifiers in ClothesDB, revealing a significant performance improvement in gender classification. version:1
arxiv-1611-03993 | Riemannian Tensor Completion with Side Information | http://arxiv.org/abs/1611.03993 | id:1611.03993 author:Tengfei Zhou, Hui Qian, Zebang Shen, Congfu Xu category:stat.ML cs.LG cs.NA  published:2016-11-12 summary:Riemannian optimization methods have shown to be both fast and accurate in recovering a large-scale tensor from its incomplete observation. However, in almost all recent Riemannian tensor completion methods, only low rank constraint is considered. Another important fact, side information or features, remains far from exploiting within the Riemannian optimization framework. In this paper, we explicitly incorporate the side information into a Riemannian minimization model. Specifically, a feature-embedded objective is designed to substantially reduce the sample complexity. For such a Riemannian optimization, a particular metric can be constructed based on the curvature of the objective, which leads to a novel Riemannian conjugate gradient descent solver. Numerical experiments suggest that our solver is more efficient than the state-of-the-art when a highly accurate solution is required. version:1
arxiv-1611-03981 | Dual Teaching: A Practical Semi-supervised Wrapper Method | http://arxiv.org/abs/1611.03981 | id:1611.03981 author:Fuqaing Liu, Chenwei Deng, Fukun Bi, Yiding Yang category:cs.LG stat.ML  published:2016-11-12 summary:Semi-supervised wrapper methods are concerned with building effective supervised classifiers from partially labeled data. Though previous works have succeeded in some fields, it is still difficult to apply semi-supervised wrapper methods to practice because the assumptions those methods rely on tend to be unrealistic in practice. For practical use, this paper proposes a novel semi-supervised wrapper method, Dual Teaching, whose assumptions are easy to set up. Dual Teaching adopts two external classifiers to estimate the false positives and false negatives of the base learner. Only if the recall of every external classifier is greater than zero and the sum of the precision is greater than one, Dual Teaching will train a base learner from partially labeled data as effectively as the fully-labeled-data-trained classifier. The effectiveness of Dual Teaching is proved in both theory and practice. version:1
arxiv-1611-03979 | Kernel regression, minimax rates and effective dimensionality: beyond the regular case | http://arxiv.org/abs/1611.03979 | id:1611.03979 author:Gilles Blanchard, Nicole M√ºcke category:stat.ML  published:2016-11-12 summary:We investigate if kernel regularization methods can achieve minimax convergence rates over a source condition regularity assumption for the target function. These questions have been considered in past literature, but only under specific assumptions about the decay, typically polynomial, of the spectrum of the the kernel mapping covariance operator. In the perspective of distribution-free results, we investigate this issue under much weaker assumption on the eigenvalue decay, allowing for more complex behavior that can reflect different structure of the data at different scales. version:1
arxiv-1611-03969 | An Introduction to MM Algorithms for Machine Learning and Statistical | http://arxiv.org/abs/1611.03969 | id:1611.03969 author:Hien D. Nguyen category:stat.CO cs.LG stat.ML  published:2016-11-12 summary:MM (majorization--minimization) algorithms are an increasingly popular tool for solving optimization problems in machine learning and statistical estimation. This article introduces the MM algorithm framework in general and via three popular example applications: Gaussian mixture regressions, multinomial logistic regressions, and support vector machines. Specific algorithms for the three examples are derived and numerical demonstrations are presented. Theoretical and practical aspects of MM algorithm design are discussed. version:1
arxiv-1611-03968 | Autonomous Learning Framework Based on Online Hybrid Classifier for Multi-view Object Detection in Video | http://arxiv.org/abs/1611.03968 | id:1611.03968 author:Dapeng Luo, Zhipeng Zeng, Longsheng Wei, Yongwen Liu, Chen Luo, Jun Chen, Nong Sang category:cs.CV  published:2016-11-12 summary:In this paper, our proposed framework takes a remarkably different direction to resolve multi-view detection problem in a bottom-up fashion. Firstly, a state of the art scene-specific objector is obtained from a fully autonomous learning process triggered by marking several bounding boxes around the object in the first video frame via a mouse, where the human labeled training data or a generic detector are not needed. Secondly, this learning process is conveniently replicated many times in different surveillance scenes and result in a particular detector under various camera viewpoints. Thus, the proposed framework can be employed in multi-view object detection application with unsupervised manner. Obviously, the initial scene-specific detector, initialed by several bounding boxes, exhibits poor detection performance and difficult to improve by traditional online learning algorithm. Consequently, in our self-learning process, we propose to partition detection responses space by online hybrid classifiers and assign each partition individual classifier that achieves the high classification accuracy progressively. A novel online gradual learning algorithm is proposed to train the hybrid classifiers automatically and focus online learning on the hard samples, the most informative samples lying around the decision boundary. The output is a hybrid classifier based scene-specific detector which achieves decent performance under different viewing angles. Experimental results on several video datasets show that our approach achieves comparable performance to robust supervised methods and outperforms the state of the art online learning methods in varying imaging conditions. version:1
arxiv-1611-04871 | Audio Event and Scene Recognition: A Unified Approach using Strongly and Weakly Labeled Data | http://arxiv.org/abs/1611.04871 | id:1611.04871 author:Anurag Kumar, Bhiksha Raj category:cs.LG cs.CV cs.SD  published:2016-11-12 summary:In this paper we propose a novel learning framework called Supervised and Weakly Supervised Learning where the goal is to learn simultaneously from weakly and strongly labeled data. Strongly labeled data can be simply understood as fully supervised data where all labeled instances are available. In weakly supervised learning only weak labels are available. Our proposed framework is motivated by the fact that a small amount of strongly labeled data can give considerable improvement over only weakly supervised learning. The primary problem domain focus of this paper is acoustic event and scene detection in audio recordings. We first propose a naive formulation for leveraging labeled data in both forms. We then propose a more general framework for Supervised and Weakly Supervised Learning (SWSL). Based on this general framework, we propose a graph based approach for SWSL. Our main method is based on manifold regularization on graphs in which we show that the unified learning can be formulated as a constraint optimization problem which can be solved by iterative concave-convex procedure (CCCP). Our experiments show that our proposed framework can address several concerns of audio content analysis using weakly labeled data. version:1
arxiv-1611-03954 | Multi-lingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment | http://arxiv.org/abs/1611.03954 | id:1611.03954 author:Muhao Chen, Yingtao Tian, Mohan Yang, Carlo Zaniolo category:cs.AI cs.CL  published:2016-11-12 summary:Many recent works have demonstrated the benefits of knowledge graph embeddings in completing monolingual knowledge graphs. Inasmuch as related knowledge bases are built in several different languages, achieving cross-lingual knowledge alignment will help people in constructing a coherent knowledge base, and assist machines in dealing with different expressions of entity relationships across diverse human languages. Unfortunately, achieving this highly desirable crosslingual alignment by human labor is very costly and errorprone. Thus, we propose MTransE, a translation-based model for multilingual knowledge graph embeddings, to provide a simple and automated solution. By encoding entities and relations of each language in a separated embedding space, MTransE provides transitions for each embedding vector to its cross-lingual counterparts in other spaces, while preserving the functionalities of monolingual embeddings. We deploy three different techniques to represent cross-lingual transitions, namely axis calibration, translation vectors, and linear transformations, and derive five variants for MTransE using different loss functions. Our models can be trained on partially aligned graphs, where just a small portion of triples are aligned with their cross-lingual counterparts. The experiments on cross-lingual entity matching and triple-wise alignment verification show promising results, with some variants consistently outperforming others on different tasks. We also explore how MTransE preserves the key properties of its monolingual counterpart TransE. version:1
arxiv-1611-03949 | Linguistically Regularized LSTMs for Sentiment Classification | http://arxiv.org/abs/1611.03949 | id:1611.03949 author:Qiao Qian, Minlie Huang, Xiaoyan Zhu category:cs.CL  published:2016-11-12 summary:Sentiment understanding has been a long-term goal of AI in the past decades. This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed very recently, however, previous models either depend on expensive phrase-level annotation, whose performance drops substantially when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words), thus not being able to produce linguistically coherent representations. In this paper, we propose simple models trained with sentence-level annotation, but also attempt to generating linguistically coherent representations by employing regularizers that model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are effective to capture the sentiment shifting effect of sentiment, negation, and intensity words, while still obtain competitive results without sacrificing the models' simplicity. version:1
arxiv-1611-03941 | Anomaly Detection in Bitcoin Network Using Unsupervised Learning Methods | http://arxiv.org/abs/1611.03941 | id:1611.03941 author:Thai Pham, Steven Lee category:cs.LG cs.CR  published:2016-11-12 summary:The problem of anomaly detection has been studied for a long time. In short, anomalies are abnormal or unlikely things. In financial networks, thieves and illegal activities are often anomalous in nature. Members of a network want to detect anomalies as soon as possible to prevent them from harming the network's community and integrity. Many Machine Learning techniques have been proposed to deal with this problem; some results appear to be quite promising but there is no obvious superior method. In this paper, we consider anomaly detection particular to the Bitcoin transaction network. Our goal is to detect which users and transactions are the most suspicious; in this case, anomalous behavior is a proxy for suspicious behavior. To this end, we use three unsupervised learning methods including k-means clustering, Mahalanobis distance, and Unsupervised Support Vector Machine (SVM) on two graphs generated by the Bitcoin transaction network: one graph has users as nodes, and the other has transactions as nodes. version:1
arxiv-1611-03934 | Personalized Donor-Recipient Matching for Organ Transplantation | http://arxiv.org/abs/1611.03934 | id:1611.03934 author:Jinsung Yoon, Ahmed M. Alaa, Martin Cadeiras, Mihaela van der Schaar category:cs.LG  published:2016-11-12 summary:Organ transplants can improve the life expectancy and quality of life for the recipient but carries the risk of serious post-operative complications, such as septic shock and organ rejection. The probability of a successful transplant depends in a very subtle fashion on compatibility between the donor and the recipient but current medical practice is short of domain knowledge regarding the complex nature of recipient-donor compatibility. Hence a data-driven approach for learning compatibility has the potential for significant improvements in match quality. This paper proposes a novel system (ConfidentMatch) that is trained using data from electronic health records. ConfidentMatch predicts the success of an organ transplant (in terms of the 3 year survival rates) on the basis of clinical and demographic traits of the donor and recipient. ConfidentMatch captures the heterogeneity of the donor and recipient traits by optimally dividing the feature space into clusters and constructing different optimal predictive models to each cluster. The system controls the complexity of the learned predictive model in a way that allows for assuring more granular and confident predictions for a larger number of potential recipient-donor pairs, thereby ensuring that predictions are "personalized" and tailored to individual characteristics to the finest possible granularity. Experiments conducted on the UNOS heart transplant dataset show the superiority of the prognostic value of ConfidentMatch to other competing benchmarks; ConfidentMatch can provide predictions of success with 95% confidence for 5,489 patients of a total population of 9,620 patients, which corresponds to 410 more patients than the most competitive benchmark algorithm (DeepBoost). version:1
arxiv-1611-03932 | Training IBM Watson using Automatically Generated Question-Answer Pairs | http://arxiv.org/abs/1611.03932 | id:1611.03932 author:Jangho Lee, Gyuwan Kim, Jaeyoon Yoo, Changwoo Jung, Minseok Kim, Sungroh Yoon category:cs.CL  published:2016-11-12 summary:IBM Watson is a cognitive computing system capable of question answering in natural languages. It is believed that IBM Watson can understand large corpora and answer relevant questions more effectively than any other question-answering system currently available. To unleash the full power of Watson, however, we need to train its instance with a large number of well-prepared question-answer pairs. Obviously, manually generating such pairs in a large quantity is prohibitively time consuming and significantly limits the efficiency of Watson's training. Recently, a large-scale dataset of over 30 million question-answer pairs was reported. Under the assumption that using such an automatically generated dataset could relieve the burden of manual question-answer generation, we tried to use this dataset to train an instance of Watson and checked the training efficiency and accuracy. According to our experiments, using this auto-generated dataset was effective for training Watson, complementing manually crafted question-answer pairs. To the best of the authors' knowledge, this work is the first attempt to use a large-scale dataset of automatically generated question-answer pairs for training IBM Watson. We anticipate that the insights and lessons obtained from our experiments will be useful for researchers who want to expedite Watson training leveraged by automatically generated question-answer pairs. version:1
arxiv-1611-03915 | When Fashion Meets Big Data: Discriminative Mining of Best Selling Clothing Features | http://arxiv.org/abs/1611.03915 | id:1611.03915 author:Kuan-Ting Chen, Jiebo Luo category:cs.CV  published:2016-11-11 summary:With the prevalence of e-commence websites and the ease of online shopping, consumers are embracing huge amounts of various options in products. Undeniably, shopping is one of the most essential activities in our society and studying consumer's shopping behavior is important for the industry as well as sociology and psychology. Not surprisingly, one of the most popular e-commerce categories is clothing business. There arises the needs for analysis of popular and attractive clothing features which could further boost many emerging applications, such as clothing recommendation and advertising. In this work, we design a novel system that consists of three major components: 1) exploring and organizing a large-scale clothing dataset from a online shopping website, 2) pruning and extracting images of best-selling products in clothing item data and user transaction history, and 3) utilizing a machine learning based approach to discovering clothing attributes as the representative and discriminative characteristics of popular clothing style elements. Through the experiments over a large-scale online clothing dataset, we demonstrate the effectiveness of our proposed system, and obtain useful insights on clothing consumption trends and profitable clothing features. version:1
arxiv-1611-01628 | Reference-Aware Language Models | http://arxiv.org/abs/1611.01628 | id:1611.01628 author:Zichao Yang, Phil Blunsom, Chris Dyer, Wang Ling category:cs.CL  published:2016-11-05 summary:We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or discourse context, even when the targets of the reference may be rare words. Experiments on three tasks shows our model variants based on deterministic attention. version:2
arxiv-1611-03907 | Reinforcement Learning of Contextual MDPs using Spectral Methods | http://arxiv.org/abs/1611.03907 | id:1611.03907 author:Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar category:cs.AI cs.LG stat.ML  published:2016-11-11 summary:We propose a new reinforcement learning (RL) algorithm for contextual Markov decision processes (CMDP) using spectral methods. CMDPs are structured MDPs where the dynamics and rewards depend on a smaller number of hidden states or contexts. If the mapping between the hidden and observed states is known a priori, then standard RL algorithms such as UCRL are guaranteed to attain low regret. Is it possible to achieve regret of the same order even when the mapping is unknown? We provide an affirmative answer in this paper. We exploit spectral methods to learn the mapping from hidden to observed states with guaranteed confidence bounds, and incorporate it into the UCRL-based framework to obtain order-optimal regret. version:1
arxiv-1611-03898 | Low Latency Anomaly Detection and Bayesian Network Prediction of Anomaly Likelihood | http://arxiv.org/abs/1611.03898 | id:1611.03898 author:Derek Farren, Thai Pham, Marco Alban-Hidalgo category:cs.LG stat.ML  published:2016-11-11 summary:We develop a supervised machine learning model that detects anomalies in systems in real time. Our model processes unbounded streams of data into time series which then form the basis of a low-latency anomaly detection model. Moreover, we extend our preliminary goal of just anomaly detection to simultaneous anomaly prediction. We approach this very challenging problem by developing a Bayesian Network framework that captures the information about the parameters of the lagged regressors calibrated in the first part of our approach and use this structure to learn local conditional probability distributions. version:1
arxiv-1611-03894 | Unsupervised Learning For Effective User Engagement on Social Media | http://arxiv.org/abs/1611.03894 | id:1611.03894 author:Thai Pham, Camelia Simoiu category:cs.LG  published:2016-11-11 summary:In this paper, we investigate the effectiveness of unsupervised feature learning techniques in predicting user engagement on social media. Specifically, we compare two methods to predict the number of feedbacks (i.e., comments) that a blog post is likely to receive. We compare Principal Component Analysis (PCA) and sparse Autoencoder to a baseline method where the data are only centered and scaled, on each of two models: Linear Regression and Regression Tree. We find that unsupervised learning techniques significantly improve the prediction accuracy on both models. For the Linear Regression model, sparse Autoencoder achieves the best result, with an improvement in the root mean squared error (RMSE) on the test set of 42% over the baseline method. For the Regression Tree model, PCA achieves the best result, with an improvement in RMSE of 15% over the baseline. version:1
arxiv-1611-03879 | Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM | http://arxiv.org/abs/1611.03879 | id:1611.03879 author:Chun-Liang Li, Siamak Ravanbakhsh, Barnabas Poczos category:stat.ML cs.LG  published:2016-11-11 summary:Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to numerical stability and quantifiability of the likelihood, RBM is commonly used with Bernoulli units. Here, we consider an alternative member of exponential family RBM with leaky rectified linear units -- called leaky RBM. We first study the joint and marginal distributions of leaky RBM under different leakiness, which provides us important insights by connecting the leaky RBM model and truncated Gaussian distributions. The connection leads us to a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; -- i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations. This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and more accurate than the commonly used annealed importance sampling (AIS). We further demonstrate that the proposed sampling algorithm enjoys faster mixing property than contrastive divergence algorithm, which benefits the training without any additional computational cost. version:1
arxiv-1611-03873 | Effective sparse representation of X-Ray medical images | http://arxiv.org/abs/1611.03873 | id:1611.03873 author:Laura Rebollo-Neira category:cs.CV  published:2016-11-11 summary:Effective sparse representation of X-Ray medical images within the context of data reduction is considered. The proposed framework is shown to render an enormous reduction in the cardinality of the data set required to represent this class of images at very good quality. The particularity of the approach is that it can be implemented at very competitive processing time and low memory requirements version:1
arxiv-1611-01491 | Understanding Deep Neural Networks with Rectified Linear Units | http://arxiv.org/abs/1611.01491 | id:1611.01491 author:Raman Arora, Amitabh Basu, Poorya Mianjy, Anirbit Mukherjee category:cs.LG cond-mat.dis-nn cs.AI cs.CC stat.ML  published:2016-11-04 summary:In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial (in data size and circuit size) time algorithm to train a ReLU DNN with one hidden layer and a single input to global optimality. This follows from our complete characterization of the ReLU DNN function class whereby we show that a $\mathbb{R}^n \to \mathbb{R}$ function is representable by a ReLU DNN if and only if it is a continuous piecewise linear function. The main tool used to prove this characterization is an elegant result from tropical geometry. Further, for the $n=1$ case, we show that a single hidden layer suffices to express all piecewise linear functions, and we give tight bounds for the size of such a ReLU DNN.We follow up with gap results showing that there is a smoothly parameterized family of $\mathbb{R}\to \mathbb{R}$ "hard" functions that lead to an exponential blow-up in size, if the number of layers is decreased by a small amount. An example consequence of our gap theorem is that for every natural number $N$, there exists a function representable by a ReLU DNN with depth $N^2+1$ and total size $N^3$, such that any ReLU DNN with depth at most $N + 1$ will require at least $\frac12N^{N+1}-1$ total nodes. Finally, we construct a family of $\mathbb{R}^n\to \mathbb{R}$ functions for $n\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of "hard" functions in the neural nets literature. version:2
arxiv-1611-03824 | Learning to Learn for Global Optimization of Black Box Functions | http://arxiv.org/abs/1611.03824 | id:1611.03824 author:Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Nando de Freitas category:stat.ML cs.LG  published:2016-11-11 summary:We present a learning to learn approach for training recurrent neural networks to perform black-box global optimization. In the meta-learning phase we use a large set of smooth target functions to learn a recurrent neural network (RNN) optimizer, which is either a long-short term memory network or a differentiable neural computer. After learning, the RNN can be applied to learn policies in reinforcement learning, as well as other black-box learning tasks, including continuous correlated bandits and experimental design. We compare this approach to Bayesian optimization, with emphasis on the issues of computation speed, horizon length, and exploration-exploitation trade-offs. version:1
arxiv-1611-03819 | Recovery Guarantee of Non-negative Matrix Factorization via Alternating Updates | http://arxiv.org/abs/1611.03819 | id:1611.03819 author:Yuanzhi Li, Yingyu Liang, Andrej Risteski category:cs.LG cs.DS stat.ML  published:2016-11-11 summary:Non-negative matrix factorization is a popular tool for decomposing data into feature and weight matrices under non-negativity constraints. It enjoys practical success but is poorly understood theoretically. This paper proposes an algorithm that alternates between decoding the weights and updating the features, and shows that assuming a generative model of the data, it provably recovers the ground-truth under fairly mild conditions. In particular, its only essential requirement on features is linear independence. Furthermore, the algorithm uses ReLU to exploit the non-negativity for decoding the weights, and thus can tolerate adversarial noise that can potentially be as large as the signal, and can tolerate unbiased noise much larger than the signal. The analysis relies on a carefully designed coupling between two potential functions, which we believe is of independent interest. version:1
arxiv-1611-03814 | Towards the Science of Security and Privacy in Machine Learning | http://arxiv.org/abs/1611.03814 | id:1611.03814 author:Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, Michael Wellman category:cs.CR cs.LG  published:2016-11-11 summary:Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive---new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used. version:1
arxiv-1611-03811 | HoneyFaces: Increasing the Security and Privacy of Authentication Using Synthetic Facial Images | http://arxiv.org/abs/1611.03811 | id:1611.03811 author:Mor Ohana, Orr Dunkelman, Stuart Gibson, Margarita Osadchy category:cs.CR cs.CV  published:2016-11-11 summary:One of the main challenges faced by Biometric-based authentication systems is the need to offer secure authentication while maintaining the privacy of the biometric data. Previous solutions, such as Secure Sketch and Fuzzy Extractors, rely on assumptions that cannot be guaranteed in practice, and often affect the authentication accuracy. In this paper, we introduce HoneyFaces: the concept of adding a large set of synthetic faces (indistinguishable from real) into the biometric "password file". This password inflation protects the privacy of users and increases the security of the system without affecting the accuracy of the authentication. In particular, privacy for the real users is provided by "hiding" them among a large number of fake users (as the distributions of synthetic and real faces are equal). In addition to maintaining the authentication accuracy, and thus not affecting the security of the authentication process, HoneyFaces offer several security improvements: increased exfiltration hardness, improved leakage detection, and the ability to use a Two-server setting like in HoneyWords. Finally, HoneyFaces can be combined with other security and privacy mechanisms for biometric data. We implemented the HoneyFaces system and tested it with a password file composed of 270 real users. The "password file" was then inflated to accommodate up to $2^{36.5}$ users (resulting in a 56.6 TB "password file"). At the same time, the inclusion of additional faces does not affect the true acceptance rate or false acceptance rate which were 93.33\% and 0.01\%, respectively. version:1
arxiv-1611-03787 | Understanding the 2016 US Presidential Election using ecological inference and distribution regression with census microdata | http://arxiv.org/abs/1611.03787 | id:1611.03787 author:Seth Flaxman, Dougal Sutherland, Yu-Xiang Wang, Yee Whye Teh category:stat.AP stat.ML  published:2016-11-11 summary:We combine fine-grained spatially referenced census data with the vote outcomes from the 2016 US presidential election. Using this dataset, we perform ecological inference using distribution regression (Flaxman et al, KDD 2015) with a multinomial-logit regression so as to model the vote outcome Trump, Clinton, Other / Didn't vote as a function of demographic and socioeconomic features. Ecological inference allows us to estimate "exit poll" style results like what was Trump's support among white women, but for entirely novel categories. We also perform exploratory data analysis to understand which census variables are predictive of voting for Trump, voting for Clinton, or not voting for either. All of our methods are implemented in python and R and are available online for replication. version:1
arxiv-1611-03749 | MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors | http://arxiv.org/abs/1611.03749 | id:1611.03749 author:Ertunc Erdil, Sinan Yƒ±ldƒ±rƒ±m, M√ºjdat √áetin, Tolga Ta≈üdizen category:cs.CV  published:2016-11-11 summary:Segmenting images of low quality or with missing data is a challenging problem. Integrating statistical prior information about the shapes to be segmented can improve the segmentation results significantly. Most shape-based segmentation algorithms optimize an energy functional and find a point estimate for the object to be segmented. This does not provide a measure of the degree of confidence in that result, neither does it provide a picture of other probable solutions based on the data and the priors. With a statistical view, addressing these issues would involve the problem of characterizing the posterior densities of the shapes of the objects to be segmented. For such characterization, we propose a Markov chain Monte Carlo (MCMC) sampling-based image segmentation algorithm that uses statistical shape priors. In addition to better characterization of the statistical structure of the problem, such an approach would also have the potential to address issues with getting stuck at local optima, suffered by existing shape-based segmentation methods. Our approach is able to characterize the posterior probability density in the space of shapes through its samples, and to return multiple solutions, potentially from different modes of a multimodal probability density, which would be encountered, e.g., in segmenting objects from multiple shape classes. We present promising results on a variety of data sets. We also provide an extension for segmenting shapes of objects with parts that can go through independent shape variations. This extension involves the use of local shape priors on object parts and provides robustness to limitations in shape training data size. version:1
arxiv-1611-02956 | A Comparison of Word Embeddings for English and Cross-Lingual Chinese Word Sense Disambiguation | http://arxiv.org/abs/1611.02956 | id:1611.02956 author:Hong Jin Kang, Tao Chen, Muthu Kumar Chandrasekaran, Min-Yen Kan category:cs.CL  published:2016-11-09 summary:Word embeddings are now ubiquitous forms of word representation in natural language processing. There have been applications of word embeddings for monolingual word sense disambiguation (WSD) in English, but few comparisons have been done. This paper attempts to bridge that gap by examining popular embeddings for the task of monolingual English WSD. Our simplified method leads to comparable state-of-the-art performance without expensive retraining. Cross-Lingual WSD - where the word senses of a word in a source language e come from a separate target translation language f - can also assist in language learning; for example, when providing translations of target vocabulary for learners. Thus we have also applied word embeddings to the novel task of cross-lingual WSD for Chinese and provide a public dataset for further benchmarking. We have also experimented with using word embeddings for LSTM networks and found surprisingly that a basic LSTM network does not work well. We discuss the ramifications of this outcome. version:2
arxiv-1611-03718 | Hierarchical Object Detection with Deep Reinforcement Learning | http://arxiv.org/abs/1611.03718 | id:1611.03718 author:Miriam Bellver, Xavier Giro-i-Nieto, Ferran Marques, Jordi Torres category:cs.CV cs.LG  published:2016-11-11 summary:We present a method for performing hierarchical object detection in images guided by a deep reinforcement learning agent. The key idea is to focus on those parts of the image that contain richer information and zoom on them. We train an intelligent agent that, given an image window, is capable of deciding where to focus the attention among five different predefined region candidates (smaller windows). This procedure is iterated providing a hierarchical image analysis.We compare two different candidate proposal strategies to guide the object search: with and without overlap. Moreover, our work compares two different strategies to extract features from a convolutional neural network for each region proposal: a first one that computes new feature maps for each region proposal, and a second one that computes the feature maps for the whole image to later generate crops for each region proposal. Experiments indicate better results for the overlapping candidate proposal strategy and a loss of performance for the cropped image features due to the loss of spatial resolution. We argue that, while this loss seems unavoidable when working with large amounts of object candidates, the much more reduced amount of region proposals generated by our reinforcement learning agent allows considering to extract features for each location without sharing convolutional computation among regions. version:1
arxiv-1611-01457 | Multi-task learning with deep model based reinforcement learning | http://arxiv.org/abs/1611.01457 | id:1611.01457 author:Asier Mujika category:cs.LG  published:2016-11-04 summary:In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a new kind of recurrent neural network inspired by residual networks that decouples memory from computation allowing to model complex environments that do not require lots of memory. The code will be released before ICLR 2017. version:2
arxiv-1611-03679 | Deep Convolutional Neural Network for Inverse Problems in Imaging | http://arxiv.org/abs/1611.03679 | id:1611.03679 author:Kyong Hwan Jin, Michael T. McCann, Emmanuel Froustey, Michael Unser category:cs.CV  published:2016-11-11 summary:In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyper parameter selection. The starting point of our work is the observation that unrolled iterative methods have the form of a CNN (filtering followed by point-wise non-linearity) when the normal operator (H*H, the adjoint of H times H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill-posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 x 512 image on GPU. version:1
arxiv-1611-03363 | Proceedings of the Workshop on Brain Analysis using COnnectivity Networks - BACON 2016 | http://arxiv.org/abs/1611.03363 | id:1611.03363 author:Sarah Parisot, Jonathan Passerat-Palmbach, Markus D. Schirmer, Boris Gutman category:cs.NE  published:2016-11-10 summary:Understanding brain connectivity in a network-theoretic context has shown much promise in recent years. This type of analysis identifies brain organisational principles, bringing a new perspective to neuroscience. At the same time, large public databases of connectomic data are now available. However, connectome analysis is still an emerging field and there is a crucial need for robust computational methods to fully unravelits potential. This workshop provides a platform to discuss the development of new analytic techniques; methods for evaluating and validating commonly used approaches; as well as the effects of variations in pre-processing steps. version:2
arxiv-1611-03673 | Learning to Navigate in Complex Environments | http://arxiv.org/abs/1611.03673 | id:1611.03673 author:Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, Raia Hadsell category:cs.AI cs.CV cs.LG cs.RO  published:2016-11-11 summary:Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks to bootstrap learning. In particular we consider jointly learning the goal-driven reinforcement learning problem with an unsupervised depth prediction task and a self-supervised loop closure classification task. Using this approach we can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics. We then show that the agent implicitly learns key navigation abilities, through reinforcement learning with sparse rewards and without direct supervision. version:1
arxiv-1611-03666 | Oriented bounding boxes using multiresolution contours for fast interference detection of arbitrary geometry objects | http://arxiv.org/abs/1611.03666 | id:1611.03666 author:L. A. Rivera, Vania V. Estrela, P. C. P. Carvalho category:cs.GR cs.CV  published:2016-11-11 summary:Interference detection of arbitrary geometric objects is not a trivial task due to the heavy computational load imposed by implementation issues. The hierarchically structured bounding boxes help us to quickly isolate the contour of segments in interference. In this paper, a new approach is introduced to treat the interference detection problem involving the representation of arbitrary shaped objects. Our proposed method relies upon searching for the best possible way to represent contours by means of hierarchically structured rectangular oriented bounding boxes. This technique handles 2D objects boundaries defined by closed B-spline curves with roughness details. Each oriented box is adapted and fitted to the segments of the contour using second order statistical indicators from some elements of the segments of the object contour in a multiresolution framework. Our method is efficient and robust when it comes to 2D animations in real time. It can deal with smooth curves and polygonal approximations as well results are present to illustrate the performance of the new method. version:1
arxiv-1611-02588 | Contradiction Detection for Rumorous Claims | http://arxiv.org/abs/1611.02588 | id:1611.02588 author:Piroska Lendvai, Uwe D. Reichel category:cs.CL  published:2016-11-08 summary:The utilization of social media material in journalistic workflows is increasing, demanding automated methods for the identification of mis- and disinformation. Since textual contradiction across social media posts can be a signal of rumorousness, we seek to model how claims in Twitter posts are being textually contradicted. We identify two different contexts in which contradiction emerges: its broader form can be observed across independently posted tweets and its more specific form in threaded conversations. We define how the two scenarios differ in terms of central elements of argumentation: claims and conversation structure. We design and evaluate models for the two scenarios uniformly as 3-way Recognizing Textual Entailment tasks in order to represent claims and conversation structure implicitly in a generic inference model, while previous studies used explicit or no representation of these properties. To address noisy text, our classifiers use simple similarity features derived from the string and part-of-speech level. Corpus statistics reveal distribution differences for these features in contradictory as opposed to non-contradictory tweet relations, and the classifiers yield state of the art performance. version:2
arxiv-1611-03641 | Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure | http://arxiv.org/abs/1611.03641 | id:1611.03641 author:Oded Avraham, Yoav Goldberg category:cs.CL  published:2016-11-11 summary:We suggest a new method for creating and using gold-standard datasets for word similarity evaluation. Our goal is to improve the reliability of the evaluation, and we do this by redesigning the annotation task to achieve higher inter-rater agreement, and by defining a performance measure which takes the reliability of each annotation decision in the dataset into account. version:1
arxiv-1611-03608 | Greedy Step Averaging: A parameter-free stochastic optimization method | http://arxiv.org/abs/1611.03608 | id:1611.03608 author:Xiatian Zhang, Fan Yao, Yongjun Tian category:cs.LG  published:2016-11-11 summary:In this paper we present the greedy step averaging(GSA) method, a parameter-free stochastic optimization algorithm for a variety of machine learning problems. As a gradient-based optimization method, GSA makes use of the information from the minimizer of a single sample's loss function, and takes average strategy to calculate reasonable learning rate sequence. While most existing gradient-based algorithms introduce an increasing number of hyper parameters or try to make a trade-off between computational cost and convergence rate, GSA avoids the manual tuning of learning rate and brings in no more hyper parameters or extra cost. We perform exhaustive numerical experiments for logistic and softmax regression to compare our method with the other state of the art ones on 16 datasets. Results show that GSA is robust on various scenarios. version:1
arxiv-1611-03607 | Deep Recurrent Neural Network for Mobile Human Activity Recognition with High Throughput | http://arxiv.org/abs/1611.03607 | id:1611.03607 author:Masaya Inoue, Sozo Inoue, Takeshi Nishida category:cs.CV cs.NE  published:2016-11-11 summary:In this paper, we propose a method of human activity recognition with high throughput from raw accelerometer data applying a deep recurrent neural network (DRNN), and investigate various architectures and its combination to find the best parameter values. The "high throughput" refers to short time at a time of recognition. We investigated various parameters and architectures of the DRNN by using the training dataset of 432 trials with 6 activity classes from 7 people. The maximum recognition rate was 95.42% and 83.43% against the test data of 108 segmented trials each of which has single activity class and 18 multiple sequential trials, respectively. Here, the maximum recognition rates by traditional methods were 71.65% and 54.97% for each. In addition, the efficiency of the found parameters was evaluated by using additional dataset. Further, as for throughput of the recognition per unit time, the constructed DRNN was requiring only 1.347 [ms], while the best traditional method required 11.031 [ms] which includes 11.027 [ms] for feature calculation. These advantages are caused by the compact and small architecture of the constructed real time oriented DRNN. version:1
arxiv-1611-03599 | UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text | http://arxiv.org/abs/1611.03599 | id:1611.03599 author:Wei-Fan Chen, Lun-Wei Ku category:cs.CL cs.AI cs.LG  published:2016-11-11 summary:Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform. version:1
arxiv-1611-03596 | Generalized Entropies and the Similarity of Texts | http://arxiv.org/abs/1611.03596 | id:1611.03596 author:Eduardo G. Altmann, Laercio Dias, Martin Gerlach category:physics.soc-ph cs.CL  published:2016-11-11 summary:We show how generalized Gibbs-Shannon entropies can provide new insights on the statistical properties of texts. The universal distribution of word frequencies (Zipf's law) implies that the generalized entropies, computed at the word level, are dominated by words in a specific range of frequencies. Here we show that this is the case not only for the generalized entropies but also for the generalized (Jensen-Shannon) divergences, used to compute the similarity between different texts. This finding allows us to identify the contribution of specific words (and word frequencies) for the different generalized entropies and also to estimate the size of the databases needed to obtain a reliable estimation of the divergences. We test our results in large databases of books (from the Google n-gram database) and scientific papers (indexed by Web of Science). version:1
arxiv-1611-03591 | Learning Multi-Scale Deep Features for High-Resolution Satellite Image Classification | http://arxiv.org/abs/1611.03591 | id:1611.03591 author:Qingshan Liu, Renlong Hang, Huihui Song, Zhi Li category:cs.CV  published:2016-11-11 summary:In this paper, we propose a multi-scale deep feature learning method for high-resolution satellite image classification. Specifically, we firstly warp the original satellite image into multiple different scales. The images in each scale are employed to train a deep convolutional neural network (DCNN). However, simultaneously training multiple DCNNs is time-consuming. To address this issue, we explore DCNN with spatial pyramid pooling (SPP-net). Since different SPP-nets have the same number of parameters, which share the identical initial values, and only fine-tuning the parameters in fully-connected layers ensures the effectiveness of each network, thereby greatly accelerating the training process. Then, the multi-scale satellite images are fed into their corresponding SPP-nets respectively to extract multi-scale deep features. Finally, a multiple kernel learning method is developed to automatically learn the optimal combination of such features. Experiments on two difficult datasets show that the proposed method achieves favorable performance compared to other state-of-the-art methods. version:1
arxiv-1611-03589 | Adaptive Deep Pyramid Matching for Remote Sensing Scene Classification | http://arxiv.org/abs/1611.03589 | id:1611.03589 author:Qingshan Liu, Renlong Hang, Huihui Song, Fuping Zhu, Javier Plaza, Antonio Plaza category:cs.CV  published:2016-11-11 summary:Convolutional neural networks (CNNs) have attracted increasing attention in the remote sensing community. Most CNNs only take the last fully-connected layers as features for the classification of remotely sensed images, discarding the other convolutional layer features which may also be helpful for classification purposes. In this paper, we propose a new adaptive deep pyramid matching (ADPM) model that takes advantage of the features from all of the convolutional layers for remote sensing image classification. To this end, the optimal fusing weights for different convolutional layers are learned from the data itself. In remotely sensed scenes, the objects of interest exhibit different scales in distinct scenes, and even a single scene may contain objects with different sizes. To address this issue, we select the CNN with spatial pyramid pooling (SPP-net) as the basic deep network, and further construct a multi-scale ADPM model to learn complementary information from multi-scale images. Our experiments have been conducted using two widely used remote sensing image databases, and the results show that the proposed method significantly improves the performance when compared to other state-of-the-art methods. version:1
arxiv-1611-03579 | Collision-based Testers are Optimal for Uniformity and Closeness | http://arxiv.org/abs/1611.03579 | id:1611.03579 author:Ilias Diakonikolas, Themis Gouleakis, John Peebles, Eric Price category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH  published:2016-11-11 summary:We study the fundamental problems of (i) uniformity testing of a discrete distribution, and (ii) closeness testing between two discrete distributions with bounded $\ell_2$-norm. These problems have been extensively studied in distribution testing and sample-optimal estimators are known for them~\cite{Paninski:08, CDVV14, VV14, DKN:15}. In this work, we show that the original collision-based testers proposed for these problems ~\cite{GRdist:00, BFR+:00} are sample-optimal, up to constant factors. Previous analyses showed sample complexity upper bounds for these testers that are optimal as a function of the domain size $n$, but suboptimal by polynomial factors in the error parameter $\epsilon$. Our main contribution is a new tight analysis establishing that these collision-based testers are information-theoretically optimal, up to constant factors, both in the dependence on $n$ and in the dependence on $\epsilon$. version:1
arxiv-1611-03578 | Simple and Efficient Parallelization for Probabilistic Temporal Tensor Factorization | http://arxiv.org/abs/1611.03578 | id:1611.03578 author:Guangxi Li, Zenglin Xu, Linnan Wang, Jinmian Ye, Irwin King, Michael Lyu category:stat.ML cs.LG  published:2016-11-11 summary:Probabilistic Temporal Tensor Factorization (PTTF) is an effective algorithm to model the temporal tensor data. It leverages a time constraint to capture the evolving properties of tensor data. Nowadays the exploding dataset demands a large scale PTTF analysis, and a parallel solution is critical to accommodate the trend. Whereas, the parallelization of PTTF still remains unexplored. In this paper, we propose a simple yet efficient Parallel Probabilistic Temporal Tensor Factorization, referred to as P$^2$T$^2$F, to provide a scalable PTTF solution. P$^2$T$^2$F is fundamentally disparate from existing parallel tensor factorizations by considering the probabilistic decomposition and the temporal effects of tensor data. It adopts a new tensor data split strategy to subdivide a large tensor into independent sub-tensors, the computation of which is inherently parallel. We train P$^2$T$^2$F with an efficient algorithm of stochastic Alternating Direction Method of Multipliers, and show that the convergence is guaranteed. Experiments on several real-word tensor datasets demonstrate that P$^2$T$^2$F is a highly effective and efficiently scalable algorithm dedicated for large scale probabilistic temporal tensor analysis. version:1
arxiv-1611-03566 | Construction Inspection through Spatial Database | http://arxiv.org/abs/1611.03566 | id:1611.03566 author:Ahmad Hasan, Ashraf Qadir, Ian Nordeng, Jeremiah Neubert category:cs.CV  published:2016-11-11 summary:This paper presents the development of an efficient set of tools for extracting information from the video of a structure captured by an Unmanned Aircraft System (UAS) to produce as-built documentation to aid inspection of large multi-storied building during construction. Our system uses the output from a sequential structure from motion system and a 3D CAD model of the structure in order to construct a spatial database to store images into the 3D CAD model space. This allows the user to perform a spatial query for images through spatial indexing into the 3D CAD model space. The image returned by the spatial query is used to extract metric information and perform crack detection on the brick pattern. The spatial database is also used to generate a 3D textured model which provides a visual as-built documentation. version:1
arxiv-1611-03558 | Neural Networks Models for Entity Discovery and Linking | http://arxiv.org/abs/1611.03558 | id:1611.03558 author:Dan Liu, Wei Lin, Shiliang Zhang, Si Wei, Hui Jiang category:cs.CL cs.AI cs.IR  published:2016-11-11 summary:This paper describes the USTC_NELSLIP systems submitted to the Trilingual Entity Detection and Linking (EDL) track in 2016 TAC Knowledge Base Population (KBP) contests. We have built two systems for entity discovery and mention detection (MD): one uses the conditional RNNLM and the other one uses the attention-based encoder-decoder framework. The entity linking (EL) system consists of two modules: a rule based candidate generation and a neural networks probability ranking model. Moreover, some simple string matching rules are used for NIL clustering. At the end, our best system has achieved an F1 score of 0.624 in the end-to-end typed mention ceaf plus metric. version:1
arxiv-1611-02590 | Veracity Computing from Lexical Cues and Perceived Certainty Trends | http://arxiv.org/abs/1611.02590 | id:1611.02590 author:Uwe D. Reichel, Piroska Lendvai category:cs.CL  published:2016-11-08 summary:We present a data-driven method for determining the veracity of a set of rumorous claims on social media data. Tweets from different sources pertaining to a rumor are processed on three levels: first, factuality values are assigned to each tweet based on four textual cue categories relevant for our journalism use case; these amalgamate speaker support in terms of polarity and commitment in terms of certainty and speculation. Next, the proportions of these lexical cues are utilized as predictors for tweet certainty in a generalized linear regression model. Subsequently, lexical cue proportions, predicted certainty, as well as their time course characteristics are used to compute veracity for each rumor in terms of the identity of the rumor-resolving tweet and its binary resolution value judgment. The system operates without access to extralinguistic resources. Evaluated on the data portion for which hand-labeled examples were available, it achieves .74 F1-score on identifying rumor resolving tweets and .76 F1-score on predicting if a rumor is resolved as true or false. version:2
arxiv-1611-01587 | A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks | http://arxiv.org/abs/1611.01587 | id:1611.01587 author:Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher category:cs.CL cs.AI  published:2016-11-05 summary:Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search. version:2
arxiv-1611-03553 | The Sum-Product Theorem: A Foundation for Learning Tractable Models | http://arxiv.org/abs/1611.03553 | id:1611.03553 author:Abram L. Friesen, Pedro Domingos category:cs.LG cs.AI  published:2016-11-11 summary:Inference in expressive probabilistic models is generally intractable, which makes them difficult to learn and limits their applicability. Sum-product networks are a class of deep models where, surprisingly, inference remains tractable even when an arbitrary number of hidden layers are present. In this paper, we generalize this result to a much broader set of learning problems: all those where inference consists of summing a function over a semiring. This includes satisfiability, constraint satisfaction, optimization, integration, and others. In any semiring, for summation to be tractable it suffices that the factors of every product have disjoint scopes. This unifies and extends many previous results in the literature. Enforcing this condition at learning time thus ensures that the learned models are tractable. We illustrate the power and generality of this approach by applying it to a new type of structured prediction problem: learning a nonconvex function that can be globally optimized in polynomial time. We show empirically that this greatly outperforms the standard approach of learning without regard to the cost of optimization. version:1
arxiv-1611-03533 | Landmark-based consonant voicing detection on multilingual corpora | http://arxiv.org/abs/1611.03533 | id:1611.03533 author:Xiang Kong, Xuesong Yang, Mark Hasegawa-Johnson, Jeung-Yoon Choi, Stefanie Shattuck-Hufnagel category:cs.CL cs.SD  published:2016-11-10 summary:This paper tests the hypothesis that distinctive feature classifiers anchored at phonetic landmarks can be transferred cross-lingually without loss of accuracy. Three consonant voicing classifiers were developed: (1) manually selected acoustic features anchored at a phonetic landmark, (2) MFCCs (either averaged across the segment or anchored at the landmark), and(3) acoustic features computed using a convolutional neural network (CNN). All detectors are trained on English data (TIMIT),and tested on English, Turkish, and Spanish (performance measured using F1 and accuracy). Experiments demonstrate that manual features outperform all MFCC classifiers, while CNNfeatures outperform both. MFCC-based classifiers suffer an F1reduction of 16% absolute when generalized from English to other languages. Manual features suffer only a 5% F1 reduction,and CNN features actually perform better in Turkish and Span-ish than in the training language, demonstrating that features capable of representing long-term spectral dynamics (CNN and landmark-based features) are able to generalize cross-lingually with little or no loss of accuracy version:1
arxiv-1611-03531 | Estimating Dynamic Treatment Regimes in Mobile Health Using V-learning | http://arxiv.org/abs/1611.03531 | id:1611.03531 author:Daniel J. Luckett, Eric B. Laber, Anna R. Kahkoska, David M. Maahs, Elizabeth Mayer-Davis, Michael R. Kosorok category:stat.ML  published:2016-11-10 summary:The vision for precision medicine is to use individual patient characteristics to inform a personalized treatment plan that leads to the best healthcare possible for each patient. Mobile technologies have an important role to play in this vision as they offer a means to monitor a patient's health status in real-time and subsequently to deliver interventions if, when, and in the dose that they are needed. Dynamic treatment regimes formalize individualized treatment plans as sequences of decision rules, one per stage of clinical intervention, that map current patient information to a recommended treatment. However, existing methods for estimating optimal dynamic treatment regimes are designed for a small number of fixed decision points occurring on a coarse time-scale. We propose a new reinforcement learning method for estimating an optimal treatment regime that is applicable to data collected using mobile technologies in an outpatient setting. The proposed method accommodates an indefinite time horizon and minute-by-minute decision making that are common in mobile health applications. We show the proposed estimators are consistent and asymptotically normal under mild conditions. The proposed methods are applied to estimate an optimal dynamic treatment regime for controlling blood glucose levels in patients with type 1 diabetes. version:1
arxiv-1611-03530 | Understanding deep learning requires rethinking generalization | http://arxiv.org/abs/1611.03530 | id:1611.03530 author:Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals category:cs.LG  published:2016-11-10 summary:Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. version:1
arxiv-1611-03473 | Statistical Query Lower Bounds for Robust Estimation of High-dimensional Gaussians and Gaussian Mixtures | http://arxiv.org/abs/1611.03473 | id:1611.03473 author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.LG cs.CC cs.DS cs.IT math.IT math.ST stat.TH  published:2016-11-10 summary:We prove the first {\em Statistical Query lower bounds} for two fundamental high-dimensional learning problems involving Gaussian distributions: (1) learning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of a single unknown mean Gaussian. In particular, we show a {\em super-polynomial gap} between the (information-theoretic) sample complexity and the complexity of {\em any} Statistical Query algorithm for these problems. Our SQ lower bound for Problem (1) implies that -- as far as SQ algorithms are concerned -- the computational complexity of learning GMMs is inherently exponential {\em in the dimension of the latent space} -- even though there is no such information-theoretic barrier. Our lower bound for Problem (2) implies that the accuracy of the robust learning algorithm in~\cite{DiakonikolasKKLMS16} is essentially best possible among all polynomial-time SQ algorithms. On the positive side, we give a new SQ learning algorithm for this problem with optimal accuracy whose running time nearly matches our lower bound. Both our SQ lower bounds are attained via a unified moment-matching technique that may be useful in other contexts. Our SQ learning algorithm for Problem (2) relies on a filtering technique that removes outliers based on higher-order tensors. Our lower bound technique also has implications for related inference problems, specifically for the problem of robust {\em testing} of an unknown mean Gaussian. Here we show an information-theoretic lower bound which separates the sample complexity of the robust testing problem from its non-robust variant. This result is surprising because such a separation does not exist for the corresponding learning problem. version:1
arxiv-1611-03469 | Evaluating Urbanization from Satellite and Aerial Images by means of a statistical approach to the texture analysis | http://arxiv.org/abs/1611.03469 | id:1611.03469 author:Amelia Carolina Sparavigna category:cs.CV  published:2016-11-10 summary:Statistical methods are usually applied in the processing of digital images for the analysis of the textures displayed by them. Aiming to evaluate the urbanization of a given location from satellite or aerial images, here we consider a simple processing to distinguish in them the 'urban' from the 'rural' texture. The method is based on the mean values and the standard deviations of the colour tones of image pixels. The processing of the input images allows to obtain some maps from which a quantitative evaluation of the textures can be obtained. version:1
arxiv-1611-03466 | Roadmap Enhanced Improvement to the VSIMM Tracker via a Constrained Stochastic Context Free Grammar | http://arxiv.org/abs/1611.03466 | id:1611.03466 author:Sijia Gao, Vikram Krishnamurthy category:cs.CL  published:2016-11-10 summary:The aim of syntactic tracking is to classify spatio-temporal patterns of a target's motion using natural language processing models. In this paper, we generalize earlier work by considering a constrained stochastic context free grammar (CSCFG) for modeling patterns confined to a roadmap. The constrained grammar facilitates modeling specific directions and road names in a roadmap. We present a novel particle filtering algorithm that exploits the CSCFG model for estimating the target's patterns. This meta-level algorithm operates in conjunction with a base-level tracking algorithm. Extensive numerical results using simulated ground moving target indicator (GMTI) radar measurements show substantial improvement in target tracking accuracy. version:1
arxiv-1611-03451 | Importance Sampling with Unequal Support | http://arxiv.org/abs/1611.03451 | id:1611.03451 author:Philip S. Thomas, Emma Brunskill category:cs.LG cs.AI stat.ML  published:2016-11-10 summary:Importance sampling is often used in machine learning when training and testing data come from different distributions. In this paper we propose a new variant of importance sampling that can reduce the variance of importance sampling-based estimates by orders of magnitude when the supports of the training and testing distributions differ. After motivating and presenting our new importance sampling estimator, we provide a detailed theoretical analysis that characterizes both its bias and variance relative to the ordinary importance sampling estimator (in various settings, which include cases where ordinary importance sampling is biased, while our new estimator is not, and vice versa). We conclude with an example of how our new importance sampling estimator can be used to improve estimates of how well a new treatment policy for diabetes will work for an individual, using only data from when the individual used a previous treatment policy. version:1
arxiv-1611-02796 | Tuning Recurrent Neural Networks with Reinforcement Learning | http://arxiv.org/abs/1611.02796 | id:1611.02796 author:Natasha Jaques, Shixiang Gu, Richard E. Turner, Douglas Eck category:cs.LG cs.AI  published:2016-11-09 summary:Sequence models can be trained using supervised learning and a next-step prediction objective. This approach, however, suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. Motivated by the fact that reinforcement learning (RL) can be used to impose arbitrary properties on generated data by choosing appropriate reward functions, in this paper we propose a novel approach for sequence training which combines Maximum Likelihood (ML) and RL training. We refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from stochastic optimal control (SOC). We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note-RNN is then refined using RL, where the reward function is a combination of rewards based on rules of music theory, as well as the output of another trained Note-RNN. We show that by combining ML and RL, this RL Tuner method can not only produce more pleasing melodies, but that it can significantly reduce unwanted behaviors and failure modes of the RNN. version:2
arxiv-1611-03777 | Tricks from Deep Learning | http://arxiv.org/abs/1611.03777 | id:1611.03777 author:Atƒ±lƒ±m G√ºne≈ü Baydin, Barak A. Pearlmutter, Jeffrey Mark Siskind category:cs.LG stat.ML  published:2016-11-10 summary:The deep learning community has devised a diverse set of methods to make gradient optimization, using large datasets, of large and highly complex models with deeply cascaded nonlinearities, practical. Taken as a whole, these methods constitute a breakthrough, allowing computational structures which are quite wide, very deep, and with an enormous number and variety of free parameters to be effectively optimized. The result now dominates much of practical machine learning, with applications in machine translation, computer vision, and speech recognition. Many of these methods, viewed through the lens of algorithmic differentiation (AD), can be seen as either addressing issues with the gradient itself, or finding ways of achieving increased efficiency using tricks that are AD-related, but not provided by current AD systems. The goal of this paper is to explain not just those methods of most relevance to AD, but also the technical constraints and mindset which led to their discovery. After explaining this context, we present a "laundry list" of methods developed by the deep learning community. Two of these are discussed in further mathematical detail: a way to dramatically reduce the size of the tape when performing reverse-mode AD on a (theoretically) time-reversible process like an ODE integrator; and a new mathematical insight that allows for the implementation of a stochastic Newton's method. version:1
arxiv-1611-03427 | Multi-Task Multiple Kernel Relationship Learning | http://arxiv.org/abs/1611.03427 | id:1611.03427 author:Keerthiram Murugesan, Jaime Carbonell category:stat.ML cs.LG  published:2016-11-10 summary:This paper presents a novel multitask multiple-kernel learning framework that efficiently learns the kernel weights leveraging the relationship across multiple tasks. The idea is to automatically infer this task relationship in the \textit{RKHS} space corresponding to the given base kernels. The problem is formulated as a regularization-based approach called \textit{Multi-Task Multiple Kernel Relationship Learning} (\textit{MK-MTRL}), which models the task relationship matrix from the weights learned from latent feature spaces of task-specific base kernels. Unlike in previous work, the proposed formulation allows one to incorporate prior knowledge for simultaneously learning several related task. We propose an alternating minimization algorithm to learn the model parameters, kernel weights and task relationship matrix. In order to tackle large-scale problems, we further propose a two-stage \textit{MK-MTRL} online learning algorithm and show that it significantly reduces the computational time, and also achieves performance comparable to that of the joint learning framework. Experimental results on benchmark datasets show that the proposed formulations outperform several state-of-the-art multi-task learning methods. version:1
arxiv-1611-03426 | Why is it Difficult to Detect Sudden and Unexpected Epidemic Outbreaks in Twitter? | http://arxiv.org/abs/1611.03426 | id:1611.03426 author:Avar√© Stewart, Sara Romano, Nattiya Kanhabua, Sergio Di Martino, Wolf Siberski, Antonino Mazzeo, Wolfgang Nejdl, Ernesto Diaz-Aviles category:cs.CY cs.IR cs.SI stat.ML  published:2016-11-10 summary:Social media services such as Twitter are a valuable source of information for decision support systems. Many studies have shown that this also holds for the medical domain, where Twitter is considered a viable tool for public health officials to sift through relevant information for the early detection, management, and control of epidemic outbreaks. This is possible due to the inherent capability of social media services to transmit information faster than traditional channels. However, the majority of current studies have limited their scope to the detection of common and seasonal health recurring events (e.g., Influenza-like Illness), partially due to the noisy nature of Twitter data, which makes outbreak detection and management very challenging. Within the European project M-Eco, we developed a Twitter-based Epidemic Intelligence (EI) system, which is designed to also handle a more general class of unexpected and aperiodic outbreaks. In particular, we faced three main research challenges in this endeavor: 1) dynamic classification to manage terminology evolution of Twitter messages, 2) alert generation to produce reliable outbreak alerts analyzing the (noisy) tweet time series, and 3) ranking and recommendation to support domain experts for better assessment of the generated alerts. In this paper, we empirically evaluate our proposed approach to these challenges using real-world outbreak datasets and a large collection of tweets. We validate our solution with domain experts, describe our experiences, and give a more realistic view on the benefits and issues of analyzing social media for public health. version:1
arxiv-1611-03423 | DiffSharp: An AD Library for .NET Languages | http://arxiv.org/abs/1611.03423 | id:1611.03423 author:Atƒ±lƒ±m G√ºne≈ü Baydin, Barak A. Pearlmutter, Jeffrey Mark Siskind category:cs.MS cs.LG  published:2016-11-10 summary:DiffSharp is an algorithmic differentiation or automatic differentiation (AD) library for the .NET ecosystem, which is targeted by the C# and F# languages, among others. The library has been designed with machine learning applications in mind, allowing very succinct implementations of models and optimization routines. DiffSharp is implemented in F# and exposes forward and reverse AD operators as general nestable higher-order functions, usable by any .NET language. It provides high-performance linear algebra primitives---scalars, vectors, and matrices, with a generalization to tensors underway---that are fully supported by all the AD operators, and which use a BLAS/LAPACK backend via the highly optimized OpenBLAS library. DiffSharp currently uses operator overloading, but we are developing a transformation-based version of the library using F#'s "code quotation" metaprogramming facility. Work on a CUDA-based GPU backend is also underway. version:1
arxiv-1611-03410 | Binomial Checkpointing for Arbitrary Programs with No User Annotation | http://arxiv.org/abs/1611.03410 | id:1611.03410 author:Jeffrey Mark Siskind, Barak A. Pearlmutter category:cs.PL cs.LG cs.MS  published:2016-11-10 summary:Heretofore, automatic checkpointing at procedure-call boundaries, to reduce the space complexity of reverse mode, has been provided by systems like Tapenade. However, binomial checkpointing, or treeverse, has only been provided in Automatic Differentiation (AD) systems in special cases, e.g., through user-provided pragmas on DO loops in Tapenade, or as the nested taping mechanism in adol-c for time integration processes, which requires that user code be refactored. We present a framework for applying binomial checkpointing to arbitrary code with no special annotation or refactoring required. This is accomplished by applying binomial checkpointing directly to a program trace. This trace is produced by a general-purpose checkpointing mechanism that is orthogonal to AD. version:1
arxiv-1611-03404 | Learning an Astronomical Catalog of the Visible Universe through Scalable Bayesian Inference | http://arxiv.org/abs/1611.03404 | id:1611.03404 author:Jeffrey Regier, Kiran Pamnany, Ryan Giordano, Rollin Thomas, David Schlegel, Jon McAuliffe, Prabhat category:cs.DC astro-ph.IM cs.LG stat.AP stat.ML  published:2016-11-10 summary:Celeste is a procedure for inferring astronomical catalogs that attains state-of-the-art scientific results. To date, Celeste has been scaled to at most hundreds of megabytes of astronomical images: Bayesian posterior inference is notoriously demanding computationally. In this paper, we report on a scalable, parallel version of Celeste, suitable for learning catalogs from modern large-scale astronomical datasets. Our algorithmic innovations include a fast numerical optimization routine for Bayesian posterior inference and a statistically efficient scheme for decomposing astronomical optimization problems into subproblems. Our scalable implementation is written entirely in Julia, a new high-level dynamic programming language designed for scientific and numerical computing. We use Julia's high-level constructs for shared and distributed memory parallelism, and demonstrate effective load balancing and efficient scaling on up to 8192 Xeon cores on the NERSC Cori supercomputer. version:1
arxiv-1611-03383 | Disentangling factors of variation in deep representations using adversarial training | http://arxiv.org/abs/1611.03383 | id:1611.03383 author:Michael Mathieu, Junbo Zhao, Pablo Sprechmann, Aditya Ramesh, Yann LeCun category:cs.LG stat.ML  published:2016-11-10 summary:We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentanglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities. version:1
arxiv-1611-03382 | Efficient Summarization with Read-Again and Copy Mechanism | http://arxiv.org/abs/1611.03382 | id:1611.03382 author:Wenyuan Zeng, Wenjie Luo, Sanja Fidler, Raquel Urtasun category:cs.CL  published:2016-11-10 summary:Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current decoders utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art. version:1
arxiv-1611-04465 | Advancing Memristive Analog Neuromorphic Networks: Increasing Complexity, and Coping with Imperfect Hardware Components | http://arxiv.org/abs/1611.04465 | id:1611.04465 author:F. Merrikh Bayat, M. Prezioso, B. Chakrabarti, I. Kataeva, D. B. Strukov category:cs.ET cs.NE  published:2016-11-10 summary:We experimentally demonstrate classification of 4x4 binary images into 4 classes, using a 3-layer mixed-signal neuromorphic network ("MLP perceptron"), based on two passive 20x20 memristive crossbar arrays, board-integrated with discrete CMOS components. The network features 10 hidden-layer and 4 output-layer analog CMOS neurons and 428 metal-oxide memristors, i.e. is almost an order of magnitude more complex than any previously reported functional memristor circuit. Moreover, the inference operation of this classifier is performed entirely in the integrated hardware. To deal with larger crossbar arrays, we have developed a semi-automatic approach to their forming and testing, and compared several memristor training schemes for coping with imperfect behavior of these devices, as well as with variability of analog CMOS neurons. The effectiveness of the proposed schemes for defect and variation tolerance was verified experimentally using the implemented network and, additionally, by modeling the operation of a larger network, with 300 hidden-layer neurons, on the MNIST benchmark. Finally, we propose a simple modification of the implemented memristor-based vector-by-matrix multiplier to allow its operation in a wider temperature range. version:1
arxiv-1611-03328 | Distributed Estimation and Learning over Heterogeneous Networks | http://arxiv.org/abs/1611.03328 | id:1611.03328 author:M. Amin Rahimian, Ali Jadbabaie category:stat.AP cs.SI cs.SY math.ST stat.ML stat.TH  published:2016-11-10 summary:We consider several estimation and learning problems that networked agents face when making decisions given their uncertainty about an unknown variable. Our methods are designed to efficiently deal with heterogeneity in both size and quality of the observed data, as well as heterogeneity over time (intermittence). The goal of the studied aggregation schemes is to efficiently combine the observed data that is spread over time and across several network nodes, accounting for all the network heterogeneities. Moreover, we require no form of coordination beyond the local neighborhood of every network agent or sensor node. The three problems that we consider are (i) maximum likelihood estimation of the unknown given initial data sets, (ii) learning the true model parameter from streams of data that the agents receive intermittently over time, and (iii) minimum variance estimation of a complete sufficient statistic from several data points that the networked agents collect over time. In each case we rely on an aggregation scheme to combine the observations of all agents; moreover, when the agents receive streams of data over time, we modify the update rules to accommodate the most recent observations. In every case, we demonstrate the efficiency of our algorithms by proving convergence to the globally efficient estimators given the observations of all agents. We supplement these results by investigating the rate of convergence and providing finite-time performance guarantees. version:1
arxiv-1611-03321 | Computing threshold functions using dendrites | http://arxiv.org/abs/1611.03321 | id:1611.03321 author:Romain Caz√©, Bartozs Tele≈Ñczuk, Alain Destexhe category:cs.NE  published:2016-11-10 summary:Neurons, modeled as linear threshold unit (LTU), can in theory compute all thresh- old functions. In practice, however, some of these functions require synaptic weights of arbitrary large precision. We show here that dendrites can alleviate this requirement. We introduce here the non-Linear Threshold Unit (nLTU) that integrates synaptic input sub-linearly within distinct subunits to take into account local saturation in dendrites. We systematically search parameter space of the nTLU and TLU to compare them. Firstly, this shows that the nLTU can compute all threshold functions with smaller precision weights than the LTU. Secondly, we show that a nLTU can compute significantly more functions than a LTU when an input can only make a single synapse. This work paves the way for a new generation of network made of nLTU with binary synapses. version:1
arxiv-1611-03318 | Variables effecting photomosaic reconstruction and ortho-rectification from aerial survey datasets | http://arxiv.org/abs/1611.03318 | id:1611.03318 author:Jonathan Byrne, Debra Laefer category:cs.CV  published:2016-11-10 summary:Unmanned aerial vehicles now make it possible to obtain high quality aerial imagery at a low cost, but processing those images into a single, useful entity is neither simple nor seamless. Specifically, there are factors that must be addressed when merging multiple images into a single coherent one. While ortho-rectification can be done, it tends to be expensive and time consuming. Image stitching offers a more economical, low-tech approach. However direct application tends to fail for low-elevation imagery due to one or more factors including insufficient keypoints, parallax issues, and homogeneity of the surveyed area. This paper discusses these problems and possible solutions when using techniques such as image stitching and structure from motion for generating ortho-rectified imagery. These are presented in terms of actual Irish projects including the Boland's Mills building in Dublin's city centre, the Kilmoon Cross Farm, and the Richview buildings on the University College Dublin campus. Implications for various Irish industries are explained in terms of both urban and rural projects. version:1
arxiv-1611-03313 | X-ray Scattering Image Classification Using Deep Learning | http://arxiv.org/abs/1611.03313 | id:1611.03313 author:Boyu Wang, Kevin Yager, Dantong Yu, Minh Hoai category:cs.CV  published:2016-11-10 summary:Visual inspection of x-ray scattering images is a powerful technique for probing the physical structure of materials at the molecular scale. In this paper, we explore the use of deep learning to develop methods for automatically analyzing x-ray scattering images. In particular, we apply Convolutional Neural Networks and Convolutional Autoencoders for x-ray scattering image classification. To acquire enough training data for deep learning, we use simulation software to generate synthetic x-ray scattering images. Experiments show that deep learning methods outperform previously published methods by 10\% on synthetic and real datasets. version:1
arxiv-1611-03279 | Tracing metaphors in time through self-distance in vector spaces | http://arxiv.org/abs/1611.03279 | id:1611.03279 author:Marco Del Tredici, Malvina Nissim, Andrea Zaninello category:cs.CL  published:2016-11-10 summary:From a diachronic corpus of Italian, we build consecutive vector spaces in time and use them to compare a term's cosine similarity to itself in different time spans. We assume that a drop in similarity might be related to the emergence of a metaphorical sense at a given time. Similarity-based observations are matched to the actual year when a figurative meaning was documented in a reference dictionary and through manual inspection of corpus occurrences. version:1
arxiv-1611-03270 | Detecting Moving Regions in CrowdCam Images | http://arxiv.org/abs/1611.03270 | id:1611.03270 author:Adi Dafni, Yael Moses, Shai Avidan category:cs.CV  published:2016-11-10 summary:We address the novel problem of detecting dynamic regions in CrowdCam images, a set of still images captured by a group of people. These regions capture the most interesting parts of the scene, and detecting them plays an important role in the analysis of visual data. Our method is based on the observation that matching static points must satisfy the epipolar geometry constraints, but computing exact matches is challenging. Instead, we compute the probability that a pixel has a match, not necessarily the correct one, along the corresponding epipolar line. The complement of this probability is not necessarily the probability of a dynamic point because of occlusions, noise, and matching errors. Therefore, information from all pairs of images is aggregated to obtain a high quality dynamic probability map, per image. Experiments on challenging datasets demonstrate the effectiveness of the algorithm on a broad range of settings; no prior knowledge about the scene, the camera characteristics or the camera locations is required. version:1
arxiv-1611-03268 | Error concealment by means of motion refinement and regularized Bregman divergence | http://arxiv.org/abs/1611.03268 | id:1611.03268 author:Alessandra M. Coelho, Vania V. Estrela, Felipe P. do Carmo, Sandro R. Fernandes category:cs.CV  published:2016-11-10 summary:This work addresses the problem of error concealment in video transmission systems over noisy channels employing Bregman divergences along with regularization. Error concealment intends to improve the effects of disturbances at the reception due to bit-errors or cell loss in packet networks. Bregman regularization gives accurate answers after just some iterations with fast convergence, better accuracy, and stability. This technique has an adaptive nature: the regularization functional is updated according to Bregman functions that change from iteration to iteration according to the nature of the neighborhood under study at iteration n. Numerical experiments show that high-quality regularization parameter estimates can be obtained. The convergence is sped up while turning the regularization parameter estimation less empiric, and more automatic. version:1
arxiv-1611-02024 | Sigma Delta Quantized Networks | http://arxiv.org/abs/1611.02024 | id:1611.02024 author:Peter O'Connor, Max Welling category:cs.NE 68T01 F.1.1  published:2016-11-07 summary:Deep neural networks can be obscenely wasteful. When processing video, a convolutional network expends a fixed amount of computation for each frame with no regard to the similarity between neighbouring frames. As a result, it ends up repeatedly doing very similar computations. To put an end to such waste, we introduce Sigma-Delta networks. With each new input, each layer in this network sends a discretized form of its change in activation to the next layer. Thus the amount of computation that the network does scales with the amount of change in the input and layer activations, rather than the size of the network. We introduce an optimization method for converting any pre-trained deep network into an optimally efficient Sigma-Delta network, and show that our algorithm, if run on the appropriate hardware, could cut at least an order of magnitude from the computational cost of processing video data. version:2
arxiv-1611-04847 | Subgraph Detection with cues using Belief Propagation | http://arxiv.org/abs/1611.04847 | id:1611.04847 author:Arun Kadavankandy, Konstantin Avrachenkov, Laura Cottatellucci, Sundaresan Rajesh category:cs.LG cs.DS  published:2016-11-10 summary:We consider an Erdos-Renyi graph with $n$ nodes and edge probability $q$ that is embedded with a random subgraph of size $K$ with edge probabilities $p$ such that $p>q$. We address the problem of detecting the subgraph nodes when only the graph edges are observed, along with some extra knowledge of a small fraction of subgraph nodes, called cued vertices or cues. We employ a local and distributed algorithm called belief propagation (BP). Recent works on subgraph detection without cues have shown that global maximum likelihood (ML) detection strictly outperforms BP in terms of asymptotic error rate, namely, there is a threshold condition that the subgraph parameters should satisfy below which BP fails in achieving asymptotically zero error, but ML succeeds. In contrast, we show that when the fraction of cues is strictly bounded away from zero, i.e., when there exists non-trivial side-information, BP achieves zero asymptotic error even below this threshold, thus approaching the performance of ML detection. version:1
arxiv-1611-01639 | Representation of uncertainty in deep neural networks through sampling | http://arxiv.org/abs/1611.01639 | id:1611.01639 author:Patrick McClure, Nikolaus Kriegeskorte category:cs.LG cs.NE q-bio.NC  published:2016-11-05 summary:As deep neural networks (DNNs) are applied to increasingly challenging problems, they will need to be able to represent their own uncertainty. Modeling uncertainty is one of the key features of Bayesian methods. Scalable Bayesian DNNs that use dropout-based variational distributions have recently been proposed. Here we evaluate the ability of Bayesian DNNs trained with Bernoulli or Gaussian distributions over units (dropout) or weights (dropconnect) to represent their own uncertainty at the time of inference through sampling. We tested how well Bayesian fully connected and convolutional DNNs represented their own uncertainty in classifying the MNIST handwritten digits. By adding different levels of Gaussian noise to the test images, we assessed how DNNs represented their uncertainty about regions of input space not covered by the training set. Bayesian DNNs estimated their own uncertainty more accurately than traditional DNNs with a softmax output. These results are important for building better deep learning systems and for investigating the hypothesis that biological neural networks use sampling to represent uncertainty. version:2
arxiv-1611-03231 | Policy Search with High-Dimensional Context Variables | http://arxiv.org/abs/1611.03231 | id:1611.03231 author:Voot Tangkaratt, Herke van Hoof, Simone Parisi, Gerhard Neumann, Jan Peters, Masashi Sugiyama category:stat.ML cs.LG  published:2016-11-10 summary:Direct contextual policy search methods learn to improve policy parameters and simultaneously generalize these parameters to different context or task variables. However, learning from high-dimensional context variables, such as camera images, is still a prominent problem in many real-world tasks. A naive application of unsupervised dimensionality reduction methods to the context variables, such as principal component analysis, is insufficient as task-relevant input may be ignored. In this paper, we propose a contextual policy search method in the model-based relative entropy stochastic search framework with integrated dimensionality reduction. We learn a model of the reward that is locally quadratic in both the policy parameters and the context variables. Furthermore, we perform supervised linear dimensionality reduction on the context variables by nuclear norm regularization. The experimental results show that the proposed method outperforms naive dimensionality reduction via principal component analysis and a state-of-the-art contextual policy search method. version:1
arxiv-1611-03227 | Feature Selection with the R Package MXM: Discovering Statistically-Equivalent Feature Subsets | http://arxiv.org/abs/1611.03227 | id:1611.03227 author:Vincenzo Lagani, Giorgos Athineou, Alessio Farcomeni, Michail Tsagris, Ioannis Tsamardinos category:stat.ML q-bio.QM  published:2016-11-10 summary:The statistically equivalent signature (SES) algorithm is a method for feature selection inspired by the principles of constrained-based learning of Bayesian Networks. Most of the currently available feature-selection methods return only a single subset of features, supposedly the one with the highest predictive power. We argue that in several domains multiple subsets can achieve close to maximal predictive accuracy, and that arbitrarily providing only one has several drawbacks. The SES method attempts to identify multiple, predictive feature subsets whose performances are statistically equivalent. Under that respect SES subsumes and extends previous feature selection algorithms, like the max-min parent children algorithm. SES is implemented in an homonym function included in the R package MXM, standing for mens ex machina, meaning 'mind from the machine' in Latin. The MXM implementation of SES handles several data-analysis tasks, namely classification, regression and survival analysis. In this paper we present the SES algorithm, its implementation, and provide examples of use of the SES function in R. Furthermore, we analyze three publicly available data sets to illustrate the equivalence of the signatures retrieved by SES and to contrast SES against the state-of-the-art feature selection method LASSO. Our results provide initial evidence that the two methods perform comparably well in terms of predictive accuracy and that multiple, equally predictive signatures are actually present in real world data. version:1
arxiv-1611-03225 | Sharper Bounds for Regression and Low-Rank Approximation with Regularization | http://arxiv.org/abs/1611.03225 | id:1611.03225 author:Haim Avron, Kenneth L. Clarkson, David P. Woodruff category:cs.DS cs.LG cs.NA math.NA  published:2016-11-10 summary:The technique of matrix sketching, such as the use of random projections, has been shown in recent years to be a powerful tool for accelerating many important statistical learning techniques. Research has so far focused largely on using sketching for the "vanilla" un-regularized versions of these techniques. Here we study sketching methods for regularized variants of linear regression, low rank approximations, and canonical correlation analysis. We study regularization both in a fairly broad setting, and in the specific context of the popular and widely used technique of ridge regularization; for the latter, as applied to each of these problems, we show algorithmic resource bounds in which the {\em statistical dimension} appears in places where in previous bounds the rank would appear. The statistical dimension is always smaller than the rank, and decreases as the amount of regularization increases. In particular, for the ridge low-rank approximation problem $\min_{Y,X} \lVert YX - A \rVert_F^2 + \lambda \lVert Y\rVert_F^2 + \lambda\lVert X \rVert_F^2$, where $Y\in\mathbb{R}^{n\times k}$ and $X\in\mathbb{R}^{k\times d}$, we give an approximation algorithm needing \[ O(\mathtt{nnz}(A)) + \tilde{O}((n+d)\varepsilon^{-1}k \min\{k, \varepsilon^{-1}\mathtt{sd}_\lambda(Y^*)\})+ \tilde{O}(\varepsilon^{-8} \mathtt{sd}_\lambda(Y^*)^3) \] time, where $s_{\lambda}(Y^*)\le k$ is the statistical dimension of $Y^*$, $Y^*$ is an optimal $Y$, $\varepsilon$ is an error parameter, and $\mathtt{nnz}(A)$ is the number of nonzero entries of $A$. We also study regularization in a much more general setting. For example, we obtain sketching-based algorithms for the low-rank approximation problem $\min_{X,Y} \lVert YX - A \rVert_F^2 + f(Y,X)$ where $f(\cdot,\cdot)$ is a regularizing function satisfying some very general conditions (chiefly, invariance under orthogonal transformations). version:1
arxiv-1611-03220 | Faster Kernel Ridge Regression Using Sketching and Preconditioning | http://arxiv.org/abs/1611.03220 | id:1611.03220 author:Haim Avron, Kenneth L. Clarkson, David P. Woodruff category:cs.NA cs.DS cs.LG math.NA  published:2016-11-10 summary:Random feature maps, such as random Fourier features, have recently emerged as a powerful technique for speeding up and scaling the training of kernel-based methods such as kernel ridge regression. However, random feature maps only provide crude approximations to the kernel function, so delivering state-of-the-art results requires the number of random features to be very large. Nevertheless, in some cases, even when the number of random features is driven to be as large as the training size, full recovery of the performance of the exact kernel method is not attained. In order to address this issue, we propose to use random feature maps to form preconditioners to be used in solving kernel ridge regression to high accuracy. We provide theoretical conditions on when this yields an effective preconditioner, and empirically evaluate our method and show it is highly effective for datasets of up to one million training examples. version:1
arxiv-1611-03218 | Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence | http://arxiv.org/abs/1611.03218 | id:1611.03218 author:Emilio Jorge, Mikael K√•geb√§ck, Emil Gustavsson category:cs.AI cs.CL cs.LG cs.MA  published:2016-11-10 summary:Learning your first language is an incredible feat and not easily duplicated. Doing this using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. As an alternative we propose to use situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks (DRQN) for learning a common language grounded in the provided environment. We task the agents with interactive image search in the form of the game Guess Who?. The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that it is possible to learn this task using DRQN and even more importantly that the words the agents use correspond to physical attributes present in the images that make up the agents environment. version:1
arxiv-1611-03217 | Real Time Video Analysis using Smart Phone Camera for Stroboscopic Image | http://arxiv.org/abs/1611.03217 | id:1611.03217 author:Somnath Mukherjee, Soumyajit Ganguly category:cs.CV  published:2016-11-10 summary:Motion capturing and there by segmentation of the motion of any moving object from a sequence of continuous images or a video is not an exceptional task in computer vision area. Smart-phone camera application is an added integration for the development of such tasks and it also provides for a smooth testing. A new approach has been proposed for segmenting out the foreground moving object from the background and then masking the sequential motion with the static background which is commonly known as stroboscopic image. In this paper the whole process of the stroboscopic image construction technique has been clearly described along with some necessary constraints which is due to the traditional problem of estimating and modeling dynamic background changes. The background subtraction technique has been properly estimated here and number of sequential motion have also been calculated with the correlation between the motion of the object and its time of occurrence. This can be a very effective application that can replace the traditional stroboscopic system using high end SLR cameras, tripod stand, shutter speed control and position etc. version:1
arxiv-1611-03214 | Ultimate tensorization: compressing convolutional and FC layers alike | http://arxiv.org/abs/1611.03214 | id:1611.03214 author:Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, Dmitry Vetrov category:cs.LG  published:2016-11-10 summary:Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80x network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset. version:1
arxiv-1611-03199 | Low Data Drug Discovery with One-shot Learning | http://arxiv.org/abs/1611.03199 | id:1611.03199 author:Han Altae-Tran, Bharath Ramsundar, Aneesh S. Pappu, Vijay Pande category:cs.LG stat.ML  published:2016-11-10 summary:Recent advances in machine learning have made significant contributions to drug discovery. Deep neural networks in particular have been demonstrated to provide significant boosts in predictive power when inferring the properties and activities of small-molecule compounds. However, the applicability of these techniques has been limited by the requirement for large amounts of training data. In this work, we demonstrate how one-shot learning can be used to significantly lower the amounts of data required to make meaningful predictions in drug discovery applications. We introduce a new architecture, the residual LSTM embedding, that, when combined with graph convolutional neural networks, significantly improves the ability to learn meaningful distance metrics over small-molecules. We open source all models introduced in this work as part of DeepChem, an open-source framework for deep-learning in drug discovery. version:1
arxiv-1610-09534 | FlyCap: Markerless Motion Capture Using Multiple Autonomous Flying Cameras | http://arxiv.org/abs/1610.09534 | id:1610.09534 author:Lan Xu, Lu Fang, Wei Cheng, Kaiwen Guo, Guyue Zhou, Qionghai Dai, Yebin Liu category:cs.CV cs.GR cs.RO  published:2016-10-29 summary:Aiming at automatic, convenient and non-instrusive motion capture, this paper presents a new generation markerless motion capture technique, the FlyCap system, to capture surface motions of moving characters using multiple autonomous flying cameras (autonomous unmanned aerial vehicles(UAV) each integrated with an RGBD video camera). During data capture, three cooperative flying cameras automatically track and follow the moving target who performs large scale motions in a wide space. We propose a novel non-rigid surface registration method to track and fuse the depth of the three flying cameras for surface motion tracking of the moving target, and simultaneously calculate the pose of each flying camera. We leverage the using of visual-odometry information provided by the UAV platform, and formulate the surface tracking problem in a non-linear objective function that can be linearized and effectively minimized through a Gaussian-Newton method. Quantitative and qualitative experimental results demonstrate the competent and plausible surface and motion reconstruction results version:2
arxiv-1611-03186 | SoK: Applying Machine Learning in Security - A Survey | http://arxiv.org/abs/1611.03186 | id:1611.03186 author:Heju Jiang, Jasvir Nagra, Parvez Ahammad category:cs.CR cs.LG  published:2016-11-10 summary:The idea of applying machine learning(ML) to solve problems in security domains is almost 3 decades old. As information and communications grow more ubiquitous and more data become available, many security risks arise as well as appetite to manage and mitigate such risks. Consequently, research on applying and designing ML algorithms and systems for security has grown fast, ranging from intrusion detection systems(IDS) and malware classification to security policy management(SPM) and information leak checking. In this paper, we systematically study the methods, algorithms, and system designs in academic publications from 2008-2015 that applied ML in security domains. 98 percent of the surveyed papers appeared in the 6 highest-ranked academic security conferences and 1 conference known for pioneering ML applications in security. We examine the generalized system designs, underlying assumptions, measurements, and use cases in active research. Our examinations lead to 1) a taxonomy on ML paradigms and security domains for future exploration and exploitation, and 2) an agenda detailing open and upcoming challenges. Based on our survey, we also suggest a point of view that treats security as a game theory problem instead of a batch-trained ML problem. version:1
arxiv-1611-01584 | Efficient Branching Cascaded Regression for Face Alignment under Significant Head Rotation | http://arxiv.org/abs/1611.01584 | id:1611.01584 author:Brandon M. Smith, Charles R. Dyer category:cs.CV  published:2016-11-05 summary:Despite much interest in face alignment in recent years, the large majority of work has focused on near-frontal faces. Algorithms typically break down on profile faces, or are too slow for real-time applications. In this work we propose an efficient approach to face alignment that can handle 180 degrees of head rotation in a unified way (e.g., without resorting to view-based models) using 2D training data. The foundation of our approach is cascaded shape regression (CSR), which has emerged recently as the leading strategy. We propose a generalization of conventional CSRs that we call branching cascaded regression (BCR). Conventional CSRs are single-track; that is, they progress from one cascade level to the next in a straight line, with each regressor attempting to fit the entire dataset. We instead split the regression problem into two or more simpler ones after each cascade level. Intuitively, each regressor can then operate on a simpler objective function (i.e., with fewer conflicting gradient directions). Within the BCR framework, we model and infer pose-related landmark visibility and face shape simultaneously using Structured Point Distribution Models (SPDMs). We propose to learn task-specific feature mapping functions that are adaptive to landmark visibility, and that use SPDM parameters as regression targets instead of 2D landmark coordinates. Additionally, we introduce a new in-the-wild dataset of profile faces to validate our approach. version:2
arxiv-1611-02568 | PixelSNE: Visualizing Fast with Just Enough Precision via Pixel-Aligned Stochastic Neighbor Embedding | http://arxiv.org/abs/1611.02568 | id:1611.02568 author:Minjeong Kim, Minsuk Choi, Sunwoong Lee, Jian Tang, Haesun Park, Jaegul Choo category:cs.LG  published:2016-11-08 summary:Embedding and visualizing large-scale high-dimensional data in a two-dimensional space is an important problem since such visualization can reveal deep insights out of complex data. Most of the existing embedding approaches, however, run on an excessively high precision, ignoring the fact that at the end, embedding outputs are converted into coarse-grained discrete pixel coordinates in a screen space. Motivated by such an observation and directly considering pixel coordinates in an embedding optimization process, we accelerate Barnes-Hut tree-based t-distributed stochastic neighbor embedding (BH-SNE), known as a state-of-the-art 2D embedding method, and propose a novel method called PixelSNE, a highly-efficient, screen resolution-driven 2D embedding method with a linear computational complexity in terms of the number of data items. Our experimental results show the significantly fast running time of PixelSNE by a large margin against BH-SNE, while maintaining the minimal degradation in the embedding quality. Finally, the source code of our method is publicly available at https://github.com/awesome-davian/PixelSNE version:2
arxiv-1611-03158 | Using Neural Networks for Fast Reachable Set Computations | http://arxiv.org/abs/1611.03158 | id:1611.03158 author:Frank Jiang, Glen Chou, Mo Chen, Claire J. Tomlin category:cs.LG  published:2016-11-10 summary:Hamilton-Jacobi (HJ) reachability is a powerful tool that provides performance and safety guarantees for dynamical systems. Unfortunately, using the state-of-the-art dynamic programming-based approaches, HJ reachability is intractable for systems with more than five dimensions because its computational complexity scales exponentially with system dimension. To sidestep the curse of dimensionality, we propose an algorithm that leverages a neural network to approximate the minimum time-to-reach function to synthesize controls. We show that our neural network generates near optimal controls which are guaranteed to successfully drive the system to a target state. Our framework is not dependent on state space discretization, leading to a significant reduction in computation time and space complexity in comparison with dynamic programming-based approaches. Using this grid-free approach also enables us to plan over longer time horizons with relatively little additional computation overhead. Unlike many previous neural network reachability formulations, our approximation is conservative and hence any trajectories we generate will be strictly feasible. For demonstration, we specialize our new general framework to the Dubins car model and discuss how the general framework can be applied to other models with higher-dimensional state spaces. version:1
arxiv-1611-02779 | RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning | http://arxiv.org/abs/1611.02779 | id:1611.02779 author:Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, Pieter Abbeel category:cs.AI cs.LG cs.NE stat.ML  published:2016-11-09 summary:Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems. version:2
arxiv-1611-03131 | Diversity Leads to Generalization in Neural Networks | http://arxiv.org/abs/1611.03131 | id:1611.03131 author:Bo Xie, Yingyu Liang, Le Song category:cs.LG stat.ML  published:2016-11-09 summary:Neural networks are a powerful class of functions that can be trained with simple gradient descent to achieve state-of-the-art performance on a variety of applications. Despite their practical success, there is a paucity of results that provide theoretical guarantees on why they are so effective. Lying in the center of the problem is the difficulty of analyzing the non-convex objective function with potentially numerous local minima and saddle points. Can neural networks corresponding to the stationary points of the objective function learn the true labeling function? If yes, what are the key factors contributing to such generalization ability? In this paper, we provide answers to these questions by analyzing one-hidden-layer neural networks with ReLU activation, and show that despite the non-convexity, neural networks with diverse units can learn the true function. We bypass the non-convexity issue by directly analyzing the first order condition, and show that the loss is bounded if the smallest singular value of the "extended feature matrix" is large enough. We make novel use of techniques from kernel methods and geometric discrepancy, and identify a new relation linking the smallest singular value to the spectrum of a kernel function associated with the activation function and to the diversity of the units. Our results also suggest a novel regularization function to promote unit diversity for potentially better generalization ability. version:1
arxiv-1611-03130 | Computationally Efficient Target Classification in Multispectral Image Data with Deep Neural Networks | http://arxiv.org/abs/1611.03130 | id:1611.03130 author:Lukas Cavigelli, Dominic Bernath, Michele Magno, Luca Benini category:cs.CV cs.AI cs.NE  published:2016-11-09 summary:Detecting and classifying targets in video streams from surveillance cameras is a cumbersome, error-prone and expensive task. Often, the incurred costs are prohibitive for real-time monitoring. This leads to data being stored locally or transmitted to a central storage site for post-incident examination. The required communication links and archiving of the video data are still expensive and this setup excludes preemptive actions to respond to imminent threats. An effective way to overcome these limitations is to build a smart camera that transmits alerts when relevant video sequences are detected. Deep neural networks (DNNs) have come to outperform humans in visual classifications tasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be extended to make use of higher-dimensional input data such as multispectral data. We explore this opportunity in terms of achievable accuracy and required computational effort. To analyze the precision of DNNs for scene labeling in an urban surveillance scenario we have created a dataset with 8 classes obtained in a field experiment. We combine an RGB camera with a 25-channel VIS-NIR snapshot sensor to assess the potential of multispectral image data for target classification. We evaluate several new DNNs, showing that the spectral information fused together with the RGB frames can be used to improve the accuracy of the system or to achieve similar accuracy with a 3x smaller computation effort. We achieve a very high per-pixel accuracy of 99.1%. Even for scarcely occurring, but particularly interesting classes, such as cars, 75% of the pixels are labeled correctly with errors occurring only around the border of the objects. This high accuracy was obtained with a training set of only 30 labeled images, paving the way for fast adaptation to various application scenarios. version:1
arxiv-1611-03125 | A Modular Theory of Feature Learning | http://arxiv.org/abs/1611.03125 | id:1611.03125 author:Daniel McNamara, Cheng Soon Ong, Robert C. Williamson category:cs.LG  published:2016-11-09 summary:Learning representations of data, and in particular learning features for a subsequent prediction task, has been a fruitful area of research delivering impressive empirical results in recent years. However, relatively little is understood about what makes a representation `good'. We propose the idea of a risk gap induced by representation learning for a given prediction context, which measures the difference in the risk of some learner using the learned features as compared to the original inputs. We describe a set of sufficient conditions for unsupervised representation learning to provide a benefit, as measured by this risk gap. These conditions decompose the problem of when representation learning works into its constituent parts, which can be separately evaluated using an unlabeled sample, suitable domain-specific assumptions about the joint distribution, and analysis of the feature learner and subsequent supervised learner. We provide two examples of such conditions in the context of specific properties of the unlabeled distribution, namely when the data lies close to a low-dimensional manifold and when it forms clusters. We compare our approach to a recently proposed analysis of semi-supervised learning. version:1
arxiv-1611-03068 | Incremental Sequence Learning | http://arxiv.org/abs/1611.03068 | id:1611.03068 author:Edwin D. de Jong category:cs.LG cs.NE  published:2016-11-09 summary:Deep learning research over the past years has shown that by increasing the scope or difficulty of the learning problem over time, increasingly complex learning problems can be addressed. We study incremental learning in the context of sequence learning, using generative RNNs in the form of multi-layer recurrent Mixture Density Networks. We introduce Incremental Sequence Learning, a simple incremental approach to sequence learning. Incremental Sequence Learning starts out by using only the first few steps of each sequence as training data. Each time a performance criterion has been reached, the length of the parts of the sequences used for training is increased. To evaluate Incremental Sequence Learning and comparison methods, we introduce and make available a novel sequence learning task and data set: predicting and classifying MNIST pen stroke sequences, where the familiar handwritten digit images have been transformed to pen stroke sequences representing the skeletons of the digits. We find that Incremental Sequence Learning greatly speeds up sequence learning and reaches the best test performance level of regular sequence learning 20 times faster, reduces the test error by 74%, and in general performs more robustly; it displays lower variance and achieves sustained progress after all three comparison method have stopped improving. A trained sequence prediction model is also used in transfer learning to the task of sequence classification, where it is found that transfer learning realizes improved classification performance compared to methods that learn to classify from scratch. version:1
arxiv-1611-03059 | Optimal Multiple Surface Segmentation with Convex Priors in Irregularly Sampled Space | http://arxiv.org/abs/1611.03059 | id:1611.03059 author:Abhay Shah, Junjie Bai, Michael D. Abramoff, Xiaodong Wu category:cs.CV  published:2016-11-09 summary:Optimal surface segmentation is widely used in numerous medical image segmentation applications. However, nodes in the graph based optimal surface segmentation method typically encode uniformly distributed orthogonal voxels of the volume. Thus the segmentation cannot attain an accuracy greater than a single unit voxel, i.e. the distance between two adjoining nodes in graph space. Segmentation accuracy higher than a unit voxel is achievable by exploiting partial volume information in the voxels which shall result in non-equidistant spacing between adjoining graph nodes. This paper reports a generalized graph based optimal multiple surface segmentation method with convex priors which segments the target surfaces in irregularly sampled space. The proposed method allows non-equidistant spacing between the adjoining graph nodes to achieve subvoxel accurate segmentation by utilizing the partial volume information in the voxels. The partial volume information in the voxels is exploited by computing a displacement field from the original volume data to identify the subvoxel accurate centers within each voxel resulting in non-equidistant spacing between the adjoining graph nodes. The smoothness of each surface modelled as a convex constraint governs the connectivity and regularity of the surface. We employ an edge-based graph representation to incorporate the necessary constraints and the globally optimal solution is obtained by computing a minimum s-t cut. The proposed method was validated on 25 optical coherence tomography image volumes of the retina and 10 intravascular multi-frame ultrasound image datasets for subvoxel and super resolution segmentation accuracy. In all cases, the approach yielded highly accurate results. Our approach can be readily extended to higher-dimensional segmentations. version:1
arxiv-1611-03057 | When silver glitters more than gold: Bootstrapping an Italian part-of-speech tagger for Twitter | http://arxiv.org/abs/1611.03057 | id:1611.03057 author:Barbara Plank, Malvina Nissim category:cs.CL  published:2016-11-09 summary:We bootstrap a state-of-the-art part-of-speech tagger to tag Italian Twitter data, in the context of the Evalita 2016 PoSTWITA shared task. We show that training the tagger on native Twitter data enriched with little amounts of specifically selected gold data and additional silver-labelled data scraped from Facebook, yields better results than using large amounts of manually annotated data from a mix of genres. version:1
arxiv-1611-03033 | On the Diffusion Geometry of Graph Laplacians and Applications | http://arxiv.org/abs/1611.03033 | id:1611.03033 author:Xiuyuan Cheng, Manas Rachh, Stefan Steinerberger category:math.SP math-ph math.AP math.MP stat.ML  published:2016-11-09 summary:We study directed, weighted graphs $G=(V,E)$ and consider the (not necessarily symmetric) averaging operator $$ (\mathcal{L}u)(i) = -\sum_{j \sim_{} i}{p_{ij} (u(j) - u(i))},$$ where $p_{ij}$ are normalized edge weights. Given a vertex $i \in V$, we define the diffusion distance to a set $B \subset V$ as the smallest number of steps $d_{B}(i) \in \mathbb{N}$ required for half of all random walks started in $i$ and moving randomly with respect to the weights $p_{ij}$ to visit $B$ within $d_{B}(i)$ steps. Our main result is that the eigenfunctions interact nicely with this notion of distance. In particular, if $u$ satisfies $\mathcal{L}u = \lambda u$ on $V$ and $$ B = \left\{ i \in V: - \varepsilon \leq u(i) \leq \varepsilon \right\} \neq \emptyset,$$ then, for all $i \in V$, $$ d_{B}(i) \log{\left( \frac{1}{ 1-\lambda } \right) } \geq \log{\left( \frac{ u(i) }{\ u\ _{L^{\infty}}} \right)} - \log{\left(\frac{1}{2} + \varepsilon\right)}.$$ $d_B(i)$ is a remarkably good approximation of $ u $ in the sense of having very high correlation. The result implies that the classical one-dimensional spectral embedding preserves particular aspects of geometry in the presence of clustered data. We also give a continuous variant of the result which has a connection to the hot spots conjecture. version:1
arxiv-1611-03028 | Node Embedding via Word Embedding for Network Community Discovery | http://arxiv.org/abs/1611.03028 | id:1611.03028 author:Weicong Ding, Christy Lin, Prakash Ishwar category:cs.SI physics.soc-ph stat.ML  published:2016-11-09 summary:Neural node embeddings have recently emerged as a powerful representation for supervised learning tasks involving graph-structured data. We leverage this recent advance to develop a novel algorithm for unsupervised community discovery in graphs. Through extensive experimental studies on simulated and real-world data, we demonstrate that the proposed approach consistently improves over the current state-of-the-art. Specifically, our approach empirically attains the information-theoretic limits for community recovery under the benchmark Stochastic Block Models for graph generation and exhibits better stability and accuracy over both Spectral Clustering and Acyclic Belief Propagation in the community recovery limits. version:1
arxiv-1611-03000 | Bio-Inspired Spiking Convolutional Neural Network using Layer-wise Sparse Coding and STDP Learning | http://arxiv.org/abs/1611.03000 | id:1611.03000 author:Amirhossein Tavanaei, Anthony S. Maida category:cs.NE  published:2016-11-09 summary:Hierarchical feature discovery using non-spiking convolutional neural networks (CNNs) has attracted much recent interest in machine learning and computer vision. However, it is still not well understood how to create spiking deep networks with multi-layer, unsupervised learning. One advantage of spiking CNNs is their bio-realism. Another advantage is that they represent information using sparse spike-trains which enable power-efficient implementation. This paper explores a novel bio-inspired spiking CNN that is trained in a greedy, layer-wise fashion. The proposed network consists of a spiking convolutional-pooling layer followed by a feature discovery layer. Kernels for the convolutional layer are trained using local learning. The learning is implemented using a sparse, spiking auto-encoder representing primary visual features. The feature discovery layer is equipped with a probabilistic spike-timing-dependent plasticity (STDP) learning rule. This layer represents complex visual features using probabilistic leaky, integrate-and-fire (LIF) neurons. Our results show that the convolutional layer is stack-admissible, enabling it to support a multi-layer learning. The visual features obtained from the proposed probabilistic LIF neurons in the feature discovery layer are utilized for training a classifier. Classification results contribute to the independent and informative visual features extracted in a hierarchy of convolutional and feature discovery layers. The proposed model is evaluated on the MNIST digit dataset using clean and noisy images. The recognition performance for clean images is above 98%. The performance loss for recognizing the noisy images is in the range 0.1% to 8.5% depending on noise types and densities. This level of performance loss indicates that the network is robust to additive noise. version:1
arxiv-1611-02988 | Distant supervision for emotion detection using Facebook reactions | http://arxiv.org/abs/1611.02988 | id:1611.02988 author:Chris Pool, Malvina Nissim category:cs.CL  published:2016-11-09 summary:We exploit the Facebook reaction feature in a distant supervised fashion to train a support vector machine classifier for emotion detection, using several feature combinations and combining different Facebook pages. We test our models on existing benchmarks for emotion detection and show that employing only information that is derived completely automatically, thus without relying on any handcrafted lexicon as it's usually done, we can achieve competitive results. The results also show that there is large room for improvement, especially by gearing the collection of Facebook pages, with a view to the target domain. version:1
arxiv-1611-02960 | A Unified Maximum Likelihood Approach for Optimal Distribution Property Estimation | http://arxiv.org/abs/1611.02960 | id:1611.02960 author:Jayadev Acharya, Hirakendu Das, Alon Orlitsky, Ananda Theertha Suresh category:cs.IT cs.DS cs.LG math.IT  published:2016-11-09 summary:The advent of data science has spurred interest in estimating properties of distributions over large alphabets. Fundamental symmetric properties such as support size, support coverage, entropy, and proximity to uniformity, received most attention, with each property estimated using a different technique and often intricate analysis tools. We prove that for all these properties, a single, simple, plug-in estimator---profile maximum likelihood (PML)---performs as well as the best specialized techniques. This raises the possibility that PML may optimally estimate many other symmetric properties. version:1
arxiv-1611-02944 | Increasing the throughput of machine translation systems using clouds | http://arxiv.org/abs/1611.02944 | id:1611.02944 author:Jernej Viƒçiƒç, Andrej Brodnik category:cs.CL cs.DC  published:2016-11-09 summary:The manuscript presents an experiment at implementation of a Machine Translation system in a MapReduce model. The empirical evaluation was done using fully implemented translation systems embedded into the MapReduce programming model. Two machine translation paradigms were studied: shallow transfer Rule Based Machine Translation and Statistical Machine Translation. The results show that the MapReduce model can be successfully used to increase the throughput of a machine translation system. Furthermore this method enhances the throughput of a machine translation system without decreasing the quality of the translation output. Thus, the present manuscript also represents a contribution to the seminal work in natural language processing, specifically Machine Translation. It first points toward the importance of the definition of the metric of throughput of translation system and, second, the applicability of the machine translation task to the MapReduce paradigm. version:1
arxiv-1611-02941 | Predicting User Roles in Social Networks using Transfer Learning with Feature Transformation | http://arxiv.org/abs/1611.02941 | id:1611.02941 author:Jun Sun, J√©r√¥me Kunegis, Steffen Staab category:cs.SI stat.ML  published:2016-11-09 summary:How can we recognise social roles of people, given a completely unlabelled social network? We present a transfer learning approach to network role classification based on feature transformations from each network's local feature distribution to a global feature space. Experiments are carried out on real-world datasets. (See manuscript for the full abstract.) version:1
arxiv-1611-02886 | Node-Adapt, Path-Adapt and Tree-Adapt:Model-Transfer Domain Adaptation for Random Forest | http://arxiv.org/abs/1611.02886 | id:1611.02886 author:Azadeh S. Mozafari, David Vazquez, Mansour Jamzad, Antonio M. Lopez category:cs.CV  published:2016-11-09 summary:Random Forest (RF) is a successful paradigm for learning classifiers due to its ability to learn from large feature spaces and seamlessly integrate multi-class classification, as well as the achieved accuracy and processing efficiency. However, as many other classifiers, RF requires domain adaptation (DA) provided that there is a mismatch between the training (source) and testing (target) domains which provokes classification degradation. Consequently, different RF-DA methods have been proposed, which not only require target-domain samples but revisiting the source-domain ones, too. As novelty, we propose three inherently different methods (Node-Adapt, Path-Adapt and Tree-Adapt) that only require the learned source-domain RF and a relatively few target-domain samples for DA, i.e. source-domain samples do not need to be available. To assess the performance of our proposals we focus on image-based object detection, using the pedestrian detection problem as challenging proof-of-concept. Moreover, we use the RF with expert nodes because it is a competitive patch-based pedestrian model. We test our Node-, Path- and Tree-Adapt methods in standard benchmarks, showing that DA is largely achieved. version:1
arxiv-1611-00142 | Deep fusion of visual signatures for client-server facial analysis | http://arxiv.org/abs/1611.00142 | id:1611.00142 author:Binod Bhattarai, Gaurav Sharma, Frederic Jurie category:cs.CV  published:2016-11-01 summary:Facial analysis is a key technology for enabling human-machine interaction. In this context, we present a client-server framework, where a client transmits the signature of a face to be analyzed to the server, and, in return, the server sends back various information describing the face e.g. is the person male or female, is she/he bald, does he have a mustache, etc. We assume that a client can compute one (or a combination) of visual features; from very simple and efficient features, like Local Binary Patterns, to more complex and computationally heavy, like Fisher Vectors and CNN based, depending on the computing resources available. The challenge addressed in this paper is to design a common universal representation such that a single merged signature is transmitted to the server, whatever be the type and number of features computed by the client, ensuring nonetheless an optimal performance. Our solution is based on learning of a common optimal subspace for aligning the different face features and merging them into a universal signature. We have validated the proposed method on the challenging CelebA dataset, on which our method outperforms existing state-of-the-art methods when rich representation is available at test time, while giving competitive performance when only simple signatures (like LBP) are available at test time due to resource constraints on the client. version:2
arxiv-1611-02879 | Audio Visual Speech Recognition using Deep Recurrent Neural Networks | http://arxiv.org/abs/1611.02879 | id:1611.02879 author:Abhinav Thanda, Shankar M Venkatesan category:cs.CV cs.CL cs.LG  published:2016-11-09 summary:In this work, we propose a training algorithm for an audio-visual automatic speech recognition (AV-ASR) system using deep recurrent neural network (RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal Classification (CTC) objective function. The frame labels obtained from the acoustic model are then used to perform a non-linear dimensionality reduction of the visual features using a deep bottleneck network. Audio and visual features are fused and used to train a fusion RNN. The use of bottleneck features for visual modality helps the model to converge properly during training. Our system is evaluated on GRID corpus. Our results show that presence of visual modality gives significant improvement in character error rate (CER) at various levels of noise even when the model is trained without noisy data. We also provide a comparison of two fusion methods: feature fusion and decision fusion. version:1
arxiv-1611-02869 | Gaussian process regression can turn non-uniform and undersampled diffusion MRI data into diffusion spectrum imaging | http://arxiv.org/abs/1611.02869 | id:1611.02869 author:Jens Sj√∂lund, Anders Eklund, Evren √ñzarslan, Hans Knutsson category:stat.AP cs.CV stat.ML  published:2016-11-09 summary:We propose to use Gaussian process regression to accurately estimate the diffusion MRI signal at arbitrary locations in q-space. By estimating the signal on a grid, we can do synthetic diffusion spectrum imaging: reconstructing the ensemble averaged propagator (EAP) by an inverse Fourier transform. We also propose an alternative reconstruction method guaranteeing a nonnegative EAP that integrates to unity. The reconstruction is validated on data simulated from two Gaussians at various crossing angles. Moreover, we demonstrate on non-uniformly sampled in vivo data that the method is far superior to linear interpolation, and allows a drastic undersampling of the data with only a minor loss of accuracy. We envision the method as a potential replacement for standard diffusion spectrum imaging, in particular when acquistion time is limited. version:1
arxiv-1611-02862 | The Little Engine that Could: Regularization by Denoising (RED) | http://arxiv.org/abs/1611.02862 | id:1611.02862 author:Yaniv Romano, Michael Elad, Peyman Milanfar category:cs.CV cs.NA  published:2016-11-09 summary:Removal of noise from an image is an extensively studied problem in image processing. Indeed, the recent advent of sophisticated and highly effective denoising algorithms lead some to believe that existing methods are touching the ceiling in terms of noise removal performance. Can we leverage this impressive achievement to treat other tasks in image processing? Recent work has answered this question positively, in the form of the Plug-and-Play Prior ($P^3$) method, showing that any inverse problem can be handled by sequentially applying image denoising steps. This relies heavily on the ADMM optimization technique in order to obtain this chained denoising interpretation. Is this the only way in which tasks in image processing can exploit the image denoising engine? In this paper we provide an alternative, more powerful and more flexible framework for achieving the same goal. As opposed to the $P^3$ method, we offer Regularization by Denoising (RED): using the denoising engine in defining the regularization of the inverse problem. We propose an explicit image-adaptive Laplacian-based regularization functional, making the overall objective functional clearer and better defined. With a complete flexibility to choose the iterative optimization procedure for minimizing the above functional, RED is capable of incorporating any image denoising algorithm, treat general inverse problems very effectively, and is guaranteed to converge to the globally optimal result. We test this approach and demonstrate state-of-the-art results in the image deblurring and super-resolution problems. version:1
arxiv-1611-02854 | Lie-Access Neural Turing Machines | http://arxiv.org/abs/1611.02854 | id:1611.02854 author:Greg Yang, Alexander M. Rush category:cs.NE cs.LG  published:2016-11-09 summary:Recent work has demonstrated the effectiveness of employing explicit external memory structures in conjunction with deep neural models for algorithmic learning (Graves et al. 2014; Weston et al. 2014). These models utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the variable-length storage necessary for computational tasks. In this work, we propose an alternative model, Lie-access memory, that is explicitly designed for the neural setting. In this paradigm, memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and soft memory access is performed by considering the distance to keys associated with each memory. We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintain differentiability. To experiment with this approach, we implement several simplified Lie-access neural Turing machine (LANTM) with different Lie groups. We find that this approach is able to perform well on a range of algorithmic tasks. version:1
arxiv-1611-02839 | Old Content and Modern Tools - Searching Named Entities in a Finnish OCRed Historical Newspaper Collection 1771-1910 | http://arxiv.org/abs/1611.02839 | id:1611.02839 author:Kimmo Kettunen, Eetu M√§kel√§, Teemu Ruokolainen, Juha Kuokkala, Laura L√∂fberg category:cs.CL  published:2016-11-09 summary:Named Entity Recognition (NER), search, classification and tagging of names and name like frequent informational elements in texts, has become a standard information extraction procedure for textual data. NER has been applied to many types of texts and different types of entities: newspapers, fiction, historical records, persons, locations, chemical compounds, protein families, animals etc. In general a NER system's performance is genre and domain dependent and also used entity categories vary (Nadeau and Sekine, 2007). The most general set of named entities is usually some version of three partite categorization of locations, persons and organizations. In this paper we report first large scale trials and evaluation of NER with data out of a digitized Finnish historical newspaper collection Digi. Experiments, results and discussion of this research serve development of the Web collection of historical Finnish newspapers. Digi collection contains 1,960,921 pages of newspaper material from years 1771-1910 both in Finnish and Swedish. We use only material of Finnish documents in our evaluation. The OCRed newspaper collection has lots of OCR errors; its estimated word level correctness is about 70-75 % (Kettunen and P\"a\"akk\"onen, 2016). Our principal NER tagger is a rule-based tagger of Finnish, FiNER, provided by the FIN-CLARIN consortium. We show also results of limited category semantic tagging with tools of the Semantic Computing Research Group (SeCo) of the Aalto University. Three other tools are also evaluated briefly. This research reports first published large scale results of NER in a historical Finnish OCRed newspaper collection. Results of the research supplement NER results of other languages with similar noisy data. version:1
arxiv-1611-02304 | Normalizing Flows on Riemannian Manifolds | http://arxiv.org/abs/1611.02304 | id:1611.02304 author:Mevlana C. Gemici, Danilo Rezende, Shakir Mohamed category:stat.ML cs.AI math.ST stat.TH  published:2016-11-07 summary:We consider the problem of density estimation on Riemannian manifolds. Density estimation on manifolds has many applications in fluid-mechanics, optics and plasma physics and it appears often when dealing with angular variables (such as used in protein folding, robot limbs, gene-expression) and in general directional statistics. In spite of the multitude of algorithms available for density estimation in the Euclidean spaces $\mathbf{R}^n$ that scale to large n (e.g. normalizing flows, kernel methods and variational approximations), most of these methods are not immediately suitable for density estimation in more general Riemannian manifolds. We revisit techniques related to homeomorphisms from differential geometry for projecting densities to sub-manifolds and use it to generalize the idea of normalizing flows to more general Riemannian manifolds. The resulting algorithm is scalable, simple to implement and suitable for use with automatic differentiation. We demonstrate concrete examples of this method on the n-sphere $\mathbf{S}^n$. version:2
arxiv-1611-02830 | Online Learning for Wireless Distributed Computing | http://arxiv.org/abs/1611.02830 | id:1611.02830 author:Yi-Hsuan Kao, Kwame Wright, Bhaskar Krishnamachari, Fan Bai category:cs.LG  published:2016-11-09 summary:There has been a growing interest for Wireless Distributed Computing (WDC), which leverages collaborative computing over multiple wireless devices. WDC enables complex applications that a single device cannot support individually. However, the problem of assigning tasks over multiple devices becomes challenging in the dynamic environments encountered in real-world settings, considering that the resource availability and channel conditions change over time in unpredictable ways due to mobility and other factors. In this paper, we formulate a task assignment problem as an online learning problem using an adversarial multi-armed bandit framework. We propose MABSTA, a novel online learning algorithm that learns the performance of unknown devices and channel qualities continually through exploratory probing and makes task assignment decisions by exploiting the gained knowledge. For maximal adaptability, MABSTA is designed to make no stochastic assumption about the environment. We analyze it mathematically and provide a worst-case performance guarantee for any dynamic environment. We also compare it with the optimal offline policy as well as other baselines via emulations on trace-data obtained from a wireless IoT testbed, and show that it offers competitive and robust performance in all cases. To the best of our knowledge, MABSTA is the first online algorithm in this domain of task assignment problems and provides provable performance guarantee. version:1
arxiv-1611-02803 | Semi-Supervised Recognition of the Diploglossus Millepunctatus Lizard Species using Artificial Vision Algorithms | http://arxiv.org/abs/1611.02803 | id:1611.02803 author:Jhony-Heriberto Giraldo-Zuluaga, Augusto Salazar, Juan M. Daza category:cs.CV  published:2016-11-09 summary:Animal biometrics is an important requirement for monitoring and conservation tasks. The classical animal biometrics risk the animals' integrity, are expensive for numerous animals, and depend on expert criterion. The non-invasive biometrics techniques offer alternatives to manage the aforementioned problems. In this paper we propose an automatic segmentation and identification algorithm based on artificial vision algorithms to recognize Diploglossus millepunctatus. Diploglossus millepunctatus is an endangered lizard species. The algorithm is based on two stages: automatic segmentation to remove the subjective evaluation, and one identification stage to reduce the analysis time. A 82.87% of correct segmentation in average is reached. Meanwhile the identification algorithm is achieved with euclidean distance point algorithms such as Iterative Closest Point and Procrustes Analysis. A performance of 92.99% on the top 1, and a 96.82% on the top 5 is reached. The developed software, and the database used in this paper are publicly available for download from the web page of the project. version:1
arxiv-1611-02788 | Generative Shape Models: Joint Text Recognition and Segmentation with Very Little Training Data | http://arxiv.org/abs/1611.02788 | id:1611.02788 author:Xinghua Lou, Ken Kansky, Wolfgang Lehrach, CC Laan, Bhaskara Marthi, D. Scott Phoenix, Dileep George category:cs.CV  published:2016-11-09 summary:We demonstrate that a generative model for object shapes can achieve state of the art results on challenging scene text recognition tasks, and with orders of magnitude fewer training images than required for competing discriminative methods. In addition to transcribing text from challenging images, our method performs fine-grained instance segmentation of characters. We show that our model is more robust to both affine transformations and non-affine deformations compared to previous approaches. version:1
arxiv-1611-02776 | Deep Convolutional Neural Network for 6-DOF Image Localization | http://arxiv.org/abs/1611.02776 | id:1611.02776 author:Daoyuan Jia, Yongchi Su, Chunping Li category:cs.CV  published:2016-11-08 summary:We present an accurate and robust method for six degree of freedom image localization. There are two key-points of our method, 1. automatic immense photo synthesis and labeling from point cloud model and, 2. pose estimation with deep convolutional neural networks regression. Our model can directly regresses 6-DOF camera poses from images, accurately describing where and how it was captured. We achieved an accuracy within 1 meters and 1 degree on our out-door dataset, which covers about 2 acres on our school campus. version:1
arxiv-1611-02185 | Trusting SVM for Piecewise Linear CNNs | http://arxiv.org/abs/1611.02185 | id:1611.02185 author:Leonard Berrada, Andrew Zisserman, M. Pawan Kumar category:cs.LG  published:2016-11-07 summary:We present a novel layerwise optimization algorithm for the learning objective of a large class of convolutional neural networks (CNNs). Specifically, we consider CNNs that employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the problem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave- convex procedure, which requires us to iteratively solve a structured SVM problem. To this end, we extend the block-coordinate Frank-Wolfe (BCFW) algorithm in three important ways: (i) we include a trust-region for the parameters, which allows us to use the previous parameters as an initialization; (ii) we reduce the memory requirement of BCFW by potentially several orders of magnitude for the dense layers, which enables us to learn a large set of parameters; and (iii) we observe that, empirically, the optimal solution of the structured SVM problem can be obtained efficiently by solving a related, but significantly easier, multi-class SVM problem. Using publicly available data sets, we show that our approach outperforms the state of the art variants of backpropagation, and is also more robust to the hyperparameters of the learning objective. version:2
arxiv-1611-02770 | Delving into Transferable Adversarial Examples and Black-box Attacks | http://arxiv.org/abs/1611.02770 | id:1611.02770 author:Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song category:cs.LG  published:2016-11-08 summary:An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system. version:1
arxiv-1611-02767 | A backward pass through a CNN using a generative model of its activations | http://arxiv.org/abs/1611.02767 | id:1611.02767 author:Huayan Wang, Anna Chen, Yi Liu, Dileep George, D. Scott Phoenix category:cs.CV  published:2016-11-08 summary:Neural networks have shown to be a practical way of building a very complex mapping between a pre-specified input space and output space. For example, a convolutional neural network (CNN) mapping an image into one of a thousand object labels is approaching human performance in this particular task. However the mapping (neural network) does not automatically lend itself to other forms of queries, for example, to detect/reconstruct object instances, to enforce top-down signal on ambiguous inputs, or to recover object instances from occlusion. One way to address these queries is a backward pass through the network that fuses top-down and bottom-up information. In this paper, we show a way of building such a backward pass by defining a generative model of the neural network's activations. Approximate inference of the model would naturally take the form of a backward pass through the CNN layers, and it addresses the aforementioned queries in a unified framework. version:1
arxiv-1611-02764 | Inferring low-dimensional microstructure representations using convolutional neural networks | http://arxiv.org/abs/1611.02764 | id:1611.02764 author:Nicholas Lubbers, Turab Lookman, Kipton Barros category:physics.comp-ph cond-mat.mtrl-sci cs.CV  published:2016-11-08 summary:We apply recent advances in machine learning and computer vision to a central problem in materials informatics: The statistical representation of microstructural images. We use activations in a pre-trained convolutional neural network to provide a high-dimensional characterization of a set of synthetic microstructural images. Next, we use manifold learning to obtain a low-dimensional embedding of this statistical characterization. We show that the low-dimensional embedding extracts the parameters used to generate the images. According to a variety of metrics, the convolutional neural network method yields dramatically better embeddings than the analogous method derived from two-point correlations alone. version:1
arxiv-1611-02755 | Recursive Decomposition for Nonconvex Optimization | http://arxiv.org/abs/1611.02755 | id:1611.02755 author:Abram L. Friesen, Pedro Domingos category:cs.AI cs.LG stat.ML  published:2016-11-08 summary:Continuous optimization is an important problem in many areas of AI, including vision, robotics, probabilistic inference, and machine learning. Unfortunately, most real-world optimization problems are nonconvex, causing standard convex techniques to find only local optima, even with extensions like random restarts and simulated annealing. We observe that, in many cases, the local modes of the objective function have combinatorial structure, and thus ideas from combinatorial optimization can be brought to bear. Based on this, we propose a problem-decomposition approach to nonconvex optimization. Similarly to DPLL-style SAT solvers and recursive conditioning in probabilistic inference, our algorithm, RDIS, recursively sets variables so as to simplify and decompose the objective function into approximately independent sub-functions, until the remaining functions are simple enough to be optimized by standard techniques like gradient descent. The variables to set are chosen by graph partitioning, ensuring decomposition whenever possible. We show analytically that RDIS can solve a broad class of nonconvex optimization problems exponentially faster than gradient descent with random restarts. Experimentally, RDIS outperforms standard techniques on problems like structure from motion and protein folding. version:1
arxiv-1611-02739 | Recursive Regression with Neural Networks: Approximating the HJI PDE Solution | http://arxiv.org/abs/1611.02739 | id:1611.02739 author:Vicen√ß Rubies Royo category:cs.LG math.DS  published:2016-11-08 summary:Most machine learning applications using neural networks seek to approximate some function g(x) by minimizing some cost criterion. In the simplest case, if one has access to pairs of the form (x, y) where y = g(x), the problem can be framed as a simple regression problem. Beyond this family of problems, we find many important cases where g(x) is unknown so this approach is not always viable. However, similar to what we find in the work of Mnih et al. (2013), if we have some known properties of the function we are seeking to approximate, there is still hope to frame the problem as a regression problem. In this work, we show this in the context of trying to approximate the solution to a particular partial differential equation known as the Hamilton-Jacobi-Isaacs PDE found in the fields of control theory and robotics. version:1
arxiv-1611-02731 | Variational Lossy Autoencoder | http://arxiv.org/abs/1611.02731 | id:1611.02731 author:Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel category:cs.LG stat.ML  published:2016-11-08 summary:Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the code only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution $p(z)$ and decoding distribution $p(x z)$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks. version:1
arxiv-1611-02704 | Accelerating the BSM interpretation of LHC data with machine learning | http://arxiv.org/abs/1611.02704 | id:1611.02704 author:Gianfranco Bertone, Marc Peter Deisenroth, Jong Soo Kim, Sebastian Liem, Roberto Ruiz de Austri, Max Welling category:hep-ph stat.ML  published:2016-11-08 summary:The interpretation of Large Hadron Collider (LHC) data in the framework of Beyond the Standard Model (BSM) theories is hampered by the need to run computationally expensive event generators and detector simulators. Performing statistically convergent scans of high-dimensional BSM theories is consequently challenging, and in practice unfeasible for very high-dimensional BSM theories. We present here a new machine learning method that accelerates the interpretation of LHC data, by learning the relationship between BSM theory parameters and data. As a proof-of-concept, we demonstrate that this technique accurately predicts natural SUSY signal events in two signal regions at the High Luminosity LHC, up to four orders of magnitude faster than standard techniques. The new approach makes it possible to rapidly and accurately reconstruct the theory parameters of complex BSM theories, should an excess in the data be discovered at the LHC. version:1
arxiv-1611-02683 | Unsupervised Pretraining for Sequence to Sequence Learning | http://arxiv.org/abs/1611.02683 | id:1611.02683 author:Prajit Ramachandran, Peter J. Liu, Quoc V. Le category:cs.CL cs.LG cs.NE  published:2016-11-08 summary:Sequence to sequence models are successful tools for supervised sequence learning tasks, such as machine translation. Despite their success, these models still require much labeled data and it is unclear how to improve them using unlabeled data, which is much less expensive to obtain. In this paper, we present simple changes that lead to a significant improvement in the accuracy of seq2seq models when the labeled set is small. Our method intializes the encoder and decoder of the seq2seq model with the trained weights of two language models, and then all weights are jointly fine-tuned with labeled data. An additional language modeling loss can be used to regularize the model during fine-tuning. We apply this method to low-resource tasks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main finding is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state-of-the-art results on the WMT English$\rightarrow$German task. Our model obtains an improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English$\rightarrow$German. Our ablation study shows that pretraining helps seq2seq models in different ways depending on the nature of the task: translation benefits from the improved generalization whereas summarization benefits from the improved optimization. version:1
arxiv-1611-02945 | Heter-LP: A heterogeneous label propagation algorithm and its application in drug repositioning | http://arxiv.org/abs/1611.02945 | id:1611.02945 author:Maryam Lotfi Shahreza, Nasser Ghadiri, Seyed Rasul Mossavi, Jaleh Varshosaz, James Green category:q-bio.QM cs.LG 05C82  published:2016-11-08 summary:Drug repositioning offers an effective solution to drug discovery, saving both time and resources by finding new indications for existing drugs. Typically, a drug takes effect via its protein targets in the cell. As a result, it is necessary for drug development studies to conduct an investigation into the interrelationships of drugs, protein targets, and diseases. Although previous studies have made a strong case for the effectiveness of integrative network-based methods for predicting these interrelationships, little progress has been achieved in this regard within drug repositioning research. Moreover, the interactions of new drugs and targets (lacking any known targets and drugs, respectively) cannot be accurately predicted by most established methods. In this paper, we propose a novel semi-supervised heterogeneous label propagation algorithm named Heter-LP, which applies both local as well as global network features for data integration. To predict drug-target, disease-target, and drug-disease associations, we use information about drugs, diseases, and targets as collected from multiple sources at different levels. Our algorithm integrates these various types of data into a heterogeneous network and implements a label propagation algorithm to find new interactions. Statistical analyses of 10-fold cross-validation results and experimental analysis support the effectiveness of the proposed algorithm. version:1
arxiv-1611-02654 | Sentence Ordering using Recurrent Neural Networks | http://arxiv.org/abs/1611.02654 | id:1611.02654 author:Lajanugen Logeswaran, Honglak Lee, Dragomir Radev category:cs.CL cs.AI cs.LG  published:2016-11-08 summary:Modeling the structure of coherent texts is a task of great importance in NLP. The task of organizing a given set of sentences into a coherent order has been commonly used to build and evaluate models that understand such structure. In this work we propose an end-to-end neural approach based on the recently proposed set to sequence mapping framework to address the sentence ordering problem. Our model achieves state-of-the-art performance in the order discrimination task on two datasets widely used in the literature. We also consider a new interesting task of ordering abstracts from conference papers and research proposals and demonstrate strong performance against recent methods. Visualizing the sentence representations learned by the model shows that the model has captured high level logical structure in these paragraphs. The model also learns rich semantic sentence representations by learning to order texts, performing comparably to recent unsupervised representation learning methods in the sentence similarity and paraphrase detection tasks. version:1
arxiv-1611-02648 | Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders | http://arxiv.org/abs/1611.02648 | id:1611.02648 author:Nat Dilokthanakul, Pedro A. M. Mediano, Marta Garnelo, Matthew C. H. Lee, Hugh Salimbeni, Kai Arulkumaran, Murray Shanahan category:cs.LG cs.NE stat.ML  published:2016-11-08 summary:We study a variant of the variational autoencoder model with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the standard variational approach in these models is unsuited for unsupervised clustering, and mitigate this problem by leveraging a principled information-theoretic regularisation term known as consistency violation. Adding this term to the standard variational optimisation objective yields networks with both meaningful internal representations and well-defined clusters. We demonstrate the performance of this scheme on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving higher performance on unsupervised clustering classification than previous approaches. version:1
arxiv-1611-02644 | Multispectral Deep Neural Networks for Pedestrian Detection | http://arxiv.org/abs/1611.02644 | id:1611.02644 author:Jingjing Liu, Shaoting Zhang, Shu Wang, Dimitris N. Metaxas category:cs.CV  published:2016-11-08 summary:Multispectral pedestrian detection is essential for around-the-clock applications, e.g., surveillance and autonomous driving. We deeply analyze Faster R-CNN for multispectral pedestrian detection task and then model it into a convolutional network (ConvNet) fusion problem. Further, we discover that ConvNet-based pedestrian detectors trained by color or thermal images separately provide complementary information in discriminating human instances. Thus there is a large potential to improve pedestrian detection by using color and thermal images in DNNs simultaneously. We carefully design four ConvNet fusion architectures that integrate two-branch ConvNets on different DNNs stages, all of which yield better performance compared with the baseline detector. Our experimental results on KAIST pedestrian benchmark show that the Halfway Fusion model that performs fusion on the middle-level convolutional features outperforms the baseline method by 11% and yields a missing rate 3.5% lower than the other proposed architectures. version:1
arxiv-1611-02637 | Estimating motion with principal component regression strategies | http://arxiv.org/abs/1611.02637 | id:1611.02637 author:Felipe P. do Carmo, Vania Vieira Estrela, Joaquim Teixeira de Assis category:cs.CV  published:2016-11-08 summary:In this paper, two simple principal component regression methods for estimating the optical flow between frames of video sequences according to a pel-recursive manner are introduced. These are easy alternatives to dealing with mixtures of motion vectors in addition to the lack of prior information on spatial-temporal statistics (although they are supposed to be normal in a local sense). The 2D motion vector estimation approaches take into consideration simple image properties and are used to harmonize regularized least square estimates. Their main advantage is that no knowledge of the noise distribution is necessary, although there is an underlying assumption of localized smoothness. Preliminary experiments indicate that this approach provides robust estimates of the optical flow. version:1
arxiv-1611-01802 | Self-Wiring Question Answering Systems | http://arxiv.org/abs/1611.01802 | id:1611.01802 author:Ricardo Usbeck, Jonathan Huthmann, Nico Duldhardt, Axel-Cyrille Ngonga Ngomo category:cs.AI cs.CL cs.IR  published:2016-11-06 summary:Question answering (QA) has been the subject of a resurgence over the past years. The said resurgence has led to a multitude of question answering (QA) systems being developed both by companies and research facilities. While a few components of QA systems get reused across implementations, most systems do not leverage the full potential of component reuse. Hence, the development of QA systems is currently still a tedious and time-consuming process. We address the challenge of accelerating the creation of novel or tailored QA systems by presenting a concept for a self-wiring approach to composing QA systems. Our approach will allow the reuse of existing, web-based QA systems or modules while developing new QA platforms. To this end, it will rely on QA modules being described using the Web Ontology Language. Based on these descriptions, our approach will be able to automatically compose QA systems using a data-driven approach automatically. version:2
arxiv-1611-02815 | An Automated System for Essay Scoring of Online Exams in Arabic based on Stemming Techniques and Levenshtein Edit Operations | http://arxiv.org/abs/1611.02815 | id:1611.02815 author:Emad Fawzi Al-Shalabi category:cs.IR cs.CL  published:2016-11-08 summary:In this article, an automated system is proposed for essay scoring in Arabic language for online exams based on stemming techniques and Levenshtein edit operations. An online exam has been developed on the proposed mechanisms, exploiting the capabilities of light and heavy stemming. The implemented online grading system has shown to be an efficient tool for automated scoring of essay questions. version:1
arxiv-1611-02554 | The Neural Noisy Channel | http://arxiv.org/abs/1611.02554 | id:1611.02554 author:Lei Yu, Phil Blunsom, Chris Dyer, Edward Grefenstette, Tomas Kocisky category:cs.CL cs.AI cs.NE  published:2016-11-08 summary:We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use. version:1
arxiv-1611-02550 | Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches | http://arxiv.org/abs/1611.02550 | id:1611.02550 author:Shane Settle, Karen Livescu category:cs.CL  published:2016-11-08 summary:Acoustic word embeddings --- fixed-dimensional vector representations of variable-length spoken word segments --- have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still relatively unexplored. In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs). We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a "Siamese network" training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure. version:1
arxiv-1611-03305 | Getting Started with Neural Models for Semantic Matching in Web Search | http://arxiv.org/abs/1611.03305 | id:1611.03305 author:Kezban Dilek Onal, Ismail Sengor Altingovde, Pinar Karagoz, Maarten de Rijke category:cs.IR cs.CL  published:2016-11-08 summary:The vocabulary mismatch problem is a long-standing problem in information retrieval. Semantic matching holds the promise of solving the problem. Recent advances in language technology have given rise to unsupervised neural models for learning representations of words as well as bigger textual units. Such representations enable powerful semantic matching methods. This survey is meant as an introduction to the use of neural models for semantic matching. To remain focused we limit ourselves to web search. We detail the required background and terminology, a taxonomy grouping the rapidly growing body of work in the area, and then survey work on neural models for semantic matching in the context of three tasks: query suggestion, ad retrieval, and document retrieval. We include a section on resources and best practices that we believe will help readers who are new to the area. We conclude with an assessment of the state-of-the-art and suggestions for future work. version:1
arxiv-1611-02525 | The Loss Surface of Residual Networks: Ensembles and the Role of Batch Normalization | http://arxiv.org/abs/1611.02525 | id:1611.02525 author:Etai Littwin, Lior Wolf category:cs.CV  published:2016-11-08 summary:Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensembles are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network's depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch Normalization technique. We explain this behavior and demonstrate the driving force behind it. As a main tool in our analysis, we employ generalized spin glass models, which we also use in order to study the number of critical points in the optimization of Residual Networks. version:1
arxiv-1611-02512 | Cognitive Discriminative Mappings for Rapid Learning | http://arxiv.org/abs/1611.02512 | id:1611.02512 author:Wen-Chieh Fang, Yi-ting Chiang category:cs.AI cs.LG cs.NE  published:2016-11-08 summary:Humans can learn concepts or recognize items from just a handful of examples, while machines require many more samples to perform the same task. In this paper, we build a computational model to investigate the possibility of this kind of rapid learning. The proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long-term memory. We present a simple and intuitive technique called cognitive discriminative mappings (CDM) to explore the cognitive problem. First, CDM separates and clusters the data instances retrieved from long-term memory into distinct classes with a discrimination method in working memory when a sensory input triggers the algorithm. CDM then maps each sensory data instance to be as close as possible to the median point of the data group with the same class. The experimental results demonstrate that the CDM approach is effective for learning the discriminative features of supervised classifications with few training sensory input instances. version:1
arxiv-1611-02695 | Automatic recognition of child speech for robotic applications in noisy environments | http://arxiv.org/abs/1611.02695 | id:1611.02695 author:Samuel Fernando, Roger K. Moore, David Cameron, Emily C. Collins, Abigail Millings, Amanda J. Sharkey, Tony J. Prescott category:cs.CL cs.SD  published:2016-11-08 summary:Automatic speech recognition (ASR) allows a natural and intuitive interface for robotic educational applications for children. However there are a number of challenges to overcome to allow such an interface to operate robustly in realistic settings, including the intrinsic difficulties of recognising child speech and high levels of background noise often present in classrooms. As part of the EU EASEL project we have provided several contributions to address these challenges, implementing our own ASR module for use in robotics applications. We used the latest deep neural network algorithms which provide a leap in performance over the traditional GMM approach, and apply data augmentation methods to improve robustness to noise and speaker variation. We provide a close integration between the ASR module and the rest of the dialogue system, allowing the ASR to receive in real-time the language models relevant to the current section of the dialogue, greatly improving the accuracy. We integrated our ASR module into an interactive, multimodal system using a small humanoid robot to help children learn about exercise and energy. The system was installed at a public museum event as part of a research study where 320 children (aged 3 to 14) interacted with the robot, with our ASR achieving 90% accuracy for fluent and near-fluent speech. version:1
arxiv-1611-02443 | Domain Adaptation with L2 constraints for classifying images from different endoscope systems | http://arxiv.org/abs/1611.02443 | id:1611.02443 author:Toru Tamaki, Shoji Sonoyama, Takio Kurita, Tsubasa Hirakawa, Bisser Raytchev, Kazufumi Kaneda, Tetsushi Koide, Shigeto Yoshida, Hiroshi Mieno, Shinji Tanaka, Kazuaki Chayama category:cs.CV cs.LG  published:2016-11-08 summary:This paper proposes a method for domain adaptation that extends the maximum margin domain transfer (MMDT) proposed by Hoffman et al., by introducing L_2 distance constraints between samples of different domains; thus, our method is denoted as MMDTL2. Motivated by the differences between the images taken by narrow band imaging (NBI) endoscopic devices, we utilize different NBI devices as different domains and estimate the transformations between samples of different domains, i.e., image samples taken by different NBI endoscope systems. We first formulate the problem in the primal form, and then derive the dual form with much lesser computational costs as compared to the naive approach. From our experimental results using NBI image datasets from two different NBI endoscopic devices, we find that MMDTL2 is more stable than MMDT and better than support vector machines without adaptation. version:1
arxiv-1611-02440 | A Bayesian optimization approach to find Nash equilibria | http://arxiv.org/abs/1611.02440 | id:1611.02440 author:Victor Picheny, Mickael Binois, Abderrahmane Habbal category:stat.ML  published:2016-11-08 summary:Game theory finds nowadays a broad range of applications in engineering and machine learning. However, in a derivative-free, expensive black-box context, very few algorithmic solutions are available to find game equilibria. Here, we propose a novel Gaussian-process based approach for solving games in this context. We follow a classical Bayesian optimization framework, with sequential sampling decisions based on acquisition functions. Two strategies are proposed, based either on the probability of achieving equilibrium or on the Stepwise Uncertainty Reduction paradigm. Practical and numerical aspects are discussed in order to enhance the scalability and reduce computation time. Our approach is evaluated on several synthetic game problems with varying number of players and decision space dimensions. We show that equilibria can be found reliably for a fraction of the cost (in terms of black-box evaluations) compared to classical, derivative-based algorithms. version:1
arxiv-1611-02091 | Building a comprehensive syntactic and semantic corpus of Chinese clinical texts | http://arxiv.org/abs/1611.02091 | id:1611.02091 author:Bin He, Bin Dong, Yi Guan, Jinfeng Yang, Zhipeng Jiang, Qiubin Yu, Jianyi Cheng, Chunyan Qu category:cs.CL  published:2016-11-07 summary:Objective: To build a comprehensive corpus covering syntactic and semantic annotations of Chinese clinical texts with corresponding annotation guidelines and methods as well as to develop tools trained on the annotated corpus, which supplies baselines for research on Chinese texts in the clinical domain. Materials and methods: An iterative annotation method was proposed to train annotators and to develop annotation guidelines. Then, by using annotation quality assurance measures, a comprehensive corpus was built, containing annotations of part-of-speech (POS) tags, syntactic tags, entities, assertions, and relations. Inter-annotator agreement (IAA) was calculated to evaluate the annotation quality and a Chinese clinical text processing and information extraction system (CCTPIES) was developed based on our annotated corpus. Results: The syntactic corpus consists of 138 Chinese clinical documents with 47,424 tokens and 2553 full parsing trees, while the semantic corpus includes 992 documents that annotated 39,511 entities with their assertions and 7695 relations. IAA evaluation shows that this comprehensive corpus is of good quality, and the system modules are effective. Discussion: The annotated corpus makes a considerable contribution to natural language processing (NLP) research into Chinese texts in the clinical domain. However, this corpus has a number of limitations. Some additional types of clinical text should be introduced to improve corpus coverage and active learning methods should be utilized to promote annotation efficiency. Conclusions: In this study, several annotation guidelines and an annotation method for Chinese clinical texts were proposed, and a comprehensive corpus with its NLP modules were constructed, providing a foundation for further study of applying NLP techniques to Chinese texts in the clinical domain. version:2
arxiv-1611-01929 | Deep Reinforcement Learning with Averaged Target DQN | http://arxiv.org/abs/1611.01929 | id:1611.01929 author:Oron Anschel, Nir Baram, Nahum Shimkin category:cs.AI cs.LG stat.ML  published:2016-11-07 summary:The commonly used Q-learning algorithm combined with function approximation induces systematic overestimations of state-action values. These systematic errors might cause instability, poor performance and sometimes divergence of learning. In this work, we present the Averaged Target DQN (ADQN) algorithm, an adaptation to the DQN class of algorithms which uses a weighted average over past learned networks to reduce generalization noise variance. As a consequence, this leads to reduced overestimations, more stable learning process and improved performance. Additionally, we analyze ADQN variance reduction along trajectories and demonstrate the performance of ADQN on a toy Gridworld problem, as well as on several of the Atari 2600 games from the Arcade Learning Environment. version:2
arxiv-1611-02416 | An Efficient Approach to Boosting Performance of Deep Spiking Network Training | http://arxiv.org/abs/1611.02416 | id:1611.02416 author:Seongsik Park, Sang-gil Lee, Hyunha Nam, Sungroh Yoon category:cs.LG cs.NE  published:2016-11-08 summary:Nowadays deep learning is dominating the field of machine learning with state-of-the-art performance in various application areas. Recently, spiking neural networks (SNNs) have been attracting a great deal of attention, notably owning to their power efficiency, which can potentially allow us to implement a low-power deep learning engine suitable for real-time/mobile applications. However, implementing SNN-based deep learning remains challenging, especially gradient-based training of SNNs by error backpropagation. We cannot simply propagate errors through SNNs in conventional way because of the property of SNNs that process discrete data in the form of a series. Consequently, most of the previous studies employ a workaround technique, which first trains a conventional weighted-sum deep neural network and then maps the learning weights to the SNN under training, instead of training SNN parameters directly. In order to eliminate this workaround, recently proposed is a new class of SNN named deep spiking networks (DSNs), which can be trained directly (without a mapping from conventional deep networks) by error backpropagation with stochastic gradient descent. In this paper, we show that the initialization of the membrane potential on the backward path is an important step in DSN training, through diverse experiments performed under various conditions. Furthermore, we propose a simple and efficient method that can improve DSN training by controlling the initial membrane potential on the backward path. In our experiments, adopting the proposed approach allowed us to boost the performance of DSN training in terms of converging time and accuracy. version:1
arxiv-1611-02385 | Combining observational and experimental data to find heterogeneous treatment effects | http://arxiv.org/abs/1611.02385 | id:1611.02385 author:Alexander Peysakhovich, Akos Lada category:cs.AI stat.ML  published:2016-11-08 summary:Every design choice will have different effects on different units. However traditional A/B tests are often underpowered to identify these heterogeneous effects. This is especially true when the set of unit-level attributes is high-dimensional and our priors are weak about which particular covariates are important. However, there are often observational data sets available that are orders of magnitude larger. We propose a method to combine these two data sources to estimate heterogeneous treatment effects. First, we use observational time series data to estimate a mapping from covariates to unit-level effects. These estimates are likely biased but under some conditions the bias preserves unit-level relative rank orderings. If these conditions hold, we only need sufficient experimental data to identify a monotonic, one-dimensional transformation from observationally predicted treatment effects to real treatment effects. This reduces power demands greatly and makes the detection of heterogeneous effects much easier. As an application, we show how our method can be used to improve Facebook page recommendations. version:1
arxiv-1611-02378 | A Surrogate-based Generic Classifier for Chinese Movie Reviews | http://arxiv.org/abs/1611.02378 | id:1611.02378 author:Yufeng Ma, Long Xia, Wenqi Shen, Weiguo Fan category:cs.CL  published:2016-11-08 summary:With the emerging of various online video platforms like Youtube, Youku and LeTV, online movie reviews become more and more important both for movie viewers and producers. As a result, automatically classifying reviews according to different requirements evolves as a popular research topic and is very essential in our daily life. In this paper, we focused on reviews of hot TV series in China and successfully trained generic classifiers based on 8 predefined categories. The experimental results showed promising performance and effectiveness of its generalization to different TV series. version:1
arxiv-1611-02365 | An Online Prediction Framework for Non-Stationary Time Series | http://arxiv.org/abs/1611.02365 | id:1611.02365 author:Christopher Xie, Avleen Bijral, Juan Lavista Ferres category:stat.ML cs.LG  published:2016-11-08 summary:We extend an online time series prediction algorithm for ARMA processes to describe a framework for time series prediction that can efficiently handle non-stationarities that exist in many real time series. We show that appropriate transformations to such time series can lead to theoretical and empirical gains. To account for the phenomenon of cointegration in the multivariate case, we present a novel algorithm EC-VARMA-OGD that estimates both the auto-regressive and the cointegrating parameters. Relaxing the assumptions for the analysis, we prove a sub-linear regret bound for all the methods described. We note that the theoretical guarantees do not provide a complete picture, thus we provide a data-dependent analysis of the follow-the-leader algorithm for least squares loss that explains the success of using non-stationary transformations. We support all of our results with experiments on simulated and real data. version:1
arxiv-1611-02364 | Multiple Object Tracking with Kernelized Correlation Filters in Urban Mixed Traffic | http://arxiv.org/abs/1611.02364 | id:1611.02364 author:Yuebin Yang, Guillaume-Alexandre Bilodeau category:cs.CV  published:2016-11-08 summary:Recently, the Kernelized Correlation Filters tracker (KCF) achieved competitive performance and robustness in visual object tracking. On the other hand, visual trackers are not typically used in multiple object tracking. In this paper, we investigate how a robust visual tracker like KCF can improve multiple object tracking. Since KCF is a fast tracker, many can be used in parallel and still result in fast tracking. We build a multiple object tracking system based on KCF and background subtraction. Background subtraction is applied to extract moving objects and get their scale and size in combination with KCF outputs, while KCF is used for data association and to handle fragmentation and occlusion problems. As a result, KCF and background subtraction help each other to take tracking decision at every frame. Sometimes KCF outputs are the most trustworthy (e.g. during occlusion), while in some other case, it is the background subtraction outputs. To validate the effectiveness of our system, the algorithm is demonstrated on four urban video recordings from a standard dataset. Results show that our method is competitive with state-of-the-art trackers even if we use a much simpler data association step. version:1
arxiv-1611-02361 | Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents | http://arxiv.org/abs/1611.02361 | id:1611.02361 author:Rui Zhang, Honglak Lee, Dragomir Radev category:cs.CL  published:2016-11-08 summary:The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a general-purpose classification system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long Short-Term Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentence-level tasks. Moreover, unlike other CNN-based models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-of-the-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification. version:1
arxiv-1611-02360 | Cruciform: Solving Crosswords with Natural Language Processing | http://arxiv.org/abs/1611.02360 | id:1611.02360 author:Dragomir Radev, Rui Zhang, Steve Wilson, Derek Van Assche, Henrique Spyra Gubert, Alisa Krivokapic, MeiXing Dong, Chongruo Wu, Spruce Bondera, Luke Brandl, Jeremy Dohmann category:cs.CL  published:2016-11-08 summary:Crossword puzzles are popular word games that require not only a large vocabulary, but also a broad knowledge of topics. Answering each clue is a natural language task on its own as many clues contain nuances, puns, or counter-intuitive word definitions. Additionally, it can be extremely difficult to ascertain definitive answers without the constraints of the crossword grid itself. This task is challenging for both humans and computers. We describe here a new crossword solving system, Cruciform. We employ a group of natural language components, each of which returns a list of candidate words with scores when given a clue. These lists are used in conjunction with the fill intersections in the puzzle grid to formulate a constraint satisfaction problem, in a manner similar to the one used in the Dr. Fill system. We describe the results of several of our experiments with the system. version:1
arxiv-1611-02345 | Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks | http://arxiv.org/abs/1611.02345 | id:1611.02345 author:David Balduzzi, Brian McWilliams, Tony Butler-Yeoman category:cs.LG cs.NE stat.ML  published:2016-11-07 summary:Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets. The guarantee matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation -- a straightforward application of Taylor expansions to neural networks -- and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization. The second half of the paper applies the Taylor approximation to isolate the main difficulty in training rectifier nets: that gradients are shattered. We investigate the hypothesis that, by exploring the space of activation configurations more thoroughly, adaptive optimizers such as RMSProp and Adam are able to converge to better solutions. version:1
arxiv-1611-02337 | Balotage in Argentina 2015, a sentiment analysis of tweets | http://arxiv.org/abs/1611.02337 | id:1611.02337 author:Daniel Robins, Fernando Emmanuel Frati, Jonatan Alvarez, Jose Texier category:cs.IR cs.CL cs.SI  published:2016-11-07 summary:Twitter social network contains a large amount of information generated by its users. That information is composed of opinions and comments that may reflect trends in social behavior. There is talk of trend when it is possible to identify opinions and comments geared towards the same shared by a lot of people direction. To determine if two or more written opinions share the same address, techniques Natural Language Processing (NLP) are used. This paper proposes a methodology for predicting reflected in Twitter from the use of sentiment analysis functions NLP based on social behaviors. The case study was selected the 2015 Presidential in Argentina, and a software architecture Big Data composed Vertica data base with the component called Pulse was used. Through the analysis it was possible to detect trends in voting intentions with regard to the presidential candidates, achieving greater accuracy in predicting that achieved with traditional systems surveys. version:1
arxiv-1611-02320 | Adversarial Ladder Networks | http://arxiv.org/abs/1611.02320 | id:1611.02320 author:Juan Maro√±as Molano, Alberto Albiol Colomer, Roberto Paredes Palacios category:cs.NE cs.LG stat.ML  published:2016-11-07 summary:The use of unsupervised data in addition to supervised data in training discriminative neural networks has improved the performance of this clas- sification scheme. However, the best results were achieved with a training process that is divided in two parts: first an unsupervised pre-training step is done for initializing the weights of the network and after these weights are refined with the use of supervised data. On the other hand adversarial noise has improved the results of clas- sical supervised learning. Recently, a new neural network topology called Ladder Network, where the key idea is based in some properties of hierar- chichal latent variable models, has been proposed as a technique to train a neural network using supervised and unsupervised data at the same time with what is called semi-supervised learning. This technique has reached state of the art classification. In this work we add adversarial noise to the ladder network and get state of the art classification, with several important conclusions on how adversarial noise can help in addition with new possible lines of investi- gation. We also propose an alternative to add adversarial noise to unsu- pervised data. version:1
arxiv-1611-02315 | Learning from Untrusted Data | http://arxiv.org/abs/1611.02315 | id:1611.02315 author:Moses Charikar, Jacob Steinhardt, Gregory Valiant category:cs.LG cs.AI cs.CC cs.CR math.ST stat.TH  published:2016-11-07 summary:The vast majority of theoretical results in machine learning and statistics assume that the available training data is a reasonably reliable reflection of the phenomena to be learned or estimated. Similarly, the majority of machine learning and statistical techniques used in practice are brittle to the presence of large amounts of biased or malicious data. In this work we propose two novel frameworks in which to study estimation, learning, and optimization in the presence of significant fractions of arbitrary data. The first framework, which we term list-decodable learning, asks whether it is possible to return a list of answers, with the guarantee that at least one of them is accurate. For example, given a dataset of $n$ points for which an unknown subset of $\alpha n$ points are drawn from a distribution of interest, and no assumptions are made about the remaining $(1-\alpha)n$ points, is it possible to return a list of $poly(1/\alpha)$ answers, one of which is correct? The second framework, which we term the semi-verified learning model considers the extent to which a small dataset of trusted data (drawn from the distribution in question) can be leveraged to enable the accurate extraction of information from a much larger but untrusted dataset (of which only an $\alpha$-fraction is drawn from the distribution). We show strong positive results in both settings, and provide an algorithm for robust learning in a very general stochastic optimization setting. This general result has immediate implications for robust estimation in a number of settings, including for robustly estimating the mean of distributions with bounded second moments, robustly learning mixtures of such distributions, and robustly finding planted partitions in random graphs in which significant portions of the graph have been perturbed by an adversary. version:1
arxiv-1611-02305 | Learning Influence Functions from Incomplete Observations | http://arxiv.org/abs/1611.02305 | id:1611.02305 author:Xinran He, Ke Xu, David Kempe, Yan Liu category:cs.SI cs.LG stat.ML  published:2016-11-07 summary:We study the problem of learning influence functions under incomplete observations of node activations. Incomplete observations are a major concern as most (online and real-world) social networks are not fully observable. We establish both proper and improper PAC learnability of influence functions under randomly missing observations. Proper PAC learnability under the Discrete-Time Linear Threshold (DLT) and Discrete-Time Independent Cascade (DIC) models is established by reducing incomplete observations to complete observations in a modified graph. Our improper PAC learnability result applies for the DLT and DIC models as well as the Continuous-Time Independent Cascade (CIC) model. It is based on a parametrization in terms of reachability features, and also gives rise to an efficient and practical heuristic. Experiments on synthetic and real-world datasets demonstrate the ability of our method to compensate even for a fairly large fraction of missing observations. version:1
arxiv-1611-02268 | Optimal Binary Autoencoding with Pairwise Correlations | http://arxiv.org/abs/1611.02268 | id:1611.02268 author:Akshay Balsubramani category:cs.LG cs.AI stat.ML  published:2016-11-07 summary:We formulate learning of a binary autoencoder as a biconvex optimization problem which learns from the pairwise correlations between encoded and decoded bits. Among all possible algorithms that use this information, ours finds the autoencoder that reconstructs its inputs with worst-case optimal loss. The optimal decoder is a single layer of artificial neurons, emerging entirely from the minimax loss minimization, and with weights learned by convex optimization. All this is reflected in competitive experimental results, demonstrating that binary autoencoding can be done efficiently by conveying information in pairwise correlations in an optimal fashion. version:1
arxiv-1611-02266 | Gaussian Attention Model and Its Application to Knowledgebase Embedding and Question Answering | http://arxiv.org/abs/1611.02266 | id:1611.02266 author:Liwen Zhang, John Winn, Ryota Tomioka category:stat.ML cs.AI cs.CL cs.LG  published:2016-11-07 summary:We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to blurred attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledgebase into a continuous vector space and then train a model that performs question answering about the entities in the knowledgebase. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of thoughts in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well. version:1
arxiv-1611-02260 | Meat adulteration detection through digital image analysis of histological cuts using LBP | http://arxiv.org/abs/1611.02260 | id:1611.02260 author:Jo√£o J. de Macedo Neto, Jefersson A. dos Santos, William Robson Schwartz category:cs.CV  published:2016-11-07 summary:Food fraud has been an area of great concern due to its risk to public health, reduction of food quality or nutritional value and for its economic consequences. For this reason, it's been object of regulation in many countries (e.g. [1], [2]). One type of food that has been frequently object of fraud through the addition of water or an aqueous solution is bovine meat. The traditional methods used to detect this kind of fraud are expensive, time-consuming and depend on physicochemical analysis that require complex laboratory techniques, specific for each added substance. In this paper, based on digital images of histological cuts of adulterated and not-adulterated (normal) bovine meat, we evaluate the of digital image analysis methods to identify the aforementioned kind of fraud, with focus on the Local Binary Pattern (LBP) algorithm. version:1
arxiv-1611-02258 | Learning Time Series Detection Models from Temporally Imprecise Labels | http://arxiv.org/abs/1611.02258 | id:1611.02258 author:Roy J. Adams, Benjamin M. Marlin category:stat.ML cs.LG  published:2016-11-07 summary:In this paper, we consider a new low-quality label learning problem: learning time series detection models from temporally imprecise labels. In this problem, the data consist of a set of input time series, and supervision is provided by a sequence of noisy time stamps corresponding to the occurrence of positive class events. Such temporally imprecise labels commonly occur in areas like mobile health research where human annotators are tasked with labeling the occurrence of very short duration events. We propose a general learning framework for this problem that can accommodate different base classifiers and noise models. We present results on real mobile health data showing that the proposed framework significantly outperforms a number of alternatives including assuming that the label time stamps are noise-free, transforming the problem into the multiple instance learning framework, and learning on labels that were manually re-aligned. version:1
arxiv-1611-02252 | Hierarchical compositional feature learning | http://arxiv.org/abs/1611.02252 | id:1611.02252 author:Miguel L√°zaro-Gredilla, Yi Liu, D. Scott Phoenix, Dileep George category:cs.LG cs.AI stat.ML  published:2016-11-07 summary:We introduce the hierarchical compositional network (HCN), a directed generative model able to discover and disentangle, without supervision, the building blocks of a set of binary images. The building blocks are binary features defined hierarchically as a composition of some of the features in the layer immediately below, arranged in a particular manner. At a high level, HCN is similar to a sigmoid belief network with pooling. Inference and learning in HCN are very challenging and existing variational approximations do not work satisfactorily. A main contribution of this work is to show that both can be addressed using max-product message passing (MPMP) with a particular schedule (no EM required). Also, using MPMP as an inference engine for HCN makes new tasks simple: adding supervision information, classifying images, or performing inpainting all correspond to clamping some variables of the model to their known values and running MPMP on the rest. When used for classification, fast inference with HCN has exactly the same functional form as a convolutional neural network (CNN) with linear activations and binary weights. However, HCN's features are qualitatively very different. version:1
arxiv-1611-02247 | Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic | http://arxiv.org/abs/1611.02247 | id:1611.02247 author:Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, Sergey Levine category:cs.LG  published:2016-11-07 summary:Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is the high sample complexity of such methods. Unbiased batch policy-gradient methods offer stable learning, but at the cost of high variance, which often requires large batches, while TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of unbiased policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments. version:1
arxiv-1611-02221 | Minimax-optimal semi-supervised regression on unknown manifolds | http://arxiv.org/abs/1611.02221 | id:1611.02221 author:Amit Moscovich, Ariel Jaffe, Boaz Nadler category:stat.ML cs.LG 62G08  published:2016-11-07 summary:We consider the problem of semi-supervised regression when the predictor variables are drawn from an unknown manifold. A simple approach to this problem is to first use both the labeled and unlabeled data to estimate the manifold geodesic distance between pairs of points, and then apply a k nearest neighbor regressor based on these distance estimates. We prove that given sufficiently many unlabeled points, this simple method which we dub geodesic kNN regression, achieves the optimal finite-sample minimax bound on the mean squared error, as if the manifold were completely specified. Furthermore, we show how this approach can be efficiently implemented requiring only O(k N log N) operations to estimate the regression function at all N labeled and unlabeled points. We illustrate this approach on two datasets with a manifold structure: indoor localization using WiFi fingerprints and facial pose estimation. For both problems we obtain better results than the popular procedure based on Laplacian eigenvectors. version:1
arxiv-1611-02205 | Playing SNES in the Retro Learning Environment | http://arxiv.org/abs/1611.02205 | id:1611.02205 author:Nadav Bhonker, Shai Rozenberg, Itay Hubara category:cs.LG cs.AI  published:2016-11-07 summary:Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a results, the Arcade Learning Environment (ALE) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. Most Atari games no longer pose a challenge to state-of-the-art algorithms. In this paper we introduce a new learning environment, the Retro Learning Environment --- RLE, based on the Super Nintendo Entertainment System (SNES). The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility. To overcome these challenges, we introduce a novel training method based on training two agents against each other. version:1
arxiv-1611-02200 | Unsupervised Cross-Domain Image Generation | http://arxiv.org/abs/1611.02200 | id:1611.02200 author:Yaniv Taigman, Adam Polyak, Lior Wolf category:cs.CV  published:2016-11-07 summary:We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity. version:1
arxiv-1611-02189 | CoCoA: A General Framework for Communication-Efficient Distributed Optimization | http://arxiv.org/abs/1611.02189 | id:1611.02189 author:Virginia Smith, Simone Forte, Chenxin Ma, Martin Takac, Michael I. Jordan, Martin Jaggi category:cs.LG  published:2016-11-07 summary:The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for the distributed environment, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets. version:1
arxiv-1611-02181 | Using Social Dynamics to Make Individual Predictions: Variational Inference with a Stochastic Kinetic Model | http://arxiv.org/abs/1611.02181 | id:1611.02181 author:Zhen Xu, Wen Dong, Sargur Srihari category:stat.ML cs.LG  published:2016-11-07 summary:Social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors, modeling the temporal evolution of social systems via the interactions of individuals within these systems. In particular, the availability of large-scale data from social networks and sensor networks offers an unprecedented opportunity to predict state-changing events at the individual level. Examples of such events include disease transmission, opinion transition in elections, and rumor propagation. Unlike previous research focusing on the collective effects of social systems, this study makes efficient inferences at the individual level. In order to cope with dynamic interactions among a large number of individuals, we introduce the stochastic kinetic model to capture adaptive transition probabilities and propose an efficient variational inference algorithm the complexity of which grows linearly --- rather than exponentially --- with the number of individuals. To validate this method, we have performed epidemic-dynamics experiments on wireless sensor network data collected from more than ten thousand people over three years. The proposed algorithm was used to track disease transmission and predict the probability of infection for each individual. Our results demonstrate that this method is more efficient than sampling while nonetheless achieving high accuracy. version:1
arxiv-1611-02167 | Designing Neural Network Architectures using Reinforcement Learning | http://arxiv.org/abs/1611.02167 | id:1611.02167 author:Bowen Baker, Otkrist Gupta, Nikhil Naik, Ramesh Raskar category:cs.LG  published:2016-11-07 summary:At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We propose a meta-modelling approach based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Q-learning with an $\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing network design meta-modelling approaches on image classification. version:1
arxiv-1611-02163 | Unrolled Generative Adversarial Networks | http://arxiv.org/abs/1611.02163 | id:1611.02163 author:Luke Metz, Ben Poole, David Pfau, Jascha Sohl-Dickstein category:cs.LG stat.ML  published:2016-11-07 summary:We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator. version:1
arxiv-1611-02155 | Spatiotemporal Residual Networks for Video Action Recognition | http://arxiv.org/abs/1611.02155 | id:1611.02155 author:Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes category:cs.CV  published:2016-11-07 summary:Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping these with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. version:1
arxiv-1611-02145 | Crowdsourcing in Computer Vision | http://arxiv.org/abs/1611.02145 | id:1611.02145 author:Adriana Kovashka, Olga Russakovsky, Li Fei-Fei, Kristen Grauman category:cs.CV cs.HC  published:2016-11-07 summary:Computer vision systems require large amounts of manually annotated data to properly learn challenging visual concepts. Crowdsourcing platforms offer an inexpensive method to capture human knowledge and understanding, for a vast number of visual perception tasks. In this survey, we describe the types of annotations computer vision researchers have collected using crowdsourcing, and how they have ensured that this data is of high quality while annotation effort is minimized. We begin by discussing data collection on both classic (e.g., object recognition) and recent (e.g., visual story-telling) vision tasks. We then summarize key design decisions for creating effective data collection interfaces and workflows, and present strategies for intelligently selecting the most important data instances to annotate. Finally, we conclude with some thoughts on the future of crowdsourcing in computer vision. version:1
arxiv-1611-02120 | Neural Networks Designing Neural Networks: Multi-Objective Hyper-Parameter Optimization | http://arxiv.org/abs/1611.02120 | id:1611.02120 author:Sean C. Smithson, Guang Yang, Warren J. Gross, Brett H. Meyer category:cs.NE cs.LG  published:2016-11-07 summary:Artificial neural networks have gone through a recent rise in popularity, achieving state-of-the-art results in various fields, including image classification, speech recognition, and automated control. Both the performance and computational complexity of such models are heavily dependant on the design of characteristic hyper-parameters (e.g., number of hidden layers, nodes per layer, or choice of activation functions), which have traditionally been optimized manually. With machine learning penetrating low-power mobile and embedded areas, the need to optimize not only for performance (accuracy), but also for implementation complexity, becomes paramount. In this work, we present a multi-objective design space exploration method that reduces the number of solution networks trained and evaluated through response surface modelling. Given spaces which can easily exceed 1020 solutions, manually designing a near-optimal architecture is unlikely as opportunities to reduce network complexity, while maintaining performance, may be overlooked. This problem is exacerbated by the fact that hyper-parameters which perform well on specific datasets may yield sub-par results on others, and must therefore be designed on a per-application basis. In our work, machine learning is leveraged by training an artificial neural network to predict the performance of future candidate networks. The method is evaluated on the MNIST and CIFAR-10 image datasets, optimizing for both recognition accuracy and computational complexity. Experimental results demonstrate that the proposed method can closely approximate the Pareto-optimal front, while only exploring a small fraction of the design space. version:1
arxiv-1611-03021 | Attributing Hacks | http://arxiv.org/abs/1611.03021 | id:1611.03021 author:Ziqi Liu, Alexander J. Smola, Kyle Soska, Yu-Xiang Wang, Qinghua Zheng category:cs.LG cs.CR stat.AP  published:2016-11-07 summary:In this paper we describe an algorithm for estimating the provenance of hacks on websites. That is, given properties of sites and the temporal occurrence of attacks, we are able to attribute individual attacks to joint causes and vulnerabilities, as well as estimating the evolution of these vulnerabilities over time. Specifically, we use hazard regression with a time-varying additive hazard function parameterized in a generalized linear form. The activation coefficients on each feature are continuous-time functions over time. We formulate the problem of learning these functions as a constrained variational maximum likelihood estimation problem with total variation penalty and show that the optimal solution is a 0th order spline (a piecewise constant function) with a finite number of known knots. This allows the inference problem to be solved efficiently and at scale by solving a finite dimensional optimization problem. Extensive experiments on real data sets show that our method significantly outperforms Cox's proportional hazard model. We also conduct a case study and verify that the fitted functions are indeed recovering vulnerable features and real-life events such as the release of code to exploit these features in hacker blogs. version:1
arxiv-1611-02109 | Lifelong Perceptual Programming By Example | http://arxiv.org/abs/1611.02109 | id:1611.02109 author:Alexander L. Gaunt, Marc Brockschmidt, Nate Kushman, Daniel Tarlow category:cs.LG  published:2016-11-07 summary:We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks. version:1
arxiv-1611-02102 | Texture and Color-based Image Retrieval Using the Local Extrema Features and Riemannian Distance | http://arxiv.org/abs/1611.02102 | id:1611.02102 author:Minh-Tan Pham, Gr√©goire Mercier, Lionel Bombrun, Julien Michel category:cs.CV  published:2016-11-07 summary:A novel efficient method for content-based image retrieval (CBIR) is developed in this paper using both texture and color features. Our motivation is to represent and characterize an input image by a set of local descriptors extracted at characteristic points (i.e. keypoints) within the image. Then, dissimilarity measure between images is calculated based on the geometric distance between the topological feature spaces (i.e. manifolds) formed by the sets of local descriptors generated from these images. In this work, we propose to extract and use the local extrema pixels as our feature points. Then, the so-called local extrema-based descriptor (LED) is generated for each keypoint by integrating all color, spatial as well as gradient information captured by a set of its nearest local extrema. Hence, each image is encoded by a LED feature point cloud and riemannian distances between these point clouds enable us to tackle CBIR. Experiments performed on Vistex, Stex and colored Brodatz texture databases using the proposed approach provide very efficient and competitive results compared to the state-of-the-art methods. version:1
arxiv-1611-02101 | Distributed Coordinate Descent for Generalized Linear Models with Regularization | http://arxiv.org/abs/1611.02101 | id:1611.02101 author:Ilya Trofimov, Alexander Genkin category:stat.ML cs.DC cs.LG  published:2016-11-07 summary:Generalized linear model with $L_1$ and $L_2$ regularization is a widely used technique for solving classification, class probability estimation and regression problems. With the numbers of both features and examples growing rapidly in the fields like text mining and clickstream data analysis parallelization and the use of cluster architectures becomes important. We present a novel algorithm for fitting regularized generalized linear models in the distributed environment. The algorithm splits data between nodes by features, uses coordinate descent on each node and line search to merge results globally. Convergence proof is provided. A modifications of the algorithm addresses slow node problem. For an important particular case of logistic regression we empirically compare our program with several state-of-the art approaches that rely on different algorithmic and data spitting methods. Experiments demonstrate that our approach is scalable and superior when training on large and sparse datasets. version:1
arxiv-1610-09296 | Improving Sampling from Generative Autoencoders with Markov Chains | http://arxiv.org/abs/1610.09296 | id:1610.09296 author:Kai Arulkumaran, Antonia Creswell, Anil Anthony Bharath category:cs.LG cs.AI stat.ML  published:2016-10-28 summary:We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. Generative autoencoders are those which are trained to softly enforce a prior on the latent distribution learned by the inference model. However, the inference model may not always map inputs to latent samples that are consistent with the prior. We formulate a Markov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively encoding and decoding, which allows us to sample from the learned latent distribution. Using this, we can improve the quality of samples drawn from the model, especially when the learned distribution is far from the prior. Using MCMC sampling, we are able to reveal previously unseen differences between generative autoencoders trained either with or without a denoising criterion. version:2
arxiv-1611-02061 | Efficient Vision Data Processing on a Mobile Device for Indoor Localization | http://arxiv.org/abs/1611.02061 | id:1611.02061 author:Micha≈Ç Nowicki, Jan Wietrzykowski, Piotr Skrzypczy≈Ñski category:cs.RO cs.CV  published:2016-11-07 summary:The paper presents an approach to indoor personal localization using a sequence of captured images. The solution is tailored specifically for processing vision data on mobile devices with limited computing power. The presented FastABLE system is based on the ABLE-M place recognition algorithm, but introduces major modifications to the concept of image matching, in order to radically cut down the processing time, and to improve scalability. FastABLE is compared against the original OpenABLE and the popular OpenFABMAP place recognition systems. version:1
arxiv-1611-02053 | Reinforcement-based Simultaneous Algorithm and its Hyperparameters Selection | http://arxiv.org/abs/1611.02053 | id:1611.02053 author:Valeria Efimova, Andrey Filchenkov, Anatoly Shalyto category:cs.LG cs.AI stat.ML  published:2016-11-07 summary:Many algorithms for data analysis exist, especially for classification problems. To solve a data analysis problem, a proper algorithm should be chosen, and also its hyperparameters should be selected. In this paper, we present a new method for the simultaneous selection of an algorithm and its hyperparameters. In order to do so, we reduced this problem to the multi-armed bandit problem. We consider an algorithm as an arm and algorithm hyperparameters search during a fixed time as the corresponding arm play. We also suggest a problem-specific reward function. We performed the experiments on 10 real datasets and compare the suggested method with the existing one implemented in Auto-WEKA. The results show that our method is significantly better in most of the cases and never worse than the Auto-WEKA. version:1
arxiv-1611-02049 | Low-effort place recognition with WiFi fingerprints using deep learning | http://arxiv.org/abs/1611.02049 | id:1611.02049 author:Micha≈Ç Nowicki, Jan Wietrzykowski category:cs.RO cs.NE  published:2016-11-07 summary:Using WiFi signals for indoor localization is the main localization modality of the existing personal indoor localization systems operating on mobile devices. WiFi fingerprinting is also used for mobile robots, as WiFi signals are usually available indoors and can provide rough initial position estimate or can be used together with other positioning systems. Currently, the best solutions rely on filtering, manual data analysis, and time-consuming parameter tuning to achieve reliable and accurate localization. In this work, we propose to use deep neural networks to significantly lower the work-force burden of the localization system design, while still achieving satisfactory results. Assuming the state-of-the-art hierarchical approach, we employ the DNN system for building/floor classification. We show that stacked autoencoders allow to efficiently reduce the feature space in order to achieve robust and precise classification. The proposed architecture is verified on the publicly available UJIIndoorLoc dataset and the results are compared with other solutions. version:1
arxiv-1611-02047 | Reinforcement Learning Approach for Parallelization in Filters Aggregation Based Feature Selection Algorithms | http://arxiv.org/abs/1611.02047 | id:1611.02047 author:Ivan Smetannikov, Ilya Isaev, Andrey Filchenkov category:cs.LG cs.AI stat.ML  published:2016-11-07 summary:One of the classical problems in machine learning and data mining is feature selection. A feature selection algorithm is expected to be quick, and at the same time it should show high performance. MeLiF algorithm effectively solves this problem using ensembles of ranking filters. This article describes two different ways to improve MeLiF algorithm performance with parallelization. Experiments show that proposed schemes significantly improves algorithm performance and increase feature selection quality. version:1
arxiv-1611-02041 | Robust supervised learning under uncertainty in dataset shift | http://arxiv.org/abs/1611.02041 | id:1611.02041 author:Weihua Hu, Issei Sato, Masashi Sugiyama category:stat.ML cs.LG  published:2016-11-07 summary:When machine learning is deployed in the real world, its performance can be significantly undermined because test data may follow a different distribution from training data. To build a reliable machine learning system in such a scenario, we propose a supervised learning framework that is explicitly robust to the uncertainty of dataset shift. Our robust learning framework is flexible in modeling various dataset shift scenarios. It is also computationally efficient in that it acts as a robust wrapper around existing gradient-based supervised learning algorithms, while adding negligible computational overheads. We discuss practical considerations in robust supervised learning and show the effectiveness of our approach on both synthetic and benchmark datasets. version:1
arxiv-1611-02027 | :telephone::person::sailboat::whale::okhand:; or "Call me Ishmael" - How do you translate emoji? | http://arxiv.org/abs/1611.02027 | id:1611.02027 author:Will Radford, Andrew Chisholm, Ben Hachey, Bo Han category:cs.CL  published:2016-11-07 summary:We report on an exploratory analysis of Emoji Dick, a project that leverages crowdsourcing to translate Melville's Moby Dick into emoji. This distinctive use of emoji removes textual context, and leads to a varying translation quality. In this paper, we use statistical word alignment and part-of-speech tagging to explore how people use emoji. Despite these simple methods, we observed differences in token and part-of-speech distributions. Experiments also suggest that semantics are preserved in the translation, and repetition is more common in emoji. version:1
