arxiv-1306-1043 | Structural Intervention Distance (SID) for Evaluating Causal Graphs | http://arxiv.org/abs/1306.1043 | id:1306.1043 author:Jonas Peters, Peter BÃ¼hlmann category:stat.ML  published:2013-06-05 summary:Causal inference relies on the structure of a graph, often a directed acyclic graph (DAG). Different graphs may result in different causal inference statements and different intervention distributions. To quantify such differences, we propose a (pre-) distance between DAGs, the structural intervention distance (SID). The SID is based on a graphical criterion only and quantifies the closeness between two DAGs in terms of their corresponding causal inference statements. It is therefore well-suited for evaluating graphs that are used for computing interventions. Instead of DAGs it is also possible to compare CPDAGs, completed partially directed acyclic graphs that represent Markov equivalence classes. Since it differs significantly from the popular Structural Hamming Distance (SHD), the SID constitutes a valuable additional measure. We discuss properties of this distance and provide an efficient implementation with software code available on the first author's homepage (an R package is under construction). version:2
arxiv-1404-1831 | Improving Bilayer Product Quantization for Billion-Scale Approximate Nearest Neighbors in High Dimensions | http://arxiv.org/abs/1404.1831 | id:1404.1831 author:Artem Babenko, Victor Lempitsky category:cs.CV H.3.3  published:2014-04-07 summary:The top-performing systems for billion-scale high-dimensional approximate nearest neighbor (ANN) search are all based on two-layer architectures that include an indexing structure and a compressed datapoints layer. An indexing structure is crucial as it allows to avoid exhaustive search, while the lossy data compression is needed to fit the dataset into RAM. Several of the most successful systems use product quantization (PQ) for both the indexing and the dataset compression layers. These systems are however limited in the way they exploit the interaction of product quantization processes that happen at different stages of these systems. Here we introduce and evaluate two approximate nearest neighbor search systems that both exploit the synergy of product quantization processes in a more efficient way. The first system, called Fast Bilayer Product Quantization (FBPQ), speeds up the runtime of the baseline system (Multi-D-ADC) by several times, while achieving the same accuracy. The second system, Hierarchical Bilayer Product Quantization (HBPQ) provides a significantly better recall for the same runtime at a cost of small memory footprint increase. For the BIGANN dataset of billion SIFT descriptors, the 10% increase in Recall@1 and the 17% increase in Recall@10 is observed. version:1
arxiv-1312-7604 | Probabilistic Archetypal Analysis | http://arxiv.org/abs/1312.7604 | id:1312.7604 author:Sohan Seth, Manuel J. A. Eugster category:stat.ML  published:2013-12-29 summary:Archetypal analysis represents a set of observations as convex combinations of pure patterns, or archetypes. The original geometric formulation of finding archetypes by approximating the convex hull of the observations assumes them to be real valued. This, unfortunately, is not compatible with many practical situations. In this paper we revisit archetypal analysis from the basic principles, and propose a probabilistic framework that accommodates other observation types such as integers, binary, and probability vectors. We corroborate the proposed methodology with convincing real-world applications on finding archetypal winter tourists based on binary survey data, archetypal disaster-affected countries based on disaster count data, and document archetypes based on term-frequency data. We also present an appropriate visualization tool to summarize archetypal analysis solution better. version:2
arxiv-1404-0600 | MBIS: Multivariate Bayesian Image Segmentation Tool | http://arxiv.org/abs/1404.0600 | id:1404.0600 author:Oscar Esteban, Gert Wollny, Subrahmanyam Gorthi, Maria-J. Ledesma-Carbayo, Jean-Philippe Thiran, Andres Santos, Meritxell Bach-Cuadra category:cs.CV 62P10  62F15  published:2014-04-02 summary:We present MBIS (Multivariate Bayesian Image Segmentation tool), a clustering tool based on the mixture of multivariate normal distributions model. MBIS supports multi-channel bias field correction based on a B-spline model. A second methodological novelty is the inclusion of graph-cuts optimization for the stationary anisotropic hidden Markov random field model. Along with MBIS, we release an evaluation framework that contains three different experiments on multi-site data. We first validate the accuracy of segmentation and the estimated bias field for each channel. MBIS outperforms a widely used segmentation tool in a cross-comparison evaluation. The second experiment demonstrates the robustness of results on atlas-free segmentation of two image sets from scan-rescan protocols on 21 healthy subjects. Multivariate segmentation is more replicable than the monospectral counterpart on T1-weighted images. Finally, we provide a third experiment to illustrate how MBIS can be used in a large-scale study of tissue volume change with increasing age in 584 healthy subjects. This last result is meaningful as multivariate segmentation performs robustly without the need for prior knowledge version:2
arxiv-1310-0740 | Pseudo-Marginal Bayesian Inference for Gaussian Processes | http://arxiv.org/abs/1310.0740 | id:1310.0740 author:Maurizio Filippone, Mark Girolami category:stat.ML cs.LG stat.ME  published:2013-10-02 summary:The main challenges that arise when adopting Gaussian Process priors in probabilistic modeling are how to carry out exact Bayesian inference and how to account for uncertainty on model parameters when making model-based predictions on out-of-sample data. Using probit regression as an illustrative working example, this paper presents a general and effective methodology based on the pseudo-marginal approach to Markov chain Monte Carlo that efficiently addresses both of these issues. The results presented in this paper show improvements over existing sampling methods to simulate from the posterior distribution over the parameters defining the covariance function of the Gaussian Process prior. This is particularly important as it offers a powerful tool to carry out full Bayesian inference of Gaussian Process based hierarchic statistical models in general. The results also demonstrate that Monte Carlo based integration of all model parameters is actually feasible in this class of models providing a superior quantification of uncertainty in predictions. Extensive comparisons with respect to state-of-the-art probabilistic classifiers confirm this assertion. version:4
arxiv-1310-7529 | Successive Nonnegative Projection Algorithm for Robust Nonnegative Blind Source Separation | http://arxiv.org/abs/1310.7529 | id:1310.7529 author:Nicolas Gillis category:stat.ML cs.LG math.NA math.OC  published:2013-10-28 summary:In this paper, we propose a new fast and robust recursive algorithm for near-separable nonnegative matrix factorization, a particular nonnegative blind source separation problem. This algorithm, which we refer to as the successive nonnegative projection algorithm (SNPA), is closely related to the popular successive projection algorithm (SPA), but takes advantage of the nonnegativity constraint in the decomposition. We prove that SNPA is more robust than SPA and can be applied to a broader class of nonnegative matrices. This is illustrated on some synthetic data sets, and on a real-world hyperspectral image. version:3
arxiv-1404-1664 | Icon Based Information Retrieval and Disease Identification in Agriculture | http://arxiv.org/abs/1404.1664 | id:1404.1664 author:Namita Mittal, Basant Agarwal, Ajay Gupta, Hemant Madhur category:cs.HC cs.CV cs.CY cs.IR  published:2014-04-07 summary:Recent developments in the ICT industry in past few decades has enabled the quick and easy access to the information available on the internet. But, digital literacy is the pre-requisite for its use. The main purpose of this paper is to provide an interface for digitally illiterate users, especially farmers to efficiently and effectively retrieve information through Internet. In addition, to enable the farmers to identify the disease in their crop, its cause and symptoms using digital image processing and pattern recognition instantly without waiting for an expert to visit the farms and identify the disease. version:1
arxiv-1303-2270 | Penalty-regulated dynamics and robust learning procedures in games | http://arxiv.org/abs/1303.2270 | id:1303.2270 author:Pierre Coucheney, Bruno Gaujal, Panayotis Mertikopoulos category:math.OC cs.GT cs.LG  published:2013-03-09 summary:Starting from a heuristic learning scheme for N-person games, we derive a new class of continuous-time learning dynamics consisting of a replicator-like drift adjusted by a penalty term that renders the boundary of the game's strategy space repelling. These penalty-regulated dynamics are equivalent to players keeping an exponentially discounted aggregate of their on-going payoffs and then using a smooth best response to pick an action based on these performance scores. Owing to this inherent duality, the proposed dynamics satisfy a variant of the folk theorem of evolutionary game theory and they converge to (arbitrarily precise) approximations of Nash equilibria in potential games. Motivated by applications to traffic engineering, we exploit this duality further to design a discrete-time, payoff-based learning algorithm which retains these convergence properties and only requires players to observe their in-game payoffs: moreover, the algorithm remains robust in the presence of stochastic perturbations and observation errors, and it does not require any synchronization between players. version:2
arxiv-1404-1614 | A Denoising Autoencoder that Guides Stochastic Search | http://arxiv.org/abs/1404.1614 | id:1404.1614 author:Alexander W. Churchill, Siddharth Sigtia, Chrisantha Fernando category:cs.NE cs.LG  published:2014-04-06 summary:An algorithm is described that adaptively learns a non-linear mutation distribution. It works by training a denoising autoencoder (DA) online at each generation of a genetic algorithm to reconstruct a slowly decaying memory of the best genotypes so far. A compressed hidden layer forces the autoencoder to learn hidden features in the training set that can be used to accelerate search on novel problems with similar structure. Its output neurons define a probability distribution that we sample from to produce offspring solutions. The algorithm outperforms a canonical genetic algorithm on several combinatorial optimisation problems, e.g. multidimensional 0/1 knapsack problem, MAXSAT, HIFF, and on parameter optimisation problems, e.g. Rastrigin and Rosenbrock functions. version:1
arxiv-1401-3700 | Convex Relaxations of SE(2) and SE(3) for Visual Pose Estimation | http://arxiv.org/abs/1401.3700 | id:1401.3700 author:Matanya B. Horowitz, Nikolai Matni, Joel W. Burdick category:cs.CV  published:2014-01-15 summary:This paper proposes a new method for rigid body pose estimation based on spectrahedral representations of the tautological orbitopes of $SE(2)$ and $SE(3)$. The approach can use dense point cloud data from stereo vision or an RGB-D sensor (such as the Microsoft Kinect), as well as visual appearance data. The method is a convex relaxation of the classical pose estimation problem, and is based on explicit linear matrix inequality (LMI) representations for the convex hulls of $SE(2)$ and $SE(3)$. Given these representations, the relaxed pose estimation problem can be framed as a robust least squares problem with the optimization variable constrained to these convex sets. Although this formulation is a relaxation of the original problem, numerical experiments indicate that it is indeed exact - i.e. its solution is a member of $SE(2)$ or $SE(3)$ - in many interesting settings. We additionally show that this method is guaranteed to be exact for a large class of pose estimation problems. version:2
arxiv-1309-6779 | Causal Discovery with Continuous Additive Noise Models | http://arxiv.org/abs/1309.6779 | id:1309.6779 author:Jonas Peters, Joris Mooij, Dominik Janzing, Bernhard SchÃ¶lkopf category:stat.ML  published:2013-09-26 summary:We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (Regression with Subsequent Independence Test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation. version:4
arxiv-1404-1559 | Sparse Coding: A Deep Learning using Unlabeled Data for High - Level Representation | http://arxiv.org/abs/1404.1559 | id:1404.1559 author:R. Vidya, Dr. G. M. Nasira, R. P. Jaia Priyankka category:cs.LG cs.NE  published:2014-04-06 summary:Sparse coding algorithm is an learning algorithm mainly for unsupervised feature for finding succinct, a little above high - level Representation of inputs, and it has successfully given a way for Deep learning. Our objective is to use High - Level Representation data in form of unlabeled category to help unsupervised learning task. when compared with labeled data, unlabeled data is easier to acquire because, unlike labeled data it does not follow some particular class labels. This really makes the Deep learning wider and applicable to practical problems and learning. The main problem with sparse coding is it uses Quadratic loss function and Gaussian noise mode. So, its performs is very poor when binary or integer value or other Non- Gaussian type data is applied. Thus first we propose an algorithm for solving the L1 - regularized convex optimization algorithm for the problem to allow High - Level Representation of unlabeled data. Through this we derive a optimal solution for describing an approach to Deep learning algorithm by using sparse code. version:1
arxiv-1404-1514 | Text Based Approach For Indexing And Retrieval Of Image And Video: A Review | http://arxiv.org/abs/1404.1514 | id:1404.1514 author:Avinash N Bhute, B. B. Meshram category:cs.IR cs.CV cs.DL cs.MM  published:2014-04-05 summary:Text data present in multimedia contain useful information for automatic annotation, indexing. Extracted information used for recognition of the overlay or scene text from a given video or image. The Extracted text can be used for retrieving the videos and images. In this paper, firstly, we are discussed the different techniques for text extraction from images and videos. Secondly, we are reviewed the techniques for indexing and retrieval of image and videos by using extracted text. version:1
arxiv-1404-1504 | A Compression Technique for Analyzing Disagreement-Based Active Learning | http://arxiv.org/abs/1404.1504 | id:1404.1504 author:Yair Wiener, Steve Hanneke, Ran El-Yaniv category:cs.LG stat.ML  published:2014-04-05 summary:We introduce a new and improved characterization of the label complexity of disagreement-based active learning, in which the leading quantity is the version space compression set size. This quantity is defined as the size of the smallest subset of the training data that induces the same version space. We show various applications of the new characterization, including a tight analysis of CAL and refined label complexity bounds for linear separators under mixtures of Gaussians and axis-aligned rectangles under product densities. The version space compression set size, as well as the new characterization of the label complexity, can be naturally extended to agnostic learning problems, for which we show new speedup results for two well known active learning algorithms. version:1
arxiv-1404-1492 | Ensemble Committees for Stock Return Classification and Prediction | http://arxiv.org/abs/1404.1492 | id:1404.1492 author:James Brofos category:stat.ML cs.LG  published:2014-04-05 summary:This paper considers a portfolio trading strategy formulated by algorithms in the field of machine learning. The profitability of the strategy is measured by the algorithm's capability to consistently and accurately identify stock indices with positive or negative returns, and to generate a preferred portfolio allocation on the basis of a learned model. Stocks are characterized by time series data sets consisting of technical variables that reflect market conditions in a previous time interval, which are utilized produce binary classification decisions in subsequent intervals. The learned model is constructed as a committee of random forest classifiers, a non-linear support vector machine classifier, a relevance vector machine classifier, and a constituent ensemble of k-nearest neighbors classifiers. The Global Industry Classification Standard (GICS) is used to explore the ensemble model's efficacy within the context of various fields of investment including Energy, Materials, Financials, and Information Technology. Data from 2006 to 2012, inclusive, are considered, which are chosen for providing a range of market circumstances for evaluating the model. The model is observed to achieve an accuracy of approximately 70% when predicting stock price returns three months in advance. version:1
arxiv-1403-7737 | Sharpened Error Bounds for Random Sampling Based $\ell_2$ Regression | http://arxiv.org/abs/1403.7737 | id:1403.7737 author:Shusen Wang category:cs.LG cs.NA stat.ML  published:2014-03-30 summary:Given a data matrix $X \in R^{n\times d}$ and a response vector $y \in R^{n}$, suppose $n>d$, it costs $O(n d^2)$ time and $O(n d)$ space to solve the least squares regression (LSR) problem. When $n$ and $d$ are both large, exactly solving the LSR problem is very expensive. When $n \gg d$, one feasible approach to speeding up LSR is to randomly embed $y$ and all columns of $X$ into a smaller subspace $R^c$; the induced LSR problem has the same number of columns but much fewer number of rows, and it can be solved in $O(c d^2)$ time and $O(c d)$ space. We discuss in this paper two random sampling based methods for solving LSR more efficiently. Previous work showed that the leverage scores based sampling based LSR achieves $1+\epsilon$ accuracy when $c \geq O(d \epsilon^{-2} \log d)$. In this paper we sharpen this error bound, showing that $c = O(d \log d + d \epsilon^{-1})$ is enough for achieving $1+\epsilon$ accuracy. We also show that when $c \geq O(\mu d \epsilon^{-2} \log d)$, the uniform sampling based LSR attains a $2+\epsilon$ bound with positive probability. version:2
arxiv-1207-4684 | The Fast Cauchy Transform and Faster Robust Linear Regression | http://arxiv.org/abs/1207.4684 | id:1207.4684 author:Kenneth L. Clarkson, Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, Xiangrui Meng, David P. Woodruff category:cs.DS stat.ML  published:2012-07-19 summary:We provide fast algorithms for overconstrained $\ell_p$ regression and related problems: for an $n\times d$ input matrix $A$ and vector $b\in\mathbb{R}^n$, in $O(nd\log n)$ time we reduce the problem $\min_{x\in\mathbb{R}^d} \ Ax-b\ _p$ to the same problem with input matrix $\tilde A$ of dimension $s \times d$ and corresponding $\tilde b$ of dimension $s\times 1$. Here, $\tilde A$ and $\tilde b$ are a coreset for the problem, consisting of sampled and rescaled rows of $A$ and $b$; and $s$ is independent of $n$ and polynomial in $d$. Our results improve on the best previous algorithms when $n\gg d$, for all $p\in[1,\infty)$ except $p=2$. We also provide a suite of improved results for finding well-conditioned bases via ellipsoidal rounding, illustrating tradeoffs between running time and conditioning quality, including a one-pass conditioning algorithm for general $\ell_p$ problems. We also provide an empirical evaluation of implementations of our algorithms for $p=1$, comparing them with related algorithms. Our empirical results show that, in the asymptotic regime, the theory is a very good guide to the practical performance of these algorithms. Our algorithms use our faster constructions of well-conditioned bases for $\ell_p$ spaces and, for $p=1$, a fast subspace embedding of independent interest that we call the Fast Cauchy Transform: a distribution over matrices $\Pi:\mathbb{R}^n\mapsto \mathbb{R}^{O(d\log d)}$, found obliviously to $A$, that approximately preserves the $\ell_1$ norms: that is, with large probability, simultaneously for all $x$, $\ Ax\ _1 \approx \ \Pi Ax\ _1$, with distortion $O(d^{2+\eta})$, for an arbitrarily small constant $\eta>0$; and, moreover, $\Pi A$ can be computed in $O(nd\log d)$ time. The techniques underlying our Fast Cauchy Transform include fast Johnson-Lindenstrauss transforms, low-coherence matrices, and rescaling by Cauchy random variables. version:3
arxiv-1404-1371 | Multiple Testing for Neuroimaging via Hidden Markov Random Field | http://arxiv.org/abs/1404.1371 | id:1404.1371 author:Hai Shu, Bin Nan, Robert Koeppe, the Alzheimer's Dise category:stat.AP stat.ML  published:2014-04-04 summary:One of the important objectives that the Alzheimer's Disease Neuroimaging Initiative (ADNI) tries to achieve is to understand how the human brain changes over the course of disease progression. We consider voxel-level analysis for the 18F-Fluorodeoxyglucose positron emission tomography (FDG-PET) imaging study in ADNI for such a purpose. Traditional voxel-level multiple testing procedures in neuroimaging, which are mostly p-value based, often ignore the spatial correlations among neighboring voxels and thus suffer from substantial loss of power. We extend the local-significance-index based procedure, which aims to minimize the false nondiscovery rate subject to a constraint on the false discovery rate, to three-dimensional neuroimaging data using a hidden Markov random field model. A generalized expectation-maximization algorithm is proposed for estimating the model parameters. Extensive simulations show that the proposed approach is more powerful than conventional false discovery rate procedures. We apply the method to the comparison between mild cognitive impairment, a disease status with increased risk of developing Alzheimer's or another dementia, and normal controls in the ADNI's FDG-PET imaging study. version:1
arxiv-1211-6616 | TACT: A Transfer Actor-Critic Learning Framework for Energy Saving in Cellular Radio Access Networks | http://arxiv.org/abs/1211.6616 | id:1211.6616 author:Rongpeng Li, Zhifeng Zhao, Xianfu Chen, Jacques Palicot, Honggang Zhang category:cs.NI cs.AI cs.IT cs.LG math.IT  published:2012-11-28 summary:Recent works have validated the possibility of improving energy efficiency in radio access networks (RANs), achieved by dynamically turning on/off some base stations (BSs). In this paper, we extend the research over BS switching operations, which should match up with traffic load variations. Instead of depending on the dynamic traffic loads which are still quite challenging to precisely forecast, we firstly formulate the traffic variations as a Markov decision process. Afterwards, in order to foresightedly minimize the energy consumption of RANs, we design a reinforcement learning framework based BS switching operation scheme. Furthermore, to avoid the underlying curse of dimensionality in reinforcement learning, a transfer actor-critic algorithm (TACT), which utilizes the transferred learning expertise in historical periods or neighboring regions, is proposed and provably converges. In the end, we evaluate our proposed scheme by extensive simulations under various practical configurations and show that the proposed TACT algorithm contributes to a performance jumpstart and demonstrates the feasibility of significant energy efficiency improvement at the expense of tolerable delay performance. version:3
arxiv-1404-1144 | AIS-MACA- Z: MACA based Clonal Classifier for Splicing Site, Protein Coding and Promoter Region Identification in Eukaryotes | http://arxiv.org/abs/1404.1144 | id:1404.1144 author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG  published:2014-04-04 summary:Bioinformatics incorporates information regarding biological data storage, accessing mechanisms and presentation of characteristics within this data. Most of the problems in bioinformatics and be addressed efficiently by computer techniques. This paper aims at building a classifier based on Multiple Attractor Cellular Automata (MACA) which uses fuzzy logic with version Z to predict splicing site, protein coding and promoter region identification in eukaryotes. It is strengthened with an artificial immune system technique (AIS), Clonal algorithm for choosing rules of best fitness. The proposed classifier can handle DNA sequences of lengths 54,108,162,252,354. This classifier gives the exact boundaries of both protein and promoter regions with an average accuracy of 90.6%. This classifier can predict the splicing site with 97% accuracy. This classifier was tested with 1, 97,000 data components which were taken from Fickett & Toung , EPDnew, and other sequences from a renowned medical university. version:1
arxiv-1404-1116 | Resolving Multi-path Interference in Time-of-Flight Imaging via Modulation Frequency Diversity and Sparse Regularization | http://arxiv.org/abs/1404.1116 | id:1404.1116 author:Ayush Bhandari, Achuta Kadambi, Refael Whyte, Christopher Barsi, Micha Feigin, Adrian Dorrington, Ramesh Raskar category:cs.CV cs.IT math.IT physics.optics  published:2014-04-03 summary:Time-of-flight (ToF) cameras calculate depth maps by reconstructing phase shifts of amplitude-modulated signals. For broad illumination or transparent objects, reflections from multiple scene points can illuminate a given pixel, giving rise to an erroneous depth map. We report here a sparsity regularized solution that separates K-interfering components using multiple modulation frequency measurements. The method maps ToF imaging to the general framework of spectral estimation theory and has applications in improving depth profiles and exploiting multiple scattering. version:1
arxiv-1309-4132 | Attribute-Efficient Evolvability of Linear Functions | http://arxiv.org/abs/1309.4132 | id:1309.4132 author:Elaine Angelino, Varun Kanade category:cs.LG q-bio.PE  published:2013-09-16 summary:In a seminal paper, Valiant (2006) introduced a computational model for evolution to address the question of complexity that can arise through Darwinian mechanisms. Valiant views evolution as a restricted form of computational learning, where the goal is to evolve a hypothesis that is close to the ideal function. Feldman (2008) showed that (correlational) statistical query learning algorithms could be framed as evolutionary mechanisms in Valiant's model. P. Valiant (2012) considered evolvability of real-valued functions and also showed that weak-optimization algorithms that use weak-evaluation oracles could be converted to evolutionary mechanisms. In this work, we focus on the complexity of representations of evolutionary mechanisms. In general, the reductions of Feldman and P. Valiant may result in intermediate representations that are arbitrarily complex (polynomial-sized circuits). We argue that biological constraints often dictate that the representations have low complexity, such as constant depth and fan-in circuits. We give mechanisms for evolving sparse linear functions under a large class of smooth distributions. These evolutionary algorithms are attribute-efficient in the sense that the size of the representations and the number of generations required depend only on the sparsity of the target function and the accuracy parameter, but have no dependence on the total number of attributes. version:2
arxiv-1404-1100 | A Tutorial on Principal Component Analysis | http://arxiv.org/abs/1404.1100 | id:1404.1100 author:Jonathon Shlens category:cs.LG stat.ML  published:2014-04-03 summary:Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique. version:1
arxiv-1403-6499 | Optimal Schatten-q and Ky-Fan-k Norm Rate of Low Rank Matrix Estimation | http://arxiv.org/abs/1403.6499 | id:1403.6499 author:Dong Xia category:stat.ML  published:2014-03-25 summary:In this paper, we consider low rank matrix estimation using either matrix-version Dantzig Selector $\hat{A}_{\lambda}^d$ or matrix-version LASSO estimator $\hat{A}_{\lambda}^L$. We consider sub-Gaussian measurements, $i.e.$, the measurements $X_1,\ldots,X_n\in\mathbb{R}^{m\times m}$ have $i.i.d.$ sub-Gaussian entries. Suppose $\textrm{rank}(A_0)=r$. We proved that, when $n\geq Cm[r^2\vee r\log(m)\log(n)]$ for some $C>0$, both $\hat{A}_{\lambda}^d$ and $\hat{A}_{\lambda}^L$ can obtain optimal upper bounds(except some logarithmic terms) for estimation accuracy under spectral norm. By applying metric entropy of Grassmann manifolds, we construct (near) matching minimax lower bound for estimation accuracy under spectral norm. We also give upper bounds and matching minimax lower bound(except some logarithmic terms) for estimation accuracy under Schatten-q norm for every $1\leq q\leq\infty$. As a direct corollary, we show both upper bounds and minimax lower bounds of estimation accuracy under Ky-Fan-k norms for every $1\leq k\leq m$. version:2
arxiv-1404-1066 | Parallel Support Vector Machines in Practice | http://arxiv.org/abs/1404.1066 | id:1404.1066 author:Stephen Tyree, Jacob R. Gardner, Kilian Q. Weinberger, Kunal Agrawal, John Tran category:cs.LG  published:2014-04-03 summary:In this paper, we evaluate the performance of various parallel optimization methods for Kernel Support Vector Machines on multicore CPUs and GPUs. In particular, we provide the first comparison of algorithms with explicit and implicit parallelization. Most existing parallel implementations for multi-core or GPU architectures are based on explicit parallelization of Sequential Minimal Optimization (SMO)---the programmers identified parallelizable components and hand-parallelized them, specifically tuned for a particular architecture. We compare these approaches with each other and with implicitly parallelized algorithms---where the algorithm is expressed such that most of the work is done within few iterations with large dense linear algebra operations. These can be computed with highly-optimized libraries, that are carefully parallelized for a large variety of parallel platforms. We highlight the advantages and disadvantages of both approaches and compare them on various benchmark data sets. We find an approximate implicitly parallel algorithm which is surprisingly efficient, permits a much simpler implementation, and leads to unprecedented speedups in SVM training. version:1
arxiv-1404-0933 | Bayes and Naive Bayes Classifier | http://arxiv.org/abs/1404.0933 | id:1404.0933 author:Vikramkumar, Vijaykumar B, Trilochan category:cs.LG  published:2014-04-03 summary:The Bayesian Classification represents a supervised learning method as well as a statistical method for classification. Assumes an underlying probabilistic model and it allows us to capture uncertainty about the model in a principled way by determining probabilities of the outcomes. This Classification is named after Thomas Bayes (1702-1761), who proposed the Bayes Theorem. Bayesian classification provides practical learning algorithms and prior knowledge and observed data can be combined. Bayesian Classification provides a useful perspective for understanding and evaluating many learning algorithms. It calculates explicit probabilities for hypothesis and it is robust to noise in input data. In statistical classification the Bayes classifier minimises the probability of misclassification. That was a visual intuition for a simple case of the Bayes classifier, also called: 1)Idiot Bayes 2)Naive Bayes 3)Simple Bayes version:1
arxiv-1404-0868 | A Novel Genetic Algorithm using Helper Objectives for the 0-1 Knapsack Problem | http://arxiv.org/abs/1404.0868 | id:1404.0868 author:Jun He, Feidun He, Hongbin Dong category:cs.NE  published:2014-04-03 summary:The 0-1 knapsack problem is a well-known combinatorial optimisation problem. Approximation algorithms have been designed for solving it and they return provably good solutions within polynomial time. On the other hand, genetic algorithms are well suited for solving the knapsack problem and they find reasonably good solutions quickly. A naturally arising question is whether genetic algorithms are able to find solutions as good as approximation algorithms do. This paper presents a novel multi-objective optimisation genetic algorithm for solving the 0-1 knapsack problem. Experiment results show that the new algorithm outperforms its rivals, the greedy algorithm, mixed strategy genetic algorithm, and greedy algorithm + mixed strategy genetic algorithm. version:1
arxiv-1404-0850 | Application of Ontologies in Identifying Requirements Patterns in Use Cases | http://arxiv.org/abs/1404.0850 | id:1404.0850 author:Rui Couto, AntÃ³nio Nestor Ribeiro, JosÃ© Creissac Campos category:cs.SE cs.CL cs.IR  published:2014-04-03 summary:Use case specifications have successfully been used for requirements description. They allow joining, in the same modeling space, the expectations of the stakeholders as well as the needs of the software engineer and analyst involved in the process. While use cases are not meant to describe a system's implementation, by formalizing their description we are able to extract implementation relevant information from them. More specifically, we are interested in identifying requirements patterns (common requirements with typical implementation solutions) in support for a requirements based software development approach. In the paper we propose the transformation of Use Case descriptions expressed in a Controlled Natural Language into an ontology expressed in the Web Ontology Language (OWL). OWL's query engines can then be used to identify requirements patterns expressed as queries over the ontology. We describe a tool that we have developed to support the approach and provide an example of usage. version:1
arxiv-1303-5248 | Methods Of Measurement The Three-Dimensional Wind Waves Spectra, Based On The Processing Of Video Images Of The Sea Surface | http://arxiv.org/abs/1303.5248 | id:1303.5248 author:Boris M. Salin, Mikhail B. Salin category:physics.ao-ph cs.CV  published:2013-03-21 summary:Optical instruments for measuring surface-wave characteristics provide a better spatial and temporal resolution than other methods, but they face difficulties while converting the results of indirect measurements into absolute levels of the waves. We have solved this problem to some extent. In this paper, we propose an optical method for measuring the 3D power spectral density of the surface waves and spatio-temporal samples of the wave profiles. The method involves, first, synchronous recording of the brightness field over a patch of a rough surface and measurement of surface oscillations at one or more points and, second, filtering of the spatial image spectrum. Filter parameters are chosen to maximize the correlation of the surface oscillations recovered and measured at one or two points. In addition to the measurement procedure, the paper provides experimental results of measuring multidimensional spectra of roughness, which generally agree with theoretical expectations and the results of other authors. version:2
arxiv-1404-0774 | GPU Accelerated Fractal Image Compression for Medical Imaging in Parallel Computing Platform | http://arxiv.org/abs/1404.0774 | id:1404.0774 author:Md. Enamul Haque, Abdullah Al Kaisan, Mahmudur R Saniat, Aminur Rahman category:cs.DC cs.CV  published:2014-04-03 summary:In this paper, we implemented both sequential and parallel version of fractal image compression algorithms using CUDA (Compute Unified Device Architecture) programming model for parallelizing the program in Graphics Processing Unit for medical images, as they are highly similar within the image itself. There are several improvement in the implementation of the algorithm as well. Fractal image compression is based on the self similarity of an image, meaning an image having similarity in majority of the regions. We take this opportunity to implement the compression algorithm and monitor the effect of it using both parallel and sequential implementation. Fractal compression has the property of high compression rate and the dimensionless scheme. Compression scheme for fractal image is of two kind, one is encoding and another is decoding. Encoding is very much computational expensive. On the other hand decoding is less computational. The application of fractal compression to medical images would allow obtaining much higher compression ratios. While the fractal magnification an inseparable feature of the fractal compression would be very useful in presenting the reconstructed image in a highly readable form. However, like all irreversible methods, the fractal compression is connected with the problem of information loss, which is especially troublesome in the medical imaging. A very time consuming encoding pro- cess, which can last even several hours, is another bothersome drawback of the fractal compression. version:1
arxiv-1404-0752 | An Efficient Search Strategy for Aggregation and Discretization of Attributes of Bayesian Networks Using Minimum Description Length | http://arxiv.org/abs/1404.0752 | id:1404.0752 author:Jem Corcoran, Daniel Tran, Nicholas Levine category:stat.ML  published:2014-04-03 summary:Bayesian networks are convenient graphical expressions for high dimensional probability distributions representing complex relationships between a large number of random variables. They have been employed extensively in areas such as bioinformatics, artificial intelligence, diagnosis, and risk management. The recovery of the structure of a network from data is of prime importance for the purposes of modeling, analysis, and prediction. Most recovery algorithms in the literature assume either discrete of continuous but Gaussian data. For general continuous data, discretization is usually employed but often destroys the very structure one is out to recover. Friedman and Goldszmidt suggest an approach based on the minimum description length principle that chooses a discretization which preserves the information in the original data set, however it is one which is difficult, if not impossible, to implement for even moderately sized networks. In this paper we provide an extremely efficient search strategy which allows one to use the Friedman and Goldszmidt discretization in practice. version:1
arxiv-1404-0751 | Subspace Learning from Extremely Compressed Measurements | http://arxiv.org/abs/1404.0751 | id:1404.0751 author:Akshay Krishnamurthy, Martin Azizyan, Aarti Singh category:stat.ML cs.LG  published:2014-04-03 summary:We consider learning the principal subspace of a large set of vectors from an extremely small number of compressive measurements of each vector. Our theoretical results show that even a constant number of measurements per column suffices to approximate the principal subspace to arbitrary precision, provided that the number of vectors is large. This result is achieved by a simple algorithm that computes the eigenvectors of an estimate of the covariance matrix. The main insight is to exploit an averaging effect that arises from applying a different random projection to each vector. We provide a number of simulations confirming our theoretical results. version:1
arxiv-1404-0708 | Computational Optimization, Modelling and Simulation: Recent Trends and Challenges | http://arxiv.org/abs/1404.0708 | id:1404.0708 author:Xin-She Yang, Slawomir Koziel, Leifur Leifsson category:cs.NE math.OC 90C26  published:2014-04-02 summary:Modelling, simulation and optimization form an integrated part of modern design practice in engineering and industry. Tremendous progress has been observed for all three components over the last few decades. However, many challenging issues remain unresolved, and the current trends tend to use nature-inspired algorithms and surrogate-based techniques for modelling and optimization. This 4th workshop on Computational Optimization, Modelling and Simulation (COMS 2013) at ICCS 2013 will further summarize the latest developments of optimization and modelling and their applications in science, engineering and industry. In this review paper, we will analyse the recent trends in modelling and optimization, and their associated challenges. We will discuss important topics for further research, including parameter-tuning, large-scale problems, and the gaps between theory and applications. version:1
arxiv-1404-0695 | Multi-objective Flower Algorithm for Optimization | http://arxiv.org/abs/1404.0695 | id:1404.0695 author:Xin-She Yang, M. Karamanoglu, Xingshi He category:cs.NE math.OC 90C26  published:2014-04-02 summary:Flower pollination algorithm is a new nature-inspired algorithm, based on the characteristics of flowering plants. In this paper, we extend this flower algorithm to solve multi-objective optimization problems in engineering. By using the weighted sum method with random weights, we show that the proposed multi-objective flower algorithm can accurately find the Pareto fronts for a set of test functions. We then solve a bi-objective disc brake design problem, which indeed converges quickly. version:1
arxiv-1404-0334 | Active Deformable Part Models | http://arxiv.org/abs/1404.0334 | id:1404.0334 author:Menglong Zhu, Nikolay Atanasov, George J. Pappas, Kostas Daniilidis category:cs.CV cs.LG  published:2014-04-01 summary:This paper presents an active approach for part-based object detection, which optimizes the order of part filter evaluations and the time at which to stop and make a prediction. Statistics, describing the part responses, are learned from training data and are used to formalize the part scheduling problem as an offline optimization. Dynamic programming is applied to obtain a policy, which balances the number of part evaluations with the classification accuracy. During inference, the policy is used as a look-up table to choose the part order and the stopping time based on the observed filter responses. The method is faster than cascade detection with deformable part models (which does not optimize the part order) with negligible loss in accuracy when evaluated on the PASCAL VOC 2007 and 2010 datasets. version:2
arxiv-1404-0627 | Extraction of Projection Profile, Run-Histogram and Entropy Features Straight from Run-Length Compressed Text-Documents | http://arxiv.org/abs/1404.0627 | id:1404.0627 author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV  published:2014-04-02 summary:Document Image Analysis, like any Digital Image Analysis requires identification and extraction of proper features, which are generally extracted from uncompressed images, though in reality images are made available in compressed form for the reasons such as transmission and storage efficiency. However, this implies that the compressed image should be decompressed, which indents additional computing resources. This limitation induces the motivation to research in extracting features directly from the compressed image. In this research, we propose to extract essential features such as projection profile, run-histogram and entropy for text document analysis directly from run-length compressed text-documents. The experimentation illustrates that features are extracted directly from the compressed image without going through the stage of decompression, because of which the computing time is reduced. The feature values so extracted are exactly identical to those extracted from uncompressed images. version:1
arxiv-1404-0554 | From ADP to the Brain: Foundations, Roadmap, Challenges and Research Priorities | http://arxiv.org/abs/1404.0554 | id:1404.0554 author:Paul J Werbos category:cs.NE  published:2014-04-02 summary:This paper defines and discusses Mouse Level Computational Intelligence (MLCI) as a grand challenge for the coming century. It provides a specific roadmap to reach that target, citing relevant work and review papers and discussing the relation to funding priorities in two NSF funding activities: the ongoing Energy, Power and Adaptive Systems program (EPAS) and the recent initiative in Cognitive Optimization and Prediction (COPN). It elaborates on the first step, vector intelligence, a challenge in the development of universal learning systems, which itself will require considerable new research to attain. This in turn is a crucial prerequisite to true functional understanding of how mammal brains achieve such general learning capabilities. version:1
arxiv-1404-0533 | A Comparative Study of Modern Inference Techniques for Structured Discrete Energy Minimization Problems | http://arxiv.org/abs/1404.0533 | id:1404.0533 author:JÃ¶rg H. Kappes, Bjoern Andres, Fred A. Hamprecht, Christoph SchnÃ¶rr, Sebastian Nowozin, Dhruv Batra, Sungwoong Kim, Bernhard X. Kausler, Thorben KrÃ¶ger, Jan Lellmann, Nikos Komodakis, Bogdan Savchynskyy, Carsten Rother category:cs.CV  published:2014-04-02 summary:Szeliski et al. published an influential study in 2006 on energy minimization methods for Markov Random Fields (MRF). This study provided valuable insights in choosing the best optimization technique for certain classes of problems. While these insights remain generally useful today, the phenomenal success of random field models means that the kinds of inference problems that have to be solved changed significantly. Specifically, the models today often include higher order interactions, flexible connectivity structures, large la\-bel-spaces of different cardinalities, or learned energy tables. To reflect these changes, we provide a modernized and enlarged study. We present an empirical comparison of 32 state-of-the-art optimization techniques on a corpus of 2,453 energy minimization instances from diverse applications in computer vision. To ensure reproducibility, we evaluate all methods in the OpenGM 2 framework and report extensive results regarding runtime and solution quality. Key insights from our study agree with the results of Szeliski et al. for the types of models they studied. However, on new and challenging types of models our findings disagree and suggest that polyhedral methods and integer programming solvers are competitive in terms of runtime and solution quality over a large range of model types. version:1
arxiv-1403-7084 | Constrained speaker linking | http://arxiv.org/abs/1403.7084 | id:1403.7084 author:David A. van Leeuwen, Niko BrÃ¼mmer category:stat.ML cs.SD  published:2014-03-26 summary:In this paper we study speaker linking (a.k.a.\ partitioning) given constraints of the distribution of speaker identities over speech recordings. Specifically, we show that the intractable partitioning problem becomes tractable when the constraints pre-partition the data in smaller cliques with non-overlapping speakers. The surprisingly common case where speakers in telephone conversations are known, but the assignment of channels to identities is unspecified, is treated in a Bayesian way. We show that for the Dutch CGN database, where this channel assignment task is at hand, a lightweight speaker recognition system can quite effectively solve the channel assignment problem, with 93% of the cliques solved. We further show that the posterior distribution over channel assignment configurations is well calibrated. version:2
arxiv-1404-2903 | Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition | http://arxiv.org/abs/1404.2903 | id:1404.2903 author:Marius Leordeanu, Rahul Sukthankar category:cs.CV cs.LG cs.NE  published:2014-04-02 summary:We propose a general multi-class visual recognition model, termed the Classifier Graph, which aims to generalize and integrate ideas from many of today's successful hierarchical recognition approaches. Our graph-based model has the advantage of enabling rich interactions between classes from different levels of interpretation and abstraction. The proposed multi-class system is efficiently learned using step by step updates. The structure consists of simple logistic linear layers with inputs from features that are automatically selected from a large pool. Each newly learned classifier becomes a potential new feature. Thus, our feature pool can consist both of initial manually designed features as well as learned classifiers from previous steps (graph nodes), each copied many times at different scales and locations. In this manner we can learn and grow both a deep, complex graph of classifiers and a rich pool of features at different levels of abstraction and interpretation. Our proposed graph of classifiers becomes a multi-class system with a recursive structure, suitable for deep detection and recognition of several classes simultaneously. version:1
arxiv-1404-0453 | Cellular Automata and Its Applications in Bioinformatics: A Review | http://arxiv.org/abs/1404.0453 | id:1404.0453 author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG  published:2014-04-02 summary:This paper aims at providing a survey on the problems that can be easily addressed by cellular automata in bioinformatics. Some of the authors have proposed algorithms for addressing some problems in bioinformatics but the application of cellular automata in bioinformatics is a virgin field in research. None of the researchers has tried to relate the major problems in bioinformatics and find a common solution. Extensive literature surveys were conducted. We have considered some papers in various journals and conferences for conduct of our research. This paper provides intuition towards relating various problems in bioinformatics logically and tries to attain a common frame work for addressing the same. version:1
arxiv-1308-0227 | An Enhanced Features Extractor for a Portfolio of Constraint Solvers | http://arxiv.org/abs/1308.0227 | id:1308.0227 author:Roberto Amadini, Maurizio Gabbrielli, Jacopo Mauro category:cs.AI cs.LG  published:2013-08-01 summary:Recent research has shown that a single arbitrarily efficient solver can be significantly outperformed by a portfolio of possibly slower on-average solvers. The solver selection is usually done by means of (un)supervised learning techniques which exploit features extracted from the problem specification. In this paper we present an useful and flexible framework that is able to extract an extensive set of features from a Constraint (Satisfaction/Optimization) Problem defined in possibly different modeling languages: MiniZinc, FlatZinc or XCSP. We also report some empirical results showing that the performances that can be obtained using these features are effective and competitive with state of the art CSP portfolio techniques. version:7
arxiv-1404-0437 | Theory and Application of Shapelets to the Analysis of Surface Self-assembly Imaging | http://arxiv.org/abs/1404.0437 | id:1404.0437 author:Robert Suderman, Daniel Lizotte, Nasser Mohieddin Abukhdeir category:cs.CV physics.data-an  published:2014-04-02 summary:A method for quantitative analysis of local pattern strength and defects in surface self-assembly imaging is presented and applied to images of stripe and hexagonal ordered domains. The presented method uses "shapelet" functions which were originally developed for quantitative analysis of images of galaxies ($\propto 10^{20}\mathrm{m}$). In this work, they are used instead to quantify the presence of translational order in surface self-assembled films ($\propto 10^{-9}\mathrm{m}$) through reformulation into "steerable" filters. The resulting method is both computationally efficient (with respect to the number of filter evaluations), robust to variation in pattern feature shape, and, unlike previous approaches, is applicable to a wide variety of pattern types. An application of the method is presented which uses a nearest-neighbour analysis to distinguish between uniform (defect-free) and non-uniform (strained, defect-containing) regions within imaged self-assembled domains, both with striped and hexagonal patterns. version:1
arxiv-1306-3171 | Confidence Intervals and Hypothesis Testing for High-Dimensional Regression | http://arxiv.org/abs/1306.3171 | id:1306.3171 author:Adel Javanmard, Andrea Montanari category:stat.ME cs.IT cs.LG math.IT  published:2013-06-13 summary:Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the \emph{uncertainty} associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or $p$-values for these models. We consider here high-dimensional linear regression problem, and propose an efficient algorithm for constructing confidence intervals and $p$-values. The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a `de-biased' version of regularized M-estimators. The new construction improves over recent work in the field in that it does not assume a special structure on the design matrix. We test our method on synthetic data and a high-throughput genomic data set about riboflavin production rate. version:2
arxiv-1402-2679 | Equivalence of Kernel Machine Regression and Kernel Distance Covariance for Multidimensional Trait Association Studies | http://arxiv.org/abs/1402.2679 | id:1402.2679 author:Wen-Yu Hua, Debashis Ghosh category:stat.ML stat.ME  published:2014-02-11 summary:Associating genetic markers with a multidimensional phenotype is an important yet challenging problem. In this work, we establish the equivalence between two popular methods: kernel-machine regression (KMR), and kernel distance covariance (KDC). KMR is a semiparametric regression frameworks that models the covariate effects parametrically, while the genetic markers are considered non-parametrically. KDC represents a class of methods that includes distance covariance (DC) and Hilbert-Schmidt Independence Criterion (HSIC), which are nonparametric tests of independence. We show the equivalence between the score test of KMR and the KDC statistic under certain conditions. This result leads to a novel generalization of the KDC test that incorporates the covariates. Our contributions are three-fold: (1) establishing the equivalence between KMR and KDC; (2) showing that the principles of kernel machine regression can be applied to the interpretation of KDC; (3) the development of a broader class of KDC statistics, that the members are the quantities of different kernels. We demonstrate the proposals using simulation studies. Data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) is used to explore the association between the genetic variants on gene \emph{FLJ16124} and phenotypes represented in 3D structural brain MR images adjusting for age and gender. The results suggest that SNPs of \emph{FLJ16124} exhibit strong pairwise interaction effects that are correlated to the changes of brain region volumes. version:2
arxiv-1404-0400 | A Deep Representation for Invariance And Music Classification | http://arxiv.org/abs/1404.0400 | id:1404.0400 author:Chiyuan Zhang, Georgios Evangelopoulos, Stephen Voinea, Lorenzo Rosasco, Tomaso Poggio category:cs.SD cs.LG stat.ML  published:2014-04-01 summary:Representations in the auditory cortex might be based on mechanisms similar to the visual ventral stream; modules for building invariance to transformations and multiple layers for compositionality and selectivity. In this paper we propose the use of such computational modules for extracting invariant and discriminative audio representations. Building on a theory of invariance in hierarchical architectures, we propose a novel, mid-level representation for acoustical signals, using the empirical distributions of projections on a set of templates and their transformations. Under the assumption that, by construction, this dictionary of templates is composed from similar classes, and samples the orbit of variance-inducing signal transformations (such as shift and scale), the resulting signature is theoretically guaranteed to be unique, invariant to transformations and stable to deformations. Modules of projection and pooling can then constitute layers of deep networks, for learning composite representations. We present the main theoretical and computational aspects of a framework for unsupervised learning of invariant audio representations, empirically evaluated on music genre classification. version:1
arxiv-1404-0329 | Toward computational cumulative biology by combining models of biological datasets | http://arxiv.org/abs/1404.0329 | id:1404.0329 author:Ali Faisal, Jaakko Peltonen, Elisabeth Georgii, Johan Rung, Samuel Kaski category:q-bio.QM q-bio.GN stat.ML  published:2014-04-01 summary:A main challenge of data-driven sciences is how to make maximal use of the progressively expanding databases of experimental datasets in order to keep research cumulative. We introduce the idea of a modeling-based dataset retrieval engine designed for relating a researcher's experimental dataset to earlier work in the field. The search is (i) data-driven to enable new findings, going beyond the state of the art of keyword searches in annotations, (ii) modeling-driven, to both include biological knowledge and insights learned from data, and (iii) scalable, as it is accomplished without building one unified grand model of all data. Assuming each dataset has been modeled beforehand, by the researchers or by database managers, we apply a rapidly computable and optimizable combination model to decompose a new dataset into contributions from earlier relevant models. By using the data-driven decomposition we identify a network of interrelated datasets from a large annotated human gene expression atlas. While tissue type and disease were major driving forces for determining relevant datasets, the found relationships were richer and the model-based search was more accurate than keyword search; it moreover recovered biologically meaningful relationships that are not straightforwardly visible from annotations, for instance, between cells in different developmental stages such as thymocytes and T-cells. Data-driven links and citations matched to a large extent; the data-driven links even uncovered corrections to the publication data, as two of the most linked datasets were not highly cited and turned out to have wrong publication entries in the database. version:1
arxiv-1311-4468 | Stochastic processes and feedback-linearisation for online identification and Bayesian adaptive control of fully-actuated mechanical systems | http://arxiv.org/abs/1311.4468 | id:1311.4468 author:Jan-Peter Calliess, Antonis Papachristodoulou, Stephen J. Roberts category:cs.LG cs.SY physics.data-an stat.ML I.2.9; I.2.8; I.2.6  published:2013-11-18 summary:This work proposes a new method for simultaneous probabilistic identification and control of an observable, fully-actuated mechanical system. Identification is achieved by conditioning stochastic process priors on observations of configurations and noisy estimates of configuration derivatives. In contrast to previous work that has used stochastic processes for identification, we leverage the structural knowledge afforded by Lagrangian mechanics and learn the drift and control input matrix functions of the control-affine system separately. We utilise feedback-linearisation to reduce, in expectation, the uncertain nonlinear control problem to one that is easy to regulate in a desired manner. Thereby, our method combines the flexibility of nonparametric Bayesian learning with epistemological guarantees on the expected closed-loop trajectory. We illustrate our method in the context of torque-actuated pendula where the dynamics are learned with a combination of normal and log-normal processes. version:3
arxiv-1404-0200 | Household Electricity Demand Forecasting -- Benchmarking State-of-the-Art Methods | http://arxiv.org/abs/1404.0200 | id:1404.0200 author:Andreas Veit, Christoph Goebel, Rohit Tidke, Christoph Doblander, Hans-Arno Jacobsen category:cs.LG stat.AP I.2.6  published:2014-04-01 summary:The increasing use of renewable energy sources with variable output, such as solar photovoltaic and wind power generation, calls for Smart Grids that effectively manage flexible loads and energy storage. The ability to forecast consumption at different locations in distribution systems will be a key capability of Smart Grids. The goal of this paper is to benchmark state-of-the-art methods for forecasting electricity demand on the household level across different granularities and time scales in an explorative way, thereby revealing potential shortcomings and find promising directions for future research in this area. We apply a number of forecasting methods including ARIMA, neural networks, and exponential smoothening using several strategies for training data selection, in particular day type and sliding window based strategies. We consider forecasting horizons ranging between 15 minutes and 24 hours. Our evaluation is based on two data sets containing the power usage of individual appliances at second time granularity collected over the course of several months. The results indicate that forecasting accuracy varies significantly depending on the choice of forecasting methods/strategy and the parameter configuration. Measured by the Mean Absolute Percentage Error (MAPE), the considered state-of-the-art forecasting methods rarely beat corresponding persistence forecasts. Overall, we observed MAPEs in the range between 5 and >100%. The average MAPE for the first data set was ~30%, while it was ~85% for the other data set. These results show big room for improvement. Based on the identified trends and experiences from our experiments, we contribute a detailed discussion of promising future research. version:1
arxiv-1404-0138 | Efficient Algorithms and Error Analysis for the Modified Nystrom Method | http://arxiv.org/abs/1404.0138 | id:1404.0138 author:Shusen Wang, Zhihua Zhang category:cs.LG  published:2014-04-01 summary:Many kernel methods suffer from high time and space complexities and are thus prohibitive in big-data applications. To tackle the computational challenge, the Nystr\"om method has been extensively used to reduce time and space complexities by sacrificing some accuracy. The Nystr\"om method speedups computation by constructing an approximation of the kernel matrix using only a few columns of the matrix. Recently, a variant of the Nystr\"om method called the modified Nystr\"om method has demonstrated significant improvement over the standard Nystr\"om method in approximation accuracy, both theoretically and empirically. In this paper, we propose two algorithms that make the modified Nystr\"om method practical. First, we devise a simple column selection algorithm with a provable error bound. Our algorithm is more efficient and easier to implement than and nearly as accurate as the state-of-the-art algorithm. Second, with the selected columns at hand, we propose an algorithm that computes the approximation in lower time complexity than the approach in the previous work. Furthermore, we prove that the modified Nystr\"om method is exact under certain conditions, and we establish a lower error bound for the modified Nystr\"om method. version:1
arxiv-1404-0106 | Traffic Monitoring Using M2M Communication | http://arxiv.org/abs/1404.0106 | id:1404.0106 author:Shiu Kumar, Eun Sik Ham, Seong Ro Lee category:cs.CV  published:2014-04-01 summary:This paper presents an intelligent traffic monitoring system using wireless vision sensor network that captures and processes the real-time video image to obtain the traffic flow rate and vehicle speeds along different urban roadways. This system will display the traffic states on the front roadways that can guide the drivers to select the right way and avoid potential traffic congestions. On the other hand, it will also monitor the vehicle speeds and store the vehicle details, for those breaking the roadway speed limits, in its database. The real-time traffic data is processed by the Personal Computer (PC) at the sub roadway station and the traffic flow rate data is transmitted to the main roadway station Arduino 3G via email, where the data is extracted and traffic flow rate displayed. version:1
arxiv-1404-0099 | Venture: a higher-order probabilistic programming platform with programmable inference | http://arxiv.org/abs/1404.0099 | id:1404.0099 author:Vikash Mansinghka, Daniel Selsam, Yura Perov category:cs.AI cs.PL stat.CO stat.ML  published:2014-04-01 summary:We describe Venture, an interactive virtual machine for probabilistic programming that aims to be sufficiently expressive, extensible, and efficient for general-purpose use. Like Church, probabilistic models and inference problems in Venture are specified via a Turing-complete, higher-order probabilistic language descended from Lisp. Unlike Church, Venture also provides a compositional language for custom inference strategies built out of scalable exact and approximate techniques. We also describe four key aspects of Venture's implementation that build on ideas from probabilistic graphical models. First, we describe the stochastic procedure interface (SPI) that specifies and encapsulates primitive random variables. The SPI supports custom control flow, higher-order probabilistic procedures, partially exchangeable sequences and ``likelihood-free'' stochastic simulators. It also supports external models that do inference over latent variables hidden from Venture. Second, we describe probabilistic execution traces (PETs), which represent execution histories of Venture programs. PETs capture conditional dependencies, existential dependencies and exchangeable coupling. Third, we describe partitions of execution histories called scaffolds that factor global inference problems into coherent sub-problems. Finally, we describe a family of stochastic regeneration algorithms for efficiently modifying PET fragments contained within scaffolds. Stochastic regeneration linear runtime scaling in cases where many previous approaches scaled quadratically. We show how to use stochastic regeneration and the SPI to implement general-purpose inference strategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals based on particle Markov chain Monte Carlo and mean-field variational inference techniques. version:1
arxiv-1404-0086 | Using HMM in Strategic Games | http://arxiv.org/abs/1404.0086 | id:1404.0086 author:Mario Benevides, Isaque Lima, Rafael Nader, Pedro Rougemont category:cs.GT cs.IR cs.LG I.2.8  published:2014-04-01 summary:In this paper we describe an approach to resolve strategic games in which players can assume different types along the game. Our goal is to infer which type the opponent is adopting at each moment so that we can increase the player's odds. To achieve that we use Markov games combined with hidden Markov model. We discuss a hypothetical example of a tennis game whose solution can be applied to any game with similar characteristics. version:1
arxiv-1403-4514 | Simultaneous Perturbation Algorithms for Batch Off-Policy Search | http://arxiv.org/abs/1403.4514 | id:1403.4514 author:Raphael Fonteneau, L. A. Prashanth category:math.OC cs.LG  published:2014-03-18 summary:We propose novel policy search algorithms in the context of off-policy, batch mode reinforcement learning (RL) with continuous state and action spaces. Given a batch collection of trajectories, we perform off-line policy evaluation using an algorithm similar to that by [Fonteneau et al., 2010]. Using this Monte-Carlo like policy evaluator, we perform policy search in a class of parameterized policies. We propose both first order policy gradient and second order policy Newton algorithms. All our algorithms incorporate simultaneous perturbation estimates for the gradient as well as the Hessian of the cost-to-go vector, since the latter is unknown and only biased estimates are available. We demonstrate their practicality on a simple 1-dimensional continuous state space problem. version:2
arxiv-1403-8144 | Coding for Random Projections and Approximate Near Neighbor Search | http://arxiv.org/abs/1403.8144 | id:1403.8144 author:Ping Li, Michael Mitzenmacher, Anshumali Shrivastava category:cs.LG cs.DB cs.DS stat.CO  published:2014-03-31 summary:This technical note compares two coding (quantization) schemes for random projections in the context of sub-linear time approximate near neighbor search. The first scheme is based on uniform quantization while the second scheme utilizes a uniform quantization plus a uniformly random offset (which has been popular in practice). The prior work compared the two schemes in the context of similarity estimation and training linear classifiers, with the conclusion that the step of random offset is not necessary and may hurt the performance (depending on the similarity level). The task of near neighbor search is related to similarity estimation with importance distinctions and requires own study. In this paper, we demonstrate that in the context of near neighbor search, the step of random offset is not needed either and may hurt the performance (sometimes significantly so, depending on the similarity and other parameters). version:1
arxiv-1403-8084 | Privacy Tradeoffs in Predictive Analytics | http://arxiv.org/abs/1403.8084 | id:1403.8084 author:Stratis Ioannidis, Andrea Montanari, Udi Weinsberg, Smriti Bhagat, Nadia Fawaz, Nina Taft category:cs.CR cs.LG  published:2014-03-31 summary:Online services routinely mine user data to predict user preferences, make recommendations, and place targeted ads. Recent research has demonstrated that several private user attributes (such as political affiliation, sexual orientation, and gender) can be inferred from such data. Can a privacy-conscious user benefit from personalization while simultaneously protecting her private attributes? We study this question in the context of a rating prediction service based on matrix factorization. We construct a protocol of interactions between the service and users that has remarkable optimality properties: it is privacy-preserving, in that no inference algorithm can succeed in inferring a user's private attribute with a probability better than random guessing; it has maximal accuracy, in that no other privacy-preserving protocol improves rating prediction; and, finally, it involves a minimal disclosure, as the prediction accuracy strictly decreases when the service reveals less information. We extensively evaluate our protocol using several rating datasets, demonstrating that it successfully blocks the inference of gender, age and political affiliation, while incurring less than 5% decrease in the accuracy of rating prediction. version:1
arxiv-1403-8003 | Probabilistic Intra-Retinal Layer Segmentation in 3-D OCT Images Using Global Shape Regularization | http://arxiv.org/abs/1403.8003 | id:1403.8003 author:Fabian Rathke, Stefan Schmidt, Christoph SchnÃ¶rr category:cs.CV  published:2014-03-31 summary:With the introduction of spectral-domain optical coherence tomography (OCT), resulting in a significant increase in acquisition speed, the fast and accurate segmentation of 3-D OCT scans has become evermore important. This paper presents a novel probabilistic approach, that models the appearance of retinal layers as well as the global shape variations of layer boundaries. Given an OCT scan, the full posterior distribution over segmentations is approximately inferred using a variational method enabling efficient probabilistic inference in terms of computationally tractable model components: Segmenting a full 3-D volume takes around a minute. Accurate segmentations demonstrate the benefit of using global shape regularization: We segmented 35 fovea-centered 3-D volumes with an average unsigned error of 2.46 $\pm$ 0.22 {\mu}m as well as 80 normal and 66 glaucomatous 2-D circular scans with errors of 2.92 $\pm$ 0.53 {\mu}m and 4.09 $\pm$ 0.98 {\mu}m respectively. Furthermore, we utilized the inferred posterior distribution to rate the quality of the segmentation, point out potentially erroneous regions and discriminate normal from pathological scans. No pre- or postprocessing was required and we used the same set of parameters for all data sets, underlining the robustness and out-of-the-box nature of our approach. version:1
arxiv-1403-6392 | Implementation of an Automatic Sign Language Lexical Annotation Framework based on Propositional Dynamic Logic | http://arxiv.org/abs/1403.6392 | id:1403.6392 author:Arturo Curiel, Christophe Collet category:cs.CL I.2.7  published:2014-03-25 summary:In this paper, we present the implementation of an automatic Sign Language (SL) sign annotation framework based on a formal logic, the Propositional Dynamic Logic (PDL). Our system relies heavily on the use of a specific variant of PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which lets us describe SL signs as formulae and corpora videos as labeled transition systems (LTSs). Here, we intend to show how a generic annotation system can be constructed upon these underlying theoretical principles, regardless of the tracking technologies available or the input format of corpora. With this in mind, we generated a development framework that adapts the system to specific use cases. Furthermore, we present some results obtained by our application when adapted to one distinct case, 2D corpora analysis with pre-processed tracking information. We also present some insights on how such a technology can be used to analyze 3D real-time data, captured with a depth device. version:2
arxiv-1307-3040 | Between Sense and Sensibility: Declarative narrativisation of mental models as a basis and benchmark for visuo-spatial cognition and computation focussed collaborative cognitive systems | http://arxiv.org/abs/1307.3040 | id:1307.3040 author:Mehul Bhatt category:cs.AI cs.CL cs.CV cs.HC cs.RO  published:2013-07-11 summary:What lies between `\emph{sensing}' and `\emph{sensibility}'? In other words, what kind of cognitive processes mediate sensing capability, and the formation of sensible impressions ---e.g., abstractions, analogies, hypotheses and theory formation, beliefs and their revision, argument formation--- in domain-specific problem solving, or in regular activities of everyday living, working and simply going around in the environment? How can knowledge and reasoning about such capabilities, as exhibited by humans in particular problem contexts, be used as a model and benchmark for the development of collaborative cognitive (interaction) systems concerned with human assistance, assurance, and empowerment? We pose these questions in the context of a range of assistive technologies concerned with \emph{visuo-spatial perception and cognition} tasks encompassing aspects such as commonsense, creativity, and the application of specialist domain knowledge and problem-solving thought processes. Assistive technologies being considered include: (a) human activity interpretation; (b) high-level cognitive rovotics; (c) people-centred creative design in domains such as architecture & digital media creation, and (d) qualitative analyses geographic information systems. Computational narratives not only provide a rich cognitive basis, but they also serve as a benchmark of functional performance in our development of computational cognitive assistance systems. We posit that computational narrativisation pertaining to space, actions, and change provides a useful model of \emph{visual} and \emph{spatio-temporal thinking} within a wide-range of problem-solving tasks and application areas where collaborative cognitive systems could serve an assistive and empowering function. version:2
arxiv-1403-7890 | Sparse K-Means with $\ell_{\infty}/\ell_0$ Penalty for High-Dimensional Data Clustering | http://arxiv.org/abs/1403.7890 | id:1403.7890 author:Xiangyu Chang, Yu Wang, Rongjian Li, Zongben Xu category:stat.ML cs.LG stat.ME  published:2014-03-31 summary:Sparse clustering, which aims to find a proper partition of an extremely high-dimensional data set with redundant noise features, has been attracted more and more interests in recent years. The existing studies commonly solve the problem in a framework of maximizing the weighted feature contributions subject to a $\ell_2/\ell_1$ penalty. Nevertheless, this framework has two serious drawbacks: One is that the solution of the framework unavoidably involves a considerable portion of redundant noise features in many situations, and the other is that the framework neither offers intuitive explanations on why this framework can select relevant features nor leads to any theoretical guarantee for feature selection consistency. In this article, we attempt to overcome those drawbacks through developing a new sparse clustering framework which uses a $\ell_{\infty}/\ell_0$ penalty. First, we introduce new concepts on optimal partitions and noise features for the high-dimensional data clustering problems, based on which the previously known framework can be intuitively explained in principle. Then, we apply the suggested $\ell_{\infty}/\ell_0$ framework to formulate a new sparse k-means model with the $\ell_{\infty}/\ell_0$ penalty ($\ell_0$-k-means for short). We propose an efficient iterative algorithm for solving the $\ell_0$-k-means. To deeply understand the behavior of $\ell_0$-k-means, we prove that the solution yielded by the $\ell_0$-k-means algorithm has feature selection consistency whenever the data matrix is generated from a high-dimensional Gaussian mixture model. Finally, we provide experiments with both synthetic data and the Allen Developing Mouse Brain Atlas data to support that the proposed $\ell_0$-k-means exhibits better noise feature detection capacity over the previously known sparse k-means with the $\ell_2/\ell_1$ penalty ($\ell_1$-k-means for short). version:1
arxiv-1311-0689 | Particle filter-based Gaussian process optimisation for parameter inference | http://arxiv.org/abs/1311.0689 | id:1311.0689 author:Johan Dahlin, Fredrik Lindsten category:stat.CO stat.ML  published:2013-11-04 summary:We propose a novel method for maximum likelihood-based parameter inference in nonlinear and/or non-Gaussian state space models. The method is an iterative procedure with three steps. At each iteration a particle filter is used to estimate the value of the log-likelihood function at the current parameter iterate. Using these log-likelihood estimates, a surrogate objective function is created by utilizing a Gaussian process model. Finally, we use a heuristic procedure to obtain a revised parameter iterate, providing an automatic trade-off between exploration and exploitation of the surrogate model. The method is profiled on two state space models with good performance both considering accuracy and computational cost. version:2
arxiv-1403-7876 | Correlation Filters with Limited Boundaries | http://arxiv.org/abs/1403.7876 | id:1403.7876 author:Hamed Kiani Galoogahi, Terence Sim, Simon Lucey category:cs.CV  published:2014-03-31 summary:Correlation filters take advantage of specific properties in the Fourier domain allowing them to be estimated efficiently: O(NDlogD) in the frequency domain, versus O(D^3 + ND^2) spatially where D is signal length, and N is the number of signals. Recent extensions to correlation filters, such as MOSSE, have reignited interest of their use in the vision community due to their robustness and attractive computational properties. In this paper we demonstrate, however, that this computational efficiency comes at a cost. Specifically, we demonstrate that only 1/D proportion of shifted examples are unaffected by boundary effects which has a dramatic effect on detection/tracking performance. In this paper, we propose a novel approach to correlation filter estimation that: (i) takes advantage of inherent computational redundancies in the frequency domain, and (ii) dramatically reduces boundary effects. Impressive object tracking and detection results are presented in terms of both accuracy and computational efficiency. version:1
arxiv-1404-0649 | A probabilistic estimation and prediction technique for dynamic continuous social science models: The evolution of the attitude of the Basque Country population towards ETA as a case study | http://arxiv.org/abs/1404.0649 | id:1404.0649 author:Juan-Carlos CortÃ©s, Francisco-J. Santonja, Ana-C. Tarazona, Rafael-J. Villanueva, Javier Villanueva-Oller category:cs.LG  published:2014-03-30 summary:In this paper, we present a computational technique to deal with uncertainty in dynamic continuous models in Social Sciences. Considering data from surveys, the method consists of determining the probability distribution of the survey output and this allows to sample data and fit the model to the sampled data using a goodness-of-fit criterion based on the chi-square-test. Taking the fitted parameters non-rejected by the chi-square-test, substituting them into the model and computing their outputs, we build 95% confidence intervals in each time instant capturing uncertainty of the survey data (probabilistic estimation). Using the same set of obtained model parameters, we also provide a prediction over the next few years with 95% confidence intervals (probabilistic prediction). This technique is applied to a dynamic social model describing the evolution of the attitude of the Basque Country population towards the revolutionary organization ETA. version:1
arxiv-1109-1646 | Exact Subspace Segmentation and Outlier Detection by Low-Rank Representation | http://arxiv.org/abs/1109.1646 | id:1109.1646 author:Guangcan Liu, Huan Xu, Shuicheng Yan category:cs.IT cs.CV math.IT  published:2011-09-08 summary:In this work, we address the following matrix recovery problem: suppose we are given a set of data points containing two parts, one part consists of samples drawn from a union of multiple subspaces and the other part consists of outliers. We do not know which data points are outliers, or how many outliers there are. The rank and number of the subspaces are unknown either. Can we detect the outliers and segment the samples into their right subspaces, efficiently and exactly? We utilize a so-called {\em Low-Rank Representation} (LRR) method to solve this problem, and prove that under mild technical conditions, any solution to LRR exactly recovers the row space of the samples and detect the outliers as well. Since the subspace membership is provably determined by the row space, this further implies that LRR can perform exact subspace segmentation and outlier detection, in an efficient way. version:3
arxiv-1403-7795 | Bio-Inspired Computation: Success and Challenges of IJBIC | http://arxiv.org/abs/1403.7795 | id:1403.7795 author:Xin-She Yang, Zhihua Cui category:math.OC cs.NE 78M32  published:2014-03-30 summary:It is now five years since the launch of the International Journal of Bio-Inspired Computation (IJBIC). At the same time, significant new progress has been made in the area of bio-inspired computation. This review paper summarizes the success and achievements of IJBIC in the past five years, and also highlights the challenges and key issues for further research. version:1
arxiv-1403-7793 | True Global Optimality of the Pressure Vessel Design Problem: A Benchmark for Bio-Inspired Optimisation Algorithms | http://arxiv.org/abs/1403.7793 | id:1403.7793 author:Xin-She Yang, Christian Huyck, Mehmet Karamanoglu, Nawaz Khan category:math.OC cs.NE nlin.AO  published:2014-03-30 summary:The pressure vessel design problem is a well-known design benchmark for validating bio-inspired optimization algorithms. However, its global optimality is not clear and there has been no mathematical proof put forward. In this paper, a detailed mathematical analysis of this problem is provided that proves that 6059.714335048436 is the global minimum. The Lagrange multiplier method is also used as an alternative proof and this method is extended to find the global optimum of a cantilever beam design problem. version:1
arxiv-1403-7792 | Swarm Intelligence Based Algorithms: A Critical Analysis | http://arxiv.org/abs/1403.7792 | id:1403.7792 author:Xin-She Yang category:math.OC cs.NE nlin.AO 78M32  published:2014-03-30 summary:Many optimization algorithms have been developed by drawing inspiration from swarm intelligence (SI). These SI-based algorithms can have some advantages over traditional algorithms. In this paper, we carry out a critical analysis of these SI-based algorithms by analyzing their ways to mimic evolutionary operators. We also analyze the ways of achieving exploration and exploitation in algorithms by using mutation, crossover and selection. In addition, we also look at algorithms using dynamic systems, self-organization and Markov chain framework. Finally, we provide some discussions and topics for further research. version:1
arxiv-1403-7783 | Extraction of Line Word Character Segments Directly from Run Length Compressed Printed Text Documents | http://arxiv.org/abs/1403.7783 | id:1403.7783 author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV  published:2014-03-30 summary:Segmentation of a text-document into lines, words and characters, which is considered to be the crucial pre-processing stage in Optical Character Recognition (OCR) is traditionally carried out on uncompressed documents, although most of the documents in real life are available in compressed form, for the reasons such as transmission and storage efficiency. However, this implies that the compressed image should be decompressed, which indents additional computing resources. This limitation has motivated us to take up research in document image analysis using compressed documents. In this paper, we think in a new way to carry out segmentation at line, word and character level in run-length compressed printed-text-documents. We extract the horizontal projection profile curve from the compressed file and using the local minima points perform line segmentation. However, tracing vertical information which leads to tracking words-characters in a run-length compressed file is not very straight forward. Therefore, we propose a novel technique for carrying out simultaneous word and character segmentation by popping out column runs from each row in an intelligent sequence. The proposed algorithms have been validated with 1101 text-lines, 1409 words and 7582 characters from a data-set of 35 noise and skew free compressed documents of Bengali, Kannada and English Scripts. version:1
arxiv-1403-7746 | Multi-label Ferns for Efficient Recognition of Musical Instruments in Recordings | http://arxiv.org/abs/1403.7746 | id:1403.7746 author:Miron B. Kursa, Alicja A. Wieczorkowska category:cs.LG cs.SD  published:2014-03-30 summary:In this paper we introduce multi-label ferns, and apply this technique for automatic classification of musical instruments in audio recordings. We compare the performance of our proposed method to a set of binary random ferns, using jazz recordings as input data. Our main result is obtaining much faster classification and higher F-score. We also achieve substantial reduction of the model size. version:1
arxiv-1403-7726 | Relevant Feature Selection Model Using Data Mining for Intrusion Detection System | http://arxiv.org/abs/1403.7726 | id:1403.7726 author:Ayman I. Madbouly, Amr M. Gody, Tamer M. Barakat category:cs.CR cs.LG  published:2014-03-30 summary:Network intrusions have become a significant threat in recent years as a result of the increased demand of computer networks for critical systems. Intrusion detection system (IDS) has been widely deployed as a defense measure for computer networks. Features extracted from network traffic can be used as sign to detect anomalies. However with the huge amount of network traffic, collected data contains irrelevant and redundant features that affect the detection rate of the IDS, consumes high amount of system resources, and slowdown the training and testing process of the IDS. In this paper, a new feature selection model is proposed; this model can effectively select the most relevant features for intrusion detection. Our goal is to build a lightweight intrusion detection system by using a reduced features set. Deleting irrelevant and redundant features helps to build a faster training and testing process, to have less resource consumption as well as to maintain high detection rates. The effectiveness and the feasibility of our feature selection model were verified by several experiments on KDD intrusion detection dataset. The experimental results strongly showed that our model is not only able to yield high detection rates but also to speed up the detection process. version:1
arxiv-1303-2651 | Hybrid Q-Learning Applied to Ubiquitous recommender system | http://arxiv.org/abs/1303.2651 | id:1303.2651 author:Djallel Bouneffouf category:cs.LG cs.IR I.2  published:2013-03-10 summary:Ubiquitous information access becomes more and more important nowadays and research is aimed at making it adapted to users. Our work consists in applying machine learning techniques in order to bring a solution to some of the problems concerning the acceptance of the system by users. To achieve this, we propose a fundamental shift in terms of how we model the learning of recommender system: inspired by models of human reasoning developed in robotic, we combine reinforcement learning and case-base reasoning to define a recommendation process that uses these two approaches for generating recommendations on different context dimensions (social, temporal, geographic). We describe an implementation of the recommender system based on this framework. We also present preliminary results from experiments with the system and show how our approach increases the recommendation quality. version:2
arxiv-1403-7683 | Approximate Matrix Multiplication with Application to Linear Embeddings | http://arxiv.org/abs/1403.7683 | id:1403.7683 author:Anastasios Kyrillidis, Michail Vlachos, Anastasios Zouzias category:math.ST cs.IT math.IT stat.ML stat.TH  published:2014-03-30 summary:In this paper, we study the problem of approximately computing the product of two real matrices. In particular, we analyze a dimensionality-reduction-based approximation algorithm due to Sarlos [1], introducing the notion of nuclear rank as the ratio of the nuclear norm over the spectral norm. The presented bound has improved dependence with respect to the approximation error (as compared to previous approaches), whereas the subspace -- on which we project the input matrices -- has dimensions proportional to the maximum of their nuclear rank and it is independent of the input dimensions. In addition, we provide an application of this result to linear low-dimensional embeddings. Namely, we show that any Euclidean point-set with bounded nuclear rank is amenable to projection onto number of dimensions that is independent of the input dimensionality, while achieving additive error guarantees. version:1
arxiv-1403-7591 | Building A Large Concept Bank for Representing Events in Video | http://arxiv.org/abs/1403.7591 | id:1403.7591 author:Yin Cui, Dong Liu, Jiawei Chen, Shih-Fu Chang category:cs.MM cs.CV cs.IR H.3.1  published:2014-03-29 summary:Concept-based video representation has proven to be effective in complex event detection. However, existing methods either manually design concepts or directly adopt concept libraries not specifically designed for events. In this paper, we propose to build Concept Bank, the largest concept library consisting of 4,876 concepts specifically designed to cover 631 real-world events. To construct the Concept Bank, we first gather a comprehensive event collection from WikiHow, a collaborative writing project that aims to build the world's largest manual for any possible How-To event. For each event, we then search Flickr and discover relevant concepts from the tags of the returned images. We train a Multiple Kernel Linear SVM for each discovered concept as a concept detector in Concept Bank. We organize the concepts into a five-layer tree structure, in which the higher-level nodes correspond to the event categories while the leaf nodes are the event-specific concepts discovered for each event. Based on such tree ontology, we develop a semantic matching method to select relevant concepts for each textual event query, and then apply the corresponding concept detectors to generate concept-based video representations. We use TRECVID Multimedia Event Detection 2013 and Columbia Consumer Video open source event definitions and videos as our test sets and show very promising results on two video event detection tasks: event modeling over concept space and zero-shot event retrieval. To the best of our knowledge, this is the largest concept library covering the largest number of real-world events. version:1
arxiv-1403-7588 | Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods | http://arxiv.org/abs/1403.7588 | id:1403.7588 author:Cun Mu, Yuqian Zhang, John Wright, Donald Goldfarb category:math.OC cs.CV cs.NA stat.ML  published:2014-03-29 summary:Recovering matrices from compressive and grossly corrupted observations is a fundamental problem in robust statistics, with rich applications in computer vision and machine learning. In theory, under certain conditions, this problem can be solved in polynomial time via a natural convex relaxation, known as Compressive Principal Component Pursuit (CPCP). However, all existing provable algorithms for CPCP suffer from superlinear per-iteration cost, which severely limits their applicability to large scale problems. In this paper, we propose provable, scalable and efficient methods to solve CPCP with (essentially) linear per-iteration cost. Our method combines classical ideas from Frank-Wolfe and proximal methods. In each iteration, we mainly exploit Frank-Wolfe to update the low-rank component with rank-one SVD and exploit the proximal step for the sparse term. Convergence results and implementation details are also discussed. We demonstrate the scalability of the proposed approach with promising numerical experiments on visual data. version:1
arxiv-1311-4555 | Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife | http://arxiv.org/abs/1311.4555 | id:1311.4555 author:Stefan Wager, Trevor Hastie, Bradley Efron category:stat.ML stat.CO stat.ME  published:2013-11-18 summary:We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2012) that are based on the jackknife and the infinitesimal jackknife (IJ). In practice, bagged predictors are computed using a finite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B on the order of n^{1.5} bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B on the order of n replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our findings with multiple experiments and simulation studies. version:2
arxiv-1403-7543 | A sparse Kaczmarz solver and a linearized Bregman method for online compressed sensing | http://arxiv.org/abs/1403.7543 | id:1403.7543 author:Dirk A. Lorenz, Stephan Wenger, Frank SchÃ¶pfer, Marcus Magnor category:math.OC cs.CV cs.IT math.IT math.NA  published:2014-03-28 summary:An algorithmic framework to compute sparse or minimal-TV solutions of linear systems is proposed. The framework includes both the Kaczmarz method and the linearized Bregman method as special cases and also several new methods such as a sparse Kaczmarz solver. The algorithmic framework has a variety of applications and is especially useful for problems in which the linear measurements are slow and expensive to obtain. We present examples for online compressed sensing, TV tomographic reconstruction and radio interferometry. version:1
arxiv-1403-7455 | Hybrid Approach to English-Hindi Name Entity Transliteration | http://arxiv.org/abs/1403.7455 | id:1403.7455 author:Shruti Mathur, Varun Prakash Saxena category:cs.CL  published:2014-03-28 summary:Machine translation (MT) research in Indian languages is still in its infancy. Not much work has been done in proper transliteration of name entities in this domain. In this paper we address this issue. We have used English-Hindi language pair for our experiments and have used a hybrid approach. At first we have processed English words using a rule based approach which extracts individual phonemes from the words and then we have applied statistical approach which converts the English into its equivalent Hindi phoneme and in turn the corresponding Hindi word. Through this approach we have attained 83.40% accuracy. version:1
arxiv-1403-7429 | Distributed Reconstruction of Nonlinear Networks: An ADMM Approach | http://arxiv.org/abs/1403.7429 | id:1403.7429 author:Wei Pan, Aivar Sootla, Guy-Bart Stan category:math.OC cs.DC cs.LG cs.SY  published:2014-03-28 summary:In this paper, we present a distributed algorithm for the reconstruction of large-scale nonlinear networks. In particular, we focus on the identification from time-series data of the nonlinear functional forms and associated parameters of large-scale nonlinear networks. Recently, a nonlinear network reconstruction problem was formulated as a nonconvex optimisation problem based on the combination of a marginal likelihood maximisation procedure with sparsity inducing priors. Using a convex-concave procedure (CCCP), an iterative reweighted lasso algorithm was derived to solve the initial nonconvex optimisation problem. By exploiting the structure of the objective function of this reweighted lasso algorithm, a distributed algorithm can be designed. To this end, we apply the alternating direction method of multipliers (ADMM) to decompose the original problem into several subproblems. To illustrate the effectiveness of the proposed methods, we use our approach to identify a network of interconnected Kuramoto oscillators with different network sizes (500~100,000 nodes). version:1
arxiv-1212-1707 | Lossy Compression via Sparse Linear Regression: Computationally Efficient Encoding and Decoding | http://arxiv.org/abs/1212.1707 | id:1212.1707 author:Ramji Venkataramanan, Tuhin Sarkar, Sekhar Tatikonda category:cs.IT math.IT stat.ML  published:2012-12-07 summary:We propose computationally efficient encoders and decoders for lossy compression using a Sparse Regression Code. The codebook is defined by a design matrix and codewords are structured linear combinations of columns of this matrix. The proposed encoding algorithm sequentially chooses columns of the design matrix to successively approximate the source sequence. It is shown to achieve the optimal distortion-rate function for i.i.d Gaussian sources under the squared-error distortion criterion. For a given rate, the parameters of the design matrix can be varied to trade off distortion performance with encoding complexity. An example of such a trade-off as a function of the block length n is the following. With computational resource (space or time) per source sample of O((n/\log n)^2), for a fixed distortion-level above the Gaussian distortion-rate function, the probability of excess distortion decays exponentially in n. The Sparse Regression Code is robust in the following sense: for any ergodic source, the proposed encoder achieves the optimal distortion-rate function of an i.i.d Gaussian source with the same variance. Simulations show that the encoder has good empirical performance, especially at low and moderate rates. version:2
arxiv-1403-7365 | Expectation-Maximization Technique and Spatial-Adaptation Applied to Pel-Recursive Motion Estimation | http://arxiv.org/abs/1403.7365 | id:1403.7365 author:Vania Vieira Estrela, Marcos Henrique da Silva Bassani category:cs.CV  published:2014-03-28 summary:Pel-recursive motion estimation isa well-established approach. However, in the presence of noise, it becomes an ill-posed problem that requires regularization. In this paper, motion vectors are estimated in an iterative fashion by means of the Expectation-Maximization (EM) algorithm and a Gaussian data model. Our proposed algorithm also utilizes the local image properties of the scene to improve the motion vector estimates following a spatially adaptive approach. Numerical experiments are presented that demonstrate the merits of our method. version:1
arxiv-1403-7335 | Emotion Analysis Platform on Chinese Microblog | http://arxiv.org/abs/1403.7335 | id:1403.7335 author:Duyu Tang, Bing Qin, Ting Liu, Qiuhui Shi category:cs.CL cs.CY cs.IR  published:2014-03-28 summary:Weibo, as the largest social media service in China, has billions of messages generated every day. The huge number of messages contain rich sentimental information. In order to analyze the emotional changes in accordance with time and space, this paper presents an Emotion Analysis Platform (EAP), which explores the emotional distribution of each province, so that can monitor the global pulse of each province in China. The massive data of Weibo and the real-time requirements make the building of EAP challenging. In order to solve the above problems, emoticons, emotion lexicon and emotion-shifting rules are adopted in EAP to analyze the emotion of each tweet. In order to verify the effectiveness of the platform, case study on the Sichuan earthquake is done, and the analysis result of the platform accords with the fact. In order to analyze from quantity, we manually annotate a test set and conduct experiment on it. The experimental results show that the macro-Precision of EAP reaches 80% and the EAP works effectively. version:1
arxiv-1403-7321 | Learning detectors quickly using structured covariance matrices | http://arxiv.org/abs/1403.7321 | id:1403.7321 author:Jack Valmadre, Sridha Sridharan, Simon Lucey category:cs.CV  published:2014-03-28 summary:Computer vision is increasingly becoming interested in the rapid estimation of object detectors. Canonical hard negative mining strategies are slow as they require multiple passes of the large negative training set. Recent work has demonstrated that if the distribution of negative examples is assumed to be stationary, then Linear Discriminant Analysis (LDA) can learn comparable detectors without ever revisiting the negative set. Even with this insight, however, the time to learn a single object detector can still be on the order of tens of seconds on a modern desktop computer. This paper proposes to leverage the resulting structured covariance matrix to obtain detectors with identical performance in orders of magnitude less time and memory. We elucidate an important connection to the correlation filter literature, demonstrating that these can also be trained without ever revisiting the negative set. version:1
arxiv-1403-7311 | Performance Evaluation of Raster Based Shape Vectors in Object Recognition | http://arxiv.org/abs/1403.7311 | id:1403.7311 author:Akbar Khan, Pratap Reddy L category:cs.CV  published:2014-03-28 summary:Object recognition is still an impediment in the field of computer vision and multimedia retrieval.Defining an object model is a critical task. Shape information of an object play a critical role in the process of object recognition. Extraction of boundary information of an object from the multimedia data and classifying this information with associated objects is the primary step towards object recognition. Rasters play an important role while computing object boundary. The trade-off lies with the dimensionality of the object versus computational cost while achieving maximum efficiency. In this treatise an attempt is made to evaluate the performance of circular and spiral raster models in terms of average retrieval efficiency and computational cost. version:1
arxiv-1403-7308 | Data generator based on RBF network | http://arxiv.org/abs/1403.7308 | id:1403.7308 author:Marko Robnik-Å ikonja category:stat.ML cs.AI cs.LG  published:2014-03-28 summary:There are plenty of problems where the data available is scarce and expensive. We propose a generator of semi-artificial data with similar properties to the original data which enables development and testing of different data mining algorithms and optimization of their parameters. The generated data allow a large scale experimentation and simulations without danger of overfitting. The proposed generator is based on RBF networks which learn sets of Gaussian kernels. Learned Gaussian kernels can be used in a generative mode to generate the data from the same distributions. To asses quality of the generated data we developed several workflows and used them to evaluate the statistical properties of the generated data, structural similarity, and predictive similarity using supervised and unsupervised learning techniques. To determine usability of the proposed generator we conducted a large scale evaluation using 51 UCI data sets. The results show a considerable similarity between the original and generated data and indicate that the method can be useful in several development and simulation scenarios. version:1
arxiv-1307-5161 | Random Binary Mappings for Kernel Learning and Efficient SVM | http://arxiv.org/abs/1307.5161 | id:1307.5161 author:Gemma Roig, Xavier Boix, Luc Van Gool category:cs.CV cs.LG stat.ML  published:2013-07-19 summary:Support Vector Machines (SVMs) are powerful learners that have led to state-of-the-art results in various computer vision problems. SVMs suffer from various drawbacks in terms of selecting the right kernel, which depends on the image descriptors, as well as computational and memory efficiency. This paper introduces a novel kernel, which serves such issues well. The kernel is learned by exploiting a large amount of low-complex, randomized binary mappings of the input feature. This leads to an efficient SVM, while also alleviating the task of kernel selection. We demonstrate the capabilities of our kernel on 6 standard vision benchmarks, in which we combine several common image descriptors, namely histograms (Flowers17 and Daimler), attribute-like descriptors (UCI, OSR, and a-VOC08), and Sparse Quantization (ImageNet). Results show that our kernel learning adapts well to the different descriptors types, achieving the performance of the kernels specifically tuned for each image descriptor, and with similar evaluation cost as efficient SVM methods. version:2
arxiv-1307-3463 | Non-Elitist Genetic Algorithm as a Local Search Method | http://arxiv.org/abs/1307.3463 | id:1307.3463 author:Anton Eremeev category:cs.NE  published:2013-07-12 summary:Sufficient conditions are found under which the iterated non-elitist genetic algorithm with tournament selection first visits a local optimum in polynomially bounded time on average. It is shown that these conditions are satisfied on a class of problems with guaranteed local optima (GLO) if appropriate parameters of the algorithm are chosen. version:4
arxiv-1312-5785 | EXMOVES: Classifier-based Features for Scalable Action Recognition | http://arxiv.org/abs/1312.5785 | id:1312.5785 author:Du Tran, Lorenzo Torresani category:cs.CV  published:2013-12-20 summary:This paper introduces EXMOVES, learned exemplar-based features for efficient recognition of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input sequence. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the state-of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of different actions even in large video databases. We show the generality of our approach by building our mid-level descriptors from two different low-level feature representations. The accuracy and efficiency of the approach are demonstrated on several large-scale action recognition benchmarks. version:3
arxiv-1403-7267 | Systematic Ensemble Learning for Regression | http://arxiv.org/abs/1403.7267 | id:1403.7267 author:Roberto Aldave, Jean-Pierre Dussault category:stat.ML  published:2014-03-28 summary:The motivation of this work is to improve the performance of standard stacking approaches or ensembles, which are composed of simple, heterogeneous base models, through the integration of the generation and selection stages for regression problems. We propose two extensions to the standard stacking approach. In the first extension we combine a set of standard stacking approaches into an ensemble of ensembles using a two-step ensemble learning in the regression setting. The second extension consists of two parts. In the initial part a diversity mechanism is injected into the original training data set, systematically generating different training subsets or partitions, and corresponding ensembles of ensembles. In the final part after measuring the quality of the different partitions or ensembles, a max-min rule-based selection algorithm is used to select the most appropriate ensemble/partition on which to make the final prediction. We show, based on experiments over a broad range of data sets, that the second extension performs better than the best of the standard stacking approaches, and is as good as the oracle of databases, which has the best base model selected by cross-validation for each data set. In addition to that, the second extension performs better than two state-of-the-art ensemble methods for regression, and it is as good as a third state-of-the-art ensemble method. version:1
arxiv-1403-7265 | Accelerating MCMC via Parallel Predictive Prefetching | http://arxiv.org/abs/1403.7265 | id:1403.7265 author:Elaine Angelino, Eddie Kohler, Amos Waterland, Margo Seltzer, Ryan P. Adams category:stat.ML stat.CO  published:2014-03-28 summary:We present a general framework for accelerating a large class of widely used Markov chain Monte Carlo (MCMC) algorithms. Our approach exploits fast, iterative approximations to the target density to speculatively evaluate many potential future steps of the chain in parallel. The approach can accelerate computation of the target distribution of a Bayesian inference problem, without compromising exactness, by exploiting subsets of data. It takes advantage of whatever parallel resources are available, but produces results exactly equivalent to standard serial execution. In the initial burn-in phase of chain evaluation, it achieves speedup over serial evaluation that is close to linear in the number of available cores. version:1
arxiv-1305-1912 | Automated polyp detection in colon capsule endoscopy | http://arxiv.org/abs/1305.1912 | id:1305.1912 author:Alexander V. Mamonov, Isabel N. Figueiredo, Pedro N. Figueiredo, Yen-Hsi Richard Tsai category:cs.CV I.4.8  published:2013-05-08 summary:Colorectal polyps are important precursors to colon cancer, a major health problem. Colon capsule endoscopy (CCE) is a safe and minimally invasive examination procedure, in which the images of the intestine are obtained via digital cameras on board of a small capsule ingested by a patient. The video sequence is then analyzed for the presence of polyps. We propose an algorithm that relieves the labor of a human operator analyzing the frames in the video sequence. The algorithm acts as a binary classifier, which labels the frame as either containing polyps or not, based on the geometrical analysis and the texture content of the frame. The geometrical analysis is based on a segmentation of an image with the help of a mid-pass filter. The features extracted by the segmentation procedure are classified according to an assumption that the polyps are characterized as protrusions that are mostly round in shape. Thus, we use a best fit ball radius as a decision parameter of a binary classifier. We present a statistical study of the performance of our approach on a data set containing over 18,900 frames from the endoscopic video sequences of five adult patients. The algorithm demonstrates a solid performance, achieving 47% sensitivity per frame and over 81% sensitivity per polyp at a specificity level of 90%. On average, with a video sequence length of 3747 frames, only 367 false positive frames need to be inspected by a human operator. version:4
arxiv-1403-7178 | Offshore Wind Farm Layout Optimization Using Adapted Genetic Algorithm: A different perspective | http://arxiv.org/abs/1403.7178 | id:1403.7178 author:Feng Liu, Zhifang Wang category:cs.NE  published:2014-03-27 summary:In this paper we study the problem of optimal layout of an offshore wind farm to minimize the wake effect impacts. Considering the specific requirements of concerned offshore wind farm, we propose an adaptive genetic algorithm (AGA) which introduces location swaps to replace random crossovers in conventional GAs. That way the total number of turbines in the resulting layout will be effectively kept to the initially specified value. We experiment the proposed AGA method on three cases with free wind speed of 12 m/s, 20 m/s, and a typical offshore wind distribution setting respectively. Numerical results verify the effectiveness of our proposed algorithm which achieves a much faster convergence compared to conventional GA algorithms. version:1
arxiv-1403-7057 | Closed-Form Training of Conditional Random Fields for Large Scale Image Segmentation | http://arxiv.org/abs/1403.7057 | id:1403.7057 author:Alexander Kolesnikov, Matthieu Guillaumin, Vittorio Ferrari, Christoph H. Lampert category:cs.LG cs.CV  published:2014-03-27 summary:We present LS-CRF, a new method for very efficient large-scale training of Conditional Random Fields (CRFs). It is inspired by existing closed-form expressions for the maximum likelihood parameters of a generative graphical model with tree topology. LS-CRF training requires only solving a set of independent regression problems, for which closed-form expression as well as efficient iterative solvers are available. This makes it orders of magnitude faster than conventional maximum likelihood learning for CRFs that require repeated runs of probabilistic inference. At the same time, the models learned by our method still allow for joint inference at test time. We apply LS-CRF to the task of semantic image segmentation, showing that it is highly efficient, even for loopy models where probabilistic inference is problematic. It allows the training of image segmentation models from significantly larger training sets than had been used previously. We demonstrate this on two new datasets that form a second contribution of this paper. They consist of over 180,000 images with figure-ground segmentation annotations. Our large-scale experiments show that the possibilities of CRF-based image segmentation are far from exhausted, indicating, for example, that semi-supervised learning and the use of non-linear predictors are promising directions for achieving higher segmentation accuracy in the future. version:1
arxiv-1403-6958 | Compressive Pattern Matching on Multispectral Data | http://arxiv.org/abs/1403.6958 | id:1403.6958 author:S. Rousseau, D. Helbert, P. CarrÃ©, J. Blanc-Talon category:cs.CV  published:2014-03-27 summary:We introduce a new constrained minimization problem that performs template and pattern detection on a multispectral image in a compressive sensing context. We use an original minimization problem from Guo and Osher that uses $L_1$ minimization techniques to perform template detection in a multispectral image. We first adapt this minimization problem to work with compressive sensing data. Then we extend it to perform pattern detection using a formal transform called the spectralization along a pattern. That extension brings out the problem of measurement reconstruction. We introduce shifted measurements that allow us to reconstruct all the measurement with a small overhead and we give an optimality constraint for simple patterns. We present numerical results showing the performances of the original minimization problem and the compressed ones with different measurement rates and applied on remotely sensed data. version:1
arxiv-1403-6950 | Pyramidal Fisher Motion for Multiview Gait Recognition | http://arxiv.org/abs/1403.6950 | id:1403.6950 author:F. M. Castro, M. J. Marin-Jimenez, R. Medina-Carnicer category:cs.CV  published:2014-03-27 summary:The goal of this paper is to identify individuals by analyzing their gait. Instead of using binary silhouettes as input data (as done in many previous works) we propose and evaluate the use of motion descriptors based on densely sampled short-term trajectories. We take advantage of state-of-the-art people detectors to define custom spatial configurations of the descriptors around the target person. Thus, obtaining a pyramidal representation of the gait motion. The local motion features (described by the Divergence-Curl-Shear descriptor) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding. The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on the recent `AVA Multiview Gait' dataset. The results show that this new approach achieves promising results in the problem of gait recognition. version:1
arxiv-1402-0936 | An Optimization Method For Slice Interpolation Of Medical Images | http://arxiv.org/abs/1402.0936 | id:1402.0936 author:Ahmadreza Baghaie, Zeyun Yu category:cs.CV cs.CE  published:2014-02-05 summary:Slice interpolation is a fast growing field in medical image processing. Intensity-based interpolation and object-based interpolation are two major groups of methods in the literature. In this paper, we describe an object-oriented, optimization method based on a modified version of curvature-based image registration, in which a displacement field is computed for the missing slice between two known slices and used to interpolate the intensities of the missing slice. The proposed approach is evaluated quantitatively by using the Mean Squared Difference (MSD) as a metric. The produced results also show visual improvement in preserving sharp edges in images. version:2
arxiv-1403-6901 | Automatic Segmentation of Broadcast News Audio using Self Similarity Matrix | http://arxiv.org/abs/1403.6901 | id:1403.6901 author:Sapna Soni, Ahmed Imran, Sunil Kumar Kopparapu category:cs.SD cs.LG cs.MM  published:2014-03-27 summary:Generally audio news broadcast on radio is com- posed of music, commercials, news from correspondents and recorded statements in addition to the actual news read by the newsreader. When news transcripts are available, automatic segmentation of audio news broadcast to time align the audio with the text transcription to build frugal speech corpora is essential. We address the problem of identifying segmentation in the audio news broadcast corresponding to the news read by the newsreader so that they can be mapped to the text transcripts. The existing techniques produce sub-optimal solutions when used to extract newsreader read segments. In this paper, we propose a new technique which is able to identify the acoustic change points reliably using an acoustic Self Similarity Matrix (SSM). We describe the two pass technique in detail and verify its performance on real audio news broadcast of All India Radio for different languages. version:1
arxiv-1403-6863 | Online Learning of k-CNF Boolean Functions | http://arxiv.org/abs/1403.6863 | id:1403.6863 author:Joel Veness, Marcus Hutter category:cs.LG  published:2014-03-26 summary:This paper revisits the problem of learning a k-CNF Boolean function from examples in the context of online learning under the logarithmic loss. In doing so, we give a Bayesian interpretation to one of Valiant's celebrated PAC learning algorithms, which we then build upon to derive two efficient, online, probabilistic, supervised learning algorithms for predicting the output of an unknown k-CNF Boolean function. We analyze the loss of our methods, and show that the cumulative log-loss can be upper bounded, ignoring logarithmic factors, by a polynomial function of the size of each example. version:1
arxiv-1403-6794 | KPCA Spatio-temporal trajectory point cloud classifier for recognizing human actions in a CBVR system | http://arxiv.org/abs/1403.6794 | id:1403.6794 author:IvÃ¡n GÃ³mez-Conde, David N. Olivieri category:cs.IR cs.CV  published:2014-03-26 summary:We describe a content based video retrieval (CBVR) software system for identifying specific locations of a human action within a full length film, and retrieving similar video shots from a query. For this, we introduce the concept of a trajectory point cloud for classifying unique actions, encoded in a spatio-temporal covariant eigenspace, where each point is characterized by its spatial location, local Frenet-Serret vector basis, time averaged curvature and torsion and the mean osculating hyperplane. Since each action can be distinguished by their unique trajectories within this space, the trajectory point cloud is used to define an adaptive distance metric for classifying queries against stored actions. Depending upon the distance to other trajectories, the distance metric uses either large scale structure of the trajectory point cloud, such as the mean distance between cloud centroids or the difference in hyperplane orientation, or small structure such as the time averaged curvature and torsion, to classify individual points in a fuzzy-KNN. Our system can function in real-time and has an accuracy greater than 93% for multiple action recognition within video repositories. We demonstrate the use of our CBVR system in two situations: by locating specific frame positions of trained actions in two full featured films, and video shot retrieval from a database with a web search application. version:1
arxiv-1403-6774 | Optimized imaging using non-rigid registration | http://arxiv.org/abs/1403.6774 | id:1403.6774 author:Benjamin Berkels, Peter Binev, Douglas A. Blom, Wolfgang Dahmen, Robert C. Sharpley, Thomas Vogt category:cs.CV  published:2014-03-26 summary:The extraordinary improvements of modern imaging devices offer access to data with unprecedented information content. However, widely used image processing methodologies fall far short of exploiting the full breadth of information offered by numerous types of scanning probe, optical, and electron microscopies. In many applications, it is necessary to keep measurement intensities below a desired threshold. We propose a methodology for extracting an increased level of information by processing a series of data sets suffering, in particular, from high degree of spatial uncertainty caused by complex multiscale motion during the acquisition process. An important role is played by a nonrigid pixel-wise registration method that can cope with low signal-to-noise ratios. This is accompanied by formulating objective quality measures which replace human intervention and visual inspection in the processing chain. Scanning transmission electron microscopy of siliceous zeolite material exhibits the above-mentioned obstructions and therefore serves as orientation and a test of our procedures. version:1
arxiv-1403-6822 | Comparison of Multi-agent and Single-agent Inverse Learning on a Simulated Soccer Example | http://arxiv.org/abs/1403.6822 | id:1403.6822 author:Xiaomin Lin, Peter A. Beling, Randy Cogill category:cs.LG cs.GT  published:2014-03-26 summary:We compare the performance of Inverse Reinforcement Learning (IRL) with the relative new model of Multi-agent Inverse Reinforcement Learning (MIRL). Before comparing the methods, we extend a published Bayesian IRL approach that is only applicable to the case where the reward is only state dependent to a general one capable of tackling the case where the reward depends on both state and action. Comparison between IRL and MIRL is made in the context of an abstract soccer game, using both a game model in which the reward depends only on state and one in which it depends on both state and action. Results suggest that the IRL approach performs much worse than the MIRL approach. We speculate that the underperformance of IRL is because it fails to capture equilibrium information in the manner possible in MIRL. version:1
arxiv-1403-6706 | Beyond L2-Loss Functions for Learning Sparse Models | http://arxiv.org/abs/1403.6706 | id:1403.6706 author:Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J. Thiagarajan category:stat.ML cs.CV cs.LG math.OC I.2.6; G.1.6  published:2014-03-26 summary:Incorporating sparsity priors in learning tasks can give rise to simple, and interpretable models for complex high dimensional data. Sparse models have found widespread use in structure discovery, recovering data from corruptions, and a variety of large scale unsupervised and supervised learning problems. Assuming the availability of sufficient data, these methods infer dictionaries for sparse representations by optimizing for high-fidelity reconstruction. In most scenarios, the reconstruction quality is measured using the squared Euclidean distance, and efficient algorithms have been developed for both batch and online learning cases. However, new application domains motivate looking beyond conventional loss functions. For example, robust loss functions such as $\ell_1$ and Huber are useful in learning outlier-resilient models, and the quantile loss is beneficial in discovering structures that are the representative of a particular quantile. These new applications motivate our work in generalizing sparse learning to a broad class of convex loss functions. In particular, we consider the class of piecewise linear quadratic (PLQ) cost functions that includes Huber, as well as $\ell_1$, quantile, Vapnik, hinge loss, and smoothed variants of these penalties. We propose an algorithm to learn dictionaries and obtain sparse codes when the data reconstruction fidelity is measured using any smooth PLQ cost function. We provide convergence guarantees for the proposed algorithm, and demonstrate the convergence behavior using empirical experiments. Furthermore, we present three case studies that require the use of PLQ cost functions: (i) robust image modeling, (ii) tag refinement for image annotation and retrieval and (iii) computing empirical confidence limits for subspace clustering. version:1
arxiv-1403-6636 | Sign Language Lexical Recognition With Propositional Dynamic Logic | http://arxiv.org/abs/1403.6636 | id:1403.6636 author:Arturo Curiel, Christophe Collet category:cs.CL I.2.7  published:2014-03-26 summary:This paper explores the use of Propositional Dynamic Logic (PDL) as a suitable formal framework for describing Sign Language (SL), the language of deaf people, in the context of natural language processing. SLs are visual, complete, standalone languages which are just as expressive as oral languages. Signs in SL usually correspond to sequences of highly specific body postures interleaved with movements, which make reference to real world objects, characters or situations. Here we propose a formal representation of SL signs, that will help us with the analysis of automatically-collected hand tracking data from French Sign Language (FSL) video corpora. We further show how such a representation could help us with the design of computer aided SL verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages. version:1
arxiv-1403-6614 | QCMC: Quasi-conformal Parameterizations for Multiply-connected domains | http://arxiv.org/abs/1403.6614 | id:1403.6614 author:Kin Tat Ho, Lok Ming Lui category:cs.CG cs.CV math.DG  published:2014-03-26 summary:This paper presents a method to compute the {\it quasi-conformal parameterization} (QCMC) for a multiply-connected 2D domain or surface. QCMC computes a quasi-conformal map from a multiply-connected domain $S$ onto a punctured disk $D_S$ associated with a given Beltrami differential. The Beltrami differential, which measures the conformality distortion, is a complex-valued function $\mu:S\to\mathbb{C}$ with supremum norm strictly less than 1. Every Beltrami differential gives a conformal structure of $S$. Hence, the conformal module of $D_S$, which are the radii and centers of the inner circles, can be fully determined by $\mu$, up to a M\"obius transformation. In this paper, we propose an iterative algorithm to simultaneously search for the conformal module and the optimal quasi-conformal parameterization. The key idea is to minimize the Beltrami energy subject to the boundary constraints. The optimal solution is our desired quasi-conformal parameterization onto a punctured disk. The parameterization of the multiply-connected domain simplifies numerical computations and has important applications in various fields, such as in computer graphics and vision. Experiments have been carried out on synthetic data together with real multiply-connected Riemann surfaces. Results show that our proposed method can efficiently compute quasi-conformal parameterizations of multiply-connected domains and outperforms other state-of-the-art algorithms. Applications of the proposed parameterization technique have also been explored. version:1
arxiv-1311-4082 | Can a biologically-plausible hierarchy effectively replace face detection, alignment, and recognition pipelines? | http://arxiv.org/abs/1311.4082 | id:1311.4082 author:Qianli Liao, Joel Z Leibo, Youssef Mroueh, Tomaso Poggio category:cs.CV  published:2013-11-16 summary:The standard approach to unconstrained face recognition in natural photographs is via a detection, alignment, recognition pipeline. While that approach has achieved impressive results, there are several reasons to be dissatisfied with it, among them is its lack of biological plausibility. A recent theory of invariant recognition by feedforward hierarchical networks, like HMAX, other convolutional networks, or possibly the ventral stream, implies an alternative approach to unconstrained face recognition. This approach accomplishes detection and alignment implicitly by storing transformations of training images (called templates) rather than explicitly detecting and aligning faces at test time. Here we propose a particular locality-sensitive hashing based voting scheme which we call "consensus of collisions" and show that it can be used to approximate the full 3-layer hierarchy implied by the theory. The resulting end-to-end system for unconstrained face recognition operates on photographs of faces taken under natural conditions, e.g., Labeled Faces in the Wild (LFW), without aligning or cropping them, as is normally done. It achieves a drastic improvement in the state of the art on this end-to-end task, reaching the same level of performance as the best systems operating on aligned, closely cropped images (no outside training data). It also performs well on two newer datasets, similar to LFW, but more difficult: LFW-jittered (new here) and SUFR-W. version:3
arxiv-1403-7100 | A study on cost behaviors of binary classification measures in class-imbalanced problems | http://arxiv.org/abs/1403.7100 | id:1403.7100 author:Bao-Gang Hu, Wei-Ming Dong category:cs.LG  published:2014-03-26 summary:This work investigates into cost behaviors of binary classification measures in a background of class-imbalanced problems. Twelve performance measures are studied, such as F measure, G-means in terms of accuracy rates, and of recall and precision, balance error rate (BER), Matthews correlation coefficient (MCC), Kappa coefficient, etc. A new perspective is presented for those measures by revealing their cost functions with respect to the class imbalance ratio. Basically, they are described by four types of cost functions. The functions provides a theoretical understanding why some measures are suitable for dealing with class-imbalanced problems. Based on their cost functions, we are able to conclude that G-means of accuracy rates and BER are suitable measures because they show "proper" cost behaviors in terms of "a misclassification from a small class will cause a greater cost than that from a large class". On the contrary, F1 measure, G-means of recall and precision, MCC and Kappa coefficient measures do not produce such behaviors so that they are unsuitable to serve our goal in dealing with the problems properly. version:1
arxiv-1403-6397 | Evaluating topic coherence measures | http://arxiv.org/abs/1403.6397 | id:1403.6397 author:Frank Rosner, Alexander Hinneburg, Michael RÃ¶der, Martin Nettling, Andreas Both category:cs.LG cs.CL cs.IR  published:2014-03-25 summary:Topic models extract representative word sets - called topics - from word counts in documents without requiring any semantic annotations. Topics are not guaranteed to be well interpretable, therefore, coherence measures have been proposed to distinguish between good and bad topics. Studies of topic coherence so far are limited to measures that score pairs of individual words. For the first time, we include coherence measures from scientific philosophy that score pairs of more complex word subsets and apply them to topic scoring. version:1
arxiv-1403-6318 | Stabilizing dual-energy X-ray computed tomography reconstructions using patch-based regularization | http://arxiv.org/abs/1403.6318 | id:1403.6318 author:Brian H. Tracey, Eric L. Miller category:cs.CV physics.med-ph  published:2014-03-25 summary:Recent years have seen growing interest in exploiting dual- and multi-energy measurements in computed tomography (CT) in order to characterize material properties as well as object shape. Material characterization is performed by decomposing the scene into constitutive basis functions, such as Compton scatter and photoelectric absorption functions. While well motivated physically, the joint recovery of the spatial distribution of photoelectric and Compton properties is severely complicated by the fact that the data are several orders of magnitude more sensitive to Compton scatter coefficients than to photoelectric absorption, so small errors in Compton estimates can create large artifacts in the photoelectric estimate. To address these issues, we propose a model-based iterative approach which uses patch-based regularization terms to stabilize inversion of photoelectric coefficients, and solve the resulting problem though use of computationally attractive Alternating Direction Method of Multipliers (ADMM) solution techniques. Using simulations and experimental data acquired on a commercial scanner, we demonstrate that the proposed processing can lead to more stable material property estimates which should aid materials characterization in future dual- and multi-energy CT systems. version:1
arxiv-1310-4377 | Hierarchical Block Structures and High-resolution Model Selection in Large Networks | http://arxiv.org/abs/1310.4377 | id:1310.4377 author:Tiago P. Peixoto category:physics.data-an cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.soc-ph stat.ML  published:2013-10-16 summary:Discovering and characterizing the large-scale topological features in empirical networks are crucial steps in understanding how complex systems function. However, most existing methods used to obtain the modular structure of networks suffer from serious problems, such as being oblivious to the statistical evidence supporting the discovered patterns, which results in the inability to separate actual structure from noise. In addition to this, one also observes a resolution limit on the size of communities, where smaller but well-defined clusters are not detectable when the network becomes large. This phenomenon occurs not only for the very popular approach of modularity optimization, which lacks built-in statistical validation, but also for more principled methods based on statistical inference and model selection, which do incorporate statistical validation in a formally correct way. Here we construct a nested generative model that, through a complete description of the entire network hierarchy at multiple scales, is capable of avoiding this limitation, and enables the detection of modular structure at levels far beyond those possible with current approaches. Even with this increased resolution, the method is based on the principle of parsimony, and is capable of separating signal from noise, and thus will not lead to the identification of spurious modules even on sparse networks. Furthermore, it fully generalizes other approaches in that it is not restricted to purely assortative mixing patterns, directed or undirected graphs, and ad hoc hierarchical structures such as binary trees. Despite its general character, the approach is tractable, and can be combined with advanced techniques of community detection to yield an efficient algorithm that scales well for very large networks. version:6
arxiv-1403-6275 | A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies | http://arxiv.org/abs/1403.6275 | id:1403.6275 author:Vibhav Vineet, Jonathan Warrell, Philip H. S. Torr category:cs.CV  published:2014-03-25 summary:A large number of problems in computer vision can be modelled as energy minimization problems in a Markov Random Field (MRF) or Conditional Random Field (CRF) framework. Graph-cuts based $\alpha$-expansion is a standard move-making method to minimize the energy functions with sub-modular pairwise terms. However, certain problems require more complex pairwise terms where the $\alpha$-expansion method is generally not applicable. In this paper, we propose an iterative {\em tiered move making algorithm} which is able to handle general pairwise terms. Each move to the next configuration is based on the current labeling and an optimal tiered move, where each tiered move requires one application of the dynamic programming based tiered labeling method introduced in Felzenszwalb et. al. \cite{tiered_cvpr_felzenszwalbV10}. The algorithm converges to a local minimum for any general pairwise potential, and we give a theoretical analysis of the properties of the algorithm, characterizing the situations in which we can expect good performance. We first evaluate our method on an object-class segmentation problem using the Pascal VOC-11 segmentation dataset where we learn general pairwise terms. Further we evaluate the algorithm on many other benchmark labeling problems such as stereo, image segmentation, image stitching and image denoising. Our method consistently gets better accuracy and energy values than alpha-expansion, loopy belief propagation (LBP), quadratic pseudo-boolean optimization (QPBO), and is competitive with TRWS. version:1
arxiv-1403-6260 | Capturing and Recognizing Objects Appearance Employing Eigenspace | http://arxiv.org/abs/1403.6260 | id:1403.6260 author:M. Ashrafuzzaman, M. M . Rahman, M. M. A. Hashem category:cs.CV  published:2014-03-25 summary:This paper presents a method of capturing objects appearances from its environment and it also describes how to recognize unknown appearances creating an eigenspace. This representation and recognition can be done automatically taking objects various appearances by using robotic vision from a defined environment. This technique also allows extracting objects from some sort of complicated scenes. In this case, some of object appearances are taken with defined occlusions and eigenspaces are created by accepting both of non-occluded and occluded appearances together. Eigenspace is constructed successfully every times when a new object appears, and various appearances accumulated gradually. A sequence of appearances is generated from its accumulated shapes, which is used for recognition of the unknown objects appearances. Various objects environments are shown in the experiment to capture objects appearances and experimental results show effectiveness of the proposed approach. version:1
arxiv-1403-6248 | Classroom Video Assessment and Retrieval via Multiple Instance Learning | http://arxiv.org/abs/1403.6248 | id:1403.6248 author:Qifeng Qiao, Peter A. Beling category:cs.IR cs.CY cs.LG  published:2014-03-25 summary:We propose a multiple instance learning approach to content-based retrieval of classroom video for the purpose of supporting human assessing the learning environment. The key element of our approach is a mapping between the semantic concepts of the assessment system and features of the video that can be measured using techniques from the fields of computer vision and speech analysis. We report on a formative experiment in content-based video retrieval involving trained experts in the Classroom Assessment Scoring System, a widely used framework for assessment and improvement of learning environments. The results of this experiment suggest that our approach has potential application to productivity enhancement in assessment and to broader retrieval tasks. version:1
arxiv-1106-2363 | Random design analysis of ridge regression | http://arxiv.org/abs/1106.2363 | id:1106.2363 author:Daniel Hsu, Sham M. Kakade, Tong Zhang category:math.ST cs.AI cs.LG stat.ML stat.TH  published:2011-06-13 summary:This work gives a simultaneous analysis of both the ordinary least squares estimator and the ridge regression estimator in the random design setting under mild assumptions on the covariate/response distributions. In particular, the analysis provides sharp results on the ``out-of-sample'' prediction error, as opposed to the ``in-sample'' (fixed design) error. The analysis also reveals the effect of errors in the estimated covariance structure, as well as the effect of modeling errors, neither of which effects are present in the fixed design setting. The proofs of the main results are based on a simple decomposition lemma combined with concentration inequalities for random vectors and matrices. version:2
arxiv-1310-5415 | Disease Prediction based on Functional Connectomes using a Scalable and Spatially-Informed Support Vector Machine | http://arxiv.org/abs/1310.5415 | id:1310.5415 author:Takanori Watanabe, Daniel Kessler, Clayton Scott, Michael Angstadt, Chandra Sripada category:stat.ML  published:2013-10-21 summary:Substantial evidence indicates that major psychiatric disorders are associated with distributed neural dysconnectivity, leading to strong interest in using neuroimaging methods to accurately predict disorder status. In this work, we are specifically interested in a multivariate approach that uses features derived from whole-brain resting state functional connectomes. However, functional connectomes reside in a high dimensional space, which complicates model interpretation and introduces numerous statistical and computational challenges. Traditional feature selection techniques are used to reduce data dimensionality, but are blind to the spatial structure of the connectomes. We propose a regularization framework where the 6-D structure of the functional connectome is explicitly taken into account via the fused Lasso or the GraphNet regularizer. Our method only restricts the loss function to be convex and margin-based, allowing non-differentiable loss functions such as the hinge-loss to be used. Using the fused Lasso or GraphNet regularizer with the hinge-loss leads to a structured sparse support vector machine (SVM) with embedded feature selection. We introduce a novel efficient optimization algorithm based on the augmented Lagrangian and the classical alternating direction method, which can solve both fused Lasso and GraphNet regularized SVM with very little modification. We also demonstrate that the inner subproblems of the algorithm can be solved efficiently in analytic form by coupling the variable splitting strategy with a data augmentation scheme. Experiments on simulated data and resting state scans from a large schizophrenia dataset show that our proposed approach can identify predictive regions that are spatially contiguous in the 6-D "connectome space," offering an additional layer of interpretability that could provide new insights about various disease processes. version:2
arxiv-1403-6183 | Development and evaluation of a 3D model observer with nonlinear spatiotemporal contrast sensitivity | http://arxiv.org/abs/1403.6183 | id:1403.6183 author:Ali R. N. Avanaki, Kathryn S. Espig, Andrew D. A. Maidment, Cedric Marchessoux, Predrag R. Bakic, Tom R. L. Kimpe category:cs.CV  published:2014-03-24 summary:We investigate improvements to our 3D model observer with the goal of better matching human observer performance as a function of viewing distance, effective contrast, maximum luminance, and browsing speed. Two nonlinear methods of applying the human contrast sensitivity function (CSF) to a 3D model observer are proposed, namely the Probability Map (PM) and Monte Carlo (MC) methods. In the PM method, the visibility probability for each frequency component of the image stack, p, is calculated taking into account Barten's spatiotemporal CSF, the component modulation, and the human psychometric function. The probability p is considered to be equal to the perceived amplitude of the frequency component and thus can be used by a traditional model observer (e.g., LG-msCHO) in the space-time domain. In the MC method, each component is randomly kept with probability p or discarded with 1-p. The amplitude of the retained components is normalized to unity. The methods were tested using DBT stacks of an anthropomorphic breast phantom processed in a comprehensive simulation pipeline. Our experiments indicate that both the PM and MC methods yield results that match human observer performance better than the linear filtering method as a function of viewing distance, effective contrast, maximum luminance, and browsing speed. version:1
arxiv-1403-6173 | Coherent Multi-Sentence Video Description with Variable Level of Detail | http://arxiv.org/abs/1403.6173 | id:1403.6173 author:Anna Senina, Marcus Rohrbach, Wei Qiu, Annemarie Friedrich, Sikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, Bernt Schiele category:cs.CV cs.CL  published:2014-03-24 summary:Humans can easily describe what they see in a coherent way and at varying level of detail. However, existing approaches for automatic video description are mainly focused on single sentence generation and produce descriptions at a fixed level of detail. In this paper, we address both of these limitations: for a variable level of detail we produce coherent multi-sentence descriptions of complex videos. We follow a two-step approach where we first learn to predict a semantic representation (SR) from video and then generate natural language descriptions from the SR. To produce consistent multi-sentence descriptions, we model across-sentence consistency at the level of the SR by enforcing a consistent topic. We also contribute both to the visual recognition of objects proposing a hand-centric approach as well as to the robust generation of sentences using a word lattice. Human judges rate our multi-sentence descriptions as more readable, correct, and relevant than related work. To understand the difference between more detailed and shorter descriptions, we collect and analyze a video description corpus of three levels of detail. version:1
arxiv-1405-1402 | New Algorithmic Approaches to Point Constellation Recognition | http://arxiv.org/abs/1405.1402 | id:1405.1402 author:Thomas Bourgeat, Julien Bringer, Herve Chabanne, Robin Champenois, Jeremie Clement, Houda Ferradi, Marc Heinrich, Paul Melotti, David Naccache, Antoine Voizard category:cs.CV  published:2014-03-24 summary:Point constellation recognition is a common problem with many pattern matching applications. Whilst useful in many contexts, this work is mainly motivated by fingerprint matching. Fingerprints are traditionally modelled as constellations of oriented points called minutiae. The fingerprint verifier's task consists in comparing two point constellations. The compared constellations may differ by rotation and translation or by much more involved transforms such as distortion or occlusion. This paper presents three new constellation matching algorithms. The first two methods generalize an algorithm by Bringer and Despiegel. Our third proposal creates a very interesting analogy between mechanical system simulation and the constellation recognition problem. version:1
arxiv-1403-6023 | Ensemble Detection of Single & Multiple Events at Sentence-Level | http://arxiv.org/abs/1403.6023 | id:1403.6023 author:LuÃ­s Marujo, Anatole Gershman, Jaime Carbonell, JoÃ£o P. Neto, David Martins de Matos category:cs.CL cs.LG  published:2014-03-24 summary:Event classification at sentence level is an important Information Extraction task with applications in several NLP, IR, and personalization systems. Multi-label binary relevance (BR) are the state-of-art methods. In this work, we explored new multi-label methods known for capturing relations between event types. These new methods, such as the ensemble Chain of Classifiers, improve the F1 on average across the 6 labels by 2.8% over the Binary Relevance. The low occurrence of multi-label sentences motivated the reduction of the hard imbalanced multi-label classification problem with low number of occurrences of multiple labels per instance to an more tractable imbalanced multiclass problem with better results (+ 4.6%). We report the results of adding new features, such as sentiment strength, rhetorical signals, domain-id (source-id and date), and key-phrases in both single-label and multi-label event classification scenarios. version:1
arxiv-1404-1491 | An Efficient Feature Selection in Classification of Audio Files | http://arxiv.org/abs/1404.1491 | id:1404.1491 author:Jayita Mitra, Diganta Saha category:cs.LG  published:2014-03-24 summary:In this paper we have focused on an efficient feature selection method in classification of audio files. The main objective is feature selection and extraction. We have selected a set of features for further analysis, which represents the elements in feature vector. By extraction method we can compute a numerical representation that can be used to characterize the audio using the existing toolbox. In this study Gain Ratio (GR) is used as a feature selection measure. GR is used to select splitting attribute which will separate the tuples into different classes. The pulse clarity is considered as a subjective measure and it is used to calculate the gain of features of audio files. The splitting criterion is employed in the application to identify the class or the music genre of a specific audio file from testing database. Experimental results indicate that by using GR the application can produce a satisfactory result for music genre classification. After dimensionality reduction best three features have been selected out of various features of audio file and in this technique we will get more than 90% successful classification result. version:1
arxiv-1403-6002 | Brain Tumor Detection Based On Mathematical Analysis and Symmetry Information | http://arxiv.org/abs/1403.6002 | id:1403.6002 author:Narkhede Sachin G., Vaishali Khairnar, Sujata Kadu category:cs.CV  published:2014-03-24 summary:Image segmentation some of the challenging issues on brain magnetic resonance image tumor segmentation caused by the weak correlation between magnetic resonance imaging intensity and anatomical meaning.With the objective of utilizing more meaningful information to improve brain tumor segmentation,an approach which employs bilateral symmetry information as an additional feature for segmentation is proposed.This is motivated by potential performance improvement in the general automatic brain tumor segmentation systems which are important for many medical and scientific applications.Brain Magnetic Resonance Imaging segmentation is a complex problem in the field of medical imaging despite various presented methods.MR image of human brain can be divided into several sub-regions especially soft tissues such as gray matter,white matter and cerebra spinal fluid.Although edge information is the main clue in image segmentation,it cannot get a better result in analysis the content of images without combining other information.Our goal is to detect the position and boundary of tumors automatically.Experiments were conducted on real pictures,and the results show that the algorithm is flexible and convenient. version:1
arxiv-1403-5994 | First Order Methods for Robust Non-negative Matrix Factorization for Large Scale Noisy Data | http://arxiv.org/abs/1403.5994 | id:1403.5994 author:Jason Gejie Liu, Shuchin Aeron category:stat.ML  published:2014-03-24 summary:Nonnegative matrix factorization (NMF) has been shown to be identifiable under the separability assumption, under which all the columns(or rows) of the input data matrix belong to the convex cone generated by only a few of these columns(or rows) [1]. In real applications, however, such separability assumption is hard to satisfy. Following [4] and [5], in this paper, we look at the Linear Programming (LP) based reformulation to locate the extreme rays of the convex cone but in a noisy setting. Furthermore, in order to deal with the large scale data, we employ First-Order Methods (FOM) to mitigate the computational complexity of LP, which primarily results from a large number of constraints. We show the performance of the algorithm on real and synthetic data sets. version:1
arxiv-1312-2482 | Automatic recognition and tagging of topologically different regimes in dynamical systems | http://arxiv.org/abs/1312.2482 | id:1312.2482 author:Jesse Berwald, Marian Gidea, Mikael Vejdemo-Johansson category:cs.CG cs.LG math.DS nlin.CD physics.data-an  published:2013-12-09 summary:Complex systems are commonly modeled using nonlinear dynamical systems. These models are often high-dimensional and chaotic. An important goal in studying physical systems through the lens of mathematical models is to determine when the system undergoes changes in qualitative behavior. A detailed description of the dynamics can be difficult or impossible to obtain for high-dimensional and chaotic systems. Therefore, a more sensible goal is to recognize and mark transitions of a system between qualitatively different regimes of behavior. In practice, one is interested in developing techniques for detection of such transitions from sparse observations, possibly contaminated by noise. In this paper we develop a framework to accurately tag different regimes of complex systems based on topological features. In particular, our framework works with a high degree of success in picking out a cyclically orbiting regime from a stationary equilibrium regime in high-dimensional stochastic dynamical systems. version:2
arxiv-1403-5933 | AIS-INMACA: A Novel Integrated MACA Based Clonal Classifier for Protein Coding and Promoter Region Prediction | http://arxiv.org/abs/1403.5933 | id:1403.5933 author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu category:cs.CE cs.LG  published:2014-03-24 summary:Most of the problems in bioinformatics are now the challenges in computing. This paper aims at building a classifier based on Multiple Attractor Cellular Automata (MACA) which uses fuzzy logic. It is strengthened with an artificial Immune System Technique (AIS), Clonal algorithm for identifying a protein coding and promoter region in a given DNA sequence. The proposed classifier is named as AIS-INMACA introduces a novel concept to combine CA with artificial immune system to produce a better classifier which can address major problems in bioinformatics. This will be the first integrated algorithm which can predict both promoter and protein coding regions. To obtain good fitness rules the basic concept of Clonal selection algorithm was used. The proposed classifier can handle DNA sequences of lengths 54,108,162,252,354. This classifier gives the exact boundaries of both protein and promoter regions with an average accuracy of 89.6%. This classifier was tested with 97,000 data components which were taken from Fickett & Toung, MPromDb, and other sequences from a renowned medical university. This proposed classifier can handle huge data sets and can find protein and promoter regions even in mixed and overlapped DNA sequences. This work also aims at identifying the logicality between the major problems in bioinformatics and tries to obtaining a common frame work for addressing major problems in bioinformatics like protein structure prediction, RNA structure prediction, predicting the splicing pattern of any primary transcript and analysis of information content in DNA, RNA, protein sequences and structure. This work will attract more researchers towards application of CA as a potential pattern classifier to many important problems in bioinformatics version:1
arxiv-1403-5919 | SRA: Fast Removal of General Multipath for ToF Sensors | http://arxiv.org/abs/1403.5919 | id:1403.5919 author:Daniel Freedman, Eyal Krupka, Yoni Smolin, Ido Leichter, Mirko Schmidt category:cs.CV  published:2014-03-24 summary:A major issue with Time of Flight sensors is the presence of multipath interference. We present Sparse Reflections Analysis (SRA), an algorithm for removing this interference which has two main advantages. First, it allows for very general forms of multipath, including interference with three or more paths, diffuse multipath resulting from Lambertian surfaces, and combinations thereof. SRA removes this general multipath with robust techniques based on $L_1$ optimization. Second, due to a novel dimension reduction, we are able to produce a very fast version of SRA, which is able to run at frame rate. Experimental results on both synthetic data with ground truth, as well as real images of challenging scenes, validate the approach. version:1
arxiv-1403-5912 | The state of play of ASC-Inclusion: An Integrated Internet-Based Environment for Social Inclusion of Children with Autism Spectrum Conditions | http://arxiv.org/abs/1403.5912 | id:1403.5912 author:BjÃ¶rn Schuller, Erik Marchi, Simon Baron-Cohen, Helen O'Reilly, Delia Pigat, Peter Robinson, Ian Daves category:cs.HC cs.CV cs.CY  published:2014-03-24 summary:Individuals with Autism Spectrum Conditions (ASC) have marked difficulties using verbal and non-verbal communication for social interaction. The running ASC-Inclusion project aims to help children with ASC by allowing them to learn how emotions can be expressed and recognised via playing games in a virtual world. The platform includes analysis of users' gestures, facial, and vocal expressions using standard microphone and web-cam or a depth sensor, training through games, text communication with peers, animation, video and audio clips. We present the state of play in realising such a serious game platform and provide results for the different modalities. version:1
arxiv-1312-5192 | Nonlinear Eigenproblems in Data Analysis - Balanced Graph Cuts and the RatioDCA-Prox | http://arxiv.org/abs/1312.5192 | id:1312.5192 author:Leonardo Jost, Simon Setzer, Matthias Hein category:stat.ML cs.LG math.OC  published:2013-12-18 summary:It has been recently shown that a large class of balanced graph cuts allows for an exact relaxation into a nonlinear eigenproblem. We review briefly some of these results and propose a family of algorithms to compute nonlinear eigenvectors which encompasses previous work as special cases. We provide a detailed analysis of the properties and the convergence behavior of these algorithms and then discuss their application in the area of balanced graph cuts. version:2
arxiv-1403-5877 | Non-uniform Feature Sampling for Decision Tree Ensembles | http://arxiv.org/abs/1403.5877 | id:1403.5877 author:Anastasios Kyrillidis, Anastasios Zouzias category:stat.ML cs.IT cs.LG math.IT stat.AP  published:2014-03-24 summary:We study the effectiveness of non-uniform randomized feature selection in decision tree classification. We experimentally evaluate two feature selection methodologies, based on information extracted from the provided dataset: $(i)$ \emph{leverage scores-based} and $(ii)$ \emph{norm-based} feature selection. Experimental evaluation of the proposed feature selection techniques indicate that such approaches might be more effective compared to naive uniform feature selection and moreover having comparable performance to the random forest algorithm [3] version:1
arxiv-1403-5869 | Block Motion Based Dynamic Texture Analysis: A Review | http://arxiv.org/abs/1403.5869 | id:1403.5869 author:Akhlaqur Rahman, Sumaira Tasnim category:cs.CV  published:2014-03-24 summary:Dynamic texture refers to image sequences of non-rigid objects that exhibit some regularity in their movement. Videos of smoke, fire etc. fall under the category of dynamic texture. Researchers have investigated different ways to analyze dynamic textures since early nineties. Both appearance based (image intensities) and motion based approaches are investigated. Motion based approaches turn out to be more effective. A group of researchers have investigated ways to utilize the motion vectors readily available with the blocks in video codes like MGEG/H26X. In this paper we provide a review of the dynamic texture analysis methods using block motion. Research into dynamic texture analysis using block motion includes recognition, motion computation, segmentation, and synthesis. We provide a comprehensive review of these approaches. version:1
arxiv-1405-1403 | MCL-3D: a database for stereoscopic image quality assessment using 2D-image-plus-depth source | http://arxiv.org/abs/1405.1403 | id:1405.1403 author:Rui Song, Hyunsuk Ko, C. C. Jay Kuo category:cs.CV  published:2014-03-23 summary:A new stereoscopic image quality assessment database rendered using the 2D-image-plus-depth source, called MCL-3D, is described and the performance benchmarking of several known 2D and 3D image quality metrics using the MCL-3D database is presented in this work. Nine image-plus-depth sources are first selected, and a depth image-based rendering (DIBR) technique is used to render stereoscopic image pairs. Distortions applied to either the texture image or the depth image before stereoscopic image rendering include: Gaussian blur, additive white noise, down-sampling blur, JPEG and JPEG-2000 (JP2K) compression and transmission error. Furthermore, the distortion caused by imperfect rendering is also examined. The MCL-3D database contains 693 stereoscopic image pairs, where one third of them are of resolution 1024x728 and two thirds are of resolution 1920x1080. The pair-wise comparison was adopted in the subjective test for user friendliness, and the Mean Opinion Score (MOS) can be computed accordingly. Finally, we evaluate the performance of several 2D and 3D image quality metrics applied to MCL-3D. All texture images, depth images, rendered image pairs in MCL-3D and their MOS values obtained in the subjective test are available to the public (http://mcl.usc.edu/mcl-3d-database/) for future research and development. version:1
arxiv-1312-1031 | Analysis of Distributed Stochastic Dual Coordinate Ascent | http://arxiv.org/abs/1312.1031 | id:1312.1031 author:Tianbao Yang, Shenghuo Zhu, Rong Jin, Yuanqing Lin category:cs.DC cs.LG  published:2013-12-04 summary:In \citep{Yangnips13}, the author presented distributed stochastic dual coordinate ascent (DisDCA) algorithms for solving large-scale regularized loss minimization. Extraordinary performances have been observed and reported for the well-motivated updates, as referred to the practical updates, compared to the naive updates. However, no serious analysis has been provided to understand the updates and therefore the convergence rates. In the paper, we bridge the gap by providing a theoretical analysis of the convergence rates of the practical DisDCA algorithm. Our analysis helped by empirical studies has shown that it could yield an exponential speed-up in the convergence by increasing the number of dual updates at each iteration. This result justifies the superior performances of the practical DisDCA as compared to the naive variant. As a byproduct, our analysis also reveals the convergence behavior of the one-communication DisDCA. version:2
arxiv-1403-6384 | Human brain distinctiveness based on EEG spectral coherence connectivity | http://arxiv.org/abs/1403.6384 | id:1403.6384 author:Daria La Rocca, Patrizio Campisi, Balazs Vegso, Peter Cserti, Gyorgy Kozmann, Fabio Babiloni, Fabrizio De Vico Fallani category:q-bio.NC stat.ML  published:2014-03-23 summary:The use of EEG biometrics, for the purpose of automatic people recognition, has received increasing attention in the recent years. Most of current analysis rely on the extraction of features characterizing the activity of single brain regions, like power-spectrum estimates, thus neglecting possible temporal dependencies between the generated EEG signals. However, important physiological information can be extracted from the way different brain regions are functionally coupled. In this study, we propose a novel approach that fuses spectral coherencebased connectivity between different brain regions as a possibly viable biometric feature. The proposed approach is tested on a large dataset of subjects (N=108) during eyes-closed (EC) and eyes-open (EO) resting state conditions. The obtained recognition performances show that using brain connectivity leads to higher distinctiveness with respect to power-spectrum measurements, in both the experimental conditions. Notably, a 100% recognition accuracy is obtained in EC and EO when integrating functional connectivity between regions in the frontal lobe, while a lower 97.41% is obtained in EC (96.26% in EO) when fusing power spectrum information from centro-parietal regions. Taken together, these results suggest that functional connectivity patterns represent effective features for improving EEG-based biometric systems. version:1
arxiv-1312-7292 | Two Timescale Convergent Q-learning for Sleep--Scheduling in Wireless Sensor Networks | http://arxiv.org/abs/1312.7292 | id:1312.7292 author:Prashanth L. A., Abhranil Chatterjee, Shalabh Bhatnagar category:cs.SY cs.LG  published:2013-12-27 summary:In this paper, we consider an intrusion detection application for Wireless Sensor Networks (WSNs). We study the problem of scheduling the sleep times of the individual sensors to maximize the network lifetime while keeping the tracking error to a minimum. We formulate this problem as a partially-observable Markov decision process (POMDP) with continuous state-action spaces, in a manner similar to (Fuemmeler and Veeravalli [2008]). However, unlike their formulation, we consider infinite horizon discounted and average cost objectives as performance criteria. For each criterion, we propose a convergent on-policy Q-learning algorithm that operates on two timescales, while employing function approximation to handle the curse of dimensionality associated with the underlying POMDP. Our proposed algorithm incorporates a policy gradient update using a one-simulation simultaneous perturbation stochastic approximation (SPSA) estimate on the faster timescale, while the Q-value parameter (arising from a linear function approximation for the Q-values) is updated in an on-policy temporal difference (TD) algorithm-like fashion on the slower timescale. The feature selection scheme employed in each of our algorithms manages the energy and tracking components in a manner that assists the search for the optimal sleep-scheduling policy. For the sake of comparison, in both discounted and average settings, we also develop a function approximation analogue of the Q-learning algorithm. This algorithm, unlike the two-timescale variant, does not possess theoretical convergence guarantees. Finally, we also adapt our algorithms to include a stochastic iterative estimation scheme for the intruder's mobility model. Our simulation results on a 2-dimensional network setting suggest that our algorithms result in better tracking accuracy at the cost of only a few additional sensors, in comparison to a recent prior work. version:2
arxiv-1403-5718 | SmartAnnotator: An Interactive Tool for Annotating RGBD Indoor Images | http://arxiv.org/abs/1403.5718 | id:1403.5718 author:Yu-Shiang Wong, Hung-Kuo Chu, Niloy J. Mitra category:cs.CV  published:2014-03-23 summary:RGBD images with high quality annotations in the form of geometric (i.e., segmentation) and structural (i.e., how do the segments are mutually related in 3D) information provide valuable priors to a large number of scene and image manipulation applications. While it is now simple to acquire RGBD images, annotating them, automatically or manually, remains challenging especially in cluttered noisy environments. We present SmartAnnotator, an interactive system to facilitate annotating RGBD images. The system performs the tedious tasks of grouping pixels, creating potential abstracted cuboids, inferring object interactions in 3D, and comes up with various hypotheses. The user simply has to flip through a list of suggestions for segment labels, finalize a selection, and the system updates the remaining hypotheses. As objects are finalized, the process speeds up with fewer ambiguities to resolve. Further, as more scenes are annotated, the system makes better suggestions based on structural and geometric priors learns from the previous annotation sessions. We test our system on a large number of database scenes and report significant improvements over naive low-level annotation tools. version:1
arxiv-1312-6115 | Neuronal Synchrony in Complex-Valued Deep Networks | http://arxiv.org/abs/1312.6115 | id:1312.6115 author:David P. Reichert, Thomas Serre category:stat.ML cs.LG cs.NE q-bio.NC  published:2013-12-20 summary:Deep learning has recently led to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to cortical circuits. The challenge is to identify which neuronal mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile representations. We introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter, we demonstrate the potential of the approach in several simple experiments. Thus, neuronal synchrony could be a flexible mechanism that fulfills multiple functional roles in deep networks. version:5
arxiv-1403-5693 | Firefly Monte Carlo: Exact MCMC with Subsets of Data | http://arxiv.org/abs/1403.5693 | id:1403.5693 author:Dougal Maclaurin, Ryan P. Adams category:stat.ML cs.LG stat.CO  published:2014-03-22 summary:Markov chain Monte Carlo (MCMC) is a popular and successful general-purpose tool for Bayesian inference. However, MCMC cannot be practically applied to large data sets because of the prohibitive cost of evaluating every likelihood term at every iteration. Here we present Firefly Monte Carlo (FlyMC) an auxiliary variable MCMC algorithm that only queries the likelihoods of a potentially small subset of the data at each iteration yet simulates from the exact posterior distribution, in contrast to recent proposals that are approximate even in the asymptotic limit. FlyMC is compatible with a wide variety of modern MCMC algorithms, and only requires a lower bound on the per-datum likelihood factors. In experiments, we find that FlyMC generates samples from the posterior more than an order of magnitude faster than regular MCMC, opening up MCMC methods to larger datasets than were previously considered feasible. version:1
arxiv-1312-5663 | k-Sparse Autoencoders | http://arxiv.org/abs/1312.5663 | id:1312.5663 author:Alireza Makhzani, Brendan Frey category:cs.LG  published:2013-12-19 summary:Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is an autoencoder with linear activation function, where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied. version:2
arxiv-1403-5647 | CUR Algorithm with Incomplete Matrix Observation | http://arxiv.org/abs/1403.5647 | id:1403.5647 author:Rong Jin, Shenghuo Zhu category:cs.LG stat.ML  published:2014-03-22 summary:CUR matrix decomposition is a randomized algorithm that can efficiently compute the low rank approximation for a given rectangle matrix. One limitation with the existing CUR algorithms is that they require an access to the full matrix A for computing U. In this work, we aim to alleviate this limitation. In particular, we assume that besides having an access to randomly sampled d rows and d columns from A, we only observe a subset of randomly sampled entries from A. Our goal is to develop a low rank approximation algorithm, similar to CUR, based on (i) randomly sampled rows and columns from A, and (ii) randomly sampled entries from A. The proposed algorithm is able to perfectly recover the target matrix A with only O(rn log n) number of observed entries. In addition, instead of having to solve an optimization problem involved trace norm regularization, the proposed algorithm only needs to solve a standard regression problem. Finally, unlike most matrix completion theories that hold only when the target matrix is of low rank, we show a strong guarantee for the proposed algorithm even when the target matrix is not low rank. version:1
arxiv-1403-5607 | Bayesian Optimization with Unknown Constraints | http://arxiv.org/abs/1403.5607 | id:1403.5607 author:Michael A. Gelbart, Jasper Snoek, Ryan P. Adams category:stat.ML cs.LG  published:2014-03-22 summary:Recent work on Bayesian optimization has shown its effectiveness in global optimization of difficult black-box objective functions. Many real-world optimization problems of interest also have constraints which are unknown a priori. In this paper, we study Bayesian optimization for constrained problems in the general case that noise may be present in the constraint functions, and the objective and constraints may be evaluated independently. We provide motivating practical examples, and present a general framework to solve such problems. We demonstrate the effectiveness of our approach on optimizing the performance of online latent Dirichlet allocation subject to topic sparsity constraints, tuning a neural network given test-time memory constraints, and optimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed time, subject to passing standard convergence diagnostics. version:1
arxiv-1403-5603 | Forecasting Popularity of Videos using Social Media | http://arxiv.org/abs/1403.5603 | id:1403.5603 author:Jie Xu, Mihaela van der Schaar, Jiangchuan Liu, Haitao Li category:cs.LG cs.SI  published:2014-03-22 summary:This paper presents a systematic online prediction method (Social-Forecast) that is capable to accurately forecast the popularity of videos promoted by social media. Social-Forecast explicitly considers the dynamically changing and evolving propagation patterns of videos in social media when making popularity forecasts, thereby being situation and context aware. Social-Forecast aims to maximize the forecast reward, which is defined as a tradeoff between the popularity prediction accuracy and the timeliness with which a prediction is issued. The forecasting is performed online and requires no training phase or a priori knowledge. We analytically bound the prediction performance loss of Social-Forecast as compared to that obtained by an omniscient oracle and prove that the bound is sublinear in the number of video arrivals, thereby guaranteeing its short-term performance as well as its asymptotic convergence to the optimal performance. In addition, we conduct extensive experiments using real-world data traces collected from the videos shared in RenRen, one of the largest online social networks in China. These experiments show that our proposed method outperforms existing view-based approaches for popularity prediction (which are not context-aware) by more than 30% in terms of prediction rewards. version:1
arxiv-1403-5596 | A Lemma Based Evaluator for Semitic Language Text Summarization Systems | http://arxiv.org/abs/1403.5596 | id:1403.5596 author:Tarek El-Shishtawy, Fatma El-Ghannam category:cs.CL cs.IR  published:2014-03-22 summary:Matching texts in highly inflected languages such as Arabic by simple stemming strategy is unlikely to perform well. In this paper, we present a strategy for automatic text matching technique for for inflectional languages, using Arabic as the test case. The system is an extension of ROUGE test in which texts are matched on token's lemma level. The experimental results show an enhancement of detecting similarities between different sentences having same semantics but written in different lexical forms.. version:1
arxiv-1312-5650 | Zero-Shot Learning by Convex Combination of Semantic Embeddings | http://arxiv.org/abs/1312.5650 | id:1312.5650 author:Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S. Corrado, Jeffrey Dean category:cs.LG  published:2013-12-19 summary:Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional \nway{} classification framing of image understanding, particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing \nway{} image classifier and a semantic word embedding model, which contains the $\n$ class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task. version:3
arxiv-1403-5590 | Continuous Optimization for Fields of Experts Denoising Works | http://arxiv.org/abs/1403.5590 | id:1403.5590 author:Petter Strandmark, Sameer Agarwal category:cs.CV  published:2014-03-21 summary:Several recent papers use image denoising with a Fields of Experts prior to benchmark discrete optimization methods. We show that a non-linear least squares solver significantly outperforms all known discrete methods on this problem. version:1
arxiv-1403-5488 | Missing Data Prediction and Classification: The Use of Auto-Associative Neural Networks and Optimization Algorithms | http://arxiv.org/abs/1403.5488 | id:1403.5488 author:Collins Leke, Bhekisipho Twala, T. Marwala category:cs.NE cs.LG  published:2014-03-21 summary:This paper presents methods which are aimed at finding approximations to missing data in a dataset by using optimization algorithms to optimize the network parameters after which prediction and classification tasks can be performed. The optimization methods that are considered are genetic algorithm (GA), simulated annealing (SA), particle swarm optimization (PSO), random forest (RF) and negative selection (NS) and these methods are individually used in combination with auto-associative neural networks (AANN) for missing data estimation and the results obtained are compared. The methods suggested use the optimization algorithms to minimize an error function derived from training the auto-associative neural network during which the interrelationships between the inputs and the outputs are obtained and stored in the weights connecting the different layers of the network. The error function is expressed as the square of the difference between the actual observations and predicted values from an auto-associative neural network. In the event of missing data, all the values of the actual observations are not known hence, the error function is decomposed to depend on the known and unknown variable values. Multi-layer perceptron (MLP) neural network is employed to train the neural networks using the scaled conjugate gradient (SCG) method. Prediction accuracy is determined by mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and correlation coefficient (r) computations. Accuracy in classification is obtained by plotting ROC curves and calculating the areas under these. Analysis of results depicts that the approach using RF with AANN produces the most accurate predictions and classifications while on the other end of the scale is the approach which entails using NS with AANN. version:1
arxiv-1304-2986 | Adaptive piecewise polynomial estimation via trend filtering | http://arxiv.org/abs/1304.2986 | id:1304.2986 author:Ryan J. Tibshirani category:math.ST stat.ME stat.ML stat.TH  published:2013-04-10 summary:We study trend filtering, a recently proposed tool of Kim et al. [SIAM Rev. 51 (2009) 339-360] for nonparametric regression. The trend filtering estimate is defined as the minimizer of a penalized least squares criterion, in which the penalty term sums the absolute $k$th order discrete derivatives over the input points. Perhaps not surprisingly, trend filtering estimates appear to have the structure of $k$th degree spline functions, with adaptively chosen knot points (we say ``appear'' here as trend filtering estimates are not really functions over continuous domains, and are only defined over the discrete set of inputs). This brings to mind comparisons to other nonparametric regression tools that also produce adaptive splines; in particular, we compare trend filtering to smoothing splines, which penalize the sum of squared derivatives across input points, and to locally adaptive regression splines [Ann. Statist. 25 (1997) 387-413], which penalize the total variation of the $k$th derivative. Empirically, we discover that trend filtering estimates adapt to the local level of smoothness much better than smoothing splines, and further, they exhibit a remarkable similarity to locally adaptive regression splines. We also provide theoretical support for these empirical findings; most notably, we prove that (with the right choice of tuning parameter) the trend filtering estimate converges to the true underlying function at the minimax rate for functions whose $k$th derivative is of bounded variation. This is done via an asymptotic pairing of trend filtering and locally adaptive regression splines, which have already been shown to converge at the minimax rate [Ann. Statist. 25 (1997) 387-413]. At the core of this argument is a new result tying together the fitted values of two lasso problems that share the same outcome vector, but have different predictor matrices. version:2
arxiv-1403-5427 | The quasispecies regime for the simple genetic algorithm with ranking selection | http://arxiv.org/abs/1403.5427 | id:1403.5427 author:RaphaÃ«l Cerf category:math.PR cs.NE  published:2014-03-21 summary:We study the simple genetic algorithm with a ranking selection mechanism (linear ranking or tournament). We denote by $\ell$ the length of the chromosomes, by $m$ the population size, by $p_C$ the crossover probability and by $p_M$ the mutation probability. We introduce a parameter $\sigma$, called the selection drift, which measures the selection intensity of the fittest chromosome. We show that the dynamics of the genetic algorithm depend in a critical way on the parameter $$\pi \,=\,\sigma(1-p_C)(1-p_M)^\ell\,.$$ If $\pi<1$, then the genetic algorithm operates in a disordered regime: an advantageous mutant disappears with probability larger than $1-1/m^\beta$, where $\beta$ is a positive exponent. If $\pi>1$, then the genetic algorithm operates in a quasispecies regime: an advantageous mutant invades a positive fraction of the population with probability larger than a constant $p^*$ (which does not depend on $m$). We estimate next the probability of the occurrence of a catastrophe (the whole population falls below a fitness level which was previously reached by a positive fraction of the population). The asymptotic results suggest the following rules: $\pi=\sigma(1-p_C)(1-p_M)^\ell$ should be slightly larger than $1$; $p_M$ should be of order $1/\ell$; $m$ should be larger than $\ell\ln\ell$; the running time should be of exponential order in $m$. The first condition requires that $ \ell p_M +p_C< \ln\sigma$. These conclusions must be taken with great care: they come from an asymptotic regime, and it is a formidable task to understand the relevance of this regime for a real-world problem. At least, we hope that these conclusions provide interesting guidelines for the practical implementation of the simple genetic algorithm. version:1
arxiv-1405-1999 | Model-Driven Applications of Fractional Derivatives and Integrals | http://arxiv.org/abs/1405.1999 | id:1405.1999 author:William A. Sethares, SelÃ§uk Å. BayÄ±n category:cs.CV  published:2014-03-21 summary:Fractional order derivatives and integrals (differintegrals) are viewed from a frequency-domain perspective using the formalism of Riesz, providing a computational tool as well as a way to interpret the operations in the frequency domain. Differintegrals provide a logical extension of current techniques, generalizing the notion of integral and differential operators and acting as kind of frequency-domain filtering that has many of the advantages of a nonlocal linear operator. Several important properties of differintegrals are presented, and sample applications are given to one- and two-dimensional signals. Computer code to carry out the computations is made available on the author's website. version:1
arxiv-1401-0711 | Computing Entropy Rate Of Symbol Sources & A Distribution-free Limit Theorem | http://arxiv.org/abs/1401.0711 | id:1401.0711 author:Ishanu Chattopadhyay, Hod Lipson category:cs.IT cs.LG math.IT math.PR stat.CO stat.ML  published:2014-01-03 summary:Entropy rate of sequential data-streams naturally quantifies the complexity of the generative process. Thus entropy rate fluctuations could be used as a tool to recognize dynamical perturbations in signal sources, and could potentially be carried out without explicit background noise characterization. However, state of the art algorithms to estimate the entropy rate have markedly slow convergence; making such entropic approaches non-viable in practice. We present here a fundamentally new approach to estimate entropy rates, which is demonstrated to converge significantly faster in terms of input data lengths, and is shown to be effective in diverse applications ranging from the estimation of the entropy rate of English texts to the estimation of complexity of chaotic dynamical systems. Additionally, the convergence rate of entropy estimates do not follow from any standard limit theorem, and reported algorithms fail to provide any confidence bounds on the computed values. Exploiting a connection to the theory of probabilistic automata, we establish a convergence rate of $O(\log \vert s \vert/\sqrt[3]{\vert s \vert})$ as a function of the input length $\vert s \vert$, which then yields explicit uncertainty estimates, as well as required data lengths to satisfy pre-specified confidence bounds. version:2
arxiv-1403-5370 | Using n-grams models for visual semantic place recognition | http://arxiv.org/abs/1403.5370 | id:1403.5370 author:Mathieu Dubois, Frenoux Emmanuelle, Philippe Tarroux category:stat.ML cs.CV cs.LG  published:2014-03-21 summary:The aim of this paper is to present a new method for visual place recognition. Our system combines global image characterization and visual words, which allows to use efficient Bayesian filtering methods to integrate several images. More precisely, we extend the classical HMM model with techniques inspired by the field of Natural Language Processing. This paper presents our system and the Bayesian filtering algorithm. The performance of our system and the influence of the main parameters are evaluated on a standard database. The discussion highlights the interest of using such models and proposes improvements. version:1
arxiv-1403-6381 | An efficiency dependency parser using hybrid approach for tamil language | http://arxiv.org/abs/1403.6381 | id:1403.6381 author:K. Sureka, K. G. Srinivasagan, S. Suganthi category:cs.CL  published:2014-03-21 summary:Natural language processing is a prompt research area across the country. Parsing is one of the very crucial tool in language analysis system which aims to forecast the structural relationship among the words in a given sentence. Many researchers have already developed so many language tools but the accuracy is not meet out the human expectation level, thus the research is still exists. Machine translation is one of the major application area under Natural Language Processing. While translation between one language to another language, the structure identification of a sentence play a key role. This paper introduces the hybrid way to solve the identification of relationship among the given words in a sentence. In existing system is implemented using rule based approach, which is not suited in huge amount of data. The machine learning approaches is suitable for handle larger amount of data and also to get better accuracy via learning and training the system. The proposed approach takes a Tamil sentence as an input and produce the result of a dependency relation as a tree like structure using hybrid approach. This proposed tool is very helpful for researchers and act as an odd-on improve the quality of existing approaches. version:1
arxiv-1311-4780 | Asymptotically Exact, Embarrassingly Parallel MCMC | http://arxiv.org/abs/1311.4780 | id:1311.4780 author:Willie Neiswanger, Chong Wang, Eric Xing category:stat.ML cs.DC cs.LG stat.CO  published:2013-11-19 summary:Communication costs, resulting from synchronization requirements during learning, can greatly slow down many parallel machine learning algorithms. In this paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm in which subsets of data are processed independently, with very little communication. First, we arbitrarily partition data onto multiple machines. Then, on each machine, any classical MCMC method (e.g., Gibbs sampling) may be used to draw samples from a posterior distribution given the data subset. Finally, the samples from each machine are combined to form samples from the full posterior. This embarrassingly parallel algorithm allows each machine to act independently on a subset of the data (without communication) until the final combination stage. We prove that our algorithm generates asymptotically exact samples and empirically demonstrate its ability to parallelize burn-in and sampling in several models. version:2
arxiv-1302-0581 | SMML estimators for exponential families with continuous sufficient statistics | http://arxiv.org/abs/1302.0581 | id:1302.0581 author:James G. Dowty category:cs.IT math.IT math.ST stat.ML stat.TH  published:2013-02-04 summary:The minimum message length principle is an information theoretic criterion that links data compression with statistical inference. This paper studies the strict minimum message length (SMML) estimator for $d$-dimensional exponential families with continuous sufficient statistics, for all $d \ge 1$. The partition of an SMML estimator is shown to consist of convex polytopes (i.e. convex polygons when $d=2$) which can be described explicitly in terms of the assertions and coding probabilities. While this result is known, we give a new proof based on the calculus of variations, and this approach gives some interesting new inequalities for SMML estimators. We also use this result to construct an SMML estimator for a $2$-dimensional normal random variable with known variance and a normal prior on its mean. version:2
arxiv-1403-5345 | A Physarum-Inspired Approach to Optimal Supply Chain Network Design at Minimum Total Cost with Demand Satisfaction | http://arxiv.org/abs/1403.5345 | id:1403.5345 author:Xiaoge Zhang, Andrew Adamatzky, Xin-She Yang, Hai Yang, Sankaran Mahadevan, Yong Deng category:cs.NE  published:2014-03-21 summary:A supply chain is a system which moves products from a supplier to customers. The supply chains are ubiquitous. They play a key role in all economic activities. Inspired by biological principles of nutrients' distribution in protoplasmic networks of slime mould Physarum polycephalum we propose a novel algorithm for a supply chain design. The algorithm handles the supply networks where capacity investments and product flows are variables. The networks are constrained by a need to satisfy product demands. Two features of the slime mould are adopted in our algorithm. The first is the continuity of a flux during the iterative process, which is used in real-time update of the costs associated with the supply links. The second feature is adaptivity. The supply chain can converge to an equilibrium state when costs are changed. Practicality and flexibility of our algorithm is illustrated on numerical examples. version:1
arxiv-1403-5287 | Online Local Learning via Semidefinite Programming | http://arxiv.org/abs/1403.5287 | id:1403.5287 author:Paul Christiano category:cs.LG  published:2014-03-20 summary:In many online learning problems we are interested in predicting local information about some universe of items. For example, we may want to know whether two items are in the same cluster rather than computing an assignment of items to clusters; we may want to know which of two teams will win a game rather than computing a ranking of teams. Although finding the optimal clustering or ranking is typically intractable, it may be possible to predict the relationships between items as well as if you could solve the global optimization problem exactly. Formally, we consider an online learning problem in which a learner repeatedly guesses a pair of labels (l(x), l(y)) and receives an adversarial payoff depending on those labels. The learner's goal is to receive a payoff nearly as good as the best fixed labeling of the items. We show that a simple algorithm based on semidefinite programming can obtain asymptotically optimal regret in the case where the number of possible labels is O(1), resolving an open problem posed by Hazan, Kale, and Shalev-Schwartz. Our main technical contribution is a novel use and analysis of the log determinant regularizer, exploiting the observation that log det(A + I) upper bounds the entropy of any distribution with covariance matrix A. version:1
arxiv-1404-1292 | Review of Face Detection Systems Based Artificial Neural Networks Algorithms | http://arxiv.org/abs/1404.1292 | id:1404.1292 author:Omaima N. A. AL-Allaf category:cs.CV cs.NE  published:2014-03-20 summary:Face detection is one of the most relevant applications of image processing and biometric systems. Artificial neural networks (ANN) have been used in the field of image processing and pattern recognition. There is lack of literature surveys which give overview about the studies and researches related to the using of ANN in face detection. Therefore, this research includes a general review of face detection studies and systems which based on different ANN approaches and algorithms. The strengths and limitations of these literature studies and systems were included also. version:1
arxiv-1211-4601 | Smoothing Dynamic Systems with State-Dependent Covariance Matrices | http://arxiv.org/abs/1211.4601 | id:1211.4601 author:Aleksandr Y. Aravkin, James V. Burke category:math.OC stat.CO stat.ML 62F35  65K10  published:2012-11-19 summary:Kalman filtering and smoothing algorithms are used in many areas, including tracking and navigation, medical applications, and financial trend filtering. One of the basic assumptions required to apply the Kalman smoothing framework is that error covariance matrices are known and given. In this paper, we study a general class of inference problems where covariance matrices can depend functionally on unknown parameters. In the Kalman framework, this allows modeling situations where covariance matrices may depend functionally on the state sequence being estimated. We present an extended formulation and generalized Gauss-Newton (GGN) algorithm for inference in this context. When applied to dynamic systems inference, we show the algorithm can be implemented to preserve the computational efficiency of the classic Kalman smoother. The new approach is illustrated with a synthetic numerical example. version:2
arxiv-1403-5177 | Sparse Learning over Infinite Subgraph Features | http://arxiv.org/abs/1403.5177 | id:1403.5177 author:Ichigaku Takigawa, Hiroshi Mamitsuka category:stat.ML  published:2014-03-20 summary:We present a supervised-learning algorithm from graph data (a set of graphs) for arbitrary twice-differentiable loss functions and sparse linear models over all possible subgraph features. To date, it has been shown that under all possible subgraph features, several types of sparse learning, such as Adaboost, LPBoost, LARS/LASSO, and sparse PLS regression, can be performed. Particularly emphasis is placed on simultaneous learning of relevant features from an infinite set of candidates. We first generalize techniques used in all these preceding studies to derive an unifying bounding technique for arbitrary separable functions. We then carefully use this bounding to make block coordinate gradient descent feasible over infinite subgraph features, resulting in a fast converging algorithm that can solve a wider class of sparse learning problems over graph data. We also empirically study the differences from the existing approaches in convergence property, selected subgraph features, and search-space sizes. We further discuss several unnoticed issues in sparse learning over all possible subgraph features. version:1
arxiv-1312-6173 | Multilingual Distributed Representations without Word Alignment | http://arxiv.org/abs/1312.6173 | id:1312.6173 author:Karl Moritz Hermann, Phil Blunsom category:cs.CL  published:2013-12-20 summary:Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. Recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning distributed representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used. version:4
arxiv-1403-5115 | Unconfused Ultraconservative Multiclass Algorithms | http://arxiv.org/abs/1403.5115 | id:1403.5115 author:Ugo Louche, Liva Ralaivola category:cs.LG  published:2014-03-20 summary:We tackle the problem of learning linear classifiers from noisy datasets in a multiclass setting. The two-class version of this problem was studied a few years ago by, e.g. Bylander (1994) and Blum et al. (1996): in these contributions, the proposed approaches to fight the noise revolve around a Perceptron learning scheme fed with peculiar examples computed through a weighted average of points from the noisy training set. We propose to build upon these approaches and we introduce a new algorithm called UMA (for Unconfused Multiclass additive Algorithm) which may be seen as a generalization to the multiclass setting of the previous approaches. In order to characterize the noise we use the confusion matrix as a multiclass extension of the classification noise studied in the aforementioned literature. Theoretically well-founded, UMA furthermore displays very good empirical noise robustness, as evidenced by numerical simulations conducted on both synthetic and real data. Keywords: Multiclass classification, Perceptron, Noisy labels, Confusion Matrix version:1
arxiv-1403-5112 | On The Sample Complexity of Sparse Dictionary Learning | http://arxiv.org/abs/1403.5112 | id:1403.5112 author:Matthias Seibert, Martin Kleinsteuber, RÃ©mi Gribonval, Rodolphe Jenatton, Francis Bach category:stat.ML  published:2014-03-20 summary:In the synthesis model signals are represented as a sparse combinations of atoms from a dictionary. Dictionary learning describes the acquisition process of the underlying dictionary for a given set of training samples. While ideally this would be achieved by optimizing the expectation of the factors over the underlying distribution of the training data, in practice the necessary information about the distribution is not available. Therefore, in real world applications it is achieved by minimizing an empirical average over the available samples. The main goal of this paper is to provide a sample complexity estimate that controls to what extent the empirical average deviates from the cost function. This estimate then provides a suitable estimate to the accuracy of the representation of the learned dictionary. The presented approach exemplifies the general results proposed by the authors in Sample Complexity of Dictionary Learning and other Matrix Factorizations, Gribonval et al. and gives more concrete bounds of the sample complexity of dictionary learning. We cover a variety of sparsity measures employed in the learning procedure. version:1
arxiv-1402-5076 | Robust Binary Fused Compressive Sensing using Adaptive Outlier Pursuit | http://arxiv.org/abs/1402.5076 | id:1402.5076 author:Xiangrong Zeng, MÃ¡rio A. T. Figueiredo category:cs.CV cs.IT math.IT  published:2014-02-20 summary:We propose a new method, {\it robust binary fused compressive sensing} (RoBFCS), to recover sparse piece-wise smooth signals from 1-bit compressive measurements. The proposed method is a modification of our previous {\it binary fused compressive sensing} (BFCS) algorithm, which is based on the {\it binary iterative hard thresholding} (BIHT) algorithm. As in BIHT, the data term of the objective function is a one-sided $\ell_1$ (or $\ell_2$) norm. Experiments show that the proposed algorithm is able to take advantage of the piece-wise smoothness of the original signal and detect sign flips and correct them, achieving more accurate recovery than BFCS and BIHT. version:2
arxiv-1403-4928 | Clinical TempEval | http://arxiv.org/abs/1403.4928 | id:1403.4928 author:Steven Bethard, Leon Derczynski, James Pustejovsky, Marc Verhagen category:cs.CL  published:2014-03-19 summary:We describe the Clinical TempEval task which is currently in preparation for the SemEval-2015 evaluation exercise. This task involves identifying and describing events, times and the relations between them in clinical text. Six discrete subtasks are included, focusing on recognising mentions of times and events, describing those mentions for both entity types, identifying the relation between an event and the document creation time, and identifying narrative container relations. version:1
arxiv-1403-4887 | Using Entropy Estimates for DAG-Based Ontologies | http://arxiv.org/abs/1403.4887 | id:1403.4887 author:Andrew Warren, Joao Setubal category:cs.CL  published:2014-03-19 summary:Motivation: Entropy measurements on hierarchical structures have been used in methods for information retrieval and natural language modeling. Here we explore its application to semantic similarity. By finding shared ontology terms, semantic similarity can be established between annotated genes. A common procedure for establishing semantic similarity is to calculate the descriptiveness (information content) of ontology terms and use these values to determine the similarity of annotations. Most often information content is calculated for an ontology term by analyzing its frequency in an annotation corpus. The inherent problems in using these values to model functional similarity motivates our work. Summary: We present a novel calculation for establishing the entropy of a DAG-based ontology, which can be used in an alternative method for establishing the information content of its terms. We also compare our IC metric to two others using semantic and sequence similarity. version:1
arxiv-1403-4871 | Evolutionary Algorithm for Drug Discovery Interim Design Report | http://arxiv.org/abs/1403.4871 | id:1403.4871 author:Mark Shackelford category:cs.NE cs.CE  published:2014-03-19 summary:A software program which aims to provide an exploration capability over the Search Space of potential drug molecules. The program explores the search space by generating random molecules, determining their fitness and then breeding a new generation from the fittest individuals. The search space, in theory any combination of any elements in any order, is constrained by the use of a subset of elements and a list of fragments, molecular parts that are known to be useful in drug development. The resultant molecules from each generation are stored in a searchable database, so that the user can browse through previous generations looking for interesting molecules. version:1
arxiv-1403-4267 | Balancing Sparsity and Rank Constraints in Quadratic Basis Pursuit | http://arxiv.org/abs/1403.4267 | id:1403.4267 author:Cagdas Bilen, Gilles Puy, RÃ©mi Gribonval, Laurent Daudet category:cs.NA cs.LG  published:2014-03-17 summary:We investigate the methods that simultaneously enforce sparsity and low-rank structure in a matrix as often employed for sparse phase retrieval problems or phase calibration problems in compressive sensing. We propose a new approach for analyzing the trade off between the sparsity and low rank constraints in these approaches which not only helps to provide guidelines to adjust the weights between the aforementioned constraints, but also enables new simulation strategies for evaluating performance. We then provide simulation results for phase retrieval and phase calibration cases both to demonstrate the consistency of the proposed method with other approaches and to evaluate the change of performance with different weights for the sparsity and low rank structure constraints. version:2
arxiv-1403-4781 | A Split-and-Merge Dictionary Learning Algorithm for Sparse Representation | http://arxiv.org/abs/1403.4781 | id:1403.4781 author:Subhadip Mukherjee, Chandra Sekhar Seelamantula category:cs.LG stat.ML  published:2014-03-19 summary:In big data image/video analytics, we encounter the problem of learning an overcomplete dictionary for sparse representation from a large training dataset, which can not be processed at once because of storage and computational constraints. To tackle the problem of dictionary learning in such scenarios, we propose an algorithm for parallel dictionary learning. The fundamental idea behind the algorithm is to learn a sparse representation in two phases. In the first phase, the whole training dataset is partitioned into small non-overlapping subsets, and a dictionary is trained independently on each small database. In the second phase, the dictionaries are merged to form a global dictionary. We show that the proposed algorithm is efficient in its usage of memory and computational complexity, and performs on par with the standard learning strategy operating on the entire data at a time. As an application, we consider the problem of image denoising. We present a comparative analysis of our algorithm with the standard learning techniques, that use the entire database at a time, in terms of training and denoising performance. We observe that the split-and-merge algorithm results in a remarkable reduction of training time, without significantly affecting the denoising performance. version:1
arxiv-1403-4777 | MFCC based Enlargement of the Training Set for Emotion Recognition in Speech | http://arxiv.org/abs/1403.4777 | id:1403.4777 author:Inma Mohino-Herranz, Roberto Gil-Pita, Sagrario Alonso-Diaz, Manuel Rosa-Zurera category:cs.CV  published:2014-03-19 summary:Emotional state recognition through speech is being a very interesting research topic nowadays. Using subliminal information of speech, denominated as prosody, it is possible to recognize the emotional state of the person. One of the main problems in the design of automatic emotion recognition systems is the small number of available patterns. This fact makes the learning process more difficult, due to the generalization problems that arise under these conditions. In this work we propose a solution to this problem consisting in enlarging the training set through the creation the new virtual patterns. In the case of emotional speech, most of the emotional information is included in speed and pitch variations. So, a change in the average pitch that does not modify neither the speed nor the pitch variations does not affect the expressed emotion. Thus, we use this prior information in order to create new patterns applying a gender dependent pitch shift modification in the feature extraction process of the classification system. For this purpose, we propose a frequency scaling modification of the Mel Frequency Cepstral Coefficients, used to classify the emotion. For this purpose, we propose a gender dependent frequency scaling modification. This proposed process allows us to synthetically increase the number of available patterns in the training set, thus increasing the generalization capability of the system and reducing the test error. Results carried out with two different classifiers with different degree of generalization capability demonstrate the suitability of the proposal. version:1
arxiv-1403-4759 | Spelling Error Trends and Patterns in Sindhi | http://arxiv.org/abs/1403.4759 | id:1403.4759 author:Zeeshan Bhatti, Imdad Ali Ismaili, Asad Ali Shaikh, Waseem Javaid category:cs.CL  published:2014-03-19 summary:Statistical error Correction technique is the most accurate and widely used approach today, but for a language like Sindhi which is a low resourced language the trained corpora's are not available, so the statistical techniques are not possible at all. Instead a useful alternative would be to exploit various spelling error trends in Sindhi by using a Rule based approach. For designing such technique an essential prerequisite would be to study the various error patterns in a language. This pa per presents various studies of spelling error trends and their types in Sindhi Language. The research shows that the error trends common to all languages are also encountered in Sindhi but their do exist some error patters that are catered specifically to a Sindhi language. version:1
arxiv-1405-1020 | Study on performance improvement of oil paint image filter algorithm using parallel pattern library | http://arxiv.org/abs/1405.1020 | id:1405.1020 author:Siddhartha Mukherjee category:cs.CV cs.DC  published:2014-03-19 summary:This paper gives a detailed study on the performance of oil paint image filter algorithm with various parameters applied on an image of RGB model. Oil Paint image processing, being very performance hungry, current research tries to find improvement using parallel pattern library. With increasing kernel-size, the processing time of oil paint image filter algorithm increases exponentially. version:1
arxiv-1403-4699 | A Proximal Stochastic Gradient Method with Progressive Variance Reduction | http://arxiv.org/abs/1403.4699 | id:1403.4699 author:Lin Xiao, Tong Zhang category:math.OC stat.ML  published:2014-03-19 summary:We consider the problem of minimizing the sum of two convex functions: one is the average of a large number of smooth component functions, and the other is a general convex function that admits a simple proximal mapping. We assume the whole objective function is strongly convex. Such problems often arise in machine learning, known as regularized empirical risk minimization. We propose and analyze a new proximal stochastic gradient method, which uses a multi-stage scheme to progressively reduce the variance of the stochastic gradient. While each iteration of this algorithm has similar cost as the classical stochastic gradient method (or incremental gradient method), we show that the expected objective value converges to the optimum at a geometric rate. The overall complexity of this method is much lower than both the proximal full gradient method and the standard proximal stochastic gradient method. version:1
arxiv-1403-4682 | Structured Sparse Method for Hyperspectral Unmixing | http://arxiv.org/abs/1403.4682 | id:1403.4682 author:Feiyun Zhu, Ying Wang, Shiming Xiang, Bin Fan, Chunhong Pan category:cs.CV cs.AI  published:2014-03-19 summary:Hyperspectral Unmixing (HU) has received increasing attention in the past decades due to its ability of unveiling information latent in hyperspectral data. Unfortunately, most existing methods fail to take advantage of the spatial information in data. To overcome this limitation, we propose a Structured Sparse regularized Nonnegative Matrix Factorization (SS-NMF) method from the following two aspects. First, we incorporate a graph Laplacian to encode the manifold structures embedded in the hyperspectral data space. In this way, the highly similar neighboring pixels can be grouped together. Second, the lasso penalty is employed in SS-NMF for the fact that pixels in the same manifold structure are sparsely mixed by a common set of relevant bases. These two factors act as a new structured sparse constraint. With this constraint, our method can learn a compact space, where highly similar pixels are grouped to share correlated sparse representations. Experiments on real hyperspectral data sets with different noise levels demonstrate that our method outperforms the state-of-the-art methods significantly. version:1
arxiv-1403-4626 | Bayesian Source Separation Applied to Identifying Complex Organic Molecules in Space | http://arxiv.org/abs/1403.4626 | id:1403.4626 author:Kevin H. Knuth, Man Kit Tse, Joshua Choinsky, Haley A. Maunu, Duane F. Carbon category:astro-ph.IM physics.data-an stat.ML  published:2014-03-18 summary:Emission from a class of benzene-based molecules known as Polycyclic Aromatic Hydrocarbons (PAHs) dominates the infrared spectrum of star-forming regions. The observed emission appears to arise from the combined emission of numerous PAH species, each with its unique spectrum. Linear superposition of the PAH spectra identifies this problem as a source separation problem. It is, however, of a formidable class of source separation problems given that different PAH sources potentially number in the hundreds, even thousands, and there is only one measured spectral signal for a given astrophysical site. Fortunately, the source spectra of the PAHs are known, but the signal is also contaminated by other spectral sources. We describe our ongoing work in developing Bayesian source separation techniques relying on nested sampling in conjunction with an ON/OFF mechanism enabling simultaneous estimation of the probability that a particular PAH species is present and its contribution to the spectrum. version:1
arxiv-1403-4608 | Can Cascades be Predicted? | http://arxiv.org/abs/1403.4608 | id:1403.4608 author:Justin Cheng, Lada A. Adamic, P. Alex Dow, Jon Kleinberg, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8  published:2014-03-18 summary:On many social networking web sites such as Facebook and Twitter, resharing or reposting functionality allows users to share others' content with their own friends or followers. As content is reshared from user to user, large cascades of reshares can form. While a growing body of research has focused on analyzing and characterizing such cascades, a recent, parallel line of work has argued that the future trajectory of a cascade may be inherently unpredictable. In this work, we develop a framework for addressing cascade prediction problems. On a large sample of photo reshare cascades on Facebook, we find strong performance in predicting whether a cascade will continue to grow in the future. We find that the relative growth of a cascade becomes more predictable as we observe more of its reshares, that temporal and structural features are key predictors of cascade size, and that initially, breadth, rather than depth in a cascade is a better indicator of larger cascades. This prediction performance is robust in the sense that multiple distinct classes of features all achieve similar performance. We also discover that temporal features are predictive of a cascade's eventual shape. Observing independent cascades of the same content, we find that while these cascades differ greatly in size, we are still able to predict which ends up the largest. version:1
arxiv-1403-3668 | Language Heedless of Logic - Philosophy Mindful of What? Failures of Distributive and Absorption Laws | http://arxiv.org/abs/1403.3668 | id:1403.3668 author:Arthur Merin category:cs.CL  published:2014-03-14 summary:Much of philosophical logic and all of philosophy of language make empirical claims about the vernacular natural language. They presume semantics under which `and' and `or' are related by the dually paired distributive and absorption laws. However, at least one of each pair of laws fails in the vernacular. `Implicature'-based auxiliary theories associated with the programme of H.P. Grice do not prove remedial. Conceivable alternatives that might replace the familiar logics as descriptive instruments are briefly noted: (i) substructural logics and (ii) meaning composition in linear algebras over the reals, occasionally constrained by norms of classical logic. Alternative (ii) locates the problem in violations of one of the idempotent laws. Reasons for a lack of curiosity about elementary and easily testable implications of the received theory are considered. The concept of `reflective equilibrium' is critically examined for its role in reconciling normative desiderata and descriptive commitments. version:2
arxiv-1108-4135 | Complex-Valued Autoencoders | http://arxiv.org/abs/1108.4135 | id:1108.4135 author:Pierre Baldi, Zhiqin Lu category:cs.NE math.RA  published:2011-08-20 summary:Autoencoders are unsupervised machine learning circuits whose learning goal is to minimize a distortion measure between inputs and outputs. Linear autoencoders can be defined over any field and only real-valued linear autoencoder have been studied so far. Here we study complex-valued linear autoencoders where the components of the training vectors and adjustable matrices are defined over the complex field with the $L_2$ norm. We provide simpler and more general proofs that unify the real-valued and complex-valued cases, showing that in both cases the landscape of the error function is invariant under certain groups of transformations. The landscape has no local minima, a family of global minima associated with Principal Component Analysis, and many families of saddle points associated with orthogonal projections onto sub-space spanned by sub-optimal subsets of eigenvectors of the covariance matrix. The theory yields several iterative, convergent, learning algorithms, a clear understanding of the generalization properties of the trained autoencoders, and can equally be applied to the hetero-associative case when external targets are provided. Partial results on deep architecture as well as the differential geometry of autoencoders are also presented. The general framework described here is useful to classify autoencoders and identify general common properties that ought to be investigated for each class, illuminating some of the connections between information theory, unsupervised learning, clustering, Hebbian learning, and autoencoders. version:2
arxiv-1403-4540 | Similarity networks for classification: a case study in the Horse Colic problem | http://arxiv.org/abs/1403.4540 | id:1403.4540 author:LluÃ­s Belanche, JerÃ³nimo HernÃ¡ndez category:cs.LG cs.NE  published:2014-03-18 summary:This paper develops a two-layer neural network in which the neuron model computes a user-defined similarity function between inputs and weights. The neuron transfer function is formed by composition of an adapted logistic function with the mean of the partial input-weight similarities. The resulting neuron model is capable of dealing directly with variables of potentially different nature (continuous, fuzzy, ordinal, categorical). There is also provision for missing values. The network is trained using a two-stage procedure very similar to that used to train a radial basis function (RBF) neural network. The network is compared to two types of RBF networks in a non-trivial dataset: the Horse Colic problem, taken as a case study and analyzed in detail. version:1
arxiv-1304-0499 | Splitting Methods for Convex Clustering | http://arxiv.org/abs/1304.0499 | id:1304.0499 author:Eric C. Chi, Kenneth Lange category:stat.ML math.NA math.OC stat.CO 62H30  90C25  90C90  published:2013-04-01 summary:Clustering is a fundamental problem in many scientific applications. Standard methods such as $k$-means, Gaussian mixture models, and hierarchical clustering, however, are beset by local minima, which are sometimes drastically suboptimal. Recently introduced convex relaxations of $k$-means and hierarchical clustering shrink cluster centroids toward one another and ensure a unique global minimizer. In this work we present two splitting methods for solving the convex clustering problem. The first is an instance of the alternating direction method of multipliers (ADMM); the second is an instance of the alternating minimization algorithm (AMA). In contrast to previously considered algorithms, our ADMM and AMA formulations provide simple and unified frameworks for solving the convex clustering problem under the previously studied norms and open the door to potentially novel norms. We demonstrate the performance of our algorithm on both simulated and real data examples. While the differences between the two algorithms appear to be minor on the surface, complexity analysis and numerical experiments show AMA to be significantly more efficient. version:2
arxiv-1306-4080 | Parallel Coordinate Descent Newton Method for Efficient $\ell_1$-Regularized Minimization | http://arxiv.org/abs/1306.4080 | id:1306.4080 author:Yatao Bian, Xiong Li, Yuncai Liu, Ming-Hsuan Yang category:cs.LG cs.NA  published:2013-06-18 summary:The recent years have witnessed advances in parallel algorithms for large scale optimization problems. Notwithstanding demonstrated success, existing algorithms that parallelize over features are usually limited by divergence issues under high parallelism or require data preprocessing to alleviate these problems. In this work, we propose a Parallel Coordinate Descent Newton algorithm using multidimensional approximate Newton steps (PCDN), where the off-diagonal elements of the Hessian are set to zero to enable parallelization. It randomly partitions the feature set into $b$ bundles/subsets with size of $P$, and sequentially processes each bundle by first computing the descent directions for each feature in parallel and then conducting $P$-dimensional line search to obtain the step size. We show that: (1) PCDN is guaranteed to converge globally despite increasing parallelism; (2) PCDN converges to the specified accuracy $\epsilon$ within the limited iteration number of $T_\epsilon$, and $T_\epsilon$ decreases with increasing parallelism (bundle size $P$). Using the implementation technique of maintaining intermediate quantities, we minimize the data transfer and synchronization cost of the $P$-dimensional line search. For concreteness, the proposed PCDN algorithm is applied to $\ell_1$-regularized logistic regression and $\ell_2$-loss SVM. Experimental evaluations on six benchmark datasets show that the proposed PCDN algorithm exploits parallelism well and outperforms the state-of-the-art methods in speed without losing accuracy. version:3
arxiv-1403-4473 | Sign Language Gibberish for syntactic parsing evaluation | http://arxiv.org/abs/1403.4473 | id:1403.4473 author:RÃ©mi Dubot, Christophe Collet category:cs.CL  published:2014-03-18 summary:Sign Language (SL) automatic processing slowly progresses bottom-up. The field has seen proposition to handle the video signal, to recognize and synthesize sublexical and lexical units. It starts to see the development of supra-lexical processing. But the recognition, at this level, lacks data. The syntax of SL appears very specific as it uses massively the multiplicity of articulators and its access to the spatial dimensions. Therefore new parsing techniques are developed. However these need to be evaluated. The shortage on real data restrains the corpus-based models to small sizes. We propose here a solution to produce data-sets for the evaluation of parsers on the specific properties of SL. The article first describes the general model used to generates dependency grammars and the phrase generation from these lasts. It then discusses the limits of approach. The solution shows to be of particular interest to evaluate the scalability of the techniques on big models. version:1
arxiv-1312-5542 | Word Emdeddings through Hellinger PCA | http://arxiv.org/abs/1312.5542 | id:1312.5542 author:RÃ©mi Lebret, Ronan Collobert category:cs.CL cs.LG  published:2013-12-19 summary:Word embeddings resulting from neural language models have been shown to be successful for a large variety of NLP tasks. However, such architecture might be difficult to train and time-consuming. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word co-occurence matrix. We compare those new word embeddings with some well-known embeddings on NER and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks. version:2
arxiv-1403-4378 | Spectral Clustering with Jensen-type kernels and their multi-point extensions | http://arxiv.org/abs/1403.4378 | id:1403.4378 author:Debarghya Ghoshdastidar, Ambedkar Dukkipati, Ajay P. Adsul, Aparna S. Vijayan category:cs.LG  published:2014-03-18 summary:Motivated by multi-distribution divergences, which originate in information theory, we propose a notion of `multi-point' kernels, and study their applications. We study a class of kernels based on Jensen type divergences and show that these can be extended to measure similarity among multiple points. We study tensor flattening methods and develop a multi-point (kernel) spectral clustering (MSC) method. We further emphasize on a special case of the proposed kernels, which is a multi-point extension of the linear (dot-product) kernel and show the existence of cubic time tensor flattening algorithm in this case. Finally, we illustrate the usefulness of our contributions using standard data sets and image segmentation tasks. version:1
arxiv-1403-4362 | Concept Based vs. Pseudo Relevance Feedback Performance Evaluation for Information Retrieval System | http://arxiv.org/abs/1403.4362 | id:1403.4362 author:Mohammed El Amine Abderrahim category:cs.IR cs.CL  published:2014-03-18 summary:This article evaluates the performance of two techniques for query reformulation in a system for information retrieval, namely, the concept based and the pseudo relevance feedback reformulation. The experiments performed on a corpus of Arabic text have allowed us to compare the contribution of these two reformulation techniques in improving the performance of an information retrieval system for Arabic texts. version:1
arxiv-1402-4566 | Transduction on Directed Graphs via Absorbing Random Walks | http://arxiv.org/abs/1402.4566 | id:1402.4566 author:Jaydeep De, Xiaowei Zhang, Li Cheng category:cs.CV cs.LG stat.ML  published:2014-02-19 summary:In this paper we consider the problem of graph-based transductive classification, and we are particularly interested in the directed graph scenario which is a natural form for many real world applications. Different from existing research efforts that either only deal with undirected graphs or circumvent directionality by means of symmetrization, we propose a novel random walk approach on directed graphs using absorbing Markov chains, which can be regarded as maximizing the accumulated expected number of visits from the unlabeled transient states. Our algorithm is simple, easy to implement, and works with large-scale graphs. In particular, it is capable of preserving the graph structure even when the input graph is sparse and changes over time, as well as retaining weak signals presented in the directed edges. We present its intimate connections to a number of existing methods, including graph kernels, graph Laplacian based methods, and interestingly, spanning forest of graphs. Its computational complexity and the generalization error are also studied. Empirically our algorithm is systematically evaluated on a wide range of applications, where it has shown to perform competitively comparing to a suite of state-of-the-art methods. version:2
arxiv-1312-7258 | Active Discovery of Network Roles for Predicting the Classes of Network Nodes | http://arxiv.org/abs/1312.7258 | id:1312.7258 author:Leto Peel category:cs.LG cs.SI stat.ML  published:2013-12-27 summary:Nodes in real world networks often have class labels, or underlying attributes, that are related to the way in which they connect to other nodes. Sometimes this relationship is simple, for instance nodes of the same class are may be more likely to be connected. In other cases, however, this is not true, and the way that nodes link in a network exhibits a different, more complex relationship to their attributes. Here, we consider networks in which we know how the nodes are connected, but we do not know the class labels of the nodes or how class labels relate to the network links. We wish to identify the best subset of nodes to label in order to learn this relationship between node attributes and network links. We can then use this discovered relationship to accurately predict the class labels of the rest of the network nodes. We present a model that identifies groups of nodes with similar link patterns, which we call network roles, using a generative blockmodel. The model then predicts labels by learning the mapping from network roles to class labels using a maximum margin classifier. We choose a subset of nodes to label according to an iterative margin-based active learning strategy. By integrating the discovery of network roles with the classifier optimisation, the active learning process can adapt the network roles to better represent the network for node classification. We demonstrate the model by exploring a selection of real world networks, including a marine food web and a network of English words. We show that, in contrast to other network classifiers, this model achieves good classification accuracy for a range of networks with different relationships between class labels and network links. version:2
arxiv-1309-7643 | Rotationally Invariant Image Representation for Viewing Direction Classification in Cryo-EM | http://arxiv.org/abs/1309.7643 | id:1309.7643 author:Zhizhen Zhao, Amit Singer category:q-bio.BM cs.CV  published:2013-09-29 summary:We introduce a new rotationally invariant viewing angle classification method for identifying, among a large number of Cryo-EM projection images, similar views without prior knowledge of the molecule. Our rotationally invariant features are based on the bispectrum. Each image is denoised and compressed using steerable principal component analysis (PCA) such that rotating an image is equivalent to phase shifting the expansion coefficients. Thus we are able to extend the theory of bispectrum of 1D periodic signals to 2D images. The randomized PCA algorithm is then used to efficiently reduce the dimensionality of the bispectrum coefficients, enabling fast computation of the similarity between any pair of images. The nearest neighbors provide an initial classification of similar viewing angles. In this way, rotational alignment is only performed for images with their nearest neighbors. The initial nearest neighbor classification and alignment are further improved by a new classification method called vector diffusion maps. Our pipeline for viewing angle classification and alignment is experimentally shown to be faster and more accurate than reference-free alignment with rotationally invariant K-means clustering, MSA/MRA 2D classification, and their modern approximations. version:4
arxiv-1403-4232 | Automatic Image Registration in Infrared-Visible Videos using Polygon Vertices | http://arxiv.org/abs/1403.4232 | id:1403.4232 author:Tanushri Chakravorty, Guillaume-Alexandre Bilodeau, Eric Granger category:cs.CV  published:2014-03-17 summary:In this paper, an automatic method is proposed to perform image registration in visible and infrared pair of video sequences for multiple targets. In multimodal image analysis like image fusion systems, color and IR sensors are placed close to each other and capture a same scene simultaneously, but the videos are not properly aligned by default because of different fields of view, image capturing information, working principle and other camera specifications. Because the scenes are usually not planar, alignment needs to be performed continuously by extracting relevant common information. In this paper, we approximate the shape of the targets by polygons and use affine transformation for aligning the two video sequences. After background subtraction, keypoints on the contour of the foreground blobs are detected using DCE (Discrete Curve Evolution)technique. These keypoints are then described by the local shape at each point of the obtained polygon. The keypoints are matched based on the convexity of polygon's vertices and Euclidean distance between them. Only good matches for each local shape polygon in a frame, are kept. To achieve a global affine transformation that maximises the overlapping of infrared and visible foreground pixels, the matched keypoints of each local shape polygon are stored temporally in a buffer for a few number of frames. The matrix is evaluated at each frame using the temporal buffer and the best matrix is selected, based on an overlapping ratio criterion. Our experimental results demonstrate that this method can provide highly accurate registered images and that we outperform a previous related method. version:1
arxiv-1403-4206 | A reversible infinite HMM using normalised random measures | http://arxiv.org/abs/1403.4206 | id:1403.4206 author:Konstantina Palla, David A. Knowles, Zoubin Ghahramani category:stat.ML  published:2014-03-17 summary:We present a nonparametric prior over reversible Markov chains. We use completely random measures, specifically gamma processes, to construct a countably infinite graph with weighted edges. By enforcing symmetry to make the edges undirected we define a prior over random walks on graphs that results in a reversible Markov chain. The resulting prior over infinite transition matrices is closely related to the hierarchical Dirichlet process but enforces reversibility. A reinforcement scheme has recently been proposed with similar properties, but the de Finetti measure is not well characterised. We take the alternative approach of explicitly constructing the mixing measure, which allows more straightforward and efficient inference at the cost of no longer having a closed form predictive distribution. We use our process to construct a reversible infinite HMM which we apply to two real datasets, one from epigenomics and one ion channel recording. version:1
arxiv-1403-4238 | Computer Vision Accelerators for Mobile Systems based on OpenCL GPGPU Co-Processing | http://arxiv.org/abs/1403.4238 | id:1403.4238 author:Guohui Wang, Yingen Xiong, Jay Yun, Joseph R. Cavallaro category:cs.DC cs.CV cs.MS  published:2014-03-17 summary:In this paper, we present an OpenCL-based heterogeneous implementation of a computer vision algorithm -- image inpainting-based object removal algorithm -- on mobile devices. To take advantage of the computation power of the mobile processor, the algorithm workflow is partitioned between the CPU and the GPU based on the profiling results on mobile devices, so that the computationally-intensive kernels are accelerated by the mobile GPGPU (general-purpose computing using graphics processing units). By exploring the implementation trade-offs and utilizing the proposed optimization strategies at different levels including algorithm optimization, parallelism optimization, and memory access optimization, we significantly speed up the algorithm with the CPU-GPU heterogeneous implementation, while preserving the quality of the output images. Experimental results show that heterogeneous computing based on GPGPU co-processing can significantly speed up the computer vision algorithms and makes them practical on real-world mobile devices. version:1
arxiv-1403-2395 | A-infinity Persistence | http://arxiv.org/abs/1403.2395 | id:1403.2395 author:Francisco BelchÃ­ GuillamÃ³n, Aniceto Murillo Mas category:math.AT cs.CG cs.CV  published:2014-03-10 summary:We introduce and study A-infinity persistence of a given homology filtration of topological spaces. This is a family, one for each n > 0, of homological invariants which provide information not readily available by the (persistent) Betti numbers of the given filtration. This may help to detect noise, not just in the simplicial structure of the filtration but in further geometrical properties in which the higher codiagonals of the A-infinity structure are translated. Based in the classification of zigzag modules, a characterization of the A-infinity persistence in terms of its associated barcode is given. version:3
arxiv-1403-4017 | Multi-task Feature Selection based Anomaly Detection | http://arxiv.org/abs/1403.4017 | id:1403.4017 author:Longqi Yang, Yibing Wang, Zhisong Pan, Guyu Hu category:stat.ML cs.LG  published:2014-03-17 summary:Network anomaly detection is still a vibrant research area. As the fast growth of network bandwidth and the tremendous traffic on the network, there arises an extremely challengeable question: How to efficiently and accurately detect the anomaly on multiple traffic? In multi-task learning, the traffic consisting of flows at different time periods is considered as a task. Multiple tasks at different time periods performed simultaneously to detect anomalies. In this paper, we apply the multi-task feature selection in network anomaly detection area which provides a powerful method to gather information from multiple traffic and detect anomalies on it simultaneously. In particular, the multi-task feature selection includes the well-known l1-norm based feature selection as a special case given only one task. Moreover, we show that the multi-task feature selection is more accurate by utilizing more information simultaneously than the l1-norm based method. At the evaluation stage, we preprocess the raw data trace from trans-Pacific backbone link between Japan and the United States, label with anomaly communities, and generate a 248-feature dataset. We show empirically that the multi-task feature selection outperforms independent l1-norm based feature selection on real traffic dataset. version:1
arxiv-1403-3964 | Image processing using miniKanren | http://arxiv.org/abs/1403.3964 | id:1403.3964 author:Hirotaka Niitsuma category:cs.CV cs.PL  published:2014-03-16 summary:An integral image is one of the most efficient optimization technique for image processing. However an integral image is only a special case of delayed stream or memoization. This research discusses generalizing concept of integral image optimization technique, and how to generate an integral image optimized program code automatically from abstracted image processing algorithm. In oder to abstruct algorithms, we forces to miniKanren. version:1
arxiv-1304-2024 | A General Framework for Interacting Bayes-Optimally with Self-Interested Agents using Arbitrary Parametric Model and Model Prior | http://arxiv.org/abs/1304.2024 | id:1304.2024 author:Trong Nghia Hoang, Kian Hsiang Low category:cs.LG cs.AI cs.MA stat.ML  published:2013-04-07 summary:Recent advances in Bayesian reinforcement learning (BRL) have shown that Bayes-optimality is theoretically achievable by modeling the environment's latent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. In self-interested multi-agent environments, the transition dynamics are mainly controlled by the other agent's stochastic behavior for which FDM's independence and modeling assumptions do not hold. As a result, FDM does not allow the other agent's behavior to be generalized across different states nor specified using prior domain knowledge. To overcome these practical limitations of FDM, we propose a generalization of BRL to integrate the general class of parametric models and model priors, thus allowing practitioners' domain knowledge to be exploited to produce a fine-grained and compact representation of the other agent's behavior. Empirical evaluation shows that our approach outperforms existing multi-agent reinforcement learning algorithms. version:3
arxiv-1303-6149 | Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression | http://arxiv.org/abs/1303.6149 | id:1303.6149 author:Francis Bach category:math.ST cs.LG math.OC stat.TH  published:2013-03-25 summary:In this paper, we consider supervised learning problems such as logistic regression and study the stochastic gradient method with averaging, in the usual stochastic approximation setting where observations are used only once. We show that after $N$ iterations, with a constant step-size proportional to $1/R^2 \sqrt{N}$ where $N$ is the number of observations and $R$ is the maximum norm of the observations, the convergence rate is always of order $O(1/\sqrt{N})$, and improves to $O(R^2 / \mu N)$ where $\mu$ is the lowest eigenvalue of the Hessian at the global optimum (when this eigenvalue is greater than $R^2/\sqrt{N}$). Since $\mu$ does not need to be known in advance, this shows that averaged stochastic gradient is adaptive to \emph{unknown local} strong convexity of the objective function. Our proof relies on the generalized self-concordance properties of the logistic loss and thus extends to all generalized linear models with uniformly bounded features. version:3
arxiv-1308-3541 | Knapsack Constrained Contextual Submodular List Prediction with Application to Multi-document Summarization | http://arxiv.org/abs/1308.3541 | id:1308.3541 author:Jiaji Zhou, Stephane Ross, Yisong Yue, Debadeepta Dey, J. Andrew Bagnell category:cs.LG  published:2013-08-16 summary:We study the problem of predicting a set or list of options under knapsack constraint. The quality of such lists are evaluated by a submodular reward function that measures both quality and diversity. Similar to DAgger (Ross et al., 2010), by a reduction to online learning, we show how to adapt two sequence prediction models to imitate greedy maximization under knapsack constraint problems: CONSEQOPT (Dey et al., 2012) and SCP (Ross et al., 2013). Experiments on extractive multi-document summarization show that our approach outperforms existing state-of-the-art methods. version:2
arxiv-1403-3829 | Geometric VLAD for Large Scale Image Search | http://arxiv.org/abs/1403.3829 | id:1403.3829 author:Zixuan Wang, Wei Di, Anurag Bhardwaj, Vignesh Jagadeesh, Robinson Piramuthu category:cs.CV  published:2014-03-15 summary:We present a novel compact image descriptor for large scale image search. Our proposed descriptor - Geometric VLAD (gVLAD) is an extension of VLAD (Vector of Locally Aggregated Descriptors) that incorporates weak geometry information into the VLAD framework. The proposed geometry cues are derived as a membership function over keypoint angles which contain evident and informative information but yet often discarded. A principled technique for learning the membership function by clustering angles is also presented. Further, to address the overhead of iterative codebook training over real-time datasets, a novel codebook adaptation strategy is outlined. Finally, we demonstrate the efficacy of proposed gVLAD based retrieval framework where we achieve more than 15% improvement in mAP over existing benchmarks. version:1
arxiv-1403-3780 | Automatic Classification of Human Epithelial Type 2 Cell Indirect Immunofluorescence Images using Cell Pyramid Matching | http://arxiv.org/abs/1403.3780 | id:1403.3780 author:Arnold Wiliem, Conrad Sanderson, Yongkang Wong, Peter Hobson, Rodney F. Minchin, Brian C. Lovell category:q-bio.CB cs.CV q-bio.QM  published:2014-03-15 summary:This paper describes a novel system for automatic classification of images obtained from Anti-Nuclear Antibody (ANA) pathology tests on Human Epithelial type 2 (HEp-2) cells using the Indirect Immunofluorescence (IIF) protocol. The IIF protocol on HEp-2 cells has been the hallmark method to identify the presence of ANAs, due to its high sensitivity and the large range of antigens that can be detected. However, it suffers from numerous shortcomings, such as being subjective as well as time and labour intensive. Computer Aided Diagnostic (CAD) systems have been developed to address these problems, which automatically classify a HEp-2 cell image into one of its known patterns (eg. speckled, homogeneous). Most of the existing CAD systems use handpicked features to represent a HEp-2 cell image, which may only work in limited scenarios. We propose a novel automatic cell image classification method termed Cell Pyramid Matching (CPM), which is comprised of regional histograms of visual words coupled with the Multiple Kernel Learning framework. We present a study of several variations of generating histograms and show the efficacy of the system on two publicly available datasets: the ICPR HEp-2 cell classification contest dataset and the SNPHEp-2 dataset. version:1
arxiv-1206-5580 | A Geometric Algorithm for Scalable Multiple Kernel Learning | http://arxiv.org/abs/1206.5580 | id:1206.5580 author:John Moeller, Parasaran Raman, Avishek Saha, Suresh Venkatasubramanian category:cs.LG stat.ML  published:2012-06-25 summary:We present a geometric formulation of the Multiple Kernel Learning (MKL) problem. To do so, we reinterpret the problem of learning kernel weights as searching for a kernel that maximizes the minimum (kernel) distance between two convex polytopes. This interpretation combined with novel structural insights from our geometric formulation allows us to reduce the MKL problem to a simple optimization routine that yields provable convergence as well as quality guarantees. As a result our method scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with a uniform unweighted combination of kernels. version:2
arxiv-1403-3707 | Learning the Latent State Space of Time-Varying Graphs | http://arxiv.org/abs/1403.3707 | id:1403.3707 author:Nesreen K. Ahmed, Christopher Cole, Jennifer Neville category:cs.SI cs.LG physics.soc-ph stat.ML  published:2014-03-14 summary:From social networks to Internet applications, a wide variety of electronic communication tools are producing streams of graph data; where the nodes represent users and the edges represent the contacts between them over time. This has led to an increased interest in mechanisms to model the dynamic structure of time-varying graphs. In this work, we develop a framework for learning the latent state space of a time-varying email graph. We show how the framework can be used to find subsequences that correspond to global real-time events in the Email graph (e.g. vacations, breaks, ...etc.). These events impact the underlying graph process to make its characteristics non-stationary. Within the framework, we compare two different representations of the temporal relationships; discrete vs. probabilistic. We use the two representations as inputs to a mixture model to learn the latent state transitions that correspond to important changes in the Email graph structure over time. version:1
arxiv-1403-3683 | Removal and Contraction Operations in $n$D Generalized Maps for Efficient Homology Computation | http://arxiv.org/abs/1403.3683 | id:1403.3683 author:Guillaume Damiand, Rocio Gonzalez-Diaz, Samuel Peltier category:cs.CV  published:2014-03-14 summary:In this paper, we show that contraction operations preserve the homology of $n$D generalized maps, under some conditions. Removal and contraction operations are used to propose an efficient algorithm that compute homology generators of $n$D generalized maps. Its principle consists in simplifying a generalized map as much as possible by using removal and contraction operations. We obtain a generalized map having the same homology than the initial one, while the number of cells decreased significantly. Keywords: $n$D Generalized Maps; Cellular Homology; Homology Generators; Contraction and Removal Operations. version:1
arxiv-1312-0790 | Test Set Selection using Active Information Acquisition for Predictive Models | http://arxiv.org/abs/1312.0790 | id:1312.0790 author:Sneha Chaudhari, Pankaj Dayama, Vinayaka Pandit, Indrajit Bhattacharya category:cs.AI cs.LG stat.ML  published:2013-12-03 summary:In this paper, we consider active information acquisition when the prediction model is meant to be applied on a targeted subset of the population. The goal is to label a pre-specified fraction of customers in the target or test set by iteratively querying for information from the non-target or training set. The number of queries is limited by an overall budget. Arising in the context of two rather disparate applications- banking and medical diagnosis, we pose the active information acquisition problem as a constrained optimization problem. We propose two greedy iterative algorithms for solving the above problem. We conduct experiments with synthetic data and compare results of our proposed algorithms with few other baseline approaches. The experimental results show that our proposed approaches perform better than the baseline schemes. version:2
arxiv-1403-3628 | Mixed-norm Regularization for Brain Decoding | http://arxiv.org/abs/1403.3628 | id:1403.3628 author:RÃ©mi Flamary, Nisrine Jrad, Ronald Phlypo, Marco Congedo, Alain Rakotomamonjy category:cs.LG  published:2014-03-14 summary:This work investigates the use of mixed-norm regularization for sensor selection in Event-Related Potential (ERP) based Brain-Computer Interfaces (BCI). The classification problem is cast as a discriminative optimization framework where sensor selection is induced through the use of mixed-norms. This framework is extended to the multi-task learning situation where several similar classification tasks related to different subjects are learned simultaneously. In this case, multi-task learning helps in leveraging data scarcity issue yielding to more robust classifiers. For this purpose, we have introduced a regularizer that induces both sensor selection and classifier similarities. The different regularization approaches are compared on three ERP datasets showing the interest of mixed-norm regularization in terms of sensor selection. The multi-task approaches are evaluated when a small number of learning examples are available yielding to significant performance improvements especially for subjects performing poorly. version:1
arxiv-1304-0869 | Patch-based Probabilistic Image Quality Assessment for Face Selection and Improved Video-based Face Recognition | http://arxiv.org/abs/1304.0869 | id:1304.0869 author:Yongkang Wong, Shaokang Chen, Sandra Mau, Conrad Sanderson, Brian C. Lovell category:cs.CV stat.AP  published:2013-04-03 summary:In video based face recognition, face images are typically captured over multiple frames in uncontrolled conditions, where head pose, illumination, shadowing, motion blur and focus change over the sequence. Additionally, inaccuracies in face localisation can also introduce scale and alignment variations. Using all face images, including images of poor quality, can actually degrade face recognition performance. While one solution it to use only the "best" subset of images, current face selection techniques are incapable of simultaneously handling all of the abovementioned issues. We propose an efficient patch-based face image quality assessment algorithm which quantifies the similarity of a face image to a probabilistic face model, representing an "ideal" face. Image characteristics that affect recognition are taken into account, including variations in geometric alignment (shift, rotation and scale), sharpness, head pose and cast shadows. Experiments on FERET and PIE datasets show that the proposed algorithm is able to identify images which are simultaneously the most frontal, aligned, sharp and well illuminated. Further experiments on a new video surveillance dataset (termed ChokePoint) show that the proposed method provides better face subsets than existing face selection techniques, leading to significant improvements in recognition accuracy. version:2
arxiv-1403-3602 | Spontaneous expression classification in the encrypted domain | http://arxiv.org/abs/1403.3602 | id:1403.3602 author:Segun Aina, Yogachandran Rahulamathavan, Raphael C. -W. Phan, Jonathon A. Chambers category:cs.CV cs.CR  published:2014-03-14 summary:To date, most facial expression analysis have been based on posed image databases and is carried out without being able to protect the identity of the subjects whose expressions are being recognised. In this paper, we propose and implement a system for classifying facial expressions of images in the encrypted domain based on a Paillier cryptosystem implementation of Fisher Linear Discriminant Analysis and k-nearest neighbour (FLDA + kNN). We present results of experiments carried out on a recently developed natural visible and infrared facial expression (NVIE) database of spontaneous images. To the best of our knowledge, this is the first system that will allow the recog-nition of encrypted spontaneous facial expressions by a remote server on behalf of a client. version:1
arxiv-1403-3100 | Engaging with Massive Online Courses | http://arxiv.org/abs/1403.3100 | id:1403.3100 author:Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8  published:2014-03-12 summary:The Web has enabled one of the most visible recent developments in education---the deployment of massive open online courses. With their global reach and often staggering enrollments, MOOCs have the potential to become a major new mechanism for learning. Despite this early promise, however, MOOCs are still relatively unexplored and poorly understood. In a MOOC, each student's complete interaction with the course materials takes place on the Web, thus providing a record of learner activity of unprecedented scale and resolution. In this work, we use such trace data to develop a conceptual framework for understanding how users currently engage with MOOCs. We develop a taxonomy of individual behavior, examine the different behavioral patterns of high- and low-achieving students, and investigate how forum participation relates to other parts of the course. We also report on a large-scale deployment of badges as incentives for engagement in a MOOC, including randomized experiments in which the presentation of badges was varied across sub-populations. We find that making badges more salient produced increases in forum engagement. version:2
arxiv-1403-3460 | Scalable and Robust Construction of Topical Hierarchies | http://arxiv.org/abs/1403.3460 | id:1403.3460 author:Chi Wang, Xueqing Liu, Yanglei Song, Jiawei Han category:cs.LG cs.CL cs.DB cs.IR  published:2014-03-13 summary:Automated generation of high-quality topical hierarchies for a text collection is a dream problem in knowledge engineering with many valuable applications. In this paper a scalable and robust algorithm is proposed for constructing a hierarchy of topics from a text collection. We divide and conquer the problem using a top-down recursive framework, based on a tensor orthogonal decomposition technique. We solve a critical challenge to perform scalable inference for our newly designed hierarchical topic model. Experiments with various real-world datasets illustrate its ability to generate robust, high-quality hierarchies efficiently. Our method reduces the time of construction by several orders of magnitude, and its robust feature renders it possible for users to interactively revise the hierarchy. version:1
arxiv-1403-3438 | Neighborhood Selection for Thresholding-based Subspace Clustering | http://arxiv.org/abs/1403.3438 | id:1403.3438 author:Reinhard Heckel, Eirikur Agustsson, Helmut BÃ¶lcskei category:stat.ML cs.IT math.IT  published:2014-03-13 summary:Subspace clustering refers to the problem of clustering high-dimensional data points into a union of low-dimensional linear subspaces, where the number of subspaces, their dimensions and orientations are all unknown. In this paper, we propose a variation of the recently introduced thresholding-based subspace clustering (TSC) algorithm, which applies spectral clustering to an adjacency matrix constructed from the nearest neighbors of each data point with respect to the spherical distance measure. The new element resides in an individual and data-driven choice of the number of nearest neighbors. Previous performance results for TSC, as well as for other subspace clustering algorithms based on spectral clustering, come in terms of an intermediate performance measure, which does not address the clustering error directly. Our main analytical contribution is a performance analysis of the modified TSC algorithm (as well as the original TSC algorithm) in terms of the clustering error directly. version:1
arxiv-1403-3369 | Controlling Recurrent Neural Networks by Conceptors | http://arxiv.org/abs/1403.3369 | id:1403.3369 author:Herbert Jaeger category:cs.NE I.2.6  published:2014-03-13 summary:The human brain is a dynamical system whose extremely complex sensor-driven neural processes give rise to conceptual, logical cognition. Understanding the interplay between nonlinear neural dynamics and concept-level cognition remains a major scientific challenge. Here I propose a mechanism of neurodynamical organization, called conceptors, which unites nonlinear dynamics with basic principles of conceptual abstraction and logic. It becomes possible to learn, store, abstract, focus, morph, generalize, de-noise and recognize a large number of dynamical patterns within a single neural system; novel patterns can be added without interfering with previously acquired ones; neural noise is automatically filtered. Conceptors help explaining how conceptual-level information processing emerges naturally and robustly in neural systems, and remove a number of roadblocks in the theory and applications of recurrent neural networks. version:1
arxiv-1403-3351 | Semantic Unification A sheaf theoretic approach to natural language | http://arxiv.org/abs/1403.3351 | id:1403.3351 author:Samson Abramsky, Mehrnoosh Sadrzadeh category:cs.CL  published:2014-03-13 summary:Language is contextual and sheaf theory provides a high level mathematical framework to model contextuality. We show how sheaf theory can model the contextual nature of natural language and how gluing can be used to provide a global semantics for a discourse by putting together the local logical semantics of each sentence within the discourse. We introduce a presheaf structure corresponding to a basic form of Discourse Representation Structures. Within this setting, we formulate a notion of semantic unification --- gluing meanings of parts of a discourse into a coherent whole --- as a form of sheaf-theoretic gluing. We illustrate this idea with a number of examples where it can used to represent resolutions of anaphoric references. We also discuss multivalued gluing, described using a distributions functor, which can be used to represent situations where multiple gluings are possible, and where we may need to rank them using quantitative measures. Dedicated to Jim Lambek on the occasion of his 90th birthday. version:1
arxiv-1403-3342 | The Potential Benefits of Filtering Versus Hyper-Parameter Optimization | http://arxiv.org/abs/1403.3342 | id:1403.3342 author:Michael R. Smith, Tony Martinez, Christophe Giraud-Carrier category:stat.ML cs.LG  published:2014-03-13 summary:The quality of an induced model by a learning algorithm is dependent on the quality of the training data and the hyper-parameters supplied to the learning algorithm. Prior work has shown that improving the quality of the training data (i.e., by removing low quality instances) or tuning the learning algorithm hyper-parameters can significantly improve the quality of an induced model. A comparison of the two methods is lacking though. In this paper, we estimate and compare the potential benefits of filtering and hyper-parameter optimization. Estimating the potential benefit gives an overly optimistic estimate but also empirically demonstrates an approximation of the maximum potential benefit of each method. We find that, while both significantly improve the induced model, improving the quality of the training set has a greater potential effect than hyper-parameter optimization. version:1
arxiv-1308-1981 | A Framework for the Analysis of Computational Imaging Systems with Practical Applications | http://arxiv.org/abs/1308.1981 | id:1308.1981 author:Kaushik Mitra, Oliver Cossairt, Ashok Veeraraghavan category:cs.CV I.4  published:2013-08-08 summary:Over the last decade, a number of Computational Imaging (CI) systems have been proposed for tasks such as motion deblurring, defocus deblurring and multispectral imaging. These techniques increase the amount of light reaching the sensor via multiplexing and then undo the deleterious effects of multiplexing by appropriate reconstruction algorithms. Given the widespread appeal and the considerable enthusiasm generated by these techniques, a detailed performance analysis of the benefits conferred by this approach is important. Unfortunately, a detailed analysis of CI has proven to be a challenging problem because performance depends equally on three components: (1) the optical multiplexing, (2) the noise characteristics of the sensor, and (3) the reconstruction algorithm. A few recent papers have performed analysis taking multiplexing and noise characteristics into account. However, analysis of CI systems under state-of-the-art reconstruction algorithms, most of which exploit signal prior models, has proven to be unwieldy. In this paper, we present a comprehensive analysis framework incorporating all three components. In order to perform this analysis, we model the signal priors using a Gaussian Mixture Model (GMM). A GMM prior confers two unique characteristics. Firstly, GMM satisfies the universal approximation property which says that any prior density function can be approximated to any fidelity using a GMM with appropriate number of mixtures. Secondly, a GMM prior lends itself to analytical tractability allowing us to derive simple expressions for the `minimum mean square error' (MMSE), which we use as a metric to characterize the performance of CI systems. We use our framework to analyze several previously proposed CI techniques, giving conclusive answer to the question: `How much performance gain is due to use of a signal prior and how much is due to multiplexing? version:3
arxiv-1403-3305 | Noise Facilitation in Associative Memories of Exponential Capacity | http://arxiv.org/abs/1403.3305 | id:1403.3305 author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney category:cs.NE  published:2014-03-13 summary:Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns. Although these designs correct external errors in recall, they assume neurons that compute noiselessly, in contrast to the highly variable neurons in brain regions thought to operate associatively such as hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as the internal noise level is below a specified threshold, the error probability in the recall phase can be made exceedingly small. More surprisingly, we show that internal noise actually improves the performance of the recall phase while the pattern retrieval capacity remains intact, i.e., the number of stored patterns does not reduce with noise (up to a threshold). Computational experiments lend additional support to our theoretical analysis. This work suggests a functional benefit to noisy neurons in biological neuronal networks. version:1
arxiv-1402-4802 | Ambiguity in language networks | http://arxiv.org/abs/1402.4802 | id:1402.4802 author:Ricard V. SolÃ©, LuÃ­s F. Seoane category:physics.soc-ph cs.CL q-bio.NC  published:2014-02-18 summary:Human language defines the most complex outcomes of evolution. The emergence of such an elaborated form of communication allowed humans to create extremely structured societies and manage symbols at different levels including, among others, semantics. All linguistic levels have to deal with an astronomic combinatorial potential that stems from the recursive nature of languages. This recursiveness is indeed a key defining trait. However, not all words are equally combined nor frequent. In breaking the symmetry between less and more often used and between less and more meaning-bearing units, universal scaling laws arise. Such laws, common to all human languages, appear on different stages from word inventories to networks of interacting words. Among these seemingly universal traits exhibited by language networks, ambiguity appears to be a specially relevant component. Ambiguity is avoided in most computational approaches to language processing, and yet it seems to be a crucial element of language architecture. Here we review the evidence both from language network architecture and from theoretical reasonings based on a least effort argument. Ambiguity is shown to play an essential role in providing a source of language efficiency, and is likely to be an inevitable byproduct of network growth. version:2
arxiv-1312-0512 | Sensing-Aware Kernel SVM | http://arxiv.org/abs/1312.0512 | id:1312.0512 author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama, W. Clem Karl category:cs.LG  published:2013-12-02 summary:We propose a novel approach for designing kernels for support vector machines (SVMs) when the class label is linked to the observation through a latent state and the likelihood function of the observation given the state (the sensing model) is available. We show that the Bayes-optimum decision boundary is a hyperplane under a mapping defined by the likelihood function. Combining this with the maximum margin principle yields kernels for SVMs that leverage knowledge of the sensing model in an optimal way. We derive the optimum kernel for the bag-of-words (BoWs) sensing model and demonstrate its superior performance over other kernels in document and image classification tasks. These results indicate that such optimum sensing-aware kernel SVMs can match the performance of rather sophisticated state-of-the-art approaches. version:2
arxiv-1403-3185 | Sentiment Analysis by Using Fuzzy Logic | http://arxiv.org/abs/1403.3185 | id:1403.3185 author:Md. Ansarul Haque category:cs.IR cs.CL  published:2014-03-13 summary:How could a product or service is reasonably evaluated by anyone in the shortest time? A million dollar question but it is having a simple answer: Sentiment analysis. Sentiment analysis is consumers review on products and services which helps both the producers and consumers (stakeholders) to take effective and efficient decision within a shortest period of time. Producers can have better knowledge of their products and services through the sentiment analysis (ex. positive and negative comments or consumers likes and dislikes) which will help them to know their products status (ex. product limitations or market status). Consumers can have better knowledge of their interested products and services through the sentiment analysis (ex. positive and negative comments or consumers likes and dislikes) which will help them to know their deserving products status (ex. product limitations or market status). For more specification of the sentiment values, fuzzy logic could be introduced. Therefore, sentiment analysis with the help of fuzzy logic (deals with reasoning and gives closer views to the exact sentiment values) will help the producers or consumers or any interested person for taking the effective decision according to their product or service interest. version:1
arxiv-1302-5554 | Self-similar prior and wavelet bases for hidden incompressible turbulent motion | http://arxiv.org/abs/1302.5554 | id:1302.5554 author:Patrick HÃ©as, FrÃ©dÃ©ric Lavancier, Souleymane Kadri-Harouna category:stat.AP cs.CV cs.NA physics.flu-dyn  published:2013-02-22 summary:This work is concerned with the ill-posed inverse problem of estimating turbulent flows from the observation of an image sequence. From a Bayesian perspective, a divergence-free isotropic fractional Brownian motion (fBm) is chosen as a prior model for instantaneous turbulent velocity fields. This self-similar prior characterizes accurately second-order statistics of velocity fields in incompressible isotropic turbulence. Nevertheless, the associated maximum a posteriori involves a fractional Laplacian operator which is delicate to implement in practice. To deal with this issue, we propose to decompose the divergent-free fBm on well-chosen wavelet bases. As a first alternative, we propose to design wavelets as whitening filters. We show that these filters are fractional Laplacian wavelets composed with the Leray projector. As a second alternative, we use a divergence-free wavelet basis, which takes implicitly into account the incompressibility constraint arising from physics. Although the latter decomposition involves correlated wavelet coefficients, we are able to handle this dependence in practice. Based on these two wavelet decompositions, we finally provide effective and efficient algorithms to approach the maximum a posteriori. An intensive numerical evaluation proves the relevance of the proposed wavelet-based self-similar priors. version:2
arxiv-1310-4794 | The Gaussian Radon Transform and Machine Learning | http://arxiv.org/abs/1310.4794 | id:1310.4794 author:Irina Holmes, Ambar Sengupta category:stat.ML math.FA  published:2013-10-17 summary:There has been growing recent interest in probabilistic interpretations of kernel-based methods as well as learning in Banach spaces. The absence of a useful Lebesgue measure on an infinite-dimensional reproducing kernel Hilbert space is a serious obstacle for such stochastic models. We propose an estimation model for the ridge regression problem within the framework of abstract Wiener spaces and show how the support vector machine solution to such problems can be interpreted in terms of the Gaussian Radon transform. version:2
arxiv-1104-5061 | On Combining Machine Learning with Decision Making | http://arxiv.org/abs/1104.5061 | id:1104.5061 author:Theja Tulabandhula, Cynthia Rudin category:math.OC cs.LG stat.ML  published:2011-04-27 summary:We present a new application and covering number bound for the framework of "Machine Learning with Operational Costs (MLOC)," which is an exploratory form of decision theory. The MLOC framework incorporates knowledge about how a predictive model will be used for a subsequent task, thus combining machine learning with the decision that is made afterwards. In this work, we use the MLOC framework to study a problem that has implications for power grid reliability and maintenance, called the Machine Learning and Traveling Repairman Problem ML&TRP. The goal of the ML&TRP is to determine a route for a "repair crew," which repairs nodes on a graph. The repair crew aims to minimize the cost of failures at the nodes, but as in many real situations, the failure probabilities are not known and must be estimated. The MLOC framework allows us to understand how this uncertainty influences the repair route. We also present new covering number generalization bounds for the MLOC framework. version:2
arxiv-1403-3118 | Parallel WiSARD object tracker: a ram-based tracking system | http://arxiv.org/abs/1403.3118 | id:1403.3118 author:Rodrigo da Silva Moreira, Nelson Francisco Favilla Ebecken category:cs.CV  published:2014-03-12 summary:This paper proposes the Parallel WiSARD Object Tracker (PWOT), a new object tracker based on the WiSARD weightless neural network that is robust against quantization errors. Object tracking in video is an important and challenging task in many applications. Difficulties can arise due to weather conditions, target trajectory and appearance, occlusions, lighting conditions and noise. Tracking is a high-level application and requires the object location frame by frame in real time. This paper proposes a fast hybrid image segmentation (threshold and edge detection) in YcbCr color model and a parallel RAM based discriminator that improves efficiency when quantization errors occur. The original WiSARD training algorithm was changed to allow the tracking. version:1
arxiv-1403-3115 | Memory Capacity of Neural Networks using a Circulant Weight Matrix | http://arxiv.org/abs/1403.3115 | id:1403.3115 author:Vamsi Sashank Kotagiri category:cs.NE  published:2014-03-12 summary:This paper presents results on the memory capacity of a generalized feedback neural network using a circulant matrix. Children are capable of learning soon after birth which indicates that the neural networks of the brain have prior learnt capacity that is a consequence of the regular structures in the brain's organization. Motivated by this idea, we consider the capacity of circulant matrices as weight matrices in a feedback network. version:1
arxiv-1403-3109 | Sparse Recovery with Linear and Nonlinear Observations: Dependent and Noisy Data | http://arxiv.org/abs/1403.3109 | id:1403.3109 author:Cem Aksoylar, Venkatesh Saligrama category:cs.IT cs.LG math.IT math.ST stat.TH  published:2014-03-12 summary:We formulate sparse support recovery as a salient set identification problem and use information-theoretic analyses to characterize the recovery performance and sample complexity. We consider a very general model where we are not restricted to linear models or specific distributions. We state non-asymptotic bounds on recovery probability and a tight mutual information formula for sample complexity. We evaluate our bounds for applications such as sparse linear regression and explicitly characterize effects of correlation or noisy features on recovery performance. We show improvements upon previous work and identify gaps between the performance of recovery algorithms and fundamental information. version:1
arxiv-1403-3057 | Evaluation of Image Segmentation and Filtering With ANN in the Papaya Leaf | http://arxiv.org/abs/1403.3057 | id:1403.3057 author:Maicon A. Sartin, Alexandre C. R. da Silva category:cs.NE cs.CV  published:2014-03-12 summary:Precision agriculture is area with lack of cheap technology. The refinement of the production system brings large advantages to the producer and the use of images makes the monitoring a more cheap methodology. Macronutrients monitoring can to determine the health and vulnerability of the plant in specific stages. In this paper is analyzed the method based on computational intelligence to work with image segmentation in the identification of symptoms of plant nutrient deficiency. Artificial neural networks are evaluated for image segmentation and filtering, several variations of parameters and insertion impulsive noise were evaluated too. Satisfactory results are achieved with artificial neural for segmentation same with high noise levels. version:1
arxiv-1403-3022 | Efficient Legendre moment computation for grey level images | http://arxiv.org/abs/1403.3022 | id:1403.3022 author:Guanyu Yang, Huazhong Shu, Christine Toumoulin, Guo-Niu Han, Limin M. Luo category:cs.CV math.NA  published:2014-03-12 summary:Legendre orthogonal moments have been widely used in the field of image analysis. Because their computation by a direct method is very time expensive, recent efforts have been devoted to the reduction of computational complexity. Nevertheless, the existing algorithms are mainly focused on binary images. We propose here a new fast method for computing the Legendre moments, which is not only suitable for binary images but also for grey levels. We first set up the recurrence formula of one-dimensional (1D) Legendre moments by using the recursive property of Legendre polynomials. As a result, the 1D Legendre moments of order p, Lp = Lp(0), can be expressed as a linear combination of Lp-1(1) and Lp-2(0). Based on this relationship, the 1D Legendre moments Lp(0) is thus obtained from the array of L1(a) and L0(a) where a is an integer number less than p. To further decrease the computation complexity, an algorithm, in which no multiplication is required, is used to compute these quantities. The method is then extended to the calculation of the two-dimensional Legendre moments Lpq. We show that the proposed method is more efficient than the direct method. version:1
arxiv-1403-3021 | Image reconstruction from limited range projections using orthogonal moments | http://arxiv.org/abs/1403.3021 | id:1403.3021 author:Huazhong Shu, Jian Zhou, Guo-Niu Han, Limin M. Luo, Jean-Louis Coatrieux category:cs.CV math.NA  published:2014-03-12 summary:A set of orthonormal polynomials is proposed for image reconstruction from projection data. The relationship between the projection moments and image moments is discussed in detail, and some interesting properties are demonstrated. Simulation results are provided to validate the method and to compare its performance with previous works. version:1
arxiv-1403-2980 | 3D Well-composed Polyhedral Complexes | http://arxiv.org/abs/1403.2980 | id:1403.2980 author:Rocio Gonzalez-Diaz, Maria-Jose Jimenez, Belen Medrano category:cs.CV  published:2014-03-12 summary:A binary three-dimensional (3D) image $I$ is well-composed if the boundary surface of its continuous analog is a 2D manifold. Since 3D images are not often well-composed, there are several voxel-based methods ("repairing" algorithms) for turning them into well-composed ones but these methods either do not guarantee the topological equivalence between the original image and its corresponding well-composed one or involve sub-sampling the whole image. In this paper, we present a method to locally "repair" the cubical complex $Q(I)$ (embedded in $\mathbb{R}^3$) associated to $I$ to obtain a polyhedral complex $P(I)$ homotopy equivalent to $Q(I)$ such that the boundary of every connected component of $P(I)$ is a 2D manifold. The reparation is performed via a new codification system for $P(I)$ under the form of a 3D grayscale image that allows an efficient access to cells and their faces. version:1
arxiv-1403-2950 | Cancer Prognosis Prediction Using Balanced Stratified Sampling | http://arxiv.org/abs/1403.2950 | id:1403.2950 author:J S Saleema, N Bhagawathi, S Monica, P Deepa Shenoy, K R Venugopal, L M Patnaik category:cs.LG 62D05 I.2.6; H.2.8  published:2014-03-12 summary:High accuracy in cancer prediction is important to improve the quality of the treatment and to improve the rate of survivability of patients. As the data volume is increasing rapidly in the healthcare research, the analytical challenge exists in double. The use of effective sampling technique in classification algorithms always yields good prediction accuracy. The SEER public use cancer database provides various prominent class labels for prognosis prediction. The main objective of this paper is to find the effect of sampling techniques in classifying the prognosis variable and propose an ideal sampling method based on the outcome of the experimentation. In the first phase of this work the traditional random sampling and stratified sampling techniques have been used. At the next level the balanced stratified sampling with variations as per the choice of the prognosis class labels have been tested. Much of the initial time has been focused on performing the pre_processing of the SEER data set. The classification model for experimentation has been built using the breast cancer, respiratory cancer and mixed cancer data sets with three traditional classifiers namely Decision Tree, Naive Bayes and K-Nearest Neighbor. The three prognosis factors survival, stage and metastasis have been used as class labels for experimental comparisons. The results shows a steady increase in the prediction accuracy of balanced stratified model as the sample size increases, but the traditional approach fluctuates before the optimum results. version:1
arxiv-1403-2906 | Uav Route Planning For Maximum Target Coverage | http://arxiv.org/abs/1403.2906 | id:1403.2906 author:Murat Karakaya category:cs.RO cs.NE  published:2014-03-12 summary:Utilization of Unmanned Aerial Vehicles (UAVs) in military and civil operations is getting popular. One of the challenges in effectively tasking these expensive vehicles is planning the flight routes to monitor the targets. In this work, we aim to develop an algorithm which produces routing plans for a limited number of UAVs to cover maximum number of targets considering their flight range. The proposed solution for this practical optimization problem is designed by modifying the Max-Min Ant System (MMAS) algorithm. To evaluate the success of the proposed method, an alternative approach, based on the Nearest Neighbour (NN) heuristic, has been developed as well. The results showed the success of the proposed MMAS method by increasing the number of covered targets compared to the solution based on the NN heuristic. version:1
arxiv-1403-2895 | Indoor 3D Video Monitoring Using Multiple Kinect Depth-Cameras | http://arxiv.org/abs/1403.2895 | id:1403.2895 author:M. MartÃ­nez-Zarzuela, M. Pedraza-Hueso, F. J. DÃ­az-Pernas, D. GonzÃ¡lez-Ortega, M. AntÃ³n-RodrÃ­guez category:cs.CV  published:2014-03-12 summary:This article describes the design and development of a system for remote indoor 3D monitoring using an undetermined number of Microsoft(R) Kinect sensors. In the proposed client-server system, the Kinect cameras can be connected to different computers, addressing this way the hardware limitation of one sensor per USB controller. The reason behind this limitation is the high bandwidth needed by the sensor, which becomes also an issue for the distributed system TCP/IP communications. Since traffic volume is too high, 3D data has to be compressed before it can be sent over the network. The solution consists in selfcoding the Kinect data into RGB images and then using a standard multimedia codec to compress color maps. Information from different sources is collected into a central client computer, where point clouds are transformed to reconstruct the scene in 3D. An algorithm is proposed to merge the skeletons detected locally by each Kinect conveniently, so that monitoring of people is robust to self and inter-user occlusions. Final skeletons are labeled and trajectories of every joint can be saved for event reconstruction or further analysis. version:1
arxiv-1402-0779 | UNLocBoX A matlab convex optimization toolbox using proximal splitting methods | http://arxiv.org/abs/1402.0779 | id:1402.0779 author:Nathanael Perraudin, David Shuman, Gilles Puy, Pierre Vandergheynst category:cs.LG stat.ML  published:2014-02-04 summary:Nowadays the trend to solve optimization problems is to use specific algorithms rather than very general ones. The UNLocBoX provides a general framework allowing the user to design his own algorithms. To do so, the framework try to stay as close from the mathematical problem as possible. More precisely, the UNLocBoX is a Matlab toolbox designed to solve convex optimization problem of the form $$ \min_{x \in \mathcal{C}} \sum_{n=1}^K f_n(x), $$ using proximal splitting techniques. It is mainly composed of solvers, proximal operators and demonstration files allowing the user to quickly implement a problem. version:2
arxiv-1403-2877 | A survey of dimensionality reduction techniques | http://arxiv.org/abs/1403.2877 | id:1403.2877 author:C. O. S. Sorzano, J. Vargas, A. Pascual Montano category:stat.ML cs.LG q-bio.QM  published:2014-03-12 summary:Experimental life sciences like biology or chemistry have seen in the recent decades an explosion of the data available from experiments. Laboratory instruments become more and more complex and report hundreds or thousands measurements for a single experiment and therefore the statistical methods face challenging tasks when dealing with such high dimensional data. However, much of the data is highly redundant and can be efficiently brought down to a much smaller number of variables without a significant loss of information. The mathematical procedures making possible this reduction are called dimensionality reduction techniques; they have widely been developed by fields like Statistics or Machine Learning, and are currently a hot research topic. In this review we categorize the plethora of dimension reduction techniques available and give the mathematical insight behind them. version:1
arxiv-1403-2871 | Shape-Based Plagiarism Detection for Flowchart Figures in Texts | http://arxiv.org/abs/1403.2871 | id:1403.2871 author:Senosy Arrish, Fadhil Noer Afif, Ahmadu Maidorawa, Naomie Salim category:cs.CV cs.IR  published:2014-03-12 summary:Plagiarism detection is well known phenomenon in the academic arena. Copying other people is considered as serious offence that needs to be checked. There are many plagiarism detection systems such as turn-it-in that has been developed to provide this checks. Most, if not all, discard the figures and charts before checking for plagiarism. Discarding the figures and charts results in look holes that people can take advantage. That means people can plagiarized figures and charts easily without the current plagiarism systems detecting it. There are very few papers which talks about flowcharts plagiarism detection. Therefore, there is a need to develop a system that will detect plagiarism in figures and charts. This paper presents a method for detecting flow chart figure plagiarism based on shape-based image processing and multimedia retrieval. The method managed to retrieve flowcharts with ranked similarity according to different matching sets. version:1
arxiv-1403-2842 | Application of Particle Swarm Optimization to Microwave Tapered Microstrip Lines | http://arxiv.org/abs/1403.2842 | id:1403.2842 author:Ezgi Deniz Ulker, Sadik Ulker category:cs.NE  published:2014-03-12 summary:Application of metaheuristic algorithms has been of continued interest in the field of electrical engineering because of their powerful features. In this work special design is done for a tapered transmission line used for matching an arbitrary real load to a 50{\Omega} line. The problem at hand is to match this arbitrary load to 50 {\Omega} line using three section tapered transmission line with impedances in decreasing order from the load. So the problem becomes optimizing an equation with three unknowns with various conditions. The optimized values are obtained using Particle Swarm Optimization. It can easily be shown that PSO is very strong in solving this kind of multiobjective optimization problems. version:1
arxiv-1403-2837 | HPS: a hierarchical Persian stemming method | http://arxiv.org/abs/1403.2837 | id:1403.2837 author:Ayshe Rashidi, Mina Zolfy Lighvan category:cs.CL  published:2014-03-12 summary:In this paper, a novel hierarchical Persian stemming approach based on the Part-Of-Speech of the word in a sentence is presented. The implemented stemmer includes hash tables and several deterministic finite automata in its different levels of hierarchy for removing the prefixes and suffixes of the words. We had two intentions in using hash tables in our method. The first one is that the DFA don't support some special words, so hash table can partly solve the addressed problem. the second goal is to speed up the implemented stemmer with omitting the time that deterministic finite automata need. Because of the hierarchical organization, this method is fast and flexible enough. Our experiments on test sets from Hamshahri collection and security news (istna.ir) show that our method has the average accuracy of 95.37% which is even improved in using the method on a test set with common topics. version:1
arxiv-1107-1564 | Polyceptron: A Polyhedral Learning Algorithm | http://arxiv.org/abs/1107.1564 | id:1107.1564 author:Naresh Manwani, P. S. Sastry category:cs.LG cs.NE  published:2011-07-08 summary:In this paper we propose a new algorithm for learning polyhedral classifiers which we call as Polyceptron. It is a Perception like algorithm which updates the parameters only when the current classifier misclassifies any training data. We give both batch and online version of Polyceptron algorithm. Finally we give experimental results to show the effectiveness of our approach. version:3
arxiv-1109-5909 | Markov properties for mixed graphs | http://arxiv.org/abs/1109.5909 | id:1109.5909 author:Kayvan Sadeghi, Steffen Lauritzen category:stat.OT math.ST stat.ML stat.TH  published:2011-09-27 summary:In this paper, we unify the Markov theory of a variety of different types of graphs used in graphical Markov models by introducing the class of loopless mixed graphs, and show that all independence models induced by $m$-separation on such graphs are compositional graphoids. We focus in particular on the subclass of ribbonless graphs which as special cases include undirected graphs, bidirected graphs, and directed acyclic graphs, as well as ancestral graphs and summary graphs. We define maximality of such graphs as well as a pairwise and a global Markov property. We prove that the global and pairwise Markov properties of a maximal ribbonless graph are equivalent for any independence model that is a compositional graphoid. version:5
arxiv-1403-1891 | Counterfactual Estimation and Optimization of Click Metrics for Search Engines | http://arxiv.org/abs/1403.1891 | id:1403.1891 author:Lihong Li, Shunbao Chen, Jim Kleban, Ankur Gupta category:cs.LG cs.AI stat.AP stat.ML G.3; H.3.3  published:2014-03-07 summary:Optimizing an interactive system against a predefined online metric is particularly challenging, when the metric is computed from user feedback such as clicks and payments. The key challenge is the counterfactual nature: in the case of Web search, any change to a component of the search engine may result in a different search result page for the same query, but we normally cannot infer reliably from search log how users would react to the new result page. Consequently, it appears impossible to accurately estimate online metrics that depend on user feedback, unless the new engine is run to serve users and compared with a baseline in an A/B test. This approach, while valid and successful, is unfortunately expensive and time-consuming. In this paper, we propose to address this problem using causal inference techniques, under the contextual-bandit framework. This approach effectively allows one to run (potentially infinitely) many A/B tests offline from search log, making it possible to estimate and optimize online metrics quickly and inexpensively. Focusing on an important component in a commercial search engine, we show how these ideas can be instantiated and applied, and obtain very promising results that suggest the wide applicability of these techniques. version:2
arxiv-1403-2802 | Learning Deep Face Representation | http://arxiv.org/abs/1403.2802 | id:1403.2802 author:Haoqiang Fan, Zhimin Cao, Yuning Jiang, Qi Yin, Chinchilla Doudou category:cs.CV cs.LG  published:2014-03-12 summary:Face representation is a crucial step of face recognition systems. An optimal face representation should be discriminative, robust, compact, and very easy-to-implement. While numerous hand-crafted and learning-based representations have been proposed, considerable room for improvement is still present. In this paper, we present a very easy-to-implement deep learning framework for face representation. Our method bases on a new structure of deep network (called Pyramid CNN). The proposed Pyramid CNN adopts a greedy-filter-and-down-sample operation, which enables the training procedure to be very fast and computation-efficient. In addition, the structure of Pyramid CNN can naturally incorporate feature sharing across multi-scale face representations, increasing the discriminative ability of resulting representation. Our basic network is capable of achieving high recognition accuracy ($85.8\%$ on LFW benchmark) with only 8 dimension representation. When extended to feature-sharing Pyramid CNN, our system achieves the state-of-the-art performance ($97.3\%$) on LFW benchmark. We also introduce a new benchmark of realistic face images on social network and validate our proposed representation has a good ability of generalization. version:1
arxiv-1403-2732 | The Bursty Dynamics of the Twitter Information Network | http://arxiv.org/abs/1403.2732 | id:1403.2732 author:Seth A. Myers, Jure Leskovec category:cs.SI physics.soc-ph stat.ML  published:2014-03-11 summary:In online social media systems users are not only posting, consuming, and resharing content, but also creating new and destroying existing connections in the underlying social network. While each of these two types of dynamics has individually been studied in the past, much less is known about the connection between the two. How does user information posting and seeking behavior interact with the evolution of the underlying social network structure? Here, we study ways in which network structure reacts to users posting and sharing content. We examine the complete dynamics of the Twitter information network, where users post and reshare information while they also create and destroy connections. We find that the dynamics of network structure can be characterized by steady rates of change, interrupted by sudden bursts. Information diffusion in the form of cascades of post re-sharing often creates such sudden bursts of new connections, which significantly change users' local network structure. These bursts transform users' networks of followers to become structurally more cohesive as well as more homogenous in terms of follower interests. We also explore the effect of the information content on the dynamics of the network and find evidence that the appearance of new topics and real-world events can lead to significant changes in edge creations and deletions. Lastly, we develop a model that quantifies the dynamics of the network and the occurrence of these bursts as a function of the information spreading through the network. The model can successfully predict which information diffusion events will lead to bursts in network dynamics. version:1
arxiv-1311-4158 | Unsupervised Learning of Invariant Representations in Hierarchical Architectures | http://arxiv.org/abs/1311.4158 | id:1311.4158 author:Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, Tomaso Poggio category:cs.CV cs.LG  published:2013-11-17 summary:The present phase of Machine Learning is characterized by supervised learning algorithms relying on large sets of labeled examples ($n \to \infty$). The next phase is likely to focus on algorithms capable of learning from very few labeled examples ($n \to 1$), like humans seem able to do. We propose an approach to this problem and describe the underlying theory, based on the unsupervised, automatic learning of a ``good'' representation for supervised learning, characterized by small sample complexity ($n$). We consider the case of visual object recognition though the theory applies to other domains. The starting point is the conjecture, proved in specific cases, that image representations which are invariant to translations, scaling and other transformations can considerably reduce the sample complexity of learning. We prove that an invariant and unique (discriminative) signature can be computed for each image patch, $I$, in terms of empirical distributions of the dot-products between $I$ and a set of templates stored during unsupervised learning. A module performing filtering and pooling, like the simple and complex cells described by Hubel and Wiesel, can compute such estimates. Hierarchical architectures consisting of this basic Hubel-Wiesel moduli inherit its properties of invariance, stability, and discriminability while capturing the compositional organization of the visual world in terms of wholes and parts. The theory extends existing deep learning convolutional architectures for image and speech recognition. It also suggests that the main computational goal of the ventral stream of visual cortex is to provide a hierarchical representation of new objects/images which is invariant to transformations, stable, and discriminative for recognition---and that this representation may be continuously learned in an unsupervised way during development and visual experience. version:5
arxiv-1403-2654 | Flying Insect Classification with Inexpensive Sensors | http://arxiv.org/abs/1403.2654 | id:1403.2654 author:Yanping Chen, Adena Why, Gustavo Batista, Agenor Mafra-Neto, Eamonn Keogh category:cs.LG cs.CE 68T00 I.2.6  published:2014-03-11 summary:The ability to use inexpensive, noninvasive sensors to accurately classify flying insects would have significant implications for entomological research, and allow for the development of many useful applications in vector control for both medical and agricultural entomology. Given this, the last sixty years have seen many research efforts on this task. To date, however, none of this research has had a lasting impact. In this work, we explain this lack of progress. We attribute the stagnation on this problem to several factors, including the use of acoustic sensing devices, the over-reliance on the single feature of wingbeat frequency, and the attempts to learn complex models with relatively little data. In contrast, we show that pseudo-acoustic optical sensors can produce vastly superior data, that we can exploit additional features, both intrinsic and extrinsic to the insect's flight behavior, and that a Bayesian classification approach allows us to efficiently learn classification models that are very robust to over-fitting. We demonstrate our findings with large scale experiments that dwarf all previous works combined, as measured by the number of insects and the number of species considered. version:1
arxiv-1403-2484 | Transfer Learning across Networks for Collective Classification | http://arxiv.org/abs/1403.2484 | id:1403.2484 author:Meng Fang, Jie Yin, Xingquan Zhu category:cs.LG cs.SI  published:2014-03-11 summary:This paper addresses the problem of transferring useful knowledge from a source network to predict node labels in a newly formed target network. While existing transfer learning research has primarily focused on vector-based data, in which the instances are assumed to be independent and identically distributed, how to effectively transfer knowledge across different information networks has not been well studied, mainly because networks may have their distinct node features and link relationships between nodes. In this paper, we propose a new transfer learning algorithm that attempts to transfer common latent structure features across the source and target networks. The proposed algorithm discovers these latent features by constructing label propagation matrices in the source and target networks, and mapping them into a shared latent feature space. The latent features capture common structure patterns shared by two networks, and serve as domain-independent features to be transferred between networks. Together with domain-dependent node features, we thereafter propose an iterative classification algorithm that leverages label correlations to predict node labels in the target network. Experiments on real-world networks demonstrate that our proposed algorithm can successfully achieve knowledge transfer between networks to help improve the accuracy of classifying nodes in the target network. version:1
arxiv-1403-2482 | Removing Mixture of Gaussian and Impulse Noise by Patch-Based Weighted Means | http://arxiv.org/abs/1403.2482 | id:1403.2482 author:Haijuan Hu, Bing Li, Quansheng Liu category:cs.CV  published:2014-03-11 summary:We first establish a law of large numbers and a convergence theorem in distribution to show the rate of convergence of the non-local means filter for removing Gaussian noise. We then introduce the notion of degree of similarity to measure the role of similarity for the non-local means filter. Based on the convergence theorems, we propose a patch-based weighted means filter for removing impulse noise and its mixture with Gaussian noise by combining the essential idea of the trilateral filter and that of the non-local means filter. Our experiments show that our filter is competitive compared to recently proposed methods. version:1
arxiv-1403-5473 | Image Fusion Techniques in Remote Sensing | http://arxiv.org/abs/1403.5473 | id:1403.5473 author:Reham Gharbia, Ahmad Taher Azar, Ali El Baz, Aboul Ella Hassanien category:cs.CV  published:2014-03-11 summary:Remote sensing image fusion is an effective way to use a large volume of data from multisensor images. Most earth satellites such as SPOT, Landsat 7, IKONOS and QuickBird provide both panchromatic (Pan) images at a higher spatial resolution and multispectral (MS) images at a lower spatial resolution and many remote sensing applications require both high spatial and high spectral resolutions, especially for GIS based applications. An effective image fusion technique can produce such remotely sensed images. Image fusion is the combination of two or more different images to form a new image by using a certain algorithm to obtain more and better information about an object or a study area than. The image fusion is performed at three different processing levels which are pixel level, feature level and decision level according to the stage at which the fusion takes place. There are many image fusion methods that can be used to produce high resolution multispectral images from a high resolution pan image and low resolution multispectral images. This paper explores the major remote sensing data fusion techniques at pixel level and reviews the concept, principals, limitations and advantages for each technique. This paper focused on traditional techniques like intensity hue-saturation- (HIS), Brovey, principal component analysis (PCA) and Wavelet. version:1
arxiv-1403-2433 | Generalised Mixability, Constant Regret, and Bayesian Updating | http://arxiv.org/abs/1403.2433 | id:1403.2433 author:Mark D. Reid, Rafael M. Frongillo, Robert C. Williamson category:cs.LG stat.ML  published:2014-03-10 summary:Mixability of a loss is known to characterise when constant regret bounds are achievable in games of prediction with expert advice through the use of Vovk's aggregating algorithm. We provide a new interpretation of mixability via convex analysis that highlights the role of the Kullback-Leibler divergence in its definition. This naturally generalises to what we call $\Phi$-mixability where the Bregman divergence $D_\Phi$ replaces the KL divergence. We prove that losses that are $\Phi$-mixable also enjoy constant regret bounds via a generalised aggregating algorithm that is similar to mirror descent. version:1
arxiv-1312-5940 | Generic Deep Networks with Wavelet Scattering | http://arxiv.org/abs/1312.5940 | id:1312.5940 author:Edouard Oyallon, StÃ©phane Mallat, Laurent Sifre category:cs.CV  published:2013-12-20 summary:We introduce a two-layer wavelet scattering network, for object classification. This scattering transform computes a spatial wavelet transform on the first layer and a new joint wavelet transform along spatial, angular and scale variables in the second layer. Numerical experiments demonstrate that this two layer convolution network, which involves no learning and no max pooling, performs efficiently on complex image data sets such as CalTech, with structural objects variability and clutter. It opens the possibility to simplify deep neural network learning by initializing the first layers with wavelet filters. version:3
arxiv-1403-2301 | Phase Retrieval using Lipschitz Continuous Maps | http://arxiv.org/abs/1403.2301 | id:1403.2301 author:Radu Balan, Dongmian Zou category:math.FA cs.IT math.IT stat.ML 15A29  65H10  90C26  published:2014-03-10 summary:In this note we prove that reconstruction from magnitudes of frame coefficients (the so called "phase retrieval problem") can be performed using Lipschitz continuous maps. Specifically we show that when the nonlinear analysis map $\alpha:{\mathcal H}\rightarrow\mathbb{R}^m$ is injective, with $(\alpha(x))_k= <x,f_k> ^2$, where $\{f_1,\ldots,f_m\}$ is a frame for the Hilbert space ${\mathcal H}$, then there exists a left inverse map $\omega:\mathbb{R}^m\rightarrow {\mathcal H}$ that is Lipschitz continuous. Additionally we obtain the Lipschitz constant of this inverse map in terms of the lower Lipschitz constant of $\alpha$. Surprisingly the increase in Lipschitz constant is independent of the space dimension or frame redundancy. version:1
arxiv-1403-2295 | Sublinear Models for Graphs | http://arxiv.org/abs/1403.2295 | id:1403.2295 author:Brijnesh J. Jain category:cs.LG cs.CV  published:2014-03-10 summary:This contribution extends linear models for feature vectors to sublinear models for graphs and analyzes their properties. The results are (i) a geometric interpretation of sublinear classifiers, (ii) a generic learning rule based on the principle of empirical risk minimization, (iii) a convergence theorem for the margin perceptron in the sublinearly separable case, and (iv) the VC-dimension of sublinear functions. Empirical results on graph data show that sublinear models on graphs have similar properties as linear models for feature vectors. version:1
arxiv-1312-4569 | Dropout improves Recurrent Neural Networks for Handwriting Recognition | http://arxiv.org/abs/1312.4569 | id:1312.4569 author:Vu Pham, ThÃ©odore Bluche, Christopher Kermorvant, JÃ©rÃ´me Louradour category:cs.CV cs.LG cs.NE  published:2013-11-05 summary:Recurrent neural networks (RNNs) with Long Short-Term memory cells currently hold the best known results in unconstrained handwriting recognition. We show that their performance can be greatly improved using dropout - a recently proposed regularization method for deep architectures. While previous works showed that dropout gave superior performance in the context of convolutional networks, it had never been applied to RNNs. In our approach, dropout is carefully used in the network so that it does not affect the recurrent connections, hence the power of RNNs in modeling sequence is preserved. Extensive experiments on a broad range of handwritten databases confirm the effectiveness of dropout on deep architectures even when the network mainly consists of recurrent and shared connections. version:2
arxiv-1204-2358 | Collaborative Representation based Classification for Face Recognition | http://arxiv.org/abs/1204.2358 | id:1204.2358 author:Lei Zhang, Meng Yang, Xiangchu Feng, Yi Ma, David Zhang category:cs.CV  published:2012-04-11 summary:By coding a query sample as a sparse linear combination of all training samples and then classifying it by evaluating which class leads to the minimal coding residual, sparse representation based classification (SRC) leads to interesting results for robust face recognition. It is widely believed that the l1- norm sparsity constraint on coding coefficients plays a key role in the success of SRC, while its use of all training samples to collaboratively represent the query sample is rather ignored. In this paper we discuss how SRC works, and show that the collaborative representation mechanism used in SRC is much more crucial to its success of face classification. The SRC is a special case of collaborative representation based classification (CRC), which has various instantiations by applying different norms to the coding residual and coding coefficient. More specifically, the l1 or l2 norm characterization of coding residual is related to the robustness of CRC to outlier facial pixels, while the l1 or l2 norm characterization of coding coefficient is related to the degree of discrimination of facial features. Extensive experiments were conducted to verify the face recognition accuracy and efficiency of CRC with different instantiations. version:2
arxiv-1403-2152 | Parsing using a grammar of word association vectors | http://arxiv.org/abs/1403.2152 | id:1403.2152 author:Robert John Freeman category:cs.CL cs.NE  published:2014-03-10 summary:This paper was was first drafted in 2001 as a formalization of the system described in U.S. patent U.S. 7,392,174. It describes a system for implementing a parser based on a kind of cross-product over vectors of contextually similar words. It is being published now in response to nascent interest in vector combination models of syntax and semantics. The method used aggressive substitution of contextually similar words and word groups to enable product vectors to stay in the same space as their operands and make entire sentences comparable syntactically, and potentially semantically. The vectors generated had sufficient representational strength to generate parse trees at least comparable with contemporary symbolic parsers. version:1
arxiv-1403-2150 | Constraint-based Causal Discovery from Multiple Interventions over Overlapping Variable Sets | http://arxiv.org/abs/1403.2150 | id:1403.2150 author:Sofia Triantafillou, Ioannis Tsamardinos category:stat.ML cs.AI  published:2014-03-10 summary:Scientific practice typically involves repeatedly studying a system, each time trying to unravel a different perspective. In each study, the scientist may take measurements under different experimental conditions (interventions, manipulations, perturbations) and measure different sets of quantities (variables). The result is a collection of heterogeneous data sets coming from different data distributions. In this work, we present algorithm COmbINE, which accepts a collection of data sets over overlapping variable sets under different experimental conditions; COmbINE then outputs a summary of all causal models indicating the invariant and variant structural characteristics of all models that simultaneously fit all of the input data sets. COmbINE converts estimated dependencies and independencies in the data into path constraints on the data-generating causal model and encodes them as a SAT instance. The algorithm is sound and complete in the sample limit. To account for conflicting constraints arising from statistical errors, we introduce a general method for sorting constraints in order of confidence, computed as a function of their corresponding p-values. In our empirical evaluation, COmbINE outperforms in terms of efficiency the only pre-existing similar algorithm; the latter additionally admits feedback cycles, but does not admit conflicting constraints which hinders the applicability on real data. As a proof-of-concept, COmbINE is employed to co-analyze 4 real, mass-cytometry data sets measuring phosphorylated protein concentrations of overlapping protein sets under 3 different interventions. version:1
arxiv-1403-2124 | Generating Music from Literature | http://arxiv.org/abs/1403.2124 | id:1403.2124 author:Hannah Davis, Saif M. Mohammad category:cs.CL  published:2014-03-10 summary:We present a system, TransProse, that automatically generates musical pieces from text. TransProse uses known relations between elements of music such as tempo and scale, and the emotions they evoke. Further, it uses a novel mechanism to determine sequences of notes that capture the emotional activity in the text. The work has applications in information visualization, in creating audio-visual e-books, and in developing music apps. version:1
arxiv-1312-4314 | Learning Factored Representations in a Deep Mixture of Experts | http://arxiv.org/abs/1312.4314 | id:1312.4314 author:David Eigen, Marc'Aurelio Ranzato, Ilya Sutskever category:cs.LG  published:2013-12-16 summary:Mixtures of Experts combine the outputs of several "expert" networks, each of which specializes in a different part of the input space. This is achieved by training a "gating" network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ("where") experts at the first layer, and class-specific ("what") experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations. version:3
arxiv-1311-2272 | From average case complexity to improper learning complexity | http://arxiv.org/abs/1311.2272 | id:1311.2272 author:Amit Daniely, Nati Linial, Shai Shalev-Shwartz category:cs.LG cs.CC  published:2013-11-10 summary:The basic problem in the PAC model of computational learning theory is to determine which hypothesis classes are efficiently learnable. There is presently a dearth of results showing hardness of learning problems. Moreover, the existing lower bounds fall short of the best known algorithms. The biggest challenge in proving complexity results is to establish hardness of {\em improper learning} (a.k.a. representation independent learning).The difficulty in proving lower bounds for improper learning is that the standard reductions from $\mathbf{NP}$-hard problems do not seem to apply in this context. There is essentially only one known approach to proving lower bounds on improper learning. It was initiated in (Kearns and Valiant 89) and relies on cryptographic assumptions. We introduce a new technique for proving hardness of improper learning, based on reductions from problems that are hard on average. We put forward a (fairly strong) generalization of Feige's assumption (Feige 02) about the complexity of refuting random constraint satisfaction problems. Combining this assumption with our new technique yields far reaching implications. In particular, 1. Learning $\mathrm{DNF}$'s is hard. 2. Agnostically learning halfspaces with a constant approximation ratio is hard. 3. Learning an intersection of $\omega(1)$ halfspaces is hard. version:2
arxiv-1309-2074 | Learning Transformations for Clustering and Classification | http://arxiv.org/abs/1309.2074 | id:1309.2074 author:Qiang Qiu, Guillermo Sapiro category:cs.CV cs.LG stat.ML  published:2013-09-09 summary:A low-rank transformation learning framework for subspace clustering and classification is here proposed. Many high-dimensional data, such as face images and motion sequences, approximately lie in a union of low-dimensional subspaces. The corresponding subspace clustering problem has been extensively studied in the literature to partition such high-dimensional data into clusters corresponding to their underlying low-dimensional subspaces. However, low-dimensional intrinsic structures are often violated for real-world observations, as they can be corrupted by errors or deviate from ideal models. We propose to address this by learning a linear transformation on subspaces using matrix rank, via its convex surrogate nuclear norm, as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same subspace, and, at the same time, forces a a maximally separated structure for data from different subspaces. In this way, we reduce variations within subspaces, and increase separation between subspaces for a more robust subspace clustering. This proposed learned robust subspace clustering framework significantly enhances the performance of existing subspace clustering methods. Basic theoretical results here presented help to further support the underlying framework. To exploit the low-rank structures of the transformed subspaces, we further introduce a fast subspace clustering technique, which efficiently combines robust PCA with sparse modeling. When class labels are present at the training stage, we show this low-rank transformation framework also significantly enhances classification performance. Extensive experiments using public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art methods for subspace clustering and classification. version:2
arxiv-1403-2073 | Generalized Canonical Correlation Analysis and Its Application to Blind Source Separation Based on a Dual-Linear Predictor Structure | http://arxiv.org/abs/1403.2073 | id:1403.2073 author:Wei Liu category:math.NA stat.ML  published:2014-03-09 summary:Blind source separation (BSS) is one of the most important and established research topics in signal processing and many algorithms have been proposed based on different statistical properties of the source signals. For second-order statistics (SOS) based methods, canonical correlation analysis (CCA) has been proved to be an effective solution to the problem. In this work, the CCA approach is generalized to accommodate the case with added white noise and it is then applied to the BSS problem for noisy mixtures. In this approach, the noise component is assumed to be spatially and temporally white, but the variance information of noise is not required. An adaptive blind source extraction algorithm is derived based on this idea and a further extension is proposed by employing a dual-linear predictor structure for blind source extraction (BSE). version:1
arxiv-1403-2031 | Texture Defect Detection in Gradient Space | http://arxiv.org/abs/1403.2031 | id:1403.2031 author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV  published:2014-03-09 summary:In this paper, we propose a machine vision algorithm for automatically detecting defects in patterned textures with the help of gradient space and its energy. Experiments on real fabric images with defects show that the proposed method can be used for automatic detection of fabric defects in textile industries. version:1
arxiv-1311-2236 | Fast Distribution To Real Regression | http://arxiv.org/abs/1311.2236 | id:1311.2236 author:Junier B. Oliva, Willie Neiswanger, Barnabas Poczos, Jeff Schneider, Eric Xing category:stat.ML cs.LG math.ST stat.TH  published:2013-11-10 summary:We study the problem of distribution to real-value regression, where one aims to regress a mapping $f$ that takes in a distribution input covariate $P\in \mathcal{I}$ (for a non-parametric family of distributions $\mathcal{I}$) and outputs a real-valued response $Y=f(P) + \epsilon$. This setting was recently studied, and a "Kernel-Kernel" estimator was introduced and shown to have a polynomial rate of convergence. However, evaluating a new prediction with the Kernel-Kernel estimator scales as $\Omega(N)$. This causes the difficult situation where a large amount of data may be necessary for a low estimation risk, but the computation cost of estimation becomes infeasible when the data-set is too large. To this end, we propose the Double-Basis estimator, which looks to alleviate this big data problem in two ways: first, the Double-Basis estimator is shown to have a computation complexity that is independent of the number of of instances $N$ when evaluating new predictions after training; secondly, the Double-Basis estimator is shown to have a fast rate of convergence for a general class of mappings $f\in\mathcal{F}$. version:2
arxiv-1311-2234 | FuSSO: Functional Shrinkage and Selection Operator | http://arxiv.org/abs/1311.2234 | id:1311.2234 author:Junier B. Oliva, Barnabas Poczos, Timothy Verstynen, Aarti Singh, Jeff Schneider, Fang-Cheng Yeh, Wen-Yih Tseng category:stat.ML cs.LG math.ST stat.TH  published:2013-11-10 summary:We present the FuSSO, a functional analogue to the LASSO, that efficiently finds a sparse set of functional input covariates to regress a real-valued response against. The FuSSO does so in a semi-parametric fashion, making no parametric assumptions about the nature of input functional covariates and assuming a linear form to the mapping of functional covariates to the response. We provide a statistical backing for use of the FuSSO via proof of asymptotic sparsistency under various conditions. Furthermore, we observe good results on both synthetic and real-world data. version:2
arxiv-1403-2004 | Natural Language Feature Selection via Cooccurrence | http://arxiv.org/abs/1403.2004 | id:1403.2004 author:Michael Stewart category:cs.CL  published:2014-03-08 summary:Specificity is important for extracting collocations, keyphrases, multi-word and index terms [Newman et al. 2012]. It is also useful for tagging, ontology construction [Ryu and Choi 2006], and automatic summarization of documents [Louis and Nenkova 2011, Chali and Hassan 2012]. Term frequency and inverse-document frequency (TF-IDF) are typically used to do this, but fail to take advantage of the semantic relationships between terms [Church and Gale 1995]. The result is that general idiomatic terms are mistaken for specific terms. We demonstrate use of relational data for estimation of term specificity. The specificity of a term can be learned from its distribution of relations with other terms. This technique is useful for identifying relevant words or terms for other natural language processing tasks. version:1
arxiv-1311-3257 | Compressive Nonparametric Graphical Model Selection For Time Series | http://arxiv.org/abs/1311.3257 | id:1311.3257 author:Alexander Jung, Reinhard Heckel, Helmut BÃ¶lcskei, Franz Hlawatsch category:stat.ML  published:2013-11-13 summary:We propose a method for inferring the conditional indepen- dence graph (CIG) of a high-dimensional discrete-time Gaus- sian vector random process from finite-length observations. Our approach does not rely on a parametric model (such as, e.g., an autoregressive model) for the vector random process; rather, it only assumes certain spectral smoothness proper- ties. The proposed inference scheme is compressive in that it works for sample sizes that are (much) smaller than the number of scalar process components. We provide analytical conditions for our method to correctly identify the CIG with high probability. version:2
arxiv-1403-1974 | Designing an FPGA Synthesizable Computer Vision Algorithm to Detect the Greening of Potatoes | http://arxiv.org/abs/1403.1974 | id:1403.1974 author:Jaspinder Pal Singh category:cs.CV  published:2014-03-08 summary:Potato quality control has improved in the last years thanks to automation techniques like machine vision, mainly making the classification task between different quality degrees faster, safer and less subjective. In our study we are going to design a computer vision algorithm for grading of potatoes according to the greening of the surface color of potato. The ratio of green pixels to the total number of pixels of the potato surface is found. The higher the ratio the worse is the potato. First the image is converted into serial data and then processing is done in RGB colour space. Green part of the potato is also shown by de-serializing the output. The same algorithm is then synthesized on FPGA and the result shows thousand times speed improvement in case of hardware synthesis. version:1
arxiv-1403-1949 | Combination of PCA with SMOTE Resampling to Boost the Prediction Rate in Lung Cancer Dataset | http://arxiv.org/abs/1403.1949 | id:1403.1949 author:Mehdi Naseriparsa, Mohammad Mansour Riahi Kashani category:cs.LG cs.CE  published:2014-03-08 summary:Classification algorithms are unable to make reliable models on the datasets with huge sizes. These datasets contain many irrelevant and redundant features that mislead the classifiers. Furthermore, many huge datasets have imbalanced class distribution which leads to bias over majority class in the classification process. In this paper combination of unsupervised dimensionality reduction methods with resampling is proposed and the results are tested on Lung-Cancer dataset. In the first step PCA is applied on Lung-Cancer dataset to compact the dataset and eliminate irrelevant features and in the second step SMOTE resampling is carried out to balance the class distribution and increase the variety of sample domain. Finally, Naive Bayes classifier is applied on the resulting dataset and the results are compared and evaluation metrics are calculated. The experiments show the effectiveness of the proposed method across four evaluation metrics: Overall accuracy, False Positive Rate, Precision, Recall. version:1
arxiv-1403-2372 | A Hybrid Feature Selection Method to Improve Performance of a Group of Classification Algorithms | http://arxiv.org/abs/1403.2372 | id:1403.2372 author:Mehdi Naseriparsa, Amir-Masoud Bidgoli, Touraj Varaee category:cs.LG  published:2014-03-08 summary:In this paper a hybrid feature selection method is proposed which takes advantages of wrapper subset evaluation with a lower cost and improves the performance of a group of classifiers. The method uses combination of sample domain filtering and resampling to refine the sample domain and two feature subset evaluation methods to select reliable features. This method utilizes both feature space and sample domain in two phases. The first phase filters and resamples the sample domain and the second phase adopts a hybrid procedure by information gain, wrapper subset evaluation and genetic search to find the optimal feature space. Experiments carried out on different types of datasets from UCI Repository of Machine Learning databases and the results show a rise in the average performance of five classifiers (Naive Bayes, Logistic, Multilayer Perceptron, Best First Decision Tree and JRIP) simultaneously and the classification error for these classifiers decreases considerably. The experiments also show that this method outperforms other feature selection methods with a lower cost. version:1
arxiv-1403-1946 | Improving Performance of a Group of Classification Algorithms Using Resampling and Feature Selection | http://arxiv.org/abs/1403.1946 | id:1403.1946 author:Mehdi Naseriparsa, Amir-masoud Bidgoli, Touraj Varaee category:cs.LG  published:2014-03-08 summary:In recent years the importance of finding a meaningful pattern from huge datasets has become more challenging. Data miners try to adopt innovative methods to face this problem by applying feature selection methods. In this paper we propose a new hybrid method in which we use a combination of resampling, filtering the sample domain and wrapper subset evaluation method with genetic search to reduce dimensions of Lung-Cancer dataset that we received from UCI Repository of Machine Learning databases. Finally, we apply some well- known classification algorithms (Na\"ive Bayes, Logistic, Multilayer Perceptron, Best First Decision Tree and JRIP) to the resulting dataset and compare the results and prediction rates before and after the application of our feature selection method on that dataset. The results show a substantial progress in the average performance of five classification algorithms simultaneously and the classification error for these classifiers decreases considerably. The experiments also show that this method outperforms other feature selection methods with a lower cost. version:1
arxiv-1403-1944 | Multi-label ensemble based on variable pairwise constraint projection | http://arxiv.org/abs/1403.1944 | id:1403.1944 author:Ping Li, Hong Li, Min Wu category:cs.LG cs.CV stat.ML I.2.6  published:2014-03-08 summary:Multi-label classification has attracted an increasing amount of attention in recent years. To this end, many algorithms have been developed to classify multi-label data in an effective manner. However, they usually do not consider the pairwise relations indicated by sample labels, which actually play important roles in multi-label classification. Inspired by this, we naturally extend the traditional pairwise constraints to the multi-label scenario via a flexible thresholding scheme. Moreover, to improve the generalization ability of the classifier, we adopt a boosting-like strategy to construct a multi-label ensemble from a group of base classifiers. To achieve these goals, this paper presents a novel multi-label classification framework named Variable Pairwise Constraint projection for Multi-label Ensemble (VPCME). Specifically, we take advantage of the variable pairwise constraint projection to learn a lower-dimensional data representation, which preserves the correlations between samples and labels. Thereafter, the base classifiers are trained in the new data space. For the boosting-like strategy, we employ both the variable pairwise constraints and the bootstrap steps to diversify the base classifiers. Empirical studies have shown the superiority of the proposed method in comparison with other approaches. version:1
arxiv-1403-1902 | Quality-based Multimodal Classification Using Tree-Structured Sparsity | http://arxiv.org/abs/1403.1902 | id:1403.1902 author:Soheil Bahrampour, Asok Ray, Nasser M. Nasrabadi, Kenneth W. Jenkins category:cs.CV  published:2014-03-08 summary:Recent studies have demonstrated advantages of information fusion based on sparsity models for multimodal classification. Among several sparsity models, tree-structured sparsity provides a flexible framework for extraction of cross-correlated information from different sources and for enforcing group sparsity at multiple granularities. However, the existing algorithm only solves an approximated version of the cost functional and the resulting solution is not necessarily sparse at group levels. This paper reformulates the tree-structured sparse model for multimodal classification task. An accelerated proximal algorithm is proposed to solve the optimization problem, which is an efficient tool for feature-level fusion among either homogeneous or heterogeneous sources of information. In addition, a (fuzzy-set-theoretic) possibilistic scheme is proposed to weight the available modalities, based on their respective reliability, in a joint optimization problem for finding the sparsity codes. This approach provides a general framework for quality-based fusion that offers added robustness to several sparsity-based multimodal classification algorithms. To demonstrate their efficacy, the proposed methods are evaluated on three different applications - multiview face recognition, multimodal face recognition, and target classification. version:1
arxiv-1401-0509 | Zero-Shot Learning for Semantic Utterance Classification | http://arxiv.org/abs/1401.0509 | id:1401.0509 author:Yann N. Dauphin, Gokhan Tur, Dilek Hakkani-Tur, Larry Heck category:cs.CL cs.LG  published:2013-12-20 summary:We propose a novel zero-shot learning method for semantic utterance classification (SUC). It learns a classifier $f: X \to Y$ for problems where none of the semantic categories $Y$ are present in the training set. The framework uncovers the link between categories and utterances using a semantic space. We show that this semantic space can be learned by deep neural networks trained on large amounts of search engine query log data. More precisely, we propose a novel method that can learn discriminative semantic features without supervision. It uses the zero-shot learning framework to guide the learning of the semantic features. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by (Tur, 2012). Furthermore, we achieve state-of-the-art results by combining the semantic features with a supervised method. version:3
arxiv-1403-1893 | Becoming More Robust to Label Noise with Classifier Diversity | http://arxiv.org/abs/1403.1893 | id:1403.1893 author:Michael R. Smith, Tony Martinez category:stat.ML cs.AI cs.LG  published:2014-03-07 summary:It is widely known in the machine learning community that class noise can be (and often is) detrimental to inducing a model of the data. Many current approaches use a single, often biased, measurement to determine if an instance is noisy. A biased measure may work well on certain data sets, but it can also be less effective on a broader set of data sets. In this paper, we present noise identification using classifier diversity (NICD) -- a method for deriving a less biased noise measurement and integrating it into the learning process. To lessen the bias of the noise measure, NICD selects a diverse set of classifiers (based on their predictions of novel instances) to determine which instances are noisy. We examine NICD as a technique for filtering, instance weighting, and selecting the base classifiers of a voting ensemble. We compare NICD with several other noise handling techniques that do not consider classifier diversity on a set of 54 data sets and 5 learning algorithms. NICD significantly increases the classification accuracy over the other considered approaches and is effective across a broad set of data sets and learning algorithms. version:1
arxiv-1403-1863 | Statistical Structure Learning, Towards a Robust Smart Grid | http://arxiv.org/abs/1403.1863 | id:1403.1863 author:Hanie Sedghi, Edmond Jonckheere category:cs.LG cs.SY  published:2014-03-07 summary:Robust control and maintenance of the grid relies on accurate data. Both PMUs and state estimators are prone to false data injection attacks. Thus, it is crucial to have a mechanism for fast and accurate detection of an agent maliciously tampering with the data---for both preventing attacks that may lead to blackouts, and for routine monitoring and control tasks of current and future grids. We propose a decentralized false data injection detection scheme based on Markov graph of the bus phase angles. We utilize the Conditional Covariance Test (CCT) to learn the structure of the grid. Using the DC power flow model, we show that under normal circumstances, and because of walk-summability of the grid graph, the Markov graph of the voltage angles can be determined by the power grid graph. Therefore, a discrepancy between calculated Markov graph and learned structure should trigger the alarm. Local grid topology is available online from the protection system and we exploit it to check for mismatch. Should a mismatch be detected, we use correlation anomaly score to detect the set of attacked nodes. Our method can detect the most recent stealthy deception attack on the power grid that assumes knowledge of bus-branch model of the system and is capable of deceiving the state estimator, damaging power network observatory, control, monitoring, demand response and pricing schemes. Specifically, under the stealthy deception attack, the Markov graph of phase angles changes. In addition to detect a state of attack, our method can detect the set of attacked nodes. To the best of our knowledge, our remedy is the first to comprehensively detect this sophisticated attack and it does not need additional hardware. Moreover, our detection scheme is successful no matter the size of the attacked subset. Simulation of various power networks confirms our claims. version:1
arxiv-1402-2092 | Near-Optimally Teaching the Crowd to Classify | http://arxiv.org/abs/1402.2092 | id:1402.2092 author:Adish Singla, Ilija Bogunovic, GÃ¡bor BartÃ³k, Amin Karbasi, Andreas Krause category:cs.LG  published:2014-02-10 summary:How should we present training examples to learners to teach them classification rules? This is a natural problem when training workers for crowdsourcing labeling tasks, and is also motivated by challenges in data-driven online education. We propose a natural stochastic model of the learners, modeling them as randomly switching among hypotheses based on observed feedback. We then develop STRICT, an efficient algorithm for selecting examples to teach to workers. Our solution greedily maximizes a submodular surrogate objective function in order to select examples to show to the learners. We prove that our strategy is competitive with the optimal teaching policy. Moreover, for the special case of linear separators, we prove that an exponential reduction in error probability can be achieved. Our experiments on simulated workers as well as three real image annotation tasks on Amazon Mechanical Turk show the effectiveness of our teaching algorithm. version:4
arxiv-1012-3697 | Analysis of Agglomerative Clustering | http://arxiv.org/abs/1012.3697 | id:1012.3697 author:Marcel R. Ackermann, Johannes BlÃ¶mer, Daniel Kuntze, Christian Sohler category:cs.DS cs.CG cs.LG F.2.2; H.3.3; I.5.3  published:2010-12-16 summary:The diameter $k$-clustering problem is the problem of partitioning a finite subset of $\mathbb{R}^d$ into $k$ subsets called clusters such that the maximum diameter of the clusters is minimized. One early clustering algorithm that computes a hierarchy of approximate solutions to this problem (for all values of $k$) is the agglomerative clustering algorithm with the complete linkage strategy. For decades, this algorithm has been widely used by practitioners. However, it is not well studied theoretically. In this paper, we analyze the agglomerative complete linkage clustering algorithm. Assuming that the dimension $d$ is a constant, we show that for any $k$ the solution computed by this algorithm is an $O(\log k)$-approximation to the diameter $k$-clustering problem. Our analysis does not only hold for the Euclidean distance but for any metric that is based on a norm. Furthermore, we analyze the closely related $k$-center and discrete $k$-center problem. For the corresponding agglomerative algorithms, we deduce an approximation factor of $O(\log k)$ as well. version:4
arxiv-1307-8371 | The Power of Localization for Efficiently Learning Linear Separators with Noise | http://arxiv.org/abs/1307.8371 | id:1307.8371 author:Pranjal Awasthi, Maria Florina Balcan, Philip M. Long category:cs.LG cs.CC cs.DS stat.ML  published:2013-07-31 summary:We introduce a new approach for designing computationally efficient learning algorithms that are tolerant to noise. We demonstrate the effectiveness of our approach by designing algorithms with improved noise tolerance guarantees for learning linear separators. We consider the malicious noise model of Valiant and the adversarial label noise model of Kearns, Schapire, and Sellie. For malicious noise, where the adversary can corrupt an $\eta$ of fraction both the label part and the feature part, we provide a polynomial-time algorithm for learning linear separators in $\Re^d$ under the uniform distribution with near information-theoretic optimal noise tolerance of $\eta = \Omega(\epsilon)$. We also get similar improvements for the adversarial label noise model. We obtain similar results for more general classes of distributions including isotropic log-concave distributions. In addition, our algorithms achieve a label complexity whose dependence on the error parameter $\epsilon$ is {\em exponentially better} than that of any passive algorithm. This provides the first polynomial-time active learning algorithm for learning linear separators in the presence of adversarial label noise, as well as the first analysis of active learning under the malicious noise model. version:7
arxiv-1402-2016 | Leveraging Long-Term Predictions and Online-Learning in Agent-based Multiple Person Tracking | http://arxiv.org/abs/1402.2016 | id:1402.2016 author:Wenxi Liu, Antoni B. Chan, Rynson W. H. Lau, Dinesh Manocha category:cs.CV  published:2014-02-10 summary:We present a multiple-person tracking algorithm, based on combining particle filters and RVO, an agent-based crowd model that infers collision-free velocities so as to predict pedestrian's motion. In addition to position and velocity, our tracking algorithm can estimate the internal goals (desired destination or desired velocity) of the tracked pedestrian in an online manner, thus removing the need to specify this information beforehand. Furthermore, we leverage the longer-term predictions of RVO by deriving a higher-order particle filter, which aggregates multiple predictions from different prior time steps. This yields a tracker that can recover from short-term occlusions and spurious noise in the appearance model. Experimental results show that our tracking algorithm is suitable for predicting pedestrians' behaviors online without needing scene priors or hand-annotated goal information, and improves tracking in real-world crowded scenes under low frame rates. version:2
arxiv-1403-1773 | Finding Eyewitness Tweets During Crises | http://arxiv.org/abs/1403.1773 | id:1403.1773 author:Fred Morstatter, Nichola Lubold, Heather Pon-Barry, JÃ¼rgen Pfeffer, Huan Liu category:cs.CL cs.CY  published:2014-03-07 summary:Disaster response agencies have started to incorporate social media as a source of fast-breaking information to understand the needs of people affected by the many crises that occur around the world. These agencies look for tweets from within the region affected by the crisis to get the latest updates of the status of the affected region. However only 1% of all tweets are geotagged with explicit location information. First responders lose valuable information because they cannot assess the origin of many of the tweets they collect. In this work we seek to identify non-geotagged tweets that originate from within the crisis region. Towards this, we address three questions: (1) is there a difference between the language of tweets originating within a crisis region and tweets originating outside the region, (2) what are the linguistic patterns that can be used to differentiate within-region and outside-region tweets, and (3) for non-geotagged tweets, can we automatically identify those originating within the crisis region in real-time? version:1
arxiv-1403-1735 | Ant Colony based Feature Selection Heuristics for Retinal Vessel Segmentation | http://arxiv.org/abs/1403.1735 | id:1403.1735 author:Ahmed. H. Asad, Ahmad Taher Azar, Nashwa El-Bendary, Aboul Ella Hassaanien category:cs.NE cs.CV  published:2014-03-07 summary:Features selection is an essential step for successful data classification, since it reduces the data dimensionality by removing redundant features. Consequently, that minimizes the classification complexity and time in addition to maximizing its accuracy. In this article, a comparative study considering six features selection heuristics is conducted in order to select the best relevant features subset. The tested features vector consists of fourteen features that are computed for each pixel in the field of view of retinal images in the DRIVE database. The comparison is assessed in terms of sensitivity, specificity, and accuracy measurements of the recommended features subset resulted by each heuristic when applied with the ant colony system. Experimental results indicated that the features subset recommended by the relief heuristic outperformed the subsets recommended by the other experienced heuristics. version:1
arxiv-1301-6039 | Recycling Proof Patterns in Coq: Case Studies | http://arxiv.org/abs/1301.6039 | id:1301.6039 author:JÃ³nathan Heras, Ekaterina Komendantskaya category:cs.AI cs.LG cs.LO  published:2013-01-25 summary:Development of Interactive Theorem Provers has led to the creation of big libraries and varied infrastructures for formal proofs. However, despite (or perhaps due to) their sophistication, the re-use of libraries by non-experts or across domains is a challenge. In this paper, we provide detailed case studies and evaluate the machine-learning tool ML4PG built to interactively data-mine the electronic libraries of proofs, and to provide user guidance on the basis of proof patterns found in the existing libraries. version:4
arxiv-1403-1729 | Continuous Features Discretization for Anomaly Intrusion Detectors Generation | http://arxiv.org/abs/1403.1729 | id:1403.1729 author:Amira Sayed A. Aziz, Ahmad Taher Azar, Aboul Ella Hassanien, Sanaa Al-Ola Hanafy category:cs.NI cs.CR cs.NE  published:2014-03-07 summary:Network security is a growing issue, with the evolution of computer systems and expansion of attacks. Biological systems have been inspiring scientists and designs for new adaptive solutions, such as genetic algorithms. In this paper, we present an approach that uses the genetic algorithm to generate anomaly net- work intrusion detectors. In this paper, an algorithm propose use a discretization method for the continuous features selected for the intrusion detection, to create some homogeneity between values, which have different data types. Then,the intrusion detection system is tested against the NSL-KDD data set using different distance methods. A comparison is held amongst the results, and it is shown by the end that this proposed approach has good results, and recommendations is given for future experiments. version:1
arxiv-1403-1727 | On the Sequence of State Configurations in the Garden of Eden | http://arxiv.org/abs/1403.1727 | id:1403.1727 author:Yukihiro Kamada, Kiyonori Miyasaki category:cs.NE  published:2014-03-07 summary:Autonomous threshold element circuit networks are used to investigate the structure of neural networks. With these circuits, as the transition functions are threshold functions, it is necessary to consider the existence of sequences of state configurations that cannot be transitioned. In this study, we focus on all logical functions of four or fewer variables, and we discuss the periodic sequences and transient series that transition from all sequences of state configurations. Furthermore, by using the sequences of state configurations in the Garden of Eden, we show that it is easy to obtain functions that determine the operation of circuit networks. version:1
arxiv-1401-5226 | The Why and How of Nonnegative Matrix Factorization | http://arxiv.org/abs/1401.5226 | id:1401.5226 author:Nicolas Gillis category:stat.ML cs.IR cs.LG math.OC  published:2014-01-21 summary:Nonnegative matrix factorization (NMF) has become a widely used tool for the analysis of high-dimensional data as it automatically extracts sparse and meaningful features from a set of nonnegative data vectors. We first illustrate this property of NMF on three applications, in image processing, text mining and hyperspectral imaging --this is the why. Then we address the problem of solving NMF, which is NP-hard in general. We review some standard NMF algorithms, and also present a recent subclass of NMF problems, referred to as near-separable NMF, that can be solved efficiently (that is, in polynomial time), even in the presence of noise --this is the how. Finally, we briefly describe some problems in mathematics and computer science closely related to NMF via the nonnegative rank. version:2
arxiv-1403-1697 | Compressive Hyperspectral Imaging Using Progressive Total Variation | http://arxiv.org/abs/1403.1697 | id:1403.1697 author:Simeon Kamdem Kuiteing, Giulio Coluccia, Alessandro Barducci, Mauro Barni, Enrico Magli category:cs.IT cs.CV math.IT  published:2014-03-07 summary:Compressed Sensing (CS) is suitable for remote acquisition of hyperspectral images for earth observation, since it could exploit the strong spatial and spectral correlations, llowing to simplify the architecture of the onboard sensors. Solutions proposed so far tend to decouple spatial and spectral dimensions to reduce the complexity of the reconstruction, not taking into account that onboard sensors progressively acquire spectral rows rather than acquiring spectral channels. For this reason, we propose a novel progressive CS architecture based on separate sensing of spectral rows and joint reconstruction employing Total Variation. Experimental results run on raw AVIRIS and AIRS images confirm the validity of the proposed system. version:1
arxiv-1403-1687 | Rigid-Motion Scattering for Texture Classification | http://arxiv.org/abs/1403.1687 | id:1403.1687 author:Laurent SIfre, StÃ©phane Mallat category:cs.CV  published:2014-03-07 summary:A rigid-motion scattering computes adaptive invariants along translations and rotations, with a deep convolutional network. Convolutions are calculated on the rigid-motion group, with wavelets defined on the translation and rotation variables. It preserves joint rotation and translation information, while providing global invariants at any desired scale. Texture classification is studied, through the characterization of stationary processes from a single realization. State-of-the-art results are obtained on multiple texture data bases, with important rotation and scaling variabilities. version:1
arxiv-1403-1660 | Feature Extraction of ECG Signal Using HHT Algorithm | http://arxiv.org/abs/1403.1660 | id:1403.1660 author:Neha Soorma, Jaikaran Singh, Mukesh Tiwari category:cs.CV  published:2014-03-07 summary:This paper describe the features extraction algorithm for electrocardiogram (ECG) signal using Huang Hilbert Transform and Wavelet Transform. ECG signal for an individual human being is different due to unique heart structure. The purpose of feature extraction of ECG signal would allow successful abnormality detection and efficient prognosis due to heart disorder. Some major important features will be extracted from ECG signals such as amplitude, duration, pre-gradient, post-gradient and so on. Therefore, we need a strong mathematical model to extract such useful parameter. Here an adaptive mathematical analysis model is Hilbert-Huang transform (HHT). This new approach, the Hilbert-Huang transform, is implemented to analyze the non-linear and nonstationary data. It is unique and different from the existing methods of data analysis and does not require an a priori functional basis. The effectiveness of the proposed scheme is verified through the simulation. version:1
arxiv-1403-2345 | Home Location Identification of Twitter Users | http://arxiv.org/abs/1403.2345 | id:1403.2345 author:Jalal Mahmud, Jeffrey Nichols, Clemens Drews category:cs.SI cs.CL cs.CY  published:2014-03-07 summary:We present a new algorithm for inferring the home location of Twitter users at different granularities, including city, state, time zone or geographic region, using the content of users tweets and their tweeting behavior. Unlike existing approaches, our algorithm uses an ensemble of statistical and heuristic classifiers to predict locations and makes use of a geographic gazetteer dictionary to identify place-name entities. We find that a hierarchical classification approach, where time zone, state or geographic region is predicted first and city is predicted next, can improve prediction accuracy. We have also analyzed movement variations of Twitter users, built a classifier to predict whether a user was travelling in a certain period of time and use that to further improve the location detection accuracy. Experimental evidence suggests that our algorithm works well in practice and outperforms the best existing algorithms for predicting the home location of Twitter users. version:1
arxiv-1403-1653 | Automated Tracking and Estimation for Control of Non-rigid Cloth | http://arxiv.org/abs/1403.1653 | id:1403.1653 author:Marc D. Killpack category:cs.CV  published:2014-03-07 summary:This report is a summary of research conducted on cloth tracking for automated textile manufacturing during a two semester long research course at Georgia Tech. This work was completed in 2009. Advances in current sensing technology such as the Microsoft Kinect would now allow me to relax certain assumptions and generally improve the tracking performance. This is because a major part of my approach described in this paper was to track features in a 2D image and use these to estimate the cloth deformation. Innovations such as the Kinect would improve estimation due to the automatic depth information obtained when tracking 2D pixel locations. Additionally, higher resolution camera images would probably give better quality feature tracking. However, although I would use different technology now to implement this tracker, the algorithm described and implemented in this paper is still a viable approach which is why I am publishing this as a tech report for reference. In addition, although the related work is a bit exhaustive, it will be useful to a reader who is new to methods for tracking and estimation as well as modeling of cloth. version:1
arxiv-1312-5851 | Fast Training of Convolutional Networks through FFTs | http://arxiv.org/abs/1312.5851 | id:1312.5851 author:Michael Mathieu, Mikael Henaff, Yann LeCun category:cs.CV cs.LG cs.NE  published:2013-12-20 summary:Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges. version:5
arxiv-1403-1618 | Design a Persian Automated Plagiarism Detector (AMZPPD) | http://arxiv.org/abs/1403.1618 | id:1403.1618 author:Maryam Mahmoodi, Mohammad Mahmoodi Varnamkhasti category:cs.AI cs.CL  published:2014-03-06 summary:Currently there are lots of plagiarism detection approaches. But few of them implemented and adapted for Persian languages. In this paper, our work on designing and implementation of a plagiarism detection system based on pre-processing and NLP technics will be described. And the results of testing on a corpus will be presented. version:1
arxiv-1310-2125 | Retrieval of Experiments with Sequential Dirichlet Process Mixtures in Model Space | http://arxiv.org/abs/1310.2125 | id:1310.2125 author:Ritabrata Dutta, Sohan Seth, Samuel Kaski category:stat.ML cs.IR stat.AP  published:2013-10-08 summary:We address the problem of retrieving relevant experiments given a query experiment, motivated by the public databases of datasets in molecular biology and other experimental sciences, and the need of scientists to relate to earlier work on the level of actual measurement data. Since experiments are inherently noisy and databases ever accumulating, we argue that a retrieval engine should possess two particular characteristics. First, it should compare models learnt from the experiments rather than the raw measurements themselves: this allows incorporating experiment-specific prior knowledge to suppress noise effects and focus on what is important. Second, it should be updated sequentially from newly published experiments, without explicitly storing either the measurements or the models, which is critical for saving storage space and protecting data privacy: this promotes life long learning. We formulate the retrieval as a ``supermodelling'' problem, of sequentially learning a model of the set of posterior distributions, represented as sets of MCMC samples, and suggest the use of Particle-Learning-based sequential Dirichlet process mixture (DPM) for this purpose. The relevance measure for retrieval is derived from the supermodel through the mixture representation. We demonstrate the performance of the proposed retrieval method on simulated data and molecular biological experiments. version:2
arxiv-1403-1600 | Collaborative Filtering with Information-Rich and Information-Sparse Entities | http://arxiv.org/abs/1403.1600 | id:1403.1600 author:Kai Zhu, Rui Wu, Lei Ying, R. Srikant category:stat.ML cs.IT cs.LG math.IT  published:2014-03-06 summary:In this paper, we consider a popular model for collaborative filtering in recommender systems where some users of a website rate some items, such as movies, and the goal is to recover the ratings of some or all of the unrated items of each user. In particular, we consider both the clustering model, where only users (or items) are clustered, and the co-clustering model, where both users and items are clustered, and further, we assume that some users rate many items (information-rich users) and some users rate only a few items (information-sparse users). When users (or items) are clustered, our algorithm can recover the rating matrix with $\omega(MK \log M)$ noisy entries while $MK$ entries are necessary, where $K$ is the number of clusters and $M$ is the number of items. In the case of co-clustering, we prove that $K^2$ entries are necessary for recovering the rating matrix, and our algorithm achieves this lower bound within a logarithmic factor when $K$ is sufficiently large. We compare our algorithms with a well-known algorithms called alternating minimization (AM), and a similarity score-based algorithm known as the popularity-among-friends (PAF) algorithm by applying all three to the MovieLens and Netflix data sets. Our co-clustering algorithm and AM have similar overall error rates when recovering the rating matrix, both of which are lower than the error rate under PAF. But more importantly, the error rate of our co-clustering algorithm is significantly lower than AM and PAF in the scenarios of interest in recommender systems: when recommending a few items to each user or when recommending items to users who only rated a few items (these users are the majority of the total user population). The performance difference increases even more when noise is added to the datasets. version:1
arxiv-1402-5684 | Discriminative Functional Connectivity Measures for Brain Decoding | http://arxiv.org/abs/1402.5684 | id:1402.5684 author:Orhan Firat, Mete Ozay, Ilke Oztekin, Fatos T. Yarman Vural category:cs.AI cs.CE cs.CV cs.LG  published:2014-02-23 summary:We propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain, acquired via functional magnetic resonance imaging (fMRI). In the proposed learning method, local meshes are formed around each voxel. The distance between voxels in the mesh is determined by using a functional neighbourhood concept. In order to define the functional neighbourhood, the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed. Then, the local mesh for each voxel is formed by including the functionally closest neighbouring voxels in the mesh. The relationship between the voxels within a mesh is estimated by using a linear regression model. These relationship vectors, called Functional Connectivity aware Local Relational Features (FC-LRF) are then used to train a statistical learning machine. The proposed method was tested on a recognition memory experiment, including data pertaining to encoding and retrieval of words belonging to ten different semantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn) and Support Vector Machine (SVM), are trained in order to predict the semantic category of the item being retrieved, based on activation patterns during encoding. The classification performance of the Functional Mesh Learning model, which range in 62%-71% is superior to the classical multi-voxel pattern analysis (MVPA) methods, which range in 40%-48%, for ten semantic categories. version:2
arxiv-1401-5649 | Nonlinear hyperspectral unmixing with robust nonnegative matrix factorization | http://arxiv.org/abs/1401.5649 | id:1401.5649 author:CÃ©dric FÃ©votte, Nicolas Dobigeon category:stat.ME stat.ML  published:2014-01-22 summary:This paper introduces a robust mixing model to describe hyperspectral data resulting from the mixture of several pure spectral signatures. This new model not only generalizes the commonly used linear mixing model, but also allows for possible nonlinear effects to be easily handled, relying on mild assumptions regarding these nonlinearities. The standard nonnegativity and sum-to-one constraints inherent to spectral unmixing are coupled with a group-sparse constraint imposed on the nonlinearity component. This results in a new form of robust nonnegative matrix factorization. The data fidelity term is expressed as a beta-divergence, a continuous family of dissimilarity measures that takes the squared Euclidean distance and the generalized Kullback-Leibler divergence as special cases. The penalized objective is minimized with a block-coordinate descent that involves majorization-minimization updates. Simulation results obtained on synthetic and real data show that the proposed strategy competes with state-of-the-art linear and nonlinear unmixing methods. version:2
arxiv-1403-1481 | New Perspectives on k-Support and Cluster Norms | http://arxiv.org/abs/1403.1481 | id:1403.1481 author:Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos category:stat.ML  published:2014-03-06 summary:The $k$-support norm is a regularizer which has been successfully applied to sparse vector prediction problems. We show that it belongs to a general class of norms which can be formulated as a parameterized infimum over quadratics. We further extend the $k$-support norm to matrices, and we observe that it is a special case of the matrix cluster norm. Using this formulation we derive an efficient algorithm to compute the proximity operator of both norms. This improves upon the standard algorithm for the $k$-support norm and allows us to apply proximal gradient methods to the cluster norm. We also describe how to solve regularization problems which employ centered versions of these norms. Finally, we apply the matrix regularizers to different matrix completion and multitask learning datasets. Our results indicate that the spectral $k$-support norm and the cluster norm give state of the art performance on these problems, significantly outperforming trace norm and elastic net penalties. version:1
arxiv-1403-1451 | Real-Time Classification of Twitter Trends | http://arxiv.org/abs/1403.1451 | id:1403.1451 author:Arkaitz Zubiaga, Damiano Spina, Raquel MartÃ­nez, VÃ­ctor Fresno category:cs.IR cs.CL cs.SI  published:2014-03-06 summary:Social media users give rise to social trends as they share about common interests, which can be triggered by different reasons. In this work, we explore the types of triggers that spark trends on Twitter, introducing a typology with following four types: 'news', 'ongoing events', 'memes', and 'commemoratives'. While previous research has analyzed trending topics in a long term, we look at the earliest tweets that produce a trend, with the aim of categorizing trends early on. This would allow to provide a filtered subset of trends to end users. We analyze and experiment with a set of straightforward language-independent features based on the social spread of trends to categorize them into the introduced typology. Our method provides an efficient way to accurately categorize trending topics without need of external data, enabling news organizations to discover breaking news in real-time, or to quickly identify viral memes that might enrich marketing decisions, among others. The analysis of social features also reveals patterns associated with each type of trend, such as tweets about ongoing events being shorter as many were likely sent from mobile devices, or memes having more retweets originating from a few trend-setters. version:1
arxiv-1403-1362 | Illumination,Expression and Occlusion Invariant Pose-Adaptive Face Recognition System for Real-Time Applications | http://arxiv.org/abs/1403.1362 | id:1403.1362 author:Shireesha Chintalapati, M. V. Raghunadh category:cs.CV  published:2014-03-06 summary:Face recognition in real-time scenarios is mainly affected by illumination, expression and pose variations and also by occlusion. This paper presents the framework for pose adaptive component-based face recognition system. The framework proposed deals with all the above mentioned issues. The steps involved in the presented framework are (i) facial landmark localisation, (ii) facial component extraction, (iii) pre-processing of facial image (iv) facial pose estimation (v) feature extraction using Local Binary Pattern Histograms of each component followed by (vi) fusion of pose adaptive classification of components. By employing pose adaptive classification, the recognition process is carried out on some part of database, based on estimated pose, instead of applying the recognition process on the whole database. Pre-processing techniques employed to overcome the problems due to illumination variation are also discussed in this paper. Component-based techniques provide better recognition rates when face images are occluded compared to the holistic methods. Our method is simple, feasible and provides better results when compared to other holistic methods. version:1
arxiv-1208-0432 | Efficient Point-to-Subspace Query in $\ell^1$ with Application to Robust Object Instance Recognition | http://arxiv.org/abs/1208.0432 | id:1208.0432 author:Ju Sun, Yuqian Zhang, John Wright category:cs.CV cs.LG stat.ML  published:2012-08-02 summary:Motivated by vision tasks such as robust face and object recognition, we consider the following general problem: given a collection of low-dimensional linear subspaces in a high-dimensional ambient (image) space, and a query point (image), efficiently determine the nearest subspace to the query in $\ell^1$ distance. In contrast to the naive exhaustive search which entails large-scale linear programs, we show that the computational burden can be cut down significantly by a simple two-stage algorithm: (1) projecting the query and data-base subspaces into lower-dimensional space by random Cauchy matrix, and solving small-scale distance evaluations (linear programs) in the projection space to locate candidate nearest; (2) with few candidates upon independent repetition of (1), getting back to the high-dimensional space and performing exhaustive search. To preserve the identity of the nearest subspace with nontrivial probability, the projection dimension typically is low-order polynomial of the subspace dimension multiplied by logarithm of number of the subspaces (Theorem 2.1). The reduced dimensionality and hence complexity renders the proposed algorithm particularly relevant to vision application such as robust face and object instance recognition that we investigate empirically. version:3
arxiv-1403-1353 | Collaborative Representation for Classification, Sparse or Non-sparse? | http://arxiv.org/abs/1403.1353 | id:1403.1353 author:Yang Wu, Vansteenberge Jarich, Masayuki Mukunoki, Michihiko Minoh category:cs.CV cs.AI cs.LG  published:2014-03-06 summary:Sparse representation based classification (SRC) has been proved to be a simple, effective and robust solution to face recognition. As it gets popular, doubts on the necessity of enforcing sparsity starts coming up, and primary experimental results showed that simply changing the $l_1$-norm based regularization to the computationally much more efficient $l_2$-norm based non-sparse version would lead to a similar or even better performance. However, that's not always the case. Given a new classification task, it's still unclear which regularization strategy (i.e., making the coefficients sparse or non-sparse) is a better choice without trying both for comparison. In this paper, we present as far as we know the first study on solving this issue, based on plenty of diverse classification experiments. We propose a scoring function for pre-selecting the regularization strategy using only the dataset size, the feature dimensionality and a discrimination score derived from a given feature representation. Moreover, we show that when dictionary learning is taking into account, non-sparse representation has a more significant superiority to sparse representation. This work is expected to enrich our understanding of sparse/non-sparse collaborative representation for classification and motivate further research activities. version:1
arxiv-1403-1347 | Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction | http://arxiv.org/abs/1403.1347 | id:1403.1347 author:Jian Zhou, Olga G. Troyanskaya category:q-bio.QM cs.CE cs.LG  published:2014-03-06 summary:Predicting protein secondary structure is a fundamental problem in protein structure prediction. Here we present a new supervised generative stochastic network (GSN) based method to predict local secondary structure with deep hierarchical representations. GSN is a recently proposed deep learning technique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative model. We present the supervised extension of GSN, which learns a Markov chain to sample from a conditional distribution, and applied it to protein structure prediction. To scale the model to full-sized, high-dimensional data, like protein sequences with hundreds of amino acids, we introduce a convolutional architecture, which allows efficient learning across multiple layers of hierarchical representations. Our architecture uniquely focuses on predicting structured low-level labels informed with both low and high-level representations learned by the model. In our application this corresponds to labeling the secondary structure state of each amino-acid residue. We trained and tested the model on separate sets of non-homologous proteins sharing less than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513 dataset, better than the previously reported best performance 64.9% (Wang et al., 2011) for this challenging secondary structure prediction problem. version:1
arxiv-1403-1345 | Minimax Optimal Bayesian Aggregation | http://arxiv.org/abs/1403.1345 | id:1403.1345 author:Yun Yang, David B. Dunson category:math.ST stat.ME stat.ML stat.TH  published:2014-03-06 summary:It is generally believed that ensemble approaches, which combine multiple algorithms or models, can outperform any single algorithm at machine learning tasks, such as prediction. In this paper, we propose Bayesian convex and linear aggregation approaches motivated by regression applications. We show that the proposed approach is minimax optimal when the true data-generating model is a convex or linear combination of models in the list. Moreover, the method can adapt to sparsity structure in which certain models should receive zero weights, and the method is tuning parameter free unlike competitors. More generally, under an M-open view when the truth falls outside the space of all convex/linear combinations, our theory suggests that the posterior measure tends to concentrate on the best approximation of the truth at the minimax rate. We illustrate the method through simulation studies and several applications. version:1
arxiv-1403-1336 | An Extensive Repot on the Efficiency of AIS-INMACA (A Novel Integrated MACA based Clonal Classifier for Protein Coding and Promoter Region Prediction) | http://arxiv.org/abs/1403.1336 | id:1403.1336 author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu category:cs.CE cs.LG  published:2014-03-06 summary:This paper exclusively reports the efficiency of AIS-INMACA. AIS-INMACA has created good impact on solving major problems in bioinformatics like protein region identification and promoter region prediction with less time (Pokkuluri Kiran Sree, 2014). This AIS-INMACA is now came with several variations (Pokkuluri Kiran Sree, 2014) towards projecting it as a tool in bioinformatics for solving many problems in bioinformatics. So this paper will be very much useful for so many researchers who are working in the domain of bioinformatics with cellular automata. version:1
arxiv-1311-6091 | A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property | http://arxiv.org/abs/1311.6091 | id:1311.6091 author:Jianshu Chen, Li Deng category:cs.LG cs.NE  published:2013-11-24 summary:We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that provides a sufficient condition for the stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The result approaches the best result of 17.7%, which was obtained by using RNN with long short-term memory (LSTM). The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding. version:3
arxiv-1403-1329 | Integer Programming Relaxations for Integrated Clustering and Outlier Detection | http://arxiv.org/abs/1403.1329 | id:1403.1329 author:Lionel Ott, Linsey Pang, Fabio Ramos, David Howe, Sanjay Chawla category:cs.LG  published:2014-03-06 summary:In this paper we present methods for exemplar based clustering with outlier selection based on the facility location formulation. Given a distance function and the number of outliers to be found, the methods automatically determine the number of clusters and outliers. We formulate the problem as an integer program to which we present relaxations that allow for solutions that scale to large data sets. The advantages of combining clustering and outlier selection include: (i) the resulting clusters tend to be compact and semantically coherent (ii) the clusters are more robust against data perturbations and (iii) the outliers are contextualised by the clusters and more interpretable, i.e. it is easier to distinguish between outliers which are the result of data errors from those that may be indicative of a new pattern emergent in the data. We present and contrast three relaxations to the integer program formulation: (i) a linear programming formulation (LP) (ii) an extension of affinity propagation to outlier detection (APOC) and (iii) a Lagrangian duality based formulation (LD). Evaluation on synthetic as well as real data shows the quality and scalability of these different methods. version:1
arxiv-1403-1327 | Multi-view Face Analysis Based on Gabor Features | http://arxiv.org/abs/1403.1327 | id:1403.1327 author:Hongli Liu, Weifeng Liu, Yanjiang Wang category:cs.CV  published:2014-03-06 summary:Facial analysis has attracted much attention in the technology for human-machine interface. Different methods of classification based on sparse representation and Gabor kernels have been widely applied in the fields of facial analysis. However, most of these methods treat face from a whole view standpoint. In terms of the importance of different facial views, in this paper, we present multi-view face analysis based on sparse representation and Gabor wavelet coefficients. To evaluate the performance, we conduct face analysis experiments including face recognition (FR) and face expression recognition (FER) on JAFFE database. Experiments are conducted from two parts: (1) Face images are divided into three facial parts which are forehead, eye and mouth. (2) Face images are divided into 8 parts by the orientation of Gabor kernels. Experimental results demonstrate that the proposed methods can significantly boost the performance and perform better than the other methods. version:1
arxiv-1403-1314 | Authorship detection of SMS messages using unigrams | http://arxiv.org/abs/1403.1314 | id:1403.1314 author:R. G. Ragel, P. Herath, U. Senanayake category:cs.CL cs.IR  published:2014-03-06 summary:SMS messaging is a popular media of communication. Because of its popularity and privacy, it could be used for many illegal purposes. Additionally, since they are part of the day to day life, SMSes can be used as evidence for many legal disputes. Since a cellular phone might be accessible to people close to the owner, it is important to establish the fact that the sender of the message is indeed the owner of the phone. For this purpose, the straight forward solutions seem to be the use of popular stylometric methods. However, in comparison with the data used for stylometry in the literature, SMSes have unusual characteristics making it hard or impossible to apply these methods in a conventional way. Our target is to come up with a method of authorship detection of SMS messages that could still give a usable accuracy. We argue that, considering the methods of author attribution, the best method that could be applied to SMS messages is an n-gram method. To prove our point, we checked two different methods of distribution comparison with varying number of training and testing data. We specifically try to compare how well our algorithms work under less amount of testing data and large number of candidate authors (which we believe to be the real world scenario) against controlled tests with less number of authors and selected SMSes with large number of words. To counter the lack of information in an SMS message, we propose the method of stacking together few SMSes. version:1
