arxiv-1605-04072 | Towards Empathetic Human-Robot Interactions | http://arxiv.org/abs/1605.04072 | id:1605.04072 author:Pascale Fung, Dario Bertero, Yan Wan, Anik Dey, Ricky Ho Yin Chan, Farhad Bin Siddique, Yang Yang, Chien-Sheng Wu, Ruixi Lin category:cs.CL cs.AI cs.HC cs.RO  published:2016-05-13 summary:Since the late 1990s when speech companies began providing their customer-service software in the market, people have gotten used to speaking to machines. As people interact more often with voice and gesture controlled machines, they expect the machines to recognize different emotions, and understand other high level communication features such as humor, sarcasm and intention. In order to make such communication possible, the machines need an empathy module in them which can extract emotions from human speech and behavior and can decide the correct response of the robot. Although research on empathetic robots is still in the early stage, we described our approach using signal processing techniques, sentiment analysis and machine learning algorithms to make robots that can "understand" human emotion. We propose Zara the Supergirl as a prototype system of empathetic robots. It is a software based virtual android, with an animated cartoon character to present itself on the screen. She will get "smarter" and more empathetic through its deep learning algorithms, and by gathering more data and learning from it. In this paper, we present our work so far in the areas of deep learning of emotion and sentiment recognition, as well as humor recognition. We hope to explore the future direction of android development and how it can help improve people's lives. version:1
arxiv-1605-04070 | A Reinforcement Learning System to Encourage Physical Activity in Diabetes Patients | http://arxiv.org/abs/1605.04070 | id:1605.04070 author:Irit Hochberg, Guy Feraru, Mark Kozdoba, Shie Mannor, Moshe Tennenholtz, Elad Yom-Tov category:cs.CY cs.LG  published:2016-05-13 summary:Regular physical activity is known to be beneficial to people suffering from diabetes type 2. Nevertheless, most such people are sedentary. Smartphones create new possibilities for helping people to adhere to their physical activity goals, through continuous monitoring and communication, coupled with personalized feedback. We provided 27 sedentary diabetes type 2 patients with a smartphone-based pedometer and a personal plan for physical activity. Patients were sent SMS messages to encourage physical activity between once a day and once per week. Messages were personalized through a Reinforcement Learning (RL) algorithm which optimized messages to improve each participant's compliance with the activity regimen. The RL algorithm was compared to a static policy for sending messages and to weekly reminders. Our results show that participants who received messages generated by the RL algorithm increased the amount of activity and pace of walking, while the control group patients did not. Patients assigned to the RL algorithm group experienced a superior reduction in blood glucose levels (HbA1c) compared to control policies, and longer participation caused greater reductions in blood glucose levels. The learning algorithm improved gradually in predicting which messages would lead participants to exercise. Our results suggest that a mobile phone application coupled with a learning algorithm can improve adherence to exercise in diabetic patients. As a learning algorithm is automated, and delivers personalized messages, it could be used in large populations of diabetic patients to improve health and glycemic control. Our results can be expanded to other areas where computer-led health coaching of humans may have a positive impact. version:1
arxiv-1605-04068 | Fast Semantic Image Segmentation with High Order Context and Guided Filtering | http://arxiv.org/abs/1605.04068 | id:1605.04068 author:Falong Shen, Gang Zeng category:cs.CV  published:2016-05-13 summary:This paper describes a fast and accurate semantic image segmentation approach that encodes not only the discriminative features from deep neural networks, but also the high-order context compatibility among adjacent objects as well as low level image features. We formulate the underlying problem as the conditional random field that embeds local feature extraction, clique potential construction, and guided filtering within the same framework, and provide an efficient coarse-to-fine solver. At the coarse level, we combine local feature representation and context interaction using a deep convolutional network, and directly learn the interaction from high order cliques with a message passing routine, avoiding time-consuming explicit graph inference for joint probability distribution. At the fine level, we introduce a guided filtering interpretation for the mean field algorithm, and achieve accurate object boundaries with 100+ faster than classic learning methods. The two parts are connected and jointly trained in an end-to-end fashion. Experimental results on Pascal VOC 2012 dataset have shown that the proposed algorithm outperforms the state-of-the-art, and that it achieves the rank 1 performance at the time of submission, both of which prove the effectiveness of this unified framework for semantic image segmentation. version:1
arxiv-1507-01122 | Modeling the Mind: A brief review | http://arxiv.org/abs/1507.01122 | id:1507.01122 author:Gabriel Makdah category:cs.AI cs.NE  published:2015-07-04 summary:The brain is a powerful tool used to achieve amazing feats. There have been several significant advances in neuroscience and artificial brain research in the past two decades. This article is a review of such advances, ranging from the concepts of connectionism, to neural network architectures and high-dimensional representations. There have also been advances in biologically inspired cognitive architectures of which we will cite a few. We will be positioning relatively specific models in a much broader perspective, while comparing and contrasting their advantages and weaknesses. The projects presented are targeted to model the brain at different levels, utilizing different methodologies. version:3
arxiv-1605-04046 | Track Extraction with Hidden Reciprocal Chain Models | http://arxiv.org/abs/1605.04046 | id:1605.04046 author:George Stamatescu, Langford B White, Riley Bruce-Doust category:cs.CV  published:2016-05-13 summary:This paper develops Bayesian track extraction algorithms for targets modelled as hidden reciprocal chains (HRC). HRC are a class of finite-state random process models that generalise the familiar hidden Markov chains (HMC). HRC are able to model the "intention" of a target to proceed from a given origin to a destination, behaviour which cannot be properly captured by a HMC. While Bayesian estimation problems for HRC have previously been studied, this paper focusses principally on the problem of track extraction, of which the primary task is confirming target existence in a set of detections obtained from thresholding sensor measurements. Simulation examples are presented which show that the additional model information contained in a HRC improves detection performance when compared to HMC models. version:1
arxiv-1605-04039 | Cross-Domain Visual Matching via Generalized Similarity Measure and Feature Learning | http://arxiv.org/abs/1605.04039 | id:1605.04039 author:Liang Lin, Guangrun Wang, Wangmeng Zuo, Xiangchu Feng, Lei Zhang category:cs.CV cs.AI stat.ML  published:2016-05-13 summary:Cross-domain visual data matching is one of the fundamental problems in many real-world vision tasks, e.g., matching persons across ID photos and surveillance videos. Conventional approaches to this problem usually involves two steps: i) projecting samples from different domains into a common space, and ii) computing (dis-)similarity in this space based on a certain distance. In this paper, we present a novel pairwise similarity measure that advances existing models by i) expanding traditional linear projections into affine transformations and ii) fusing affine Mahalanobis distance and Cosine similarity by a data-driven combination. Moreover, we unify our similarity measure with feature representation learning via deep convolutional neural networks. Specifically, we incorporate the similarity measure matrix into the deep architecture, enabling an end-to-end way of model optimization. We extensively evaluate our generalized similarity model in several challenging cross-domain matching tasks: person re-identification under different views and face verification over different modalities (i.e., faces from still images and videos, older and younger faces, and sketch and photo portraits). The experimental results demonstrate superior performance of our model over other state-of-the-art methods. version:1
arxiv-1605-04034 | Transfer Hashing with Privileged Information | http://arxiv.org/abs/1605.04034 | id:1605.04034 author:Joey Tianyi Zhou, Xinxing Xu, Sinno Jialin Pan, Ivor W. Tsang, Zheng Qin, Rick Siow Mong Goh category:cs.LG stat.ML  published:2016-05-13 summary:Most existing learning to hash methods assume that there are sufficient data, either labeled or unlabeled, on the domain of interest (i.e., the target domain) for training. However, this assumption cannot be satisfied in some real-world applications. To address this data sparsity issue in hashing, inspired by transfer learning, we propose a new framework named Transfer Hashing with Privileged Information (THPI). Specifically, we extend the standard learning to hash method, Iterative Quantization (ITQ), in a transfer learning manner, namely ITQ+. In ITQ+, a new slack function is learned from auxiliary data to approximate the quantization error in ITQ. We developed an alternating optimization approach to solve the resultant optimization problem for ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structure among the auxiliary data for learning more precise binary codes in the target domain. Extensive experiments on several benchmark datasets verify the effectiveness of our proposed approaches through comparisons with several state-of-the-art baselines. version:1
arxiv-1502-06930 | Tensor decomposition with generalized lasso penalties | http://arxiv.org/abs/1502.06930 | id:1502.06930 author:Oscar Hernan Madrid Padilla, James G. Scott category:stat.ME stat.CO stat.ML  published:2015-02-24 summary:We present an approach for penalized tensor decomposition (PTD) that estimates smoothly varying latent factors in multi-way data. This generalizes existing work on sparse tensor decomposition and penalized matrix decompositions, in a manner parallel to the generalized lasso for regression and smoothing problems. Our approach presents many nontrivial challenges at the intersection of modeling and computation, which are studied in detail. An efficient coordinate-wise optimization algorithm for (PTD) is presented, and its convergence properties are characterized. The method is applied both to simulated data and real data on flu hospitalizations in Texas. These results show that our penalized tensor decomposition can offer major improvements on existing methods for analyzing multi-way data that exhibit smooth spatial or temporal features. version:3
arxiv-1605-03284 | Machine Comprehension Based on Learning to Rank | http://arxiv.org/abs/1605.03284 | id:1605.03284 author:Tian Tian, Yuezhang Li category:cs.CL  published:2016-05-11 summary:Machine comprehension plays an essential role in NLP and has been widely explored with dataset like MCTest. However, this dataset is too simple and too small for learning true reasoning abilities. \cite{hermann2015teaching} therefore release a large scale news article dataset and propose a deep LSTM reader system for machine comprehension. However, the training process is expensive. We therefore try feature-engineered approach with semantics on the new dataset to see how traditional machine learning technique and semantics can help with machine comprehension. Meanwhile, our proposed L2R reader system achieves good performance with efficiency and less training data. version:2
arxiv-1605-03924 | Joint Embeddings of Hierarchical Categories and Entities | http://arxiv.org/abs/1605.03924 | id:1605.03924 author:Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, Katia Sycara category:cs.CL  published:2016-05-12 summary:Due to the lack of structured knowledge applied in learning distributed representation of categories, existing work cannot incorporate category hierarchies into entity information.~We propose a framework that embeds entities and categories into a semantic space by integrating structured knowledge and taxonomy hierarchy from large knowledge bases. The framework allows to compute meaningful semantic relatedness between entities and categories.~Compared with the previous state of the art, our framework can handle both single-word concepts and multiple-word concepts with superior performance in concept categorization and semantic relatedness. version:2
arxiv-1605-04013 | A corpus-based toy model for DisCoCat | http://arxiv.org/abs/1605.04013 | id:1605.04013 author:Stefano Gogioso category:cs.CL cs.LO math.CT  published:2016-05-13 summary:We construct an abstract categorical model for DisCoCat starting from a generic corpus annotated with constituent structure trees. Concretely, we will work with context-free grammars \`{a} la Chomsky, but Combinatory Categorial Grammar (CCG) and dependency grammars could also be used. We begin by dividing words in the corpus according to three semantic functions: (i) object words, directly modelled in the semantic space; (ii) modifier words, acting on individual object words; (iii) interaction words, connecting the meaning of distinct object words. We then consider the compact closed symmetric monoidal category of $R$-semimodules over an involutive commutative semiring $R$, and we model object words as vectors in a free $R$-semimodule $\mathcal{H}$, constructed from the corpus. Based on the grammatical structure annotating the corpus, we use Frobenius algebras to model modifier words as unary operators on $\mathcal{H}$, and interaction words as binary operators on $\mathcal{H}$. We discuss some possible future extensions and improvements of this model. version:1
arxiv-1605-02766 | LightNet: A Versatile, Standalone Matlab-based Environment for Deep Learning | http://arxiv.org/abs/1605.02766 | id:1605.02766 author:Chengxi Ye, Chen Zhao, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos category:cs.LG cs.CV cs.NE  published:2016-05-09 summary:LightNet is a lightweight, versatile and purely Matlab-based deep learning framework. The aim of the design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU for computation and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments. version:2
arxiv-1605-04874 | Gearbox Fault Detection through PSO Exact Wavelet Analysis and SVM Classifier | http://arxiv.org/abs/1605.04874 | id:1605.04874 author:Amir Hosein Zamanian, Abdolreza Ohadi category:cs.LG  published:2016-05-12 summary:Time-frequency methods for vibration-based gearbox faults detection have been considered the most efficient method. Among these methods, continuous wavelet transform (CWT) as one of the best time-frequency method has been used for both stationary and transitory signals. Some deficiencies of CWT are problem of overlapping and distortion ofsignals. In this condition, a large amount of redundant information exists so that it may cause false alarm or misinterpretation of the operator. In this paper a modified method called Exact Wavelet Analysis is used to minimize the effects of overlapping and distortion in case of gearbox faults. To implement exact wavelet analysis, Particle Swarm Optimization (PSO) algorithm has been used for this purpose. This method have been implemented for the acceleration signals from 2D acceleration sensor acquired by Advantech PCI-1710 card from a gearbox test setup in Amirkabir University of Technology. Gearbox has been considered in both healthy and chipped tooth gears conditions. Kernelized Support Vector Machine (SVM) with radial basis functions has used the extracted features from exact wavelet analysis for classification. The efficiency of this classifier is then evaluated with the other signals acquired from the setup test. The results show that in comparison of CWT, PSO Exact Wavelet Transform has better ability in feature extraction in price of more computational effort. In addition, PSO exact wavelet has better speed comparing to Genetic Algorithm (GA) exact wavelet in condition of equal population because of factoring mutation and crossover in PSO algorithm. SVM classifier with the extracted features in gearbox shows very good results and its ability has been proved. version:1
arxiv-1605-04002 | Which Learning Algorithms Can Generalize Identity-Based Rules to Novel Inputs? | http://arxiv.org/abs/1605.04002 | id:1605.04002 author:Paul Tupper, Bobak Shahriari category:cs.CL  published:2016-05-12 summary:We propose a novel framework for the analysis of learning algorithms that allows us to say when such algorithms can and cannot generalize certain patterns from training data to test data. In particular we focus on situations where the rule that must be learned concerns two components of a stimulus being identical. We call such a basis for discrimination an identity-based rule. Identity-based rules have proven to be difficult or impossible for certain types of learning algorithms to acquire from limited datasets. This is in contrast to human behaviour on similar tasks. Here we provide a framework for rigorously establishing which learning algorithms will fail at generalizing identity-based rules to novel stimuli. We use this framework to show that such algorithms are unable to generalize identity-based rules to novel inputs unless trained on virtually all possible inputs. We demonstrate these results computationally with a multilayer feedforward neural network. version:1
arxiv-1511-06349 | Generating Sentences from a Continuous Space | http://arxiv.org/abs/1511.06349 | id:1511.06349 author:Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio category:cs.LG cs.CL  published:2015-11-19 summary:The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling. version:4
arxiv-1511-00925 | Do Prices Coordinate Markets? | http://arxiv.org/abs/1511.00925 | id:1511.00925 author:Justin Hsu, Jamie Morgenstern, Ryan Rogers, Aaron Roth, Rakesh Vohra category:cs.GT cs.LG  published:2015-11-03 summary:Walrasian equilibrium prices can be said to coordinate markets: They support a welfare optimal allocation in which each buyer is buying bundle of goods that is individually most preferred. However, this clean story has two caveats. First, the prices alone are not sufficient to coordinate the market, and buyers may need to select among their most preferred bundles in a coordinated way to find a feasible allocation. Second, we don't in practice expect to encounter exact equilibrium prices tailored to the market, but instead only approximate prices, somehow encoding "distributional" information about the market. How well do prices work to coordinate markets when tie-breaking is not coordinated, and they encode only distributional information? We answer this question. First, we provide a genericity condition such that for buyers with Matroid Based Valuations, overdemand with respect to equilibrium prices is at most 1, independent of the supply of goods, even when tie-breaking is done in an uncoordinated fashion. Second, we provide learning-theoretic results that show that such prices are robust to changing the buyers in the market, so long as all buyers are sampled from the same (unknown) distribution. version:4
arxiv-1605-03956 | On the Convergent Properties of Word Embedding Methods | http://arxiv.org/abs/1605.03956 | id:1605.03956 author:Yingtao Tian, Vivek Kulkarni, Bryan Perozzi, Steven Skiena category:cs.CL  published:2016-05-12 summary:Do word embeddings converge to learn similar things over different initializations? How repeatable are experiments with word embeddings? Are all word embedding techniques equally reliable? In this paper we propose evaluating methods for learning word representations by their consistency across initializations. We propose a measure to quantify the similarity of the learned word representations under this setting (where they are subject to different random initializations). Our preliminary results illustrate that our metric not only measures a intrinsic property of word embedding methods but also correlates well with other evaluation metrics on downstream tasks. We believe our methods are is useful in characterizing robustness -- an important property to consider when developing new word embedding methods. version:1
arxiv-1605-03933 | Competitive analysis of the top-K ranking problem | http://arxiv.org/abs/1605.03933 | id:1605.03933 author:Xi Chen, Sivakanth Gopi, Jieming Mao, Jon Schneider category:cs.DS cs.IT cs.LG math.IT stat.ML  published:2016-05-12 summary:Motivated by applications in recommender systems, web search, social choice and crowdsourcing, we consider the problem of identifying the set of top $K$ items from noisy pairwise comparisons. In our setting, we are non-actively given $r$ pairwise comparisons between each pair of $n$ items, where each comparison has noise constrained by a very general noise model called the strong stochastic transitivity (SST) model. We analyze the competitive ratio of algorithms for the top-$K$ problem. In particular, we present a linear time algorithm for the top-$K$ problem which has a competitive ratio of $\tilde{O}(\sqrt{n})$; i.e. to solve any instance of top-$K$, our algorithm needs at most $\tilde{O}(\sqrt{n})$ times as many samples needed as the best possible algorithm for that instance (in contrast, all previous known algorithms for the top-$K$ problem have competitive ratios of $\tilde{\Omega}(n)$ or worse). We further show that this is tight: any algorithm for the top-$K$ problem has competitive ratio at least $\tilde{\Omega}(\sqrt{n})$. version:1
arxiv-1508-01235 | Empirical Similarity for Absent Data Generation in Imbalanced Classification | http://arxiv.org/abs/1508.01235 | id:1508.01235 author:Arash Pourhabib category:stat.ML cs.LG  published:2015-08-05 summary:When the training data in a two-class classification problem is overwhelmed by one class, most classification techniques fail to correctly identify the data points belonging to the underrepresented class. We propose Similarity-based Imbalanced Classification (SBIC) that learns patterns in the training data based on an empirical similarity function. To take the imbalanced structure of the training data into account, SBIC utilizes the concept of absent data, i.e. data from the minority class which can help better find the boundary between the two classes. SBIC simultaneously optimizes the weights of the empirical similarity function and finds the locations of absent data points. As such, SBIC uses an embedded mechanism for synthetic data generation which does not modify the training dataset, but alters the algorithm to suit imbalanced datasets. Therefore, SBIC uses the ideas of both major schools of thoughts in imbalanced classification: Like cost-sensitive approaches SBIC operates on an algorithm level to handle imbalanced structures; and similar to synthetic data generation approaches, it utilizes the properties of unobserved data points from the minority class. The application of SBIC to imbalanced datasets suggests it is comparable to, and in some cases outperforms, other commonly used classification techniques for imbalanced datasets. version:2
arxiv-1605-03884 | An Empirical-Bayes Score for Discrete Bayesian Networks | http://arxiv.org/abs/1605.03884 | id:1605.03884 author:Marco Scutari category:stat.ML stat.ME  published:2016-05-12 summary:Bayesian network structure learning is often performed in a Bayesian setting, by evaluating candidate structures using their posterior probabilities for a given data set. Score-based algorithms then use those posterior probabilities as an objective function and return the maximum a posteriori network as the learned model. For discrete Bayesian networks, the canonical choice for a posterior score is the Bayesian Dirichlet equivalent uniform (BDeu) marginal likelihood with a uniform (U) graph prior (Heckerman et al., 1995). Its favourable theoretical properties descend from assuming a uniform prior both on the space of the network structures and on the space of the parameters of the network. In this paper, we revisit the limitations of these assumptions; and we introduce an alternative set of assumptions and the resulting score: the Bayesian Dirichlet sparse (BDs) empirical Bayes marginal likelihood with a marginal uniform (MU) graph prior. We evaluate its performance in an extensive simulation study, showing that MU+BDs is more accurate than U+BDeu both in learning the structure of the network and in predicting new observations, while not being computationally more complex to estimate. version:1
arxiv-1602-00212 | Trainlets: Dictionary Learning in High Dimensions | http://arxiv.org/abs/1602.00212 | id:1602.00212 author:Jeremias Sulam, Boaz Ophir, Michael Zibulevsky, Michael Elad category:cs.CV  published:2016-01-31 summary:Sparse representations has shown to be a very powerful model for real world signals, and has enabled the development of applications with notable performance. Combined with the ability to learn a dictionary from signal examples, sparsity-inspired algorithms are often achieving state-of-the-art results in a wide variety of tasks. Yet, these methods have traditionally been restricted to small dimensions mainly due to the computational constraints that the dictionary learning problem entails. In the context of image processing, this implies handling small image patches. In this work we show how to efficiently handle bigger dimensions and go beyond the small patches in sparsity-based signal and image processing methods. We build our approach based on a new cropped wavelet decomposition, which enables a multi-scale analysis with virtually no border effects. We then employ this as the base dictionary within a double sparsity model to enable the training of adaptive dictionaries. To cope with the increase of training data, while at the same time improving the training performance, we present an Online Sparse Dictionary Learning (OSDL) algorithm to train this model effectively, enabling it to handle millions of examples. This work shows that dictionary learning can be up-scaled to tackle a new level of signal dimensions, obtaining large adaptable atoms that we call trainlets. version:4
arxiv-1605-03865 | A New Manifold Distance Measure for Visual Object Categorization | http://arxiv.org/abs/1605.03865 | id:1605.03865 author:Fengfu Li, Xiayuan Huang, Hong Qiao, Bo Zhang category:cs.CV  published:2016-05-12 summary:Manifold distances are very effective tools for visual object recognition. However, most of the traditional manifold distances between images are based on the pixel-level comparison and thus easily affected by image rotations and translations. In this paper, we propose a new manifold distance to model the dissimilarities between visual objects based on the Complex Wavelet Structural Similarity (CW-SSIM) index. The proposed distance is more robust to rotations and translations of images than the traditional manifold distance and the CW-SSIM index based distance. In addition, the proposed distance is combined with the $k$-medoids clustering method to derive a new clustering method for visual object categorization. Experiments on Coil-20, Coil-100 and Olivetti Face Databases show that the proposed distance measure is better for visual object categorization than both the traditional manifold distances and the CW-SSIM index based distances. version:1
arxiv-1605-02686 | An End-to-End System for Unconstrained Face Verification with Deep Convolutional Neural Networks | http://arxiv.org/abs/1605.02686 | id:1605.02686 author:Jun-Cheng Chen, Rajeev Ranjan, Swami Sankaranarayanan, Amit Kumar, Ching-Hui Chen, Vishal M. Patel, Carlos D. Castillo, Rama Chellappa category:cs.CV  published:2016-05-09 summary:Over the last four years, methods based on Deep Convolutional Neural Networks (DCNNs) have shown impressive performance improvements for object detection and recognition problems. This has been made possible due to the availability of large annotated datasets, a better understanding of the non-linear mapping between input images and class labels as well as the affordability of GPUs. In this paper, we present the design details of a deep learning system for end-to-end unconstrained face verification/recognition. The quantitative performance evaluation is conducted using the newly released IARPA Janus Benchmark A (IJB-A), the JANUS Challenge Set 2 (JANUS CS2), and the LFW dataset. The IJB-A dataset includes real-world unconstrained faces of 500 subjects with significant pose and illumination variations which are much harder than the Labeled Faces in the Wild (LFW) and Youtube Face (YTF) datasets. JANUS CS2 is the extended version of IJB-A which contains not only all the images/frames of IJB-A but also includes the original videos for evaluating video-based face verification system. Some open issues regarding DCNNs for object recognition problems are then discussed. version:2
arxiv-1605-03852 | Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning | http://arxiv.org/abs/1605.03852 | id:1605.03852 author:Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Chris Dyer category:cs.CL  published:2016-05-12 summary:We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned representations as features. The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order. version:1
arxiv-1605-03848 | Context-dependent feature analysis with random forests | http://arxiv.org/abs/1605.03848 | id:1605.03848 author:Antonio Sutera, Gilles Louppe, Vân Anh Huynh-Thu, Louis Wehenkel, Pierre Geurts category:stat.ML cs.LG  published:2016-05-12 summary:In many cases, feature selection is often more complicated than identifying a single subset of input variables that would together explain the output. There may be interactions that depend on contextual information, i.e., variables that reveal to be relevant only in some specific circumstances. In this setting, the contribution of this paper is to extend the random forest variable importances framework in order (i) to identify variables whose relevance is context-dependent and (ii) to characterize as precisely as possible the effect of contextual information on these variables. The usage and the relevance of our framework for highlighting context-dependent variables is illustrated on both artificial and real datasets. version:1
arxiv-1605-03835 | Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model | http://arxiv.org/abs/1605.03835 | id:1605.03835 author:Kyunghyun Cho category:cs.CL cs.LG stat.ML  published:2016-05-12 summary:Recent advances in conditional recurrent language modelling have mainly focused on network architectures (e.g., attention mechanism), learning algorithms (e.g., scheduled sampling and sequence-level training) and novel applications (e.g., image/video description generation, speech recognition, etc.) On the other hand, we notice that decoding algorithms/strategies have not been investigated as much, and it has become standard to use greedy or beam search. In this paper, we propose a novel decoding strategy motivated by an earlier observation that nonlinear hidden layers of a deep neural network stretch the data manifold. The proposed strategy is embarrassingly parallelizable without any communication overhead, while improving an existing decoding algorithm. We extensively evaluate it with attention-based neural machine translation on the task of En->Cz translation. version:1
arxiv-1605-03832 | Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning | http://arxiv.org/abs/1605.03832 | id:1605.03832 author:Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick Littell, David Mortensen, Alan W Black, Lori Levin, Chris Dyer category:cs.CL  published:2016-05-12 summary:We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually. version:1
arxiv-1605-03821 | Crowd Pedestrian Counting Considering Network Flow Constraints in Videos | http://arxiv.org/abs/1605.03821 | id:1605.03821 author:Liqing Gao, Yanzhang Wang, Xin Ye, Jian Wang category:cs.CV  published:2016-05-12 summary:A quadratic programming method with network flow constraints is proposed to improve crowd pedestrian counting in video surveillance. Most of the existing approaches estimate the number of pedestrians within one frame, which result in inconsistent predictions in temporal domain. In this paper, firstly, we segment the foreground of each frame into different groups, each of which contains several pedestrians. Then we train a regression-based map from low level features of each group to its person number. Secondly, we construct a directed graph to simulate people flow, whose vertices represent groups of each frame and edges represent people moving from one group to another. Then, the people flow can be viewed as an integer flow in the constructed directed graph. Finally, by solving a quadratic programming problem with network flow constraints in the directed graph, we obtain a consistent pedestrian counting. The experimental results show that our method can improve the crowd counting accuracy significantly. version:1
arxiv-1605-03804 | A Mid-level Video Representation based on Binary Descriptors: A Case Study for Pornography Detection | http://arxiv.org/abs/1605.03804 | id:1605.03804 author:Carlos Caetano, Sandra Avila, William Robson Schwartz, Silvio Jamil F. Guimarães, Arnaldo de A. Araújo category:cs.CV  published:2016-05-12 summary:With the growing amount of inappropriate content on the Internet, such as pornography, arises the need to detect and filter such material. The reason for this is given by the fact that such content is often prohibited in certain environments (e.g., schools and workplaces) or for certain publics (e.g., children). In recent years, many works have been mainly focused on detecting pornographic images and videos based on visual content, particularly on the detection of skin color. Although these approaches provide good results, they generally have the disadvantage of a high false positive rate since not all images with large areas of skin exposure are necessarily pornographic images, such as people wearing swimsuits or images related to sports. Local feature based approaches with Bag-of-Words models (BoW) have been successfully applied to visual recognition tasks in the context of pornography detection. Even though existing methods provide promising results, they use local feature descriptors that require a high computational processing time yielding high-dimensional vectors. In this work, we propose an approach for pornography detection based on local binary feature extraction and BossaNova image representation, a BoW model extension that preserves more richly the visual information. Moreover, we propose two approaches for video description based on the combination of mid-level representations namely BossaNova Video Descriptor (BNVD) and BoW Video Descriptor (BoW-VD). The proposed techniques are promising, achieving an accuracy of 92.40%, thus reducing the classification error by 16% over the current state-of-the-art local features approach on the Pornography dataset. version:1
arxiv-1605-03795 | Tensor Train polynomial models via Riemannian optimization | http://arxiv.org/abs/1605.03795 | id:1605.03795 author:Alexander Novikov, Mikhail Trofimov, Ivan Oseledets category:stat.ML cs.LG  published:2016-05-12 summary:Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic version of Riemannian optimization, which allows us to fit tensors with $2^{30}$ entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions. version:1
arxiv-1605-03764 | Direct Method for Training Feed-forward Neural Networks using Batch Extended Kalman Filter for Multi-Step-Ahead Predictions | http://arxiv.org/abs/1605.03764 | id:1605.03764 author:Artem Chernodub category:cs.NE  published:2016-05-12 summary:This paper is dedicated to the long-term, or multi-step-ahead, time series prediction problem. We propose a novel method for training feed-forward neural networks, such as multilayer perceptrons, with tapped delay lines. Special batch calculation of derivatives called Forecasted Propagation Through Time and batch modification of the Extended Kalman Filter are introduced. Experiments were carried out on well-known time series benchmarks, the Mackey-Glass chaotic process and the Santa Fe Laser Data Series. Recurrent and feed-forward neural networks were evaluated. version:1
arxiv-1605-03746 | Fast Graph-Based Object Segmentation for RGB-D Images | http://arxiv.org/abs/1605.03746 | id:1605.03746 author:Giorgio Toscana, Stefano Rosa category:cs.CV cs.RO  published:2016-05-12 summary:Object segmentation is an important capability for robotic systems, in particular for grasping. We present a graph- based approach for the segmentation of simple objects from RGB-D images. We are interested in segmenting objects with large variety in appearance, from lack of texture to strong textures, for the task of robotic grasping. The algorithm does not rely on image features or machine learning. We propose a modified Canny edge detector for extracting robust edges by using depth information and two simple cost functions for combining color and depth cues. The cost functions are used to build an undirected graph, which is partitioned using the concept of internal and external differences between graph regions. The partitioning is fast with O(NlogN) complexity. We also discuss ways to deal with missing depth information. We test the approach on different publicly available RGB-D object datasets, such as the Rutgers APC RGB-D dataset and the RGB-D Object Dataset, and compare the results with other existing methods. version:1
arxiv-1605-03483 | Real-time 3D Tracking of Articulated Tools for Robotic Surgery | http://arxiv.org/abs/1605.03483 | id:1605.03483 author:Menglong Ye, Lin Zhang, Stamatia Giannarou, Guang-Zhong Yang category:cs.CV cs.RO  published:2016-05-11 summary:In robotic surgery, tool tracking is important for providing safe tool-tissue interaction and facilitating surgical skills assessment. Despite recent advances in tool tracking, existing approaches are faced with major difficulties in real-time tracking of articulated tools. Most algorithms are tailored for offline processing with pre-recorded videos. In this paper, we propose a real-time 3D tracking method for articulated tools in robotic surgery. The proposed method is based on the CAD model of the tools as well as robot kinematics to generate online part-based templates for efficient 2D matching and 3D pose estimation. A robust verification approach is incorporated to reject outliers in 2D detections, which is then followed by fusing inliers with robot kinematic readings for 3D pose estimation of the tool. The proposed method has been validated with phantom data, as well as ex vivo and in vivo experiments. The results derived clearly demonstrate the performance advantage of the proposed method when compared to the state-of-the-art. version:2
arxiv-1605-03730 | Real-time Robust Manhattan Frame Estimation: Global Optimality and Applications | http://arxiv.org/abs/1605.03730 | id:1605.03730 author:Kyungdon Joo, Tae-Hyun Oh, Junsik Kim, In So Kweon category:cs.CV cs.RO  published:2016-05-12 summary:Most man-made environments, such as urban and indoor scenes, consist of a set of parallel and orthogonal planar structures. These structures are approximated by Manhattan world assumption and be referred to Manhattan Frame (MF). Given a set of inputs such as surface normals or vanishing points, we pose an MF estimation problem as a consensus set maximization that maximizes the number of inliers over the rotation search space. Conventionally this problem can be solved by a branch-and-bound framework which mathematically guarantees global optimality. However, the computational time of the conventional branch-and-bound algorithms is rather far from real-time performance. In this paper, we propose a novel bound computation method on an efficient measurement domain for MF estimation, i.e., the extended Gaussian image (EGI). By relaxing the original problem, we can compute the bounds in real-time performance, while preserving global optimality. Furthermore, we quantitatively and qualitatively demonstrate the performance of the proposed method for various synthetic and real-world data. We also show the versatility of our approach through three different applications: extension to multiple MF estimation, video stabilization and line clustering. version:1
arxiv-1605-03720 | Deformable Parts Correlation Filters for Robust Visual Tracking | http://arxiv.org/abs/1605.03720 | id:1605.03720 author:Alan Lukežič, Luka Čehovin, Matej Kristan category:cs.CV  published:2016-05-12 summary:Deformable parts models show a great potential in tracking by principally addressing non-rigid object deformations and self occlusions, but according to recent benchmarks, they often lag behind the holistic approaches. The reason is that potentially large number of degrees of freedom have to be estimated for object localization and simplifications of the constellation topology are often assumed to make the inference tractable. We present a new formulation of the constellation model with correlation filters that treats the geometric and visual constraints within a single convex cost function and derive a highly efficient optimization for MAP inference of a fully-connected constellation. We propose a tracker that models the object at two levels of detail. The coarse level corresponds a root correlation filter and a novel color model for approximate object localization, while the mid-level representation is composed of the new deformable constellation of correlation filters that refine the object location. The resulting tracker is rigorously analyzed on a highly challenging OTB, VOT2014 and VOT2015 benchmarks, exhibits a state-of-the-art performance and runs in real-time. version:1
arxiv-1605-03718 | Improved Image Boundaries for Better Video Segmentation | http://arxiv.org/abs/1605.03718 | id:1605.03718 author:Anna Khoreva, Rodrigo Benenson, Fabio Galasso, Matthias Hein, Bernt Schiele category:cs.CV  published:2016-05-12 summary:Graph-based video segmentation methods rely on superpixels as starting point. While most previous work has focused on the construction of the graph edges and weights as well as solving the graph partitioning problem, this paper focuses on better superpixels for video segmentation. We demonstrate by a comparative analysis that superpixels extracted from boundaries perform best, and show that boundary estimation can be significantly improved via image and time domain cues. With superpixels generated from our better boundaries we observe consistent improvement for two video segmentation methods in two different datasets. version:1
arxiv-1605-03705 | Movie Description | http://arxiv.org/abs/1605.03705 | id:1605.03705 author:Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, Bernt Schiele category:cs.CV cs.CL  published:2016-05-12 summary:Audio Description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed ADs, which are temporally aligned to full length movies. In addition we also collected and aligned movie scripts used in prior work and compare the two sources of descriptions. In total the Large Scale Movie Description Challenge (LSMDC) contains a parallel corpus of 118,114 sentences and video clips from 202 movies. First we characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing ADs to scripts, we find that ADs are indeed more visual and describe precisely what is shown rather than what should happen according to the scripts created prior to movie production. Furthermore, we present and compare the results of several teams who participated in a challenge organized in the context of the workshop "Describing and Understanding Video & The Large Scale Movie Description Challenge (LSMDC)", at ICCV 2015. version:1
arxiv-1605-03689 | Robust and Efficient Relative Pose with a Multi-camera System for Autonomous Vehicle in Highly Dynamic Environments | http://arxiv.org/abs/1605.03689 | id:1605.03689 author:Liu Liu, Hongdong Li, Yuchao Dai category:cs.RO cs.CV  published:2016-05-12 summary:This paper studies the relative pose problem for autonomous vehicle driving in highly dynamic and possibly cluttered environments. This is a challenging scenario due to the existence of multiple, large, and independently moving objects in the environment, which often leads to excessive portion of outliers and results in erroneous motion estimation. Existing algorithms cannot cope with such situations well. This paper proposes a new algorithm for relative pose using a multi-camera system with multiple non-overlapping individual cameras. The method works robustly even when the numbers of outliers are overwhelming. By exploiting specific prior knowledge of driving scene we have developed an efficient 4-point algorithm for multi-camera relative pose, which admits analytic solutions by solving a polynomial root-finding equation, and runs extremely fast (at about 0.5$u$s per root). When the solver is used in combination with RANSAC, we are able to quickly prune unpromising hypotheses, significantly improve the chance of finding inliers. Experiments on synthetic data have validated the performance of the proposed algorithm. Tests on real data further confirm the method's practical relevance. version:1
arxiv-1605-03688 | Going Deeper into First-Person Activity Recognition | http://arxiv.org/abs/1605.03688 | id:1605.03688 author:Minghuang Ma, Haoqi Fan, Kris M. Kitani category:cs.CV  published:2016-05-12 summary:We bring together ideas from recent work on feature design for egocentric action recognition under one framework by exploring the use of deep convolutional neural networks (CNN). Recent work has shown that features such as hand appearance, object attributes, local hand motion and camera ego-motion are important for characterizing first-person actions. To integrate these ideas under one framework, we propose a twin stream network architecture, where one stream analyzes appearance information and the other stream analyzes motion information. Our appearance stream encodes prior knowledge of the egocentric paradigm by explicitly training the network to segment hands and localize objects. By visualizing certain neuron activation of our network, we show that our proposed architecture naturally learns features that capture object attributes and hand-object configurations. Our extensive experiments on benchmark egocentric action datasets show that our deep architecture enables recognition rates that significantly outperform state-of-the-art techniques -- an average $6.6\%$ increase in accuracy over all datasets. Furthermore, by learning to recognize objects, actions and activities jointly, the performance of individual recognition tasks also increase by $30\%$ (actions) and $14\%$ (objects). We also include the results of extensive ablative analysis to highlight the importance of network design decisions.. version:1
arxiv-1605-03664 | Real-Time Web Scale Event Summarization Using Sequential Decision Making | http://arxiv.org/abs/1605.03664 | id:1605.03664 author:Chris Kedzie, Fernando Diaz, Kathleen McKeown category:cs.CL  published:2016-05-12 summary:We present a system based on sequential decision making for the online summarization of massive document streams, such as those found on the web. Given an event of interest (e.g. "Boston marathon bombing"), our system is able to filter the stream for relevance and produce a series of short text updates describing the event as it unfolds over time. Unlike previous work, our approach is able to jointly model the relevance, comprehensiveness, novelty, and timeliness required by time-sensitive queries. We demonstrate a 28.3% improvement in summary F1 and a 43.8% improvement in time-sensitive F1 metrics. version:1
arxiv-1605-03663 | Item Popularity Prediction in E-commerce Using Image Quality Feature Vectors | http://arxiv.org/abs/1605.03663 | id:1605.03663 author:Stephen Zakrewsky, Kamelia Aryafar, Ali Shokoufandeh category:cs.CV  published:2016-05-12 summary:Online retail is a visual experience- Shoppers often use images as first order information to decide if an item matches their personal style. Image characteristics such as color, simplicity, scene composition, texture, style, aesthetics and overall quality play a crucial role in making a purchase decision, clicking on or liking a product listing. In this paper we use a set of image features that indicate quality to predict product listing popularity on a major e-commerce website, Etsy. We first define listing popularity through search clicks, favoriting and purchase activity. Next, we infer listing quality from the pixel-level information of listed images as quality features. We then compare our findings to text-only models for popularity prediction. Our initial results indicate that a combined image and text modeling of product listings outperforms text-only models in popularity prediction. version:1
arxiv-1605-03662 | Subspace Perspective on Canonical Correlation Analysis: Dimension Reduction and Minimax Rates | http://arxiv.org/abs/1605.03662 | id:1605.03662 author:Zhuang Ma, Xiaodong Li category:math.ST stat.ML stat.TH  published:2016-05-12 summary:Canonical correlation analysis (CCA) is a fundamental statistical tool for exploring the correlation structure between two sets of random variables. In this paper, motivated by recent success of applying CCA to learn low dimensional representations of high dimensional objects, we propose to quantify the estimation loss of CCA by the excess prediction loss defined through a prediction-after-dimension-reduction framework. Such framework suggests viewing CCA estimation as estimating the subspaces spanned by the canonical variates. Interestedly, the proposed error metrics derived from the excess prediction loss turn out to be closely related to the principal angles between the subspaces spanned by the population and sample canonical variates respectively. We characterize the non-asymptotic minimax rates under the proposed metrics, especially the dependency of the minimax rates on the key quantities including the dimensions, the condition number of the covariance matrices, the canonical correlations and the eigen-gap, with minimal assumptions on the joint covariance matrix. To the best of our knowledge, this is the first finite sample result that captures the effect of the canonical correlations on the minimax rates. version:1
arxiv-1603-01249 | HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition | http://arxiv.org/abs/1603.01249 | id:1603.01249 author:Rajeev Ranjan, Vishal M. Patel, Rama Chellappa category:cs.CV  published:2016-03-03 summary:We present an algorithm for simultaneous face detection, landmarks localization, pose estimation and gender recognition using deep convolutional neural networks (CNN). The proposed method called, Hyperface, fuses the intermediate layers of a deep CNN using a separate CNN and trains multi-task loss on the fused features. It exploits the synergy among the tasks which boosts up their individual performances. Extensive experiments show that the proposed method is able to capture both global and local information of faces and performs significantly better than many competitive algorithms for each of these four tasks. version:2
arxiv-1508-04924 | Distributed Compressive Sensing: A Deep Learning Approach | http://arxiv.org/abs/1508.04924 | id:1508.04924 author:Hamid Palangi, Rabab Ward, Li Deng category:cs.LG cs.CV  published:2015-08-20 summary:Various studies that address the compressed sensing problem with Multiple Measurement Vectors (MMVs) have been recently carried. These studies assume the vectors of the different channels to be jointly sparse. In this paper, we relax this condition. Instead we assume that these sparse vectors depend on each other but that this dependency is unknown. We capture this dependency by computing the conditional probability of each entry in each vector being non-zero, given the "residuals" of all previous vectors. To estimate these probabilities, we propose the use of the Long Short-Term Memory (LSTM)[1], a data driven model for sequence modelling that is deep in time. To calculate the model parameters, we minimize a cross entropy cost function. To reconstruct the sparse vectors at the decoder, we propose a greedy solver that uses the above model to estimate the conditional probabilities. By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms the general MMV solver (the Simultaneous Orthogonal Matching Pursuit (SOMP)) and a number of the model-based Bayesian methods. The proposed method does not add any complexity to the general compressive sensing encoder. The trained model is used just at the decoder. As the proposed method is a data driven method, it is only applicable when training data is available. In many applications however, training data is indeed available, e.g. in recorded images and videos. version:3
arxiv-1602-06541 | A Survey of Semantic Segmentation | http://arxiv.org/abs/1602.06541 | id:1602.06541 author:Martin Thoma category:cs.CV  published:2016-02-21 summary:This survey gives an overview over different techniques used for pixel-level semantic segmentation. Metrics and datasets for the evaluation of segmentation algorithms and traditional approaches for segmentation such as unsupervised methods, Decision Forests and SVMs are described and pointers to the relevant papers are given. Recently published approaches with convolutional neural networks are mentioned and typical problematic situations for segmentation algorithms are examined. A taxonomy of segmentation algorithms is given. version:2
arxiv-1602-05257 | Peak Criterion for Choosing Gaussian Kernel Bandwidth in Support Vector Data Description | http://arxiv.org/abs/1602.05257 | id:1602.05257 author:Deovrat Kakde, Arin Chaudhuri, Seunghyun Kong, Maria Jahja, Hansi Jiang, Jorge Silva category:cs.LG  published:2016-02-17 summary:Support Vector Data Description (SVDD) is a machine-learning technique used for single class classification and outlier detection. SVDD formulation with kernel function provides a flexible boundary around data. The value of kernel function parameters affects the nature of the data boundary. For example, it is observed that with a Gaussian kernel, as the value of kernel bandwidth is lowered, the data boundary changes from spherical to wiggly. The spherical data boundary leads to underfitting, and an extremely wiggly data boundary leads to overfitting. In this paper, we propose empirical criterion to obtain good values of the Gaussian kernel bandwidth parameter. This criterion provides a smooth boundary that captures the essential geometric features of the data. version:2
arxiv-1605-03624 | Blind image separation based on exponentiated transmuted Weibull distribution | http://arxiv.org/abs/1605.03624 | id:1605.03624 author:A. M. Adam, R. M. Farouk, M. E. Abd El-aziz category:cs.CV  published:2016-05-11 summary:In recent years the processing of blind image separation has been investigated. As a result, a number of feature extraction algorithms for direct application of such image structures have been developed. For example, separation of mixed fingerprints found in any crime scene, in which a mixture of two or more fingerprints may be obtained, for identification, we have to separate them. In this paper, we have proposed a new technique for separating a multiple mixed images based on exponentiated transmuted Weibull distribution. To adaptively estimate the parameters of such score functions, an efficient method based on maximum likelihood and genetic algorithm will be used. We also calculate the accuracy of this proposed distribution and compare the algorithmic performance using the efficient approach with other previous generalized distributions. We find from the numerical results that the proposed distribution has flexibility and an efficient result version:1
arxiv-1605-03843 | Asymptotic sequential Rademacher complexity of a finite function class | http://arxiv.org/abs/1605.03843 | id:1605.03843 author:Dmitry B. Rokhlin category:cs.LG stat.ML 68Q32  60F05  published:2016-05-11 summary:For a finite function class we describe the large sample limit of the sequential Rademacher complexity in terms of the viscosity solution of a $G$-heat equation. In the language of Peng's sublinear expectation theory, the same quantity equals to the expected value of the largest order statistics of a multidimensional $G$-normal random variable. We illustrate this result by deriving upper and lower bounds for the asymptotic sequential Rademacher complexity. version:1
arxiv-1605-03560 | COCO: Performance Assessment | http://arxiv.org/abs/1605.03560 | id:1605.03560 author:Nikolaus Hansen, Anne Auger, Dimo Brockhoff, Dejan Tušar, Tea Tušar category:cs.NE  published:2016-05-11 summary:We present an any-time performance assessment for benchmarking numerical optimization algorithms in a black-box scenario, applied within the COCO benchmarking platform. The performance assessment is based on runtimes measured in number of objective function evaluations to reach one or several quality indicator target values. We argue that runtime is the only available measure with a generic, meaningful, and quantitative interpretation. We discuss the choice of the target values, runlength-based targets, and the aggregation of results by using simulated restarts, averages, and empirical distribution functions. version:1
arxiv-1605-03557 | View Synthesis by Appearance Flow | http://arxiv.org/abs/1605.03557 | id:1605.03557 author:Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, Alexei A. Efros category:cs.CV  published:2016-05-11 summary:Given one or more images of an object (or a scene), is it possible to synthesize a new image of the same instance observed from an arbitrary viewpoint? In this paper, we attempt to tackle this problem, known as novel view synthesis, by re-formulating it as a pixel copying task that avoids the notorious difficulties of generating pixels from scratch. Our approach is built on the observation that the visual appearance of different views of the same instance is highly correlated. Such correlation could be explicitly learned by training a convolutional neural network (CNN) to predict appearance flows -- 2-D coordinate vectors specifying which pixels in the input view could be used to reconstruct the target view. We show that for both objects and scenes, our approach is able to generate higher-quality synthesized views with crisp texture and boundaries than previous CNN-based techniques. version:1
arxiv-1510-08291 | Linear Shape Deformation Models with Local Support Using Graph-based Structured Matrix Factorisation | http://arxiv.org/abs/1510.08291 | id:1510.08291 author:Florian Bernard, Peter Gemmar, Frank Hertel, Jorge Goncalves, Johan Thunberg category:cs.CV math.OC stat.ML  published:2015-10-28 summary:Representing 3D shape deformations by linear models in high-dimensional space has many applications in computer vision and medical imaging, such as shape-based interpolation or segmentation. Commonly, using Principal Components Analysis a low-dimensional (affine) subspace of the high-dimensional shape space is determined. However, the resulting factors (the most dominant eigenvectors of the covariance matrix) have global support, i.e. changing the coefficient of a single factor deforms the entire shape. In this paper, a method to obtain deformation factors with local support is presented. The benefits of such models include better flexibility and interpretability as well as the possibility of interactively deforming shapes locally. For that, based on a well-grounded theoretical motivation, we formulate a matrix factorisation problem employing sparsity and graph-based regularisation terms. We demonstrate that for brain shapes our method outperforms the state of the art in local support models with respect to generalisation ability and sparse shape reconstruction, whereas for human body shapes our method gives more realistic deformations. version:2
arxiv-1512-08806 | Common Variable Learning and Invariant Representation Learning using Siamese Neural Networks | http://arxiv.org/abs/1512.08806 | id:1512.08806 author:Uri Shaham, Roy Lederman category:stat.ML cs.LG cs.NE  published:2015-12-29 summary:We consider the statistical problem of learning common source of variability in data which are synchronously captured by multiple sensors, and demonstrate that Siamese neural networks can be naturally applied to this problem. This approach is useful in particular in exploratory, data-driven applications, where neither a model nor label information is available. In recent years, many researchers have successfully applied Siamese neural networks to obtain an embedding of data which corresponds to a "semantic similarity". We present an interpretation of this "semantic similarity" as learning of equivalence classes. We discuss properties of the embedding obtained by Siamese networks and provide empirical results that demonstrate the ability of Siamese networks to learn common variability. version:3
arxiv-1605-03529 | On the Iteration Complexity of Oblivious First-Order Optimization Algorithms | http://arxiv.org/abs/1605.03529 | id:1605.03529 author:Yossi Arjevani, Ohad Shamir category:math.OC cs.LG  published:2016-05-11 summary:We consider a broad class of first-order optimization algorithms which are \emph{oblivious}, in the sense that their step sizes are scheduled regardless of the function under consideration, except for limited side-information such as smoothness or strong convexity parameters. With the knowledge of these two parameters, we show that any such algorithm attains an iteration complexity lower bound of $\Omega(\sqrt{L/\epsilon})$ for $L$-smooth convex functions, and $\tilde{\Omega}(\sqrt{L/\mu}\ln(1/\epsilon))$ for $L$-smooth $\mu$-strongly convex functions. These lower bounds are stronger than those in the traditional oracle model, as they hold independently of the dimension. To attain these, we abandon the oracle model in favor of a structure-based approach which builds upon a framework recently proposed in (Arjevani et al., 2015). We further show that without knowing the strong convexity parameter, it is impossible to attain an iteration complexity better than $\tilde{\Omega}\left((L/\mu)\ln(1/\epsilon)\right)$. This result is then used to formalize an observation regarding $L$-smooth convex functions, namely, that the iteration complexity of algorithms employing time-invariant step sizes must be at least $\Omega(L/\epsilon)$. version:1
arxiv-1605-02945 | The Yahoo Query Treebank, V. 1.0 | http://arxiv.org/abs/1605.02945 | id:1605.02945 author:Yuval Pinter, Roi Reichart, Idan Szpektor category:cs.CL cs.IR  published:2016-05-10 summary:A description and annotation guidelines for the Yahoo Webscope release of Query Treebank, Version 1.0, May 2016. version:2
arxiv-1605-02827 | When Do Luxury Cars Hit the Road? Findings by A Big Data Approach | http://arxiv.org/abs/1605.02827 | id:1605.02827 author:Yang Feng, Jiebo Luo category:cs.CY cs.CV  published:2016-05-10 summary:In this paper, we focus on studying the appearing time of different kinds of cars on the road. This information will enable us to infer the life style of the car owners. The results can further be used to guide marketing towards car owners. Conventionally, this kind of study is carried out by sending out questionnaires, which is limited in scale and diversity. To solve this problem, we propose a fully automatic method to carry out this study. Our study is based on publicly available surveillance camera data. To make the results reliable, we only use the high resolution cameras (i.e. resolution greater than $1280 \times 720$). Images from the public cameras are downloaded every minute. After obtaining 50,000 images, we apply faster R-CNN (region-based convoluntional neural network) to detect the cars in the downloaded images and a fine-tuned VGG16 model is used to recognize the car makes. Based on the recognition results, we present a data-driven analysis on the relationship between car makes and their appearing times, with implications on lifestyles. version:2
arxiv-1605-03477 | On-the-fly Network Pruning for Object Detection | http://arxiv.org/abs/1605.03477 | id:1605.03477 author:Marc Masana, Joost van de Weijer, Andrew D. Bagdanov category:cs.CV  published:2016-05-11 summary:Object detection with deep neural networks is often performed by passing a few thousand candidate bounding boxes through a deep neural network for each image. These bounding boxes are highly correlated since they originate from the same image. In this paper we investigate how to exploit feature occurrence at the image scale to prune the neural network which is subsequently applied to all bounding boxes. We show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result. version:1
arxiv-1510-08906 | Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning | http://arxiv.org/abs/1510.08906 | id:1510.08906 author:Christoph Dann, Emma Brunskill category:stat.ML cs.AI cs.LG  published:2015-10-29 summary:Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound $\tilde O(\frac{ \mathcal S ^2 \mathcal A H^2}{\epsilon^2} \ln\frac 1 \delta)$ and a lower PAC bound $\tilde \Omega(\frac{ \mathcal S \mathcal A H^2}{\epsilon^2} \ln \frac 1 {\delta + c})$ that match up to log-terms and an additional linear dependency on the number of states $ \mathcal S $. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency of at least $H^3$. version:3
arxiv-1605-03428 | Image-level Classification in Hyperspectral Images using Feature Descriptors, with Application to Face Recognition | http://arxiv.org/abs/1605.03428 | id:1605.03428 author:Vivek Sharma, Luc Van Gool category:cs.CV  published:2016-05-11 summary:In this paper, we proposed a novel pipeline for image-level classification in the hyperspectral images. By doing this, we show that the discriminative spectral information at image-level features lead to significantly improved performance in a face recognition task. We also explored the potential of traditional feature descriptors in the hyperspectral images. From our evaluations, we observe that SIFT features outperform the state-of-the-art hyperspectral face recognition methods, and also the other descriptors. With the increasing deployment of hyperspectral sensors in a multitude of applications, we believe that our approach can effectively exploit the spectral information in hyperspectral images, thus beneficial to more accurate classification. version:1
arxiv-1605-03391 | Random forests for survival analysis using maximally selected rank statistics | http://arxiv.org/abs/1605.03391 | id:1605.03391 author:Marvin N. Wright, Theresa Dankowski, Andreas Ziegler category:stat.ML cs.LG  published:2016-05-11 summary:The most popular approach for analyzing survival data is the Cox regression model. The Cox model may, however, be misspecified, and its proportionality assumption is not always fulfilled. An alternative approach is random forests for survival outcomes. The standard split criterion for random survival forests is the log-rank test statistics, which favors splitting variables with many possible split points. Conditional inference forests avoid this split point selection bias. However, linear rank statistics are utilized in current software for conditional inference forests to select the optimal splitting variable, which cannot detect non-linear effects in the independent variables. We therefore use maximally selected rank statistics for split point selection in random forests for survival analysis. As in conditional inference forests, p-values for association between split points and survival time are minimized. We describe several p-value approximations and the implementation of the proposed random forest approach. A simulation study demonstrates that unbiased split point selection is possible. However, there is a trade-off between unbiased split point selection and runtime. In benchmark studies of prediction performance on simulated and real datasets the new method performs better than random survival forests if informative dichotomous variables are combined with uninformative variables with more categories and better than conditional inference forests if non-linear covariate effects are included. In a runtime comparison the method proves to be computationally faster than both alternatives, if a simple p-value approximation is used. version:1
arxiv-1605-03389 | Efficiently Creating 3D Training Data for Fine Hand Pose Estimation | http://arxiv.org/abs/1605.03389 | id:1605.03389 author:Markus Oberweger, Gernot Riegler, Paul Wohlhart, Vincent Lepetit category:cs.CV cs.HC  published:2016-05-11 summary:While many recent hand pose estimation methods critically rely on a training set of labelled frames, the creation of such a dataset is a challenging task that has been overlooked so far. As a result, existing datasets are limited to a few sequences and individuals, with limited accuracy, and this prevents these methods from delivering their full potential. We propose a semi-automated method for efficiently and accurately labeling each frame of a hand depth video with the corresponding 3D locations of the joints: The user is asked to provide only an estimate of the 2D reprojections of the visible joints in some reference frames, which are automatically selected to minimize the labeling work by efficiently optimizing a sub-modular loss function. We then exploit spatial, temporal, and appearance constraints to retrieve the full 3D poses of the hand over the complete sequence. We show that this data can be used to train a recent state-of-the-art hand pose estimation method, leading to increased accuracy. The code and dataset can be found on our website https://cvarlab.icg.tugraz.at/projects/hand_detection/ version:1
arxiv-1605-03344 | Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration | http://arxiv.org/abs/1605.03344 | id:1605.03344 author:Jiaolong Yang, Hongdong Li, Dylan Campbell, Yunde Jia category:cs.CV  published:2016-05-11 summary:The Iterative Closest Point (ICP) algorithm is one of the most widely used methods for point-set registration. However, being based on local iterative optimization, ICP is known to be susceptible to local minima. Its performance critically relies on the quality of the initialization and only local optimality is guaranteed. This paper presents the first globally optimal algorithm, named Go-ICP, for Euclidean (rigid) registration of two 3D point-sets under the L2 error metric defined in ICP. The Go-ICP method is based on a branch-and-bound (BnB) scheme that searches the entire 3D motion space SE(3). By exploiting the special structure of SE(3) geometry, we derive novel upper and lower bounds for the registration error function. Local ICP is integrated into the BnB scheme, which speeds up the new method while guaranteeing global optimality. We also discuss extensions, addressing the issue of outlier robustness. The evaluation demonstrates that the proposed method is able to produce reliable registration results regardless of the initialization. Go-ICP can be applied in scenarios where an optimal solution is desirable or where a good initialization is not always available. version:1
arxiv-1605-03335 | Asymptotic properties for combined $L_1$ and concave regularization | http://arxiv.org/abs/1605.03335 | id:1605.03335 author:Yingying Fan, Jinchi Lv category:stat.ME stat.ML  published:2016-05-11 summary:Two important goals of high-dimensional modeling are prediction and variable selection. In this article, we consider regularization with combined $L_1$ and concave penalties, and study the sampling properties of the global optimum of the suggested method in ultra-high dimensional settings. The $L_1$-penalty provides the minimum regularization needed for removing noise variables in order to achieve oracle prediction risk, while concave penalty imposes additional regularization to control model sparsity. In the linear model setting, we prove that the global optimum of our method enjoys the same oracle inequalities as the lasso estimator and admits an explicit bound on the false sign rate, which can be asymptotically vanishing. Moreover, we establish oracle risk inequalities for the method and the sampling properties of computable solutions. Numerical studies suggest that our method yields more stable estimates than using a concave penalty alone. version:1
arxiv-1605-03328 | A robust particle detection algorithm based on symmetry | http://arxiv.org/abs/1605.03328 | id:1605.03328 author:Alvaro Rodriguez, Hanqing Zhang, Krister Wiklund, Tomas Brodin, Jonatan Klaminder, Patrik Andersson, Magnus Andersson category:cs.CV  published:2016-05-11 summary:Particle tracking is common in many biophysical, ecological, and micro-fluidic applications. Reliable tracking information is heavily dependent on of the system under study and algorithms that correctly determines particle position between images. However, in a real environmental context with the presence of noise including particular or dissolved matter in water, and low and fluctuating light conditions, many algorithms fail to obtain reliable information. We propose a new algorithm, the Circular Symmetry algorithm (C-Sym), for detecting the position of a circular particle with high accuracy and precision in noisy conditions. The algorithm takes advantage of the spatial symmetry of the particle allowing for subpixel accuracy. We compare the proposed algorithm with four different methods using both synthetic and experimental datasets. The results show that C-Sym is the most accurate and precise algorithm when tracking micro-particles in all tested conditions and it has the potential for use in applications including tracking biota in their environment. version:1
arxiv-1605-03324 | Unsupervised Semantic Action Discovery from Video Collections | http://arxiv.org/abs/1605.03324 | id:1605.03324 author:Ozan Sener, Amir Roshan Zamir, Chenxia Wu, Silvio Savarese, Ashutosh Saxena category:cs.CV cs.RO stat.ML  published:2016-05-11 summary:Human communication takes many forms, including speech, text and instructional videos. It typically has an underlying structure, with a starting point, ending, and certain objective steps between them. In this paper, we consider instructional videos where there are tens of millions of them on the Internet. We propose a method for parsing a video into such semantic steps in an unsupervised way. Our method is capable of providing a semantic "storyline" of the video composed of its objective steps. We accomplish this using both visual and language cues in a joint generative model. Our method can also provide a textual description for each of the identified semantic steps and video segments. We evaluate our method on a large number of complex YouTube videos and show that our method discovers semantically correct instructions for a variety of tasks. version:1
arxiv-1605-03321 | Tuning parameter selection in high dimensional penalized likelihood | http://arxiv.org/abs/1605.03321 | id:1605.03321 author:Yingying Fan, Cheng Yong Tang category:stat.ME stat.ML  published:2016-05-11 summary:Determining how to appropriately select the tuning parameter is essential in penalized likelihood methods for high-dimensional data analysis. We examine this problem in the setting of penalized likelihood methods for generalized linear models, where the dimensionality of covariates p is allowed to increase exponentially with the sample size n. We propose to select the tuning parameter by optimizing the generalized information criterion (GIC) with an appropriate model complexity penalty. To ensure that we consistently identify the true model, a range for the model complexity penalty is identified in GIC. We find that this model complexity penalty should diverge at the rate of some power of $\log p$ depending on the tail probability behavior of the response variables. This reveals that using the AIC or BIC to select the tuning parameter may not be adequate for consistently identifying the true model. Based on our theoretical study, we propose a uniform choice of the model complexity penalty and show that the proposed approach consistently identifies the true model among candidate models with asymptotic probability one. We justify the performance of the proposed procedure by numerical simulations and a gene expression data analysis. version:1
arxiv-1605-03315 | Interaction pursuit in high-dimensional multi-response regression via distance correlation | http://arxiv.org/abs/1605.03315 | id:1605.03315 author:Yinfei Kong, Daoji Li, Yingying Fan, Jinchi Lv category:stat.ME stat.ML  published:2016-05-11 summary:Feature interactions can contribute to a large proportion of variation in many prediction models. In the era of big data, the coexistence of high dimensionality in both responses and covariates poses unprecedented challenges in identifying important interactions. In this paper, we suggest a two-stage interaction identification method, called the interaction pursuit via distance correlation (IPDC), in the setting of high-dimensional multi-response interaction models that exploits feature screening applied to transformed variables with distance correlation followed by feature selection. Such a procedure is computationally efficient, generally applicable beyond the heredity assumption, and effective even when the number of responses diverges with the sample size. Under mild regularity conditions, we show that this method enjoys nice theoretical properties including the sure screening property, support union recovery, and oracle inequalities in prediction and estimation for both interactions and main effects. The advantages of our method are supported by several simulation studies and real data analysis. version:1
arxiv-1605-03313 | Innovated scalable efficient estimation in ultra-large Gaussian graphical models | http://arxiv.org/abs/1605.03313 | id:1605.03313 author:Yingying Fan, Jinchi Lv category:stat.ME stat.ML  published:2016-05-11 summary:Large-scale precision matrix estimation is of fundamental importance yet challenging in many contemporary applications for recovering Gaussian graphical models. In this paper, we suggest a new approach of innovated scalable efficient estimation (ISEE) for estimating large precision matrix. Motivated by the innovated transformation, we convert the original problem into that of large covariance matrix estimation. The suggested method combines the strengths of recent advances in high-dimensional sparse modeling and large covariance matrix estimation. Compared to existing approaches, our method is scalable and can deal with much larger precision matrices with simple tuning. Under mild regularity conditions, we establish that this procedure can recover the underlying graphical structure with significant probability and provide efficient estimation of link strengths. Both computational and theoretical advantages of the procedure are evidenced through simulation and real data examples. version:1
arxiv-1605-03311 | The constrained Dantzig selector with enhanced consistency | http://arxiv.org/abs/1605.03311 | id:1605.03311 author:Yinfei Kong, Zemin Zheng, Jinchi Lv category:stat.ME stat.ML  published:2016-05-11 summary:The Dantzig selector has received popularity for many applications such as compressed sensing and sparse modeling, thanks to its computational efficiency as a linear programming problem and its nice sampling properties. Existing results show that it can recover sparse signals mimicking the accuracy of the ideal procedure, up to a logarithmic factor of the dimensionality. Such a factor has been shown to hold for many regularization methods. An important question is whether this factor can be reduced to a logarithmic factor of the sample size in ultra-high dimensions under mild regularity conditions. To provide an affirmative answer, in this paper we suggest the constrained Dantzig selector, which has more flexible constraints and parameter space. We prove that the suggested method can achieve convergence rates within a logarithmic factor of the sample size of the oracle rates and improved sparsity, under a fairly weak assumption on the signal strength. Such improvement is significant in ultra-high dimensions. This method can be implemented efficiently through sequential linear programming. Numerical studies confirm that the sample size needed for a certain level of accuracy in these problems can be much reduced. version:1
arxiv-1605-03310 | Asymptotic equivalence of regularization methods in thresholded parameter space | http://arxiv.org/abs/1605.03310 | id:1605.03310 author:Yingying Fan, Jinchi Lv category:stat.ME math.ST stat.ML stat.TH  published:2016-05-11 summary:High-dimensional data analysis has motivated a spectrum of regularization methods for variable selection and sparse modeling, with two popular classes of convex ones and concave ones. A long debate has been on whether one class dominates the other, an important question both in theory and to practitioners. In this paper, we characterize the asymptotic equivalence of regularization methods, with general penalty functions, in a thresholded parameter space under the generalized linear model setting, where the dimensionality can grow up to exponentially with the sample size. To assess their performance, we establish the oracle inequalities, as in Bickel, Ritov and Tsybakov (2009), of the global minimizer for these methods under various prediction and variable selection losses. These results reveal an interesting phase transition phenomenon. For polynomially growing dimensionality, the $L_1$-regularization method of Lasso and concave methods are asymptotically equivalent, having the same convergence rates in the oracle inequalities. For exponentially growing dimensionality, concave methods are asymptotically equivalent but have faster convergence rates than the Lasso. We also establish a stronger property of the oracle risk inequalities of the regularization methods, as well as the sampling properties of computable solutions. Our new theoretical results are illustrated and justified by simulation and real data examples. version:1
arxiv-1605-03306 | High dimensional thresholded regression and shrinkage effect | http://arxiv.org/abs/1605.03306 | id:1605.03306 author:Zemin Zheng, Yingying Fan, Jinchi Lv category:stat.ME stat.ML  published:2016-05-11 summary:High-dimensional sparse modeling via regularization provides a powerful tool for analyzing large-scale data sets and obtaining meaningful, interpretable models. The use of nonconvex penalty functions shows advantage in selecting important features in high dimensions, but the global optimality of such methods still demands more understanding. In this paper, we consider sparse regression with hard-thresholding penalty, which we show to give rise to thresholded regression. This approach is motivated by its close connection with the $L_0$-regularization, which can be unrealistic to implement in practice but of appealing sampling properties, and its computational advantage. Under some mild regularity conditions allowing possibly exponentially growing dimensionality, we establish the oracle inequalities of the resulting regularized estimator, as the global minimizer, under various prediction and variable selection losses, as well as the oracle risk inequalities of the hard-thresholded estimator followed by a further $L_2$-regularization. The risk properties exhibit interesting shrinkage effects under both estimation and prediction losses. We identify the optimal choice of the ridge parameter, which is shown to have simultaneous advantages to both the $L_2$-loss and prediction loss. These new results and phenomena are evidenced by simulation and real data examples. version:1
arxiv-1309-2388 | Minimizing Finite Sums with the Stochastic Average Gradient | http://arxiv.org/abs/1309.2388 | id:1309.2388 author:Mark Schmidt, Nicolas Le Roux, Francis Bach category:math.OC cs.LG stat.CO stat.ML  published:2013-09-10 summary:We propose the stochastic average gradient (SAG) method for optimizing the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods. The convergence rate is improved from O(1/k^{1/2}) to O(1/k) in general, and when the sum is strongly-convex the convergence rate is improved from the sub-linear O(1/k) to a linear convergence rate of the form O(p^k) for p \textless{} 1. Further, in many cases the convergence rate of the new method is also faster than black-box deterministic gradient methods, in terms of the number of gradient evaluations. Numerical experiments indicate that the new algorithm often dramatically outperforms existing SG and deterministic gradient methods, and that the performance may be further improved through the use of non-uniform sampling strategies. version:2
arxiv-1603-06995 | Multi-Scale Convolutional Neural Networks for Time Series Classification | http://arxiv.org/abs/1603.06995 | id:1603.06995 author:Zhicheng Cui, Wenlin Chen, Yixin Chen category:cs.CV  published:2016-03-22 summary:Time series classification (TSC), the problem of predicting class labels of time series, has been around for decades within the community of data mining and machine learning, and found many important applications such as biomedical engineering and clinical prediction. However, it still remains challenging and falls short of classification accuracy and efficiency. Traditional approaches typically involve extracting discriminative features from the original time series using dynamic time warping (DTW) or shapelet transformation, based on which an off-the-shelf classifier can be applied. These methods are ad-hoc and separate the feature extraction part with the classification part, which limits their accuracy performance. Plus, most existing methods fail to take into account the fact that time series often have features at different time scales. To address these problems, we propose a novel end-to-end neural network model, Multi-Scale Convolutional Neural Networks (MCNN), which incorporates feature extraction and classification in a single framework. Leveraging a novel multi-branch layer and learnable convolutional layers, MCNN automatically extracts features at different scales and frequencies, leading to superior feature representation. MCNN is also computationally efficient, as it naturally leverages GPU computing. We conduct comprehensive empirical evaluation with various existing methods on a large number of benchmark datasets, and show that MCNN advances the state-of-the-art by achieving superior accuracy performance than other leading methods. version:4
arxiv-1605-03267 | Generalized Sparse Precision Matrix Selection for Fitting Multivariate Gaussian Random Fields to Large Data Sets | http://arxiv.org/abs/1605.03267 | id:1605.03267 author:Sam Davanloo Tajbakhsh, Necdet Serhat Aybat, Enrique del Castillo category:stat.ML  published:2016-05-11 summary:This paper generalizes the Sparse Precision matrix Selection (SPS) algorithm, proposed by Davanloo et al. (2015) for estimating scalar Gaussian Random Field (GRF) models, to the multivariate, second-order stationary case under a separable covariance function. Theoretical convergence rates for the estimated covariance matrix and for the estimated parameters of the correlation function are established. Numerical simulation results validate our theoretical findings. Data segmentation is used to handle large data sets. version:1
arxiv-1511-06449 | Learning to decompose for object detection and instance segmentation | http://arxiv.org/abs/1511.06449 | id:1511.06449 author:Eunbyung Park, Alexander C. Berg category:cs.CV cs.LG  published:2015-11-19 summary:Although deep convolutional neural networks(CNNs) have achieved remarkable results on object detection and segmentation, pre- and post-processing steps such as region proposals and non-maximum suppression(NMS), have been required. These steps result in high computational complexity and sensitivity to hyperparameters, e.g. thresholds for NMS. In this work, we propose a novel end-to-end trainable deep neural network architecture, which consists of convolutional and recurrent layers, that generates the correct number of object instances and their bounding boxes (or segmentation masks) given an image, using only a single network evaluation without any pre- or post-processing steps. We have tested on detecting digits in multi-digit images synthesized using MNIST, automatically segmenting digits in these images, and detecting cars in the KITTI benchmark dataset. The proposed approach outperforms a strong CNN baseline on the synthesized digits datasets and shows promising results on KITTI car detection. version:3
arxiv-1605-03261 | Sensorimotor Input as a Language Generalisation Tool: A Neurorobotics Model for Generation and Generalisation of Noun-Verb Combinations with Sensorimotor Inputs | http://arxiv.org/abs/1605.03261 | id:1605.03261 author:Junpei Zhong, Martin Peniak, Jun Tani, Tetsuya Ogata, Angelo Cangelosi category:cs.RO cs.CL  published:2016-05-11 summary:The paper presents a neurorobotics cognitive model to explain the understanding and generalisation of nouns and verbs combinations when a vocal command consisting of a verb-noun sentence is provided to a humanoid robot. This generalisation process is done via the grounding process: different objects are being interacted, and associated, with different motor behaviours, following a learning approach inspired by developmental language acquisition in infants. This cognitive model is based on Multiple Time-scale Recurrent Neural Networks (MTRNN).With the data obtained from object manipulation tasks with a humanoid robot platform, the robotic agent implemented with this model can ground the primitive embodied structure of verbs through training with verb-noun combination samples. Moreover, we show that a functional hierarchical architecture, based on MTRNN, is able to generalise and produce novel combinations of noun-verb sentences. Further analyses of the learned network dynamics and representations also demonstrate how the generalisation is possible via the exploitation of this functional hierarchical recurrent network. version:1
arxiv-1605-03259 | Deep Attributes Driven Multi-Camera Person Re-identification | http://arxiv.org/abs/1605.03259 | id:1605.03259 author:Chi Su, Shiliang Zhang, Junliang Xing, Wen Gao, Qi Tian category:cs.CV  published:2016-05-11 summary:The visual appearance of a person is easily affected by many factors like pose variations, viewpoint changes and camera parameter differences. This makes person Re-Identification (ReID) among multiple cameras a very challenging task. This work is motivated to learn mid-level human attributes which are robust to such visual appearance variations. And we propose a semi-supervised attribute learning framework which progressively boosts the accuracy of attributes only using a limited number of labeled data. Specifically, this framework involves a three-stage training. A deep Convolutional Neural Network (dCNN) is first trained on an independent dataset labeled with attributes. Then it is fine-tuned on another dataset only labeled with person IDs using our defined triplet loss. Finally, the updated dCNN predicts attribute labels for the target dataset, which is combined with the independent dataset for the final round of fine-tuning. The predicted attributes, namely \emph{deep attributes} exhibit superior generalization ability across different datasets. By directly using the deep attributes with simple Cosine distance, we have obtained surprisingly good accuracy on four person ReID datasets. Experiments also show that a simple metric learning modular further boosts our method, making it significantly outperform many recent works. version:1
arxiv-1604-05966 | Labeled Multi-Bernoulli Tracking for Industrial Mobile Platform Safety | http://arxiv.org/abs/1604.05966 | id:1604.05966 author:Tharindu Rathnayake, Reza Hoseinnezhad, Ruwan Tennakoon, Alireza Bab-Hadiashar category:cs.CV  published:2016-04-20 summary:This paper presents a track-before-detect labeled multi-Bernoulli filter tailored for industrial mobile platform safety applications. We derive two application specific separable likelihood functions that capture the geometric shape and colour information of the human targets who are wearing a high visible vest. These likelihoods are then used in a labeled multi-Bernoulli filter with a novel two step Bayesian update. Preliminary simulation results show that the proposed solution can successfully track human workers wearing a luminous yellow colour vest in an industrial environment. version:2
arxiv-1605-03234 | Performance Bounds for Sparse Signal Reconstruction with Multiple Side Information | http://arxiv.org/abs/1605.03234 | id:1605.03234 author:Huynh Van Luong, Jurgen Seiler, Andre Kaup, Soren Forchhammer, Nikos Deligiannis category:cs.IT cs.CV math.IT math.OC  published:2016-05-10 summary:In the context of compressive sensing (CS), this paper considers the problem of reconstructing sparse signals with the aid of other given correlated sources as multiple side information (SI). To address this problem, we propose a reconstruction algorithm with multiple SI (RAMSI) that solves a general weighted $n$-$\ell_{1}$ norm minimization. The proposed RAMSI algorithm takes advantage of both CS and the $n$-$\ell_{1}$ minimization by adaptively computing optimal weights among SI signals at every reconstructed iteration. In addition, we establish theoretical performance bounds on the number of measurements that are required to successfully reconstruct the original sparse source using RAMSI under arbitrary support SI conditions. The analyses of the established bounds reveal that RAMSI can achieve sharper bounds and significant performance improvements compared to classical CS. We evaluate experimentally the proposed algorithm and the established bounds using synthetic sparse signals as well as correlated feature histograms, extracted from a multiview image database for object recognition. The obtained results show clearly that the proposed RAMSI algorithm outperforms classical CS and CS with single SI in terms of both the theoretical bounds and the practical performance. version:1
arxiv-1510-05328 | Exploring the Space of Adversarial Images | http://arxiv.org/abs/1510.05328 | id:1510.05328 author:Pedro Tabacof, Eduardo Valle category:cs.NE  published:2015-10-19 summary:Adversarial examples have raised questions regarding the robustness and security of deep neural networks. In this work we formalize the problem of adversarial images given a pretrained classifier, showing that even in the linear case the resulting optimization problem is nonconvex. We generate adversarial images using shallow and deep classifiers on the MNIST and ImageNet datasets. We probe the pixel space of adversarial images using noise of varying intensity and distribution. We bring novel visualizations that showcase the phenomenon and its high variability. We show that adversarial images appear in large regions in the pixel space, but that, for the same task, a shallow classifier seems more robust to adversarial images than a deep convolutional network. version:4
arxiv-1605-03222 | Action Recognition in Video Using Sparse Coding and Relative Features | http://arxiv.org/abs/1605.03222 | id:1605.03222 author:Anali Alfaro, Domingo Mery, Alvaro Soto category:cs.CV  published:2016-05-10 summary:This work presents an approach to category-based action recognition in video using sparse coding techniques. The proposed approach includes two main contributions: i) A new method to handle intra-class variations by decomposing each video into a reduced set of representative atomic action acts or key-sequences, and ii) A new video descriptor, ITRA: Inter-Temporal Relational Act Descriptor, that exploits the power of comparative reasoning to capture relative similarity relations among key-sequences. In terms of the method to obtain key-sequences, we introduce a loss function that, for each video, leads to the identification of a sparse set of representative key-frames capturing both, relevant particularities arising in the input video, as well as relevant generalities arising in the complete class collection. In terms of the method to obtain the ITRA descriptor, we introduce a novel scheme to quantify relative intra and inter-class similarities among local temporal patterns arising in the videos. The resulting ITRA descriptor demonstrates to be highly effective to discriminate among action categories. As a result, the proposed approach reaches remarkable action recognition performance on several popular benchmark datasets, outperforming alternative state-of-the-art techniques by a large margin. version:1
arxiv-1605-02057 | Robust Bayesian Method for Simultaneous Block Sparse Signal Recovery with Applications to Face Recognition | http://arxiv.org/abs/1605.02057 | id:1605.02057 author:Igor Fedorov, Ritwik Giri, Bhaskar D. Rao, Truong Q. Nguyen category:cs.CV  published:2016-05-06 summary:In this paper, we present a novel Bayesian approach to recover simultaneously block sparse signals in the presence of outliers. The key advantage of our proposed method is the ability to handle non-stationary outliers, i.e. outliers which have time varying support. We validate our approach with empirical results showing the superiority of the proposed method over competing approaches in synthetic data experiments as well as the multiple measurement face recognition problem. version:2
arxiv-1605-03209 | Vocabulary Manipulation for Neural Machine Translation | http://arxiv.org/abs/1605.03209 | id:1605.03209 author:Haitao Mi, Zhiguo Wang, Abe Ittycheriah category:cs.CL  published:2016-05-10 summary:In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentence-level or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a word-to-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-to-French task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015). version:1
arxiv-1605-03170 | DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model | http://arxiv.org/abs/1605.03170 | id:1605.03170 author:Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, Bernt Schiele category:cs.CV  published:2016-05-10 summary:The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. We evaluate our approach on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation. Models and code available at http://pose.mpi-inf.mpg.de version:1
arxiv-1602-06612 | Clustering subgaussian mixtures by semidefinite programming | http://arxiv.org/abs/1602.06612 | id:1602.06612 author:Dustin G. Mixon, Soledad Villar, Rachel Ward category:stat.ML cs.DS cs.IT cs.LG math.IT math.ST stat.TH  published:2016-02-22 summary:We introduce a model-free relax-and-round algorithm for k-means clustering based on a semidefinite relaxation due to Peng and Wei. The algorithm interprets the SDP output as a denoised version of the original data and then rounds this output to a hard clustering. We provide a generic method for proving performance guarantees for this algorithm, and we analyze the algorithm in the context of subgaussian mixture models. We also study the fundamental limits of estimating Gaussian centers by k-means clustering in order to compare our approximation guarantee to the theoretically optimal k-means clustering solution. version:2
arxiv-1605-03150 | Road Detection through Supervised Classification | http://arxiv.org/abs/1605.03150 | id:1605.03150 author:Yasamin Alkhorshid, Kamelia Aryafar, Sven Bauer, Gerd Wanielik category:cs.CV  published:2016-05-10 summary:Autonomous driving is a rapidly evolving technology. Autonomous vehicles are capable of sensing their environment and navigating without human input through sensory information such as radar, lidar, GNSS, vehicle odometry, and computer vision. This sensory input provides a rich dataset that can be used in combination with machine learning models to tackle multiple problems in supervised settings. In this paper we focus on road detection through gray-scale images as the sole sensory input. Our contributions are twofold: first, we introduce an annotated dataset of urban roads for machine learning tasks; second, we introduce a road detection framework on this dataset through supervised classification and hand-crafted feature vectors. version:1
arxiv-1605-03148 | A Coverage Embedding Model for Neural Machine Translation | http://arxiv.org/abs/1605.03148 | id:1605.03148 author:Haitao Mi, Baskaran Sankaran, Zhiguo Wang, Abe Ittycheriah category:cs.CL  published:2016-05-10 summary:In this paper, we enhance the attention-based neural machine translation by adding an explicit coverage embedding model to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector, and then keeps updating it with a gated recurrent unit as the translation goes. All the initialized coverage embeddings and updating matrix are learned in the training procedure. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. version:1
arxiv-1511-07471 | Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize | http://arxiv.org/abs/1511.07471 | id:1511.07471 author:Huizhen Yu category:cs.LG 90C40  62L20  68W40  published:2015-11-23 summary:We consider the emphatic temporal-difference (TD) algorithm, ETD($\lambda$), for learning the value functions of stationary policies in a discounted, finite state and action Markov decision process. The ETD($\lambda$) algorithm was recently proposed by Sutton, Mahmood, and White to solve a long-standing divergence problem of the standard TD algorithm when it is applied to off-policy training, where data from an exploratory policy are used to evaluate other policies of interest. The almost sure convergence of ETD($\lambda$) has been proved in our recent work under general off-policy training conditions, but for a narrow range of diminishing stepsize. In this paper we present convergence results for constrained versions of ETD($\lambda$) with constant stepsize and with diminishing stepsize from a broad range. Our results characterize the asymptotic behavior of the trajectory of iterates produced by those algorithms, and are derived by combining key properties of ETD($\lambda$) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory. For the case of constant stepsize, in addition to analyzing the behavior of the algorithms in the limit as the stepsize parameter approaches zero, we also analyze their behavior for a fixed stepsize and bound the deviations of their averaged iterates from the desired solution. These results are obtained by exploiting the weak Feller property of the Markov chains associated with the algorithms, and by using ergodic theorems for weak Feller Markov chains, in conjunction with the convergence results we get from the weak convergence methods. Besides ETD($\lambda$), our analysis also applies to the off-policy TD($\lambda$) algorithm, when the divergence issue is avoided by setting $\lambda$ sufficiently large. version:2
arxiv-1605-03124 | PARAPH: Presentation Attack Rejection by Analyzing Polarization Hypotheses | http://arxiv.org/abs/1605.03124 | id:1605.03124 author:Ethan M. Rudd, Manuel Gunther, Terrance E. Boult category:cs.CV  published:2016-05-10 summary:For applications such as airport border control, biometric technologies that can process many capture subjects quickly, efficiently, with weak supervision, and with minimal discomfort are desirable. Facial recognition is particularly appealing because it is minimally invasive yet offers relatively good recognition performance. Unfortunately, the combination of weak supervision and minimal invasiveness makes even highly accurate facial recognition systems susceptible to spoofing via presentation attacks. Thus, there is great demand for an effective and low cost system capable of rejecting such attacks.To this end we introduce PARAPH -- a novel hardware extension that exploits different measurements of light polarization to yield an image space in which presentation media are readily discernible from Bona Fide facial characteristics. The PARAPH system is inexpensive with an added cost of less than 10 US dollars. The system makes two polarization measurements in rapid succession, allowing them to be approximately pixel-aligned, with a frame rate limited by the camera, not the system. There are no moving parts above the molecular level, due to the efficient use of twisted nematic liquid crystals. We present evaluation images using three presentation attack media next to an actual face -- high quality photos on glossy and matte paper and a video of the face on an LCD. In each case, the actual face in the image generated by PARAPH is structurally discernible from the presentations, which appear either as noise (print attacks) or saturated images (replay attacks). version:1
arxiv-1605-03122 | Kernel-Based Structural Equation Models for Topology Identification of Directed Networks | http://arxiv.org/abs/1605.03122 | id:1605.03122 author:Yanning Shen, Brian Baingana, Georgios B. Giannakis category:stat.ML  published:2016-05-10 summary:Structural equation models (SEMs) have been widely adopted for inference of causal interactions in complex networks. Recent examples include unveiling topologies of hidden causal networks over which processes such as spreading diseases, or rumors propagate. The appeal of SEMs in these settings stems from their simplicity and tractability, since they typically assume linear dependencies among observable variables. Acknowledging the limitations inherent to adopting linear models, the present paper advocates nonlinear SEMs, which account for (possible) nonlinear dependencies among network nodes. The advocated approach leverages kernels as a powerful encompassing framework for nonlinear modeling, and an efficient estimator with affordable tradeoffs is put forth. Interestingly, pursuit of the novel kernel-based approach yields a convex regularized estimator that promotes edge sparsity, and is amenable to proximal-splitting optimization methods. To this end, solvers with complementary merits are developed by leveraging the alternating direction method of multipliers, and proximal gradient iterations. Experiments conducted on simulated data demonstrate that the novel approach outperforms linear SEMs with respect to edge detection errors. Furthermore, tests on a real gene expression dataset unveil interesting new edges that were not revealed by linear SEMs, which could shed more light on regulatory behavior of human genes. version:1
arxiv-1604-03168 | Hardware-oriented Approximation of Convolutional Neural Networks | http://arxiv.org/abs/1604.03168 | id:1604.03168 author:Philipp Gysel, Mohammad Motamedi, Soheil Ghiasi category:cs.CV  published:2016-04-11 summary:High computational complexity hinders the widespread usage of Convolutional Neural Networks (CNNs), especially in mobile devices. Hardware accelerators are arguably the most promising approach for reducing both execution time and power consumption. One of the most important steps in accelerator development is hardware-oriented model approximation. In this paper we present Ristretto, a model approximation framework that analyzes a given CNN with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available. version:2
arxiv-1605-03072 | Semi-Supervised Representation Learning based on Probabilistic Labeling | http://arxiv.org/abs/1605.03072 | id:1605.03072 author:Ershad Banijamali, Ali Ghodsi category:cs.LG  published:2016-05-10 summary:In this paper we present a new way of semi-supervised representation learning. The algorithm is based on assigning class probabilities to unlabeled data. The approach will use Hilber-Schmidt Independence Criterion (HSIC) to find a mapping which takes the data to a lower dimensional space. We call this algorithm SSRL-PL. Use of unlabeled data for learning is not always beneficial and there is no algorithm which deterministically guarantee the improvement of the performance by using unlabeled data. Therefore, we also propose a bound on the performance of the algorithm which can be used to determine the effectiveness of using the structure of unlabeled data in the algorithm. version:1
arxiv-1605-03040 | A note on the statistical view of matrix completion | http://arxiv.org/abs/1605.03040 | id:1605.03040 author:Tianxi Li category:stat.ML  published:2016-05-10 summary:A very simple interpretation of matrix completion problem is introduced based on statistical models. Combined with the well-known results from missing data analysis, such interpretation indicates that matrix completion is still a valid and principled estimation procedure even without the missing completely at random (MCAR) assumption, which almost all of the current theoretical studies of matrix completion assume. version:1
arxiv-1605-03027 | Destination Prediction by Trajectory Distribution Based Model | http://arxiv.org/abs/1605.03027 | id:1605.03027 author:Philippe C. Besse, Brendan Guillouet, Jean-Michel Loubes, Francois Royer category:stat.ML  published:2016-05-10 summary:In this paper we propose a new method to predict the final destination of vehicle trips based on their initial partial trajectories. We first review how we obtained clustering of trajectories that describes user behaviour. Then, we explain how we model main traffic flow patterns by a mixture of 2d Gaussian distributions. This yielded a density based clustering of locations, which produces a data driven grid of similar points within each pattern. We present how this model can be used to predict the final destination of a new trajectory based on their first locations using a two step procedure: We first assign the new trajectory to the clusters it mot likely belongs. Secondly, we use characteristics from trajectories inside these clusters to predict the final destination. Finally, we present experimental results of our methods for classification of trajectories and final destination prediction on datasets of timestamped GPS-Location of taxi trips. We test our methods on two different datasets, to assess the capacity of our method to adapt automatically to different subsets. version:1
arxiv-1605-03012 | Automatic 3D liver location and segmentation via convolutional neural networks and graph cut | http://arxiv.org/abs/1605.03012 | id:1605.03012 author:Fang Lu, Fa Wu, Peijun Hu, Zhiyi Peng, Dexing Kong category:cs.CV  published:2016-05-10 summary:Purpose Segmentation of the liver from abdominal computed tomography (CT) image is an essential step in some computer assisted clinical interventions, such as surgery planning for living donor liver transplant (LDLT), radiotherapy and volume measurement. In this work, we develop a deep learning algorithm with graph cut refinement to automatically segment liver in CT scans. Methods The proposed method consists of two main steps: (i) simultaneously liver detection and probabilistic segmentation using 3D convolutional neural networks (CNNs); (ii) accuracy refinement of initial segmentation with graph cut and the previously learned probability map. Results The proposed approach was validated on forty CT volumes taken from two public databases MICCAI-Sliver07 and 3Dircadb. For the MICCAI-Sliver07 test set, the calculated mean ratios of volumetric overlap error (VOE), relative volume difference (RVD), average symmetric surface distance (ASD), root mean square symmetric surface distance (RMSD) and maximum symmetric surface distance (MSD) are 5.9%, 2.7%, 0.91%, 1.88 mm, and 18.94 mm, respectively. In the case of 20 3Dircadb data, the calculated mean ratios of VOE, RVD, ASD, RMSD and MSD are 9.36%, 0.97%, 1.89%, 4.15 mm and 33.14 mm, respectively. Conclusion The proposed method is fully automatic without any user interaction. Quantitative results reveal that the proposed approach is efficient and accurate for hepatic volume estimation in a clinical setup. The high correlation between the automatic and manual references shows that the proposed method can be good enough to replace the time-consuming and non-reproducible manual segmentation method. version:1
arxiv-1605-03004 | MUST-CNN: A Multilayer Shift-and-Stitch Deep Convolutional Architecture for Sequence-based Protein Structure Prediction | http://arxiv.org/abs/1605.03004 | id:1605.03004 author:Zeming Lin, Jack Lanchantin, Yanjun Qi category:cs.LG  published:2016-05-10 summary:Predicting protein properties such as solvent accessibility and secondary structure from its primary amino acid sequence is an important task in bioinformatics. Recently, a few deep learning models have surpassed the traditional window based multilayer perceptron. Taking inspiration from the image classification domain we propose a deep convolutional neural network architecture, MUST-CNN, to predict protein properties. This architecture uses a novel multilayer shift-and-stitch (MUST) technique to generate fully dense per-position predictions on protein sequences. Our model is significantly simpler than the state-of-the-art, yet achieves better results. By combining MUST and the efficient convolution operation, we can consider far more parameters while retaining very fast prediction speeds. We beat the state-of-the-art performance on two large protein property prediction datasets. version:1
arxiv-1605-02989 | An efficient K-means algorithm for Massive Data | http://arxiv.org/abs/1605.02989 | id:1605.02989 author:Marco Capó, Aritz Pérez, José Antonio Lozano category:stat.ML cs.LG  published:2016-05-10 summary:Due to the progressive growth of the amount of data available in a wide variety of scientific fields, it has become more difficult to ma- nipulate and analyze such information. Even though datasets have grown in size, the K-means algorithm remains as one of the most popular clustering methods, in spite of its dependency on the initial settings and high computational cost, especially in terms of distance computations. In this work, we propose an efficient approximation to the K-means problem intended for massive data. Our approach recursively partitions the entire dataset into a small number of sub- sets, each of which is characterized by its representative (center of mass) and weight (cardinality), afterwards a weighted version of the K-means algorithm is applied over such local representation, which can drastically reduce the number of distances computed. In addition to some theoretical properties, experimental results indicate that our method outperforms well-known approaches, such as the K-means++ and the minibatch K-means, in terms of the relation between number of distance computations and the quality of the approximation. version:1
arxiv-1605-02964 | Weakly Supervised Learning of Affordances | http://arxiv.org/abs/1605.02964 | id:1605.02964 author:Abhilash Srikantha, Juergen Gall category:cs.CV  published:2016-05-10 summary:Localizing functional regions of objects or affordances is an important aspect of scene understanding. In this work, we cast the problem of affordance segmentation as that of semantic image segmentation. In order to explore various levels of supervision, we introduce a pixel-annotated affordance dataset of 3090 images containing 9916 object instances with rich contextual information in terms of human-object interactions. We use a deep convolutional neural network within an expectation maximization framework to take advantage of weakly labeled data like image level annotations or keypoint annotations. We show that a further reduction in supervision is possible with a minimal loss in performance when human pose is used as context. version:1
arxiv-1605-02948 | A Bayesian Approach to Biomedical Text Summarization | http://arxiv.org/abs/1605.02948 | id:1605.02948 author:Milad Moradi, Nasser Ghadiri category:cs.CL cs.IR I.2.7; J.3  published:2016-05-10 summary:Many biomedical researchers and clinicians are faced with the information overload problem. Attaining desirable information from the ever-increasing body of knowledge is a difficult task without using automatic text summarization tools that help them to acquire the intended information in shorter time and with less effort. Although many text summarization methods have been proposed, developing domain-specific methods for the biomedical texts is a challenging task. In this paper, we propose a biomedical text summarization method, based on concept extraction technique and a novel sentence classification approach. We incorporate domain knowledge by utilizing the UMLS knowledge source and the na\"ive Bayes classifier to build our text summarizer. Unlike many existing methods, the system learns to classify the sentences without the need for training data, and selects them for the summary according to the distribution of essential concepts within the original text. We show that the use of critical concepts to represent the sentences as vectors of features, and classifying the sentences based on the distribution of those concepts, will improve the performance of automatic summarization. An extensive evaluation is performed on a collection of scientific articles in biomedical domain. The results show that our proposed method outperforms several well-known research-based, commercial and baseline summarizers according to the most commonly used ROUGE evaluation metrics. version:1
arxiv-1605-02917 | Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine | http://arxiv.org/abs/1605.02917 | id:1605.02917 author:Seyed Hamid Reza Mohammadi, Mohammad Ali Zare Chahooki category:cs.IR cs.LG  published:2016-05-10 summary:Search engines are the most important tools for web data acquisition. Web pages are crawled and indexed by search Engines. Users typically locate useful web pages by querying a search engine. One of the challenges in search engines administration is spam pages which waste search engine resources. These pages by deception of search engine ranking algorithms try to be showed in the first page of results. There are many approaches to web spam pages detection such as measurement of HTML code style similarity, pages linguistic pattern analysis and machine learning algorithm on page content features. One of the famous algorithms has been used in machine learning approach is Support Vector Machine (SVM) classifier. Recently basic structure of SVM has been changed by new extensions to increase robustness and classification accuracy. In this paper we improved accuracy of web spam detection by using two nonlinear kernels into Twin SVM (TSVM) as an improved extension of SVM. The classifier ability to data separation has been increased by using two separated kernels for each class of data. Effectiveness of new proposed method has been experimented with two publicly used spam datasets called UK-2007 and UK-2006. Results show the effectiveness of proposed kernelized version of TSVM in web spam page detection. version:1
arxiv-1605-02916 | Grammatical Case Based IS-A Relation Extraction with Boosting for Polish | http://arxiv.org/abs/1605.02916 | id:1605.02916 author:Paweł Łoziński, Dariusz Czerski, Mieczysław A. Kłopotek category:cs.CL cs.IR H.3.1  published:2016-05-10 summary:Pattern-based methods of IS-A relation extraction rely heavily on so called Hearst patterns. These are ways of expressing instance enumerations of a class in natural language. While these lexico-syntactic patterns prove quite useful, they may not capture all taxonomical relations expressed in text. Therefore in this paper we describe a novel method of IS-A relation extraction from patterns, which uses morpho-syntactical annotations along with grammatical case of noun phrases that constitute entities participating in IS-A relation. We also describe a method for increasing the number of extracted relations that we call pseudo-subclass boosting which has potential application in any pattern-based relation extraction method. Experiments were conducted on a corpus of about 0.5 billion web documents in Polish language. version:1
arxiv-1605-02914 | Recurrent Human Pose Estimation | http://arxiv.org/abs/1605.02914 | id:1605.02914 author:Vasileios Belagiannis, Andrew Zisserman category:cs.CV cs.NE  published:2016-05-10 summary:We propose a novel ConvNet model for predicting 2D human body poses in an image. The model regresses a heatmap representation for each body keypoint, and is able to learn and represent both the part appearances and the context of the part configuration. We make the following three contributions: (i) an architecture combining a feed forward module with a recurrent module, where the recurrent module can be run iteratively to improve the performance; (ii) the model can be trained end-to-end and from scratch, with auxiliary losses incorporated to improve performance; (iii) we investigate whether keypoint visibility can also be predicted. The model is evaluated on two benchmark datasets. The result is a simple architecture that achieves performance on par with the state of the art, but without the complexity of a graphical model stage (or layers). version:1
arxiv-1605-02892 | Compact Hash Codes for Efficient Visual Descriptors Retrieval in Large Scale Databases | http://arxiv.org/abs/1605.02892 | id:1605.02892 author:Simone Ercoli, Marco Bertini, Alberto Del Bimbo category:cs.CV  published:2016-05-10 summary:In this paper we present an efficient method for visual descriptors retrieval based on compact hash codes computed using a multiple k-means assignment. The method has been applied to the problem of approximate nearest neighbor (ANN) search of local and global visual content descriptors, and it has been tested on different datasets: three large scale public datasets of up to one billion descriptors (BIGANN) and, supported by recent progress in convolutional neural networks (CNNs), also on the CIFAR-10 and MNIST datasets. Experimental results show that, despite its simplicity, the proposed method obtains a very high performance that makes it superior to more complex state-of-the-art methods. version:1
arxiv-1605-02887 | Learning theory estimates with observations from general stationary stochastic processes | http://arxiv.org/abs/1605.02887 | id:1605.02887 author:Hanyuan Hang, Yunlong Feng, Ingo Steinwart, Johan A. K. Suykens category:stat.ML cs.LG  published:2016-05-10 summary:This paper investigates the supervised learning problem with observations drawn from certain general stationary stochastic processes. Here by \emph{general}, we mean that many stationary stochastic processes can be included. We show that when the stochastic processes satisfy a generalized Bernstein-type inequality, a unified treatment on analyzing the learning schemes with various mixing processes can be conducted and a sharp oracle inequality for generic regularized empirical risk minimization schemes can be established. The obtained oracle inequality is then applied to derive convergence rates for several learning schemes such as empirical risk minimization (ERM), least squares support vector machines (LS-SVMs) using given generic kernels, and SVMs using Gaussian kernels for both least squares and quantile regression. It turns out that for i.i.d.~processes, our learning rates for ERM recover the optimal rates. On the other hand, for non-i.i.d.~processes including geometrically $\alpha$-mixing Markov processes, geometrically $\alpha$-mixing processes with restricted decay, $\phi$-mixing processes, and (time-reversed) geometrically $\mathcal{C}$-mixing processes, our learning rates for SVMs with Gaussian kernels match, up to some arbitrarily small extra term in the exponent, the optimal rates. For the remaining cases, our rates are at least close to the optimal rates. As a by-product, the assumed generalized Bernstein-type inequality also provides an interpretation of the so-called "effective number of observations" for various mixing processes. version:1
arxiv-1605-02878 | Adaptive Combination of l0 LMS Adaptive Filters for Sparse System Identification in Fluctuating Noise Power | http://arxiv.org/abs/1605.02878 | id:1605.02878 author:Bijit Kumar Das, Mrityunjoy Chakraborty category:cs.IT cs.LG math.IT  published:2016-05-10 summary:Recently, the l0-least mean square (l0-LMS) algorithm has been proposed to identify sparse linear systems by employing a sparsity-promoting continuous function as an approximation of l0 pseudonorm penalty. However, the performance of this algorithm is sensitive to the appropriate choice of the some parameter responsible for the zero-attracting intensity. The optimum choice for this parameter depends on the signal-to-noise ratio (SNR) prevailing in the system. Thus, it becomes difficult to fix a suitable value for this parameter, particularly in a situation where SNR fluctuates over time. In this work, we propose several adaptive combinations of differently parameterized l0-LMS to get an overall satisfactory performance independent of the SNR, and discuss some issues relevant to these combination structures. We also demonstrate an efficient partial update scheme which not only reduces the number of computations per iteration, but also achieves some interesting performance gain compared with the full update case. Then, we propose a new recursive least squares (RLS)-type rule to update the combining parameter more efficiently. Finally, we extend the combination of two filters to a combination of M number adaptive filters, which manifests further improvement for M > 2. version:1
arxiv-1605-02877 | Performance Analysis of the Gradient Comparator LMS Algorithm | http://arxiv.org/abs/1605.02877 | id:1605.02877 author:Bijit Kumar Das, Mrityunjoy Chakraborty category:cs.IT cs.LG math.IT  published:2016-05-10 summary:The sparsity-aware zero attractor least mean square (ZA-LMS) algorithm manifests much lower misadjustment in strongly sparse environment than its sparsity-agnostic counterpart, the least mean square (LMS), but is shown to perform worse than the LMS when sparsity of the impulse response decreases. The reweighted variant of the ZA-LMS, namely RZA-LMS shows robustness against this variation in sparsity, but at the price of increased computational complexity. The other variants such as the l 0 -LMS and the improved proportionate normalized LMS (IPNLMS), though perform satisfactorily, are also computationally intensive. The gradient comparator LMS (GC-LMS) is a practical solution of this trade-off when hardware constraint is to be considered. In this paper, we analyse the mean and the mean square convergence performance of the GC-LMS algorithm in detail. The analyses satisfactorily match with the simulation results. version:1
arxiv-1605-02869 | Modeling Short Over-Dispersed Spike-Train Data: A Hierarchical Parametric Empirical Bayes Framework | http://arxiv.org/abs/1605.02869 | id:1605.02869 author:Qi She, Beth Jelfs, Rosa H. M. Chan category:q-bio.QM q-bio.NC stat.ML  published:2016-05-10 summary:In this letter, a Hierarchical Parametric Empirical Bayes (HPEB) model is proposed to fit spike count data. We have integrated Generalized Linear Models and empirical Bayes theory to simultaneously solve three problems: (1) over-dispersion of spike count values; (2) biased estimation of the maximum likelihood method and (3) difficulty in sampling from high-dimensional data with fully Bayes estimators. We apply the model to study both simulated data and experimental neural data from the retina. The simulation results indicate that the new model can estimate both the weights of connections among neural populations and the output firing rates efficiently and accurately. The results from the retinal datasets show that the proposed model outperforms both standard Poisson and Negative Binomial Generalized Linear Models in terms of the prediction log-likelihood of held-out datasets. version:1
arxiv-1605-01636 | Maximal Sparsity with Deep Networks? | http://arxiv.org/abs/1605.01636 | id:1605.01636 author:Bo Xin, Yizhou Wang, Wen Gao, David Wipf category:cs.LG  published:2016-05-05 summary:The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal $\ell_0$-norm representations in regimes where existing methods fail. The resulting system is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene. version:2
arxiv-1605-02178 | Fast and High-Quality Bilateral Filtering Using Gauss-Chebyshev Approximation | http://arxiv.org/abs/1605.02178 | id:1605.02178 author:Sanjay Ghosh, Kunal N. Chaudhury category:cs.CV  published:2016-05-07 summary:The bilateral filter is an edge-preserving smoother that has diverse applications in image processing, computer vision, computer graphics, and computational photography. The filter uses a spatial kernel along with a range kernel to perform edge-preserving smoothing. In this paper, we consider the Gaussian bilateral filter where both the kernels are Gaussian. A direct implementation of the Gaussian bilateral filter requires $O(\sigma_s^2)$ operations per pixel, where $\sigma_s$ is the standard deviation of the spatial Gaussian. In fact, it is well-known that the direct implementation is slow in practice. We present an approximation of the Gaussian bilateral filter, whereby we can cut down the number of operations to $O(1)$ per pixel for any arbitrary $\sigma_s$, and yet achieve very high-quality filtering that is almost indistinguishable from the output of the original filter. We demonstrate that the proposed approximation is few orders faster in practice compared to the direct implementation. We also demonstrate that the approximation is competitive with existing fast algorithms in terms of speed and accuracy. version:2
arxiv-1605-02832 | Decoding Stacked Denoising Autoencoders | http://arxiv.org/abs/1605.02832 | id:1605.02832 author:Sho Sonoda, Noboru Murata category:cs.LG stat.ML  published:2016-05-10 summary:Data representation in a stacked denoising autoencoder is investigated. Decoding is a simple technique for translating a stacked denoising autoencoder into a composition of denoising autoencoders in the ground space. In the infinitesimal limit, a composition of denoising autoencoders is reduced to a continuous denoising autoencoder, which is rich in analytic properties and geometric interpretation. For example, the continuous denoising autoencoder solves the backward heat equation and transports each data point so as to decrease entropy of the data distribution. Together with ridgelet analysis, an integral representation of a stacked denoising autoencoder is derived. version:1
arxiv-1603-06895 | A Selection of Giant Radio Sources from NVSS | http://arxiv.org/abs/1603.06895 | id:1603.06895 author:D. D. Proctor category:astro-ph.GA cs.CV stat.ML  published:2016-03-22 summary:Results of the application of pattern recognition techniques to the problem of identifying Giant Radio Sources (GRS) from the data in the NVSS catalog are presented and issues affecting the process are explored. Decision-tree pattern recognition software was applied to training set source pairs developed from known NVSS large angular size radio galaxies. The full training set consisted of 51,195 source pairs, 48 of which were known GRS for which each lobe was primarily represented by a single catalog component. The source pairs had a maximum separation of 20 arc minutes and a minimum component area of 1.87 square arc minutes at the 1.4 mJy level. The importance of comparing resulting probability distributions of the training and application sets for cases of unknown class ratio is demonstrated. The probability of correctly ranking a randomly selected (GRS, non-GRS) pair from the best of the tested classifiers was determined to be 97.8 +/- 1.5%. The best classifiers were applied to the over 870,000 candidate pairs from the entire catalog. Images of higher ranked sources were visually screened and a table of over sixteen hundred candidates, including morphological annotation, is presented. These systems include doubles and triples, Wide-Angle Tail (WAT) and Narrow-Angle Tail (NAT), S- or Z-shaped systems, and core-jets and resolved cores. While some resolved lobe systems are recovered with this technique, generally it is expected that such systems would require a different approach. version:2
arxiv-1506-07216 | Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality | http://arxiv.org/abs/1506.07216 | id:1506.07216 author:Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, David P. Woodruff category:cs.LG cs.CC cs.IT math.IT stat.ML  published:2015-06-24 summary:We study the tradeoff between the statistical error and communication cost of distributed statistical estimation problems in high dimensions. In the distributed sparse Gaussian mean estimation problem, each of the $m$ machines receives $n$ data points from a $d$-dimensional Gaussian distribution with unknown mean $\theta$ which is promised to be $k$-sparse. The machines communicate by message passing and aim to estimate the mean $\theta$. We provide a tight (up to logarithmic factors) tradeoff between the estimation error and the number of bits communicated between the machines. This directly leads to a lower bound for the distributed \textit{sparse linear regression} problem: to achieve the statistical minimax error, the total communication is at least $\Omega(\min\{n,d\}m)$, where $n$ is the number of observations that each machine receives and $d$ is the ambient dimension. These lower results improve upon [Sha14,SD'14] by allowing multi-round iterative communication model. We also give the first optimal simultaneous protocol in the dense case for mean estimation. As our main technique, we prove a \textit{distributed data processing inequality}, as a generalization of usual data processing inequalities, which might be of independent interest and useful for other problems. version:3
arxiv-1506-02640 | You Only Look Once: Unified, Real-Time Object Detection | http://arxiv.org/abs/1506.02640 | id:1506.02640 author:Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi category:cs.CV  published:2015-06-08 summary:We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset. version:5
arxiv-1603-05279 | XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks | http://arxiv.org/abs/1603.05279 | id:1603.05279 author:Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi category:cs.CV  published:2016-03-16 summary:We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32x memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.9% less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16% in top-1 accuracy. version:3
arxiv-1604-05266 | Finding Common Characteristics Among NBA Playoff and Championship Teams: A Machine Learning Approach | http://arxiv.org/abs/1604.05266 | id:1604.05266 author:Ikjyot Singh Kohli category:stat.ML stat.AP  published:2016-04-18 summary:In this paper, we employ machine learning techniques to analyze sixteen seasons of NBA regular season data from every team to determine the common characteristics among NBA playoff teams. Each team was characterized by 42 predictor variables and one binary response variable taking on a value of "TRUE" if a team had made the playoffs, and value of "FALSE" if a team had missed the playoffs. After fitting an initial classification tree to this problem, this tree was then pruned which decreased the test error rate. Further to this, a random forest of classification trees was grown which provided a very accurate model from which a variable importance plot was generated to determine which predictor variables had the greatest influence on the response variable. The result of this work was the conclusion that the most important factors in characterizing a team's playoff eligibility are the opponent field goal percentage and the opponent points per game. This seems to suggest that \emph{defensive} factors as opposed to offensive factors are the most important characteristics shared among NBA playoff teams. We also perform a classification analysis to determine common characteristics among NBA championship teams. Using an artificial neural network structure, we show that championship teams must be able to have very strong defensive characteristics, in particular, strong perimeter defense characteristics in combination with an effective half-court offense that generates high-percentage two-point shots. A key part of this offensive strategy must also be the ability to draw fouls. This analysis will hopefully dispel the rising notion that an offense geared towards shooting many three point shots is a sufficient and necessary condition for an NBA team to be successful in qualifying for the playoffs and winning a championship. version:3
arxiv-1406-4845 | Computer Vision Approach for Low Cost, High Precision Measurement of Grapevine Trunk Diameter in Outdoor Conditions | http://arxiv.org/abs/1406.4845 | id:1406.4845 author:Diego Sebastián Pérez, Facundo Bromberg, Francisco Gonzalez Antivilo category:cs.CV  published:2014-05-16 summary:Trunk diameter is a variable of agricultural interest, used mainly in the prediction of fruit trees production. It is correlated with leaf area and biomass of trees, and consequently gives a good estimate of the potential production of the plants. This work presents a low cost, high precision method for the measurement of trunk diameter of grapevines based on Computer Vision techniques. Several methods based on Computer Vision and other techniques are introduced in the literature. These methods present different advantages for crop management: they are amenable to be operated by unknowledgeable personnel, with lower operational costs; they result in lower stress levels to knowledgeable personnel, avoiding the deterioration of the measurement quality over time; and they make the measurement process amenable to be embedded in larger autonomous systems, allowing more measurements to be taken with equivalent costs. To date, all existing autonomous methods are either of low precision, or have a prohibitive cost for massive agricultural adoption, leaving the manual Vernier caliper or tape measure as the only choice in most situations. In this work we present a semi-autonomous measurement method that is susceptible to be fully automated, cost effective for mass adoption, and its precision is competitive (with slight improvements) over the caliper manual method. version:2
arxiv-1605-02784 | Identification of refugee influx patterns in Greece via model-theoretic analysis of daily arrivals | http://arxiv.org/abs/1605.02784 | id:1605.02784 author:Harris V. Georgiou category:stat.ML cs.CY  published:2016-05-09 summary:The refugee crisis is perhaps the single most challenging problem for Europe today. Hundreds of thousands of people have already traveled across dangerous sea passages from Turkish shores to Greek islands, resulting in thousands of dead and missing, despite the best rescue efforts from both sides. One of the main reasons is the total lack of any early warning-alerting system, which could provide some preparation time for the prompt and effective deployment of resources at the hot zones. This work is such an attempt for a systemic analysis of the refugee influx in Greece, aiming at (a) the statistical and signal-level characterization of the smuggling networks and (b) the formulation and preliminary assessment of such models for predictive purposes, i.e., as the basis of such an early warning-alerting protocol. To our knowledge, this is the first-ever attempt to design such a system, since this refugee crisis itself and its geographical properties are unique (intense event handling, little or no warning). The analysis employs a wide range of statistical, signal-based and matrix factorization (decomposition) techniques, including linear & linear-cosine regression, spectral analysis, ARMA, SVD, Probabilistic PCA, ICA, K-SVD for Dictionary Learning, as well as fractal dimension analysis. It is established that the behavioral patterns of the smuggling networks closely match (as expected) the regular burst and pause periods of store-and-forward networks in digital communications. There are also major periodic trends in the range of 6.2-6.5 days and strong correlations in lags of four or more days, with distinct preference in the Sunday-Monday 48-hour time frame. These results show that such models can be used successfully for short-term forecasting of the influx intensity, producing an invaluable operational asset for planners, decision-makers and first-responders. version:1
arxiv-1605-02775 | Image Classification of Grapevine Buds using Scale-Invariant Features Transform, Bag of Features and Support Vector Machines | http://arxiv.org/abs/1605.02775 | id:1605.02775 author:Diego Sebastián Pérez, Facundo Bromberg, Carlos Ariel Diaz category:cs.CV  published:2016-05-09 summary:In viticulture, there are several applications where bud detection in vineyard images is a necessary task, susceptible of being automated through the use of computer vision methods. A common and effective family of visual detection algorithms are the scanning-window type, that slide a (usually) fixed size window along the original image, classifying each resulting windowed-patch as containing or not containing the target object. The simplicity of these algorithms finds its most challenging aspect in the classification stage. Interested in grapevine buds detection in natural field conditions, this paper presents a classification method for images of grapevine buds ranging 100 to 1600 pixels in diameter, captured in outdoor, under natural field conditions, in winter (i.e., no grape bunches, very few leaves, and dormant buds), without artificial background, and with minimum equipment requirements. The proposed method uses well-known computer vision technologies: Scale-Invariant Feature Transform for calculating low-level features, Bag of Features for building an image descriptor, and Support Vector Machines for training a classifier. When evaluated over images containing buds of at least 100 pixels in diameter, the approach achieves a recall higher than 0.9 and a precision of 0.86 over all windowed-patches covering the whole bud and down to 60% of it, and scaled up to window patches containing a proportion of 20%-80% of bud versus background pixels. This robustness on the position and size of the window demonstrates its viability for use as the classification stage in a scanning-window detection algorithms. version:1
arxiv-1605-02720 | Anytime Bi-Objective Optimization with a Hybrid Multi-Objective CMA-ES (HMO-CMA-ES) | http://arxiv.org/abs/1605.02720 | id:1605.02720 author:Ilya Loshchilov, Tobias Glasmachers category:cs.NE  published:2016-05-09 summary:We propose a multi-objective optimization algorithm aimed at achieving good anytime performance over a wide range of problems. Performance is assessed in terms of the hypervolume metric. The algorithm called HMO-CMA-ES represents a hybrid of several old and new variants of CMA-ES, complemented by BOBYQA as a warm start. We benchmark HMO-CMA-ES on the recently introduced bi-objective problem suite of the COCO framework (COmparing Continuous Optimizers), consisting of 55 scalable continuous optimization problems, which is used by the Black-Box Optimization Benchmarking (BBOB) Workshop 2016. version:1
arxiv-1605-02711 | Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning | http://arxiv.org/abs/1605.02711 | id:1605.02711 author:Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, Jarvis Haupt category:cs.LG math.OC stat.ML  published:2016-05-09 summary:We propose a stochastic variance reduced optimization algorithm for solving a class of large-scale nonconvex optimization problems with cardinality constraints, and provide sufficient conditions under which the proposed algorithm enjoys strong linear convergence guarantees and optimal estimation accuracy in high dimensions. We further extend our analysis to an asynchronous variant of the approach, and demonstrate a near linear speedup in sparse settings. Numerical experiments demonstrate the efficiency of our method in terms of both parameter estimation and computational performance. version:1
arxiv-1605-02699 | A Theoretical Analysis of Deep Neural Networks for Texture Classification | http://arxiv.org/abs/1605.02699 | id:1605.02699 author:Saikat Basu, Manohar Karki, Robert DiBiano, Supratik Mukhopadhyay, Sangram Ganguly, Ramakrishna Nemani, Shreekant Gayaka category:cs.CV cs.LG stat.ML  published:2016-05-09 summary:We investigate the use of Deep Neural Networks for the classification of image datasets where texture features are important for generating class-conditional discriminative representations. To this end, we first derive the size of the feature space for some standard textural features extracted from the input dataset and then use the theory of Vapnik-Chervonenkis dimension to show that hand-crafted feature extraction creates low-dimensional representations which help in reducing the overall excess error rate. As a corollary to this analysis, we derive for the first time upper bounds on the VC dimension of Convolutional Neural Network as well as Dropout and Dropconnect networks and the relation between excess error rate of Dropout and Dropconnect networks. The concept of intrinsic dimension is used to validate the intuition that texture-based datasets are inherently higher dimensional as compared to handwritten digits or other object recognition datasets and hence more difficult to be shattered by neural networks. We then derive the mean distance from the centroid to the nearest and farthest sampling points in an n-dimensional manifold and show that the Relative Contrast of the sample data vanishes as dimensionality of the underlying vector space tends to infinity. version:1
arxiv-1605-02697 | Ask Your Neurons: A Deep Learning Approach to Visual Question Answering | http://arxiv.org/abs/1605.02697 | id:1605.02697 author:Mateusz Malinowski, Marcus Rohrbach, Mario Fritz category:cs.CV cs.AI cs.CL  published:2016-05-09 summary:We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, end-to-end formulation to this problem. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language inputs (image and question). We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extend the original DAQUAR dataset to DAQUAR-Consensus. Moreover, we also extend our analysis to VQA, a large-scale question answering about images dataset, where we investigate some particular design choices and show the importance of stronger visual models. At the same time, we achieve strong performance of our model that still uses a global image representation. Finally, based on such analysis, we refine our Ask Your Neurons on DAQUAR, which also leads to a better performance on this challenging task. version:1
arxiv-1605-02693 | Inference of High-dimensional Autoregressive Generalized Linear Models | http://arxiv.org/abs/1605.02693 | id:1605.02693 author:Eric C. Hall, Garvesh Raskutti, Rebecca Willett category:stat.ML cs.IT math.IT math.ST stat.TH  published:2016-05-09 summary:Vector autoregressive models characterize a variety of time series in which linear combinations of current and past observations can be used to accurately predict future observations. For instance, each element of an observation vector could correspond to a different node in a network, and the parameters of an autoregressive model would correspond to the impact of the network structure on the time series evolution. Often these models are used successfully in practice to learn the structure of social, epidemiological, financial, or biological neural networks. However, little is known about statistical guarantees of estimates of such models in non-Gaussian settings. This paper addresses the inference of the autoregressive parameters and associated network structure within a generalized linear model framework that includes Poisson and Bernoulli autoregressive processes. At the heart of this analysis is a sparsity-regularized maximum likelihood estimator. While sparsity-regularization is well-studied in the statistics and machine learning communities, those analysis methods cannot be applied to autoregressive generalized linear models because of the correlations and potential heteroscedasticity inherent in the observations. Sample complexity bounds are derived using a combination of martingale concentration inequalities and modified covering techniques originally proposed for high-dimensional linear regression analysis. These bounds, which are supported by several simulation studies, characterize the impact of various network parameters on estimator performance. version:1
arxiv-1605-02688 | Theano: A Python framework for fast computation of mathematical expressions | http://arxiv.org/abs/1605.02688 | id:1605.02688 author:The Theano Development Team, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Frédéric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, Yoshua Bengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, Nicolas Bouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Brébisson, Olivier Breuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooijmans, Marc-Alexandre Côté, Myriam Côté, Aaron Courville, Yann N. Dauphin, Olivier Delalleau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, Mélanie Ducoffe, Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, Mathieu Germain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, Iban Harlouchet, Jean-Philippe Heng, Balázs Hidasi, Sina Honari, Arjun Jain, Sébastien Jean, Kai Jia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, César Laurent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas Léonard, Zhouhan Lin, Jesse A. Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropietro, Robert T. McGibbon, Roland Memisevic, Bart van Merriënboer, Vincent Michalski, Mehdi Mirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel, Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Salvatier, François Savard, Jan Schlüter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban, Dmitriy Serdyuk, Samira Shabanian, Étienne Simon, Sigurd Spieckermann, S. Ramana Subramanyam, Jakub Sygnowski, Jérémie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban, Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J. Webb, Matthew Willson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, Ying Zhang category:cs.SC cs.LG cs.MS  published:2016-05-09 summary:Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it. version:1
arxiv-1605-02677 | Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark | http://arxiv.org/abs/1605.02677 | id:1605.02677 author:Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang category:cs.AI cs.CV  published:2016-05-09 summary:Psychological research results have confirmed that people can have different emotional reactions to different visual stimuli. Several papers have been published on the problem of visual emotion analysis. In particular, attempts have been made to analyze and predict people's emotional reaction towards images. To this end, different kinds of hand-tuned features are proposed. The results reported on several carefully selected and labeled small image data sets have confirmed the promise of such features. While the recent successes of many computer vision related tasks are due to the adoption of Convolutional Neural Networks (CNNs), visual emotion analysis has not achieved the same level of success. This may be primarily due to the unavailability of confidently labeled and relatively large image data sets for visual emotion analysis. In this work, we introduce a new data set, which started from 3+ million weakly labeled images of different emotions and ended up 30 times as large as the current largest publicly available visual emotion data set. We hope that this data set encourages further research on visual emotion analysis. We also perform extensive benchmarking analyses on this large data set using the state of the art methods including CNNs. version:1
arxiv-1604-01431 | A Game-Theoretic Approach to Multi-Pedestrian Activity Forecasting | http://arxiv.org/abs/1604.01431 | id:1604.01431 author:Wei-Chiu Ma, De-An Huang, Namhoon Lee, Kris M. Kitani category:cs.CV  published:2016-04-05 summary:We develop predictive models of pedestrian dynamics by encoding the coupled nature of multi-pedestrian interaction using game theory, and deep learning-based visual analysis to estimate person-specific behavior parameters. Building predictive models for multi-pedestrian interactions however, is very challenging due to two reasons: (1) the dynamics of interaction are complex interdependent processes, where the predicted behavior of one pedestrian can affect the actions taken by others and (2) dynamics are variable depending on an individuals physical characteristics (e.g., an older person may walk slowly while the younger person may walk faster). To address these challenges, we (1) utilize concepts from game theory to model the interdependent decision making process of multiple pedestrians and (2) use visual classifiers to learn a mapping from pedestrian appearance to behavior parameters. We evaluate our proposed model on several public multiple pedestrian interaction video datasets. Results show that our strategic planning model explains human interactions 25% better when compared to state-of-the-art methods. version:2
arxiv-1605-02674 | Why (and How) Avoid Orthogonal Procrustes in Regularized Multivariate Analysis | http://arxiv.org/abs/1605.02674 | id:1605.02674 author:Sergio Muñoz-Romero, Vanessa Gómez-Verdejo, Jerónimo Arenas-García category:stat.ML  published:2016-05-09 summary:Multivariate Analysis (MVA) comprises a family of well-known methods for feature extraction that exploit correlations among input variables of the data representation. One important property that is enjoyed by most such methods is uncorrelation among the extracted features. Recently, regularized versions of MVA methods have appeared in the literature, mainly with the goal to gain interpretability of the solution. In these cases, the solutions can no longer be obtained in a closed manner, and it is frequent to recur to the iteration of two steps, one of them being an orthogonal Procrustes problem. This letter shows that the Procrustes solution is not optimal from the perspective of the overall MVA method, and proposes an alternative approach based on the solution of an eigenvalue problem. Our method ensures the preservation of several properties of the original methods, most notably the uncorrelation of the extracted features, as demonstrated theoretically and through a collection of selected experiments. version:1
arxiv-1407-8032 | Population fluctuation promotes cooperation in networks | http://arxiv.org/abs/1407.8032 | id:1407.8032 author:Steve Miller, Joshua Knowles category:cs.GT cs.NE physics.soc-ph  published:2014-07-30 summary:We consider the problem of explaining the emergence and evolution of cooperation in dynamic network-structured populations. Building on seminal work by Poncela et al, which shows how cooperation (in one-shot prisoner's dilemma) is supported in growing populations by an evolutionary preferential attachment (EPA) model, we investigate the effect of fluctuations in the population size. We find that the fluctuating model is more robust than Poncela et al's in that cooperation flourishes for a wide variety of initial conditions. In terms of both the temptation to defect, and the types of strategies present in the founder network, the fluctuating population is found to lead more securely to cooperation. Further, we find that this model will also support the emergence of cooperation from pre-existing non-cooperative random networks. This model, like Poncela et al's, does not require agents to have memory, recognition of other agents, or other cognitive abilities, and so may suggest a more general explanation of the emergence of cooperation in early evolutionary transitions, than mechanisms such as kin selection, direct and indirect reciprocity. version:4
arxiv-1601-04366 | Learning the kernel matrix via predictive low-rank approximations | http://arxiv.org/abs/1601.04366 | id:1601.04366 author:Martin Stražar, Tomaž Curk category:cs.LG stat.ML  published:2016-01-17 summary:Efficient and accurate low-rank approximations of multiple data sources are essential in the era of big data. The scaling of kernel-based learning algorithms to large datasets is limited by the O(n^2) computation and storage complexity of the full kernel matrix, which is required by most of the recent kernel learning algorithms. We present the Mklaren algorithm to approximate multiple kernel matrices learn a regression model, which is entirely based on geometrical concepts. The algorithm does not require access to full kernel matrices yet it accounts for the correlations between all kernels. It uses Incomplete Cholesky decomposition, where pivot selection is based on least-angle regression in the combined, low-dimensional feature space. The algorithm has linear complexity in the number of data points and kernels. When explicit feature space induced by the kernel can be constructed, a mapping from the dual to the primal Ridge regression weights is used for model interpretation. The Mklaren algorithm was tested on eight standard regression datasets. It outperforms contemporary kernel matrix approximation approaches when learning with multiple kernels. It identifies relevant kernels, achieving highest explained variance than other multiple kernel learning methods for the same number of iterations. Test accuracy, equivalent to the one using full kernel matrices, was achieved with at significantly lower approximation ranks. A difference in run times of two orders of magnitude was observed when either the number of samples or kernels exceeds 3000. version:2
arxiv-1605-02633 | Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering | http://arxiv.org/abs/1605.02633 | id:1605.02633 author:Chong You, Chun-Guang Li, Daniel P. Robinson, Rene Vidal category:cs.LG cs.CV stat.ML  published:2016-05-09 summary:State-of-the-art subspace clustering methods are based on expressing each data point as a linear combination of other data points while regularizing the matrix of coefficients with $\ell_1$, $\ell_2$ or nuclear norms. $\ell_1$ regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad theoretical conditions, but the clusters may not be connected. $\ell_2$ and nuclear norm regularization often improve connectivity, but give a subspace-preserving affinity only for independent subspaces. Mixed $\ell_1$, $\ell_2$ and nuclear norm regularizations offer a balance between the subspace-preserving and connectedness properties, but this comes at the cost of increased computational complexity. This paper studies the geometry of the elastic net regularizer (a mixture of the $\ell_1$ and $\ell_2$ norms) and uses it to derive a provably correct and scalable active set method for finding the optimal coefficients. Our geometric analysis also provides a theoretical justification and a geometric interpretation for the balance between the connectedness (due to $\ell_2$ regularization) and subspace-preserving (due to $\ell_1$ regularization) properties for elastic net subspace clustering. Our experiments show that the proposed active set method not only achieves state-of-the-art clustering performance, but also efficiently handles large-scale datasets. version:1
arxiv-1605-02619 | On the Emergence of Shortest Paths by Reinforced Random Walks | http://arxiv.org/abs/1605.02619 | id:1605.02619 author:Daniel R. Figueiredo, Michele Garetto category:cs.NE physics.bio-ph  published:2016-05-09 summary:The co-evolution between network structure and functional performance is a fundamental and challenging problem whose complexity emerges from the intrinsic interdependent nature of structure and function. Within this context, we investigate the interplay between the efficiency of network navigation (i.e., path lengths) and network structure (i.e., edge weights). We propose a simple and tractable model based on iterative biased random walks where edge weights increase over time as function of the traversed path length. Under mild assumptions, we prove that biased random walks will eventually only traverse shortest paths in their journey towards the destination. We further characterize the transient regime proving that the probability to traverse non-shortest paths decays according to a power-law. We also highlight various properties in this dynamic, such as the trade-off between exploration and convergence, and preservation of initial network plasticity. We believe the proposed model and results can be of interest to various domains where biased random walks and decentralized navigation have been applied. version:1
arxiv-1605-02609 | Dynamic Decomposition of Spatiotemporal Neural Signals | http://arxiv.org/abs/1605.02609 | id:1605.02609 author:Luca Ambrogioni, Marcel A. J. van Gerven, Eric Maris category:q-bio.NC stat.ML  published:2016-05-09 summary:Neural signals are characterized by rich temporal and spatiotemporal dynamics that reflect the organization of cortical networks. Theoretical research has shown how neural networks can operate at different dynamic ranges that correspond to specific types of information processing. Here we present a data analysis framework that uses a linearized model of these dynamic states in order to decompose the measured neural signal into a series of components that capture both rhythmic and non-rhythmic neural activity. The method is based on stochastic differential equations and Gaussian process regression. Through computer simulations and analysis of magnetoencephalographic data, we demonstrate the efficacy of the method in identifying meaningful modulations of oscillatory signals corrupted by structured temporal and spatiotemporal noise. These results suggest that the method is particularly suitable for the analysis and interpretation of complex temporal and spatiotemporal neural signals. version:1
arxiv-1404-6580 | Multitask Learning for Sequence Labeling Tasks | http://arxiv.org/abs/1404.6580 | id:1404.6580 author:Arvind Agarwal, Saurabh Kataria category:cs.LG  published:2014-04-25 summary:In this paper, we present a learning method for sequence labeling tasks in which each example sequence has multiple label sequences. Our method learns multiple models, one model for each label sequence. Each model computes the joint probability of all label sequences given the example sequence. Although each model considers all label sequences, its primary focus is only one label sequence, and therefore, each model becomes a task-specific model, for the task belonging to that primary label. Such multiple models are learned {\it simultaneously} by facilitating the learning transfer among models through {\it explicit parameter sharing}. We experiment the proposed method on two applications and show that our method significantly outperforms the state-of-the-art method. version:2
arxiv-1605-02592 | GLEU Without Tuning | http://arxiv.org/abs/1605.02592 | id:1605.02592 author:Courtney Napoles, Keisuke Sakaguchi, Matt Post, Joel Tetreault category:cs.CL  published:2016-05-09 summary:The GLEU metric was proposed for evaluating grammatical error corrections using n-gram overlap with a set of reference sentences, as opposed to precision/recall of specific annotated errors (Napoles et al., 2015). This paper describes improvements made to the GLEU metric that address problems that arise when using an increasing number of reference sets. Unlike the originally presented metric, the modified metric does not require tuning. We recommend that this version be used instead of the original version. version:1
arxiv-1605-02560 | Studying the brain from adolescence to adulthood through sparse multi-view matrix factorisations | http://arxiv.org/abs/1605.02560 | id:1605.02560 author:Zi Wang, Vyacheslav Karolis, Chiara Nosarti, Giovanni Montana category:stat.AP cs.CV q-bio.NC  published:2016-05-09 summary:Men and women differ in specific cognitive abilities and in the expression of several neuropsychiatric conditions. Such findings could be attributed to sex hormones, brain differences, as well as a number of environmental variables. Existing research on identifying sex-related differences in brain structure have predominantly used cross-sectional studies to investigate, for instance, differences in average gray matter volumes (GMVs). In this article we explore the potential of a recently proposed multi-view matrix factorisation (MVMF) methodology to study structural brain changes in men and women that occur from adolescence to adulthood. MVMF is a multivariate variance decomposition technique that extends principal component analysis to "multi-view" datasets, i.e. where multiple and related groups of observations are available. In this application, each view represents a different age group. MVMF identifies latent factors explaining shared and age-specific contributions to the observed overall variability in GMVs over time. These latent factors can be used to produce low-dimensional visualisations of the data that emphasise age-specific effects once the shared effects have been accounted for. The analysis of two datasets consisting of individuals born prematurely as well as healthy controls provides evidence to suggest that the separation between males and females becomes increasingly larger as the brain transitions from adolescence to adulthood. We report on specific brain regions associated to these variance effects. version:1
arxiv-1605-02559 | Robust imaging of hippocampal inner structure at 7T: in vivo acquisition protocol and methodological choices | http://arxiv.org/abs/1605.02559 | id:1605.02559 author:Linda Marrakchi-Kacem, Alexandre Vignaud, Julien Sein, Johanne Germain, Thomas R Henry, Cyril Poupon, Lucie Hertz-Pannier, Stéphane Lehéricy, Olivier Colliot, Pierre-François Van de Moortele, Marie Chupin category:cs.CV  published:2016-05-09 summary:OBJECTIVE:Motion-robust multi-slab imaging of hippocampal inner structure in vivo at 7T.MATERIALS AND METHODS:Motion is a crucial issue for ultra-high resolution imaging, such as can be achieved with 7T MRI. An acquisition protocol was designed for imaging hippocampal inner structure at 7T. It relies on a compromise between anatomical details visibility and robustness to motion. In order to reduce acquisition time and motion artifacts, the full slab covering the hippocampus was split into separate slabs with lower acquisition time. A robust registration approach was implemented to combine the acquired slabs within a final 3D-consistent high-resolution slab covering the whole hippocampus. Evaluation was performed on 50 subjects overall, made of three groups of subjects acquired using three acquisition settings; it focused on three issues: visibility of hippocampal inner structure, robustness to motion artifacts and registration procedure performance.RESULTS:Overall, T2-weighted acquisitions with interleaved slabs proved robust. Multi-slab registration yielded high quality datasets in 96 % of the subjects, thus compatible with further analyses of hippocampal inner structure.CONCLUSION:Multi-slab acquisition and registration setting is efficient for reducing acquisition time and consequently motion artifacts for ultra-high resolution imaging of the inner structure of the hippocampus. version:1
arxiv-1605-02541 | Mean Absolute Percentage Error for regression models | http://arxiv.org/abs/1605.02541 | id:1605.02541 author:Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, Fabrice Rossi category:stat.ML  published:2016-05-09 summary:We study in this paper the consequences of using the Mean Absolute Percentage Error (MAPE) as a measure of quality for regression models. We prove the existence of an optimal MAPE model and we show the universal consistency of Empirical Risk Minimization based on the MAPE. We also show that finding the best model under the MAPE is equivalent to doing weighted Mean Absolute Error (MAE) regression, and we apply this weighting strategy to kernel regression. The behavior of the MAPE kernel regression is illustrated on simulated data. version:1
arxiv-1605-02540 | Exact ICL maximization in a non-stationary temporal extension of the stochastic block model for dynamic networks | http://arxiv.org/abs/1605.02540 | id:1605.02540 author:Marco Corneli, Pierre Latouche, Fabrice Rossi category:stat.ML stat.AP  published:2016-05-09 summary:The stochastic block model (SBM) is a flexible probabilistic tool that can be used to model interactions between clusters of nodes in a network. However, it does not account for interactions of time varying intensity between clusters. The extension of the SBM developed in this paper addresses this shortcoming through a temporal partition: assuming interactions between nodes are recorded on fixed-length time intervals, the inference procedure associated with the model we propose allows to cluster simultaneously the nodes of the network and the time intervals. The number of clusters of nodes and of time intervals, as well as the memberships to clusters, are obtained by maximizing an exact integrated complete-data likelihood, relying on a greedy search approach. Experiments on simulated and real data are carried out in order to assess the proposed methodology. version:1
arxiv-1605-02531 | Clustering Time Series and the Surprising Robustness of HMMs | http://arxiv.org/abs/1605.02531 | id:1605.02531 author:Mark Kozdoba, Shie Mannor category:cs.IT cs.LG math.IT stat.ML  published:2016-05-09 summary:Suppose that you are given a time series where consecutive samples are believed to come from a probabilistic source, and that the source changes from time to time. Your objective is to learn the distribution of each source and to cluster the samples according to the source that generated them. A standard approach to this problem is to model the data as a hidden Markov model (HMM). However, due to the Markov property and stationarity of HMMs, simple examples can be given where this approach yields poor results for the clustering. We propose a more general, non-stationary model of the data, where the only restriction is that the sources can not change too often. Even though the model governing the sources may not be Markovian, we show that that a maximum likelihood HMM estimator can still be used. Specifically, we show that a maximum-likelihood HMM estimator produces the correct second moment of the data, and the results can be extended to higher moments. In contrast to the existing consistency and misspecification results involving maximum likelihood for HMMs, our approach yields bounds for finite sample sizes. version:1
arxiv-1605-02486 | Efficiency Evaluation of Character-level RNN Training Schedules | http://arxiv.org/abs/1605.02486 | id:1605.02486 author:Cedric De Boom, Sam Leroux, Steven Bohez, Pieter Simoens, Thomas Demeester, Bart Dhoedt category:cs.NE  published:2016-05-09 summary:We present four training and prediction schedules from the same character-level recurrent neural network. The efficiency of these schedules is tested in terms of model effectiveness as a function of training time and amount of training data seen. We show that the choice of training and prediction schedule potentially has a considerable impact on the prediction effectiveness for a given training budget. version:1
arxiv-1412-6574 | Visual Instance Retrieval with Deep Convolutional Networks | http://arxiv.org/abs/1412.6574 | id:1412.6574 author:Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson, Atsuto Maki category:cs.CV  published:2014-12-20 summary:This paper provides an extensive study on the availability of image representations based on convolutional networks (ConvNets) for the task of visual instance retrieval. Besides the choice of convolutional layers, we present an efficient pipeline exploiting multi-scale schemes to extract local features, in particular, by taking geometric invariance into explicit account, i.e. positions, scales and spatial consistency. In our experiments using five standard image retrieval datasets, we demonstrate that generic ConvNet image representations can outperform other state-of-the-art methods if they are extracted appropriately. version:4
arxiv-1605-02470 | Randomized Kaczmarz for Rank Aggregation from Pairwise Comparisons | http://arxiv.org/abs/1605.02470 | id:1605.02470 author:Vivek S. Borkar, Nikhil Karamchandani, Sharad Mirani category:cs.LG stat.ML  published:2016-05-09 summary:We revisit the problem of inferring the overall ranking among entities in the framework of Bradley-Terry-Luce (BTL) model, based on available empirical data on pairwise preferences. By a simple transformation, we can cast the problem as that of solving a noisy linear system, for which a ready algorithm is available in the form of the randomized Kaczmarz method. This scheme is provably convergent, has excellent empirical performance, and is amenable to on-line, distributed and asynchronous variants. Convergence, convergence rate, and error analysis of the proposed algorithm are presented and several numerical experiments are conducted whose results validate our theoretical findings. version:1
arxiv-1605-02464 | Orientation Driven Bag of Appearances for Person Re-identification | http://arxiv.org/abs/1605.02464 | id:1605.02464 author:Liqian Ma, Hong Liu, Liang Hu, Can Wang, Qianru Sun category:cs.CV  published:2016-05-09 summary:Person re-identification (re-id) consists of associating individual across camera network, which is valuable for intelligent video surveillance and has drawn wide attention. Although person re-identification research is making progress, it still faces some challenges such as varying poses, illumination and viewpoints. For feature representation in re-identification, existing works usually use low-level descriptors which do not take full advantage of body structure information, resulting in low representation ability. %discrimination. To solve this problem, this paper proposes the mid-level body-structure based feature representation (BSFR) which introduces body structure pyramid for codebook learning and feature pooling in the vertical direction of human body. Besides, varying viewpoints in the horizontal direction of human body usually causes the data missing problem, $i.e.$, the appearances obtained in different orientations of the identical person could vary significantly. To address this problem, the orientation driven bag of appearances (ODBoA) is proposed to utilize person orientation information extracted by orientation estimation technic. To properly evaluate the proposed approach, we introduce a new re-identification dataset (Market-1203) based on the Market-1501 dataset and propose a new re-identification dataset (PKU-Reid). Both datasets contain multiple images captured in different body orientations for each person. Experimental results on three public datasets and two proposed datasets demonstrate the superiority of the proposed approach, indicating the effectiveness of body structure and orientation information for improving re-identification performance. version:1
arxiv-1605-02460 | Fuzzy Clustering Based Segmentation Of Vertebrae in T1-Weighted Spinal MR Images | http://arxiv.org/abs/1605.02460 | id:1605.02460 author:Jiyo. S. Athertya, G. Saravana Kumar category:cs.CV  published:2016-05-09 summary:Image segmentation in the medical domain is a challenging field owing to poor resolution and limited contrast. The predominantly used conventional segmentation techniques and the thresholding methods suffer from limitations because of heavy dependence on user interactions. Uncertainties prevalent in an image cannot be captured by these techniques. The performance further deteriorates when the images are corrupted by noise, outliers and other artifacts. The objective of this paper is to develop an effective robust fuzzy C- means clustering for segmenting vertebral body from magnetic resonance image owing to its unsupervised form of learning. The motivation for this work is detection of spine geometry and proper localisation and labelling will enhance the diagnostic output of a physician. The method is compared with Otsu thresholding and K-means clustering to illustrate the robustness.The reference standard for validation was the annotated images from the radiologist, and the Dice coefficient and Hausdorff distance measures were used to evaluate the segmentation. version:1
arxiv-1605-02457 | The Controlled Natural Language of Randall Munroe's Thing Explainer | http://arxiv.org/abs/1605.02457 | id:1605.02457 author:Tobias Kuhn category:cs.CL  published:2016-05-09 summary:It is rare that texts or entire books written in a Controlled Natural Language (CNL) become very popular, but exactly this has happened with a book that has been published last year. Randall Munroe's Thing Explainer uses only the 1'000 most often used words of the English language together with drawn pictures to explain complicated things such as nuclear reactors, jet engines, the solar system, and dishwashers. This restricted language is a very interesting new case for the CNL community. I describe here its place in the context of existing approaches on Controlled Natural Languages, and I provide a first analysis from a scientific perspective, covering the word production rules and word distributions. version:1
arxiv-1510-00503 | A Bayesian approach to constrained single- and multi-objective optimization | http://arxiv.org/abs/1510.00503 | id:1510.00503 author:Paul Feliot, Julien Bect, Emmanuel Vazquez category:stat.CO stat.ML  published:2015-10-02 summary:This article addresses the problem of derivative-free (single- or multi-objective) optimization subject to multiple inequality constraints. Both the objective and constraint functions are assumed to be smooth, non-linear and expensive to evaluate. As a consequence, the number of evaluations that can be used to carry out the optimization is very limited, as in complex industrial design optimization problems. The method we propose to overcome this difficulty has its roots in both the Bayesian and the multi-objective optimization literatures. More specifically, an extended domination rule is used to handle objectives and constraints in a unified way, and a corresponding expected hyper-volume improvement sampling criterion is proposed. This new criterion is naturally adapted to the search of a feasible point when none is available, and reduces to existing Bayesian sampling criteria---the classical Expected Improvement (EI) criterion and some of its constrained/multi-objective extensions---as soon as at least one feasible point is available. The calculation and optimization of the criterion are performed using Sequential Monte Carlo techniques. In particular, an algorithm similar to the subset simulation method, which is well known in the field of structural reliability, is used to estimate the criterion. The method, which we call BMOO (for Bayesian Multi-Objective Optimization), is compared to state-of-the-art algorithms for single- and multi-objective constrained optimization. version:3
arxiv-1605-02442 | Machine Learning Techniques with Ontology for Subjective Answer Evaluation | http://arxiv.org/abs/1605.02442 | id:1605.02442 author:M. Syamala Devi, Himani Mittal category:cs.AI cs.CL cs.IR I.2.7  published:2016-05-09 summary:Computerized Evaluation of English Essays is performed using Machine learning techniques like Latent Semantic Analysis (LSA), Generalized LSA, Bilingual Evaluation Understudy and Maximum Entropy. Ontology, a concept map of domain knowledge, can enhance the performance of these techniques. Use of Ontology makes the evaluation process holistic as presence of keywords, synonyms, the right word combination and coverage of concepts can be checked. In this paper, the above mentioned techniques are implemented both with and without Ontology and tested on common input data consisting of technical answers of Computer Science. Domain Ontology of Computer Graphics is designed and developed. The software used for implementation includes Java Programming Language and tools such as MATLAB, Prot\'eg\'e, etc. Ten questions from Computer Graphics with sixty answers for each question are used for testing. The results are analyzed and it is concluded that the results are more accurate with use of Ontology. version:1
arxiv-1605-02424 | Learning Discriminative Features with Class Encoder | http://arxiv.org/abs/1605.02424 | id:1605.02424 author:Hailin Shi, Xiangyu Zhu, Zhen Lei, Shengcai Liao, Stan Z. Li category:cs.CV  published:2016-05-09 summary:Deep neural networks usually benefit from unsupervised pre-training, e.g. auto-encoders. However, the classifier further needs supervised fine-tuning methods for good discrimination. Besides, due to the limits of full-connection, the application of auto-encoders is usually limited to small, well aligned images. In this paper, we incorporate the supervised information to propose a novel formulation, namely class-encoder, whose training objective is to reconstruct a sample from another one of which the labels are identical. Class-encoder aims to minimize the intra-class variations in the feature space, and to learn a good discriminative manifolds on a class scale. We impose the class-encoder as a constraint into the softmax for better supervised training, and extend the reconstruction on feature-level to tackle the parameter size issue and translation issue. The experiments show that the class-encoder helps to improve the performance on benchmarks of classification and face recognition. This could also be a promising direction for fast training of face recognition models. version:1
arxiv-1605-02408 | Structured Nonconvex and Nonsmooth Optimization: Algorithms and Iteration Complexity Analysis | http://arxiv.org/abs/1605.02408 | id:1605.02408 author:Bo Jiang, Tianyi Lin, Shiqian Ma, Shuzhong Zhang category:math.OC cs.LG stat.ML  published:2016-05-09 summary:Nonconvex optimization problems are frequently encountered in much of statistics, business, science and engineering, but they are not yet widely recognized as a technology. A reason for this relatively low degree of popularity is the lack of a well developed system of theory and algorithms to support the applications, as is the case for its convex counterpart. This paper aims to take one step in the direction of disciplined nonconvex optimization. In particular, we consider in this paper some constrained nonconvex optimization models in block decision variables, with or without coupled affine constraints. In the case of no coupled constraints, we show a sublinear rate of convergence to an $\epsilon$-stationary solution in the form of variational inequality for a generalized conditional gradient method, where the convergence rate is shown to be dependent on the H\"olderian continuity of the gradient of the smooth part of the objective. For the model with coupled affine constraints, we introduce corresponding $\epsilon$-stationarity conditions, and propose two proximal-type variants of the ADMM to solve such a model, assuming the proximal ADMM updates can be implemented for all the block variables except for the last block, for which either a gradient step or a majorization-minimization step is implemented. We show an iteration complexity bound of $O(1/\epsilon^2)$ to reach an $\epsilon$-stationary solution for both algorithms. Moreover, we show that the same iteration complexity of a proximal BCD method follows immediately. Numerical results are provided to illustrate the efficacy of the proposed algorithms for tensor robust PCA. version:1
arxiv-1512-03965 | The Power of Depth for Feedforward Neural Networks | http://arxiv.org/abs/1512.03965 | id:1512.03965 author:Ronen Eldan, Ohad Shamir category:cs.LG cs.NE stat.ML  published:2015-12-12 summary:We show that there is a simple (approximately radial) function on $\reals^d$, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for virtually all known activation functions, including rectified linear units, sigmoids and thresholds, and formally demonstrates that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different. version:4
arxiv-1510-00012 | Fast Discrete Distribution Clustering Using Wasserstein Barycenter with Sparse Support | http://arxiv.org/abs/1510.00012 | id:1510.00012 author:Jianbo Ye, Panruo Wu, James Z. Wang, Jia Li category:stat.CO cs.LG stat.ML  published:2015-09-30 summary:In a variety of research areas, the bag of weighted vectors and the histogram are widely used descriptors for complex objects. Both can be expressed as discrete distributions. D2-clustering pursues the minimum total within-cluster variation for a set of discrete distributions subject to the Kantorovich-Wasserstein metric. D2-clustering has a severe scalability issue, the bottleneck being the computation of a centroid distribution, called Wasserstein barycenter, that minimizes its sum of squared distances to the cluster members. In this paper, we develop a modified Bregman ADMM approach for computing the approximate discrete Wasserstein barycenter of large clusters. In the case when the support points of the barycenters are unknown and of low cardinality, our method achieves high accuracy empirically at a much reduced computational cost. The strengths and weaknesses of our method and its alternatives are examined through experiments; and scenarios for their respective usage are recommended. Moreover, we develop both serial and parallelized versions of the algorithm. By experimenting with large-scale data, we demonstrate the computational efficiency of the new methods and investigate their convergence properties and numerical stability. The clustering results obtained on several datasets in different domains are highly competitive in comparison with some widely used methods' in the corresponding areas. version:2
arxiv-1605-02372 | Active Learning for Community Detection in Stochastic Block Models | http://arxiv.org/abs/1605.02372 | id:1605.02372 author:Akshay Gadde, Eyal En Gad, Salman Avestimehr, Antonio Ortega category:cs.LG cs.SI math.PR  published:2016-05-08 summary:The stochastic block model (SBM) is an important generative model for random graphs in network science and machine learning, useful for benchmarking community detection (or clustering) algorithms. The symmetric SBM generates a graph with $2n$ nodes which cluster into two equally sized communities. Nodes connect with probability $p$ within a community and $q$ across different communities. We consider the case of $p=a\ln (n)/n$ and $q=b\ln (n)/n$. In this case, it was recently shown that recovering the community membership (or label) of every node with high probability (w.h.p.) using only the graph is possible if and only if the Chernoff-Hellinger (CH) divergence $D(a,b)=(\sqrt{a}-\sqrt{b})^2 \geq 1$. In this work, we study if, and by how much, community detection below the clustering threshold (i.e. $D(a,b)<1$) is possible by querying the labels of a limited number of chosen nodes (i.e., active learning). Our main result is to show that, under certain conditions, sampling the labels of a vanishingly small fraction of nodes (a number sub-linear in $n$) is sufficient for exact community detection even when $D(a,b)<1$. Furthermore, we provide an efficient learning algorithm which recovers the community memberships of all nodes w.h.p. as long as the number of sampled points meets the sufficient condition. We also show that recovery is not possible if the number of observed labels is less than $n^{1-D(a,b)}$. The validity of our results is demonstrated through numerical experiments. version:1
arxiv-1605-01713 | Not Just a Black Box: Learning Important Features Through Propagating Activation Differences | http://arxiv.org/abs/1605.01713 | id:1605.01713 author:Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, Anshul Kundaje category:cs.LG cs.CV cs.NE  published:2016-05-05 summary:The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods. version:2
arxiv-1512-01722 | Vanishing point attracts gaze in free-viewing and visual search tasks | http://arxiv.org/abs/1512.01722 | id:1512.01722 author:Ali Borji, Mengyang Feng category:cs.CV  published:2015-12-06 summary:To investigate whether the vanishing point (VP) plays a significant role in gaze guidance, we ran two experiments. In the first one, we recorded fixations of 10 observers (4 female; mean age 22; SD=0.84) freely viewing 532 images, out of which 319 had VP (shuffled presentation; each image for 4 secs). We found that the average number of fixations at a local region (80x80 pixels) centered at the VP is significantly higher than the average fixations at random locations (t-test; n=319; p=1.8e-35). To address the confounding factor of saliency, we learned a combined model of bottom-up saliency and VP. AUC score of our model (0.85; SD=0.01) is significantly higher than the original saliency model (e.g., 0.8 using AIM model by Bruce & Tsotsos (2009), t-test; p= 3.14e-16) and the VP-only model (0.64, t-test; p= 4.02e-22). In the second experiment, we asked 14 subjects (4 female, mean age 23.07, SD=1.26) to search for a target character (T or L) placed randomly on a 3x3 imaginary grid overlaid on top of an image. Subjects reported their answers by pressing one of two keys. Stimuli consisted of 270 color images (180 with a single VP, 90 without). The target happened with equal probability inside each cell (15 times L, 15 times T). We found that subjects were significantly faster (and more accurate) when target happened inside the cell containing the VP compared to cells without VP (median across 14 subjects 1.34 sec vs. 1.96; Wilcoxon rank-sum test; p = 0.0014). Response time at VP cells were also significantly lower than response time on images without VP (median 2.37; p= 4.77e-05). These findings support the hypothesis that vanishing point, similar to face and text (Cerf et al., 2009) as well as gaze direction (Borji et al., 2014) attracts attention in free-viewing and visual search. version:2
arxiv-1307-4847 | Efficient Reinforcement Learning in Deterministic Systems with Value Function Generalization | http://arxiv.org/abs/1307.4847 | id:1307.4847 author:Zheng Wen, Benjamin Van Roy category:cs.LG cs.AI cs.SY stat.ML  published:2013-07-18 summary:We consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efficient exploration and value function generalization. We establish that when the true value function lies within a given hypothesis class, OCP selects optimal actions over all but at most K episodes, where K is the eluder dimension of the given hypothesis class. We establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis class, for the special case where the hypothesis class is the span of pre-specified indicator functions over disjoint sets. We also discuss the computational complexity of OCP and present computational results involving two illustrative examples. version:3
arxiv-1506-02162 | Learning from Rational Behavior: Predicting Solutions to Unknown Linear Programs | http://arxiv.org/abs/1506.02162 | id:1506.02162 author:Shahin Jabbari, Ryan Rogers, Aaron Roth, Zhiwei Steven Wu category:cs.DS cs.GT cs.LG  published:2015-06-06 summary:We define and study the problem of predicting the solution to a linear program, given only partial information about its objective and constraints. This generalizes the problem of learning to predict the purchasing behavior of a rational agent who has an unknown objective function, which has been studied under the name "Learning from Revealed Preferences". We give mistake bound learning algorithms in two settings: in the first, the objective of the linear program is known to the learner, but there is an arbitrary, fixed set of constraints which are unknown. Each example given to the learner is defined by an additional, known constraint, and the goal of the learner is to predict the optimal solution of the linear program given the union of the known and unknown constraints. This models, among other things, the problem of predicting the behavior of a rational agent whose goals are known, but whose resources are unknown. In the second setting, the objective of the linear program is unknown, and changing in a controlled way. The constraints of the linear program may also change every day, but are known. An example is given by a set of constraints and partial information about the objective, and the task of the learner is again to predict the optimal solution of the partially known linear program. version:2
arxiv-1605-02346 | Chained Predictions Using Convolutional Neural Networks | http://arxiv.org/abs/1605.02346 | id:1605.02346 author:Georgia Gkioxari, Alexander Toshev, Navdeep Jaitly category:cs.CV  published:2016-05-08 summary:In this paper, we present an adaptation of the sequence-to-sequence model for structured output prediction in vision tasks. In this model the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each time step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted in different steps. We show that chained predictions achieve top performing results on human pose estimation from single images and videos. version:1
arxiv-1604-05417 | Triplet Probabilistic Embedding for Face Verification and Clustering | http://arxiv.org/abs/1604.05417 | id:1604.05417 author:Swami Sankaranarayanan, Azadeh Alavi, Carlos Castillo, Rama Chellappa category:cs.CV cs.LG stat.ML  published:2016-04-19 summary:Despite significant progress made over the past twenty five years, unconstrained face verification remains a challenging problem. This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding learned using triplet probability constraints to solve the unconstrained face verification problem. Aside from yielding performance improvements, this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering. Experiments on the challenging IJB-A dataset show that the proposed algorithm performs comparably or better than the state of the art methods in verification and identification metrics, while requiring much less training data and training time. The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to extreme pose variation. Furthermore, we demonstrate the robustness of the deep features to challenges including age, pose, blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets. version:2
arxiv-1507-01279 | $M$-Statistic for Kernel Change-Point Detection | http://arxiv.org/abs/1507.01279 | id:1507.01279 author:Shuang Li, Yao Xie, Hanjun Dai, Le Song category:cs.LG math.ST stat.ML stat.TH  published:2015-07-05 summary:Detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning. Kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach. However, none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic. Such characterization is crucial for setting the detection threshold, to control the significance level in the offline case as well as the false alarm rate (captured by the average run length) in the online case. In this paper we focus on the scenario when the amount of background data is large, and propose two related computationally efficient kernel-based statistics for change-point detection, which we call "$M$-statistics". A novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-of-measure. Such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner, without the need to resort to the more expensive simulations such as bootstrapping. Moreover, our $M$-statistic can be applied to high-dimensional data by choosing a proper kernel. We show that our methods perform well in both synthetic and real world data. version:3
arxiv-1505-04630 | Recurrent Neural Network Training with Dark Knowledge Transfer | http://arxiv.org/abs/1505.04630 | id:1505.04630 author:Zhiyuan Tang, Dong Wang, Zhiyong Zhang category:stat.ML cs.CL cs.LG cs.NE  published:2015-05-18 summary:Recurrent neural networks (RNNs), particularly long short-term memory (LSTM), have gained much attention in automatic speech recognition (ASR). Although some successful stories have been reported, training RNNs remains highly challenging, especially with limited training data. Recent research found that a well-trained model can be used as a teacher to train other child models, by using the predictions generated by the teacher model as supervision. This knowledge transfer learning has been employed to train simple neural nets with a complex one, so that the final performance can reach a level that is infeasible to obtain by regular training. In this paper, we employ the knowledge transfer learning approach to train RNNs (precisely LSTM) using a deep neural network (DNN) model as the teacher. This is different from most of the existing research on knowledge transfer learning, since the teacher (DNN) is assumed to be weaker than the child (RNN); however, our experiments on an ASR task showed that it works fairly well: without applying any tricks on the learning scheme, this approach can train RNNs successfully even with limited training data. version:5
arxiv-1605-02315 | Information Recovery in Shuffled Graphs via Graph Matching | http://arxiv.org/abs/1605.02315 | id:1605.02315 author:Vince Lyzinski category:stat.ML cs.IT math.CO math.IT  published:2016-05-08 summary:In a number of methodologies for joint inference across graphs, it is assumed that an explicit vertex correspondence is a priori known across the vertex sets of the graphs. While this assumption is often reasonable, in practice these correspondences may be unobserved and/or errorfully observed, and graph matching---aligning a pair of graphs to minimize their edge disagreements---is used to align the graphs before performing subsequent inference. Herein, we explore the duality between the loss of mutual information due to an errorfully observed vertex correspondence and the ability of graph matching algorithms to recover the true correspondence across graphs. We then demonstrate the practical effect that graph shuffling---and matching---can have on subsequent inference, with examples from two sample graph hypothesis testing and joint graph clustering. version:1
arxiv-1605-02305 | Estimating Depth from Monocular Images as Classification Using Deep Fully Convolutional Residual Networks | http://arxiv.org/abs/1605.02305 | id:1605.02305 author:Yuanzhouhan Cao, Zifeng Wu, Chunhua Shen category:cs.CV  published:2016-05-08 summary:Depth estimation from single monocular images is a key component of scene understanding and has benefited largely from deep convolutional neural networks (CNN) recently. In this article, we take advantage of the recent deep residual networks and propose a simple yet effective approach to this problem. We formulate depth estimation as a pixel-wise classification task. Specifically, we first discretize the continuous depth values into multiple bins and label the bins according to their depth range. Then we train fully convolutional deep residual networks to predict the depth label of each pixel. Performing discrete depth label classification instead of continuous depth value regression allows us to predict a confidence in the form of probability distribution. We further apply fully-connected conditional random fields (CRF) as a post processing step to enforce local smoothness interactions, which improves the results. We evaluate our approach on the NYUDepth v2 dataset and achieve state-of-the-art performance. version:1
arxiv-1604-06985 | Deep Learning with Eigenvalue Decay Regularizer | http://arxiv.org/abs/1604.06985 | id:1604.06985 author:Oswaldo Ludwig category:cs.LG  published:2016-04-24 summary:This paper extends our previous work on regularization of neural networks using Eigenvalue Decay by employing a soft approximation of the dominant eigenvalue in order to enable the calculation of its derivatives in relation to the synaptic weights, and therefore the application of back-propagation, which is a primary demand for deep learning. Moreover, we extend our previous theoretical analysis to deep neural networks and multiclass classification problems. Our method is implemented as an additional regularizer in Keras, a modular neural networks library written in Python, and evaluated in the benchmark data sets Reuters Newswire Topics Classification, IMDB database for binary sentiment classification, MNIST database of handwritten digits and CIFAR-10 data set for image classification. version:3
arxiv-1602-07572 | Ultradense Word Embeddings by Orthogonal Transformation | http://arxiv.org/abs/1602.07572 | id:1602.07572 author:Sascha Rothe, Sebastian Ebert, Hinrich Schütze category:cs.CL  published:2016-02-24 summary:Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information - sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space. version:2
arxiv-1605-02289 | Detecting Ground Control Points via Convolutional Neural Network for Stereo Matching | http://arxiv.org/abs/1605.02289 | id:1605.02289 author:Zhun Zhong, Songzhi Su, Donglin Cao, Shaozi Li category:cs.CV  published:2016-05-08 summary:In this paper, we present a novel approach to detect ground control points (GCPs) for stereo matching problem. First of all, we train a convolutional neural network (CNN) on a large stereo set, and compute the matching confidence of each pixel by using the trained CNN model. Secondly, we present a ground control points selection scheme according to the maximum matching confidence of each pixel. Finally, the selected GCPs are used to refine the matching costs, and we apply the new matching costs to perform optimization with semi-global matching algorithm for improving the final disparity maps. We evaluate our approach on the KITTI 2012 stereo benchmark dataset. Our experiments show that the proposed approach significantly improves the accuracy of disparity maps. version:1
arxiv-1605-02277 | On-Average KL-Privacy and its equivalence to Generalization for Max-Entropy Mechanisms | http://arxiv.org/abs/1605.02277 | id:1605.02277 author:Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg category:stat.ML cs.CR  published:2016-05-08 summary:We define On-Average KL-Privacy and present its properties and connections to differential privacy, generalization and information-theoretic quantities including max-information and mutual information. The new definition significantly weakens differential privacy, while preserving its minimalistic design features such as composition over small group and multiple queries as well as closeness to post-processing. Moreover, we show that On-Average KL-Privacy is **equivalent** to generalization for a large class of commonly-used tools in statistics and machine learning that samples from Gibbs distributions---a class of distributions that arises naturally from the maximum entropy principle. In addition, a byproduct of our analysis yields a lower bound for generalization error in terms of mutual information which reveals an interesting interplay with known upper bounds that use the same quantity. version:1
arxiv-1605-02276 | Problems With Evaluation of Word Embeddings Using Word Similarity Tasks | http://arxiv.org/abs/1605.02276 | id:1605.02276 author:Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer category:cs.CL  published:2016-05-08 summary:Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods. version:1
arxiv-1605-02269 | Predicting Performance on MOOC Assessments using Multi-Regression Models | http://arxiv.org/abs/1605.02269 | id:1605.02269 author:Zhiyun Ren, Huzefa Rangwala, Aditya Johri category:cs.CY cs.LG  published:2016-05-08 summary:The past few years has seen the rapid growth of data min- ing approaches for the analysis of data obtained from Mas- sive Open Online Courses (MOOCs). The objectives of this study are to develop approaches to predict the scores a stu- dent may achieve on a given grade-related assessment based on information, considered as prior performance or prior ac- tivity in the course. We develop a personalized linear mul- tiple regression (PLMR) model to predict the grade for a student, prior to attempting the assessment activity. The developed model is real-time and tracks the participation of a student within a MOOC (via click-stream server logs) and predicts the performance of a student on the next as- sessment within the course offering. We perform a com- prehensive set of experiments on data obtained from three openEdX MOOCs via a Stanford University initiative. Our experimental results show the promise of the proposed ap- proach in comparison to baseline approaches and also helps in identification of key features that are associated with the study habits and learning behaviors of students. version:1
arxiv-1605-02268 | Rate-Distortion Bounds on Bayes Risk in Supervised Learning | http://arxiv.org/abs/1605.02268 | id:1605.02268 author:Matthew Nokleby, Ahmad Beirami, Robert Calderbank category:cs.IT cs.LG math.IT stat.ML  published:2016-05-08 summary:An information-theoretic framework is presented for estimating the number of labeled samples needed to train a classifier in a parametric Bayesian setting. Ideas from rate-distortion theory are used to derive bounds on the average $L_1$ or $L_\infty$ distance between the learned classifier and the true maximum a posteriori classifier---which are well-established surrogates for the excess classification error due to imperfect learning---in terms of the differential entropy of the posterior distribution, the Fisher information of the parametric family, and the number of training samples available. The maximum {\em a posteriori} classifier is viewed as a random source, labeled training data are viewed as a finite-rate encoding of the source, and the $L_1$ or $L_\infty$ Bayes risk is viewed as the average distortion. The result is a complementary framework to the well-known probably approximately correct (PAC) framework. PAC bounds characterize worst-case learning performance of a family of classifiers whose complexity is captured by the Vapnik-Chervonenkis (VC) dimension. The rate-distortion framework, on the other hand, characterizes the average-case performance of a family of data distributions in terms of a quantity called the interpolation dimension, which represents the complexity of the family of data distributions. The resulting bounds do not suffer from the pessimism typical of the PAC framework, particularly when the training set is small. The framework also naturally accommodates multi-class settings. Furthermore, Monte Carlo methods provide accurate estimates of the bounds even for complicated distributions. The effectiveness of this framework is demonstrated in both a binary and multi-class Gaussian setting. version:1
arxiv-1605-02266 | Robust and Low-Rank Representation for Fast Face Identification with Occlusions | http://arxiv.org/abs/1605.02266 | id:1605.02266 author:Michael Iliadis, Haohong Wang, Rafael Molina, Aggelos K. Katsaggelos category:cs.CV  published:2016-05-08 summary:In this paper we propose an iterative method to address the face identification problem with block occlusions. Our approach utilizes a robust representation based on two characteristics in order to model contiguous errors (e.g., block occlusion) effectively. The first fits to the errors a distribution described by a tailored loss function. The second describes the error image as having a specific structure (resulting in low-rank). We will show that this joint characterization is effective for describing errors with spatial continuity. Our approach is computationally efficient due to the utilization of the Alternating Direction Method of Multipliers (ADMM). A special case of our fast iterative algorithm leads to the robust representation method which is normally used to handle non-contiguous errors (e.g., pixel corruption). Extensive results on representative face databases document the effectiveness of our method over existing robust representation methods with respect to both identification rates and computational time. Code is available at Github, where you can find implementations of the F-LR-IRNNLS and F-IRNNLS (fast version of the RRC) : \url{https://github.com/miliadis/FIRC} version:1
arxiv-1605-02264 | Laplacian Reconstruction and Refinement for Semantic Segmentation | http://arxiv.org/abs/1605.02264 | id:1605.02264 author:Golnaz Ghiasi, Charless Fowlkes category:cs.CV  published:2016-05-08 summary:CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense pixel-accurate labeling. This paper makes two contributions: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture, akin to a Laplacian pyramid, that uses skip connections from higher resolution feature maps to successively refine segment boundaries reconstructed from lower resolution maps. This approach yields state-of-the-art semantic segmentation results on PASCAL without resorting to more complex CRF or detection driven architectures. version:1
arxiv-1605-02260 | Deeply Exploit Depth Information for Object Detection | http://arxiv.org/abs/1605.02260 | id:1605.02260 author:Saihui Hou, Zilei Wang, Feng Wu category:cs.CV  published:2016-05-08 summary:This paper addresses the issue on how to more effectively coordinate the depth with RGB aiming at boosting the performance of RGB-D object detection. Particularly, we investigate two primary ideas under the CNN model: property derivation and property fusion. Firstly, we propose that the depth can be utilized not only as a type of extra information besides RGB but also to derive more visual properties for comprehensively describing the objects of interest. So a two-stage learning framework consisting of property derivation and fusion is constructed. Here the properties can be derived either from the provided color/depth or their pairs (e.g. the geometry contour adopted in this paper). Secondly, we explore the fusion method of different properties in feature learning, which is boiled down to, under the CNN model, from which layer the properties should be fused together. The analysis shows that different semantic properties should be learned separately and combined before passing into the final classifier. Actually, such a detection way is in accordance with the mechanism of the primary neural cortex (V1) in brain. We experimentally evaluate the proposed method on the challenging dataset, and have achieved state-of-the-art performance. version:1
arxiv-1605-02257 | A corpus of preposition supersenses in English web reviews | http://arxiv.org/abs/1605.02257 | id:1605.02257 author:Nathan Schneider, Jena D. Hwang, Vivek Srikumar, Meredith Green, Kathryn Conger, Tim O'Gorman, Martha Palmer category:cs.CL  published:2016-05-08 summary:We present the first corpus annotated with preposition supersenses, unlexicalized categories for semantic functions that can be marked by English prepositions (Schneider et al., 2015). That scheme improves upon its predecessors to better facilitate comprehensive manual annotation. Moreover, unlike the previous schemes, the preposition supersenses are organized hierarchically. Our data will be publicly released on the web upon publication. version:1
arxiv-1605-02240 | On Image segmentation using Fractional Gradients-Learning Model Parameters using Approximate Marginal Inference | http://arxiv.org/abs/1605.02240 | id:1605.02240 author:Anish Acharya, Uddipan Mukherjee, Charless Fowlkes category:cs.CV  published:2016-05-07 summary:Estimates of image gradients play a ubiquitous role in image segmentation and classification problems since gradients directly relate to the boundaries or the edges of a scene. This paper proposes an unified approach to gradient estimation based on fractional calculus that is computationally cheap and readily applicable to any existing algorithm that relies on image gradients. We show experiments on edge detection and image segmentation on the Stanford Backgrounds Dataset where these improved local gradients outperforms state of the art, achieving a performance of 79.2% average accuracy. version:1
arxiv-1605-02234 | A Bayesian Group Sparse Multi-Task Regression Model for Imaging Genetics | http://arxiv.org/abs/1605.02234 | id:1605.02234 author:Keelin Greenlaw, Elena Szefer, Jinko Graham, Mary Lesperance, Farouk S. Nathoo category:stat.ME stat.AP stat.ML  published:2016-05-07 summary:Motivation: Recent advances in technology for brain imaging and high-throughput genotyping have motivated studies examining the influence of genetic variation on brain structure. Wang et al. (Bioinformatics, 2012) have developed an approach for the analysis of imaging genomic studies using penalized multi-task regression with regularization based on a novel group $l_{2,1}$-norm penalty which encourages structured sparsity at both the gene level and SNP level. While incorporating a number of useful features, the proposed method only furnishes a point estimate of the regression coefficients; techniques for conducting statistical inference are not provided. A new Bayesian method is proposed here to overcome this limitation. Results: We develop a Bayesian hierarchical modeling formulation where the posterior mode corresponds to the estimator proposed by Wang et al. (\textit{Bioinformatics}, 2012), and an approach that allows for full posterior inference including the construction of interval estimates for the regression parameters. We show that the proposed hierarchical model can be expressed as a three-level Gaussian scale mixture and this representation facilitates the use of a Gibbs sampling algorithm for posterior simulation. Simulation studies demonstrate that the interval estimates obtained using our approach achieve adequate coverage probabilities that outperform those obtained from the nonparametric bootstrap. Our proposed methodology is applied to the analysis of neuroimaging and genetic data collected as part of the Alzheimer's Disease Neuroimaging Initiative (ADNI), and this analysis of the ADNI cohort demonstrates clearly the value added of incorporating interval estimation beyond only point estimation when relating SNPs to brain imaging endophenotypes. version:1
arxiv-1604-00289 | Building Machines That Learn and Think Like People | http://arxiv.org/abs/1604.00289 | id:1604.00289 author:Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman category:cs.AI cs.CV cs.LG cs.NE stat.ML  published:2016-04-01 summary:Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models. version:2
arxiv-1605-02216 | Distributed stochastic optimization for deep learning (thesis) | http://arxiv.org/abs/1605.02216 | id:1605.02216 author:Sixin Zhang category:cs.LG  published:2016-05-07 summary:We study the problem of how to distribute the training of large-scale deep learning models in the parallel computing environment. We propose a new distributed stochastic optimization method called Elastic Averaging SGD (EASGD). We analyze the convergence rate of the EASGD method in the synchronous scenario and compare its stability condition with the existing ADMM method in the round-robin scheme. An asynchronous and momentum variant of the EASGD method is applied to train deep convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Our approach accelerates the training and furthermore achieves better test accuracy. It also requires a much smaller amount of communication than other common baseline approaches such as the DOWNPOUR method. We then investigate the limit in speedup of the initial and the asymptotic phase of the mini-batch SGD, the momentum SGD, and the EASGD methods. We find that the spread of the input data distribution has a big impact on their initial convergence rate and stability region. We also find a surprising connection between the momentum SGD and the EASGD method with a negative moving average rate. A non-convex case is also studied to understand when EASGD can get trapped by a saddle point. Finally, we scale up the EASGD method by using a tree structured network topology. We show empirically its advantage and challenge. We also establish a connection between the EASGD and the DOWNPOUR method with the classical Jacobi and the Gauss-Seidel method, thus unifying a class of distributed stochastic optimization methods. version:1
arxiv-1605-02196 | All Weather Perception: Joint Data Association, Tracking, and Classification for Autonomous Ground Vehicles | http://arxiv.org/abs/1605.02196 | id:1605.02196 author:Peter Radecki, Mark Campbell, Kevin Matzen category:cs.SY cs.CV cs.LG cs.RO  published:2016-05-07 summary:A novel probabilistic perception algorithm is presented as a real-time joint solution to data association, object tracking, and object classification for an autonomous ground vehicle in all-weather conditions. The presented algorithm extends a Rao-Blackwellized Particle Filter originally built with a particle filter for data association and a Kalman filter for multi-object tracking (Miller et al. 2011a) to now also include multiple model tracking for classification. Additionally a state-of-the-art vision detection algorithm that includes heading information for autonomous ground vehicle (AGV) applications was implemented. Cornell's AGV from the DARPA Urban Challenge was upgraded and used to experimentally examine if and how state-of-the-art vision algorithms can complement or replace lidar and radar sensors. Sensor and algorithm performance in adverse weather and lighting conditions is tested. Experimental evaluation demonstrates robust all-weather data association, tracking, and classification where camera, lidar, and radar sensors complement each other inside the joint probabilistic perception algorithm. version:1
arxiv-1605-02190 | Matching models across abstraction levels with Gaussian Processes | http://arxiv.org/abs/1605.02190 | id:1605.02190 author:Giulio Caravagna, Luca Bortolussi, Guido Sanguinetti category:stat.ML  published:2016-05-07 summary:Biological systems are often modelled at different levels of abstraction depending on the particular aims/resources of a study. Such different models often provide qualitatively concordant predictions over specific parametrisations, but it is generally unclear whether model predictions are quantitatively in agreement, and whether such agreement holds for different parametrisations. Here we present a generally applicable statistical machine learning methodology to automatically reconcile the predictions of different models across abstraction levels. Our approach is based on defining a correction map, a random function which modifies the output of a model in order to match the statistics of the output of a different model of the same system. We use two biological examples to give a proof-of-principle demonstration of the methodology, and discuss its advantages and potential further applications. version:1
arxiv-1602-01208 | Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences | http://arxiv.org/abs/1602.01208 | id:1602.01208 author:Akira Taniguchi, Tadahiro Taniguchi, Tetsunari Inamura category:cs.AI cs.CL cs.RO  published:2016-02-03 summary:In this paper, we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots, from human continuous speech signals. We address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model. Further, we propose a method that allows a robot to effectively use the learned words and their meanings for self-localization tasks. The proposed method is nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates the generative model for self-localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept. We implemented the proposed method SpCoA on SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile robot in a real environment. Further, we conducted experiments for evaluating the performance of SpCoA. The experimental results showed that SpCoA enabled the robot to acquire the names of places from speech sentences. They also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self-localization. version:3
arxiv-1605-02164 | Fast Bilateral Filtering of Vector-Valued Images | http://arxiv.org/abs/1605.02164 | id:1605.02164 author:Sanjay Ghosh, Kunal N. Chaudhury category:cs.CV  published:2016-05-07 summary:In this paper, we consider a natural extension of the edge-preserving bilateral filter for vector-valued images. The direct computation of this non-linear filter is slow in practice. We demonstrate how a fast algorithm can be obtained by first approximating the Gaussian kernel of the bilateral filter using raised-cosines, and then using Monte Carlo sampling. We present simulation results on color images to demonstrate the accuracy of the algorithm and the speedup over the direct implementation. version:1
arxiv-1511-07972 | Learning with Memory Embeddings | http://arxiv.org/abs/1511.07972 | id:1511.07972 author:Volker Tresp, Cristóbal Esteban, Yinchong Yang, Stephan Baier, Denis Krompaß category:cs.AI cs.CL cs.LG  published:2015-11-25 summary:Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Latent variable models are well suited to deal with the high dimensionality and sparsity of typical knowledge graphs. In recent publications the embedding models were extended to also consider time evolutions, time patterns and subsymbolic representations. In this paper we map embedding models, which were developed purely as solutions to technical problems for modelling temporal knowledge graphs, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory, sensory memory, short-term memory, and working memory. We discuss learning, query answering, the path from sensory input to semantic decoding, and the relationship between episodic memory and semantic memory. We introduce a number of hypotheses on human memory that can be derived from the developed mathematical models. version:9
arxiv-1605-02150 | On Improving Informativity and Grammaticality for Multi-Sentence Compression | http://arxiv.org/abs/1605.02150 | id:1605.02150 author:Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond Wong, Fang Chen category:cs.CL  published:2016-05-07 summary:Multi Sentence Compression (MSC) is of great value to many real world applications, such as guided microblog summarization, opinion summarization and newswire summarization. Recently, word graph-based approaches have been proposed and become popular in MSC. Their key assumption is that redundancy among a set of related sentences provides a reliable way to generate informative and grammatical sentences. In this paper, we propose an effective approach to enhance the word graph-based MSC and tackle the issue that most of the state-of-the-art MSC approaches are confronted with: i.e., improving both informativity and grammaticality at the same time. Our approach consists of three main components: (1) a merging method based on Multiword Expressions (MWE); (2) a mapping strategy based on synonymy between words; (3) a re-ranking step to identify the best compression candidates generated using a POS-based language model (POS-LM). We demonstrate the effectiveness of this novel approach using a dataset made of clusters of English newswire sentences. The observed improvements on informativity and grammaticality of the generated compressions show that our approach is superior to state-of-the-art MSC methods. version:1
arxiv-1602-03616 | Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks | http://arxiv.org/abs/1602.03616 | id:1602.03616 author:Anh Nguyen, Jason Yosinski, Jeff Clune category:cs.NE cs.CV  published:2016-02-11 summary:We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron. version:2
arxiv-1605-02140 | Matrix Factorization-Based Clustering Of Image Features For Bandwidth-Constrained Information Retrieval | http://arxiv.org/abs/1605.02140 | id:1605.02140 author:Jacob Chakareski, Immanuel Manohar, Shantanu Rane category:cs.CV 62h25 I.4  published:2016-05-07 summary:We consider the problem of accurately and efficiently querying a remote server to retrieve information about images captured by a mobile device. In addition to reduced transmission overhead and computational complexity, the retrieval protocol should be robust to variations in the image acquisition process, such as translation, rotation, scaling, and sensor-related differences. We propose to extract scale-invariant image features and then perform clustering to reduce the number of features needed for image matching. Principal Component Analysis (PCA) and Non-negative Matrix Factorization (NMF) are investigated as candidate clustering approaches. The image matching complexity at the database server is quadratic in the (small) number of clusters, not in the (very large) number of image features. We employ an image-dependent information content metric to approximate the model order, i.e., the number of clusters, needed for accurate matching, which is preferable to setting the model order using trial and error. We show how to combine the hypotheses provided by PCA and NMF factor loadings, thereby obtaining more accurate retrieval than using either approach alone. In experiments on a database of urban images, we obtain a top-1 retrieval accuracy of 89% and a top-3 accuracy of 92.5%. version:1
arxiv-1605-02134 | Neural Recovery Machine for Chinese Dropped Pronoun | http://arxiv.org/abs/1605.02134 | id:1605.02134 author:Wei-Nan Zhang, Ting Liu, Qingyu Yin, Yu Zhang category:cs.CL  published:2016-05-07 summary:Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese, Japanese etc. Previous work mainly focused on painstakingly exploring the empirical features for DPs recovery. In this paper, we propose a neural recovery machine (NRM) to model and recover DPs in Chinese, so that to avoid the non-trivial feature engineering process. The experimental results show that the proposed NRM significantly outperforms the state-of-the-art approaches on both two heterogeneous datasets. Further experiment results of Chinese zero pronoun (ZP) resolution show that the performance of ZP resolution can also be improved by recovering the ZPs to DPs. version:1
arxiv-1605-02130 | Robust Dialog State Tracking for Large Ontologies | http://arxiv.org/abs/1605.02130 | id:1605.02130 author:Franck Dernoncourt, Ji Young Lee, Trung H. Bui, Hung H. Bui category:cs.CL cs.AI cs.LG  published:2016-05-07 summary:The Dialog State Tracking Challenge 4 (DSTC 4) differentiates itself from the previous three editions as follows: the number of slot-value pairs present in the ontology is much larger, no spoken language understanding output is given, and utterances are labeled at the subdialog level. This paper describes a novel dialog state tracking method designed to work robustly under these conditions, using elaborate string matching, coreference resolution tailored for dialogs and a few other improvements. The method can correctly identify many values that are not explicitly present in the utterance. On the final evaluation, our method came in first among 7 competing teams and 24 entries. The F1-score achieved by our method was 9 and 7 percentage points higher than that of the runner-up for the utterance-level evaluation and for the subdialog-level evaluation, respectively. version:1
arxiv-1605-02129 | Adobe-MIT submission to the DSTC 4 Spoken Language Understanding pilot task | http://arxiv.org/abs/1605.02129 | id:1605.02129 author:Franck Dernoncourt, Ji Young Lee, Trung H. Bui, Hung H. Bui category:cs.CL cs.AI cs.LG  published:2016-05-07 summary:The Dialog State Tracking Challenge 4 (DSTC 4) proposes several pilot tasks. In this paper, we focus on the spoken language understanding pilot task, which consists of tagging a given utterance with speech acts and semantic slots. We compare different classifiers: the best system obtains 0.52 and 0.67 F1-scores on the test set for speech act recognition for the tourist and the guide respectively, and 0.52 F1-score for semantic tagging for both the guide and the tourist. version:1
arxiv-1310-1177 | Clustering on Multiple Incomplete Datasets via Collective Kernel Learning | http://arxiv.org/abs/1310.1177 | id:1310.1177 author:Weixiang Shao, Xiaoxiao Shi, Philip S. Yu category:cs.LG H.2.8; I.5.3  published:2013-10-04 summary:Multiple datasets containing different types of features may be available for a given task. For instance, users' profiles can be used to group users for recommendation systems. In addition, a model can also use users' historical behaviors and credit history to group users. Each dataset contains different information and suffices for learning. A number of clustering algorithms on multiple datasets were proposed during the past few years. These algorithms assume that at least one dataset is complete. So far as we know, all the previous methods will not be applicable if there is no complete dataset available. However, in reality, there are many situations where no dataset is complete. As in building a recommendation system, some new users may not have a profile or historical behaviors, while some may not have a credit history. Hence, no available dataset is complete. In order to solve this problem, we propose an approach called Collective Kernel Learning to infer hidden sample similarity from multiple incomplete datasets. The idea is to collectively completes the kernel matrices of incomplete datasets by optimizing the alignment of the shared instances of the datasets. Furthermore, a clustering algorithm is proposed based on the kernel matrix. The experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The proposed clustering algorithm outperforms the comparison algorithms by as much as two times in normalized mutual information. version:2
arxiv-1605-02113 | Likelihood Inflating Sampling Algorithm | http://arxiv.org/abs/1605.02113 | id:1605.02113 author:Reihaneh Entezari, Radu V. Craiu, Jeffrey S. Rosenthal category:stat.ML stat.CO  published:2016-05-06 summary:Markov Chain Monte Carlo (MCMC) sampling from a posterior distribution corresponding to a massive data set can be computationally prohibitive since producing one sample requires a number of operations that is linear in the data size. In this paper, we introduce a new communication-free parallel method, the Likelihood Inflating Sampling Algorithm (LISA), that significantly reduces computational costs by randomly splitting the dataset into smaller subsets and running MCMC methods independently and in parallel on each subset using different processors. Each processor will draw sub-samples from sub-posterior distributions that are defined by "inflating" the likelihood function and the sub-samples are then combined using the importance re-sampling method to perform approximate full-data posterior samples. We test our method on several examples including the important case of Bayesian Additive Regression Trees (BART) using both simulated and real datasets. The method we propose shows significant efficiency gains over the existing Consensus Monte Carlo of Scott et al. (2013). version:1
arxiv-1605-02112 | Attribute And-Or Grammar for Joint Parsing of Human Attributes, Part and Pose | http://arxiv.org/abs/1605.02112 | id:1605.02112 author:Seyoung Park, Bruce Xiaohan Nie, Song-Chun Zhu category:cs.CV  published:2016-05-06 summary:This paper presents an attribute and-or grammar (A-AOG) model for jointly inferring human body pose and human attributes in a parse graph with attributes augmented to nodes in the hierarchical representation. In contrast to other popular methods in the current literature that train separate classifiers for poses and individual attributes, our method explicitly represents the decomposition and articulation of body parts, and account for the correlations between poses and attributes. The A-AOG model is an amalgamation of three traditional grammar formulations: (i) Phrase structure grammar representing the hierarchical decomposition of the human body from whole to parts; (ii) Dependency grammar modeling the geometric articulation by a kinematic tree of the body pose; and (iii) Attribute grammar accounting for the compatibility relations between different parts in the hierarchy so that their appearances follow a consistent style. The parse graph outputs human detection, pose estimation, and attribute prediction simultaneously, which are intuitive and interpretable. We conduct experiments on two tasks on two datasets, and experimental results demonstrate the advantage of joint modeling in comparison with computing poses and attributes independently. Furthermore, our model obtains better performance over existing methods for both pose estimation and attribute prediction tasks. version:1
arxiv-1605-02105 | Distributed Learning with Infinitely Many Hypotheses | http://arxiv.org/abs/1605.02105 | id:1605.02105 author:Angelia Nedić, Alex Olshevsky, César Uribe category:math.OC cs.LG stat.ML  published:2016-05-06 summary:We consider a distributed learning setup where a network of agents sequentially access realizations of a set of random variables with unknown distributions. The network objective is to find a parametrized distribution that best describes their joint observations in the sense of the Kullback-Leibler divergence. Apart from recent efforts in the literature, we analyze the case of countably many hypotheses and the case of a continuum of hypotheses. We provide non-asymptotic bounds for the concentration rate of the agents' beliefs around the correct hypothesis in terms of the number of agents, the network parameters, and the learning abilities of the agents. Additionally, we provide a novel motivation for a general set of distributed Non-Bayesian update rules as instances of the distributed stochastic mirror descent algorithm. version:1
arxiv-1511-06732 | Sequence Level Training with Recurrent Neural Networks | http://arxiv.org/abs/1511.06732 | id:1511.06732 author:Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba category:cs.LG cs.CL  published:2015-11-20 summary:Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster. version:7
arxiv-1604-04024 | Towards Automated Melanoma Screening: Proper Computer Vision & Reliable Results | http://arxiv.org/abs/1604.04024 | id:1604.04024 author:Michel Fornaciali, Micael Carvalho, Flávia Vasques Bittencourt, Sandra Avila, Eduardo Valle category:cs.CV  published:2016-04-14 summary:In this paper we survey, analyze and criticize current art on automated melanoma screening, reimplementing a baseline technique, and proposing two novel ones. Melanoma, although highly curable when detected early, ends as one of the most dangerous types of cancer, due to delayed diagnosis and treatment. Its incidence is soaring, much faster than the number of trained professionals able to diagnose it. Automated screening appears as an alternative to make the most of those professionals, focusing their time on the patients at risk while safely discharging the other patients. However, the potential of automated melanoma diagnosis is currently unfulfilled, due to the emphasis of current literature on outdated computer vision models. Even more problematic is the irreproducibility of current art. We show how streamlined pipelines based upon current Computer Vision outperform conventional models - a model based on an advanced bags of words reaches an AUC of 84.6%, and a model based on deep neural networks reaches 89.3%, while the baseline (a classical bag of words) stays at 81.2%. We also initiate a dialog to improve reproducibility in our community version:3
arxiv-1605-02099 | Some Simulation Results for Emphatic Temporal-Difference Learning Algorithms | http://arxiv.org/abs/1605.02099 | id:1605.02099 author:Huizhen Yu category:cs.LG  published:2016-05-06 summary:This is a companion note to our recent study of the weak convergence properties of constrained emphatic temporal-difference learning (ETD) algorithms from a theoretic perspective. It supplements the latter analysis with simulation results and illustrates the behavior of some of the ETD algorithms using three example problems. version:1
arxiv-1605-02097 | ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning | http://arxiv.org/abs/1605.02097 | id:1605.02097 author:Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Jaśkowski category:cs.LG cs.AI cs.CV  published:2016-05-06 summary:The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible. version:1
arxiv-1605-00176 | Stochastic Contextual Bandits with Known Reward Functions | http://arxiv.org/abs/1605.00176 | id:1605.00176 author:Pranav Sakulkar, Bhaskar Krishnamachari category:cs.LG  published:2016-04-30 summary:Many sequential decision-making problems in communication networks can be modeled as contextual bandit problems, which are natural extensions of the well-known multi-armed bandit problem. In contextual bandit problems, at each time, an agent observes some side information or context, pulls one arm and receives the reward for that arm. We consider a stochastic formulation where the context-reward tuples are independently drawn from an unknown distribution in each trial. Motivated by networking applications, we analyze a setting where the reward is a known non-linear function of the context and the chosen arm's current state. We first consider the case of discrete and finite context-spaces and propose DCB($\epsilon$), an algorithm that we prove, through a careful analysis, yields regret (cumulative reward gap compared to a distribution-aware genie) scaling logarithmically in time and linearly in the number of arms that are not optimal for any context, improving over existing algorithms where the regret scales linearly in the total number of arms. We then study continuous context-spaces with Lipschitz reward functions and propose CCB($\epsilon, \delta$), an algorithm that uses DCB($\epsilon$) as a subroutine. CCB($\epsilon, \delta$) reveals a novel regret-storage trade-off that is parametrized by $\delta$. Tuning $\delta$ to the time horizon allows us to obtain sub-linear regret bounds, while requiring sub-linear storage. By exploiting joint learning for all contexts we get regret bounds for CCB($\epsilon, \delta$) that are unachievable by any existing contextual bandit algorithm for continuous context-spaces. We also show similar performance bounds for the unknown horizon case. version:2
arxiv-1605-02077 | Function-Specific Mixing Times and Concentration Away from Equilibrium | http://arxiv.org/abs/1605.02077 | id:1605.02077 author:Maxim Rabinovich, Aaditya Ramdas, Michael I. Jordan, Martin J. Wainwright category:math.ST cs.LG math.PR stat.TH  published:2016-05-06 summary:Slow mixing is the central hurdle when working with Markov chains, especially those used for Monte Carlo approximations (MCMC). In many applications, it is only of interest to to estimate the stationary expectations of a small set of functions, and so the usual definition of mixing based on total variation convergence may be too conservative. Accordingly, we introduce function-specific analogs of mixing times and spectral gaps, and use them to prove Hoeffding-like function-specific concentration inequalities. These results show that it is possible for empirical expectations of functions to concentrate long before the underlying chain has mixed in the classical sense. We use our techniques to derive confidence intervals that are sharper than those implied by both classical Markov chain Hoeffding bounds and Berry-Esseen-corrected CLT bounds. For applications that require testing, rather than point estimation, we show similar improvements over recent sequential testing results for MCMC. We conclude by applying our framework to real data examples of MCMC, providing evidence that our theory is both accurate and relevant to practice. version:1
arxiv-1605-02065 | Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds | http://arxiv.org/abs/1605.02065 | id:1605.02065 author:Mark Bun, Thomas Steinke category:cs.CR cs.DS cs.IT cs.LG math.IT  published:2016-05-06 summary:"Concentrated differential privacy" was recently introduced by Dwork and Rothblum as a relaxation of differential privacy, which permits sharper analyses of many privacy-preserving computations. We present an alternative formulation of the concept of concentrated differential privacy in terms of the Renyi divergence between the distributions obtained by running an algorithm on neighboring inputs. With this reformulation in hand, we prove sharper quantitative results, establish lower bounds, and raise a few new questions. We also unify this approach with approximate differential privacy by giving an appropriate definition of "approximate concentrated differential privacy." version:1
arxiv-1605-02060 | Deformably Registering and Annotating Whole CLARITY Brains to an Atlas via Masked LDDMM | http://arxiv.org/abs/1605.02060 | id:1605.02060 author:Kwame S. Kutten, Joshua T. Vogelstein, Nicolas Charon, Li Ye, Karl Deisseroth, Michael I. Miller category:q-bio.QM cs.CV  published:2016-05-06 summary:The CLARITY method renders brains optically transparent to enable high-resolution imaging in the structurally intact brain. Anatomically annotating CLARITY brains is necessary for discovering which regions contain signals of interest. Manually annotating whole-brain, terabyte CLARITY images is difficult, time-consuming, subjective, and error-prone. Automatically registering CLARITY images to a pre-annotated brain atlas offers a solution, but is difficult for several reasons. Removal of the brain from the skull and subsequent storage and processing cause variable non-rigid deformations, thus compounding inter-subject anatomical variability. Additionally, the signal in CLARITY images arises from various biochemical contrast agents which only sparsely label brain structures. This sparse labeling challenges the most commonly used registration algorithms that need to match image histogram statistics to the more densely labeled histological brain atlases. The standard method is a multiscale Mutual Information B-spline algorithm that dynamically generates an average template as an intermediate registration target. We determined that this method performs poorly when registering CLARITY brains to the Allen Institute's Mouse Reference Atlas (ARA), because the image histogram statistics are poorly matched. Therefore, we developed a method (Mask-LDDMM) for registering CLARITY images, that automatically find the brain boundary and learns the optimal deformation between the brain and atlas masks. Using Mask-LDDMM without an average template provided better results than the standard approach when registering CLARITY brains to the ARA. The LDDMM pipelines developed here provide a fast automated way to anatomically annotate CLARITY images. Our code is available as open source software at http://NeuroData.io. version:1
arxiv-1605-02046 | Low-Complexity Stochastic Generalized Belief Propagation | http://arxiv.org/abs/1605.02046 | id:1605.02046 author:Farzin Haddadpour, Mahdi Jafari Siavoshani, Morteza Noshad category:cs.LG cs.AI cs.IT math.IT  published:2016-05-06 summary:The generalized belief propagation (GBP), introduced by Yedidia et al., is an extension of the belief propagation (BP) algorithm, which is widely used in different problems involved in calculating exact or approximate marginals of probability distributions. In many problems, it has been observed that the accuracy of GBP considerably outperforms that of BP. However, because in general the computational complexity of GBP is higher than BP, its application is limited in practice. In this paper, we introduce a stochastic version of GBP called stochastic generalized belief propagation (SGBP) that can be considered as an extension to the stochastic BP (SBP) algorithm introduced by Noorshams et al. They have shown that SBP reduces the complexity per iteration of BP by an order of magnitude in alphabet size. In contrast to SBP, SGBP can reduce the computation complexity if certain topological conditions are met by the region graph associated to a graphical model. However, this reduction can be larger than only one order of magnitude in alphabet size. In this paper, we characterize these conditions and the amount of computation gain that we can obtain by using SGBP. Finally, using similar proof techniques employed by Noorshams et al., for general graphical models satisfy contraction conditions, we prove the asymptotic convergence of SGBP to the unique GBP fixed point, as well as providing non-asymptotic upper bounds on the mean square error and on the high probability error. version:1
arxiv-1603-08661 | Regret Analysis of the Anytime Optimally Confident UCB Algorithm | http://arxiv.org/abs/1603.08661 | id:1603.08661 author:Tor Lattimore category:cs.LG math.ST stat.ML stat.TH  published:2016-03-29 summary:I introduce and analyse an anytime version of the Optimally Confident UCB (OCUCB) algorithm designed for minimising the cumulative regret in finite-armed stochastic bandits with subgaussian noise. The new algorithm is simple, intuitive (in hindsight) and comes with the strongest finite-time regret guarantees for a horizon-free algorithm so far. I also show a finite-time lower bound that nearly matches the upper bound. version:2
arxiv-1605-02026 | Training Neural Networks Without Gradients: A Scalable ADMM Approach | http://arxiv.org/abs/1605.02026 | id:1605.02026 author:Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, Tom Goldstein category:cs.LG  published:2016-05-06 summary:With the growing importance of large network models and enormous training datasets, GPUs have become increasingly necessary to train neural networks. This is largely because conventional optimization algorithms rely on stochastic gradient methods that don't scale well to large numbers of cores in a cluster setting. Furthermore, the convergence of all gradient methods, including batch methods, suffers from common problems like saturation effects, poor conditioning, and saddle points. This paper explores an unconventional training method that uses alternating direction methods and Bregman iteration to train networks without gradient descent steps. The proposed method reduces the network training problem to a sequence of minimization sub-steps that can each be solved globally in closed form. The proposed method is advantageous because it avoids many of the caveats that make gradient methods slow on highly non-convex problems. The method exhibits strong scaling in the distributed setting, yielding linear speedups even when split over thousands of cores. version:1
arxiv-1605-02019 | Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec | http://arxiv.org/abs/1605.02019 | id:1605.02019 author:Christopher E Moody category:cs.CL  published:2016-05-06 summary:Distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them. version:1
arxiv-1512-08030 | Device and System Level Design Considerations for Analog-Non-Volatile-Memory Based Neuromorphic Architectures | http://arxiv.org/abs/1512.08030 | id:1512.08030 author:Sukru Burc Eryilmaz, Duygu Kuzum, Shimeng Yu, H. -S. Philip Wong category:cs.NE cs.AI  published:2015-12-25 summary:This paper gives an overview of recent progress in the brain inspired computing field with a focus on implementation using emerging memories as electronic synapses. Design considerations and challenges such as requirements and design targets on multilevel states, device variability, programming energy, array-level connectivity, fan-in/fanout, wire energy, and IR drop are presented. Wires are increasingly important in design decisions, especially for large systems, and cycle-to-cycle variations have large impact on learning performance. version:2
arxiv-1605-01999 | Visual Saliency Based on Scale-Space Analysis in the Frequency Domain | http://arxiv.org/abs/1605.01999 | id:1605.01999 author:Jian Li, Martin Levine, Xiangjing An, Xin Xu, Hangen He category:cs.CV  published:2016-05-06 summary:We address the issue of visual saliency from three perspectives. First, we consider saliency detection as a frequency domain analysis problem. Second, we achieve this by employing the concept of {\it non-saliency}. Third, we simultaneously consider the detection of salient regions of different size. The paper proposes a new bottom-up paradigm for detecting visual saliency, characterized by a scale-space analysis of the amplitude spectrum of natural images. We show that the convolution of the {\it image amplitude spectrum} with a low-pass Gaussian kernel of an appropriate scale is equivalent to such an image saliency detector. The saliency map is obtained by reconstructing the 2-D signal using the original phase and the amplitude spectrum, filtered at a scale selected by minimizing saliency map entropy. A Hypercomplex Fourier Transform performs the analysis in the frequency domain. Using available databases, we demonstrate experimentally that the proposed model can predict human fixation data. We also introduce a new image database and use it to show that the saliency detector can highlight both small and large salient regions, as well as inhibit repeated distractors in cluttered images. In addition, we show that it is able to predict salient regions on which people focus their attention. version:1
arxiv-1605-01950 | Automatic LQR Tuning Based on Gaussian Process Global Optimization | http://arxiv.org/abs/1605.01950 | id:1605.01950 author:Alonso Marco, Philipp Hennig, Jeannette Bohg, Stefan Schaal, Sebastian Trimpe category:cs.RO cs.LG cs.SY  published:2016-05-06 summary:This paper proposes an automatic controller tuning framework based on linear optimal control combined with Bayesian optimization. With this framework, an initial set of controller gains is automatically improved according to a pre-defined performance objective evaluated from experimental data. The underlying Bayesian optimization algorithm is Entropy Search, which represents the latent objective as a Gaussian process and constructs an explicit belief over the location of the objective minimum. This is used to maximize the information gain from each experimental evaluation. Thus, this framework shall yield improved controllers with fewer evaluations compared to alternative approaches. A seven-degree-of-freedom robot arm balancing an inverted pole is used as the experimental demonstrator. Results of a two- and four-dimensional tuning problems highlight the method's potential for automatic controller tuning on robotic platforms. version:1
arxiv-1605-01939 | Energy Disaggregation for Real-Time Building Flexibility Detection | http://arxiv.org/abs/1605.01939 | id:1605.01939 author:Elena Mocanu, Phuong H. Nguyen, Madeleine Gibescu category:stat.ML cs.AI cs.LG  published:2016-05-06 summary:Energy is a limited resource which has to be managed wisely, taking into account both supply-demand matching and capacity constraints in the distribution grid. One aspect of the smart energy management at the building level is given by the problem of real-time detection of flexible demand available. In this paper we propose the use of energy disaggregation techniques to perform this task. Firstly, we investigate the use of existing classification methods to perform energy disaggregation. A comparison is performed between four classifiers, namely Naive Bayes, k-Nearest Neighbors, Support Vector Machine and AdaBoost. Secondly, we propose the use of Restricted Boltzmann Machine to automatically perform feature extraction. The extracted features are then used as inputs to the four classifiers and consequently shown to improve their accuracy. The efficiency of our approach is demonstrated on a real database consisting of detailed appliance-level measurements with high temporal resolution, which has been used for energy disaggregation in previous studies, namely the REDD. The results show robustness and good generalization capabilities to newly presented buildings with at least 96% accuracy. version:1
arxiv-1605-01923 | UAV-based Autonomous Image Acquisition with Multi-View Stereo Quality Assurance by Confidence Prediction | http://arxiv.org/abs/1605.01923 | id:1605.01923 author:Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, Horst Bischof category:cs.CV cs.RO  published:2016-05-06 summary:In this paper we present an autonomous system for acquiring close-range high-resolution images that maximize the quality of a later-on 3D reconstruction with respect to coverage, ground resolution and 3D uncertainty. In contrast to previous work, our system uses the already acquired images to predict the confidence in the output of a dense multi-view stereo approach without executing it. This confidence encodes the likelihood of a successful reconstruction with respect to the observed scene and potential camera constellations. Our prediction module runs in real-time and can be trained without any externally recorded ground truth. We use the confidence prediction for on-site quality assurance and for planning further views that are tailored for a specific multi-view stereo approach with respect to the given scene. We demonstrate the capabilities of our approach with an autonomous Unmanned Aerial Vehicle (UAV) in a challenging outdoor scenario. version:1
arxiv-1605-01919 | User Reviews and Language: How Language Influences Ratings | http://arxiv.org/abs/1605.01919 | id:1605.01919 author:Scott A. Hale category:cs.HC cs.CL cs.CY H.5.m; H.3.5  published:2016-05-06 summary:The number of user reviews of tourist attractions, restaurants, mobile apps, etc. is increasing for all languages; yet, research is lacking on how reviews in multiple languages should be aggregated and displayed. Speakers of different languages may have consistently different experiences, e.g., different information available in different languages at tourist attractions or different user experiences with software due to internationalization/localization choices. This paper assesses the similarity in the ratings given by speakers of different languages to London tourist attractions on TripAdvisor. The correlations between different languages are generally high, but some language pairs are more correlated than others. The results question the common practice of computing average ratings from reviews in many languages. version:1
arxiv-1504-07829 | Market forecasting using Hidden Markov Models | http://arxiv.org/abs/1504.07829 | id:1504.07829 author:Sara Rebagliati, Emanuela Sasso, Samuele Soraggi category:stat.ML cs.LG 91B84  published:2015-04-29 summary:Working on the daily closing prices and logreturns, in this paper we deal with the use of Hidden Markov Models (HMMs) to forecast the price of the EUR/USD Futures. The aim of our work is to understand how the HMMs describe different financial time series depending on their structure. Subsequently, we analyse the forecasting methods exposed in the previous literature, putting on evidence their pros and cons. version:2
arxiv-1507-00333 | Notes on Low-rank Matrix Factorization | http://arxiv.org/abs/1507.00333 | id:1507.00333 author:Yuan Lu, Jie Yang category:cs.NA cs.IR cs.LG  published:2015-06-30 summary:Low-rank matrix factorization (MF) is an important technique in data science. The key idea of MF is that there exists latent structures in the data, by uncovering which we could obtain a compressed representation of the data. By factorizing an original matrix to low-rank matrices, MF provides a unified method for dimension reduction, clustering, and matrix completion. In this article we review several important variants of MF, including: Basic MF, Non-negative MF, Orthogonal non-negative MF. As can be told from their names, non-negative MF and orthogonal non-negative MF are variants of basic MF with non-negativity and/or orthogonality constraints. Such constraints are useful in specific senarios. In the first part of this article, we introduce, for each of these models, the application scenarios, the distinctive properties, and the optimizing method. By properly adapting MF, we can go beyond the problem of clustering and matrix completion. In the second part of this article, we will extend MF to sparse matrix compeletion, enhance matrix compeletion using various regularization methods, and make use of MF for (semi-)supervised learning by introducing latent space reinforcement and transformation. We will see that MF is not only a useful model but also as a flexible framework that is applicable for various prediction problems. version:3
arxiv-1605-01855 | Resource allocation using metaheuristic search | http://arxiv.org/abs/1605.01855 | id:1605.01855 author:Andy M. Connor, Amit Shah category:cs.NE  published:2016-05-06 summary:This research is focused on solving problems in the area of software project management using metaheuristic search algorithms and as such is research in the field of search based software engineering. The main aim of this research is to evaluate the performance of different metaheuristic search techniques in resource allocation and scheduling problems that would be typical of software development projects. This paper reports a set of experiments which evaluate the performance of three algorithms, namely simulated annealing, tabu search and genetic algorithms. The experimental results indicate that all of the metaheuristics search techniques can be used to solve problems in resource allocation and scheduling within a software project. Finally, a comparative analysis suggests that overall the genetic algorithm had performed better than simulated annealing and tabu search. version:1
arxiv-1605-01845 | Detecting Context Dependence in Exercise Item Candidates Selected from Corpora | http://arxiv.org/abs/1605.01845 | id:1605.01845 author:Ildikó Pilán category:cs.CL  published:2016-05-06 summary:We explore the factors influencing the dependence of single sentences on their larger textual context in order to automatically identify candidate sentences for language learning exercises from corpora which are presentable in isolation. An in-depth investigation of this question has not been previously carried out. Understanding this aspect can contribute to a more efficient selection of candidate sentences which, besides reducing the time required for item writing, can also ensure a higher degree of variability and authenticity. We present a set of relevant aspects collected based on the qualitative analysis of a smaller set of context-dependent corpus example sentences. Furthermore, we implemented a rule-based algorithm using these criteria which achieved an average precision of 0.76 for the identification of different issues related to context dependence. The method has also been evaluated empirically where 80% of the sentences in which our system did not detect context-dependent elements were also considered context-independent by human raters. version:1
arxiv-1603-05729 | Convergence of Contrastive Divergence Algorithm in Exponential Family | http://arxiv.org/abs/1603.05729 | id:1603.05729 author:Tung-Yu Wu, Bai Jiang, Yifan Jin, Wing H. Wong category:stat.ML 68W48  60J20  93E15  published:2016-03-17 summary:This paper studies the convergence properties of contrastive divergence algorithm for parameter inference in exponential family, by relating it to Markov chain theory and stochastic stability literature. We prove that, under mild conditions and given a finite data sample $X_1,\dots,X_n \sim p_{\theta^*}$ i.i.d. in an event with probability approaching to 1, the sequence $\{\theta_t\}_{t \ge 0}$ generated by CD algorithm is a positive Harris recurrent chain, and thus processes an unique invariant distribution $\pi_n$. The invariant distribution concentrates around the Maximum Likelihood Estimate at a speed arbitrarily slower than $\sqrt{n}$, and the number of steps in Markov Chain Monte Carlo only affects the coefficient factor of the concentration rate. Finally we conclude that as $n \to \infty$, $$\limsup_{t \to \infty} \left\Vert \frac{1}{t} \sum_{s=1}^t \theta_s - \theta^*\right\Vert \overset{p}{\to} 0.$$ version:2
arxiv-1605-01843 | Perceptually Consistent Color-to-Gray Image Conversion | http://arxiv.org/abs/1605.01843 | id:1605.01843 author:Shaodi You, Nick Barnes, Janine Walker category:cs.CV  published:2016-05-06 summary:In this paper, we propose a color to grayscale image conversion algorithm (C2G) that aims to preserve the perceptual properties of the color image as much as possible. To this end, we propose measures for two perceptual properties based on contemporary research in vision science: brightness and multi-scale contrast. The brightness measurement is based on the idea that the brightness of a grayscale image will affect the perception of the probability of color information. The color contrast measurement is based on the idea that the contrast of a given pixel to its surroundings can be measured as a linear combination of color contrast at different scales. Based on these measures we propose a graph based optimization framework to balance the brightness and contrast measurements. To solve the optimization, an $\ell_1$-norm based method is provided which converts color discontinuities to brightness discontinuities. To validate our methods, we evaluate against the existing \cadik and Color250 datasets, and against NeoColor, a new dataset that improves over existing C2G datasets. NeoColor contains around 300 images from typical C2G scenarios, including: commercial photograph, printing, books, magazines, masterpiece artworks and computer designed graphics. We show improvements in metrics of performance, and further through a user study, we validate the performance of both the algorithm and the metric. version:1
arxiv-1605-01839 | Beyond Local Search: Tracking Objects Everywhere with Instance-Specific Proposals | http://arxiv.org/abs/1605.01839 | id:1605.01839 author:Gao Zhu, Fatih Porikli, Hongdong Li category:cs.CV  published:2016-05-06 summary:Most tracking-by-detection methods employ a local search window around the predicted object location in the current frame assuming the previous location is accurate, the trajectory is smooth, and the computational capacity permits a search radius that can accommodate the maximum speed yet small enough to reduce mismatches. These, however, may not be valid always, in particular for fast and irregularly moving objects. Here, we present an object tracker that is not limited to a local search window and has ability to probe efficiently the entire frame. Our method generates a small number of "high-quality" proposals by a novel instance-specific objectness measure and evaluates them against the object model that can be adopted from an existing tracking-by-detection approach as a core tracker. During the tracking process, we update the object model concentrating on hard false-positives supplied by the proposals, which help suppressing distractors caused by difficult background clutters, and learn how to re-rank proposals according to the object model. Since we reduce significantly the number of hypotheses the core tracker evaluates, we can use richer object descriptors and stronger detector. Our method outperforms most recent state-of-the-art trackers on popular tracking benchmarks, and provides improved robustness for fast moving objects as well as for ultra low-frame-rate videos. version:1
arxiv-1605-01838 | DeepPicker: a Deep Learning Approach for Fully Automated Particle Picking in Cryo-EM | http://arxiv.org/abs/1605.01838 | id:1605.01838 author:Feng Wang, Huichao Gong, Gaochao liu, Meijing Li, Chuangye Yan, Tian Xia, Xueming Li, Jianyang Zeng category:q-bio.QM cs.LG  published:2016-05-06 summary:Particle picking is a time-consuming step in single-particle analysis and often requires significant interventions from users, which has become a bottleneck for future automated electron cryo-microscopy (cryo-EM). Here we report a deep learning framework, called DeepPicker, to address this problem and fill the current gaps toward a fully automated cryo-EM pipeline. DeepPicker employs a novel cross-molecule training strategy to capture common features of particles from previously-analyzed micrographs, and thus does not require any human intervention during particle picking. Tests on the recently-published cryo-EM data of three complexes have demonstrated that our deep learning based scheme can successfully accomplish the human-level particle picking process and identify a sufficient number of particles that are comparable to those manually by human experts. These results indicate that DeepPicker can provide a practically useful tool to significantly reduce the time and manual effort spent in single-particle analysis and thus greatly facilitate high-resolution cryo-EM structure determination. version:1
arxiv-1605-01832 | Cross-Graph Learning of Multi-Relational Associations | http://arxiv.org/abs/1605.01832 | id:1605.01832 author:Hanxiao Liu, Yiming Yang category:cs.LG  published:2016-05-06 summary:Cross-graph Relational Learning (CGRL) refers to the problem of predicting the strengths or labels of multi-relational tuples of heterogeneous object types, through the joint inference over multiple graphs which specify the internal connections among each type of objects. CGRL is an open challenge in machine learning due to the daunting number of all possible tuples to deal with when the numbers of nodes in multiple graphs are large, and because the labeled training instances are extremely sparse as typical. Existing methods such as tensor factorization or tensor-kernel machines do not work well because of the lack of convex formulation for the optimization of CGRL models, the poor scalability of the algorithms in handling combinatorial numbers of tuples, and/or the non-transductive nature of the learning methods which limits their ability to leverage unlabeled data in training. This paper proposes a novel framework which formulates CGRL as a convex optimization problem, enables transductive learning using both labeled and unlabeled tuples, and offers a scalable algorithm that guarantees the optimal solution and enjoys a linear time complexity with respect to the sizes of input graphs. In our experiments with a subset of DBLP publication records and an Enzyme multi-source dataset, the proposed method successfully scaled to the large cross-graph inference problem, and outperformed other representative approaches significantly. version:1
arxiv-1605-01825 | Robust Optical Flow Estimation of Double-Layer Images under Transparency or Reflection | http://arxiv.org/abs/1605.01825 | id:1605.01825 author:Jiaolong Yang, Hongdong Li, Yuchao Dai, Robby T. Tan category:cs.CV  published:2016-05-06 summary:This paper deals with a challenging, frequently encountered, yet not properly investigated problem in two-frame optical flow estimation. That is, the input frames are compounds of two imaging layers -- one desired background layer of the scene, and one distracting, possibly moving layer due to transparency or reflection. In this situation, the conventional brightness constancy constraint -- the cornerstone of most existing optical flow methods -- will no longer be valid. In this paper, we propose a robust solution to this problem. The proposed method performs both optical flow estimation, and image layer separation. It exploits a generalized double-layer brightness consistency constraint connecting these two tasks, and utilizes the priors for both of them. Experiments on both synthetic data and real images have confirmed the efficacy of the proposed method. To the best of our knowledge, this is the first attempt towards handling generic optical flow fields of two-frame images containing transparency or reflection. version:1
arxiv-1502-03571 | Weighted SGD for $\ell_p$ Regression with Randomized Preconditioning | http://arxiv.org/abs/1502.03571 | id:1502.03571 author:Jiyan Yang, Yin-Lam Chow, Christopher Ré, Michael W. Mahoney category:math.OC stat.ML  published:2015-02-12 summary:In recent years, stochastic gradient descent (SGD) methods and randomized linear algebra (RLA) algorithms have been applied to many large-scale problems in machine learning and data analysis. We aim to bridge the gap between these two methods in solving constrained overdetermined linear regression problems---e.g., $\ell_2$ and $\ell_1$ regression problems. We propose a hybrid algorithm named pwSGD that uses RLA techniques for preconditioning and constructing an importance sampling distribution, and then performs an SGD-like iterative process with weighted sampling on the preconditioned system. We prove that pwSGD inherits faster convergence rates that only depend on the lower dimension of the linear system, while maintaining low computation complexity. Particularly, when solving $\ell_1$ regression with size $n$ by $d$, pwSGD returns an approximate solution with $\epsilon$ relative error in the objective value in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d)/\epsilon^2)$ time. This complexity is uniformly better than that of RLA methods in terms of both $\epsilon$ and $d$ when the problem is unconstrained. For $\ell_2$ regression, pwSGD returns an approximate solution with $\epsilon$ relative error in the objective value and the solution vector measured in prediction norm in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d) \log(1/\epsilon) /\epsilon)$ time. We also provide lower bounds on the coreset complexity for more general regression problems, indicating that still new ideas will be needed to extend similar RLA preconditioning ideas to weighted SGD algorithms for more general regression problems. Finally, the effectiveness of such algorithms is illustrated numerically on both synthetic and real datasets. version:3
arxiv-1605-01813 | Estimating Sparse Signals with Smooth Support via Convex Programming and Block Sparsity | http://arxiv.org/abs/1605.01813 | id:1605.01813 author:Sohil Shah, Tom Goldstein, Christoph Studer category:cs.CV  published:2016-05-06 summary:Conventional algorithms for sparse signal recovery and sparse representation rely on $l_1$-norm regularized variational methods. However, when applied to the reconstruction of $\textit{sparse images}$, i.e., images where only a few pixels are non-zero, simple $l_1$-norm-based methods ignore potential correlations in the support between adjacent pixels. In a number of applications, one is interested in images that are not only sparse, but also have a support with smooth (or contiguous) boundaries. Existing algorithms that take into account such a support structure mostly rely on non-convex methods and---as a consequence---do not scale well to high-dimensional problems and/or do not converge to global optima. In this paper, we explore the use of new block $l_1$-norm regularizers, which enforce image sparsity while simultaneously promoting smooth support structure. By exploiting the convexity of our regularizers, we develop new computationally-efficient recovery algorithms that guarantee global optimality. We demonstrate the efficacy of our regularizers on a variety of imaging tasks including compressive image recovery, image restoration, and robust PCA. version:1
arxiv-1504-01369 | Information Recovery from Pairwise Measurements | http://arxiv.org/abs/1504.01369 | id:1504.01369 author:Yuxin Chen, Changho Suh, Andrea J. Goldsmith category:cs.IT cs.DM cs.LG math.IT math.ST stat.ML stat.TH  published:2015-04-06 summary:This paper is concerned with jointly recovering $n$ node-variables $\left\{ x_{i}\right\}_{1\leq i\leq n}$ from a collection of pairwise difference measurements. Imagine we acquire a few observations taking the form of $x_{i}-x_{j}$; the observation pattern is represented by a measurement graph $\mathcal{G}$ with an edge set $\mathcal{E}$ such that $x_{i}-x_{j}$ is observed if and only if $(i,j)\in\mathcal{E}$. To account for noisy measurements in a general manner, we model the data acquisition process by a set of channels with given input/output transition measures. Employing information-theoretic tools applied to channel decoding problems, we develop a \emph{unified} framework to characterize the fundamental recovery criterion, which accommodates general graph structures, alphabet sizes, and channel transition measures. In particular, our results isolate a family of \emph{minimum} \emph{channel divergence measures} to characterize the degree of measurement corruption, which together with the size of the minimum cut of $\mathcal{G}$ dictates the feasibility of exact information recovery. For various homogeneous graphs, the recovery condition depends almost only on the edge sparsity of the measurement graph irrespective of other graphical metrics; alternatively, the minimum sample complexity required for these graphs scales like \[ \text{minimum sample complexity }\asymp\frac{n\log n}{\mathsf{Hel}_{1/2}^{\min}} \] for certain information metric $\mathsf{Hel}_{1/2}^{\min}$ defined in the main text, as long as the alphabet size is not super-polynomial in $n$. We apply our general theory to three concrete applications, including the stochastic block model, the outlier model, and the haplotype assembly problem. Our theory leads to order-wise tight recovery conditions for all these scenarios. version:4
arxiv-1605-01790 | Robust SAR STAP via Kronecker Decomposition | http://arxiv.org/abs/1605.01790 | id:1605.01790 author:Kristjan Greenewald, Edmund Zelnio, Alfred Hero category:cs.CV  published:2016-05-05 summary:This paper proposes a spatio-temporal decomposition for the detection of moving targets in multiantenna SAR. As a high resolution radar imaging modality, SAR detects and localizes non-moving targets accurately, giving it an advantage over lower resolution GMTI radars. Moving target detection is more challenging due to target smearing and masking by clutter. Space-time adaptive processing (STAP) is often used to remove the stationary clutter and enhance the moving targets. In this work, it is shown that the performance of STAP can be improved by modeling the clutter covariance as a space vs. time Kronecker product with low rank factors. Based on this model, a low-rank Kronecker product covariance estimation algorithm is proposed, and a novel separable clutter cancelation filter based on the Kronecker covariance estimate is introduced. The proposed method provides orders of magnitude reduction in the required number of training samples, as well as improved robustness to corruption of the training data. Simulation results and experiments using the Gotcha SAR GMTI challenge dataset are presented that confirm the advantages of our approach relative to existing techniques. version:1
arxiv-1603-06140 | Adaptive coherence estimator (ACE) for explosive hazard detection using wideband electromagnetic induction (WEMI) | http://arxiv.org/abs/1603.06140 | id:1603.06140 author:Brendan Alvey, Alina Zare, Matthew Cook, Dominic K. Ho category:cs.CV  published:2016-03-19 summary:The adaptive coherence estimator (ACE) estimates the squared cosine of the angle between a known target vector and a sample vector in a whitened coordinate space. The space is whitened according to an estimation of the background statistics, which directly effects the performance of the statistic as a target detector. In this paper, the ACE detection statistic is used to detect buried explosive hazards with data from a Wideband Electromagnetic Induction (WEMI) sensor. Target signatures are based on a dictionary defined using a Discrete Spectrum of Relaxation Frequencies (DSRF) model. Results are summarized as a receiver operator curve (ROC) and compared to other leading methods. version:3
arxiv-1506-03101 | Provable Bayesian Inference via Particle Mirror Descent | http://arxiv.org/abs/1506.03101 | id:1506.03101 author:Bo Dai, Niao He, Hanjun Dai, Le Song category:cs.LG stat.CO stat.ML  published:2015-06-09 summary:Bayesian methods are appealing in their flexibility in modeling complex data and ability in capturing uncertainty in parameters. However, when Bayes' rule does not result in tractable closed-form, most approximate inference algorithms lack either scalability or rigorous guarantees. To tackle this challenge, we propose a simple yet provable algorithm, \emph{Particle Mirror Descent} (PMD), to iteratively approximate the posterior density. PMD is inspired by stochastic functional mirror descent where one descends in the density space using a small batch of data points at each iteration, and by particle filtering where one uses samples to approximate a function. We prove result of the first kind that, with $m$ particles, PMD provides a posterior density estimator that converges in terms of $KL$-divergence to the true posterior in rate $O(1/\sqrt{m})$. We demonstrate competitive empirical performances of PMD compared to several approximate inference algorithms in mixture models, logistic regression, sparse Gaussian processes and latent Dirichlet allocation on large scale datasets. version:3
arxiv-1605-01779 | Clustering on the Edge: Learning Structure in Graphs | http://arxiv.org/abs/1605.01779 | id:1605.01779 author:Matt Barnes, Artur Dubrawski category:stat.ML  published:2016-05-05 summary:With the recent popularity of graphical clustering methods, there has been an increased focus on the information between samples. We show how learning cluster structure using edge features naturally and simultaneously determines the most likely number of clusters and addresses data scale issues. These results are particularly useful in instances where (a) there are a large number of clusters and (b) we have some labeled edges. Applications in this domain include image segmentation, community discovery and entity resolution. Our model is an extension of the planted partition model and our solution uses results of correlation clustering, which achieves a partition O(log(n))-close to the log-likelihood of the true clustering. version:1
arxiv-1512-04407 | We Are Humor Beings: Understanding and Predicting Visual Humor | http://arxiv.org/abs/1512.04407 | id:1512.04407 author:Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh category:cs.CV cs.CL cs.LG  published:2015-12-14 summary:Humor is an integral part of human lives. Despite being tremendously impactful, it is perhaps surprising that we do not have a detailed understanding of humor yet. As interactions between humans and AI systems increase, it is imperative that these systems are taught to understand subtleties of human expressions such as humor. In this work, we are interested in the question - what content in a scene causes it to be funny? As a first step towards understanding visual humor, we analyze the humor manifested in abstract scenes and design computational models for them. We collect two datasets of abstract scenes that facilitate the study of humor at both the scene-level and the object-level. We analyze the funny scenes and explore the different types of humor depicted in them via human studies. We model two tasks that we believe demonstrate an understanding of some aspects of visual humor. The tasks involve predicting the funniness of a scene and altering the funniness of a scene. We show that our models perform well quantitatively, and qualitatively through human studies. Our datasets are publicly available. version:4
arxiv-1605-01749 | Rank Ordered Autoencoders | http://arxiv.org/abs/1605.01749 | id:1605.01749 author:Paul Bertens category:cs.LG stat.ML I.2.6; I.5.1; I.4.2  published:2016-05-05 summary:A new method for the unsupervised learning of sparse representations using autoencoders is proposed and implemented by ordering the output of the hidden units by their activation value and progressively reconstructing the input in this order. This can be done efficiently in parallel with the use of cumulative sums and sorting only slightly increasing the computational costs. Minimizing the difference of this progressive reconstruction with respect to the input can be seen as minimizing the number of active output units required for the reconstruction of the input. The model thus learns to reconstruct optimally using the least number of active output units. This leads to high sparsity without the need for extra hyperparameters, the amount of sparsity is instead implicitly learned by minimizing this progressive reconstruction error. Results of the trained model are given for patches of the CIFAR10 dataset, showing rapid convergence of features and extremely sparse output activations while maintaining a minimal reconstruction error and showing extreme robustness to overfitting. Additionally the reconstruction as function of number of active units is presented which shows the autoencoder learns a rank order code over the input where the highest ranked units correspond to the highest decrease in reconstruction error. version:1
arxiv-1507-01238 | Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit | http://arxiv.org/abs/1507.01238 | id:1507.01238 author:Chong You, Daniel P. Robinson, Rene Vidal category:cs.CV cs.LG stat.ML  published:2015-07-05 summary:Subspace clustering methods based on $\ell_1$, $\ell_2$ or nuclear norm regularization have become very popular due to their simplicity, theoretical guarantees and empirical success. However, the choice of the regularizer can greatly impact both theory and practice. For instance, $\ell_1$ regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad conditions (e.g., arbitrary subspaces and corrupted data). However, it requires solving a large scale convex optimization problem. On the other hand, $\ell_2$ and nuclear norm regularization provide efficient closed form solutions, but require very strong assumptions to guarantee a subspace-preserving affinity, e.g., independent subspaces and uncorrupted data. In this paper we study a subspace clustering method based on orthogonal matching pursuit. We show that the method is both computationally efficient and guaranteed to give a subspace-preserving affinity under broad conditions. Experiments on synthetic data verify our theoretical analysis, and applications in handwritten digit and face clustering show that our approach achieves the best trade off between accuracy and efficiency. version:3
arxiv-1605-01746 | Biobjective Performance Assessment with the COCO Platform | http://arxiv.org/abs/1605.01746 | id:1605.01746 author:Dimo Brockhoff, Tea Tušar, Dejan Tušar, Tobias Wagner, Nikolaus Hansen, Anne Auger category:cs.NE  published:2016-05-05 summary:This document details the rationales behind assessing the performance of numerical black-box optimizers on multi-objective problems within the COCO platform and in particular on the biobjective test suite bbob-biobj. The evaluation is based on a hypervolume of all non-dominated solutions in the archive of candidate solutions and measures the runtime until the hypervolume value succeeds prescribed target values. version:1
arxiv-1605-01744 | Improving Automated Patent Claim Parsing: Dataset, System, and Experiments | http://arxiv.org/abs/1605.01744 | id:1605.01744 author:Mengke Hu, David Cinciruk, John MacLaren Walsh category:cs.CL  published:2016-05-05 summary:Off-the-shelf natural language processing software performs poorly when parsing patent claims owing to their use of irregular language relative to the corpora built from news articles and the web typically utilized to train this software. Stopping short of the extensive and expensive process of accumulating a large enough dataset to completely retrain parsers for patent claims, a method of adapting existing natural language processing software towards patent claims via forced part of speech tag correction is proposed. An Amazon Mechanical Turk collection campaign organized to generate a public corpus to train such an improved claim parsing system is discussed, identifying lessons learned during the campaign that can be of use in future NLP dataset collection campaigns with AMT. Experiments utilizing this corpus and other patent claim sets measure the parsing performance improvement garnered via the claim parsing system. Finally, the utility of the improved claim parsing system within other patent processing applications is demonstrated via experiments showing improved automated patent subject classification when the new claim parsing system is utilized to generate the features. version:1
arxiv-1605-01710 | Plug-and-Play ADMM for Image Restoration: Fixed Point Convergence and Applications | http://arxiv.org/abs/1605.01710 | id:1605.01710 author:Stanley H. Chan, Xiran Wang, Omar A. Elgendy category:cs.CV  published:2016-05-05 summary:Alternating direction method of multiplier (ADMM) is a widely used algorithm for solving constrained optimization problems in image restoration. Among many useful features, one critical feature of the ADMM algorithm is its modular structure which allows one to plug in any off-the-shelf image denoising algorithm for a subproblem in the ADMM algorithm. Because of the plug-in nature, this type of ADMM algorithms is coined the name "Plug-and-Play ADMM". Plug-and-Play ADMM has demonstrated promising empirical results in a number of recent papers. However, it is unclear under what conditions and for what denoising algorithms would it guarantee convergence. Also, it is unclear to what extent would Plug-and-Play ADMM be compared to existing methods for common Gaussian and Poissonian image restoration problems. In this paper, we propose a Plug-and-Play ADMM algorithm with provable fixed point convergence. We show that for any denoising algorithm satisfying a boundedness criteria, called bounded denoisers, Plug-and-Play ADMM converges to a fixed point under a continuation scheme. We demonstrate applications of Plug-and-Play ADMM on two image restoration problems including single image super-resolution and quantized Poisson image recovery for single-photon imaging. We compare Plug-and-Play ADMM with state-of-the-art algorithms in each problem type, and demonstrate promising experimental results of the algorithm. version:1
arxiv-1605-01703 | A note on adjusting $R^2$ for using with cross-validation | http://arxiv.org/abs/1605.01703 | id:1605.01703 author:Indre Zliobaite, Nikolaj Tatti category:cs.LG cs.AI stat.ML  published:2016-05-05 summary:We show how to adjust the coefficient of determination ($R^2$) when used for measuring predictive accuracy via leave-one-out cross-validation. version:1
arxiv-1604-02125 | Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes | http://arxiv.org/abs/1604.02125 | id:1604.02125 author:Gordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol, Yash Goyal, Kevin Kochersberger, Dhruv Batra category:cs.CV cs.CL cs.LG  published:2016-04-07 summary:We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. The motivation for this work comes from the fact that some ambiguities in language simply cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence "I shot an elephant in my pajamas", looking at the language alone (and not reasoning about common sense), it is unclear if it is the person or the elephant that is wearing the pajamas or both. Our approach involves producing a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly re-ranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. We also show that multiple hypotheses are crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms a state-of-the-art NLP system (Stanford Parser [16,27]) by 17.91% (28.69% relative) in one experiment, and by 12.83% (25.28% relative) in another. We also make small improvements over a state-of-the-art vision system (DeepLab-CRF [13]). version:2
arxiv-1605-00972 | Phase 2: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Machine Learning Detection Algorithms | http://arxiv.org/abs/1605.00972 | id:1605.00972 author:Peter J. Dugan, Christopher W. Clark, Yann André LeCun, Sofie M. Van Parijs category:cs.CV  published:2016-05-03 summary:Overarching goals for this work aim to advance the state of the art for detection, classification and localization (DCL) in the field of bioacoustics. This goal is primarily achieved by building a generic framework for detection-classification (DC) using a fast, efficient and scalable architecture, demonstrating the capabilities of this system using on a variety of low-frequency mid-frequency cetacean sounds. Two primary goals are to develop transferable technologies for detection and classification in, one: the area of advanced algorithms, such as deep learning and other methods; and two: advanced systems, capable of real-time and archival processing. For each key area, we will focus on producing publications from this work and providing tools and software to the community where/when possible. Currently massive amounts of acoustic data are being collected by various institutions, corporations and national defense agencies. The long-term goal is to provide technical capability to analyze the data using automatic algorithms for (DC) based on machine intelligence. The goal of the automation is to provide effective and efficient mechanisms by which to process large acoustic datasets for understanding the bioacoustic behaviors of marine mammals. This capability will provide insights into the potential ecological impacts and influences of anthropogenic ocean sounds. This work focuses on building technologies using a maturity model based on DARPA 6.1 and 6.2 processes, for basic and applied research, respectively. version:2
arxiv-1605-01679 | Learning Action Maps of Large Environments via First-Person Vision | http://arxiv.org/abs/1605.01679 | id:1605.01679 author:Nicholas Rhinehart, Kris M. Kitani category:cs.CV  published:2016-05-05 summary:When people observe and interact with physical spaces, they are able to associate functionality to regions in the environment. Our goal is to automate dense functional understanding of large spaces by leveraging sparse activity demonstrations recorded from an ego-centric viewpoint. The method we describe enables functionality estimation in large scenes where people have behaved, as well as novel scenes where no behaviors are observed. Our method learns and predicts "Action Maps", which encode the ability for a user to perform activities at various locations. With the usage of an egocentric camera to observe human activities, our method scales with the size of the scene without the need for mounting multiple static surveillance cameras and is well-suited to the task of observing activities up-close. We demonstrate that by capturing appearance-based attributes of the environment and associating these attributes with activity demonstrations, our proposed mathematical framework allows for the prediction of Action Maps in new environments. Additionally, we offer a preliminary glance of the applicability of Action Maps by demonstrating a proof-of-concept application in which they are used in concert with activity detections to perform localization. version:1
arxiv-1605-01661 | Parallels of human language in the behavior of bottlenose dolphins | http://arxiv.org/abs/1605.01661 | id:1605.01661 author:R. Ferrer-i-Cancho, D. Lusseau, B. McCowan category:q-bio.NC cs.CL  published:2016-05-05 summary:A short review of similarities between dolphins and humans with the help of quantitative linguistics and information theory. version:1
arxiv-1605-01656 | A Tight Bound of Hard Thresholding | http://arxiv.org/abs/1605.01656 | id:1605.01656 author:Jie Shen, Ping Li category:stat.ML cs.IT math.IT  published:2016-05-05 summary:This paper is concerned with the hard thresholding technique which sets all but the $k$ largest absolute elements to zero. We establish a tight bound that quantitatively characterizes the deviation of the thresholded solution from a given signal. Our theoretical result is universal in the sense that it holds for all choices of parameters, and the underlying analysis only depends on fundamental arguments in mathematical optimization. We discuss the implications for the literature: Compressed Sensing. On account of the crucial estimate, we bridge the connection between restricted isometry property (RIP) and the sparsity parameter of $k$ for a vast volume of hard thresholding based algorithms, which renders an improvement on the RIP condition especially when the true sparsity is unknown. This suggests that in essence, many more kinds of sensing matrices or fewer measurements are admissible for the data acquisition procedure. Machine Learning. In terms of large-scale machine learning, a significant yet challenging problem is producing sparse solutions in online setting. In stark contrast to prior works that attempted the $\ell_1$ relaxation for promoting sparsity, we present a novel algorithm which performs hard thresholding in each iteration to ensure such parsimonious solutions. Equipped with the developed bound for hard thresholding, we prove global linear convergence for a number of prevalent statistical models under mild assumptions, even though the problem turns out to be non-convex. version:1
arxiv-1605-01655 | Stance and Sentiment in Tweets | http://arxiv.org/abs/1605.01655 | id:1605.01655 author:Saif M. Mohammad, Parinaz Sobhani, Svetlana Kiritchenko category:cs.CL  published:2016-05-05 summary:We can often detect from a person's utterances whether he/she is in favor of or against a given target entity -- their stance towards the target. However, a person may express the same stance towards a target by using negative or positive language. Here for the first time we present a dataset of tweet--target pairs annotated for both stance and sentiment. The targets may or may not be referred to in the tweets, and they may or may not be the target of opinion in the tweets. Partitions of this dataset were used as training and test sets in a SemEval-2016 shared task competition. We propose a simple stance detection system that outperforms submissions from all 19 teams that participated in the shared task. Additionally, access to both stance and sentiment annotations allows us to explore several research questions. We show that while knowing the sentiment expressed by a tweet is beneficial for stance classification, it alone is not sufficient. Finally, we use additional unlabeled data through distant supervision techniques and word embeddings to further improve stance classification. version:1
arxiv-1605-01652 | LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues | http://arxiv.org/abs/1605.01652 | id:1605.01652 author:Phong Le, Marc Dymetman, Jean-Michel Renders category:cs.AI cs.CL  published:2016-05-05 summary:We introduce an LSTM-based method for dynamically integrating several word-prediction experts to obtain a conditional language model which can be good simultaneously at several subtasks. We illustrate this general approach with an application to dialogue where we integrate a neural chat model, good at conversational aspects, with a neural question-answering model, good at retrieving precise information from a knowledge-base, and show how the integration combines the strengths of the independent components. We hope that this focused contribution will attract attention on the benefits of using such mixtures of experts in NLP. version:1
arxiv-1605-01635 | The IBM Speaker Recognition System: Recent Advances and Error Analysis | http://arxiv.org/abs/1605.01635 | id:1605.01635 author:Seyed Omid Sadjadi, Jason Pelecanos, Sriram Ganapathy category:cs.CL cs.SD stat.ML  published:2016-05-05 summary:We present the recent advances along with an error analysis of the IBM speaker recognition system for conversational speech. Some of the key advancements that contribute to our system include: a nearest-neighbor discriminant analysis (NDA) approach (as opposed to LDA) for intersession variability compensation in the i-vector space, the application of speaker and channel-adapted features derived from an automatic speech recognition (ASR) system for speaker recognition, and the use of a DNN acoustic model with a very large number of output units (~10k senones) to compute the frame-level soft alignments required in the i-vector estimation process. We evaluate these techniques on the NIST 2010 SRE extended core conditions (C1-C9), as well as the 10sec-10sec condition. To our knowledge, results achieved by our system represent the best performances published to date on these conditions. For example, on the extended tel-tel condition (C5) the system achieves an EER of 0.59%. To garner further understanding of the remaining errors (on C5), we examine the recordings associated with the low scoring target trials, where various issues are identified for the problematic recordings/trials. Interestingly, it is observed that correcting the pathological recordings not only improves the scores for the target trials but also for the nontarget trials. version:1
arxiv-1605-01623 | On the Convergence of A Family of Robust Losses for Stochastic Gradient Descent | http://arxiv.org/abs/1605.01623 | id:1605.01623 author:Bo Han, Ivor W. Tsang, Ling Chen category:cs.LG  published:2016-05-05 summary:The convergence of Stochastic Gradient Descent (SGD) using convex loss functions has been widely studied. However, vanilla SGD methods using convex losses cannot perform well with noisy labels, which adversely affect the update of the primal variable in SGD methods. Unfortunately, noisy labels are ubiquitous in real world applications such as crowdsourcing. To handle noisy labels, in this paper, we present a family of robust losses for SGD methods. By employing our robust losses, SGD methods successfully reduce negative effects caused by noisy labels on each update of the primal variable. We not only reveal that the convergence rate is O(1/T) for SGD methods using robust losses, but also provide the robustness analysis on two representative robust losses. Comprehensive experimental results on six real-world datasets show that SGD methods using robust losses are obviously more robust than other baseline methods in most situations with fast convergence. version:1
arxiv-1603-09687 | Large Scale Deep Convolutional Neural Network Features Search with Lucene | http://arxiv.org/abs/1603.09687 | id:1603.09687 author:Claudio Gennaro category:cs.CV cs.IR  published:2016-03-31 summary:In this work, we propose an approach to index Deep Convolutional Neural Network Features to support efficient content-based retrieval on large image databases. To this aim, we have converted the these features into a textual form, to index them into an inverted index by means of Lucene. In this way, we were able to set up a robust retrieval system that combines full-text search with content-based image retrieval capabilities. We evaluated different strategies of textual representation in order to optimize the index occupation and the query response time. In order to show that our approach is able to handle large datasets, we have developed a web-based prototype that provides an interface for combined textual and visual searching into a dataset of about 100 million of images. version:3
arxiv-1412-4564 | MatConvNet - Convolutional Neural Networks for MATLAB | http://arxiv.org/abs/1412.4564 | id:1412.4564 author:Andrea Vedaldi, Karel Lenc category:cs.CV cs.LG cs.MS cs.NE  published:2014-12-15 summary:MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for MATLAB. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing linear convolutions with filter banks, feature pooling, and many more. In this manner, MatConvNet allows fast prototyping of new CNN architectures; at the same time, it supports efficient computation on CPU and GPU allowing to train complex models on large datasets such as ImageNet ILSVRC. This document provides an overview of CNNs and how they are implemented in MatConvNet and gives the technical details of each computational block in the toolbox. version:3
arxiv-1605-01576 | Patch-based Texture Synthesis for Image Inpainting | http://arxiv.org/abs/1605.01576 | id:1605.01576 author:Tao Zhou, Brian Johnson, Rui Li category:cs.CV  published:2016-05-05 summary:Image inpaiting is an important task in image processing and vision. In this paper, we develop a general method for patch-based image inpainting by synthesizing new textures from existing one. A novel framework is introduced to find several optimal candidate patches and generate a new texture patch in the process. We form it as an optimization problem that identifies the potential patches for synthesis from an coarse-to-fine manner. We use the texture descriptor as a clue in searching for matching patches from the known region. To ensure the structure faithful to the original image, a geometric constraint metric is formally defined that is applied directly to the patch synthesis procedure. We extensively conducted our experiments on a wide range of testing images on various scenarios and contents by arbitrarily specifying the target the regions for inference followed by using existing evaluation metrics to verify its texture coherency and structural consistency. Our results demonstrate the high accuracy and desirable output that can be potentially used for numerous applications: object removal, background subtraction, and image retrieval. version:1
arxiv-1605-01573 | Observational-Interventional Priors for Dose-Response Learning | http://arxiv.org/abs/1605.01573 | id:1605.01573 author:Ricardo Silva category:stat.ML  published:2016-05-05 summary:Controlled interventions provide the most direct source of information for learning causal effects. In particular, a dose-response curve can be learned by varying the treatment level and observing the corresponding outcomes. However, interventions can be expensive and time-consuming. Observational data, where the treatment is not controlled by a known mechanism, is sometimes available. Under some strong assumptions, observational data allows for the estimation of dose-response curves. Estimating such curves nonparametrically is hard: sample sizes for controlled interventions may be small, while in the observational case a large number of measured confounders may need to be marginalized. In this paper, we introduce a hierarchical Gaussian process prior that constructs a distribution over the dose-response curve by learning from observational data, and reshapes the distribution with a nonparametric affine transform learned from controlled interventions. This function composition from different sources is shown to speed-up learning, which we demonstrate with a thorough sensitivity analysis and an application to modeling the effect of therapy on cognitive skills of premature infants. version:1
arxiv-1605-01569 | Classification of Human Whole-Body Motion using Hidden Markov Models | http://arxiv.org/abs/1605.01569 | id:1605.01569 author:Matthias Plappert category:cs.LG cs.CV  published:2016-05-05 summary:Human motion plays an important role in many fields. Large databases exist that store and make available recordings of human motions. However, annotating each motion with multiple labels is a cumbersome and error-prone process. This bachelor's thesis presents different approaches to solve the multi-label classification problem using Hidden Markov Models (HMMs). First, different features that can be directly obtained from the raw data are introduced. Next, additional features are derived to improve classification performance. These features are then used to perform the multi-label classification using two different approaches. The first approach simply transforms the multi-label problem into a multi-class problem. The second, novel approach solves the same problem without the need to construct a transformation by predicting the labels directly from the likelihood scores. The second approach scales linearly with the number of labels whereas the first approach is subject to combinatorial explosion. All aspects of the classification process are evaluated on a data set that consists of 454 motions. System 1 achieves an accuracy of 98.02% and system 2 an accuracy of 93.39% on the test set. version:1
arxiv-1605-01559 | Sampling from strongly log-concave distributions with the Unadjusted Langevin Algorithm | http://arxiv.org/abs/1605.01559 | id:1605.01559 author:Alain Durmus, Eric Moulines category:math.ST stat.ME stat.ML stat.TH  published:2016-05-05 summary:We consider in this paper the problem of sampling a probability distribution $\pi$ having a density with respect to the Lebesgue measure on $\mathbb{R}^d$, known up to a normalisation factor $x \mapsto \mathrm{e}^{-U(x)}/\int_{\mathbb{R}^d} \mathrm{e}^{-U(y)} \mathrm{d} y$. Under the assumption that $U$ is continuously differentiable, $\nabla U$ is globally Lipshitz and $U$ is strongly convex, we obtain non-asymptotic bounds for the convergence to stationarity in Wasserstein distances of the sampling method based on the Euler discretization of the Langevin stochastic differential equation for both constant and decreasing step sizes. The dependence on the dimension of the state space of the obtained bounds is studied to demonstrate the applicability of this method in the high dimensional setting. The convergence of an appropriately weighted empirical measure is also investigated and bounds for the mean square error and exponential deviation inequality for Lipschitz functions are reported. Some numerical results are presented to illustrate our findings. version:1
arxiv-1605-00961 | Hierarchical Modeling of Multidimensional Data in Regularly Decomposed Spaces: Main Principles | http://arxiv.org/abs/1605.00961 | id:1605.00961 author:Olivier Guye category:cs.CV  published:2016-05-03 summary:The described works have been carried out in the framework of a mid-term study initiated by the Centre Electronique de l'Armement and led by ADERSA, a French company of research under contract. The aim was to study the techniques of regular dividing of numerical data sets so as to provide tools for problem solving enabling to model multidimensional numerical objects and to be used in computer-aided design and manufacturing, in robotics, in image analysis and synthesis, in pattern recognition, in decision making, in cartography and numerical data base management. These tools are relying on the principle of regular hierarchical decomposition and led to the implementation of a multidimensional generalization of quaternary and octernary trees: the trees of order 2**k or 2**k-trees mapped in binary trees. This first tome, dedicated to the hierarchical modeling of multidimensional numerical data, describes the principles used for building, transforming, analyzing and recognizing patterns on which is relying the development of the associated algorithms. The whole so developed algorithms are detailed in pseudo-code at the end of this document. The present publication especially describes: - a building method adapted disordered and overcrowded data streams ; - its extension in inductive limits ; - the computation of the homographic transformation of a tree ; - the attribute calculus based on generalized moments and the provision of Eigen trees ; - perception procedures of objects without any covering in affine geometry ; - several supervised and unsupervised pattern recognition methods. version:2
arxiv-1605-01514 | Fitness-based Adaptive Control of Parameters in Genetic Programming: Adaptive Value Setting of Mutation Rate and Flood Mechanisms | http://arxiv.org/abs/1605.01514 | id:1605.01514 author:Michal Gregor, Juraj Spalek category:cs.NE  published:2016-05-05 summary:This paper concerns applications of genetic algorithms and genetic programming to tasks for which it is difficult to find a representation that does not map to a highly complex and discontinuous fitness landscape. In such cases the standard algorithm is prone to getting trapped in local extremes. The paper proposes several adaptive mechanisms that are useful in preventing the search from getting trapped. version:1
arxiv-1602-07563 | Multilingual Twitter Sentiment Classification: The Role of Human Annotators | http://arxiv.org/abs/1602.07563 | id:1602.07563 author:Igor Mozetic, Miha Grcar, Jasmina Smailovic category:cs.CL cs.AI  published:2016-02-24 summary:What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered. version:2
arxiv-1602-02481 | A Large Dataset of Object Scans | http://arxiv.org/abs/1602.02481 | id:1602.02481 author:Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, Vladlen Koltun category:cs.CV cs.GR  published:2016-02-08 summary:We have created a dataset of more than ten thousand 3D scans of real objects. To create the dataset, we recruited 70 operators, equipped them with consumer-grade mobile 3D scanning setups, and paid them to scan objects in their environments. The operators scanned objects of their choosing, outside the laboratory and without direct supervision by computer vision professionals. The result is a large and diverse collection of object scans: from shoes, mugs, and toys to grand pianos, construction vehicles, and large outdoor sculptures. We worked with an attorney to ensure that data acquisition did not violate privacy constraints. The acquired data was irrevocably placed in the public domain and is available freely at http://redwood-data.org/3dscan . version:3
arxiv-1605-01478 | Modeling Rich Contexts for Sentiment Classification with LSTM | http://arxiv.org/abs/1605.01478 | id:1605.01478 author:Minlie Huang, Yujie Cao, Chao Dong category:cs.CL cs.IR cs.SI  published:2016-05-05 summary:Sentiment analysis on social media data such as tweets and weibo has become a very important and challenging task. Due to the intrinsic properties of such data, tweets are short, noisy, and of divergent topics, and sentiment classification on these data requires to modeling various contexts such as the retweet/reply history of a tweet, and the social context about authors and relationships. While few prior study has approached the issue of modeling contexts in tweet, this paper proposes to use a hierarchical LSTM to model rich contexts in tweet, particularly long-range context. Experimental results show that contexts can help us to perform sentiment classification remarkably better. version:1
arxiv-1506-05555 | Hamiltonian Monte Carlo Acceleration Using Surrogate Functions with Random Bases | http://arxiv.org/abs/1506.05555 | id:1506.05555 author:Cheng Zhang, Babak Shahbaba, Hongkai Zhao category:stat.CO stat.ML  published:2015-06-18 summary:For big data analysis, high computational cost for Bayesian methods often limits their applications in practice. In recent years, there have been many attempts to improve computational efficiency of Bayesian inference. Here we propose an efficient and scalable computational technique for a state-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian Monte Carlo (HMC). The key idea is to explore and exploit the structure and regularity in parameter space for the underlying probabilistic model to construct an effective approximation of its geometric properties. To this end, we build a surrogate function to approximate the target distribution using properly chosen random bases and an efficient optimization process. The resulting method provides a flexible, scalable, and efficient sampling algorithm, which converges to the correct target distribution. We show that by choosing the basis functions and optimization process differently, our method can be related to other approaches for the construction of surrogate functions such as generalized additive models or Gaussian process models. Experiments based on simulated and real data show that our approach leads to substantially more efficient sampling algorithms compared to existing state-of-the art methods. version:4
arxiv-1604-03257 | Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization | http://arxiv.org/abs/1604.03257 | id:1604.03257 author:Tianbao Yang, Qihang Lin, Zhe Li category:math.OC stat.ML  published:2016-04-12 summary:Recently, {\it stochastic momentum} methods have been widely adopted in training deep neural networks. However, their convergence analysis is still underexplored at the moment, in particular for non-convex optimization. This paper fills the gap between practice and theory by developing a basic convergence analysis of two stochastic momentum methods, namely stochastic heavy-ball method and the stochastic variant of Nesterov's accelerated gradient method. We hope that the basic convergence results developed in this paper can serve the reference to the convergence of stochastic momentum methods and also serve the baselines for comparison in future development of stochastic momentum methods. The novelty of convergence analysis presented in this paper is a unified framework, revealing more insights about the similarities and differences between different stochastic momentum methods and stochastic gradient method. The unified framework exhibits a continuous change from the gradient method to Nesterov's accelerated gradient method and finally the heavy-ball method incurred by a free parameter, which can help explain a similar change observed in the testing error convergence behavior for deep learning. Furthermore, our empirical results for optimizing deep neural networks demonstrate that the stochastic variant of Nesterov's accelerated gradient method achieves a good tradeoff (between speed of convergence in training error and robustness of convergence in testing error) among the three stochastic methods. version:2
arxiv-1605-01451 | Boltzmann meets Nash: Energy-efficient routing in optical networks under uncertainty | http://arxiv.org/abs/1605.01451 | id:1605.01451 author:Panayotis Mertikopoulos, Aris L. Moustakas, Anna Tzanakaki category:cs.NI cs.GT cs.LG  published:2016-05-04 summary:Motivated by the massive deployment of power-hungry data centers for service provisioning, we examine the problem of routing in optical networks with the aim of minimizing traffic-driven power consumption. To tackle this issue, routing must take into account energy efficiency as well as capacity considerations; moreover, in rapidly-varying network environments, this must be accomplished in a real-time, distributed manner that remains robust in the presence of random disturbances and noise. In view of this, we derive a pricing scheme whose Nash equilibria coincide with the network's socially optimum states, and we propose a distributed learning method based on the Boltzmann distribution of statistical mechanics. Using tools from stochastic calculus, we show that the resulting Boltzmann routing scheme exhibits remarkable convergence properties under uncertainty: specifically, the long-term average of the network's power consumption converges within $\varepsilon$ of its minimum value in time which is at most $\tilde O(1/\varepsilon^2)$, irrespective of the fluctuations' magnitude; additionally, if the network admits a strict, non-mixing optimum state, the algorithm converges to it - again, no matter the noise level. Our analysis is supplemented by extensive numerical simulations which show that Boltzmann routing can lead to a significant decrease in power consumption over basic, shortest-path routing schemes in realistic network conditions. version:1
arxiv-1507-05699 | Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians | http://arxiv.org/abs/1507.05699 | id:1507.05699 author:Peiyun Hu, Deva Ramanan category:cs.CV  published:2015-07-21 summary:Convolutional neural nets (CNNs) have demonstrated remarkable performance in recent history. Such approaches tend to work in a unidirectional bottom-up feed-forward fashion. However, practical experience and biological evidence tells us that feedback plays a crucial role, particularly for detailed spatial understanding tasks. This work explores bidirectional architectures that also reason with top-down feedback: neural units are influenced by both lower and higher-level units. We do so by treating units as rectified latent variables in a quadratic energy function, which can be seen as a hierarchical Rectified Gaussian model (RGs). We show that RGs can be optimized with a quadratic program (QP), that can in turn be optimized with a recurrent neural network (with rectified linear units). This allows RGs to be trained with GPU-optimized gradient descent. From a theoretical perspective, RGs help establish a connection between CNNs and hierarchical probabilistic models. From a practical perspective, RGs are well suited for detailed spatial tasks that can benefit from top-down reasoning. We illustrate them on the challenging task of keypoint localization under occlusions, where local bottom-up evidence may be misleading. We demonstrate state-of-the-art results on challenging benchmarks. version:5
arxiv-1605-01436 | Sampling Requirements for Stable Autoregressive Estimation | http://arxiv.org/abs/1605.01436 | id:1605.01436 author:Abbas Kazemipour, Sina Miran, Piya Pal, Behtash Babadi, Min Wu category:cs.IT cs.DM math.IT math.OC stat.ME stat.ML  published:2016-05-04 summary:We consider the problem of estimating the parameters of a linear autoregressive model with sub-Gaussian innovations from a limited sequence of consecutive observations. Assuming that the parameters are compressible, we analyze the performance of the $\ell_1$-regularized least squares as well as a greedy estimator of the parameters and characterize the sampling trade-offs required for stable recovery in the non-asymptotic regime. Our results extend those of compressed sensing for linear models where the covariates are i.i.d. and independent of the observation history to autoregressive processes with highly inter-dependent covariates. We also derive sufficient conditions on the sparsity level that guarantee the minimax optimality of the $\ell_1$-regularized least squares estimate. Applying these techniques to simulated data as well as real-world datasets from crude oil prices and traffic speed data confirm our predicted theoretical performance gains in terms of estimation accuracy and model selection. version:1
arxiv-1605-01397 | Skin Lesion Analysis toward Melanoma Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC) | http://arxiv.org/abs/1605.01397 | id:1605.01397 author:David Gutman, Noel C. F. Codella, Emre Celebi, Brian Helba, Michael Marchetti, Nabin Mishra, Allan Halpern category:cs.CV  published:2016-05-04 summary:In this article, we describe the design and implementation of a publicly accessible dermatology image analysis benchmark challenge. The goal of the challenge is to sup- port research and development of algorithms for automated diagnosis of melanoma, a lethal form of skin cancer, from dermoscopic images. The challenge was divided into sub-challenges for each task involved in image analysis, including lesion segmentation, dermoscopic feature detection within a lesion, and classification of melanoma. Training data included 900 images. A separate test dataset of 379 images was provided to measure resultant performance of systems developed with the training data. Ground truth for both training and test sets was generated by a panel of dermoscopic experts. In total, there were 79 submissions from a group of 38 participants, making this the largest standardized and comparative study for melanoma diagnosis in dermoscopic images to date. While the official challenge duration and ranking of participants has concluded, the datasets remain available for further research and development. version:1
arxiv-1605-01643 | The embedding dimension of Laplacian eigenfunction maps | http://arxiv.org/abs/1605.01643 | id:1605.01643 author:Jonathan Bates category:stat.ML cs.CV math.DG  published:2016-05-04 summary:Any closed, connected Riemannian manifold $M$ can be smoothly embedded by its Laplacian eigenfunction maps into $\mathbb{R}^m$ for some $m$. We call the smallest such $m$ the maximal embedding dimension of $M$. We show that the maximal embedding dimension of $M$ is bounded from above by a constant depending only on the dimension of $M$, a lower bound for injectivity radius, a lower bound for Ricci curvature, and a volume bound. We interpret this result for the case of surfaces isometrically immersed in $\mathbb{R}^3$, showing that the maximal embedding dimension only depends on bounds for the Gaussian curvature, mean curvature, and surface area. Furthermore, we consider the relevance of these results for shape registration. version:1
arxiv-1605-01384 | Multi Level Monte Carlo methods for a class of ergodic stochastic differential equations | http://arxiv.org/abs/1605.01384 | id:1605.01384 author:Lukasz Szpruch, Sebastian Vollmer, Konstantinos Zygalakis, Michael B. Giles category:math.NA stat.ME stat.ML  published:2016-05-04 summary:We develop a framework that allows the use of the multi-level Monte Carlo (MLMC) methodology (Giles 2015) to calculate expectations with respect to the invariant measures of ergodic SDEs. In that context, we study the (over-damped) Langevin equations with strongly convex potential. We show that, when appropriate contracting couplings for the numerical integrators are available, one can obtain a time-uniform estimates of the MLMC variance in stark contrast to the majority of the results in the MLMC literature. As a consequence, one can approximate expectations with respect to the invariant measure in an unbiased way without the need of a Metropolis- Hastings step. In addition, a root mean square error of $\mathcal{O}(\epsilon)$ is achieved with $\mathcal{O}(\epsilon^{-2})$ complexity on par with Markov Chain Monte Carlo (MCMC) methods, which however can be computationally intensive when applied to large data sets. Finally, we present a multilevel version of the recently introduced Stochastic Gradient Langevin (SGLD) method (Welling and Teh, 2011) built for large datasets applications. We show that this is the first stochastic gradient MCMC method with complexity $\mathcal{O}(\epsilon^{-2} \log {\epsilon} ^{3})$, which is asymptotically an order $\epsilon$ lower than the $ \mathcal{O}(\epsilon^{-3})$ complexity of all stochastic gradient MCMC methods that are currently available. Numerical experiments confirm our theoretical findings. version:1
arxiv-1605-01379 | Leveraging Visual Question Answering for Image-Caption Ranking | http://arxiv.org/abs/1605.01379 | id:1605.01379 author:Xiao Lin, Devi Parikh category:cs.CV  published:2016-05-04 summary:Visual Question Answering (VQA) is the task of taking as input an image and a free-form natural language question about the image, and producing an accurate answer. In this work we view VQA as a "feature extraction" module to extract image and caption representations. We employ these representations for the task of image-caption ranking. Each feature dimension captures (imagines) whether a fact (question-answer pair) could plausibly be true for the image and caption. This allows the model to interpret images and captions from a wide variety of perspectives. We propose score-level and representation-level fusion models to incorporate VQA knowledge in an existing state-of-the-art VQA-agnostic image-caption ranking model. We find that incorporating and reasoning about consistency between images and captions significantly improves performance. Concretely, our model improves state-of-the-art on caption retrieval by 7.1% and on image retrieval by 4.4% on the MSCOCO dataset. version:1
arxiv-1403-2310 | Adaptive Penalized Estimation of Directed Acyclic Graphs From Categorical Data | http://arxiv.org/abs/1403.2310 | id:1403.2310 author:Jiaying Gu, Fei Fu, Qing Zhou category:stat.ME stat.ML  published:2014-03-10 summary:We develop in this article a penalized likelihood method to estimate sparse Bayesian networks from categorical data. The structure of a Bayesian network is represented by a directed acyclic graph (DAG). We model the conditional distribution of a node given its parents by multi-logit regression and estimate the structure of a DAG via maximizing a regularized likelihood. The adaptive group Lasso penalty is employed to encourage sparsity by selecting grouped dummy variables encoding the level of a factor. We develop a blockwise coordinate descent algorithm to solve the penalized likelihood problem subject to the acyclicity constraint of a DAG. When intervention data are available, our method may construct a causal network, in which a directed edge represents a causal relation. We apply our method to various simulated networks and a real biological network. The results show that our method is very competitive, compared to other existing methods, in DAG estimation from both interventional and high-dimensional observational data. We also establish consistency in parameter and structure estimation for our method when the number of nodes is fixed. version:3
arxiv-1605-01369 | Accelerating Deep Learning with Shrinkage and Recall | http://arxiv.org/abs/1605.01369 | id:1605.01369 author:Shuai Zheng, Abhinav Vishnu, Chris Ding category:cs.LG cs.CV cs.NE  published:2016-05-04 summary:Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance. version:1
arxiv-1506-03410 | Randomer Forests | http://arxiv.org/abs/1506.03410 | id:1506.03410 author:Tyler M. Tomita, Mauro Maggioni, Joshua T. Vogelstein category:stat.ML cs.LG 68T10 I.5.2  published:2015-06-10 summary:Random forests (RF) is a popular general purpose classifier that has been shown to outperform many other classifiers on a variety of datasets. The widespread use of random forests can be attributed to several factors, some of which include its excellent empirical performance, scale and unit invariance, robustness to outliers, time and space complexity, and interpretability. While RF has many desirable qualities, one drawback is its sensitivity to rotations and other operations that "mix" variables. In this work, we establish a generalized forest building scheme, linear threshold forests. Random forests and many other currently existing decision forest algorithms can be viewed as special cases of this scheme. With this scheme in mind, we propose a few special cases which we call randomer forests (RerFs). RerFs are linear threshold forest that exhibit all of the nice properties of RF, in addition to approximate affine invariance. In simulated datasets designed for RF to do well, we demonstrate that RerF outperforms RF. We also demonstrate that one particular variant of RerF is approximately affine invariant. Lastly, in an evaluation on 121 benchmark datasets, we observe that RerF outperforms RF. We therefore putatively propose that RerF be considered a replacement for RF as the general purpose classifier of choice. Open source code is available at http://ttomita.github.io/RandomerForest/. version:2
arxiv-1403-0667 | The Hidden Convexity of Spectral Clustering | http://arxiv.org/abs/1403.0667 | id:1403.0667 author:James Voss, Mikhail Belkin, Luis Rademacher category:cs.LG stat.ML  published:2014-03-04 summary:In recent years, spectral clustering has become a standard method for data analysis used in a broad range of applications. In this paper we propose a new class of algorithms for multiway spectral clustering based on optimization of a certain "contrast function" over the unit sphere. These algorithms, partly inspired by certain Independent Component Analysis techniques, are simple, easy to implement and efficient. Geometrically, the proposed algorithms can be interpreted as hidden basis recovery by means of function optimization. We give a complete characterization of the contrast functions admissible for provable basis recovery. We show how these conditions can be interpreted as a "hidden convexity" of our optimization problem on the sphere; interestingly, we use efficient convex maximization rather than the more common convex minimization. We also show encouraging experimental results on real and simulated data. version:3
arxiv-1605-01335 | Learning from the memory of Atari 2600 | http://arxiv.org/abs/1605.01335 | id:1605.01335 author:Jakub Sygnowski, Henryk Michalewski category:cs.LG cs.AI  published:2016-05-04 summary:We train a number of neural networks to play games Bowling, Breakout and Seaquest using information stored in the memory of a video game console Atari 2600. We consider four models of neural networks which differ in size and architecture: two networks which use only information contained in the RAM and two mixed networks which use both information in the RAM and information from the screen. As the benchmark we used the convolutional model proposed in NIPS and received comparable results in all considered games. Quite surprisingly, in the case of Seaquest we were able to train RAM-only agents which behave better than the benchmark screen-only agent. Mixing screen and RAM did not lead to an improved performance comparing to screen-only and RAM-only agents. version:1
arxiv-1605-01329 | Single Channel Speech Enhancement Using Outlier Detection | http://arxiv.org/abs/1605.01329 | id:1605.01329 author:Eunjoon Cho, Bowon Lee, Ronald Schafer, Bernard Widrow category:cs.SD cs.LG  published:2016-05-04 summary:Distortion of the underlying speech is a common problem for single-channel speech enhancement algorithms, and hinders such methods from being used more extensively. A dictionary based speech enhancement method that emphasizes preserving the underlying speech is proposed. Spectral patches of clean speech are sampled and clustered to train a dictionary. Given a noisy speech spectral patch, the best matching dictionary entry is selected and used to estimate the noise power at each time-frequency bin. The noise estimation step is formulated as an outlier detection problem, where the noise at each bin is assumed present only if it is an outlier to the corresponding bin of the best matching dictionary entry. This framework assigns higher priority in removing spectral elements that strongly deviate from a typical spoken unit stored in the trained dictionary. Even without the aid of a separate noise model, this method can achieve significant noise reduction for various non-stationary noises, while effectively preserving the underlying speech in more challenging noisy environments. version:1
arxiv-1605-01326 | Compression and the origins of Zipf's law for word frequencies | http://arxiv.org/abs/1605.01326 | id:1605.01326 author:Ramon Ferrer-i-Cancho category:cs.CL physics.data-an physics.soc-ph q-bio.NC  published:2016-05-04 summary:Here we sketch a new derivation of Zipf's law for word frequencies based on optimal coding. The structure of the derivation is reminiscent of Mandelbrot's random typing model but it has multiple advantages over random typing: (1) it departs from realistic cognitive pressures (2) it does not require fine tuning of parameters and (3) it sheds light on the origins of other statistical laws of language and thus can lead to a compact theory of linguistic laws. Our findings suggest that the recurrence of Zipf's law in human languages could originate from pressure for easy and fast communication. version:1
arxiv-1504-04884 | Compression and the origins of Zipf's law of abbreviation | http://arxiv.org/abs/1504.04884 | id:1504.04884 author:R. Ferrer-i-Cancho, C. Bentz, C. Seguin category:cs.IT cs.CL cs.SI math.IT physics.data-an  published:2015-04-19 summary:Languages across the world exhibit Zipf's law of abbreviation, namely more frequent words tend to be shorter. The generalized version of the law - an inverse relationship between the frequency of a unit and its magnitude - holds also for the behaviours of other species and the genetic code. The apparent universality of this pattern in human language and its ubiquity in other domains calls for a theoretical understanding of its origins. To this end, we generalize the information theoretic concept of mean code length as a mean energetic cost function over the probability and the magnitude of the types of the repertoire. We show that the minimization of that cost function and a negative correlation between probability and the magnitude of types are intimately related. version:3
arxiv-1412-6604 | Video (language) modeling: a baseline for generative models of natural videos | http://arxiv.org/abs/1412.6604 | id:1412.6604 author:MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, Sumit Chopra category:cs.LG cs.CV  published:2014-12-20 summary:We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary. We demonstrate the approach on both a filling and a generation task. For the first time, we show that, after training on natural videos, such a model can predict non-trivial motions over short video sequences. version:5
arxiv-1605-01242 | Hierarchical Modeling of Multidimensional Data in Regularly Decomposed Spaces: Applications in Image Analysis | http://arxiv.org/abs/1605.01242 | id:1605.01242 author:Olivier Guye category:cs.CV  published:2016-05-04 summary:This last document is showing the gradual introduction of hierarchical modeling techniques in image analysis. The first chapter is dealing with the first works carried out in the field of industrial applications of pattern recognition. The second chapter is focusing on the usage of these techniques in satellite imagery and on the development of a satellite data archiving system in the aim of using it in digital geography. The third chapter is about face recognition based on planar image analysis and about the recognition of partially hidden patterns. The present publication is ending with the description of a future system of self-descriptive coding of still or moving pictures in relation with the current video coding standards. As in the previous documents, it will be found in annex algorithms targeted on image analysis according two complementary approaches: - boundary-based approach for the industrial applications of artificial vision; - region-based approach for satellite image analysis. version:1
arxiv-1603-01857 | Classical Statistics and Statistical Learning in Imaging Neuroscience | http://arxiv.org/abs/1603.01857 | id:1603.01857 author:Danilo Bzdok category:stat.ML q-bio.NC  published:2016-03-06 summary:Neuroimaging research has predominantly drawn conclusions based on classical statistics, including null-hypothesis testing, t-tests, and ANOVA. Throughout recent years, statistical learning methods enjoy increasing popularity, including cross-validation, pattern classification, and sparsity-inducing regression. These two methodological families used for neuroimaging data analysis can be viewed as two extremes of a continuum. Yet, they originated from different historical contexts, build on different theories, rest on different assumptions, evaluate different outcome metrics, and permit different conclusions. This paper portrays commonalities and differences between classical statistics and statistical learning with their relation to neuroimaging research. The conceptual implications are illustrated in three common analysis scenarios. It is thus tried to resolve possible confusion between classical hypothesis testing and data-guided model estimation by discussing their ramifications for the neuroimaging access to neurobiology. version:2
arxiv-1605-01224 | Learning Covariant Feature Detectors | http://arxiv.org/abs/1605.01224 | id:1605.01224 author:Karel Lenc, Andrea Vedaldi category:cs.CV  published:2016-05-04 summary:Local covariant feature detection, namely the problem of extracting viewpoint invariant features from images, has so far largely resisted the application of machine learning techniques. In this paper, we propose the first fully general formulation for learning local covariant feature detectors. We propose to cast detection as a regression problem, enabling the use of powerful regressors such as deep neural networks. We then derive a covariance constraint that can be used to automatically learn which visual structures provide stable anchors for local feature detection. We support these ideas theoretically, proposing a novel analysis of local features in term of geometric transformations, and we show that all common and many uncommon detectors can be derived in this framework. Finally, we present empirical results on a variety of detector types and on standard feature benchmarks, showing the power and flexibility of the framework. version:1
arxiv-1605-01194 | IISCNLP at SemEval-2016 Task 2: Interpretable STS with ILP based Multiple Chunk Aligner | http://arxiv.org/abs/1605.01194 | id:1605.01194 author:Lavanya Sita Tekumalla, Sharmistha category:cs.CL stat.ML  published:2016-05-04 summary:Interpretable semantic textual similarity (iSTS) task adds a crucial explanatory layer to pairwise sentence similarity. We address various components of this task: chunk level semantic alignment along with assignment of similarity type and score for aligned chunks with a novel system presented in this paper. We propose an algorithm, iMATCH, for the alignment of multiple non-contiguous chunks based on Integer Linear Programming (ILP). Similarity type and score assignment for pairs of chunks is done using a supervised multiclass classification technique based on Random Forrest Classifier. Results show that our algorithm iMATCH has low execution time and outperforms most other participating systems in terms of alignment score. Of the three datasets, we are top ranked for answer- students dataset in terms of overall score and have top alignment score for headlines dataset in the gold chunks track. version:1
arxiv-1605-01189 | A Generic Method for Automatic Ground Truth Generation of Camera-captured Documents | http://arxiv.org/abs/1605.01189 | id:1605.01189 author:Sheraz Ahmed, Muhammad Imran Malik, Muhammad Zeshan Afzal, Koichi Kise, Masakazu Iwamura, Andreas Dengel, Marcus Liwicki category:cs.CV  published:2016-05-04 summary:The contribution of this paper is fourfold. The first contribution is a novel, generic method for automatic ground truth generation of camera-captured document images (books, magazines, articles, invoices, etc.). It enables us to build large-scale (i.e., millions of images) labeled camera-captured/scanned documents datasets, without any human intervention. The method is generic, language independent and can be used for generation of labeled documents datasets (both scanned and cameracaptured) in any cursive and non-cursive language, e.g., English, Russian, Arabic, Urdu, etc. To assess the effectiveness of the presented method, two different datasets in English and Russian are generated using the presented method. Evaluation of samples from the two datasets shows that 99:98% of the images were correctly labeled. The second contribution is a large dataset (called C3Wi) of camera-captured characters and words images, comprising 1 million word images (10 million character images), captured in a real camera-based acquisition. This dataset can be used for training as well as testing of character recognition systems on camera-captured documents. The third contribution is a novel method for the recognition of cameracaptured document images. The proposed method is based on Long Short-Term Memory and outperforms the state-of-the-art methods for camera based OCRs. As a fourth contribution, various benchmark tests are performed to uncover the behavior of commercial (ABBYY), open source (Tesseract), and the presented camera-based OCR using the presented C3Wi dataset. Evaluation results reveal that the existing OCRs, which already get very high accuracies on scanned documents, have limited performance on camera-captured document images; where ABBYY has an accuracy of 75%, Tesseract an accuracy of 50.22%, while the presented character recognition system has an accuracy of 95.10%. version:1
arxiv-1605-01185 | Linear Bandit algorithms using the Bootstrap | http://arxiv.org/abs/1605.01185 | id:1605.01185 author:Nandan Sudarsanam, Balaraman Ravindran category:stat.ML cs.LG  published:2016-05-04 summary:This study presents two new algorithms for solving linear stochastic bandit problems. The proposed methods use an approach from non-parametric statistics called bootstrapping to create confidence bounds. This is achieved without making any assumptions about the distribution of noise in the underlying system. We present the X-Random and X-Fixed bootstrap bandits which correspond to the two well-known approaches for conducting bootstraps on models, in the literature. The proposed methods are compared to other popular solutions for linear stochastic bandit problems, namely, OFUL, LinUCB and Thompson Sampling. The comparisons are carried out using a simulation study on a hierarchical probability meta-model, built from published data of experiments, which are run on real systems. The model representing the response surfaces is conceptualized as a Bayesian Network which is presented with varying degrees of noise for the simulations. One of the proposed methods, X-Random bootstrap, performs better than the baselines in-terms of cumulative regret across various degrees of noise and different number of trials. In certain settings the cumulative regret of this method is less than half of the best baseline. The X-Fixed bootstrap performs comparably in most situations and particularly well when the number of trials is low. The study concludes that these algorithms could be a preferred alternative for solving linear bandit problems, especially when the distribution of the noise in the system is unknown. version:1
arxiv-1605-01177 | A metric on the space of finite sets of trajectories for evaluation of multi-target tracking algorithms | http://arxiv.org/abs/1605.01177 | id:1605.01177 author:Abu Sajana Rahmathullah, Ángel F. García-Fernández, Lennart Svensson category:cs.CV cs.SY  published:2016-05-04 summary:In this paper, we propose a metric on the space of finite sets of trajectories for assessing multi-target tracking algorithms in a mathematically sound way. The metric can be used, e.g., to compare estimates from algorithms with the ground truth. It includes intuitive costs associated to localization, missed and false targets and track switches. The metric computation is based on multi-dimensional assignments, which is an NP hard problem. Therefore, we also propose a lower bound for the metric, which is also a metric for sets of trajectories and is computable in polynomial time using linear programming (LP). The LP metric can be implemented using alternating direction method of multipliers such that the complexity scales linearly with the length of the trajectories. version:1
arxiv-1604-07043 | Towards Better Analysis of Deep Convolutional Neural Networks | http://arxiv.org/abs/1604.07043 | id:1604.07043 author:Mengchen Liu, Jiaxin Shi, Zhen Li, Chongxuan Li, Jun Zhu, Shixia Liu category:cs.CV  published:2016-04-24 summary:Deep convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks such as image classification. However, the development of high-quality deep models typically relies on a substantial amount of trial-and-error, as there is still no clear understanding of when and why a deep model works. In this paper, we present a visual analytics approach for better understanding, diagnosing, and refining deep CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this formulation, a hybrid visualization is developed to disclose the multiple facets of each neuron and the interactions between them. In particular, we introduce a hierarchical rectangle packing algorithm and a matrix reordering algorithm to show the derived features of a neuron cluster. We also propose a biclustering-based edge bundling method to reduce visual clutter caused by a large number of connections between neurons. We evaluated our method on a set of CNNs and the results are generally favorable. version:3
arxiv-1605-01156 | Application of Deep Convolutional Neural Networks for Detecting Extreme Weather in Climate Datasets | http://arxiv.org/abs/1605.01156 | id:1605.01156 author:Yunjie Liu, Evan Racah, Prabhat, Joaquin Correa, Amir Khosrowshahi, David Lavers, Kenneth Kunkel, Michael Wehner, William Collins category:cs.CV  published:2016-05-04 summary:Detecting extreme events in large datasets is a major challenge in climate science research. Current algorithms for extreme event detection are build upon human expertise in defining events based on subjective thresholds of relevant physical variables. Often, multiple competing methods produce vastly different results on the same dataset. Accurate characterization of extreme events in climate simulations and observational data archives is critical for understanding the trends and potential impacts of such events in a climate change content. This study presents the first application of Deep Learning techniques as alternative methodology for climate extreme events detection. Deep neural networks are able to learn high-level representations of a broad class of patterns from labeled data. In this work, we developed deep Convolutional Neural Network (CNN) classification system and demonstrated the usefulness of Deep Learning technique for tackling climate pattern detection problems. Coupled with Bayesian based hyper-parameter optimization scheme, our deep CNN system achieves 89\%-99\% of accuracy in detecting extreme events (Tropical Cyclones, Atmospheric Rivers and Weather Fronts version:1
arxiv-1508-07744 | Ethnicity sensitive author disambiguation using semi-supervised learning | http://arxiv.org/abs/1508.07744 | id:1508.07744 author:Gilles Louppe, Hussein Al-Natsheh, Mateusz Susik, Eamonn Maguire category:cs.DL cs.IR stat.ML  published:2015-08-31 summary:Author name disambiguation in bibliographic databases is the problem of grouping together scientific publications written by the same person, accounting for potential homonyms and/or synonyms. Among solutions to this problem, digital libraries are increasingly offering tools for authors to manually curate their publications and claim those that are theirs. Indirectly, these tools allow for the inexpensive collection of large annotated training data, which can be further leveraged to build a complementary automated disambiguation system capable of inferring patterns for identifying publications written by the same person. Building on more than 1 million publicly released crowdsourced annotations, we propose an automated author disambiguation solution exploiting this data (i) to learn an accurate classifier for identifying coreferring authors and (ii) to guide the clustering of scientific publications by distinct authors in a semi-supervised way. To the best of our knowledge, our analysis is the first to be carried out on data of this size and coverage. With respect to the state of the art, we validate the general pipeline used in most existing solutions, and improve by: (i) proposing phonetic-based blocking strategies, thereby increasing recall; and (ii) adding strong ethnicity-sensitive features for learning a linkage function, thereby tailoring disambiguation to non-Western author names whenever necessary. version:2
arxiv-1605-00751 | Learning from Binary Labels with Instance-Dependent Corruption | http://arxiv.org/abs/1605.00751 | id:1605.00751 author:Aditya Krishna Menon, Brendan van Rooyen, Nagarajan Natarajan category:cs.LG  published:2016-05-03 summary:Suppose we have a sample of instances paired with binary labels corrupted by arbitrary instance- and label-dependent noise. With sufficiently many such samples, can we optimally classify and rank instances with respect to the noise-free distribution? We provide a theoretical analysis of this question, with three main contributions. First, we prove that for instance-dependent noise, any algorithm that is consistent for classification on the noisy distribution is also consistent on the clean distribution. Second, we prove that for a broad class of instance- and label-dependent noise, a similar consistency result holds for the area under the ROC curve. Third, for the latter noise model, when the noise-free class-probability function belongs to the generalised linear model family, we show that the Isotron can efficiently and provably learn from the corrupted sample. version:2
arxiv-1605-01138 | A Comparative Evaluation of Approximate Probabilistic Simulation and Deep Neural Networks as Accounts of Human Physical Scene Understanding | http://arxiv.org/abs/1605.01138 | id:1605.01138 author:Renqiao Zhang, Jiajun Wu, Chengkai Zhang, William T. Freeman, Joshua B. Tenenbaum category:cs.AI cs.CV q-bio.NC  published:2016-05-04 summary:Humans demonstrate remarkable abilities to predict physical events in complex scenes. Two classes of models for physical scene understanding have recently been proposed: "Intuitive Physics Engines", or IPEs, which posit that people make predictions by running approximate probabilistic simulations in causal mental models similar in nature to video-game physics engines, and memory-based models, which make judgments based on analogies to stored experiences of previously encountered scenes and physical outcomes. Versions of the latter have recently been instantiated in convolutional neural network (CNN) architectures. Here we report four experiments that, to our knowledge, are the first rigorous comparisons of simulation-based and CNN-based models, where both approaches are concretely instantiated in algorithms that can run on raw image inputs and produce as outputs physical judgments such as whether a stack of blocks will fall. Both approaches can achieve super-human accuracy levels and can quantitatively predict human judgments to a similar degree, but only the simulation-based models generalize to novel situations in ways that people do, and are qualitatively consistent with systematic perceptual illusions and judgment asymmetries that people show. version:1
arxiv-1512-03107 | RSG: Beating SG without Smoothness and/or Strong Convexity | http://arxiv.org/abs/1512.03107 | id:1512.03107 author:Tianbao Yang, Qihang Lin category:math.OC stat.ML  published:2015-12-09 summary:In this paper, we propose novel deterministic and stochastic {\bf R}estarted {\bf S}ub{\bf G}radient (RSG) methods that can find an $\epsilon$-optimal solution for a broad class of non-smooth and/or non-strongly convex optimization problems faster than the vanilla deterministic or stochastic subgradient method (SG). We show that for non-smooth and non-strongly convex optimization, RSG can reduce the dependence of SG's iteration complexity on the distance to the optimal set of the initial solution to that of points on the $\epsilon$-level set. For a special family of non-smooth and non-strongly convex optimization problems whose epigraph is a polyhedron, we further show that RSG could converge linearly. In addition, RSG has an $O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity for problems with a much weaker notion of strong convexity, namely locally semi-strongly convexity. For a family of non-smooth optimization problems that admit a local Kurdyka-\L ojasiewicz property with a power constant of $\beta\in(0,1)$, RSG has an $O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity, which is better than that of SG for such optimization problems whose iteration complexity is $O(\frac{1}{\epsilon^2})$. The novelty of our analysis lies at exploiting the lower bound of the subgradient of the objective function at the $\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local semi-strong convexity, local Kurdyka-\L ojasiewicz property, more generally local error bounds) to develop improved convergence of RSG. version:9
arxiv-1605-01130 | Mining Discriminative Triplets of Patches for Fine-Grained Classification | http://arxiv.org/abs/1605.01130 | id:1605.01130 author:Yaming Wang, Jonghyun Choi, Vlad I. Morariu, Larry S. Davis category:cs.CV  published:2016-05-04 summary:Fine-grained classification involves distinguishing between similar sub-categories based on subtle differences in highly localized regions; therefore, accurate localization of discriminative regions remains a major challenge. We describe a patch-based framework to address this problem. We introduce triplets of patches with geometric constraints to improve the accuracy of patch localization, and automatically mine discriminative geometrically-constrained triplets for classification. The resulting approach only requires object bounding boxes. Its effectiveness is demonstrated using four publicly available fine-grained datasets, on which it outperforms or achieves comparable performance to the state-of-the-art in classification. version:1
arxiv-1604-07361 | Persistence Lenses: Segmentation, Simplification, Vectorization, Scale Space and Fractal Analysis of Images | http://arxiv.org/abs/1604.07361 | id:1604.07361 author:Martin Brooks category:cs.CV cs.CG math.GN  published:2016-04-25 summary:A persistence lens is a hierarchy of disjoint upper and lower level sets of a continuous luminance image's Reeb graph. The boundary components of a persistence lens's interior components are Jordan curves that serve as a hierarchical segmentation of the image, and may be rendered as vector graphics. A persistence lens determines a varilet basis for the luminance image, in which image simplification is a realized by subspace projection. Image scale space, and image fractal analysis, result from applying a scale measure to each basis function. version:2
arxiv-1503-05671 | Optimizing Neural Networks with Kronecker-factored Approximate Curvature | http://arxiv.org/abs/1503.05671 | id:1503.05671 author:James Martens, Roger Grosse category:cs.LG cs.NE stat.ML  published:2015-03-19 summary:We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-Factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC's approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix. version:6
arxiv-1405-0352 | Asymptotic Theory for Random Forests | http://arxiv.org/abs/1405.0352 | id:1405.0352 author:Stefan Wager category:math.ST stat.ML stat.TH  published:2014-05-02 summary:Random forests have proven to be reliable predictive algorithms in many application areas. Not much is known, however, about the statistical properties of random forests. Several authors have established conditions under which their predictions are consistent, but these results do not provide practical estimates of random forest errors. In this paper, we analyze a random forest model based on subsampling, and show that random forest predictions are asymptotically normal provided that the subsample size s scales as s(n)/n = o(log(n)^{-d}), where n is the number of training examples and d is the number of features. Moreover, we show that the asymptotic variance can consistently be estimated using an infinitesimal jackknife for bagged ensembles recently proposed by Efron (2014). In other words, our results let us both characterize and estimate the error-distribution of random forest predictions, thus taking a step towards making random forests tools for statistical inference instead of just black-box predictive algorithms. version:2
arxiv-1603-00831 | MOT16: A Benchmark for Multi-Object Tracking | http://arxiv.org/abs/1603.00831 | id:1603.00831 author:Anton Milan, Laura Leal-Taixe, Ian Reid, Stefan Roth, Konrad Schindler category:cs.CV  published:2016-03-02 summary:Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for reseach. Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal of collecting existing and new data and creating a framework for the standardized evaluation of multiple object tracking methods. The first release of the benchmark focuses on multiple people tracking, since pedestrians are by far the most studied object in the tracking community. This paper accompanies a new release of the MOTChallenge benchmark. Unlike the initial release, all videos of MOT16 have been carefully annotated following a consistent protocol. Moreover, it not only offers a significant increase in the number of labeled boxes, but also provides multiple object classes beside pedestrians and the level of visibility for every single object of interest. version:2
arxiv-1605-01116 | An evaluation of randomized machine learning methods for redundant data: Predicting short and medium-term suicide risk from administrative records and risk assessments | http://arxiv.org/abs/1605.01116 | id:1605.01116 author:Thuong Nguyen, Truyen Tran, Shivapratap Gopakumar, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG  published:2016-05-03 summary:Accurate prediction of suicide risk in mental health patients remains an open problem. Existing methods including clinician judgments have acceptable sensitivity, but yield many false positives. Exploiting administrative data has a great potential, but the data has high dimensionality and redundancies in the recording processes. We investigate the efficacy of three most effective randomized machine learning techniques random forests, gradient boosting machines, and deep neural nets with dropout in predicting suicide risk. Using a cohort of mental health patients from a regional Australian hospital, we compare the predictive performance with popular traditional approaches clinician judgments based on a checklist, sparse logistic regression and decision trees. The randomized methods demonstrated robustness against data redundancies and superior predictive performance on AUC and F-measure. version:1
arxiv-1605-01107 | Decentralized Dynamic Discriminative Dictionary Learning | http://arxiv.org/abs/1605.01107 | id:1605.01107 author:Alec Koppel, Garrett Warnell, Ethan Stump, Alejandro Ribeiro category:stat.ML cs.LG  published:2016-05-03 summary:We consider discriminative dictionary learning in a distributed online setting, where a network of agents aims to learn a common set of dictionary elements of a feature space and model parameters while sequentially receiving observations. We formulate this problem as a distributed stochastic program with a non-convex objective and present a block variant of the Arrow-Hurwicz saddle point algorithm to solve it. Using Lagrange multipliers to penalize the discrepancy between them, only neighboring nodes exchange model information. We show that decisions made with this saddle point algorithm asymptotically achieve a first-order stationarity condition on average. version:1
arxiv-1511-05493 | Gated Graph Sequence Neural Networks | http://arxiv.org/abs/1511.05493 | id:1511.05493 author:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel category:cs.LG cs.AI cs.NE stat.ML  published:2015-11-17 summary:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures. version:3
arxiv-1605-01101 | WEPSAM: Weakly Pre-Learnt Saliency Model | http://arxiv.org/abs/1605.01101 | id:1605.01101 author:Avisek Lahiri, Sourya Roy, Anirban Santara, Pabitra Mitra, Prabir Kumar Biswas category:cs.CV  published:2016-05-03 summary:Visual saliency detection tries to mimic human vision psychology which concentrates on sparse, important areas in natural image. Saliency prediction research has been traditionally based on low level features such as contrast, edge, etc. Recent thrust in saliency prediction research is to learn high level semantics using ground truth eye fixation datasets. In this paper we present, WEPSAM : Weakly Pre-Learnt Saliency Model as a pioneering effort of using domain specific pre-learing on ImageNet for saliency prediction using a light weight CNN architecture. The paper proposes a two step hierarchical learning, in which the first step is to develop a framework for weakly pre-training on a large scale dataset such as ImageNet which is void of human eye fixation maps. The second step refines the pre-trained model on a limited set of ground truth fixations. Analysis of loss on iSUN and SALICON datasets reveal that pre-trained network converges much faster compared to randomly initialized network. WEPSAM also outperforms some recent state-of-the-art saliency prediction models on the challenging MIT300 dataset. version:1
arxiv-1605-01046 | Logarithmic proximity measures outperform plain ones in graph nodes clustering | http://arxiv.org/abs/1605.01046 | id:1605.01046 author:Vladimir Ivashkin, Pavel Chebotarev category:cs.LG cs.DM  published:2016-05-03 summary:We consider a number of graph kernels and proximity measures: commute time kernel, regularized Laplacian kernel, heat kernel, communicability, etc., and the corresponding distances as applied to clustering nodes in random graphs. The model of generating graphs involves edge probabilities for the pairs of nodes that belong to the same class or different classes. It turns out that in most cases, logarithmic measures (i.e., measures resulting after taking logarithm of the proximities) perform much better while distinguishing classes than the "plain" measures. A direct comparison of inter-class and intra-class distances confirms this conclusion. A possible explanation of this fact is that most kernels have a multiplicative nature, while the nature of distances used in cluster algorithms is an additive one (cf. the triangle inequality). The logarithmic transformation is just a tool to transform one nature to another. Moreover, some distances corresponding to the logarithmic measures possess a meaningful cutpoint additivity property. In our experiments, the leader is the so-called logarithmic communicability measure, which distinctly outperforms the other measures under study. version:1
arxiv-1605-01042 | Hierarchical Bayesian Noise Inference for Robust Real-time Probabilistic Object Classification | http://arxiv.org/abs/1605.01042 | id:1605.01042 author:Shayegan Omidshafiei, Brett T. Lopez, Jonathan P. How, John Vian category:cs.CV  published:2016-05-03 summary:Robust environment perception is essential for decision-making on robots operating in complex domains. Principled treatment of uncertainty sources in a robot's observation model is necessary for accurate mapping and object detection. This is important not only for low-level observations (e.g., accelerometer data), but for high-level observations such as semantic object labels as well. This paper presents an approach for filtering sequences of object classification probabilities using online modeling of the noise characteristics of the classifier outputs. A hierarchical Bayesian approach is used to model per-class noise distributions, while simultaneously allowing sharing of high-level noise characteristics between classes. The proposed filtering scheme, called Hierarchical Bayesian Noise Inference (HBNI), is shown to outperform classification accuracy of existing methods. The paper also presents real-time filtered classification hardware experiments running fully onboard a moving quadrotor, where the proposed approach is demonstrated to work in a challenging domain where noise-agnostic filtering fails. version:1
arxiv-1605-01029 | Online Machine Learning Techniques for Predicting Operator Performance | http://arxiv.org/abs/1605.01029 | id:1605.01029 author:Ahmet Anil Pala category:cs.LG  published:2016-05-03 summary:This thesis explores a number of online machine learning algorithms. From a theoret- ical perspective, it assesses their employability for a particular function approximation problem where the analytical models fall short. Furthermore, it discusses the applica- tion of theoretically suitable learning algorithms to the function approximation problem at hand through an efficient implementation that exploits various computational and mathematical shortcuts. Finally, this thesis work evaluates the implemented learning algorithms according to various evaluation criteria through rigorous testing. version:1
arxiv-1605-01014 | Deep Deformation Network for Object Landmark Localization | http://arxiv.org/abs/1605.01014 | id:1605.01014 author:Xiang Yu, Feng Zhou, Manmohan Chandraker category:cs.CV  published:2016-05-03 summary:We propose a novel cascaded framework, called deep deformation network (DDN), for localizing landmarks in non-rigid objects. The hallmarks of DDN are its incorporation of geometric constraints within a convolutional neural network (CNN) framework, ease and efficiency of training, as well as generality of application. A novel shape basis network (SBN) forms the first stage of the cascade, whereby landmarks are initialized by combining the benefits of CNN features and a learned shape basis to reduce the complexity of the highly nonlinear pose manifold. In the second stage, a point transformer network (PTN) estimates local deformations parameterized as thin-plate spline transformations for a finer refinement. Our framework does not incorporate either handcrafted features or part connectivity, which enables an end-to-end shape prediction pipeline during both training and testing. In contrast to prior cascaded networks for landmark localization that learn a mapping from feature space to landmark locations, we demonstrate that the regularization induced through geometric priors in the DDN makes it easier to train, yet produces superior results. The efficacy and generality of the architecture is demonstrated through state-of-the-art performances on several benchmarks for multiple tasks such as facial landmark localization, human body pose estimation and bird part localization. version:1
arxiv-1605-00959 | Personalized Risk Scoring for Critical Care Patients using Mixtures of Gaussian Process Experts | http://arxiv.org/abs/1605.00959 | id:1605.00959 author:Ahmed M. Alaa, Jinsung Yoon, Scott Hu, Mihaela van der Schaar category:cs.LG stat.ML  published:2016-05-03 summary:We develop a personalized real time risk scoring algorithm that provides timely and granular assessments for the clinical acuity of ward patients based on their (temporal) lab tests and vital signs. Heterogeneity of the patients population is captured via a hierarchical latent class model. The proposed algorithm aims to discover the number of latent classes in the patients population, and train a mixture of Gaussian Process (GP) experts, where each expert models the physiological data streams associated with a specific class. Self-taught transfer learning is used to transfer the knowledge of latent classes learned from the domain of clinically stable patients to the domain of clinically deteriorating patients. For new patients, the posterior beliefs of all GP experts about the patient's clinical status given her physiological data stream are computed, and a personalized risk score is evaluated as a weighted average of those beliefs, where the weights are learned from the patient's hospital admission information. Experiments on a heterogeneous cohort of 6,313 patients admitted to Ronald Regan UCLA medical center show that our risk score outperforms the currently deployed risk scores, such as MEWS and Rothman scores. version:1
arxiv-1605-00942 | TheanoLM - An Extensible Toolkit for Neural Network Language Modeling | http://arxiv.org/abs/1605.00942 | id:1605.00942 author:Seppo Enarvi, Mikko Kurimo category:cs.CL cs.NE  published:2016-05-03 summary:We present a new tool for training neural network language models (NNLMs), scoring sentences, and generating text. The tool has been written using Python library Theano, which allows researcher to easily extend it and tune any aspect of the training process. Regardless of the flexibility, Theano is able to generate extremely fast native code that can utilize a GPU or multiple CPU cores in order to parallelize the heavy numerical computations. The tool has been evaluated in difficult Finnish and English conversational speech recognition tasks, and significant improvement was obtained over our best back-off n-gram models. The results that we obtained in the Finnish task were compared to those from existing RNNLM and RWTHLM toolkits, and found to be as good or better, while training times were an order of magnitude shorter. version:1
arxiv-1506-08448 | Neural Simpletrons - Minimalistic Directed Generative Networks for Learning with Few Labels | http://arxiv.org/abs/1506.08448 | id:1506.08448 author:Dennis Forster, Abdul-Saboor Sheikh, Jörg Lücke category:stat.ML cs.LG  published:2015-06-28 summary:Deep learning is intensively studied using supervised and unsupervised learning, and by applying probabilistic, deterministic, and bio-inspired approaches. Comparisons of different approaches such as generative and discriminative neural networks is made difficult, however, because of differences in the semantics of their graphical descriptions, different learning methods, different benchmarking objectives and different scalability. To allow for a direct functional comparison, we here study a generative multi-layer neural network in a form and setting as similar to standard discriminative networks as possible. Based on normalized Poisson mixtures, we derive a minimalistic deep neural network with local activation and learning rules. The network learns in a semi-supervised setting and can be scaled using standard deep learning tools for parallelized implementations. Empirical evaluations on standard benchmarks show that for weakly labeled data the derived minimalistic network improves on all standard deep learning approaches and is competitive with their recent variants. In comparison to recent bio-inspired approaches it suggests further improvements through top-down connections. Furthermore, we find that the studied network is the best performing monolithic (`non-hybrid') system for few labels, and that it can be applied in the limit of very few labels, where no other system has been reported to operate so far. version:3
