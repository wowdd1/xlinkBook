arxiv-1509-05064 | Exact simultaneous recovery of locations and structure from known orientations and corrupted point correspondences | http://arxiv.org/abs/1509.05064 | id:1509.05064 author:Paul Hand, Choongbum Lee, Vladislav Voroninski category:cs.CV cs.IT math.CO math.IT math.OC  published:2015-09-16 summary:Let $t_1,\ldots,t_{n_l} \in \mathbb{R}^d$ and $p_1,\ldots,p_{n_s} \in \mathbb{R}^d$ and consider the bipartite location recovery problem: given a subset of pairwise direction observations $\{(t_i - p_j) / \ t_i - p_j\ _2\}_{i,j \in [n_l] \times [n_s]}$, where a constant fraction of these observations are arbitrarily corrupted, find $\{t_i\}_{i \in [n_ll]}$ and $\{p_j\}_{j \in [n_s]}$ up to a global translation and scale. We study the recently introduced ShapeFit algorithm as a method for solving this bipartite location recovery problem. In this case, ShapeFit consists of a simple convex program over $d(n_l + n_s)$ real variables. We prove that this program recovers a set of $n_l+n_s$ i.i.d. Gaussian locations exactly and with high probability if the observations are given by a bipartite Erd\H{o}s-R\'{e}nyi graph, $d$ is large enough, and provided that at most a constant fraction of observations involving any particular location are adversarially corrupted. This recovery theorem is based on a set of deterministic conditions that we prove are sufficient for exact recovery. Finally, we propose a modified pipeline for the Structure for Motion problem, based on this bipartite location recovery problem. version:1
arxiv-1509-05054 | Overcomplete Dictionary Learning with Jacobi Atom Updates | http://arxiv.org/abs/1509.05054 | id:1509.05054 author:Paul Irofti, Bogdan Dumitrescu category:cs.CV  published:2015-09-16 summary:Dictionary learning for sparse representations is traditionally approached with sequential atom updates, in which an optimized atom is used immediately for the optimization of the next atoms. We propose instead a Jacobi version, in which groups of atoms are updated independently, in parallel. Extensive numerical evidence for sparse image representation shows that the parallel algorithms, especially when all atoms are updated simultaneously, give better dictionaries than their sequential counterparts. version:1
arxiv-1509-05016 | Recurrent Neural Networks for Driver Activity Anticipation via Sensory-Fusion Architecture | http://arxiv.org/abs/1509.05016 | id:1509.05016 author:Ashesh Jain, Avi Singh, Hema S Koppula, Shane Soh, Ashutosh Saxena category:cs.CV cs.AI cs.RO  published:2015-09-16 summary:Anticipating the future actions of a human is a widely studied problem in robotics that requires spatio-temporal reasoning. In this work we propose a deep learning approach for anticipation in sensory-rich robotics applications. We introduce a sensory-fusion architecture which jointly learns to anticipate and fuse information from multiple sensory streams. Our architecture consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to capture long temporal dependencies. We train our architecture in a sequence-to-sequence prediction manner, and it explicitly learns to predict the future given only a partial temporal context. We further introduce a novel loss layer for anticipation which prevents over-fitting and encourages early anticipation. We use our architecture to anticipate driving maneuvers several seconds before they happen on a natural driving data set of 1180 miles. The context for maneuver anticipation comes from multiple sensors installed on the vehicle. Our approach shows significant improvement over the state-of-the-art in maneuver anticipation by increasing the precision from 77.4% to 90.5% and recall from 71.2% to 87.4%. version:1
arxiv-1509-04954 | Human and Sheep Facial Landmarks Localisation by Triplet Interpolated Features | http://arxiv.org/abs/1509.04954 | id:1509.04954 author:Heng Yang, Renqiao Zhang, Peter Robinson category:cs.CV  published:2015-09-16 summary:In this paper we present a method for localisation of facial landmarks on human and sheep. We introduce a new feature extraction scheme called triplet-interpolated feature used at each iteration of the cascaded shape regression framework. It is able to extract features from similar semantic location given an estimated shape, even when head pose variations are large and the facial landmarks are very sparsely distributed. Furthermore, we study the impact of training data imbalance on model performance and propose a training sample augmentation scheme that produces more initialisations for training samples from the minority. More specifically, the augmentation number for a training sample is made to be negatively correlated to the value of the fitted probability density function at the sample's position. We evaluate the proposed scheme on both human and sheep facial landmarks localisation. On the benchmark 300w human face dataset, we demonstrate the benefits of our proposed methods and show very competitive performance when comparing to other methods. On a newly created sheep face dataset, we get very good performance despite the fact that we only have a limited number of training samples and a set of sparse landmarks are annotated. version:1
arxiv-1506-04654 | Thin Structure Estimation with Curvature Regularization | http://arxiv.org/abs/1506.04654 | id:1506.04654 author:Dmitrii Marin, Yuri Boykov, Yuchen Zhong category:cs.CV  published:2015-06-15 summary:Many applications in vision require estimation of thin structures such as boundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike most previous approaches, we simultaneously detect and delineate thin structures with sub-pixel localization and real-valued orientation estimation. This is an ill-posed problem that requires regularization. We propose an objective function combining detection likelihoods with a prior minimizing curvature of the center-lines or surfaces. Unlike simple block-coordinate descent, we develop a novel algorithm that is able to perform joint optimization of location and detection variables more effectively. Our lower bound optimization algorithm applies to quadratic or absolute curvature. The proposed early vision framework is sufficiently general and it can be used in many higher-level applications. We illustrate the advantage of our approach on a range of 2D and 3D examples. version:2
arxiv-1509-04942 | Guiding Long-Short Term Memory for Image Caption Generation | http://arxiv.org/abs/1509.04942 | id:1509.04942 author:Xu Jia, Efstratios Gavves, Basura Fernando, Tinne Tuytelaars category:cs.CV  published:2015-09-16 summary:In this work we focus on the problem of image caption generation. We propose an extension of the long short term memory (LSTM) model, which we coin gLSTM for short. In particular, we add semantic information extracted from the image as extra input to each unit of the LSTM block, with the aim of guiding the model towards solutions that are more tightly coupled to the image content. Additionally, we explore different length normalization strategies for beam search in order to prevent from favoring short sentences. On various benchmark datasets such as Flickr8K, Flickr30K and MS COCO, we obtain results that are on par with or even outperform the current state-of-the-art. version:1
arxiv-1509-04916 | Projection Bank: From High-dimensional Data to Medium-length Binary Codes | http://arxiv.org/abs/1509.04916 | id:1509.04916 author:Li Liu, Mengyang Yu, Ling Shao category:cs.CV  published:2015-09-16 summary:Recently, very high-dimensional feature representations, e.g., Fisher Vector, have achieved excellent performance for visual recognition and retrieval. However, these lengthy representations always cause extremely heavy computational and storage costs and even become unfeasible in some large-scale applications. A few existing techniques can transfer very high-dimensional data into binary codes, but they still require the reduced code length to be relatively long to maintain acceptable accuracies. To target a better balance between computational efficiency and accuracies, in this paper, we propose a novel embedding method called Binary Projection Bank (BPB), which can effectively reduce the very high-dimensional representations to medium-dimensional binary codes without sacrificing accuracies. Instead of using conventional single linear or bilinear projections, the proposed method learns a bank of small projections via the max-margin constraint to optimally preserve the intrinsic data similarity. We have systematically evaluated the proposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showing competitive retrieval and recognition accuracies compared with state-of-the-art approaches, but with a significantly smaller memory footprint and lower coding complexity. version:1
arxiv-1509-04265 | Double Relief with progressive weighting function | http://arxiv.org/abs/1509.04265 | id:1509.04265 author:Gabriel Prat Masramon, Lluís A. Belanche Muñoz category:cs.LG cs.AI  published:2015-09-12 summary:Feature weighting algorithms try to solve a problem of great importance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem's domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task. On previous work, a new extension was proposed that aimed for improving the algorithm's performance and it was shown that in certain cases it improved the weights' estimation accuracy. However, it also seemed to be sensible to some characteristics of the data. An improvement of that previously presented extension is presented in this work that aims to make it more robust to problem specific characteristics. An experimental design is proposed to test its performance. Results of the tests prove that it indeed increase the robustness of the previously proposed extension. version:2
arxiv-1509-03755 | Toward better feature weighting algorithms: a focus on Relief | http://arxiv.org/abs/1509.03755 | id:1509.03755 author:Gabriel Prat Masramon, Lluís A. Belanche Muñoz category:cs.LG  published:2015-09-12 summary:Feature weighting algorithms try to solve a problem of great importance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem's domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task. Some other feature weighting methods are reviewed in order to give some context and then the different existing extensions to the original algorithm are explained. One of Relief's known issues is the performance degradation of its estimates when redundant features are present. A novel theoretical definition of redundancy level is given in order to guide the work towards an extension of the algorithm that is more robust against redundancy. A new extension is presented that aims for improving the algorithms performance. Some experiments were driven to test this new extension against the existing ones with a set of artificial and real datasets and denoted that in certain cases it improves the weight's estimation accuracy. version:2
arxiv-1509-04612 | Adapting Resilient Propagation for Deep Learning | http://arxiv.org/abs/1509.04612 | id:1509.04612 author:Alan Mosca, George D. Magoulas category:cs.NE cs.CV cs.LG stat.ML  published:2015-09-15 summary:The Resilient Propagation (Rprop) algorithm has been very popular for backpropagation training of multilayer feed-forward neural networks in various applications. The standard Rprop however encounters difficulties in the context of deep neural networks as typically happens with gradient-based learning algorithms. In this paper, we propose a modification of the Rprop that combines standard Rprop steps with a special drop out technique. We apply the method for training Deep Neural Networks as standalone components and in ensemble formulations. Results on the MNIST dataset show that the proposed modification alleviates standard Rprop's problems demonstrating improved learning speed and accuracy. version:2
arxiv-1509-04887 | A Drowsiness Detection Scheme Based on Fusion of Voice and Vision Cues | http://arxiv.org/abs/1509.04887 | id:1509.04887 author:Anirban Dasgupta, Bibek Kabi, Anjith George, SL Happy, Aurobinda Routray category:cs.CV  published:2015-09-16 summary:Drowsiness level detection of an individual is very important in many safety critical applications such as driving. There are several invasive and contact based methods such as use of blood biochemical, brain signals etc. which can estimate the level of drowsiness very accurately. However, these methods are very difficult to implement in practical scenarios, as they cause discomfort to the user. This paper presents a combined voice and vision based drowsiness detection system well suited to detect the drowsiness level of an automotive driver. The vision and voice based detection, being non-contact methods, has the advantage of their feasibility of implementation. The authenticity of these methods have been cross-validated using brain signals. version:1
arxiv-1409-7552 | The Advantage of Cross Entropy over Entropy in Iterative Information Gathering | http://arxiv.org/abs/1409.7552 | id:1409.7552 author:Johannes Kulick, Robert Lieck, Marc Toussaint category:stat.ML cs.LG  published:2014-09-26 summary:Gathering the most information by picking the least amount of data is a common task in experimental design or when exploring an unknown environment in reinforcement learning and robotics. A widely used measure for quantifying the information contained in some distribution of interest is its entropy. Greedily minimizing the expected entropy is therefore a standard method for choosing samples in order to gain strong beliefs about the underlying random variables. We show that this approach is prone to temporally getting stuck in local optima corresponding to wrongly biased beliefs. We suggest instead maximizing the expected cross entropy between old and new belief, which aims at challenging refutable beliefs and thereby avoids these local optima. We show that both criteria are closely related and that their difference can be traced back to the asymmetry of the Kullback-Leibler divergence. In illustrative examples as well as simulated and real-world experiments we demonstrate the advantage of cross entropy over simple entropy for practical applications. version:2
arxiv-1509-04863 | Fast Template Matching by Subsampled Circulant Matrix | http://arxiv.org/abs/1509.04863 | id:1509.04863 author:Sung-Hsien Hsieh, Chun-Shien Lu, and Soo-Chang Pei category:cs.DS cs.CV  published:2015-09-16 summary:Template matching is widely used for many applications in image and signal processing and usually is time-critical. Traditional methods usually focus on how to reduce the search locations by coarse-to-fine strategy or full search combined with pruning strategy. However, the computation cost of those methods is easily dominated by the size of signal N instead of that of template K. This paper proposes a probabilistic and fast matching scheme, which computation costs requires O(N) additions and O(K \log K) multiplications, based on cross-correlation. The nuclear idea is to first downsample signal, which size becomes O(K), and then subsequent operations only involves downsampled signals. The probability of successful match depends on cross-correlation between signal and the template. We show the sufficient condition for successful match and prove that the probability is high for binary signals with K^2/log K >= O(N). The experiments shows this proposed scheme is fast and efficient and supports the theoretical results. version:1
arxiv-1404-4412 | Efficient Nonnegative Tucker Decompositions: Algorithms and Uniqueness | http://arxiv.org/abs/1404.4412 | id:1404.4412 author:Guoxu Zhou, Andrzej Cichocki, Qibin Zhao, Shengli Xie category:cs.LG cs.CV stat.ML  published:2014-04-17 summary:Nonnegative Tucker decomposition (NTD) is a powerful tool for the extraction of nonnegative parts-based and physically meaningful latent components from high-dimensional tensor data while preserving the natural multilinear structure of data. However, as the data tensor often has multiple modes and is large-scale, existing NTD algorithms suffer from a very high computational complexity in terms of both storage and computation time, which has been one major obstacle for practical applications of NTD. To overcome these disadvantages, we show how low (multilinear) rank approximation (LRA) of tensors is able to significantly simplify the computation of the gradients of the cost function, upon which a family of efficient first-order NTD algorithms are developed. Besides dramatically reducing the storage complexity and running time, the new algorithms are quite flexible and robust to noise because any well-established LRA approaches can be applied. We also show how nonnegativity incorporating sparsity substantially improves the uniqueness property and partially alleviates the curse of dimensionality of the Tucker decompositions. Simulation results on synthetic and real-world data justify the validity and high efficiency of the proposed NTD algorithms. version:2
arxiv-1509-04853 | An On-board Video Database of Human Drivers | http://arxiv.org/abs/1509.04853 | id:1509.04853 author:Anirban Dasgupta, Anjith George, SL Happy, Aurobinda Routray category:cs.CV  published:2015-09-16 summary:Detection of fatigue due to drowsiness or loss of attention in human drivers is an evolving area of research. Several algorithms have been implemented to detect the level of fatigue in human drivers by capturing videos of facial image sequences and extracting facial features such as eye closure rates, eye gaze, head nodding, blink frequency etc. However, availability of standard video database to validate such algorithms is insufficient. This paper discusses the creation of such a database created under on-board conditions during the day as well as night. Passive Near Infra-red (NIR) illumination has been used for illuminating the face during night driving since prolonged exposure to active Infra-Red lighting may lead to many health issues. The database contains videos of 30 subjects under actual driving conditions. Variation is ensured as the database contains different head orientations and with different facial expressions, facial occlusions and illumination variation. This new database can be a very valuable resource for development and evaluation of algorithms for the video based detection of driver fatigue. version:1
arxiv-1509-04811 | amLite: Amharic Transliteration Using Key Map Dictionary | http://arxiv.org/abs/1509.04811 | id:1509.04811 author:Tadele Tedla category:cs.CL cs.IR  published:2015-09-16 summary:amLite is a framework developed to map ASCII transliterated Amharic texts back to the original Amharic letter texts. The aim of such a framework is to make existing Amharic linguistic data consistent and interoperable among researchers. For achieving the objective, a key map dictionary is constructed using the possible ASCII combinations actively in use for transliterating Amharic letters; and a mapping of the combinations to the corresponding Amharic letters is done. The mapping is then used to replace the Amharic linguistic text back to form the original Amharic letters text. The framework indicated 97.7, 99.7 and 98.4 percentage accuracy on converting the three sample random test data. It is; however, possible to improve the accuracy of the framework by adding an exception to the implementation of the algorithm, or by preprocessing the input text prior to conversion. This paper outlined the rationales behind the need for developing the framework and the processes undertaken in the development. version:1
arxiv-1509-04783 | Group Membership Prediction | http://arxiv.org/abs/1509.04783 | id:1509.04783 author:Ziming Zhang, Yuting Chen, Venkatesh Saligrama category:cs.CV stat.ML  published:2015-09-16 summary:The group membership prediction (GMP) problem involves predicting whether or not a collection of instances share a certain semantic property. For instance, in kinship verification given a collection of images, the goal is to predict whether or not they share a {\it familial} relationship. In this context we propose a novel probability model and introduce latent {\em view-specific} and {\em view-shared} random variables to jointly account for the view-specific appearance and cross-view similarities among data instances. Our model posits that data from each view is independent conditioned on the shared variables. This postulate leads to a parametric probability model that decomposes group membership likelihood into a tensor product of data-independent parameters and data-dependent factors. We propose learning the data-independent parameters in a discriminative way with bilinear classifiers, and test our prediction algorithm on challenging visual recognition tasks such as multi-camera person re-identification and kinship verification. On most benchmark datasets, our method can significantly outperform the current state-of-the-art. version:1
arxiv-1509-04781 | Dirichlet Fragmentation Processes | http://arxiv.org/abs/1509.04781 | id:1509.04781 author:Hong Ge, Yarin Gal, Zoubin Ghahramani category:stat.ML  published:2015-09-16 summary:Tree structures are ubiquitous in data across many domains, and many datasets are naturally modelled by unobserved tree structures. In this paper, first we review the theory of random fragmentation processes [Bertoin, 2006], and a number of existing methods for modelling trees, including the popular nested Chinese restaurant process (nCRP). Then we define a general class of probability distributions over trees: the Dirichlet fragmentation process (DFP) through a novel combination of the theory of Dirichlet processes and random fragmentation processes. This DFP presents a stick-breaking construction, and relates to the nCRP in the same way the Dirichlet process relates to the Chinese restaurant process. Furthermore, we develop a novel hierarchical mixture model with the DFP, and empirically compare the new model to similar models in machine learning. Experiments show the DFP mixture model to be convincingly better than existing state-of-the-art approaches for hierarchical clustering and density modelling. version:1
arxiv-1509-04556 | On the evolution of word usage of classical Chinese poetry | http://arxiv.org/abs/1509.04556 | id:1509.04556 author:Liang Liu category:physics.soc-ph cs.CL  published:2015-09-10 summary:The hierarchy of classical Chinese poetry has been broadly acknowledged by a number of studies in Chinese literature. However, quantitative investigations about the evolution of classical Chinese poetry are limited. The primary goal of this study is to provide quantitative evidence of the evolutionary linkages, with emphasis on word usage, among different period genres for classical Chinese poetry. Specifically, various statistical analyses were performed to find and compare the patterns of word usage in the poems of nine period genres, including shi jing, chu ci, Han shi , Jin shi, Tang shi, Song shi, Yuan shi, Ming shi, and Qing shi. The result of analysis indicates that each of nine period genres has unique patterns of word usage, with some Chinese characters being preferably used by the poems of a particular period genre. The analysis on the general pattern of word preference implies a decreasing trend in the use of ancient Chinese characters along the timeline of dynastic types of classical Chinese poetry. The phylogenetic analysis based on the distance matrix suggests that the evolution of different types of classical Chinese poetry is congruent with their chronological order, suggesting that word frequencies contain useful phylogenetic information and thus can be used to infer evolutionary linkages among various types of classical Chinese poetry. The statistical analyses conducted in this study can be applied to the data sets of general Chinese literature. Such analyses can provide quantitative insights about the evolution of general Chinese literature. version:2
arxiv-1509-04771 | Linear Embedding of Large-Scale Brain Networks for Twin fMRI | http://arxiv.org/abs/1509.04771 | id:1509.04771 author:Moo K. Chung, Victoria G. Vilalta, Paul J. Rathouz, Benjamin B. Lahey, David H. Zald category:cs.AI q-bio.NC stat.ML  published:2015-09-15 summary:In many human brain network studies, we do not have sufficient number (n) of images relative to the number (p) of voxels due to the prohibitively expensive cost of scanning enough subjects. Thus, brain network models usually suffer the small-n large-p problem. Such a problem is often remedied by sparse network models, which are usually solved numerically by optimizing L1-penalties. Unfortunately, due to the computational bottleneck associated with optimizing L1-penalties, it is not practical to apply such methods to learn large-scale brain networks. In this paper, we introduce a new sparse network model based on cross-correlations that bypass the computational bottleneck. Our model can build the sparse brain networks at voxel level with p > 25000. Instead of using a single sparse parameter that may not be optimal in other studies and datasets, we propose to analyze the collection of networks at every possible sparse parameter in a coherent mathematical framework using graph filtrations. The method is subsequently applied in determining the extend of genetic effects on functional brain networks at voxel-level for the first time using twin fMRI. version:1
arxiv-1509-04751 | Free-body Gesture Tracking and Augmented Reality Improvisation for Floor and Aerial Dance | http://arxiv.org/abs/1509.04751 | id:1509.04751 author:Tammuz Dubnov, Cheng-i Wang category:cs.MM cs.CV cs.HC  published:2015-09-15 summary:This paper describes an updated interactive performance system for floor and Aerial Dance that controls visual and sonic aspects of the presentation via a depth sensing camera (MS Kinect). In order to detect, measure and track free movement in space, 3 degree of freedom (3-DOF) tracking in space (on the ground and in the air) is performed using IR markers with a method for multi target tracking capabilities added and described in detail. An improved gesture tracking and recognition system, called Action Graph (AG), is described in the paper. Action Graph uses an efficient incremental construction from a single long sequence of movement features and automatically captures repeated sub-segments in the movement from start to finish with no manual interaction needed with other advanced capabilities discussed as well. By using the new model for the gesture we can unify an entire choreography piece by dynamically tracking and recognizing gestures and sub-portions of the piece. This gives the performer the freedom to improvise based on a set of recorded gestures/portions of the choreography and have the system dynamically respond in relation to the performer within a set of related rehearsed actions, an ability that has not been seen in any other system to date. version:1
arxiv-1509-04740 | Modeling sequences and temporal networks with dynamic community structures | http://arxiv.org/abs/1509.04740 | id:1509.04740 author:Tiago P. Peixoto, Martin Rosvall category:cs.SI cond-mat.stat-mech physics.soc-ph stat.ML  published:2015-09-15 summary:Community-detection methods that describe large-scale patterns in the dynamics on and of networks suffer from effects of limited memory and arbitrary time binning. We develop a variable-order Markov chain model that generalizes the stochastic block model for discrete time-series as well as temporal networks. The temporal model does not use time binning but takes full advantage of the time-ordering of the tokens or edges. When the edge ordering is random, we recover the traditional static block model as a special case. Based on statistical evidence and without overfitting, we show how a Bayesian formulation of the model allows us to select the most appropriate Markov order and number of communities. version:1
arxiv-1505-03046 | Improving Computer-aided Detection using Convolutional Neural Networks and Random View Aggregation | http://arxiv.org/abs/1505.03046 | id:1505.03046 author:Holger R. Roth, Le Lu, Jiamin Liu, Jianhua Yao, Ari Seff, Kevin Cherry, Lauren Kim, Ronald M. Summers category:cs.CV  published:2015-05-12 summary:Automated computer-aided detection (CADe) in medical imaging has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities but at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities of $\sim$100% but at high FP levels. By leveraging existing CAD systems, coordinates of regions or volumes of interest (ROI or VOI) for lesion candidates are generated in this step and function as input for a second tier, which is our focus in this study. In this second stage, we generate $N$ 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations with respect to each ROI's centroid coordinates. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the trained ConvNets are employed to assign class (e.g., lesion, pathology) probabilities for a new set of $N$ random views that are then averaged at each ROI to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three different data sets with different numbers of patients: 59 patients for sclerotic metastases detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve CADe performance markedly in all cases. CADe sensitivities improved from 57% to 70%, from 43% to 77% and from 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively. version:2
arxiv-1503-03082 | Learning the Structure for Structured Sparsity | http://arxiv.org/abs/1503.03082 | id:1503.03082 author:Nino Shervashidze, Francis Bach category:stat.ML  published:2015-03-10 summary:Structured sparsity has recently emerged in statistics, machine learning and signal processing as a promising paradigm for learning in high-dimensional settings. All existing methods for learning under the assumption of structured sparsity rely on prior knowledge on how to weight (or how to penalize) individual subsets of variables during the subset selection process, which is not available in general. Inferring group weights from data is a key open research problem in structured sparsity.In this paper, we propose a Bayesian approach to the problem of group weight learning. We model the group weights as hyperparameters of heavy-tailed priors on groups of variables and derive an approximate inference scheme to infer these hyperparameters. We empirically show that we are able to recover the model hyperparameters when the data are generated from the model, and we demonstrate the utility of learning weights in synthetic and real denoising problems. version:2
arxiv-1412-1684 | How Many Communities Are There? | http://arxiv.org/abs/1412.1684 | id:1412.1684 author:Diego Franco Saldana, Yi Yu, Yang Feng category:stat.ME stat.AP stat.CO stat.ML  published:2014-12-04 summary:Stochastic blockmodels and variants thereof are among the most widely used approaches to community detection for social networks and relational data. A stochastic blockmodel partitions the nodes of a network into disjoint sets, called communities. The approach is inherently related to clustering with mixture models; and raises a similar model selection problem for the number of communities. The Bayesian information criterion (BIC) is a popular solution, however, for stochastic blockmodels, the conditional independence assumption given the communities of the endpoints among different edges is usually violated in practice. In this regard, we propose composite likelihood BIC (CL-BIC) to select the number of communities, and we show it is robust against possible misspecifications in the underlying stochastic blockmodel assumptions. We derive the requisite methodology and illustrate the approach using both simulated and real data. Supplementary materials containing the relevant computer code are available online. version:2
arxiv-1509-04706 | Direct high-order edge-preserving regularization for tomographic image reconstruction | http://arxiv.org/abs/1509.04706 | id:1509.04706 author:Daniil Kazantsev, Evgueni Ovtchinnikov, William R. B. Lionheart, Philip J. Withers, Peter D. Lee category:cs.CV cs.MS cs.NA math.NA  published:2015-09-15 summary:In this paper we present a new two-level iterative algorithm for tomographic image reconstruction. The algorithm uses a regularization technique, which we call edge-preserving Laplacian, that preserves sharp edges between objects while damping spurious oscillations in the areas where the reconstructed image is smooth. Our numerical simulations demonstrate that the proposed method outperforms total variation (TV) regularization and it is competitive with the combined TV-L2 penalty. Obtained reconstructed images show increased signal-to-noise ratio and visually appealing structural features. Computer implementation and parameter control of the proposed technique is straightforward, which increases the feasibility of it across many tomographic applications. In this paper, we applied our method to the under-sampled computed tomography (CT) projection data and also considered a case of reconstruction in emission tomography The MATLAB code is provided to support obtained results. version:1
arxiv-1509-04664 | Self-Configuring and Evolving Fuzzy Image Thresholding | http://arxiv.org/abs/1509.04664 | id:1509.04664 author:A. Othman, H. R. Tizhoosh, F. Khalvati category:cs.CV  published:2015-09-15 summary:Every segmentation algorithm has parameters that need to be adjusted in order to achieve good results. Evolving fuzzy systems for adjustment of segmentation parameters have been proposed recently (Evolving fuzzy image segmentation -- EFIS [1]. However, similar to any other algorithm, EFIS too suffers from a few limitations when used in practice. As a major drawback, EFIS depends on detection of the object of interest for feature calculation, a task that is highly application-dependent. In this paper, a new version of EFIS is proposed to overcome these limitations. The new EFIS, called self-configuring EFIS (SC-EFIS), uses available training data to auto-configure the parameters that are fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selection process that does not require the detection of a region of interest (ROI). version:1
arxiv-1509-04705 | Forecasting Method for Grouped Time Series with the Use of k-Means Algorithm | http://arxiv.org/abs/1509.04705 | id:1509.04705 author:N. N. Astakhova, L. A. Demidova, E. V. Nikulchev category:cs.LG  published:2015-09-15 summary:The paper is focused on the forecasting method for time series groups with the use of algorithms for cluster analysis. $K$-means algorithm is suggested to be a basic one for clustering. The coordinates of the centers of clusters have been put in correspondence with summarizing time series data the centroids of the clusters. A description of time series, the centroids of the clusters, is implemented with the use of forecasting models. They are based on strict binary trees and a modified clonal selection algorithm. With the help of such forecasting models, the possibility of forming analytic dependences is shown. It is suggested to use a common forecasting model, which is constructed for time series the centroid of the cluster, in forecasting the private (individual) time series in the cluster. The promising application of the suggested method for grouped time series forecasting is demonstrated. version:1
arxiv-1509-04640 | Dynamic Poisson Factorization | http://arxiv.org/abs/1509.04640 | id:1509.04640 author:Laurent Charlin, Rajesh Ranganath, James McInerney, David M. Blei category:cs.LG cs.IR stat.ML  published:2015-09-15 summary:Models for recommender systems use latent factors to explain the preferences and behaviors of users with respect to a set of items (e.g., movies, books, academic papers). Typically, the latent factors are assumed to be static and, given these factors, the observed preferences and behaviors of users are assumed to be generated without order. These assumptions limit the explorative and predictive capabilities of such models, since users' interests and item popularity may evolve over time. To address this, we propose dPF, a dynamic matrix factorization model based on the recent Poisson factorization model for recommendations. dPF models the time evolving latent factors with a Kalman filter and the actions with Poisson distributions. We derive a scalable variational inference algorithm to infer the latent factors. Finally, we demonstrate dPF on 10 years of user click data from arXiv.org, one of the largest repository of scientific papers and a formidable source of information about the behavior of scientists. Empirically we show performance improvement over both static and, more recently proposed, dynamic recommendation models. We also provide a thorough exploration of the inferred posteriors over the latent variables. version:1
arxiv-1509-04634 | Modeling and interpolation of the ambient magnetic field by Gaussian processes | http://arxiv.org/abs/1509.04634 | id:1509.04634 author:Arno Solin, Manon Kok, Niklas Wahlström, Thomas B. Schön, Simo Särkkä category:cs.RO stat.ML  published:2015-09-15 summary:Anomalies in the ambient magnetic field can be used as features in indoor positioning and navigation. By using Maxwell's equations, we derive and present a Bayesian non-parametric probabilistic modeling approach for interpolation and extrapolation of the magnetic field. We model the magnetic field components jointly by imposing a Gaussian process (GP) prior on the latent scalar potential of the magnetic field. By rewriting the GP model in terms of a Hilbert space representation, we circumvent the computational pitfalls associated with GP modeling and provide a computationally efficient and physically justified modeling tool for the ambient magnetic field. The model allows for sequential updating of the estimate and time-dependent changes in the magnetic field. The model is shown to work well in practice in different applications: we demonstrate mapping of the magnetic field both with an inexpensive Raspberry Pi powered robot and on foot using a standard smartphone. version:1
arxiv-1509-04632 | The Shape of Data and Probability Measures | http://arxiv.org/abs/1509.04632 | id:1509.04632 author:Diego Hernán Díaz Martínez, Facundo Mémoli, Washington Mio category:stat.ML math.MG math.ST stat.TH  published:2015-09-15 summary:We introduce the notion of multiscale covariance tensor fields (CTF) associated with Euclidean random variables as a gateway to the shape of their distributions. Multiscale CTFs quantify variation of the data about every point in the data landscape at all spatial scales, unlike the usual covariance tensor that only quantifies global variation about the mean. Empirical forms of localized covariance previously have been used in data analysis and visualization, but we develop a framework for the systematic treatment of theoretical questions and computational models based on localized covariance. We prove strong stability theorems with respect to the Wasserstein distance between probability measures, obtain consistency results, as well as estimates for the rate of convergence of empirical CTFs. These results ensure that CTFs are robust to sampling, noise and outliers. We provide numerous illustrations of how CTFs let us extract shape from data and also apply CTFs to manifold clustering, the problem of categorizing data points according to their noisy membership in a collection of possibly intersecting, smooth submanifolds of Euclidean space. We prove that the proposed manifold clustering method is stable and carry out several experiments to validate the method. version:1
arxiv-1509-04619 | Medical Image Classification via SVM using LBP Features from Saliency-Based Folded Data | http://arxiv.org/abs/1509.04619 | id:1509.04619 author:Zehra Camlica, H. R. Tizhoosh, Farzad Khalvati category:cs.CV  published:2015-09-15 summary:Good results on image classification and retrieval using support vector machines (SVM) with local binary patterns (LBPs) as features have been extensively reported in the literature where an entire image is retrieved or classified. In contrast, in medical imaging, not all parts of the image may be equally significant or relevant to the image retrieval application at hand. For instance, in lung x-ray image, the lung region may contain a tumour, hence being highly significant whereas the surrounding area does not contain significant information from medical diagnosis perspective. In this paper, we propose to detect salient regions of images during training and fold the data to reduce the effect of irrelevant regions. As a result, smaller image areas will be used for LBP features calculation and consequently classification by SVM. We use IRMA 2009 dataset with 14,410 x-ray images to verify the performance of the proposed approach. The results demonstrate the benefits of saliency-based folding approach that delivers comparable classification accuracies with state-of-the-art but exhibits lower computational cost and storage requirements, factors highly important for big data analytics. version:1
arxiv-1403-5029 | Network-based Isoform Quantification with RNA-Seq Data for Cancer Transcriptome Analysis | http://arxiv.org/abs/1403.5029 | id:1403.5029 author:Wei Zhang, Jae-Woong Chang, Lilong Lin, Kay Minn, Baolin Wu, Jeremy Chien, Jeongsik Yong, Hui Zheng, Rui Kuang category:cs.CE cs.AI cs.LG  published:2014-03-20 summary:High-throughput mRNA sequencing (RNA-Seq) is widely used for transcript quantification of gene isoforms. Since RNA-Seq data alone is often not sufficient to accurately identify the read origins from the isoforms for quantification, we propose to explore protein domain-domain interactions as prior knowledge for integrative analysis with RNA-seq data. We introduce a Network-based method for RNA-Seq-based Transcript Quantification (Net-RSTQ) to integrate protein domain-domain interaction network with short read alignments for transcript abundance estimation. Based on our observation that the abundances of the neighboring isoforms by domain-domain interactions in the network are positively correlated, Net-RSTQ models the expression of the neighboring transcripts as Dirichlet priors on the likelihood of the observed read alignments against the transcripts in one gene. The transcript abundances of all the genes are then jointly estimated with alternating optimization of multiple EM problems. In simulation Net-RSTQ effectively improved isoform transcript quantifications when isoform co-expressions correlate with their interactions. qRT-PCR results on 25 multi-isoform genes in a stem cell line, an ovarian cancer cell line, and a breast cancer cell line also showed that Net-RSTQ estimated more consistent isoform proportions with RNA-Seq data. In the experiments on the RNA-Seq data in The Cancer Genome Atlas (TCGA), the transcript abundances estimated by Net-RSTQ are more informative for patient sample classification of ovarian cancer, breast cancer and lung cancer. All experimental results collectively support that Net-RSTQ is a promising approach for isoform quantification. version:3
arxiv-1509-04581 | Kernelized Deep Convolutional Neural Network for Describing Complex Images | http://arxiv.org/abs/1509.04581 | id:1509.04581 author:Zhen Liu category:cs.CV cs.AI cs.IR cs.MM  published:2015-09-15 summary:With the impressive capability to capture visual content, deep convolutional neural networks (CNN) have demon- strated promising performance in various vision-based ap- plications, such as classification, recognition, and objec- t detection. However, due to the intrinsic structure design of CNN, for images with complex content, it achieves lim- ited capability on invariance to translation, rotation, and re-sizing changes, which is strongly emphasized in the s- cenario of content-based image retrieval. In this paper, to address this problem, we proposed a new kernelized deep convolutional neural network. We first discuss our motiva- tion by an experimental study to demonstrate the sensitivi- ty of the global CNN feature to the basic geometric trans- formations. Then, we propose to represent visual content with approximate invariance to the above geometric trans- formations from a kernelized perspective. We extract CNN features on the detected object-like patches and aggregate these patch-level CNN features to form a vectorial repre- sentation with the Fisher vector model. The effectiveness of our proposed algorithm is demonstrated on image search application with three benchmark datasets. version:1
arxiv-1509-04580 | Maximum Correntropy Kalman Filter | http://arxiv.org/abs/1509.04580 | id:1509.04580 author:Badong Chen, Xi Liu, Haiquan Zhao, José C. Príncipe category:stat.ML cs.SY  published:2015-09-15 summary:Traditional Kalman filter (KF) is derived under the well-known minimum mean square error (MMSE) criterion, which is optimal under Gaussian assumption. However, when the signals are non-Gaussian, especially when the system is disturbed by some heavy-tailed impulsive noises, the performance of KF will deteriorate seriously. To improve the robustness of KF against impulsive noises, we propose in this work a new Kalman filter, called the maximum correntropy Kalman filter (MCKF), which adopts the robust maximum correntropy criterion (MCC) as the optimality criterion, instead of using the MMSE. Similar to the traditional KF, the state mean and covariance matrix propagation equations are used to give prior estimations of the state and covariance matrix in MCKF. A novel fixed-point algorithm is then used to update the posterior estimations. A sufficient condition that guarantees the convergence of the fixed-point algorithm is given. Illustration examples are presented to demonstrate the effectiveness and robustness of the new algorithm. version:1
arxiv-1509-04541 | When are Kalman-filter restless bandits indexable? | http://arxiv.org/abs/1509.04541 | id:1509.04541 author:Christopher R. Dance, Tomi Silander category:stat.ML  published:2015-09-15 summary:We study the restless bandit associated with an extremely simple scalar Kalman filter model in discrete time. Under certain assumptions, we prove that the problem is indexable in the sense that the Whittle index is a non-decreasing function of the relevant belief state. In spite of the long history of this problem, this appears to be the first such proof. We use results about Schur-convexity and mechanical words, which are particular binary strings intimately related to palindromes. version:1
arxiv-1602-07620 | A Low Complexity VLSI Architecture for Multi-Focus Image Fusion in DCT Domain | http://arxiv.org/abs/1602.07620 | id:1602.07620 author:Ashutosh Mishra, Sudipta Mahapatra, Swapna Banerjee category:cs.CV  published:2015-09-15 summary:Due to the confined focal length of optical sensors, focusing all objects in a scene with a single sensor is a difficult task. To handle such a situation, image fusion methods are used in multi-focus environment. Discrete Cosine Transform (DCT) is a widely used image compression transform, image fusion in DCT domain is an efficient method. This paper presents a low complexity approach for multi-focus image fusion and its VLSI implementation using DCT. The proposed method is evaluated using reference/non-reference fusion measure criteria and the obtained results asserts it's effectiveness. The maximum synthesized frequency on FPGA is found to be 221 MHz and consumes 42% of FPGA resources. The proposed method consumes very less power and can process 4K resolution images at the rate of 60 frames per second which makes the hardware suitable for handheld portable devices such as camera module and wireless image sensors. version:1
arxiv-1509-04473 | Splitting Compounds by Semantic Analogy | http://arxiv.org/abs/1509.04473 | id:1509.04473 author:Joachim Daiber, Lautaro Quiroz, Roger Wechsler, Stella Frank category:cs.CL  published:2015-09-15 summary:Compounding is a highly productive word-formation process in some languages that is often problematic for natural language processing applications. In this paper, we investigate whether distributional semantics in the form of word embeddings can enable a deeper, i.e., more knowledge-rich, processing of compounds than the standard string-based methods. We present an unsupervised approach that exploits regularities in the semantic vector space (based on analogies such as "bookshop is to shop as bookshelf is to shelf") to produce compound analyses of high quality. A subsequent compound splitting algorithm based on these analyses is highly effective, particularly for ambiguous compounds. German to English machine translation experiments show that this semantic analogy-based compound splitter leads to better translations than a commonly used frequency-based method. version:1
arxiv-1310-1363 | Weakly supervised clustering: Learning fine-grained signals from coarse labels | http://arxiv.org/abs/1310.1363 | id:1310.1363 author:Stefan Wager, Alexander Blocker, Niall Cardin category:stat.ML cs.LG  published:2013-10-04 summary:Consider a classification problem where we do not have access to labels for individual training examples, but only have average labels over subpopulations. We give practical examples of this setup and show how such a classification task can usefully be analyzed as a weakly supervised clustering problem. We propose three approaches to solving the weakly supervised clustering problem, including a latent variables model that performs well in our experiments. We illustrate our methods on an analysis of aggregated elections data and an industry data set that was the original motivation for this research. version:3
arxiv-1509-04420 | Neuron detection in stack images: a persistent homology interpretation | http://arxiv.org/abs/1509.04420 | id:1509.04420 author:Jónathan Heras, Gadea Mata, Germán Cuesto, Julio Rubio, Miguel Morales category:cs.CV q-bio.NC  published:2015-09-15 summary:Automation and reliability are the two main requirements when computers are applied in Life Sciences. In this paper we report on an application to neuron recognition, an important step in our long-term project of providing software systems to the study of neural morphology and functionality from biomedical images. Our algorithms have been implemented in an ImageJ plugin called NeuronPersistentJ, which has been validated experimentally. The soundness and reliability of our approach are based on the interpretation of our processing methods with respect to persistent homology, a well-known tool in computational mathematics. version:1
arxiv-1502-00060 | A Random Matrix Theoretical Approach to Early Event Detection in Smart Grid | http://arxiv.org/abs/1502.00060 | id:1502.00060 author:Xing He, Robert Caiming Qiu, Qian Ai, Yinshuang Cao, Jie Gu, Zhijian Jin category:stat.ME cs.LG  published:2015-01-31 summary:Power systems are developing very fast nowadays, both in size and in complexity; this situation is a challenge for Early Event Detection (EED). This paper proposes a data- driven unsupervised learning method to handle this challenge. Specifically, the random matrix theories (RMTs) are introduced as the statistical foundations for random matrix models (RMMs); based on the RMMs, linear eigenvalue statistics (LESs) are defined via the test functions as the system indicators. By comparing the values of the LES between the experimental and the theoretical ones, the anomaly detection is conducted. Furthermore, we develop 3D power-map to visualize the LES; it provides a robust auxiliary decision-making mechanism to the operators. In this sense, the proposed method conducts EED with a pure statistical procedure, requiring no knowledge of system topologies, unit operation/control models, etc. The LES, as a key ingredient during this procedure, is a high dimensional indictor derived directly from raw data. As an unsupervised learning indicator, the LES is much more sensitive than the low dimensional indictors obtained from supervised learning. With the statistical procedure, the proposed method is universal and fast; moreover, it is robust against traditional EED challenges (such as error accumulations, spurious correlations, and even bad data in core area). Case studies, with both simulated data and real ones, validate the proposed method. To manage large-scale distributed systems, data fusion is mentioned as another data processing ingredient. version:2
arxiv-1509-04399 | Analyzing structural characteristics of object category representations from their semantic-part distributions | http://arxiv.org/abs/1509.04399 | id:1509.04399 author:Ravi Kiran Sarvadevabhatla, Venkatesh Babu R category:cs.CV  published:2015-09-15 summary:Studies from neuroscience show that part-mapping computations are employed by human visual system in the process of object recognition. In this work, we present an approach for analyzing semantic-part characteristics of object category representations. For our experiments, we use category-epitome, a recently proposed sketch-based spatial representation for objects. To enable part-importance analysis, we first obtain semantic-part annotations of hand-drawn sketches originally used to construct the corresponding epitomes. We then examine the extent to which the semantic-parts are present in the epitomes of a category and visualize the relative importance of parts as a word cloud. Finally, we show how such word cloud visualizations provide an intuitive understanding of category-level structural trends that exist in the category-epitome object representations. version:1
arxiv-1509-04397 | Exponential Family Matrix Completion under Structural Constraints | http://arxiv.org/abs/1509.04397 | id:1509.04397 author:Suriya Gunasekar, Pradeep Ravikumar, Joydeep Ghosh category:stat.ML cs.LG  published:2015-09-15 summary:We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low--rank, and the measurements consist of a subset, either of the exact individual entries, or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin--tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data--types, such as skewed--continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low--rank, such as block--sparsity, or a superposition structure of low--rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a matrix completion setting wherein the matrix entries are sampled from any member of the rich family of exponential family distributions; and impose general structural constraints on the underlying matrix, as captured by a general regularizer $\mathcal{R}(.)$. We propose a simple convex regularized $M$--estimator for the generalized framework, and provide a unified and novel statistical analysis for this general class of estimators. We finally corroborate our theoretical results on simulated datasets. version:1
arxiv-1509-04393 | Dependency length minimization: Puzzles and Promises | http://arxiv.org/abs/1509.04393 | id:1509.04393 author:Haitao Liu, Chunshan Xu, Junying Liang category:cs.CL  published:2015-09-15 summary:In the recent issue of PNAS, Futrell et al. claims that their study of 37 languages gives the first large scale cross-language evidence for Dependency Length Minimization, which is an overstatement that ignores similar previous researches. In addition,this study seems to pay no attention to factors like the uniformity of genres,which weakens the validity of the argument that DLM is universal. Another problem is that this study sets the baseline random language as projective, which fails to truly uncover the difference between natural language and random language, since projectivity is an important feature of many natural languages. Finally, the paper contends an "apparent relationship between head finality and dependency length" despite the lack of an explicit statistical comparison, which renders this conclusion rather hasty and improper. version:1
arxiv-1509-04376 | Precise Phase Transition of Total Variation Minimization | http://arxiv.org/abs/1509.04376 | id:1509.04376 author:Bingwen Zhang, Weiyu Xu, Jian-Feng Cai, Lifeng Lai category:cs.IT cs.LG math.IT math.OC stat.ML  published:2015-09-15 summary:Characterizing the phase transitions of convex optimizations in recovering structured signals or data is of central importance in compressed sensing, machine learning and statistics. The phase transitions of many convex optimization signal recovery methods such as $\ell_1$ minimization and nuclear norm minimization are well understood through recent years' research. However, rigorously characterizing the phase transition of total variation (TV) minimization in recovering sparse-gradient signal is still open. In this paper, we fully characterize the phase transition curve of the TV minimization. Our proof builds on Donoho, Johnstone and Montanari's conjectured phase transition curve for the TV approximate message passing algorithm (AMP), together with the linkage between the minmax Mean Square Error of a denoising problem and the high-dimensional convex geometry for TV minimization. version:1
arxiv-1505-02419 | Improved Relation Extraction with Feature-Rich Compositional Embedding Models | http://arxiv.org/abs/1505.02419 | id:1505.02419 author:Matthew R. Gormley, Mo Yu, Mark Dredze category:cs.CL cs.AI cs.LG  published:2015-05-10 summary:Compositional embedding models build a representation (or embedding) for a linguistic structure based on its component word embeddings. We propose a Feature-rich Compositional Embedding Model (FCM) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement. The key idea is to combine both (unlexicalized) hand-crafted features with learned word embeddings. The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition. We test the proposed model on two relation extraction tasks, and demonstrate that our model outperforms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task. The combination of our model and a log-linear classifier with hand-crafted features gives state-of-the-art results. version:3
arxiv-1509-04210 | Model Accuracy and Runtime Tradeoff in Distributed Deep Learning | http://arxiv.org/abs/1509.04210 | id:1509.04210 author:Suyog Gupta, Wei Zhang, Josh Milthorpe category:stat.ML cs.DC cs.LG cs.NE  published:2015-09-14 summary:This paper presents Rudra, a parameter server based distributed computing framework tuned for training large-scale deep neural networks. Using variants of the asynchronous stochastic gradient descent algorithm we study the impact of synchronization protocol, stale gradient updates, minibatch size, learning rates, and number of learners on runtime performance and model accuracy. We introduce a new learning rate modulation strategy to counter the effect of stale gradients and propose a new synchronization protocol that can effectively bound the staleness in gradients, improve runtime performance and achieve good model accuracy. Our empirical investigation reveals a principled approach for distributed training of neural networks: the mini-batch size per learner should be reduced as more learners are added to the system to preserve the model accuracy. We validate this approach using commonly-used image classification benchmarks: CIFAR10 and ImageNet. version:2
arxiv-1509-04355 | Towards Making High Dimensional Distance Metric Learning Practical | http://arxiv.org/abs/1509.04355 | id:1509.04355 author:Qi Qian, Rong Jin, Lijun Zhang, Shenghuo Zhu category:cs.LG  published:2015-09-15 summary:In this work, we study distance metric learning (DML) for high dimensional data. A typical approach for DML with high dimensional data is to perform the dimensionality reduction first before learning the distance metric. The main shortcoming of this approach is that it may result in a suboptimal solution due to the subspace removed by the dimensionality reduction method. In this work, we present a dual random projection frame for DML with high dimensional data that explicitly addresses the limitation of dimensionality reduction for DML. The key idea is to first project all the data points into a low dimensional space by random projection, and compute the dual variables using the projected vectors. It then reconstructs the distance metric in the original space using the estimated dual variables. The proposed method, on one hand, enjoys the light computation of random projection, and on the other hand, alleviates the limitation of most dimensionality reduction methods. We verify both empirically and theoretically the effectiveness of the proposed algorithm for high dimensional DML. version:1
arxiv-1509-04340 | Voted Kernel Regularization | http://arxiv.org/abs/1509.04340 | id:1509.04340 author:Corinna Cortes, Prasoon Goyal, Vitaly Kuznetsov, Mehryar Mohri category:cs.LG  published:2015-09-14 summary:This paper presents an algorithm, Voted Kernel Regularization , that provides the flexibility of using potentially very complex kernel functions such as predictors based on much higher-degree polynomial kernels, while benefitting from strong learning guarantees. The success of our algorithm arises from derived bounds that suggest a new regularization penalty in terms of the Rademacher complexities of the corresponding families of kernel maps. In a series of experiments we demonstrate the improved performance of our algorithm as compared to baselines. Furthermore, the algorithm enjoys several favorable properties. The optimization problem is convex, it allows for learning with non-PDS kernels, and the solutions are highly sparse, resulting in improved classification speed and memory requirements. version:1
arxiv-1412-7477 | A General Theory of Pathwise Coordinate Optimization | http://arxiv.org/abs/1412.7477 | id:1412.7477 author:Tuo Zhao, Han Liu, Tong Zhang category:stat.ML  published:2014-12-23 summary:The pathwise coordinate optimization is one of the most important computational frameworks for solving high dimensional convex and nonconvex sparse learning problems. It differs from the classical coordinate optimization algorithms in three salient features: warm start initialization, active set updating, and strong rule for coordinate preselection. These three features grant superior empirical performance, but also pose significant challenge to theoretical analysis. To tackle this long lasting problem, we develop a new theory showing that these three features play pivotal roles in guaranteeing the outstanding statistical and computational performance of the pathwise coordinate optimization framework. In particular, we analyze the existing methods for pathwise coordinate optimization and provide new theoretical insights into them. The obtained theory motivates the development of several modifications to improve the pathwise coordinate optimization framework, which guarantees linear convergence to a unique sparse local optimum with optimal statistical properties (e.g. minimax optimality and oracle properties). This is the first result establishing the computational and statistical guarantees of the pathwise coordinate optimization framework in high dimensions. Thorough numerical experiments are provided to back up our theory. version:3
arxiv-1509-04332 | Learning without Recall by Random Walks on Directed Graphs | http://arxiv.org/abs/1509.04332 | id:1509.04332 author:Mohammad Amin Rahimian, Shahin Shahrampour, Ali Jadbabaie category:cs.SY math.OC stat.ML  published:2015-09-14 summary:We consider a network of agents that aim to learn some unknown state of the world using private observations and exchange of beliefs. At each time, agents observe private signals generated based on the true unknown state. Each agent might not be able to distinguish the true state based only on her private observations. This occurs when some other states are observationally equivalent to the true state from the agent's perspective. To overcome this shortcoming, agents must communicate with each other to benefit from local observations. We propose a model where each agent selects one of her neighbors randomly at each time. Then, she refines her opinion using her private signal and the prior of that particular neighbor. The proposed rule can be thought of as a Bayesian agent who cannot recall the priors based on which other agents make inferences. This learning without recall approach preserves some aspects of the Bayesian inference while being computationally tractable. By establishing a correspondence with a random walk on the network graph, we prove that under the described protocol, agents learn the truth exponentially fast in the almost sure sense. The asymptotic rate is expressed as the sum of the relative entropies between the signal structures of every agent weighted by the stationary distribution of the random walk. version:1
arxiv-1411-1488 | Analyzing Tensor Power Method Dynamics in Overcomplete Regime | http://arxiv.org/abs/1411.1488 | id:1411.1488 author:Anima Anandkumar, Rong Ge, Majid Janzamin category:cs.LG stat.ML  published:2014-11-06 summary:We present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor CP rank is larger than the input dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in general. We consider the case where the tensor components are randomly drawn, and show that the simple power iteration recovers the components with bounded error under mild initialization conditions. We apply our analysis to unsupervised learning of latent variable models, such as multi-view mixture models and spherical Gaussian mixtures. Given the third order moment tensor, we learn the parameters using tensor power iterations. We prove it can correctly learn the model parameters when the number of hidden components $k$ is much larger than the data dimension $d$, up to $k = o(d^{1.5})$. We initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples. Our analysis significantly expands the class of latent variable models where spectral methods are applicable. Our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models. version:2
arxiv-1509-04238 | A Practioner's Guide to Evaluating Entity Resolution Results | http://arxiv.org/abs/1509.04238 | id:1509.04238 author:Matt Barnes category:cs.DB stat.ML  published:2015-09-14 summary:Entity resolution (ER) is the task of identifying records belonging to the same entity (e.g. individual, group) across one or multiple databases. Ironically, it has multiple names: deduplication and record linkage, among others. In this paper we survey metrics used to evaluate ER results in order to iteratively improve performance and guarantee sufficient quality prior to deployment. Some of these metrics are borrowed from multi-class classification and clustering domains, though some key differences exist differentiating entity resolution from general clustering. Menestrina et al. empirically showed rankings from these metrics often conflict with each other, thus our primary motivation for studying them. This paper provides practitioners the basic knowledge to begin evaluating their entity resolution results. version:1
arxiv-1509-04232 | gSLICr: SLIC superpixels at over 250Hz | http://arxiv.org/abs/1509.04232 | id:1509.04232 author:Carl Yuheng Ren, Victor Adrian Prisacariu, Ian D Reid category:cs.CV  published:2015-09-14 summary:We introduce a parallel GPU implementation of the Simple Linear Iterative Clustering (SLIC) superpixel segmentation. Using a single graphic card, our implementation achieves speedups of up to $83\times$ from the standard sequential implementation. Our implementation is fully compatible with the standard sequential implementation and the software is now available online and is open source. version:1
arxiv-1509-04219 | Twitter Sentiment Analysis | http://arxiv.org/abs/1509.04219 | id:1509.04219 author:Afroze Ibrahim Baqapuri category:cs.CL cs.IR cs.SI  published:2015-09-14 summary:This project addresses the problem of sentiment analysis in twitter; that is classifying tweets according to the sentiment expressed in them: positive, negative or neutral. Twitter is an online micro-blogging and social-networking platform which allows users to write short status updates of maximum length 140 characters. It is a rapidly expanding service with over 200 million registered users - out of which 100 million are active users and half of them log on twitter on a daily basis - generating nearly 250 million tweets per day. Due to this large amount of usage we hope to achieve a reflection of public sentiment by analysing the sentiments expressed in the tweets. Analysing the public sentiment is important for many applications such as firms trying to find out the response of their products in the market, predicting political elections and predicting socioeconomic phenomena like stock exchange. The aim of this project is to develop a functional classifier for accurate and automatic sentiment classification of an unknown tweet stream. version:1
arxiv-1601-03478 | Deep Learning Applied to Image and Text Matching | http://arxiv.org/abs/1601.03478 | id:1601.03478 author:Afroze Ibrahim Baqapuri category:cs.LG cs.CL cs.CV  published:2015-09-14 summary:The ability to describe images with natural language sentences is the hallmark for image and language understanding. Such a system has wide ranging applications such as annotating images and using natural sentences to search for images.In this project we focus on the task of bidirectional image retrieval: such asystem is capable of retrieving an image based on a sentence (image search) andretrieve sentence based on an image query (image annotation). We present asystem based on a global ranking objective function which uses a combinationof convolutional neural networks (CNN) and multi layer perceptrons (MLP).It takes a pair of image and sentence and processes them in different channels,finally embedding it into a common multimodal vector space. These embeddingsencode abstract semantic information about the two inputs and can be comparedusing traditional information retrieval approaches. For each such pair, the modelreturns a score which is interpretted as a similarity metric. If this score is high,the image and sentence are likely to convey similar meaning, and if the score is low then they are likely not to. The visual input is modeled via deep convolutional neural network. On theother hand we explore three models for the textual module. The first one isbag of words with an MLP. The second one uses n-grams (bigram, trigrams,and a combination of trigram & skip-grams) with an MLP. The third is morespecialized deep network specific for modeling variable length sequences (SSE).We report comparable performance to recent work in the field, even though ouroverall model is simpler. We also show that the training time choice of how wecan generate our negative samples has a significant impact on performance, and can be used to specialize the bi-directional system in one particular task. version:1
arxiv-1505-01596 | Learning to See by Moving | http://arxiv.org/abs/1505.01596 | id:1505.01596 author:Pulkit Agrawal, Joao Carreira, Jitendra Malik category:cs.CV cs.NE cs.RO  published:2015-05-07 summary:The dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it possible to learn useful features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigate if the awareness of egomotion can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We show that given the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on visual tasks of scene recognition, object recognition, visual odometry and keypoint matching. version:2
arxiv-1509-01549 | Giraffe: Using Deep Reinforcement Learning to Play Chess | http://arxiv.org/abs/1509.01549 | id:1509.01549 author:Matthew Lai category:cs.AI cs.LG cs.NE  published:2015-09-04 summary:This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess. version:2
arxiv-1509-04115 | Color-Phase Analysis for Sinusoidal Structured Light in Rapid Range Imaging | http://arxiv.org/abs/1509.04115 | id:1509.04115 author:Changsoo Je, Sang Wook Lee, Rae-Hong Park category:cs.CV cs.GR physics.optics I.2.10; I.4.8  published:2015-09-14 summary:Active range sensing using structured-light is the most accurate and reliable method for obtaining 3D information. However, most of the work has been limited to range sensing of static objects, and range sensing of dynamic (moving or deforming) objects has been investigated recently only by a few researchers. Sinusoidal structured-light is one of the well-known optical methods for 3D measurement. In this paper, we present a novel method for rapid high-resolution range imaging using color sinusoidal pattern. We consider the real-world problem of nonlinearity and color-band crosstalk in the color light projector and color camera, and present methods for accurate recovery of color-phase. For high-resolution ranging, we use high-frequency patterns and describe new unwrapping algorithms for reliable range recovery. The experimental results demonstrate the effectiveness of our methods. version:1
arxiv-1509-03977 | Optimization of anemia treatment in hemodialysis patients via reinforcement learning | http://arxiv.org/abs/1509.03977 | id:1509.03977 author:Pablo Escandell-Montero, Milena Chermisi, José M. Martínez-Martínez, Juan Gómez-Sanchis, Carlo Barbieri, Emilio Soria-Olivas, Flavio Mari, Joan Vila-Francés, Andrea Stopper, Emanuele Gatti, José D. Martín-Guerrero category:stat.ML cs.AI cs.LG  published:2015-09-14 summary:Objective: Anemia is a frequent comorbidity in hemodialysis patients that can be successfully treated by administering erythropoiesis-stimulating agents (ESAs). ESAs dosing is currently based on clinical protocols that often do not account for the high inter- and intra-individual variability in the patient's response. As a result, the hemoglobin level of some patients oscillates around the target range, which is associated with multiple risks and side-effects. This work proposes a methodology based on reinforcement learning (RL) to optimize ESA therapy. Methods: RL is a data-driven approach for solving sequential decision-making problems that are formulated as Markov decision processes (MDPs). Computing optimal drug administration strategies for chronic diseases is a sequential decision-making problem in which the goal is to find the best sequence of drug doses. MDPs are particularly suitable for modeling these problems due to their ability to capture the uncertainty associated with the outcome of the treatment and the stochastic nature of the underlying process. The RL algorithm employed in the proposed methodology is fitted Q iteration, which stands out for its ability to make an efficient use of data. Results: The experiments reported here are based on a computational model that describes the effect of ESAs on the hemoglobin level. The performance of the proposed method is evaluated and compared with the well-known Q-learning algorithm and with a standard protocol. Simulation results show that the performance of Q-learning is substantially lower than FQI and the protocol. Conclusion: Although prospective validation is required, promising results demonstrate the potential of RL to become an alternative to current protocols. version:1
arxiv-1509-03970 | Natural scene statistics mediate the perception of image complexity | http://arxiv.org/abs/1509.03970 | id:1509.03970 author:Nicolas Gauvrit, Fernando Soler-Toscano, Hector Zenil category:cs.AI cs.CV  published:2015-09-14 summary:Humans are sensitive to complexity and regularity in patterns. The subjective perception of pattern complexity is correlated to algorithmic (Kolmogorov-Chaitin) complexity as defined in computer science, but also to the frequency of naturally occurring patterns. However, the possible mediational role of natural frequencies in the perception of algorithmic complexity remains unclear. Here we reanalyze Hsu et al. (2010) through a mediational analysis, and complement their results in a new experiment. We conclude that human perception of complexity seems partly shaped by natural scenes statistics, thereby establishing a link between the perception of complexity and the effect of natural scene statistics. version:1
arxiv-1509-03956 | Learning to Divide and Conquer for Online Multi-Target Tracking | http://arxiv.org/abs/1509.03956 | id:1509.03956 author:Francesco Solera, Simone Calderara, Rita Cucchiara category:cs.CV  published:2015-09-14 summary:Online Multiple Target Tracking (MTT) is often addressed within the tracking-by-detection paradigm. Detections are previously extracted independently in each frame and then objects trajectories are built by maximizing specifically designed coherence functions. Nevertheless, ambiguities arise in presence of occlusions or detection errors. In this paper we claim that the ambiguities in tracking could be solved by a selective use of the features, by working with more reliable features if possible and exploiting a deeper representation of the target only if necessary. To this end, we propose an online divide and conquer tracker for static camera scenes, which partitions the assignment problem in local subproblems and solves them by selectively choosing and combining the best features. The complete framework is cast as a structural learning task that unifies these phases and learns tracker parameters from examples. Experiments on two different datasets highlights a significant improvement of tracking performances (MOTA +10%) over the state of the art. version:1
arxiv-1509-03946 | Parametric Maxflows for Structured Sparse Learning with Convex Relaxations of Submodular Functions | http://arxiv.org/abs/1509.03946 | id:1509.03946 author:Yoshinobu Kawahara, Yutaro Yamaguchi category:cs.LG cs.NA  published:2015-09-14 summary:The proximal problem for structured penalties obtained via convex relaxations of submodular functions is known to be equivalent to minimizing separable convex functions over the corresponding submodular polyhedra. In this paper, we reveal a comprehensive class of structured penalties for which penalties this problem can be solved via an efficiently solvable class of parametric maxflow optimization. We then show that the parametric maxflow algorithm proposed by Gallo et al. and its variants, which runs, in the worst-case, at the cost of only a constant factor of a single computation of the corresponding maxflow optimization, can be adapted to solve the proximal problems for those penalties. Several existing structured penalties satisfy these conditions; thus, regularized learning with these penalties is solvable quickly using the parametric maxflow algorithm. We also investigate the empirical runtime performance of the proposed framework. version:1
arxiv-1509-03942 | Geometry and dimensionality reduction of feature spaces in primary visual cortex | http://arxiv.org/abs/1509.03942 | id:1509.03942 author:Davide Barbieri category:q-bio.NC cs.CV math.GR  published:2015-09-14 summary:Some geometric properties of the wavelet analysis performed by visual neurons are discussed and compared with experimental data. In particular, several relationships between the cortical morphologies and the parametric dependencies of extracted features are formalized and considered from a harmonic analysis point of view. version:1
arxiv-1509-03936 | Learning Social Relation Traits from Face Images | http://arxiv.org/abs/1509.03936 | id:1509.03936 author:Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV cs.CY  published:2015-09-14 summary:Social relation defines the association, e.g, warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine-grained and high-level relation traits can be characterised and quantified from face images in the wild. To address this challenging problem we propose a deep model that learns a rich face representation to capture gender, expression, head pose, and age-related attributes, and then performs pairwise-face reasoning for relation prediction. To learn from heterogeneous attribute sources, we formulate a new network architecture with a bridging layer to leverage the inherent correspondences among these datasets. It can also cope with missing target attribute labels. Extensive experiments show that our approach is effective for fine-grained social relation learning in images and videos. version:1
arxiv-1509-03935 | Markov Boundary Discovery with Ridge Regularized Linear Models | http://arxiv.org/abs/1509.03935 | id:1509.03935 author:Eric V. Strobl, Shyam Visweswaran category:math.ST stat.ME stat.ML stat.TH  published:2015-09-14 summary:Ridge regularized linear models (RRLMs), such as ridge regression and the SVM, are a popular group of methods that are used in conjunction with coefficient hypothesis testing to discover explanatory variables with a significant multivariate association to a response. However, many investigators are reluctant to draw causal interpretations of the selected variables due to the incomplete knowledge of the capabilities of RRLMs in causal inference. Under reasonable assumptions, we show that a modified form of RRLMs can get very close to identifying a subset of the Markov boundary by providing a worst-case bound on the space of possible solutions. The results hold for any convex loss, even when the underlying functional relationship is nonlinear, and the solution is not unique. Our approach combines ideas in Markov boundary and sufficient dimension reduction theory. Experimental results show that the modified RRLMs are competitive against state-of-the-art algorithms in discovering part of the Markov boundary from gene expression data. version:1
arxiv-1411-2883 | A new estimate of mutual information based measure of dependence between two variables: properties and fast implementation | http://arxiv.org/abs/1411.2883 | id:1411.2883 author:Namita Jain, C. A. Murthy category:cs.IT cs.LG math.IT  published:2014-10-28 summary:This article proposes a new method to estimate an existing mutual information based dependence measure using histogram density estimates. Finding a suitable bin length for histogram is an open problem. We propose a new way of computing the bin length for histogram using a function of maximum separation between points. The chosen bin length leads to consistent density estimates for histogram method. The values of density thus obtained are used to calculate an estimate of an existing dependence measure. The proposed estimate is named as Mutual Information Based Dependence Index (MIDI). Some important properties of MIDI have also been stated. The performance of the proposed method has been compared to generally accepted measures like Distance Correlation (dcor), Maximal Information Coefficient (MINE) in terms of accuracy and computational complexity with the help of several artificial data sets with different amounts of noise. The proposed method is able to detect many types of relationships between variables, without making any assumption about the functional form of the relationship. The power statistics of proposed method illustrate their effectiveness in detecting non linear relationship. Thus, it is able to achieve generality without a high rate of false positive cases. MIDI is found to work better on a real life data set than competing methods. The proposed method is found to overcome some of the limitations which occur with dcor and MINE. Computationally, MIDI is found to be better than dcor and MINE, in terms of time and memory, making it suitable for large data sets. version:4
arxiv-1108-2401 | A More Powerful Two-Sample Test in High Dimensions using Random Projection | http://arxiv.org/abs/1108.2401 | id:1108.2401 author:Miles E. Lopes, Laurent J. Jacob, Martin J. Wainwright category:math.ST stat.ME stat.ML stat.TH  published:2011-08-11 summary:We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Specifically, we propose a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T^2 statistic. Working under a high-dimensional framework with (p,n) tending to infinity, we first derive an asymptotic power function for our test, and then provide sufficient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from synthetic data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure's false positive rate with comparisons on high-dimensional gene expression data involving the discrimination of different types of cancer. version:3
arxiv-1509-03891 | On Binary Classification with Single-Layer Convolutional Neural Networks | http://arxiv.org/abs/1509.03891 | id:1509.03891 author:Soroush Mehri category:cs.CV  published:2015-09-13 summary:Convolutional neural networks are becoming standard tools for solving object recognition and visual tasks. However, most of the design and implementation of these complex models are based on trail-and-error. In this report, the main focus is to consider some of the important factors in designing convolutional networks to perform better. Specifically, classification with wide single-layer networks with large kernels as a general framework is considered. Particularly, we will show that pre-training using unsupervised schemes is vital, reasonable regularization is beneficial and applying of strong regularizers like dropout could be devastating. Pool size is also could be as important as learning procedure itself. In addition, it has been presented that using such a simple and relatively fast model for classifying cats and dogs, performance is close to state-of-the-art achievable by a combination of SVM models on color and texture features. version:1
arxiv-1509-03870 | The USFD Spoken Language Translation System for IWSLT 2014 | http://arxiv.org/abs/1509.03870 | id:1509.03870 author:Raymond W. M. Ng, Mortaza Doulaty, Rama Doddipatla, Wilker Aziz, Kashif Shah, Oscar Saz, Madina Hasan, Ghada AlHarbi, Lucia Specia, Thomas Hain category:cs.CL  published:2015-09-13 summary:The University of Sheffield (USFD) participated in the International Workshop for Spoken Language Translation (IWSLT) in 2014. In this paper, we will introduce the USFD SLT system for IWSLT. Automatic speech recognition (ASR) is achieved by two multi-pass deep neural network systems with adaptation and rescoring techniques. Machine translation (MT) is achieved by a phrase-based system. The USFD primary system incorporates state-of-the-art ASR and MT techniques and gives a BLEU score of 23.45 and 14.75 on the English-to-French and English-to-German speech-to-text translation task with the IWSLT 2014 data. The USFD contrastive systems explore the integration of ASR and MT by using a quality estimation system to rescore the ASR outputs, optimising towards better translation. This gives a further 0.54 and 0.26 BLEU improvement respectively on the IWSLT 2012 and 2014 evaluation data. version:1
arxiv-1509-03844 | Vectors of Locally Aggregated Centers for Compact Video Representation | http://arxiv.org/abs/1509.03844 | id:1509.03844 author:Alhabib Abbas, Nikos Deligiannis, Yiannis Andreopoulos category:cs.MM cs.CV cs.IR  published:2015-09-13 summary:We propose a novel vector aggregation technique for compact video representation, with application in accurate similarity detection within large video datasets. The current state-of-the-art in visual search is formed by the vector of locally aggregated descriptors (VLAD) of Jegou et. al. VLAD generates compact video representations based on scale-invariant feature transform (SIFT) vectors (extracted per frame) and local feature centers computed over a training set. With the aim to increase robustness to visual distortions, we propose a new approach that operates at a coarser level in the feature representation. We create vectors of locally aggregated centers (VLAC) by first clustering SIFT features to obtain local feature centers (LFCs) and then encoding the latter with respect to given centers of local feature centers (CLFCs), extracted from a training set. The sum-of-differences between the LFCs and the CLFCs are aggregated to generate an extremely-compact video description used for accurate video segment similarity detection. Experimentation using a video dataset, comprising more than 1000 minutes of content from the Open Video Project, shows that VLAC obtains substantial gains in terms of mean Average Precision (mAP) against VLAD and the hyper-pooling method of Douze et. al., under the same compaction factor and the same set of distortions. version:1
arxiv-1503-04585 | Statistical Analysis of Loopy Belief Propagation in Random Fields | http://arxiv.org/abs/1503.04585 | id:1503.04585 author:Muneki Yasuda, Shun Kataoka, Kazuyuki Tanaka category:stat.ML cond-mat.dis-nn cs.CV  published:2015-03-16 summary:Loopy belief propagation (LBP), which is equivalent to the Bethe approximation in statistical mechanics, is a message-passing-type inference method that is widely used to analyze systems based on Markov random fields (MRFs). In this paper, we propose a message-passing-type method to analytically evaluate the quenched average of LBP in random fields by using the replica cluster variation method. The proposed analytical method is applicable to general pair-wise MRFs with random fields whose distributions differ from each other and can give the quenched averages of the Bethe free energies over random fields, which are consistent with numerical results. The order of its computational cost is equivalent to that of standard LBP. In the latter part of this paper, we describe the application of the proposed method to Bayesian image restoration, in which we observed that our theoretical results are in good agreement with the numerical results for natural images. version:3
arxiv-1402-6278 | Sample Complexity Bounds on Differentially Private Learning via Communication Complexity | http://arxiv.org/abs/1402.6278 | id:1402.6278 author:Vitaly Feldman, David Xiao category:cs.DS cs.CC cs.LG  published:2014-02-25 summary:In this work we analyze the sample complexity of classification by differentially private algorithms. Differential privacy is a strong and well-studied notion of privacy introduced by Dwork et al. (2006) that ensures that the output of an algorithm leaks little information about the data point provided by any of the participating individuals. Sample complexity of private PAC and agnostic learning was studied in a number of prior works starting with (Kasiviswanathan et al., 2008) but a number of basic questions still remain open, most notably whether learning with privacy requires more samples than learning without privacy. We show that the sample complexity of learning with (pure) differential privacy can be arbitrarily higher than the sample complexity of learning without the privacy constraint or the sample complexity of learning with approximate differential privacy. Our second contribution and the main tool is an equivalence between the sample complexity of (pure) differentially private learning of a concept class $C$ (or $SCDP(C)$) and the randomized one-way communication complexity of the evaluation problem for concepts from $C$. Using this equivalence we prove the following bounds: 1. $SCDP(C) = \Omega(LDim(C))$, where $LDim(C)$ is the Littlestone's (1987) dimension characterizing the number of mistakes in the online-mistake-bound learning model. Known bounds on $LDim(C)$ then imply that $SCDP(C)$ can be much higher than the VC-dimension of $C$. 2. For any $t$, there exists a class $C$ such that $LDim(C)=2$ but $SCDP(C) \geq t$. 3. For any $t$, there exists a class $C$ such that the sample complexity of (pure) $\alpha$-differentially private PAC learning is $\Omega(t/\alpha)$ but the sample complexity of the relaxed $(\alpha,\beta)$-differentially private PAC learning is $O(\log(1/\beta)/\alpha)$. This resolves an open problem of Beimel et al. (2013b). version:4
arxiv-1509-02587 | A Dual Fast and Slow Feature Interaction in Biologically Inspired Visual Recognition of Human Action | http://arxiv.org/abs/1509.02587 | id:1509.02587 author:Bardia Yousefi, C. K. Loo category:cs.CV  published:2015-09-09 summary:Computational neuroscience studies that have examined human visual system through functional magnetic resonance imaging (fMRI) have identified a model where the mammalian brain pursues two distinct pathways (for recognition of biological movement tasks). In the brain, dorsal stream analyzes the information of motion (optical flow), which is the fast features, and ventral stream (form pathway) analyzes form information (through active basis model based incremental slow feature analysis ) as slow features. The proposed approach suggests the motion perception of the human visual system composes of fast and slow feature interactions that identifies biological movements. Form features in the visual system biologically follows the application of active basis model with incremental slow feature analysis for the extraction of the slowest form features of human objects movements in the ventral stream. Applying incremental slow feature analysis provides an opportunity to use the action prototypes. To extract the slowest features episodic observation is required but the fast features updates the processing of motion information in every frames. Experimental results have shown promising accuracy for the proposed model and good performance with two datasets (KTH and Weizmann). version:2
arxiv-1308-0271 | Compositional Dictionaries for Domain Adaptive Face Recognition | http://arxiv.org/abs/1308.0271 | id:1308.0271 author:Qiang Qiu, Rama Chellappa category:cs.CV  published:2013-08-01 summary:We present a dictionary learning approach to compensate for the transformation of faces due to changes in view point, illumination, resolution, etc. The key idea of our approach is to force domain-invariant sparse coding, i.e., design a consistent sparse representation of the same face in different domains. In this way, classifiers trained on the sparse codes in the source domain consisting of frontal faces for example can be applied to the target domain (consisting of faces in different poses, illumination conditions, etc) without much loss in recognition accuracy. The approach is to first learn a domain base dictionary, and then describe each domain shift (identity, pose, illumination) using a sparse representation over the base dictionary. The dictionary adapted to each domain is expressed as sparse linear combinations of the base dictionary. In the context of face recognition, with the proposed compositional dictionary approach, a face image can be decomposed into sparse representations for a given subject, pose and illumination respectively. This approach has three advantages: first, the extracted sparse representation for a subject is consistent across domains and enables pose and illumination insensitive face recognition. Second, sparse representations for pose and illumination can subsequently be used to estimate the pose and illumination condition of a face image. Finally, by composing sparse representations for subject and the different domains, we can also perform pose alignment and illumination normalization. Extensive experiments using two public face datasets are presented to demonstrate the effectiveness of our approach for face recognition. version:2
arxiv-1508-07384 | Generalized Uniformly Optimal Methods for Nonlinear Programming | http://arxiv.org/abs/1508.07384 | id:1508.07384 author:Saeed Ghadimi, Guanghui Lan, Hongchao Zhang category:math.OC stat.ML  published:2015-08-29 summary:In this paper, we present a generic framework to extend existing uniformly optimal convex programming algorithms to solve more general nonlinear, possibly nonconvex, optimization problems. The basic idea is to incorporate a local search step (gradient descent or Quasi-Newton iteration) into these uniformly optimal convex programming methods, and then enforce a monotone decreasing property of the function values computed along the trajectory. Algorithms of these types will then achieve the best known complexity for nonconvex problems, and the optimal complexity for convex ones without requiring any problem parameters. As a consequence, we can have a unified treatment for a general class of nonlinear programming problems regardless of their convexity and smoothness level. In particular, we show that the accelerated gradient and level methods, both originally designed for solving convex optimization problems only, can be used for solving both convex and nonconvex problems uniformly. In a similar vein, we show that some well-studied techniques for nonlinear programming, e.g., Quasi-Newton iteration, can be embedded into optimal convex optimization algorithms to possibly further enhance their numerical performance. Our theoretical and algorithmic developments are complemented by some promising numerical results obtained for solving a few important nonconvex and nonlinear data analysis problems in the literature. version:2
arxiv-1509-03739 | Improving distant supervision using inference learning | http://arxiv.org/abs/1509.03739 | id:1509.03739 author:Roland Roller, Eneko Agirre, Aitor Soroa, Mark Stevenson category:cs.CL  published:2015-09-12 summary:Distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. version:1
arxiv-1508-07175 | Competitive and Penalized Clustering Auto-encoder | http://arxiv.org/abs/1508.07175 | id:1508.07175 author:Zihao Wang, Yiuming Cheung category:cs.LG  published:2015-08-28 summary:Auto-encoders (AE) has been widely applied in different fields of machine learning. However, as a deep model, there are a large amount of learnable parameters in the AE, which would cause over-fitting and slow learning speed in practice. Many researchers have been study the intrinsic structure of AE and showed different useful methods to regularize those parameters. In this paper, we present a novel regularization method based on a clustering algorithm which is able to classify the parameters into different groups. With this regularization, parameters in a given group have approximate equivalent values and over-fitting problem could be alleviated. Moreover, due to the competitive behavior of clustering algorithm, this model also overcomes some intrinsic problems of clustering algorithms like the determination of number of clusters. Experiments on handwritten digits recognition verify the effectiveness of our novel model. version:2
arxiv-1509-04385 | Kannada named entity recognition and classification (nerc) based on multinomial naïve bayes (mnb) classifier | http://arxiv.org/abs/1509.04385 | id:1509.04385 author:S. Amarappa, S. V. Sathyanarayana category:cs.CL  published:2015-09-12 summary:Named Entity Recognition and Classification (NERC) is a process of identification of proper nouns in the text and classification of those nouns into certain predefined categories like person name, location, organization, date, and time etc. NERC in Kannada is an essential and challenging task. The aim of this work is to develop a novel model for NERC, based on Multinomial Na\"ive Bayes (MNB) Classifier. The Methodology adopted in this paper is based on feature extraction of training corpus, by using term frequency, inverse document frequency and fitting them to a tf-idf-vectorizer. The paper discusses the various issues in developing the proposed model. The details of implementation and performance evaluation are discussed. The experiments are conducted on a training corpus of size 95,170 tokens and test corpus of 5,000 tokens. It is observed that the model works with Precision, Recall and F1-measure of 83%, 79% and 81% respectively. version:1
arxiv-1504-05229 | Poisson Matrix Recovery and Completion | http://arxiv.org/abs/1504.05229 | id:1504.05229 author:Yang Cao, Yao Xie category:cs.LG math.ST stat.ML stat.TH  published:2015-04-20 summary:We extend the theory of low-rank matrix recovery and completion to the case when Poisson observations for a linear combination or a subset of the entries of a matrix are available, which arises in various applications with count data. We consider the usual matrix recovery formulation through maximum likelihood with proper constraints on the matrix $M$ of size $d_1$-by-$d_2$, and establish theoretical upper and lower bounds on the recovery error. Our bounds for matrix completion are nearly optimal up to a factor on the order of $\mathcal{O}(\log(d_1 d_2))$. These bounds are obtained by combing techniques for compressed sensing for sparse vectors with Poisson noise and for analyzing low-rank matrices, as well as adapting the arguments used for one-bit matrix completion \cite{davenport20121} (although these two problems are different in nature) and the adaptation requires new techniques exploiting properties of the Poisson likelihood function and tackling the difficulties posed by the locally sub-Gaussian characteristic of the Poisson distribution. Our results highlight a few important distinctions of the Poisson case compared to the prior work including having to impose a minimum signal-to-noise requirement on each observed entry and a gap in the upper and lower bounds. We also develop a set of efficient iterative algorithms and demonstrate their good performance on synthetic examples and real data. version:2
arxiv-1506-08941 | Language Understanding for Text-based Games Using Deep Reinforcement Learning | http://arxiv.org/abs/1506.08941 | id:1506.08941 author:Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay category:cs.CL cs.AI  published:2015-06-30 summary:In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations. version:2
arxiv-1409-8185 | Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models | http://arxiv.org/abs/1409.8185 | id:1409.8185 author:Theodoros Tsiligkaridis, Keith W. Forsythe category:stat.ML cs.LG stat.ME  published:2014-09-29 summary:We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods. version:3
arxiv-1503-02398 | Learning Co-Sparse Analysis Operators with Separable Structures | http://arxiv.org/abs/1503.02398 | id:1503.02398 author:Matthias Seibert, Julian Wörmann, Rémi Gribonval, Martin Kleinsteuber category:cs.LG stat.ML  published:2015-03-09 summary:In the co-sparse analysis model a set of filters is applied to a signal out of the signal class of interest yielding sparse filter responses. As such, it may serve as a prior in inverse problems, or for structural analysis of signals that are known to belong to the signal class. The more the model is adapted to the class, the more reliable it is for these purposes. The task of learning such operators for a given class is therefore a crucial problem. In many applications, it is also required that the filter responses are obtained in a timely manner, which can be achieved by filters with a separable structure. Not only can operators of this sort be efficiently used for computing the filter responses, but they also have the advantage that less training samples are required to obtain a reliable estimate of the operator. The first contribution of this work is to give theoretical evidence for this claim by providing an upper bound for the sample complexity of the learning process. The second is a stochastic gradient descent (SGD) method designed to learn an analysis operator with separable structures, which includes a novel and efficient step size selection rule. Numerical experiments are provided that link the sample complexity to the convergence speed of the SGD algorithm. version:5
arxiv-1509-03602 | DeepSat - A Learning framework for Satellite Imagery | http://arxiv.org/abs/1509.03602 | id:1509.03602 author:Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert DiBiano, Manohar Karki, Ramakrishna Nemani category:cs.CV  published:2015-09-11 summary:Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled high-resolution dataset with multiple class labels. The contributions of this paper are twofold - (1) first, we present two new satellite datasets called SAT-4 and SAT-6, and (2) then, we propose a classification framework that extracts features from an input image, normalizes them and feeds the normalized feature vectors to a Deep Belief Network for classification. On the SAT-4 dataset, our best network produces a classification accuracy of 97.95% and outperforms three state-of-the-art object recognition algorithms, namely - Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by ~11%. On SAT-6, it produces a classification accuracy of 93.9% and outperforms the other algorithms by ~15%. Comparative studies with a Random Forest classifier show the advantage of an unsupervised learning approach over traditional supervised learning techniques. A statistical analysis based on Distribution Separability Criterion and Intrinsic Dimensionality Estimation substantiates the effectiveness of our approach in learning better representations for satellite imagery. version:1
arxiv-1509-01599 | Better Document-level Sentiment Analysis from RST Discourse Parsing | http://arxiv.org/abs/1509.01599 | id:1509.01599 author:Parminder Bhatia, Yangfeng Ji, Jacob Eisenstein category:cs.CL cs.AI  published:2015-09-04 summary:Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that reweighting discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classification-based methods. version:2
arxiv-1505-06973 | Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts | http://arxiv.org/abs/1505.06973 | id:1505.06973 author:Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guillaume Lavoué, Thomas Brox, Bjoern Andres category:cs.CV  published:2015-05-26 summary:Formulations of the Image Decomposition Problem as a Multicut Problem (MP) w.r.t. a superpixel graph have received considerable attention. In contrast, instances of the MP w.r.t. a pixel grid graph have received little attention, firstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph are hard to solve in practice, and, secondly, due to the lack of long-range terms in the objective function of the MP. We propose a generalization of the MP with long-range terms (LMP). We design and implement two efficient algorithms (primal feasible heuristics) for the MP and LMP which allow us to study instances of both problems w.r.t. the pixel grid graphs of the images in the BSDS-500 benchmark. The decompositions we obtain do not differ significantly from the state of the art, suggesting that the LMP is a competitive formulation of the Image Decomposition Problem. To demonstrate the generality of the LMP, we apply it also to the Mesh Decomposition Problem posed by the Princeton benchmark, obtaining state-of-the-art decompositions. version:2
arxiv-1509-03488 | Inferring and evaluating semantic classes of verbs signaling modality | http://arxiv.org/abs/1509.03488 | id:1509.03488 author:Judith Eckle-Kohler category:cs.CL  published:2015-09-11 summary:We infer semantic classes of verbs signaling modality from a purely syntactic classification of 637 German verbs by applying findings from linguistics about correspondences between verb meaning and syntax. Our extensive evaluation of the semantic classification is based on a linking to three other lexical resources at the word sense level: to the German wordnet GermaNet and to the English resources VerbNet and FrameNet. This way, we are able to perform a reproducible semantic characterization of the inferred German classes. We also perform a corpus-based evaluation revealing that the frequencies of the classes in corpora of different genres are significantly different. We will make the resulting bilingual resource of German-English semantically categorized verb classes publicly available. version:1
arxiv-1505-08098 | CURL: Co-trained Unsupervised Representation Learning for Image Classification | http://arxiv.org/abs/1505.08098 | id:1505.08098 author:Simone Bianco, Gianluigi Ciocca, Claudio Cusano category:cs.LG cs.CV stat.ML I.2.6  published:2015-05-29 summary:In this paper we propose a strategy for semi-supervised image classification that leverages unsupervised representation learning and co-training. The strategy, that is called CURL from Co-trained Unsupervised Representation Learning, iteratively builds two classifiers on two different views of the data. The two views correspond to different representations learned from both labeled and unlabeled data and differ in the fusion scheme used to combine the image features. To assess the performance of our proposal, we conducted several experiments on widely used data sets for scene and object recognition. We considered three scenarios (inductive, transductive and self-taught learning) that differ in the strategy followed to exploit the unlabeled data. As image features we considered a combination of GIST, PHOG, and LBP as well as features extracted from a Convolutional Neural Network. Moreover, two embodiments of CURL are investigated: one using Ensemble Projection as unsupervised representation learning coupled with Logistic Regression, and one based on LapSVM. The results show that CURL clearly outperforms other supervised and semi-supervised learning methods in the state of the art. version:2
arxiv-1502-03655 | Newton-based maximum likelihood estimation in nonlinear state space models | http://arxiv.org/abs/1502.03655 | id:1502.03655 author:Manon Kok, Johan Dahlin, Thomas B. Schön, Adrian Wills category:stat.CO stat.ML  published:2015-02-12 summary:Maximum likelihood (ML) estimation using Newton's method in nonlinear state space models (SSMs) is a challenging problem due to the analytical intractability of the log-likelihood and its gradient and Hessian. We estimate the gradient and Hessian using Fisher's identity in combination with a smoothing algorithm. We explore two approximations of the log-likelihood and of the solution of the smoothing problem. The first is a linearization approximation which is computationally cheap, but the accuracy typically varies between models. The second is a sampling approximation which is asymptotically valid for any SSM but is more computationally costly. We demonstrate our approach for ML parameter estimation on simulated data from two different SSMs with encouraging results. version:2
arxiv-1502-02536 | Nested Sequential Monte Carlo Methods | http://arxiv.org/abs/1502.02536 | id:1502.02536 author:Christian A. Naesseth, Fredrik Lindsten, Thomas B. Schön category:stat.CO stat.ME stat.ML  published:2015-02-09 summary:We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences of probability distributions, even where the random variables are high-dimensional. NSMC generalises the SMC framework by requiring only approximate, properly weighted, samples from the SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC can in itself be used to produce such properly weighted samples. Consequently, one NSMC sampler can be used to construct an efficient high-dimensional proposal distribution for another NSMC sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to consider complex and high-dimensional models using SMC. We show results that motivate the efficacy of our approach on several filtering problems with dimensions in the order of 100 to 1 000. version:3
arxiv-1509-03456 | OCR accuracy improvement on document images through a novel pre-processing approach | http://arxiv.org/abs/1509.03456 | id:1509.03456 author:Abdeslam El Harraj, Naoufal Raissouni category:cs.CV  published:2015-09-11 summary:Digital camera and mobile document image acquisition are new trends arising in the world of Optical Character Recognition and text detection. In some cases, such process integrates many distortions and produces poorly scanned text or text-photo images and natural images, leading to an unreliable OCR digitization. In this paper, we present a novel nonparametric and unsupervised method to compensate for undesirable document image distortions aiming to optimally improve OCR accuracy. Our approach relies on a very efficient stack of document image enhancing techniques to recover deformation of the entire document image. First, we propose a local brightness and contrast adjustment method to effectively handle lighting variations and the irregular distribution of image illumination. Second, we use an optimized greyscale conversion algorithm to transform our document image to greyscale level. Third, we sharpen the useful information in the resulting greyscale image using Un-sharp Masking method. Finally, an optimal global binarization approach is used to prepare the final document image to OCR recognition. The proposed approach can significantly improve text detection rate and optical character recognition accuracy. To demonstrate the efficiency of our approach, an exhaustive experimentation on a standard dataset is presented. version:1
arxiv-1509-03453 | A reliable order-statistics-based approximate nearest neighbor search algorithm | http://arxiv.org/abs/1509.03453 | id:1509.03453 author:Luisa Verdoliva, Davide Cozzolino, Giovanni Poggi category:cs.CV  published:2015-09-11 summary:We propose a new algorithm for fast approximate nearest neighbor search based on the properties of ordered vectors. Data vectors are classified based on the index and sign of their largest components, thereby partitioning the space in a number of cones centered in the origin. The query is itself classified, and the search starts from the selected cone and proceeds to neighboring ones. Overall, the proposed algorithm corresponds to locality sensitive hashing in the space of directions, with hashing based on the order of components. Thanks to the statistical features emerging through ordering, it deals very well with the challenging case of unstructured data, and is a valuable building block for more complex techniques dealing with structured data. Experiments on both simulated and real-world data prove the proposed algorithm to provide a state-of-the-art performance. version:1
arxiv-1509-03413 | Learning Sparse Feature Representations using Probabilistic Quadtrees and Deep Belief Nets | http://arxiv.org/abs/1509.03413 | id:1509.03413 author:Saikat Basu, Manohar Karki, Sangram Ganguly, Robert DiBiano, Supratik Mukhopadhyay, Ramakrishna Nemani category:cs.CV  published:2015-09-11 summary:Learning sparse feature representations is a useful instrument for solving an unsupervised learning problem. In this paper, we present three labeled handwritten digit datasets, collectively called n-MNIST. Then, we propose a novel framework for the classification of handwritten digits that learns sparse representations using probabilistic quadtrees and Deep Belief Nets. On the MNIST and n-MNIST datasets, our framework shows promising results and significantly outperforms traditional Deep Belief Networks. version:1
arxiv-1508-02473 | Order Selection of Autoregressive Processes using Bridge Criterion | http://arxiv.org/abs/1508.02473 | id:1508.02473 author:Jie Ding, Mohammad Noshad, Vahid Tarokh category:math.ST q-fin.EC stat.ML stat.TH  published:2015-08-11 summary:A new criterion is introduced for determining the order of an autoregressive model fit to time series data. The proposed technique is shown to give a consistent and asymptotically efficient order estimation. It has the benefits of the two well-known model selection techniques, the Akaike information criterion and the Bayesian information criterion. When the true order of the autoregression is relatively large compared with the sample size, the Akaike information criterion is known to be efficient, and the new criterion behaves in a similar manner. When the true order is finite and small compared with the sample size, the Bayesian information criterion is known to be consistent, and so is the new criterion. Thus the new criterion builds a bridge between the two classical criteria automatically. In practice, where the observed time series is given without any prior information about the autoregression, the proposed order selection criterion is more flexible and robust compared with classical approaches. Numerical results are presented demonstrating the robustness of the proposed technique when applied to various datasets. version:2
arxiv-1509-03381 | Learning the Number of Autoregressive Mixtures in Time Series Using the Gap Statistics | http://arxiv.org/abs/1509.03381 | id:1509.03381 author:Jie Ding, Mohammad Noshad, Vahid Tarokh category:stat.ML  published:2015-09-11 summary:Using a proper model to characterize a time series is crucial in making accurate predictions. In this work we use time-varying autoregressive process (TVAR) to describe non-stationary time series and model it as a mixture of multiple stable autoregressive (AR) processes. We introduce a new model selection technique based on Gap statistics to learn the appropriate number of AR filters needed to model a time series. We define a new distance measure between stable AR filters and draw a reference curve that is used to measure how much adding a new AR filter improves the performance of the model, and then choose the number of AR filters that has the maximum gap with the reference curve. To that end, we propose a new method in order to generate uniform random stable AR filters in root domain. Numerical results are provided demonstrating the performance of the proposed approach. version:1
arxiv-1509-00825 | A DEEP analysis of the META-DES framework for dynamic selection of ensemble of classifiers | http://arxiv.org/abs/1509.00825 | id:1509.00825 author:Rafael M. O. Cruz, Robert Sabourin, George D. C. Cavalcanti category:cs.LG stat.ML  published:2015-09-02 summary:Dynamic ensemble selection (DES) techniques work by estimating the level of competence of each classifier from a pool of classifiers. Only the most competent ones are selected to classify a given test sample. Hence, the key issue in DES is the criterion used to estimate the level of competence of the classifiers in predicting the label of a given test sample. In order to perform a more robust ensemble selection, we proposed the META-DES framework using meta-learning, where multiple criteria are encoded as meta-features and are passed down to a meta-classifier that is trained to estimate the competence level of a given classifier. In this technical report, we present a step-by-step analysis of each phase of the framework during training and test. We show how each set of meta-features is extracted as well as their impact on the estimation of the competence level of the base classifier. Moreover, an analysis of the impact of several factors in the system performance, such as the number of classifiers in the pool, the use of different linear base classifiers, as well as the size of the validation data. We show that using the dynamic selection of linear classifiers through the META-DES framework, we can solve complex non-linear classification problems where other combination techniques such as AdaBoost cannot. version:2
arxiv-1509-03371 | Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems | http://arxiv.org/abs/1509.03371 | id:1509.03371 author:Fabian Tschopp category:cs.CV cs.AI I.2.6; I.5.1  published:2015-09-11 summary:This work presents and analyzes three convolutional neural network (CNN) models for efficient pixelwise classification of images. When using convolutional neural networks to classify single pixels in patches of a whole image, a lot of redundant computations are carried out when using sliding window networks. This set of new architectures solve this issue by either removing redundant computations or using fully convolutional architectures that inherently predict many pixels at once. The implementations of the three models are accessible through a new utility on top of the Caffe library. The utility provides support for a wide range of image input and output formats, pre-processing parameters and methods to equalize the label histogram during training. The Caffe library has been extended by new layers and a new backend for availability on a wider range of hardware such as CPUs and GPUs through OpenCL. On AMD GPUs, speedups of $54\times$ (SK-Net), $437\times$ (U-Net) and $320\times$ (USK-Net) have been observed, taking the SK equivalent SW (sliding window) network as the baseline. The label throughput is up to one megapixel per second. The analyzed neural networks have distinctive characteristics that apply during training or processing, and not every data set is suitable to every architecture. The quality of the predictions is assessed on two neural tissue data sets, of which one is the ISBI 2012 challenge data set. Two different loss functions, Malis loss and Softmax loss, were used during training. The whole pipeline, consisting of models, interface and modified Caffe library, is available as Open Source software under the working title Project Greentea. version:1
arxiv-1509-03302 | Performance Bounds for Pairwise Entity Resolution | http://arxiv.org/abs/1509.03302 | id:1509.03302 author:Matt Barnes, Kyle Miller, Artur Dubrawski category:stat.ML cs.CY cs.DB cs.LG  published:2015-09-10 summary:One significant challenge to scaling entity resolution algorithms to massive datasets is understanding how performance changes after moving beyond the realm of small, manually labeled reference datasets. Unlike traditional machine learning tasks, when an entity resolution algorithm performs well on small hold-out datasets, there is no guarantee this performance holds on larger hold-out datasets. We prove simple bounding properties between the performance of a match function on a small validation set and the performance of a pairwise entity resolution algorithm on arbitrarily sized datasets. Thus, our approach enables optimization of pairwise entity resolution algorithms for large datasets, using a small set of labeled data. version:1
arxiv-1404-6473 | Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests | http://arxiv.org/abs/1404.6473 | id:1404.6473 author:Lucas Mentch, Giles Hooker category:stat.ML stat.AP stat.CO stat.ME  published:2014-04-25 summary:This work develops formal statistical inference procedures for machine learning ensemble methods. Ensemble methods based on bootstrapping, such as bagging and random forests, have improved the predictive accuracy of individual trees, but fail to provide a framework in which distributional results can be easily determined. Instead of aggregating full bootstrap samples, we consider predicting by averaging over trees built on subsamples of the training set and demonstrate that the resulting estimator takes the form of a U-statistic. As such, predictions for individual feature vectors are asymptotically normal, allowing for confidence intervals to accompany predictions. In practice, a subset of subsamples is used for computational speed; here our estimators take the form of incomplete U-statistics and equivalent results are derived. We further demonstrate that this setup provides a framework for testing the significance of features. Moreover, the internal estimation method we develop allows us to estimate the variance parameters and perform these inference procedures at no additional computational cost. Simulations and illustrations on a real dataset are provided. version:2
arxiv-1509-03257 | Rigid Multiview Varieties | http://arxiv.org/abs/1509.03257 | id:1509.03257 author:Michael Joswig, Joe Kileel, Bernd Sturmfels, André Wagner category:math.AG cs.CV math.AC 14M99  68T45  published:2015-09-10 summary:The multiview variety from computer vision is generalized to images by $n$ cameras of points linked by a distance constraint. The resulting five-dimensional variety lives in a product of $2n$ projective planes. We determine defining polynomial equations, and we explore generalizations of this variety to scenarios of interest in applications. version:1
arxiv-1506-00468 | Classifying Tweet Level Judgements of Rumours in Social Media | http://arxiv.org/abs/1506.00468 | id:1506.00468 author:Michal Lukasik, Trevor Cohn, Kalina Bontcheva category:cs.SI cs.CL cs.LG  published:2015-06-01 summary:Social media is a rich source of rumours and corresponding community reactions. Rumours reflect different characteristics, some shared and some individual. We formulate the problem of classifying tweet level judgements of rumours as a supervised learning task. Both supervised and unsupervised domain adaptation are considered, in which tweets from a rumour are classified on the basis of other annotated rumours. We demonstrate how multi-task learning helps achieve good results on rumours from the 2011 England riots. version:2
arxiv-1509-03248 | A deep matrix factorization method for learning attribute representations | http://arxiv.org/abs/1509.03248 | id:1509.03248 author:George Trigeorgis, Konstantinos Bousmalis, Stefanos Zafeiriou, Bjoern W. Schuller category:cs.CV cs.LG stat.ML  published:2015-09-10 summary:Semi-Non-negative Matrix Factorization is a technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original data matrix contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies can not interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We also present a semi-supervised version of the algorithm, named Deep WSF, that allows the use of (partial) prior information for each of the known attributes of a dataset, that allows the model to be used on datasets with mixed attribute knowledge. Finally, we show that our models are able to learn low-dimensional representations that are better suited for clustering, but also classification, outperforming Semi-Non-negative Matrix Factorization, but also other state-of-the-art methodologies variants. version:1
arxiv-1509-03242 | Gibbs Sampling Strategies for Semantic Perception of Streaming Video Data | http://arxiv.org/abs/1509.03242 | id:1509.03242 author:Yogesh Girdhar, Gregory Dudek category:cs.RO cs.LG  published:2015-09-10 summary:Topic modeling of streaming sensor data can be used for high level perception of the environment by a mobile robot. In this paper we compare various Gibbs sampling strategies for topic modeling of streaming spatiotemporal data, such as video captured by a mobile robot. Compared to previous work on online topic modeling, such as o-LDA and incremental LDA, we show that the proposed technique results in lower online and final perplexity, given the realtime constraints. version:1
arxiv-1407-5599 | Scalable Kernel Methods via Doubly Stochastic Gradients | http://arxiv.org/abs/1407.5599 | id:1407.5599 author:Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina Balcan, Le Song category:cs.LG stat.ML  published:2014-07-21 summary:The general perception is that kernel methods are not scalable, and neural nets are the methods of choice for nonlinear learning problems. Or have we simply not tried hard enough for kernel methods? Here we propose an approach that scales up kernel methods using a novel concept called "doubly stochastic functional gradients". Our approach relies on the fact that many kernel methods can be expressed as convex optimization problems, and we solve the problems by making two unbiased stochastic approximations to the functional gradient, one using random training points and another using random functions associated with the kernel, and then descending using this noisy functional gradient. We show that a function produced by this procedure after $t$ iterations converges to the optimal function in the reproducing kernel Hilbert space in rate $O(1/t)$, and achieves a generalization performance of $O(1/\sqrt{t})$. This doubly stochasticity also allows us to avoid keeping the support vectors and to implement the algorithm in a small memory footprint, which is linear in number of iterations and independent of data dimension. Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show that our method can achieve competitive performance to neural nets in datasets such as 8 million handwritten digits from MNIST, 2.3 million energy materials from MolecularSpace, and 1 million photos from ImageNet. version:4
arxiv-1506-03011 | Learning to Linearize Under Uncertainty | http://arxiv.org/abs/1506.03011 | id:1506.03011 author:Ross Goroshin, Michael Mathieu, Yann LeCun category:cs.CV  published:2015-06-09 summary:Training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture. version:2
arxiv-1509-03185 | Use it or Lose it: Selective Memory and Forgetting in a Perpetual Learning Machine | http://arxiv.org/abs/1509.03185 | id:1509.03185 author:Andrew J. R. Simpson category:cs.LG 68Txx  published:2015-09-10 summary:In a recent article we described a new type of deep neural network - a Perpetual Learning Machine (PLM) - which is capable of learning 'on the fly' like a brain by existing in a state of Perpetual Stochastic Gradient Descent (PSGD). Here, by simulating the process of practice, we demonstrate both selective memory and selective forgetting when we introduce statistical recall biases during PSGD. Frequently recalled memories are remembered, whilst memories recalled rarely are forgotten. This results in a 'use it or lose it' stimulus driven memory process that is similar to human memory. version:1
arxiv-1509-03150 | STC: A Simple to Complex Framework for Weakly-supervised Semantic Segmentation | http://arxiv.org/abs/1509.03150 | id:1509.03150 author:Yunchao Wei, Xiaodan Liang, Yunpeng Chen, Xiaohui Shen, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan category:cs.CV  published:2015-09-10 summary:Recently, significant improvement has been made on semantic object segmentation due to the development of deep convolutional neural networks (DCNNs). Training such a DCNN usually relies on a large number of images with pixel-level segmentation masks, and annotating these images is very costly in terms of both finance and human effort. In this paper, we propose a simple to complex (STC) framework in which only image-level annotations are utilized to learn DCNNs for semantic segmentation. Specifically, we first train an initial segmentation network called Initial-DCNN with the saliency maps of simple images (i.e., those with a single category of major object(s) and clean background). These saliency maps can be automatically obtained by existing bottom-up salient object detection techniques, where no supervision information is needed. Then, a better network called Enhanced-DCNN is learned with supervision from the predicted segmentation masks of simple images based on the Initial-DCNN as well as the image-level annotations. Finally, more pixel-level segmentation masks of complex images (two or more categories of objects with cluttered background), which are inferred by using Enhanced-DCNN and image-level annotations, are utilized as the supervision information to learn the Powerful-DCNN for semantic segmentation. Our method utilizes $40$K simple images from Flickr.com and 10K complex images from PASCAL VOC for step-wisely boosting the segmentation network. Extensive experimental results on PASCAL VOC 2012 segmentation benchmark demonstrate that the proposed STC framework outperforms the state-of-the-art algorithms for weakly-supervised semantic segmentation by a large margin (e.g., 10.6% over MIL-ILP-seg [1]). version:1
arxiv-1504-02644 | OneMax in Black-Box Models with Several Restrictions | http://arxiv.org/abs/1504.02644 | id:1504.02644 author:Carola Doerr, Johannes Lengler category:cs.NE cs.DS  published:2015-04-10 summary:Black-box complexity studies lower bounds for the efficiency of general-purpose black-box optimization algorithms such as evolutionary algorithms and other search heuristics. Different models exist, each one being designed to analyze a different aspect of typical heuristics such as the memory size or the variation operators in use. While most of the previous works focus on one particular such aspect, we consider in this work how the combination of several algorithmic restrictions influence the black-box complexity. Our testbed are so-called OneMax functions, a classical set of test functions that is intimately related to classic coin-weighing problems and to the board game Mastermind. We analyze in particular the combined memory-restricted ranking-based black-box complexity of OneMax for different memory sizes. While its isolated memory-restricted as well as its ranking-based black-box complexity for bit strings of length $n$ is only of order $n/\log n$, the combined model does not allow for algorithms being faster than linear in $n$, as can be seen by standard information-theoretic considerations. We show that this linear bound is indeed asymptotically tight. Similar results are obtained for other memory- and offspring-sizes. Our results also apply to the (Monte Carlo) complexity of OneMax in the recently introduced elitist model, in which only the best-so-far solution can be kept in the memory. Finally, we also provide improved lower bounds for the complexity of OneMax in the regarded models. Our result enlivens the quest for natural evolutionary algorithms optimizing OneMax in $o(n \log n)$ iterations. version:2
arxiv-1502-04168 | Nonparametric regression using needlet kernels for spherical data | http://arxiv.org/abs/1502.04168 | id:1502.04168 author:Shaobo Lin category:cs.LG stat.ML 68T05  62J02 F.2.2  published:2015-02-14 summary:Needlets have been recognized as state-of-the-art tools to tackle spherical data, due to their excellent localization properties in both spacial and frequency domains. This paper considers developing kernel methods associated with the needlet kernel for nonparametric regression problems whose predictor variables are defined on a sphere. Due to the localization property in the frequency domain, we prove that the regularization parameter of the kernel ridge regression associated with the needlet kernel can decrease arbitrarily fast. A natural consequence is that the regularization term for the kernel ridge regression is not necessary in the sense of rate optimality. Based on the excellent localization property in the spacial domain further, we also prove that all the $l^{q}$ $(01\leq q < \infty)$ kernel regularization estimates associated with the needlet kernel, including the kernel lasso estimate and the kernel bridge estimate, possess almost the same generalization capability for a large range of regularization parameters in the sense of rate optimality. This finding tentatively reveals that, if the needlet kernel is utilized, then the choice of $q$ might not have a strong impact in terms of the generalization capability in some modeling contexts. From this perspective, $q$ can be arbitrarily specified, or specified merely by other no generalization criteria like smoothness, computational complexity, sparsity, etc.. version:2
arxiv-1509-01899 | Integrate Document Ranking Information into Confidence Measure Calculation for Spoken Term Detection | http://arxiv.org/abs/1509.01899 | id:1509.01899 author:Quan Liu, Wu Guo, Zhen-Hua Ling category:cs.CL  published:2015-09-07 summary:This paper proposes an algorithm to improve the calculation of confidence measure for spoken term detection (STD). Given an input query term, the algorithm first calculates a measurement named document ranking weight for each document in the speech database to reflect its relevance with the query term by summing all the confidence measures of the hypothesized term occurrences in this document. The confidence measure of each term occurrence is then re-estimated through linear interpolation with the calculated document ranking weight to improve its reliability by integrating document-level information. Experiments are conducted on three standard STD tasks for Tamil, Vietnamese and English respectively. The experimental results all demonstrate that the proposed algorithm achieves consistent improvements over the state-of-the-art method for confidence measure calculation. Furthermore, this algorithm is still effective even if a high accuracy speech recognizer is not available, which makes it applicable for the languages with limited speech resources. version:2
arxiv-1409-2655 | Weighted Classification Cascades for Optimizing Discovery Significance in the HiggsML Challenge | http://arxiv.org/abs/1409.2655 | id:1409.2655 author:Lester Mackey, Jordan Bryan, Man Yue Mo category:stat.ML cs.LG  published:2014-09-09 summary:We introduce a minorization-maximization approach to optimizing common measures of discovery significance in high energy physics. The approach alternates between solving a weighted binary classification problem and updating class weights in a simple, closed-form manner. Moreover, an argument based on convex duality shows that an improvement in weighted classification error on any round yields a commensurate improvement in discovery significance. We complement our derivation with experimental results from the 2014 Higgs boson machine learning challenge. version:5
arxiv-1508-00413 | Identifying Emotion from Natural Walking | http://arxiv.org/abs/1508.00413 | id:1508.00413 author:Liqing Cui, Shun Li, Wan Zhang, Zhan Zhang, Tingshao Zhu category:cs.CV cs.HC  published:2015-08-03 summary:Emotion identification from gait aims to automatically determine persons affective state, it has attracted a great deal of interests and offered immense potential value in action tendency, health care, psychological detection and human-computer(robot) interaction.In this paper, we propose a new method of identifying emotion from natural walking, and analyze the relevance between the traits of walking and affective states. After obtaining the pure acceleration data of wrist and ankle, we set a moving average filter window with different sizes w, then extract 114 features including time-domain, frequency-domain, power and distribution features from each data slice, and run principal component analysis (PCA) to reduce dimension. In experiments, we train SVM, Decision Tree, multilayerperception, Random Tree and Random Forest classification models, and compare the classification accuracy on data of wrist and ankle with respect to different w. The performance of emotion identification on acceleration data of ankle is better than wrist.Comparing different classification models' results, SVM has best accuracy of identifying anger and happy could achieve 90:31% and 89:76% respectively, and identification ratio of anger-happy is 87:10%.The anger-neutral-happy classification reaches 85%-78%-78%.The results show that it is capable of identifying personal emotional states through the gait of walking. version:2
arxiv-1403-3610 | Making Risk Minimization Tolerant to Label Noise | http://arxiv.org/abs/1403.3610 | id:1403.3610 author:Aritra Ghosh, Naresh Manwani, P. S. Sastry category:cs.LG  published:2014-03-14 summary:In many applications, the training data, from which one needs to learn a classifier, is corrupted with label noise. Many standard algorithms such as SVM perform poorly in presence of label noise. In this paper we investigate the robustness of risk minimization to label noise. We prove a sufficient condition on a loss function for the risk minimization under that loss to be tolerant to uniform label noise. We show that the $0-1$ loss, sigmoid loss, ramp loss and probit loss satisfy this condition though none of the standard convex loss functions satisfy it. We also prove that, by choosing a sufficiently large value of a parameter in the loss function, the sigmoid loss, ramp loss and probit loss can be made tolerant to non-uniform label noise also if we can assume the classes to be separable under noise-free data distribution. Through extensive empirical studies, we show that risk minimization under the $0-1$ loss, the sigmoid loss and the ramp loss has much better robustness to label noise when compared to the SVM algorithm. version:2
arxiv-1509-03025 | Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees | http://arxiv.org/abs/1509.03025 | id:1509.03025 author:Yudong Chen, Martin J. Wainwright category:math.ST cs.LG stat.ML stat.TH  published:2015-09-10 summary:Optimization problems with rank constraints arise in many applications, including matrix regression, structured PCA, matrix completion and matrix decomposition problems. An attractive heuristic for solving such problems is to factorize the low-rank matrix, and to run projected gradient descent on the nonconvex factorized optimization problem. The goal of this problem is to provide a general theoretical framework for understanding when such methods work well, and to characterize the nature of the resulting fixed point. We provide a simple set of conditions under which projected gradient descent, when given a suitable initialization, converges geometrically to a statistically useful solution. Our results are applicable even when the initial solution is outside any region of local convexity, and even when the problem is globally concave. Working in a non-asymptotic framework, we show that our conditions are satisfied for a wide range of concrete models, including matrix regression, structured PCA, matrix completion with real and quantized observations, matrix decomposition, and graph clustering problems. Simulation results show excellent agreement with the theoretical predictions. version:1
arxiv-1509-03005 | Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies | http://arxiv.org/abs/1509.03005 | id:1509.03005 author:David Balduzzi, Muhammad Ghifary category:cs.LG cs.AI cs.NE stat.ML  published:2015-09-10 summary:This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation. The algorithm is based on two innovations. Firstly, we present a temporal-difference based method for learning the gradient of the value-function. Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively. We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark. GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm. version:1
arxiv-1505-05253 | Knowlege Graph Embedding by Flexible Translation | http://arxiv.org/abs/1505.05253 | id:1505.05253 author:Jun Feng, Mantong Zhou, Yu Hao, Minlie Huang, Xiaoyan Zhu category:cs.CL  published:2015-05-20 summary:Knowledge graph embedding refers to projecting entities and relations in knowledge graph into continuous vector spaces. State-of-the-art methods, such as TransE, TransH, and TransR build embeddings by treating relation as translation from head entity to tail entity. However, previous models can not deal with reflexive/one-to-many/many-to-one/many-to-many relations properly, or lack of scalability and efficiency. Thus, we propose a novel method, flexible translation, named TransF, to address the above issues. TransF regards relation as translation between head entity vector and tail entity vector with flexible magnitude. To evaluate the proposed model, we conduct link prediction and triple classification on benchmark datasets. Experimental results show that our method remarkably improve the performance compared with several state-of-the-art baselines. version:2
arxiv-1507-02407 | Planar Ultrametric Rounding for Image Segmentation | http://arxiv.org/abs/1507.02407 | id:1507.02407 author:Julian Yarkony, Charless C. Fowlkes category:cs.DS cs.CG cs.CV 68T45  published:2015-07-09 summary:We study the problem of hierarchical clustering on planar graphs. We formulate this in terms of an LP relaxation of ultrametric rounding. To solve this LP efficiently we introduce a dual cutting plane scheme that uses minimum cost perfect matching as a subroutine in order to efficiently explore the space of planar partitions. We apply our algorithm to the problem of hierarchical image segmentation. version:3
arxiv-1507-05532 | Clustering Tree-structured Data on Manifold | http://arxiv.org/abs/1507.05532 | id:1507.05532 author:Na Lu, Hongyu Miao category:cs.CV cs.LG 68T10  62H30  published:2015-07-20 summary:Tree-structured data usually contain both topological and geometrical information, and are necessarily considered on manifold instead of Euclidean space for appropriate data parameterization and analysis. In this study, we propose a novel tree-structured data parameterization, called Topology-Attribute matrix (T-A matrix), so the data clustering task can be conducted on matrix manifold. We incorporate the structure constraints embedded in data into the negative matrix factorization method to determine meta-trees from the T-A matrix, and the signature vector of each single tree can then be extracted by meta-tree decomposition. The meta-tree space turns out to be a cone space, in which we explore the distance metric and implement the clustering algorithm based on the concepts like Fr\'echet mean. Finally, the T-A matrix based clustering (TAMBAC) framework is evaluated and compared using both simulated data and real retinal images to illustrate its efficiency and accuracy. version:2
arxiv-1509-02487 | Optimizing Static and Adaptive Probing Schedules for Rapid Event Detection | http://arxiv.org/abs/1509.02487 | id:1509.02487 author:Ahmad Mahmoody, Evgenios M. Kornaropoulos, Eli Upfal category:cs.DS cs.LG  published:2015-09-08 summary:We formulate and study a fundamental search and detection problem, Schedule Optimization, motivated by a variety of real-world applications, ranging from monitoring content changes on the web, social networks, and user activities to detecting failure on large systems with many individual machines. We consider a large system consists of many nodes, where each node has its own rate of generating new events, or items. A monitoring application can probe a small number of nodes at each step, and our goal is to compute a probing schedule that minimizes the expected number of undiscovered items at the system, or equivalently, minimizes the expected time to discover a new item in the system. We study the Schedule Optimization problem both for deterministic and randomized memoryless algorithms. We provide lower bounds on the cost of an optimal schedule and construct close to optimal schedules with rigorous mathematical guarantees. Finally, we present an adaptive algorithm that starts with no prior information on the system and converges to the optimal memoryless algorithms by adapting to observed data. version:2
arxiv-1509-02636 | Proposal-free Network for Instance-level Object Segmentation | http://arxiv.org/abs/1509.02636 | id:1509.02636 author:Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Jianchao Yang, Liang Lin, Shuicheng Yan category:cs.CV  published:2015-09-09 summary:Instance-level object segmentation is an important yet under-explored task. The few existing studies are almost all based on region proposal methods to extract candidate segments and then utilize object classification to produce final results. Nonetheless, generating accurate region proposals itself is quite challenging. In this work, we propose a Proposal-Free Network (PFN ) to address the instance-level object segmentation problem, which outputs the instance numbers of different categories and the pixel-level information on 1) the coordinates of the instance bounding box each pixel belongs to, and 2) the confidences of different categories for each pixel, based on pixel-to-pixel deep convolutional neural network. All the outputs together, by using any off-the-shelf clustering method for simple post-processing, can naturally generate the ultimate instance-level object segmentation results. The whole PFN can be easily trained in an end-to-end way without the requirement of a proposal generation stage. Extensive evaluations on the challenging PASCAL VOC 2012 semantic segmentation benchmark demonstrate that the proposed PFN solution well beats the state-of-the-arts for instance-level object segmentation. In particular, the $AP^r$ over 20 classes at 0.5 IoU reaches 58.7% by PFN, significantly higher than 43.8% and 46.3% by the state-of-the-art algorithms, SDS [9] and [16], respectively. version:2
arxiv-1509-02970 | Dictionary Learning and Sparse Coding for Third-order Super-symmetric Tensors | http://arxiv.org/abs/1509.02970 | id:1509.02970 author:Piotr Koniusz, Anoop Cherian category:cs.CV  published:2015-09-09 summary:Super-symmetric tensors - a higher-order extension of scatter matrices - are becoming increasingly popular in machine learning and computer vision for modelling data statistics, co-occurrences, or even as visual descriptors. However, the size of these tensors are exponential in the data dimensionality, which is a significant concern. In this paper, we study third-order super-symmetric tensor descriptors in the context of dictionary learning and sparse coding. Our goal is to approximate these tensors as sparse conic combinations of atoms from a learned dictionary, where each atom is a symmetric positive semi-definite matrix. Apart from the significant benefits to tensor compression that this framework provides, our experiments demonstrate that the sparse coefficients produced by the scheme lead to better aggregation of high-dimensional data, and showcases superior performance on two common computer vision tasks compared to the state-of-the-art. version:1
arxiv-1509-02962 | Coarse-to-Fine Sequential Monte Carlo for Probabilistic Programs | http://arxiv.org/abs/1509.02962 | id:1509.02962 author:Andreas Stuhlmüller, Robert X. D. Hawkins, N. Siddharth, Noah D. Goodman category:cs.AI stat.ML  published:2015-09-09 summary:Many practical techniques for probabilistic inference require a sequence of distributions that interpolate between a tractable distribution and an intractable distribution of interest. Usually, the sequences used are simple, e.g., based on geometric averages between distributions. When models are expressed as probabilistic programs, the models themselves are highly structured objects that can be used to derive annealing sequences that are more sensitive to domain structure. We propose an algorithm for transforming probabilistic programs to coarse-to-fine programs which have the same marginal distribution as the original programs, but generate the data at increasing levels of detail, from coarse to fine. We apply this algorithm to an Ising model, its depth-from-disparity variation, and a factorial hidden Markov model. We show preliminary evidence that the use of coarse-to-fine models can make existing generic inference algorithms more efficient. version:1
arxiv-1509-02954 | Sensor Selection by Linear Programming | http://arxiv.org/abs/1509.02954 | id:1509.02954 author:Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama category:stat.ML cs.LG  published:2015-09-09 summary:We learn sensor trees from training data to minimize sensor acquisition costs during test time. Our system adaptively selects sensors at each stage if necessary to make a confident classification. We pose the problem as empirical risk minimization over the choice of trees and node decision rules. We decompose the problem, which is known to be intractable, into combinatorial (tree structures) and continuous parts (node decision rules) and propose to solve them separately. Using training data we greedily solve for the combinatorial tree structures and for the continuous part, which is a non-convex multilinear objective function, we derive convex surrogate loss functions that are piecewise linear. The resulting problem can be cast as a linear program and has the advantage of guaranteed convergence, global optimality, repeatability and computational efficiency. We show that our proposed approach outperforms the state-of-art on a number of benchmark datasets. version:1
arxiv-1409-0553 | Sampling-based Approximations with Quantitative Performance for the Probabilistic Reach-Avoid Problem over General Markov Processes | http://arxiv.org/abs/1409.0553 | id:1409.0553 author:Sofie Haesaert, Robert Babuska, Alessandro Abate category:cs.SY cs.LG  published:2014-09-01 summary:This article deals with stochastic processes endowed with the Markov (memoryless) property and evolving over general (uncountable) state spaces. The models further depend on a non-deterministic quantity in the form of a control input, which can be selected to affect the probabilistic dynamics. We address the computation of maximal reach-avoid specifications, together with the synthesis of the corresponding optimal controllers. The reach-avoid specification deals with assessing the likelihood that any finite-horizon trajectory of the model enters a given goal set, while avoiding a given set of undesired states. This article newly provides an approximate computational scheme for the reach-avoid specification based on the Fitted Value Iteration algorithm, which hinges on random sample extractions, and gives a-priori computable formal probabilistic bounds on the error made by the approximation algorithm: as such, the output of the numerical scheme is quantitatively assessed and thus meaningful for safety-critical applications. Furthermore, we provide tighter probabilistic error bounds that are sample-based. The overall computational scheme is put in relationship with alternative approximation algorithms in the literature, and finally its performance is practically assessed over a benchmark case study. version:2
arxiv-1509-02873 | Sélection de variables par le GLM-Lasso pour la prédiction du risque palustre | http://arxiv.org/abs/1509.02873 | id:1509.02873 author:Bienvenue Kouwayè, Noël Fonton, Fabrice Rossi category:stat.ML  published:2015-09-09 summary:In this study, we propose an automatic learning method for variables selection based on Lasso in epidemiology context. One of the aim of this approach is to overcome the pretreatment of experts in medicine and epidemiology on collected data. These pretreatment consist in recoding some variables and to choose some interactions based on expertise. The approach proposed uses all available explanatory variables without treatment and generate automatically all interactions between them. This lead to high dimension. We use Lasso, one of the robust methods of variable selection in high dimension. To avoid over fitting a two levels cross-validation is used. Because the target variable is account variable and the lasso estimators are biased, variables selected by lasso are debiased by a GLM and used to predict the distribution of the main vector of malaria which is Anopheles. Results show that only few climatic and environmental variables are the mains factors associated to the malaria risk exposure. version:1
arxiv-1509-02866 | Fast Second-Order Stochastic Backpropagation for Variational Inference | http://arxiv.org/abs/1509.02866 | id:1509.02866 author:Kai Fan, Ziteng Wang, Jeff Beck, James Kwok, Katherine Heller category:stat.ML  published:2015-09-09 summary:We propose a second-order (Hessian or Hessian-free) based optimization method for variational inference inspired by Gaussian backpropagation, and argue that quasi-Newton optimization can be developed as well. This is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity. As an illustrative example, we apply this approach to the problems of Bayesian logistic regression and variational auto-encoder (VAE). Additionally, we compute bounds on the estimator variance of intractable expectations for the family of Lipschitz continuous function. Our method is practical, scalable and model free. We demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates. version:1
arxiv-1509-02807 | Transfer learning approach for financial applications | http://arxiv.org/abs/1509.02807 | id:1509.02807 author:Cosmin Stamate, George D. Magoulas, Michael S. C. Thomas category:cs.NE  published:2015-09-09 summary:Artificial neural networks learn how to solve new problems through a computationally intense and time consuming process. One way to reduce the amount of time required is to inject preexisting knowledge into the network. To make use of past knowledge, we can take advantage of techniques that transfer the knowledge learned from one task, and reuse it on another (sometimes unrelated) task. In this paper we propose a novel selective breeding technique that extends the transfer learning with behavioural genetics approach proposed by Kohli, Magoulas and Thomas (2013), and evaluate its performance on financial data. Numerical evidence demonstrates the credibility of the new approach. We provide insights on the operation of transfer learning and highlight the benefits of using behavioural principles and selective breeding when tackling a set of diverse financial applications problems. version:1
arxiv-1509-02730 | Finite Dictionary Variants of the Diffusion KLMS Algorithm | http://arxiv.org/abs/1509.02730 | id:1509.02730 author:Rangeet Mitra, Vimal Bhatia category:cs.SY cs.DC cs.IT cs.LG math.IT  published:2015-09-09 summary:The diffusion based distributed learning approaches have been found to be a viable solution for learning over linearly separable datasets over a network. However, approaches till date are suitable for linearly separable datasets and need to be extended to scenarios in which we need to learn a non-linearity. In such scenarios, the recently proposed diffusion kernel least mean squares (KLMS) has been found to be performing better than diffusion least mean squares (LMS). The drawback of diffusion KLMS is that it requires infinite storage for observations (also called dictionary). This paper formulates the diffusion KLMS in a fixed budget setting such that the storage requirement is curtailed while maintaining appreciable performance in terms of convergence. Simulations have been carried out to validate the two newly proposed algorithms named as quantised diffusion KLMS (QDKLMS) and fixed budget diffusion KLMS (FBDKLMS) against KLMS, which indicate that both the proposed algorithms deliver better performance as compared to the KLMS while reducing the dictionary size storage requirement. version:1
arxiv-1505-05233 | Visual Understanding via Multi-Feature Shared Learning with Global Consistency | http://arxiv.org/abs/1505.05233 | id:1505.05233 author:Lei Zhang, David Zhang category:cs.CV cs.LG  published:2015-05-20 summary:Image/video data is usually represented with multiple visual features. Fusion of multi-source information for establishing the attributes has been widely recognized. Multi-feature visual recognition has recently received much attention in multimedia applications. This paper studies visual understanding via a newly proposed l_2-norm based multi-feature shared learning framework, which can simultaneously learn a global label matrix and multiple sub-classifiers with the labeled multi-feature data. Additionally, a group graph manifold regularizer composed of the Laplacian and Hessian graph is proposed for better preserving the manifold structure of each feature, such that the label prediction power is much improved through the semi-supervised learning with global label consistency. For convenience, we call the proposed approach Global-Label-Consistent Classifier (GLCC). The merits of the proposed method include: 1) the manifold structure information of each feature is exploited in learning, resulting in a more faithful classification owing to the global label consistency; 2) a group graph manifold regularizer based on the Laplacian and Hessian regularization is constructed; 3) an efficient alternative optimization method is introduced as a fast solver owing to the convex sub-problems. Experiments on several benchmark visual datasets for multimedia understanding, such as the 17-category Oxford Flower dataset, the challenging 101-category Caltech dataset, the YouTube & Consumer Videos dataset and the large-scale NUS-WIDE dataset, demonstrate that the proposed approach compares favorably with the state-of-the-art algorithms. An extensive experiment on the deep convolutional activation features also show the effectiveness of the proposed approach. The code is available on http://www.escience.cn/people/lei/index.html version:2
arxiv-1509-02649 | Shape Interaction Matrix Revisited and Robustified: Efficient Subspace Clustering with Corrupted and Incomplete Data | http://arxiv.org/abs/1509.02649 | id:1509.02649 author:Pan Ji, Mathieu Salzmann, Hongdong Li category:cs.CV  published:2015-09-09 summary:The Shape Interaction Matrix (SIM) is one of the earliest approaches to performing subspace clustering (i.e., separating points drawn from a union of subspaces). In this paper, we revisit the SIM and reveal its connections to several recent subspace clustering methods. Our analysis lets us derive a simple, yet effective algorithm to robustify the SIM and make it applicable to realistic scenarios where the data is corrupted by noise. We justify our method by intuitive examples and the matrix perturbation theory. We then show how this approach can be extended to handle missing data, thus yielding an efficient and general subspace clustering algorithm. We demonstrate the benefits of our approach over state-of-the-art subspace clustering methods on several challenging motion segmentation and face clustering problems, where the data includes corrupted and missing measurements. version:1
arxiv-1409-3257 | Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization | http://arxiv.org/abs/1409.3257 | id:1409.3257 author:Yuchen Zhang, Lin Xiao category:math.OC stat.ML  published:2014-09-10 summary:We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate (SPDC) method, which alternates between maximizing over a randomly chosen dual variable and minimizing over the primal variable. An extrapolation step on the primal variable is performed to obtain accelerated convergence rate. We also develop a mini-batch version of the SPDC method which facilitates parallel computing, and an extension with weighted sampling probabilities on the dual variables, which has a better complexity than uniform sampling on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods. version:2
arxiv-1404-5009 | Efficient Semidefinite Branch-and-Cut for MAP-MRF Inference | http://arxiv.org/abs/1404.5009 | id:1404.5009 author:Peng Wang, Chunhua Shen, Anton van den Hengel, Philip Torr category:cs.CV cs.LG cs.NA  published:2014-04-20 summary:We propose a Branch-and-Cut (B&C) method for solving general MAP-MRF inference problems. The core of our method is a very efficient bounding procedure, which combines scalable semidefinite programming (SDP) and a cutting-plane method for seeking violated constraints. In order to further speed up the computation, several strategies have been exploited, including model reduction, warm start and removal of inactive constraints. We analyze the performance of the proposed method under different settings, and demonstrate that our method either outperforms or performs on par with state-of-the-art approaches. Especially when the connectivities are dense or when the relative magnitudes of the unary costs are low, we achieve the best reported results. Experiments show that the proposed algorithm achieves better approximation than the state-of-the-art methods within a variety of time budgets on challenging non-submodular MAP-MRF inference problems. version:4
arxiv-1509-02604 | Asynchronous Distributed ADMM for Large-Scale Optimization- Part II: Linear Convergence Analysis and Numerical Performance | http://arxiv.org/abs/1509.02604 | id:1509.02604 author:Tsung-Hui Chang, Wei-Cheng Liao, Mingyi Hong, Xiangfeng Wang category:cs.DC cs.LG cs.SY  published:2015-09-09 summary:The alternating direction method of multipliers (ADMM) has been recognized as a versatile approach for solving modern large-scale machine learning and signal processing problems efficiently. When the data size and/or the problem dimension is large, a distributed version of ADMM can be used, which is capable of distributing the computation load and the data set to a network of computing nodes. Unfortunately, a direct synchronous implementation of such algorithm does not scale well with the problem size, as the algorithm speed is limited by the slowest computing nodes. To address this issue, in a companion paper, we have proposed an asynchronous distributed ADMM (AD-ADMM) and studied its worst-case convergence conditions. In this paper, we further the study by characterizing the conditions under which the AD-ADMM achieves linear convergence. Our conditions as well as the resulting linear rates reveal the impact that various algorithm parameters, network delay and network size have on the algorithm performance. To demonstrate the superior time efficiency of the proposed AD-ADMM, we test the AD-ADMM on a high-performance computer cluster by solving a large-scale logistic regression problem. version:1
arxiv-1509-01604 | A nonlinear aggregation type classifier | http://arxiv.org/abs/1509.01604 | id:1509.01604 author:Alejandro Cholaquidis, Ricardo Fraiman, Juan Kalemkerian, Pamela Llop category:math.ST stat.ML stat.TH  published:2015-09-04 summary:We introduce a nonlinear aggregation type classifier for functional data defined on a separable and complete metric space. The new rule is built up from a collection of $M$ arbitrary training classifiers. If the classifiers are consistent, then so is the aggregation rule. Moreover, asymptotically the aggregation rule behaves as well as the best of the $M$ classifiers. The results of a small simulation are reported both, for high dimensional and functional data, and a real data example is analyzed. version:2
arxiv-1505-05461 | Network driven sampling; a critical threshold for design effects | http://arxiv.org/abs/1505.05461 | id:1505.05461 author:Karl Rohe category:math.ST stat.ME stat.ML stat.TH  published:2015-05-20 summary:Web crawling, snowball sampling, and respondent-driven sampling (RDS) are three types of network driven sampling techniques that are popular when it is difficult to contact individuals in the population of interest. This paper studies network driven sampling as a Markov process on the social network that is indexed by a tree. Each node in this tree corresponds to an observation and each edge in the tree corresponds to a referral. Indexing with a tree, instead of a chain, allows for the sampled units to refer multiple future units into the sample. In survey sampling, the design effect characterizes the additional variance induced by a novel sampling strategy. If the design effect is DE, then constructing an estimator from the novel design makes the variance of the estimator DE times greater than it would be under a simple random sample. Under certain assumptions on the referral tree, the design effect of network driven sampling has a critical threshold that is a function of the referral rate $m$ and the clustering structure in the social network, represented by the second eigenvalue of the Markov transition matrix, $\lambda_2$. If $m < 1/\lambda_2^2$, then the design effect is finite (i.e. the standard estimator is $\sqrt{n}$-consistent). However, if $m > 1/\lambda_2^2$, then the design effect grows with $n$ (i.e. the standard estimator is no longer $\sqrt{n}$-consistent). Past the critical threshold, the estimator converges at the slower rate of $\log_m \lambda_2$. The Markov model allows for nodes to be resampled. Under certain conditions, the rate of resampling is not affected by the critical threshold, so long as $n = o(\sqrt{N})$, where $n$ is the sample size and $N$ is the population size. version:4
arxiv-1510-03765 | Nonlinear functional mapping of the human brain | http://arxiv.org/abs/1510.03765 | id:1510.03765 author:Nicholas Allgaier, Tobias Banaschewski, Gareth Barker, Arun L. W. Bokde, Josh C. Bongard, Uli Bromberg, Christian Büchel, Anna Cattrell, Patricia J. Conrod, Christopher M. Danforth, Sylvane Desrivières, Peter S. Dodds, Herta Flor, Vincent Frouin, Jürgen Gallinat, Penny Gowland, Andreas Heinz, Bernd Ittermann, Scott Mackey, Jean-Luc Martinot, Kevin Murphy, Frauke Nees, Dimitri Papadopoulos-Orfanos, Luise Poustka, Michael N. Smolka, Henrik Walter, Robert Whelan, Gunter Schumann, Hugh Garavan, IMAGEN Consortium category:q-bio.NC cs.NE  published:2015-09-08 summary:The field of neuroimaging has truly become data rich, and novel analytical methods capable of gleaning meaningful information from large stores of imaging data are in high demand. Those methods that might also be applicable on the level of individual subjects, and thus potentially useful clinically, are of special interest. In the present study, we introduce just such a method, called nonlinear functional mapping (NFM), and demonstrate its application in the analysis of resting state fMRI from a 242-subject subset of the IMAGEN project, a European study of adolescents that includes longitudinal phenotypic, behavioral, genetic, and neuroimaging data. NFM employs a computational technique inspired by biological evolution to discover and mathematically characterize interactions among ROI (regions of interest), without making linear or univariate assumptions. We show that statistics of the resulting interaction relationships comport with recent independent work, constituting a preliminary cross-validation. Furthermore, nonlinear terms are ubiquitous in the models generated by NFM, suggesting that some of the interactions characterized here are not discoverable by standard linear methods of analysis. We discuss one such nonlinear interaction in the context of a direct comparison with a procedure involving pairwise correlation, designed to be an analogous linear version of functional mapping. We find another such interaction that suggests a novel distinction in brain function between drinking and non-drinking adolescents: a tighter coupling of ROI associated with emotion, reward, and interoceptive processes such as thirst, among drinkers. Finally, we outline many improvements and extensions of the methodology to reduce computational expense, complement other analytical tools like graph-theoretic analysis, and allow for voxel level NFM to eliminate the necessity of ROI selection. version:1
arxiv-1509-02512 | DeepCough: A Deep Convolutional Neural Network in A Wearable Cough Detection System | http://arxiv.org/abs/1509.02512 | id:1509.02512 author:Justice Amoh, Kofi Odame category:cs.NE cs.LG  published:2015-09-08 summary:In this paper, we present a system that employs a wearable acoustic sensor and a deep convolutional neural network for detecting coughs. We evaluate the performance of our system on 14 healthy volunteers and compare it to that of other cough detection systems that have been reported in the literature. Experimental results show that our system achieves a classification sensitivity of 95.1% and a specificity of 99.5%. version:1
arxiv-1508-06916 | Nucleosome positioning: resources and tools online | http://arxiv.org/abs/1508.06916 | id:1508.06916 author:Vladimir B. Teif category:q-bio.GN physics.bio-ph q-bio.BM stat.ML  published:2015-08-27 summary:Nucleosome positioning is an important process required for proper genome packing and its accessibility to execute the genetic program in a cell-specific, timely manner. In the recent years hundreds of papers have been devoted to the bioinformatics, physics and biology of nucleosome positioning. The purpose of this review is to cover a practical aspect of this field, namely to provide a guide to the multitude of nucleosome positioning resources available online. These include almost 300 experimental datasets of genome-wide nucleosome occupancy profiles determined in different cell types and more than 40 computational tools for the analysis of experimental nucleosome positioning data and prediction of intrinsic nucleosome formation probabilities from the DNA sequence. A manually curated, up to date list of these resources will be maintained at http://generegulation.info. version:4
arxiv-1503-03621 | Designing A Composite Dictionary Adaptively From Joint Examples | http://arxiv.org/abs/1503.03621 | id:1503.03621 author:Zhangyang Wang, Yingzhen Yang, Jianchao Yang, Thomas S. Huang category:cs.CV  published:2015-03-12 summary:We study the complementary behaviors of external and internal examples in image restoration, and are motivated to formulate a composite dictionary design framework. The composite dictionary consists of the global part learned from external examples, and the sample-specific part learned from internal examples. The dictionary atoms in both parts are further adaptively weighted to emphasize their model statistics. Experiments demonstrate that the joint utilization of external and internal examples leads to substantial improvements, with successful applications in image denoising and super resolution. version:2
arxiv-1412-6056 | Unsupervised Learning of Spatiotemporally Coherent Metrics | http://arxiv.org/abs/1412.6056 | id:1412.6056 author:Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun category:cs.CV  published:2014-12-18 summary:Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric. version:6
arxiv-1509-02491 | Edge-enhancing Filters with Negative Weights | http://arxiv.org/abs/1509.02491 | id:1509.02491 author:Andrew Knyazev category:cs.CV cs.IT math.CO math.IT 68U10  05C85 I.4.3; I.4.6; I.5.4  published:2015-09-08 summary:In [DOI:10.1109/ICMEW.2014.6890711], a graph-based denoising is performed by projecting the noisy image to a lower dimensional Krylov subspace of the graph Laplacian, constructed using nonnegative weights determined by distances between image data corresponding to image pixels. We~extend the construction of the graph Laplacian to the case, where some graph weights can be negative. Removing the positivity constraint provides a more accurate inference of a graph model behind the data, and thus can improve quality of filters for graph-based signal processing, e.g., denoising, compared to the standard construction, without affecting the costs. version:1
arxiv-1509-02470 | Deep Attributes from Context-Aware Regional Neural Codes | http://arxiv.org/abs/1509.02470 | id:1509.02470 author:Jianwei Luo, Jianguo Li, Jun Wang, Zhiguo Jiang, Yurong Chen category:cs.CV cs.LG cs.NE  published:2015-09-08 summary:Recently, many researches employ middle-layer output of convolutional neural network models (CNN) as features for different visual recognition tasks. Although promising results have been achieved in some empirical studies, such type of representations still suffer from the well-known issue of semantic gap. This paper proposes so-called deep attribute framework to alleviate this issue from three aspects. First, we introduce object region proposals as intermedia to represent target images, and extract features from region proposals. Second, we study aggregating features from different CNN layers for all region proposals. The aggregation yields a holistic yet compact representation of input images. Results show that cross-region max-pooling of soft-max layer output outperform all other layers. As soft-max layer directly corresponds to semantic concepts, this representation is named "deep attributes". Third, we observe that only a small portion of generated regions by object proposals algorithm are correlated to classification target. Therefore, we introduce context-aware region refining algorithm to pick out contextual regions and build context-aware classifiers. We apply the proposed deep attributes framework for various vision tasks. Extensive experiments are conducted on standard benchmarks for three visual recognition tasks, i.e., image classification, fine-grained recognition and visual instance retrieval. Results show that deep attribute approaches achieve state-of-the-art results, and outperforms existing peer methods with a significant margin, even though some benchmarks have little overlap of concepts with the pre-trained CNN models. version:1
arxiv-1509-02468 | Accelerated graph-based spectral polynomial filters | http://arxiv.org/abs/1509.02468 | id:1509.02468 author:Andrew Knyazev, Alexander Malyshev category:cs.CV  published:2015-09-08 summary:Graph-based spectral denoising is a low-pass filtering using the eigendecomposition of the graph Laplacian matrix of a noisy signal. Polynomial filtering avoids costly computation of the eigendecomposition by projections onto suitable Krylov subspaces. Polynomial filters can be based, e.g., on the bilateral and guided filters. We propose constructing accelerated polynomial filters by running flexible Krylov subspace based linear and eigenvalue solvers such as the Block Locally Optimal Preconditioned Conjugate Gradient (LOBPCG) method. version:1
arxiv-1509-02459 | Evolving TSP heuristics using Multi Expression Programming | http://arxiv.org/abs/1509.02459 | id:1509.02459 author:Mihai Oltean, D. Dumitrescu category:cs.AI cs.NE  published:2015-09-08 summary:Multi Expression Programming (MEP) is an evolutionary technique that may be used for solving computationally difficult problems. MEP uses a linear solution representation. Each MEP individual is a string encoding complex expressions (computer programs). A MEP individual may encode multiple solutions of the current problem. In this paper MEP is used for evolving a Traveling Salesman Problem (TSP) heuristic for graphs satisfying triangle inequality. Evolved MEP heuristic is compared with Nearest Neighbor Heuristic (NN) and Minimum Spanning Tree Heuristic (MST) on some difficult problems in TSPLIB. For most of the considered problems the evolved MEP heuristic outperforms NN and MST. The obtained algorithm was tested against some problems in TSPLIB. The results emphasizes that evolved MEP heuristic is a powerful tool for solving difficult TSP instances. version:1
arxiv-1509-02458 | A Behavior Analysis-Based Game Bot Detection Approach Considering Various Play Styles | http://arxiv.org/abs/1509.02458 | id:1509.02458 author:Yeounoh Chung, Chang-yong Park, Noo-ri Kim, Hana Cho, Taebok Yoon, Hunjoo Lee, Jee-Hyong Lee category:cs.LG cs.AI  published:2015-09-08 summary:An approach for game bot detection in MMORPGs is proposed based on the analysis of game playing behavior. Since MMORPGs are large scale games, users can play in various ways. This variety in playing behavior makes it hard to detect game bots based on play behaviors. In order to cope with this problem, the proposed approach observes game playing behaviors of users and groups them by their behavioral similarities. Then, it develops a local bot detection model for each player group. Since the locally optimized models can more accurately detect game bots within each player group, the combination of those models brings about overall improvement. For a practical purpose of reducing the workloads of the game servers in service, the game data is collected at a low resolution in time. Behavioral features are selected and developed to accurately detect game bots with the low resolution data, considering common aspects of MMORPG playing. Through the experiment with the real data from a game currently in service, it is shown that the proposed local model approach yields more accurate results. version:1
arxiv-1509-02438 | A Variational Bayesian State-Space Approach to Online Passive-Aggressive Regression | http://arxiv.org/abs/1509.02438 | id:1509.02438 author:Arnold Salas, Stephen J. Roberts, Michael A. Osborne category:stat.ML  published:2015-09-08 summary:Online Passive-Aggressive (PA) learning is a class of online margin-based algorithms suitable for a wide range of real-time prediction tasks, including classification and regression. PA algorithms are formulated in terms of deterministic point-estimation problems governed by a set of user-defined hyperparameters: the approach fails to capture model/prediction uncertainty and makes their performance highly sensitive to hyperparameter configurations. In this paper, we introduce a novel PA learning framework for regression that overcomes the above limitations. We contribute a Bayesian state-space interpretation of PA regression, along with a novel online variational inference scheme, that not only produces probabilistic predictions, but also offers the benefit of automatic hyperparameter tuning. Experiments with various real-world data sets show that our approach performs significantly better than a more standard, linear Gaussian state-space model. version:1
arxiv-1509-02437 | Improved Twitter Sentiment Prediction through Cluster-then-Predict Model | http://arxiv.org/abs/1509.02437 | id:1509.02437 author:Rishabh Soni, K. James Mathai category:cs.IR cs.CL cs.LG cs.SI  published:2015-09-08 summary:Over the past decade humans have experienced exponential growth in the use of online resources, in particular social media and microblogging websites such as Facebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line, etc. Many companies have identified these resources as a rich mine of marketing knowledge. This knowledge provides valuable feedback which allows them to further develop the next generation of their product. In this paper, sentiment analysis of a product is performed by extracting tweets about that product and classifying the tweets showing it as positive and negative sentiment. The authors propose a hybrid approach which combines unsupervised learning in the form of K-means clustering to cluster the tweets and then performing supervised learning methods such as Decision Trees and Support Vector Machines for classification. version:1
arxiv-1509-02417 | Central Pattern Generators for the control of robotic systems | http://arxiv.org/abs/1509.02417 | id:1509.02417 author:Carlos Garcia-Saura category:cs.RO cs.NE  published:2015-09-08 summary:Bio-inspired control of motion is an active field of research with many applications in real world tasks. In the case of robotic systems that need to exhibit oscillatory behaviour (i.e. locomotion of snake-type or legged robots), Central Pattern Generators (CPGs) are among the most versatile solutions. These controllers are often based on loosely-coupled oscillators similar to those found in the neural circuits of many animal species, and can be more robust to uncertainty (i.e. external perturbations) than traditional control approaches. This project provides an overview of the state-of-the-art in the field of CPGs, and in particular their applications within robotic systems. The project also tackles the implementation of a CPG-based controller in a small 3D-printed hexapod. version:1
arxiv-1509-02412 | Unsupervised Domain Discovery using Latent Dirichlet Allocation for Acoustic Modelling in Speech Recognition | http://arxiv.org/abs/1509.02412 | id:1509.02412 author:Mortaza Doulaty, Oscar Saz, Thomas Hain category:cs.CL  published:2015-09-08 summary:Speech recognition systems are often highly domain dependent, a fact widely reported in the literature. However the concept of domain is complex and not bound to clear criteria. Hence it is often not evident if data should be considered to be out-of-domain. While both acoustic and language models can be domain specific, work in this paper concentrates on acoustic modelling. We present a novel method to perform unsupervised discovery of domains using Latent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is assumed to exist in the data, whereby each audio segment can be considered to be a weighted mixture of domain properties. The classification of audio segments into domains allows the creation of domain specific acoustic models for automatic speech recognition. Experiments are conducted on a dataset of diverse speech data covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech, with a joint training set of 60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to LDA based domains was shown to yield relative Word Error Rate (WER) improvements of up to 16% relative, compared to pooled training, and up to 10%, compared with models adapted with human-labelled prior domain knowledge. version:1
arxiv-1509-02409 | Data-selective Transfer Learning for Multi-Domain Speech Recognition | http://arxiv.org/abs/1509.02409 | id:1509.02409 author:Mortaza Doulaty, Oscar Saz, Thomas Hain category:cs.LG cs.CL cs.SD  published:2015-09-08 summary:Negative transfer in training of acoustic models for automatic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide-domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6--hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features. version:1
arxiv-1501-07492 | Weakly Supervised Learning for Salient Object Detection | http://arxiv.org/abs/1501.07492 | id:1501.07492 author:Huaizu Jiang category:cs.CV  published:2015-01-29 summary:Recent advances in supervised salient object detection has resulted in significant performance on benchmark datasets. Training such models, however, requires expensive pixel-wise annotations of salient objects. Moreover, many existing salient object detection models assume that at least one salient object exists in the input image. Such an assumption often leads to less appealing saliency maps on the background images, which contain no salient object at all. To avoid the requirement of expensive pixel-wise salient region annotations, in this paper, we study weakly supervised learning approaches for salient object detection. Given a set of background images and salient object images, we propose a solution toward jointly addressing the salient object existence and detection tasks. We adopt the latent SVM framework and formulate the two problems together in a single integrated objective function: saliency labels of superpixels are modeled as hidden variables and involved in a classification term conditioned to the salient object existence variable, which in turn depends on both global image and regional saliency features and saliency label assignment. Experimental results on benchmark datasets validate the effectiveness of our proposed approach. version:2
arxiv-1509-02357 | Empirical risk minimization is consistent with the mean absolute percentage error | http://arxiv.org/abs/1509.02357 | id:1509.02357 author:Arnaud De Myttenaere, Bénédicte Le Grand, Fabrice Rossi category:stat.ML  published:2015-09-08 summary:We study in this paper the consequences of using the Mean Absolute Percentage Error (MAPE) as a measure of quality for regression models. We show that finding the best model under the MAPE is equivalent to doing weighted Mean Absolute Error (MAE) regression. We also show that, under some asumptions, universal consistency of Empirical Risk Minimization remains possible using the MAPE. version:1
arxiv-1509-02348 | On the complexity of piecewise affine system identification | http://arxiv.org/abs/1509.02348 | id:1509.02348 author:Fabien Lauer category:stat.ML cs.CC  published:2015-09-08 summary:The paper provides results regarding the computational complexity of hybrid system identification. More precisely, we focus on the estimation of piecewise affine (PWA) maps from input-output data and analyze the complexity of computing a global minimizer of the error. Previous work showed that a global solution could be obtained for continuous PWA maps with a worst-case complexity exponential in the number of data. In this paper, we show how global optimality can be reached for a slightly more general class of possibly discontinuous PWA maps with a complexity only polynomial in the number of data, however with an exponential complexity with respect to the data dimension. This result is obtained via an analysis of the intrinsic classification subproblem of associating the data points to the different modes. In addition, we prove that the problem is NP-hard, and thus that the exponential complexity in the dimension is a natural expectation for any exact algorithm. version:1
arxiv-1509-02347 | Modelling time evolving interactions in networks through a non stationary extension of stochastic block models | http://arxiv.org/abs/1509.02347 | id:1509.02347 author:Marco Corneli, Pierre Latouche, Fabrice Rossi category:stat.ML  published:2015-09-08 summary:In this paper, we focus on the stochastic block model (SBM),a probabilistic tool describing interactions between nodes of a network using latent clusters. The SBM assumes that the networkhas a stationary structure, in which connections of time varying intensity are not taken into account. In other words, interactions between two groups are forced to have the same features during the whole observation time. To overcome this limitation,we propose a partition of the whole time horizon, in which interactions are observed, and develop a non stationary extension of the SBM,allowing to simultaneously cluster the nodes in a network along with fixed time intervals in which the interactions take place. The number of clusters (K for nodes, D for time intervals) as well as the class memberships are finallyobtained through maximizing the complete-data integrated likelihood by means of a greedy search approach. After showing that the model works properly with simulated data, we focus on a real data set. We thus consider the three days ACM Hypertext conference held in Turin,June 29th - July 1st 2009. Proximity interactions between attendees during the first day are modelled and an interestingclustering of the daily hours is finally obtained, with times of social gathering (e.g. coffee breaks) recovered by the approach. Applications to large networks are limited due to the computational complexity of the greedy search which is dominated bythe number $K\_{max}$ and $D\_{max}$ of clusters used in the initialization. Therefore,advanced clustering tools are considered to reduce the number of clusters expected in the data, making the greedy search applicable to large networks. version:1
arxiv-1509-02320 | HEp-2 Cell Classification: The Role of Gaussian Scale Space Theory as A Pre-processing Approach | http://arxiv.org/abs/1509.02320 | id:1509.02320 author:Xianbiao Qi, Guoying Zhao, Jie Chen, Matti Pietikäinen category:cs.CV  published:2015-09-08 summary:\textit{Indirect Immunofluorescence Imaging of Human Epithelial Type 2} (HEp-2) cells is an effective way to identify the presence of Anti-Nuclear Antibody (ANA). Most existing works on HEp-2 cell classification mainly focus on feature extraction, feature encoding and classifier design. Very few efforts have been devoted to study the importance of the pre-processing techniques. In this paper, we analyze the importance of the pre-processing, and investigate the role of Gaussian Scale Space (GSS) theory as a pre-processing approach for the HEp-2 cell classification task. We validate the GSS pre-processing under the Local Binary Pattern (LBP) and the Bag-of-Words (BoW) frameworks. Under the BoW framework, the introduced pre-processing approach, using only one Local Orientation Adaptive Descriptor (LOAD), achieved superior performance on the Executable Thematic on Pattern Recognition Techniques for Indirect Immunofluorescence (ET-PRT-IIF) image analysis. Our system, using only one feature, outperformed the winner of the ICPR 2014 contest that combined four types of features. Meanwhile, the proposed pre-processing method is not restricted to this work; it can be generalized to many existing works. version:1
arxiv-1509-02317 | Object Proposals for Text Extraction in the Wild | http://arxiv.org/abs/1509.02317 | id:1509.02317 author:Lluis Gomez, Dimosthenis Karatzas category:cs.CV  published:2015-09-08 summary:Object Proposals is a recent computer vision technique receiving increasing interest from the research community. Its main objective is to generate a relatively small set of bounding box proposals that are most likely to contain objects of interest. The use of Object Proposals techniques in the scene text understanding field is innovative. Motivated by the success of powerful while expensive techniques to recognize words in a holistic way, Object Proposals techniques emerge as an alternative to the traditional text detectors. In this paper we study to what extent the existing generic Object Proposals methods may be useful for scene text understanding. Also, we propose a new Object Proposals algorithm that is specifically designed for text and compare it with other generic methods in the state of the art. Experiments show that our proposal is superior in its ability of producing good quality word proposals in an efficient way. The source code of our method is made publicly available. version:1
arxiv-1506-02108 | Deeply Learning the Messages in Message Passing Inference | http://arxiv.org/abs/1506.02108 | id:1506.02108 author:Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel category:cs.CV cs.LG stat.ML  published:2015-06-06 summary:Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension for message estimation is the same as the number of classes, in contrast to the network output for general CNN potential functions in CRFs, which is exponential in the order of the potentials. Hence CNN message learning has fewer network parameters and is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation on the PASCAL VOC 2012 dataset. We achieve an intersection-over-union score of 73.4 on its test set, which is the best reported result for methods using the VOC training images alone. This impressive performance demonstrates the effectiveness and usefulness of our CNN message learning method. version:3
arxiv-1509-01771 | Sampled Weighted Min-Hashing for Large-Scale Topic Mining | http://arxiv.org/abs/1509.01771 | id:1509.01771 author:Gibran Fuentes-Pineda, Ivan Vladimir Meza-Ruiz category:cs.LG cs.CL cs.IR  published:2015-09-06 summary:We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to automatically mine topics from large-scale corpora. SWMH generates multiple random partitions of the corpus vocabulary based on term co-occurrence and agglomerates highly overlapping inter-partition cells to produce the mined topics. While other approaches define a topic as a probabilistic distribution over a vocabulary, SWMH topics are ordered subsets of such vocabulary. Interestingly, the topics mined by SWMH underlie themes from the corpus at different levels of granularity. We extensively evaluate the meaningfulness of the mined topics both qualitatively and quantitatively on the NIPS (1.7 K documents), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora. Additionally, we compare the quality of SWMH with Online LDA topics for document representation in classification. version:2
arxiv-1507-06682 | Supervised Collective Classification for Crowdsourcing | http://arxiv.org/abs/1507.06682 | id:1507.06682 author:Pin-Yu Chen, Chia-Wei Lien, Fu-Jen Chu, Pai-Shun Ting, Shin-Ming Cheng category:cs.SI cs.LG stat.ML  published:2015-07-23 summary:Crowdsourcing utilizes the wisdom of crowds for collective classification via information (e.g., labels of an item) provided by labelers. Current crowdsourcing algorithms are mainly unsupervised methods that are unaware of the quality of crowdsourced data. In this paper, we propose a supervised collective classification algorithm that aims to identify reliable labelers from the training data (e.g., items with known labels). The reliability (i.e., weighting factor) of each labeler is determined via a saddle point algorithm. The results on several crowdsourced data show that supervised methods can achieve better classification accuracy than unsupervised methods, and our proposed method outperforms other algorithms. version:2
arxiv-1412-5732 | Dynamic Structure Embedded Online Multiple-Output Regression for Stream Data | http://arxiv.org/abs/1412.5732 | id:1412.5732 author:Changsheng Li, Fan Wei, Weishan Dong, Qingshan Liu, Xiangfeng Wang, Xin Zhang category:cs.LG  published:2014-12-18 summary:Online multiple-output regression is an important machine learning technique for modeling, predicting, and compressing multi-dimensional correlated data streams. In this paper, we propose a novel online multiple-output regression method, called MORES, for stream data. MORES can \emph{dynamically} learn the structure of the coefficients change in each update step to facilitate the model's continuous refinement. We observe that limited expressive ability of the regression model, especially in the preliminary stage of online update, often leads to the variables in the residual errors being dependent. In light of this point, MORES intends to \emph{dynamically} learn and leverage the structure of the residual errors to improve the prediction accuracy. Moreover, we define three statistical variables to \emph{exactly} represent all the seen samples for \emph{incrementally} calculating prediction loss in each online update round, which can avoid loading all the training data into memory for updating model, and also effectively prevent drastic fluctuation of the model in the presence of noise. Furthermore, we introduce a forgetting factor to set different weights on samples so as to track the data streams' evolving characteristics quickly from the latest samples. Experiments on one synthetic dataset and three real-world datasets validate the effectiveness of the proposed method. In addition, the update speed of MORES is at least 2000 samples processed per second on the three real-world datasets, more than 15 times faster than the state-of-the-art online learning algorithm. version:2
arxiv-1509-02217 | Enhancing Automatically Discovered Multi-level Acoustic Patterns Considering Context Consistency With Applications in Spoken Term Detection | http://arxiv.org/abs/1509.02217 | id:1509.02217 author:Cheng-Tao Chung, Wei-Ning Hsu, Cheng-Yi Lee, Lin-Shan Lee category:cs.CL  published:2015-09-07 summary:This paper presents a novel approach for enhancing the multiple sets of acoustic patterns automatically discovered from a given corpus. In a previous work it was proposed that different HMM configurations (number of states per model, number of distinct models) for the acoustic patterns form a two-dimensional space. Multiple sets of acoustic patterns automatically discovered with the HMM configurations properly located on different points over this two-dimensional space were shown to be complementary to one another, jointly capturing the characteristics of the given corpus. By representing the given corpus as sequences of acoustic patterns on different HMM sets, the pattern indices in these sequences can be relabeled considering the context consistency across the different sequences. Good improvements were observed in preliminary experiments of pattern spoken term detection (STD) performed on both TIMIT and Mandarin Broadcast News with such enhanced patterns. version:1
arxiv-1509-02216 | Fuzzy Jets | http://arxiv.org/abs/1509.02216 | id:1509.02216 author:Lester Mackey, Benjamin Nachman, Ariel Schwartzman, Conrad Stansbury category:hep-ph stat.ML  published:2015-09-07 summary:Collimated streams of particles produced in high energy physics experiments are organized using clustering algorithms to form jets. To construct jets, the experimental collaborations based at the Large Hadron Collider (LHC) primarily use agglomerative hierarchical clustering schemes known as sequential recombination. We propose a new class of algorithms for clustering jets that use infrared and collinear safe mixture models. These new algorithms, known as fuzzy jets, are clustered using maximum likelihood techniques and can dynamically determine various properties of jets like their size. We show that the fuzzy jet size adds additional information to conventional jet tagging variables. Furthermore, we study the impact of pileup and show that with some slight modifications to the algorithm, fuzzy jets can be stable up to high pileup interaction multiplicities. version:1
arxiv-1509-02213 | Unsupervised Spoken Term Detection with Spoken Queries by Multi-level Acoustic Patterns with Varying Model Granularity | http://arxiv.org/abs/1509.02213 | id:1509.02213 author:Cheng-Tao Chung, Chun-an Chan, Lin-shan Lee category:cs.CL  published:2015-09-07 summary:This paper presents a new approach for unsupervised Spoken Term Detection with spoken queries using multiple sets of acoustic patterns automatically discovered from the target corpus. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a three-dimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to one another, thus can jointly capture the characteristics of the spoken terms. By representing the spoken content and spoken query as sequences of acoustic patterns, a series of approaches for matching the pattern index sequences while considering the signal variations are developed. In this way, not only the on-line computation load can be reduced, but the signal distributions caused by different speakers and acoustic conditions can be reasonably taken care of. The results indicate that this approach significantly outperformed the unsupervised feature-based DTW baseline by 16.16\% in mean average precision on the TIMIT corpus. version:1
arxiv-1509-02208 | Unsupervised Discovery of Linguistic Structure Including Two-level Acoustic Patterns Using Three Cascaded Stages of Iterative Optimization | http://arxiv.org/abs/1509.02208 | id:1509.02208 author:Cheng-Tao Chung, Chun-an Chan, Lin-shan Lee category:cs.CL  published:2015-09-07 summary:Techniques for unsupervised discovery of acoustic patterns are getting increasingly attractive, because huge quantities of speech data are becoming available but manual annotations remain hard to acquire. In this paper, we propose an approach for unsupervised discovery of linguistic structure for the target spoken language given raw speech data. This linguistic structure includes two-level (subword-like and word-like) acoustic patterns, the lexicon of word-like patterns in terms of subword-like patterns and the N-gram language model based on word-like patterns. All patterns, models, and parameters can be automatically learned from the unlabelled speech corpus. This is achieved by an initialization step followed by three cascaded stages for acoustic, linguistic, and lexical iterative optimization. The lexicon of word-like patterns defines allowed consecutive sequence of HMMs for subword-like patterns. In each iteration, model training and decoding produces updated labels from which the lexicon and HMMs can be further updated. In this way, model parameters and decoded labels are respectively optimized in each iteration, and the knowledge about the linguistic structure is learned gradually layer after layer. The proposed approach was tested in preliminary experiments on a corpus of Mandarin broadcast news, including a task of spoken term detection with performance compared to a parallel test using models trained in a supervised way. Results show that the proposed system not only yields reasonable performance on its own, but is also complimentary to existing large vocabulary ASR systems. version:1
arxiv-1403-3724 | VESICLE: Volumetric Evaluation of Synaptic Interfaces using Computer vision at Large Scale | http://arxiv.org/abs/1403.3724 | id:1403.3724 author:William Gray Roncal, Michael Pekala, Verena Kaynig-Fittkau, Dean M. Kleissas, Joshua T. Vogelstein, Hanspeter Pfister, Randal Burns, R. Jacob Vogelstein, Mark A. Chevillet, Gregory D. Hager category:cs.CV cs.CE q-bio.QM  published:2014-03-14 summary:An open challenge problem at the forefront of modern neuroscience is to obtain a comprehensive mapping of the neural pathways that underlie human brain function; an enhanced understanding of the wiring diagram of the brain promises to lead to new breakthroughs in diagnosing and treating neurological disorders. Inferring brain structure from image data, such as that obtained via electron microscopy (EM), entails solving the problem of identifying biological structures in large data volumes. Synapses, which are a key communication structure in the brain, are particularly difficult to detect due to their small size and limited contrast. Prior work in automated synapse detection has relied upon time-intensive biological preparations (post-staining, isotropic slice thicknesses) in order to simplify the problem. This paper presents VESICLE, the first known approach designed for mammalian synapse detection in anisotropic, non-post-stained data. Our methods explicitly leverage biological context, and the results exceed existing synapse detection methods in terms of accuracy and scalability. We provide two different approaches - one a deep learning classifier (VESICLE-CNN) and one a lightweight Random Forest approach (VESICLE-RF) to offer alternatives in the performance-scalability space. Addressing this synapse detection challenge enables the analysis of high-throughput imaging data soon expected to reach petabytes of data, and provide tools for more rapid estimation of brain-graphs. Finally, to facilitate community efforts, we developed tools for large-scale object detection, and demonstrated this framework to find $\approx$ 50,000 synapses in 60,000 $\mu m ^3$ (220 GB on disk) of electron microscopy data. version:4
arxiv-1505-02108 | MegaFace: A Million Faces for Recognition at Scale | http://arxiv.org/abs/1505.02108 | id:1505.02108 author:D. Miller, E. Brossard, S. Seitz, I. Kemelmacher-Shlizerman category:cs.CV  published:2015-05-08 summary:Recent face recognition experiments on the LFW benchmark show that face recognition is performing stunningly well, surpassing human recognition rates. In this paper, we study face recognition at scale. Specifically, we have collected from Flickr a \textbf{Million} faces and evaluated state of the art face recognition algorithms on this dataset. We found that the performance of algorithms varies--while all perform great on LFW, once evaluated at scale recognition rates drop drastically for most algorithms. Interestingly, deep learning based approach by \cite{schroff2015facenet} performs much better, but still gets less robust at scale. We consider both verification and identification problems, and evaluate how pose affects recognition at scale. Moreover, we ran an extensive human study on Mechanical Turk to evaluate human recognition at scale, and report results. All the photos are creative commons photos and is released at \small{\url{http://megaface.cs.washington.edu/}} for research and further experiments. version:2
arxiv-1509-02130 | Structured Prediction with Output Embeddings for Semantic Image Annotation | http://arxiv.org/abs/1509.02130 | id:1509.02130 author:Ariadna Quattoni, Arnau Ramisa, Pranava Swaroop Madhyastha, Edgar Simo-Serra, Francesc Moreno-Noguer category:cs.CV  published:2015-09-07 summary:We address the task of annotating images with semantic tuples. Solving this problem requires an algorithm which is able to deal with hundreds of classes for each argument of the tuple. In such contexts, data sparsity becomes a key challenge, as there will be a large number of classes for which only a few examples are available. We propose handling this by incorporating feature representations of both the inputs (images) and outputs (argument classes) into a factorized log-linear model, and exploiting the flexibility of scoring functions based on bilinear forms. Experiments show that integrating feature representations of the outputs in the structured prediction model leads to better overall predictions. We also conclude that the best output representation is specific for each type of argument. version:1
arxiv-1509-02122 | Convexity Shape Constraints for Image Segmentation | http://arxiv.org/abs/1509.02122 | id:1509.02122 author:Loic A. Royer, David L. Richmond, Carsten Rother, Bjoern Andres, Dagmar Kainmueller category:cs.CV  published:2015-09-07 summary:Segmenting an image into multiple components is a central task in computer vision. In many practical scenarios, prior knowledge about plausible components is available. Incorporating such prior knowledge into models and algorithms for image segmentation is highly desirable, yet can be non-trivial. In this work, we introduce a new approach that allows, for the first time, to constrain some or all components of a segmentation to have convex shapes. Specifically, we extend the Minimum Cost Multicut Problem by a class of constraints that enforce convexity. To solve instances of this APX-hard integer linear program to optimality, we separate the proposed constraints in the branch-and-cut loop of a state-of-the-art ILP solver. Results on natural and biological images demonstrate the effectiveness of the approach as well as its advantage over the state-of-the-art heuristic. version:1
arxiv-1509-02094 | Future Localization from an Egocentric Depth Image | http://arxiv.org/abs/1509.02094 | id:1509.02094 author:Hyun Soo Park, Yedong Niu, Jianbo Shi category:cs.CV  published:2015-09-07 summary:This paper presents a method for future localization: to predict a set of plausible trajectories of ego-motion given a depth image. We predict paths avoiding obstacles, between objects, even paths turning around a corner into space behind objects. As a byproduct of the predicted trajectories of ego-motion, we discover in the image the empty space occluded by foreground objects. We use no image based features such as semantic labeling/segmentation or object detection/recognition for this algorithm. Inspired by proxemics, we represent the space around a person using an EgoSpace map, akin to an illustrated tourist map, that measures a likelihood of occlusion at the egocentric coordinate system. A future trajectory of ego-motion is modeled by a linear combination of compact trajectory bases allowing us to constrain the predicted trajectory. We learn the relationship between the EgoSpace map and trajectory from the EgoMotion dataset providing in-situ measurements of the future trajectory. A cost function that takes into account partial occlusion due to foreground objects is minimized to predict a trajectory. This cost function generates a trajectory that passes through the occluded space, which allows us to discover the empty space behind the foreground objects. We quantitatively evaluate our method to show predictive validity and apply to various real world scenes including walking, shopping, and social interactions. version:1
arxiv-1509-02088 | Matrix Factorisation with Linear Filters | http://arxiv.org/abs/1509.02088 | id:1509.02088 author:Ömer Deniz Akyıldız category:stat.ML  published:2015-09-07 summary:This text investigates relations between two well-known family of algorithms, matrix factorisations and recursive linear filters, by describing a probabilistic model in which approximate inference corresponds to a matrix factorisation algorithm. Using the probabilistic model, we derive a matrix factorisation algorithm as a recursive linear filter. More precisely, we derive a matrix-variate recursive linear filter in order to perform efficient inference in high dimensions. We also show that it is possible to interpret our algorithm as a nontrivial stochastic gradient algorithm. Demonstrations and comparisons on an image restoration task are given. version:1
arxiv-1412-0436 | An Infra-Structure for Performance Estimation and Experimental Comparison of Predictive Models in R | http://arxiv.org/abs/1412.0436 | id:1412.0436 author:Luis Torgo category:cs.MS cs.LG cs.SE stat.CO  published:2014-12-01 summary:This document describes an infra-structure provided by the R package performanceEstimation that allows to estimate the predictive performance of different approaches (workflows) to predictive tasks. The infra-structure is generic in the sense that it can be used to estimate the values of any performance metrics, for any workflow on different predictive tasks, namely, classification, regression and time series tasks. The package also includes several standard workflows that allow users to easily set up their experiments limiting the amount of work and information they need to provide. The overall goal of the infra-structure provided by our package is to facilitate the task of estimating the predictive performance of different modeling approaches to predictive tasks in the R environment. version:4
arxiv-1509-02027 | A New Low-Rank Tensor Model for Video Completion | http://arxiv.org/abs/1509.02027 | id:1509.02027 author:Wenrui Hu, Dacheng Tao, Wensheng Zhang, Yuan Xie, Yehui Yang category:cs.CV  published:2015-09-07 summary:In this paper, we propose a new low-rank tensor model based on the circulant algebra, namely, twist tensor nuclear norm or t-TNN for short. The twist tensor denotes a 3-way tensor representation to laterally store 2D data slices in order. On one hand, t-TNN convexly relaxes the tensor multi-rank of the twist tensor in the Fourier domain, which allows an efficient computation using FFT. On the other, t-TNN is equal to the nuclear norm of block circulant matricization of the twist tensor in the original domain, which extends the traditional matrix nuclear norm in a block circulant way. We test the t-TNN model on a video completion application that aims to fill missing values and the experiment results validate its effectiveness, especially when dealing with video recorded by a non-stationary panning camera. The block circulant matricization of the twist tensor can be transformed into a circulant block representation with nuclear norm invariance. This representation, after transformation, exploits the horizontal translation relationship between the frames in a video, and endows the t-TNN model with a more powerful ability to reconstruct panning videos than the existing state-of-the-art low-rank models. version:1
arxiv-1409-5705 | Distributed Machine Learning via Sufficient Factor Broadcasting | http://arxiv.org/abs/1409.5705 | id:1409.5705 author:Pengtao Xie, Jin Kyu Kim, Yi Zhou, Qirong Ho, Abhimanu Kumar, Yaoliang Yu, Eric Xing category:cs.LG cs.DC  published:2014-09-19 summary:Matrix-parametrized models, including multiclass logistic regression and sparse coding, are used in machine learning (ML) applications ranging from computer vision to computational biology. When these models are applied to large-scale ML problems starting at millions of samples and tens of thousands of classes, their parameter matrix can grow at an unexpected rate, resulting in high parameter synchronization costs that greatly slow down distributed learning. To address this issue, we propose a Sufficient Factor Broadcasting (SFB) computation model for efficient distributed learning of a large family of matrix-parameterized models, which share the following property: the parameter update computed on each data sample is a rank-1 matrix, i.e., the outer product of two "sufficient factors" (SFs). By broadcasting the SFs among worker machines and reconstructing the update matrices locally at each worker, SFB improves communication efficiency --- communication costs are linear in the parameter matrix's dimensions, rather than quadratic --- without affecting computational correctness. We present a theoretical convergence analysis of SFB, and empirically corroborate its efficiency on four different matrix-parametrized ML models. version:2
arxiv-1509-01978 | An Approach to the Analysis of the South Slavic Medieval Labels Using Image Texture | http://arxiv.org/abs/1509.01978 | id:1509.01978 author:Darko Brodic, Alessia Amelio, Zoran N. Milivojevic category:cs.CV cs.AI cs.CL I.4; I.2.7  published:2015-09-07 summary:The paper presents a new script classification method for the discrimination of the South Slavic medieval labels. It consists in the textural analysis of the script types. In the first step, each letter is coded by the equivalent script type, which is defined by its typographical features. Obtained coded text is subjected to the run-length statistical analysis and to the adjacent local binary pattern analysis in order to extract the features. The result shows a diversity between the extracted features of the scripts, which makes the feature classification more effective. It is the basis for the classification process of the script identification by using an extension of a state-of-the-art approach for document clustering. The proposed method is evaluated on an example of hand-engraved in stone and hand-printed in paper labels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate very positive results, which prove the effectiveness of the proposed method. version:1
arxiv-1509-01957 | Automated Analysis of Behavioural Variability and Filial Imprinting of Chicks (G. gallus), using Autonomous Robots | http://arxiv.org/abs/1509.01957 | id:1509.01957 author:A. Gribovskiy, F. Mondada, J. L. Deneubourg, L. Cazenille, N. Bredeche, J. Halloy category:q-bio.QM cs.LG cs.RO physics.bio-ph  published:2015-09-07 summary:Inter-individual variability has various impacts in animal social behaviour. This implies that not only collective behaviours have to be studied but also the behavioural variability of each member composing the groups. To understand those effects on group behaviour, we develop a quantitative methodology based on automated ethograms and autonomous robots to study the inter-individual variability among social animals. We choose chicks of \textit{Gallus gallus domesticus} as a classic social animal model system for their suitability in laboratory and controlled experimentation. Moreover, even domesticated chicken present social structures implying forms or leadership and filial imprinting. We develop an imprinting methodology on autonomous robots to study individual and social behaviour of free moving animals. This allows to quantify the behaviours of large number of animals. We develop an automated experimental methodology that allows to make relatively fast controlled experiments and efficient data analysis. Our analysis are based on high-throughput data allowing a fine quantification of individual behavioural traits. We quantify the efficiency of various state-of-the-art algorithms to automate data analysis and produce automated ethograms. We show that the use of robots allows to provide controlled and quantified stimuli to the animals in absence of human intervention. We quantify the individual behaviour of 205 chicks obtained from hatching after synchronized fecundation. Our results show a high variability of individual behaviours and of imprinting quality and success. Three classes of chicks are observed with various level of imprinting. Our study shows that the concomitant use of autonomous robots and automated ethograms allows detailed and quantitative analysis of behavioural patterns of animals in controlled laboratory experiments. version:1
arxiv-1509-01951 | Hierarchical Deep Learning Architecture For 10K Objects Classification | http://arxiv.org/abs/1509.01951 | id:1509.01951 author:Atul Laxman Katole, Krishna Prasad Yellapragada, Amish Kumar Bedi, Sehaj Singh Kalra, Mynepalli Siva Chaitanya category:cs.CV cs.LG cs.NE  published:2015-09-07 summary:Evolution of visual object recognition architectures based on Convolutional Neural Networks & Convolutional Deep Belief Networks paradigms has revolutionized artificial Vision Science. These architectures extract & learn the real world hierarchical visual features utilizing supervised & unsupervised learning approaches respectively. Both the approaches yet cannot scale up realistically to provide recognition for a very large number of objects as high as 10K. We propose a two level hierarchical deep learning architecture inspired by divide & conquer principle that decomposes the large scale recognition architecture into root & leaf level model architectures. Each of the root & leaf level models is trained exclusively to provide superior results than possible by any 1-level deep learning architecture prevalent today. The proposed architecture classifies objects in two steps. In the first step the root level model classifies the object in a high level category. In the second step, the leaf level recognition model for the recognized high level category is selected among all the leaf models. This leaf level model is presented with the same input object image which classifies it in a specific category. Also we propose a blend of leaf level models trained with either supervised or unsupervised learning approaches. Unsupervised learning is suitable whenever labelled data is scarce for the specific leaf level models. Currently the training of leaf level models is in progress; where we have trained 25 out of the total 47 leaf level models as of now. We have trained the leaf models with the best case top-5 error rate of 3.2% on the validation data set for the particular leaf models. Also we demonstrate that the validation error of the leaf level models saturates towards the above mentioned accuracy as the number of epochs are increased to more than sixty. version:1
arxiv-1408-1656 | A Fast and Accurate Unconstrained Face Detector | http://arxiv.org/abs/1408.1656 | id:1408.1656 author:Shengcai Liao, Anil K. Jain, Stan Z. Li category:cs.CV  published:2014-08-06 summary:We propose a method to address challenges in unconstrained face detection, such as arbitrary pose variations and occlusions. First, a new image feature called Normalized Pixel Difference (NPD) is proposed. NPD feature is computed as the difference to sum ratio between two pixel values, inspired by the Weber Fraction in experimental psychology. The new feature is scale invariant, bounded, and is able to reconstruct the original image. Second, we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations, so that complex face manifolds can be partitioned by the learned rules. This way, only a single soft-cascade classifier is needed to handle unconstrained face detection. Furthermore, we show that the NPD features can be efficiently obtained from a look up table, and the detection template can be easily scaled, making the proposed face detector very fast. Experimental results on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the proposed method achieves state-of-the-art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes. version:3
arxiv-1509-01938 | Exploiting Out-of-Domain Data Sources for Dialectal Arabic Statistical Machine Translation | http://arxiv.org/abs/1509.01938 | id:1509.01938 author:Katrin Kirchhoff, Bing Zhao, Wen Wang category:cs.CL  published:2015-09-07 summary:Statistical machine translation for dialectal Arabic is characterized by a lack of data since data acquisition involves the transcription and translation of spoken language. In this study we develop techniques for extracting parallel data for one particular dialect of Arabic (Iraqi Arabic) from out-of-domain corpora in different dialects of Arabic or in Modern Standard Arabic. We compare two different data selection strategies (cross-entropy based and submodular selection) and demonstrate that a very small but highly targeted amount of found data can improve the performance of a baseline machine translation system. We furthermore report on preliminary experiments on using automatically translated speech data as additional training data. version:1
arxiv-1502-05696 | Approval Voting and Incentives in Crowdsourcing | http://arxiv.org/abs/1502.05696 | id:1502.05696 author:Nihar B. Shah, Dengyong Zhou, Yuval Peres category:cs.GT cs.AI cs.LG cs.MA  published:2015-02-19 summary:The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a ("strictly proper") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach. version:3
arxiv-1509-01865 | A Hybrid Approach to Domain-Specific Entity Linking | http://arxiv.org/abs/1509.01865 | id:1509.01865 author:Alex Olieman, Jaap Kamps, Maarten Marx, Arjan Nusselder category:cs.IR cs.CL H.3.1  published:2015-09-06 summary:The current state-of-the-art Entity Linking (EL) systems are geared towards corpora that are as heterogeneous as the Web, and therefore perform sub-optimally on domain-specific corpora. A key open problem is how to construct effective EL systems for specific domains, as knowledge of the local context should in principle increase, rather than decrease, effectiveness. In this paper we propose the hybrid use of simple specialist linkers in combination with an existing generalist system to address this problem. Our main findings are the following. First, we construct a new reusable benchmark for EL on a corpus of domain-specific conversations. Second, we test the performance of a range of approaches under the same conditions, and show that specialist linkers obtain high precision in isolation, and high recall when combined with generalist linkers. Hence, we can effectively exploit local context and get the best of both worlds. version:1
arxiv-1505-04650 | Compressed Nonnegative Matrix Factorization is Fast and Accurate | http://arxiv.org/abs/1505.04650 | id:1505.04650 author:Mariano Tepper, Guillermo Sapiro category:cs.LG stat.ML  published:2015-05-18 summary:Nonnegative matrix factorization (NMF) has an established reputation as a useful data analysis technique in numerous applications. However, its usage in practical situations is undergoing challenges in recent years. The fundamental factor to this is the increasingly growing size of the datasets available and needed in the information sciences. To address this, in this work we propose to use structured random compression, that is, random projections that exploit the data structure, for two NMF variants: classical and separable. In separable NMF (SNMF) the left factors are a subset of the columns of the input matrix. We present suitable formulations for each problem, dealing with different representative algorithms within each one. We show that the resulting compressed techniques are faster than their uncompressed variants, vastly reduce memory demands, and do not encompass any significant deterioration in performance. The proposed structured random projections for SNMF allow to deal with arbitrarily shaped large matrices, beyond the standard limit of tall-and-skinny matrices, granting access to very efficient computations in this general setting. We accompany the algorithmic presentation with theoretical foundations and numerous and diverse examples, showing the suitability of the proposed approaches. version:2
arxiv-1509-01815 | Research: Analysis of Transport Model that Approximates Decision Taker's Preferences | http://arxiv.org/abs/1509.01815 | id:1509.01815 author:Valery Vilisov category:cs.LG cs.AI math.OC stat.AP  published:2015-09-06 summary:Paper provides a method for solving the reverse Monge-Kantorovich transport problem (TP). It allows to accumulate positive decision-taking experience made by decision-taker in situations that can be presented in the form of TP. The initial data for the solution of the inverse TP is the information on orders, inventories and effective decisions take by decision-taker. The result of solving the inverse TP contains evaluations of the TPs payoff matrix elements. It can be used in new situations to select the solution corresponding to the preferences of the decision-taker. The method allows to gain decision-taker experience, so it can be used by others. The method allows to build the model of decision-taker preferences in a specific application area. The model can be updated regularly to ensure its relevance and adequacy to the decision-taker system of preferences. This model is adaptive to the current preferences of the decision taker. version:1
arxiv-1509-04237 | A Total Fractional-Order Variation Model for Image Restoration with Non-homogeneous Boundary Conditions and its Numerical Solution | http://arxiv.org/abs/1509.04237 | id:1509.04237 author:Jianping Zhang, Ke Chen category:cs.CV math.NA  published:2015-09-06 summary:To overcome the weakness of a total variation based model for image restoration, various high order (typically second order) regularization models have been proposed and studied recently. In this paper we analyze and test a fractional-order derivative based total $\alpha$-order variation model, which can outperform the currently popular high order regularization models. There exist several previous works using total $\alpha$-order variations for image restoration; however first no analysis is done yet and second all tested formulations, differing from each other, utilize the zero Dirichlet boundary conditions which are not realistic (while non-zero boundary conditions violate definitions of fractional-order derivatives). This paper first reviews some results of fractional-order derivatives and then analyzes the theoretical properties of the proposed total $\alpha$-order variational model rigorously. It then develops four algorithms for solving the variational problem, one based on the variational Split-Bregman idea and three based on direct solution of the discretise-optimization problem. Numerical experiments show that, in terms of restoration quality and solution efficiency, the proposed model can produce highly competitive results, for smooth images, to two established high order models: the mean curvature and the total generalized variation. version:1
arxiv-1310-1371 | Robust and highly performant ring detection algorithm for 3d particle tracking using 2d microscope imaging | http://arxiv.org/abs/1310.1371 | id:1310.1371 author:Eldad Afik category:cs.CV cond-mat.soft physics.flu-dyn  published:2013-10-02 summary:Three-dimensional particle tracking is an essential tool in studying dynamics under the microscope, namely, fluid dynamics in microfluidic devices, bacteria taxis, cellular trafficking. The 3d position can be determined using 2d imaging alone by measuring the diffraction rings generated by an out-of-focus fluorescent particle, imaged on a single camera. Here I present a ring detection algorithm exhibiting a high detection rate, which is robust to the challenges arising from ring occlusion, inclusions and overlaps, and allows resolving particles even when near to each other. It is capable of real time analysis thanks to its high performance and low memory footprint. The proposed algorithm, an offspring of the circle Hough transform, addresses the need to efficiently trace the trajectories of many particles concurrently, when their number in not necessarily fixed, by solving a classification problem, and overcomes the challenges of finding local maxima in the complex parameter space which results from ring clusters and noise. Several algorithmic concepts introduced here can be advantageous in other cases, particularly when dealing with noisy and sparse data. The implementation is based on open-source and cross-platform software packages only, making it easy to distribute and modify. It is implemented in a microfluidic experiment allowing real-time multi-particle tracking at 70 Hz, achieving a detection rate which exceeds 94% and only 1% false-detection. version:3
arxiv-1505-02324 | Simultaneous Clustering and Model Selection for Multinomial Distribution: A Comparative Study | http://arxiv.org/abs/1505.02324 | id:1505.02324 author:Md. Abul Hasnat, Julien Velcin, Stéphane Bonnevay, Julien Jacques category:cs.LG stat.ME stat.ML  published:2015-05-09 summary:In this paper, we study different discrete data clustering methods, which use the Model-Based Clustering (MBC) framework with the Multinomial distribution. Our study comprises several relevant issues, such as initialization, model estimation and model selection. Additionally, we propose a novel MBC method by efficiently combining the partitional and hierarchical clustering techniques. We conduct experiments on both synthetic and real data and evaluate the methods using accuracy, stability and computation time. Our study identifies appropriate strategies to be used for discrete data analysis with the MBC methods. Moreover, our proposed method is very competitive w.r.t. clustering accuracy and better w.r.t. stability and computation time. version:2
arxiv-1509-01788 | Joint Color-Spatial-Directional clustering and Region Merging (JCSD-RM) for unsupervised RGB-D image segmentation | http://arxiv.org/abs/1509.01788 | id:1509.01788 author:Md. Abul Hasnat, Olivier Alata, Alain Trémeau category:cs.CV  published:2015-09-06 summary:Recent advances in depth imaging sensors provide easy access to the synchronized depth with color, called RGB-D image. In this paper, we propose an unsupervised method for indoor RGB-D image segmentation and analysis. We consider a statistical image generation model based on the color and geometry of the scene. Our method consists of a joint color-spatial-directional clustering method followed by a statistical planar region merging method. We evaluate our method on the NYU depth database and compare it with existing unsupervised RGB-D segmentation methods. Results show that, it is comparable with the state of the art methods and it needs less computation time. Moreover, it opens interesting perspectives to fuse color and geometry in an unsupervised manner. version:1
arxiv-1509-01770 | Theoretical and Experimental Analyses of Tensor-Based Regression and Classification | http://arxiv.org/abs/1509.01770 | id:1509.01770 author:Kishan Wimalawarne, Ryota Tomioka, Masashi Sugiyama category:cs.LG stat.ML  published:2015-09-06 summary:We theoretically and experimentally investigate tensor-based regression and classification. Our focus is regularization with various tensor norms, including the overlapped trace norm, the latent trace norm, and the scaled latent trace norm. We first give dual optimization methods using the alternating direction method of multipliers, which is computationally efficient when the number of training samples is moderate. We then theoretically derive an excess risk bound for each tensor norm and clarify their behavior. Finally, we perform extensive experiments using simulated and real data and demonstrate the superiority of tensor-based learning methods over vector- and matrix-based learning methods. version:1
arxiv-1509-01722 | A commentary on "The now-or-never bottleneck: a fundamental constraint on language", by Christiansen and Chater (2015) | http://arxiv.org/abs/1509.01722 | id:1509.01722 author:Ramon Ferrer-i-Cancho category:cs.CL  published:2015-09-05 summary:In a recent article, Christiansen and Chater (2015) present a fundamental constraint on language, i.e. a now-or-never bottleneck that arises from our fleeting memory, and explore its implications, e.g., chunk-and-pass processing, outlining a framework that promises to unify different areas of research. Here we explore additional support for this constraint and suggest further connections from quantitative linguistics and information theory. version:1
arxiv-1509-01719 | Unsupervised Cross-Domain Recognition by Identifying Compact Joint Subspaces | http://arxiv.org/abs/1509.01719 | id:1509.01719 author:Yuewei Lin, Jing Chen, Yu Cao, Youjie Zhou, Lingfeng Zhang, Yuan Yan Tang, Song Wang category:cs.CV  published:2015-09-05 summary:This paper introduces a new method to solve the cross-domain recognition problem. Different from the traditional domain adaption methods which rely on a global domain shift for all classes between source and target domain, the proposed method is more flexible to capture individual class variations across domains. By adopting a natural and widely used assumption -- "the data samples from the same class should lay on a low-dimensional subspace, even if they come from different domains", the proposed method circumvents the limitation of the global domain shift, and solves the cross-domain recognition by finding the compact joint subspaces of source and target domain. Specifically, given labeled samples in source domain, we construct subspaces for each of the classes. Then we construct subspaces in the target domain, called anchor subspaces, by collecting unlabeled samples that are close to each other and highly likely all fall into the same class. The corresponding class label is then assigned by minimizing a cost function which reflects the overlap and topological structure consistency between subspaces across source and target domains, and within anchor subspaces, respectively.We further combine the anchor subspaces to corresponding source subspaces to construct the compact joint subspaces. Subsequently, one-vs-rest SVM classifiers are trained in the compact joint subspaces and applied to unlabeled data in the target domain. We evaluate the proposed method on two widely used datasets: object recognition dataset for computer vision tasks, and sentiment classification dataset for natural language processing tasks. Comparison results demonstrate that the proposed method outperforms the comparison methods on both datasets. version:1
arxiv-1508-01244 | TabletGaze: Unconstrained Appearance-based Gaze Estimation in Mobile Tablets | http://arxiv.org/abs/1508.01244 | id:1508.01244 author:Qiong Huang, Ashok Veeraraghavan, Ashutosh Sabharwal category:cs.CV  published:2015-08-05 summary:We study gaze estimation on tablets; our key design goal is uncalibrated gaze estimation using the front-facing camera during natural use of tablets, where the posture and method of holding the tablet is not constrained. We collected the first large unconstrained gaze dataset of tablet users, labeled Rice TabletGaze dataset. The dataset consists of 51 subjects, each with 4 different postures and 35 gaze locations. Subjects vary in race, gender and in their need for prescription glasses, all of which might impact gaze estimation accuracy. Driven by our observations on the collected data, we present a TabletGaze algorithm for automatic gaze estimation using multi-level HoG feature and Random Forests regressor. The TabletGaze algorithm achieves a mean error of 3.17 cm. We perform extensive evaluation on the impact of various factors such as dataset size, race, wearing glasses and user posture on the gaze estimation accuracy and make important observations about the impact of these factors. version:2
arxiv-1509-01710 | Algorithm and Theoretical Analysis for Domain Adaptation Feature Learning with Linear Classifiers | http://arxiv.org/abs/1509.01710 | id:1509.01710 author:Wenhao Jiang, Feiping Nie, Fu-lai Korris Chung, Heng Huang category:cs.LG  published:2015-09-05 summary:Domain adaptation problem arises in a variety of applications where the training set (\textit{source} domain) and testing set (\textit{target} domain) follow different distributions. The difficulty of such learning problem lies in how to bridge the gap between the source distribution and target distribution. In this paper, we give an formal analysis of feature learning algorithms for domain adaptation with linear classifiers. Our analysis shows that in order to achieve good adaptation performance, the second moments of source domain distribution and target domain distribution should be similar. Based on such a result, a new linear feature learning algorithm for domain adaptation is designed and proposed. Furthermore, the new algorithm is extended to have multiple layers, resulting in becoming another linear feature learning algorithm. The newly introduced method is effective for the domain adaptation tasks on Amazon review dataset and spam dataset from ECML/PKDD 2006 discovery challenge. version:1
arxiv-1301-4183 | On Graphical Models via Univariate Exponential Family Distributions | http://arxiv.org/abs/1301.4183 | id:1301.4183 author:Eunho Yang, Pradeep Ravikumar, Genevera I. Allen, Zhandong Liu category:math.ST stat.ML stat.TH  published:2013-01-17 summary:Undirected graphical models, or Markov networks, are a popular class of statistical models, used in a wide variety of applications. Popular instances of this class include Gaussian graphical models and Ising models. In many settings, however, it might not be clear which subclass of graphical models to use, particularly for non-Gaussian and non-categorical data. In this paper, we consider a general sub-class of graphical models where the node-wise conditional distributions arise from exponential families. This allows us to derive multivariate graphical model distributions from univariate exponential family distributions, such as the Poisson, negative binomial, and exponential distributions. Our key contributions include a class of M-estimators to fit these graphical model distributions; and rigorous statistical analysis showing that these M-estimators recover the true graphical model structure exactly, with high probability. We provide examples of genomic and proteomic networks learned via instances of our class of graphical models derived from Poisson and exponential distributions. version:2
arxiv-1509-01659 | Gravitational Clustering | http://arxiv.org/abs/1509.01659 | id:1509.01659 author:Armen Aghajanyan category:cs.LG  published:2015-09-05 summary:The downfall of many supervised learning algorithms, such as neural networks, is the inherent need for a large amount of training data. Although there is a lot of buzz about big data, there is still the problem of doing classification from a small dataset. Other methods such as support vector machines, although capable of dealing with few samples, are inherently binary classifiers, and are in need of learning strategies such as One vs All in the case of multi-classification. In the presence of a large number of classes this can become problematic. In this paper we present, a novel approach to supervised learning through the method of clustering. Unlike traditional methods such as K-Means, Gravitational Clustering does not require the initial number of clusters, and automatically builds the clusters, individual samples can be arbitrarily weighted and it requires only few samples while staying resilient to over-fitting. version:1
arxiv-1509-01654 | Co-interest Person Detection from Multiple Wearable Camera Videos | http://arxiv.org/abs/1509.01654 | id:1509.01654 author:Yuewei Lin, Kareem Ezzeldeen, Youjie Zhou, Xiaochuan Fan, Hongkai Yu, Hui Qian, Song Wang category:cs.CV  published:2015-09-05 summary:Wearable cameras, such as Google Glass and Go Pro, enable video data collection over larger areas and from different views. In this paper, we tackle a new problem of locating the co-interest person (CIP), i.e., the one who draws attention from most camera wearers, from temporally synchronized videos taken by multiple wearable cameras. Our basic idea is to exploit the motion patterns of people and use them to correlate the persons across different videos, instead of performing appearance-based matching as in traditional video co-segmentation/localization. This way, we can identify CIP even if a group of people with similar appearance are present in the view. More specifically, we detect a set of persons on each frame as the candidates of the CIP and then build a Conditional Random Field (CRF) model to select the one with consistent motion patterns in different videos and high spacial-temporal consistency in each video. We collect three sets of wearable-camera videos for testing the proposed algorithm. All the involved people have similar appearances in the collected videos and the experiments demonstrate the effectiveness of the proposed algorithm. version:1
arxiv-1509-01631 | Stochastic gradient variational Bayes for gamma approximating distributions | http://arxiv.org/abs/1509.01631 | id:1509.01631 author:David A. Knowles category:stat.ML  published:2015-09-04 summary:While stochastic variational inference is relatively well known for scaling inference in Bayesian probabilistic models, related methods also offer ways to circumnavigate the approximation of analytically intractable expectations. The key challenge in either setting is controlling the variance of gradient estimates: recent work has shown that for continuous latent variables, particularly multivariate Gaussians, this can be achieved by using the gradient of the log posterior. In this paper we apply the same idea to gamma distributed latent variables given gamma variational distributions, enabling straightforward "black box" variational inference in models where sparsity and non-negativity are appropriate. We demonstrate the method on a recently proposed gamma process model for network data, as well as a novel sparse factor analysis. We outperform generic sampling algorithms and the approach of using Gaussian variational distributions on transformed variables. version:1
arxiv-1509-01624 | Chebyshev and Conjugate Gradient Filters for Graph Image Denoising | http://arxiv.org/abs/1509.01624 | id:1509.01624 author:Dong Tian, Hassan Mansour, Andrew Knyazev, Anthony Vetro category:cs.CV  published:2015-09-04 summary:In 3D image/video acquisition, different views are often captured with varying noise levels across the views. In this paper, we propose a graph-based image enhancement technique that uses a higher quality view to enhance a degraded view. A depth map is utilized as auxiliary information to match the perspectives of the two views. Our method performs graph-based filtering of the noisy image by directly computing a projection of the image to be filtered onto a lower dimensional Krylov subspace of the graph Laplacian. We discuss two graph spectral denoising methods: first using Chebyshev polynomials, and second using iterations of the conjugate gradient algorithm. Our framework generalizes previously known polynomial graph filters, and we demonstrate through numerical simulations that our proposed technique produces subjectively cleaner images with about 1-3 dB improvement in PSNR over existing polynomial graph filters. version:1
arxiv-1509-02441 | Semantic Video Segmentation : Exploring Inference Efficiency | http://arxiv.org/abs/1509.02441 | id:1509.02441 author:Subarna Tripathi, Serge Belongie, Youngbae Hwang, Truong Nguyen category:cs.CV  published:2015-09-04 summary:We explore the efficiency of the CRF inference beyond image level semantic segmentation and perform joint inference in video frames. The key idea is to combine best of two worlds: semantic co-labeling and more expressive models. Our formulation enables us to perform inference over ten thousand images within seconds and makes the system amenable to perform video semantic segmentation most effectively. On CamVid dataset, with TextonBoost unaries, our proposed method achieves up to 8% improvement in accuracy over individual semantic image segmentation without additional time overhead. The source code is available at https://github.com/subtri/video_inference version:1
arxiv-1509-01602 | Object Recognition from Short Videos for Robotic Perception | http://arxiv.org/abs/1509.01602 | id:1509.01602 author:Ivan Bogun, Anelia Angelova, Navdeep Jaitly category:cs.CV I.5.4  published:2015-09-04 summary:Deep neural networks have become the primary learning technique for object recognition. Videos, unlike still images, are temporally coherent which makes the application of deep networks non-trivial. Here, we investigate how motion can aid object recognition in short videos. Our approach is based on Long Short-Term Memory (LSTM) deep networks. Unlike previous applications of LSTMs, we implement each gate as a convolution. We show that convolutional-based LSTM models are capable of learning motion dependencies and are able to improve the recognition accuracy when more frames in a sequence are available. We evaluate our approach on the Washington RGBD Object dataset and on the Washington RGBD Scenes dataset. Our approach outperforms deep nets applied to still images and sets a new state-of-the-art in this domain. version:1
arxiv-1504-03509 | Regret vs. Communication: Distributed Stochastic Multi-Armed Bandits and Beyond | http://arxiv.org/abs/1504.03509 | id:1504.03509 author:Shuang Liu, Cheng Chen, Zhihua Zhang category:cs.LG  published:2015-04-14 summary:In this paper, we consider the distributed stochastic multi-armed bandit problem, where a global arm set can be accessed by multiple players independently. The players are allowed to exchange their history of observations with each other at specific points in time. We study the relationship between regret and communication. When the time horizon is known, we propose the Over-Exploration strategy, which only requires one-round communication and whose regret does not scale with the number of players. When the time horizon is unknown, we measure the frequency of communication through a new notion called the density of the communication set, and give an exact characterization of the interplay between regret and communication. Specifically, a lower bound is established and stable strategies that match the lower bound are developed. The results and analyses in this paper are specific but can be translated into more general settings. version:2
arxiv-1306-0735 | Particle approximations of the score and observed information matrix for parameter estimation in state space models with linear computational cost | http://arxiv.org/abs/1306.0735 | id:1306.0735 author:Christopher Nemeth, Paul Fearnhead, Lyudmila Mihaylova category:stat.CO stat.ML  published:2013-06-04 summary:Poyiadjis et al. (2011) show how particle methods can be used to estimate both the score and the observed information matrix for state space models. These methods either suffer from a computational cost that is quadratic in the number of particles, or produce estimates whose variance increases quadratically with the amount of data. This paper introduces an alternative approach for estimating these terms at a computational cost that is linear in the number of particles. The method is derived using a combination of kernel density estimation, to avoid the particle degeneracy that causes the quadratically increasing variance, and Rao-Blackwellisation. Crucially, we show the method is robust to the choice of bandwidth within the kernel density estimation, as it has good asymptotic properties regardless of this choice. Our estimates of the score and observed information matrix can be used within both online and batch procedures for estimating parameters for state space models. Empirical results show improved parameter estimates compared to existing methods at a significantly reduced computational cost. Supplementary materials including code are available. version:3
arxiv-1509-01514 | Conjugate Gradient Acceleration of Non-Linear Smoothing Filters | http://arxiv.org/abs/1509.01514 | id:1509.01514 author:Andrew Knyazev, Alexander Malyshev category:cs.CV  published:2015-09-04 summary:The most efficient signal edge-preserving smoothing filters, e.g., for denoising, are non-linear. Thus, their acceleration is challenging and is often performed in practice by tuning filter parameters, such as by increasing the width of the local smoothing neighborhood, resulting in more aggressive smoothing of a single sweep at the cost of increased edge blurring. We propose an alternative technology, accelerating the original filters without tuning, by running them through a special conjugate gradient method, not affecting their quality. The filter non-linearity is dealt with by careful freezing and restarting. Our initial numerical experiments on toy one-dimensional signals demonstrate 20x acceleration of the classical bilateral filter and 3-5x acceleration of the recently developed guided filter. version:1
arxiv-1509-03503 | NoSPaM Manual - A Tool for Node-Specific Triad Pattern Mining | http://arxiv.org/abs/1509.03503 | id:1509.03503 author:Marco Winkler category:cs.SI cs.CV cs.DS physics.data-an physics.soc-ph  published:2015-09-04 summary:The detection of triadic subgraph motifs is a common methodology in complex-networks research. The procedure usually applied in order to detect motifs evaluates whether a certain subgraph pattern is overrepresented in a network as a whole. However, motifs do not necessarily appear frequently in every region of a graph. For this reason, we recently introduced the framework of Node-Specific Pattern Mining (NoSPaM). This work is a manual for an implementation of NoSPaM which can be downloaded from www.mwinkler.eu. version:1
arxiv-1509-01469 | Quantization based Fast Inner Product Search | http://arxiv.org/abs/1509.01469 | id:1509.01469 author:Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, David Simcha category:cs.AI cs.LG stat.ML  published:2015-09-04 summary:We propose a quantization based approach for fast approximate Maximum Inner Product Search (MIPS). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers. Different from recently proposed LSH approaches to MIPS, the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art. version:1
arxiv-1503-03637 | On Computing the Translations Norm in the Epipolar Graph | http://arxiv.org/abs/1503.03637 | id:1503.03637 author:Federica Arrigoni, Beatrice Rossi, Andrea Fusiello category:cs.CV  published:2015-03-12 summary:This paper deals with the problem of recovering the unknown norm of relative translations between cameras based on the knowledge of relative rotations and translation directions. We provide theoretical conditions for the solvability of such a problem, and we propose a two-stage method to solve it. First, a cycle basis for the epipolar graph is computed, then all the scaling factors are recovered simultaneously by solving a homogeneous linear system. We demonstrate the accuracy of our solution by means of synthetic and real experiments. version:3
arxiv-1509-01386 | Predicting SLA Violations in Real Time using Online Machine Learning | http://arxiv.org/abs/1509.01386 | id:1509.01386 author:Jawwad Ahmed, Andreas Johnsson, Rerngvit Yanggratoke, John Ardelius, Christofer Flinta, Rolf Stadler category:cs.NI cs.LG cs.SE stat.ML  published:2015-09-04 summary:Detecting faults and SLA violations in a timely manner is critical for telecom providers, in order to avoid loss in business, revenue and reputation. At the same time predicting SLA violations for user services in telecom environments is difficult, due to time-varying user demands and infrastructure load conditions. In this paper, we propose a service-agnostic online learning approach, whereby the behavior of the system is learned on the fly, in order to predict client-side SLA violations. The approach uses device-level metrics, which are collected in a streaming fashion on the server side. Our results show that the approach can produce highly accurate predictions (>90% classification accuracy and < 10% false alarm rate) in scenarios where SLA violations are predicted for a video-on-demand service under changing load patterns. The paper also highlight the limitations of traditional offline learning methods, which perform significantly worse in many of the considered scenarios. version:1
arxiv-1602-07679 | A statistical shape space model of the palate surface trained on 3D MRI scans of the vocal tract | http://arxiv.org/abs/1602.07679 | id:1602.07679 author:Alexander Hewer, Ingmar Steiner, Timo Bolkart, Stefanie Wuhrer, Korin Richmond category:cs.CV  published:2015-09-04 summary:We describe a minimally-supervised method for computing a statistical shape space model of the palate surface. The model is created from a corpus of volumetric magnetic resonance imaging (MRI) scans collected from 12 speakers. We extract a 3D mesh of the palate from each speaker, then train the model using principal component analysis (PCA). The palate model is then tested using 3D MRI from another corpus and evaluated using a high-resolution optical scan. We find that the error is low even when only a handful of measured coordinates are available. In both cases, our approach yields promising results. It can be applied to extract the palate shape from MRI data, and could be useful to other analysis modalities, such as electromagnetic articulography (EMA) and ultrasound tongue imaging (UTI). version:1
arxiv-1509-01354 | CNN Based Hashing for Image Retrieval | http://arxiv.org/abs/1509.01354 | id:1509.01354 author:Jinma Guo, Jianmin Li category:cs.CV cs.LG I.2.6; H.3.1  published:2015-09-04 summary:Along with data on the web increasing dramatically, hashing is becoming more and more popular as a method of approximate nearest neighbor search. Previous supervised hashing methods utilized similarity/dissimilarity matrix to get semantic information. But the matrix is not easy to construct for a new dataset. Rather than to reconstruct the matrix, we proposed a straightforward CNN-based hashing method, i.e. binarilizing the activations of a fully connected layer with threshold 0 and taking the binary result as hash codes. This method achieved the best performance on CIFAR-10 and was comparable with the state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested that the signs of activations may carry more information than the relative values of activations between samples, and that the co-adaption between feature extractor and hash functions is important for hashing. version:1
arxiv-1509-01352 | Diffusion-KLMS Algorithm and its Performance Analysis for Non-Linear Distributed Networks | http://arxiv.org/abs/1509.01352 | id:1509.01352 author:Rangeet Mitra, Vimal Bhatia category:cs.LG cs.DC cs.IT cs.SY math.IT  published:2015-09-04 summary:In a distributed network environment, the diffusion-least mean squares (LMS) algorithm gives faster convergence than the original LMS algorithm. It has also been observed that, the diffusion-LMS generally outperforms other distributed LMS algorithms like spatial LMS and incremental LMS. However, both the original LMS and diffusion-LMS are not applicable in non-linear environments where data may not be linearly separable. A variant of LMS called kernel-LMS (KLMS) has been proposed in the literature for such non-linearities. In this paper, we propose kernelised version of diffusion-LMS for non-linear distributed environments. Simulations show that the proposed approach has superior convergence as compared to algorithms of the same genre. We also introduce a technique to predict the transient and steady-state behaviour of the proposed algorithm. The techniques proposed in this work (or algorithms of same genre) can be easily extended to distributed parameter estimation applications like cooperative spectrum sensing and massive multiple input multiple output (MIMO) receiver design which are potential components for 5G communication systems. version:1
arxiv-1509-01349 | Parallel and Distributed Approaches for Graph Based Semi-supervised Learning | http://arxiv.org/abs/1509.01349 | id:1509.01349 author:Konstantin Avrachenkov, Vivek Borkar, Krishnakant Saboo category:cs.LG  published:2015-09-04 summary:Two approaches for graph based semi-supervised learning are proposed. The firstapproach is based on iteration of an affine map. A key element of the affine map iteration is sparsematrix-vector multiplication, which has several very efficient parallel implementations. The secondapproach belongs to the class of Markov Chain Monte Carlo (MCMC) algorithms. It is based onsampling of nodes by performing a random walk on the graph. The latter approach is distributedby its nature and can be easily implemented on several processors or over the network. Boththeoretical and practical evaluations are provided. It is found that the nodes are classified intotheir class with very small error. The sampling algorithm's ability to track new incoming nodesand to classify them is also demonstrated. version:1
arxiv-1509-01346 | Deep Broad Learning - Big Models for Big Data | http://arxiv.org/abs/1509.01346 | id:1509.01346 author:Nayyar A. Zaidi, Geoffrey I. Webb, Mark J. Carman, Francois Petitjean category:cs.LG  published:2015-09-04 summary:Deep learning has demonstrated the power of detailed modeling of complex high-order (multivariate) interactions in data. For some learning tasks there is power in learning models that are not only Deep but also Broad. By Broad, we mean models that incorporate evidence from large numbers of features. This is of especial value in applications where many different features and combinations of features all carry small amounts of information about the class. The most accurate models will integrate all that information. In this paper, we propose an algorithm for Deep Broad Learning called DBL. The proposed algorithm has a tunable parameter $n$, that specifies the depth of the model. It provides straightforward paths towards out-of-core learning for large data. We demonstrate that DBL learns models from large quantities of data with accuracy that is highly competitive with the state-of-the-art. version:1
arxiv-1509-01343 | Learning Temporal Alignment Uncertainty for Efficient Event Detection | http://arxiv.org/abs/1509.01343 | id:1509.01343 author:Iman Abbasnejad, Sridha Sridharan, Simon Denman, Clinton Fookes, Simon Lucey category:cs.CV  published:2015-09-04 summary:In this paper we tackle the problem of efficient video event detection. We argue that linear detection functions should be preferred in this regard due to their scalability and efficiency during estimation and evaluation. A popular approach in this regard is to represent a sequence using a bag of words (BOW) representation due to its: (i) fixed dimensionality irrespective of the sequence length, and (ii) its ability to compactly model the statistics in the sequence. A drawback to the BOW representation, however, is the intrinsic destruction of the temporal ordering information. In this paper we propose a new representation that leverages the uncertainty in relative temporal alignments between pairs of sequences while not destroying temporal ordering. Our representation, like BOW, is of a fixed dimensionality making it easily integrated with a linear detection function. Extensive experiments on CK+, 6DMG, and UvA-NEMO databases show significant performance improvements across both isolated and continuous event detection tasks. version:1
arxiv-1509-01329 | Semantic Amodal Segmentation | http://arxiv.org/abs/1509.01329 | id:1509.01329 author:Yan Zhu, Yuandong Tian, Dimitris Mexatas, Piotr Dollár category:cs.CV  published:2015-09-04 summary:Common visual recognition tasks such as classification, object detection, and semantic segmentation are rapidly reaching maturity, and given the recent rate of progress, it is not unreasonable to conjecture that techniques for many of these problems will approach human levels of performance in the next few years. In this paper we look to the future: what is the next frontier in visual recognition? We offer one possible answer to this question. We propose a detailed image annotation that captures information beyond the visible pixels and requires complex reasoning about full scene structure. Specifically, we create an amodal segmentation of each image: the full extent of each region is marked, not just the visible pixels. Annotators outline and name all salient regions in the image and specify a partial depth order. The result is a rich scene structure, including visible and occluded portions of each region, figure-ground edge information, semantic labels, and object overlap. To date, we have labeled 500 images in the BSDS dataset with at least five annotators per image. Critically, the resulting full scene annotation is surprisingly consistent between annotators. For example, for edge detection our annotations have substantially higher human consistency than the original BSDS edges while providing a greater challenge for existing algorithms. We are currently annotating ~5000 images from the MS COCO dataset. version:1
arxiv-1509-01323 | l1-norm Penalized Orthogonal Forward Regression | http://arxiv.org/abs/1509.01323 | id:1509.01323 author:Xia Hong, Sheng Chen, Yi Guo, Junbin Gao category:cs.LG stat.ML  published:2015-09-04 summary:A l1-norm penalized orthogonal forward regression (l1-POFR) algorithm is proposed based on the concept of leaveone- out mean square error (LOOMSE). Firstly, a new l1-norm penalized cost function is defined in the constructed orthogonal space, and each orthogonal basis is associated with an individually tunable regularization parameter. Secondly, due to orthogonal computation, the LOOMSE can be analytically computed without actually splitting the data set, and moreover a closed form of the optimal regularization parameter in terms of minimal LOOMSE is derived. Thirdly, a lower bound for regularization parameters is proposed, which can be used for robust LOOMSE estimation by adaptively detecting and removing regressors to an inactive set so that the computational cost of the algorithm is significantly reduced. Illustrative examples are included to demonstrate the effectiveness of this new l1-POFR approach. version:1
arxiv-1503-01832 | Linear Global Translation Estimation with Feature Tracks | http://arxiv.org/abs/1503.01832 | id:1503.01832 author:Zhaopeng Cui, Nianjuan Jiang, Chengzhou Tang, Ping Tan category:cs.CV  published:2015-03-06 summary:This paper derives a novel linear position constraint for cameras seeing a common scene point, which leads to a direct linear method for global camera translation estimation. Unlike previous solutions, this method deals with collinear camera motion and weak image association at the same time. The final linear formulation does not involve the coordinates of scene points, which makes it efficient even for large scale data. We solve the linear equation based on $L_1$ norm, which makes our system more robust to outliers in essential matrices and feature correspondences. We experiment this method on both sequentially captured images and unordered Internet images. The experiments demonstrate its strength in robustness, accuracy, and efficiency. version:2
arxiv-1509-01310 | The influence of Chunking on Dependency Crossing and Distance | http://arxiv.org/abs/1509.01310 | id:1509.01310 author:Qian Lu, Chunshan Xu, Haitao Liu category:cs.CL  published:2015-09-03 summary:This paper hypothesizes that chunking plays important role in reducing dependency distance and dependency crossings. Computer simulations, when compared with natural languages,show that chunking reduces mean dependency distance (MDD) of a linear sequence of nodes (constrained by continuity or projectivity) to that of natural languages. More interestingly, chunking alone brings about less dependency crossings as well, though having failed to reduce them, to such rarity as found in human languages. These results suggest that chunking may play a vital role in the minimization of dependency distance, and a somewhat contributing role in the rarity of dependency crossing. In addition, the results point to a possibility that the rarity of dependency crossings is not a mere side-effect of minimization of dependency distance, but a linguistic phenomenon with its own motivations. version:1
arxiv-1410-4391 | Multivariate Spearman's rho for aggregating ranks using copulas | http://arxiv.org/abs/1410.4391 | id:1410.4391 author:Justin Bedo, Cheng Soon Ong category:stat.ML cs.LG  published:2014-10-16 summary:We study the problem of rank aggregation: given a set of ranked lists, we want to form a consensus ranking. Furthermore, we consider the case of extreme lists: i.e., only the rank of the best or worst elements are known. We impute missing ranks by the average value and generalise Spearman's \rho to extreme ranks. Our main contribution is the derivation of a non-parametric estimator for rank aggregation based on multivariate extensions of Spearman's \rho, which measures correlation between a set of ranked lists. Multivariate Spearman's \rho is defined using copulas, and we show that the geometric mean of normalised ranks maximises multivariate correlation. Motivated by this, we propose a weighted geometric mean approach for learning to rank which has a closed form least squares solution. When only the best or worst elements of a ranked list are known, we impute the missing ranks by the average value, allowing us to apply Spearman's \rho. Finally, we demonstrate good performance on the rank aggregation benchmarks MQ2007 and MQ2008. version:2
arxiv-1509-01288 | Incremental Active Opinion Learning Over a Stream of Opinionated Documents | http://arxiv.org/abs/1509.01288 | id:1509.01288 author:Max Zimmermann, Eirini Ntoutsi, Myra Spiliopoulou category:cs.IR cs.CL cs.LG  published:2015-09-03 summary:Applications that learn from opinionated documents, like tweets or product reviews, face two challenges. First, the opinionated documents constitute an evolving stream, where both the author's attitude and the vocabulary itself may change. Second, labels of documents are scarce and labels of words are unreliable, because the sentiment of a word depends on the (unknown) context in the author's mind. Most of the research on mining over opinionated streams focuses on the first aspect of the problem, whereas for the second a continuous supply of labels from the stream is assumed. Such an assumption though is utopian as the stream is infinite and the labeling cost is prohibitive. To this end, we investigate the potential of active stream learning algorithms that ask for labels on demand. Our proposed ACOSTREAM 1 approach works with limited labels: it uses an initial seed of labeled documents, occasionally requests additional labels for documents from the human expert and incrementally adapts to the underlying stream while exploiting the available labeled documents. In its core, ACOSTREAM consists of a MNB classifier coupled with "sampling" strategies for requesting class labels for new unlabeled documents. In the experiments, we evaluate the classifier performance over time by varying: (a) the class distribution of the opinionated stream, while assuming that the set of the words in the vocabulary is fixed but their polarities may change with the class distribution; and (b) the number of unknown words arriving at each moment, while the class polarity may also change. Our results show that active learning on a stream of opinionated documents, delivers good performance while requiring a small selection of labels version:1
arxiv-1509-01287 | Image Classification with Rejection using Contextual Information | http://arxiv.org/abs/1509.01287 | id:1509.01287 author:Filipe Condessa, José Bioucas-Dias, Carlos Castro, John Ozolek, Jelena Kovačević category:cs.CV 68T10  published:2015-09-03 summary:We introduce a new supervised algorithm for image classification with rejection using multiscale contextual information. Rejection is desired in image-classification applications that require a robust classifier but not the classification of the entire image. The proposed algorithm combines local and multiscale contextual information with rejection, improving the classification performance. As a probabilistic model for classification, we adopt a multinomial logistic regression. The concept of rejection with contextual information is implemented by modeling the classification problem as an energy minimization problem over a graph representing local and multiscale similarities of the image. The rejection is introduced through an energy data term associated with the classification risk and the contextual information through an energy smoothness term associated with the local and multiscale similarities within the image. We illustrate the proposed method on the classification of images of H&E-stained teratoma tissues. version:1
arxiv-1509-01271 | Probabilistic Neural Network Training for Semi-Supervised Classifiers | http://arxiv.org/abs/1509.01271 | id:1509.01271 author:Hamidreza Farhidzadeh category:cs.LG  published:2015-09-03 summary:In this paper, we propose another version of help-training approach by employing a Probabilistic Neural Network (PNN) that improves the performance of the main discriminative classifier in the semi-supervised strategy. We introduce the PNN-training algorithm and use it for training the support vector machine (SVM) with a few numbers of labeled data and a large number of unlabeled data. We try to find the best labels for unlabeled data and then use SVM to enhance the classification rate. We test our method on two famous benchmarks and show the efficiency of our method in comparison with pervious methods. version:1
arxiv-1509-01270 | Machine Learning Methods to Analyze Arabidopsis Thaliana Plant Root Growth | http://arxiv.org/abs/1509.01270 | id:1509.01270 author:Hamidreza Farhidzadeh category:cs.LG  published:2015-09-03 summary:One of the challenging problems in biology is to classify plants based on their reaction on genetic mutation. Arabidopsis Thaliana is a plant that is so interesting, because its genetic structure has some similarities with that of human beings. Biologists classify the type of this plant to mutated and not mutated (wild) types. Phenotypic analysis of these types is a time-consuming and costly effort by individuals. In this paper, we propose a modified feature extraction step by using velocity and acceleration of root growth. In the second step, for plant classification, we employed different Support Vector Machine (SVM) kernels and two hybrid systems of neural networks. Gated Negative Correlation Learning (GNCL) and Mixture of Negatively Correlated Experts (MNCE) are two ensemble methods based on complementary feature of classical classifiers; Mixture of Expert (ME) and Negative Correlation Learning (NCL). The hybrid systems conserve of advantages and decrease the effects of disadvantages of NCL and ME. Our Experimental shows that MNCE and GNCL improve the efficiency of classical classifiers, however, some SVM kernels function has better performance than classifiers based on neural network ensemble method. Moreover, kernels consume less time to obtain a classification rate. version:1
arxiv-1509-00685 | A Neural Attention Model for Abstractive Sentence Summarization | http://arxiv.org/abs/1509.00685 | id:1509.00685 author:Alexander M. Rush, Sumit Chopra, Jason Weston category:cs.CL cs.AI  published:2015-09-02 summary:Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines. version:2
arxiv-1506-00976 | Toward a generic representation of random variables for machine learning | http://arxiv.org/abs/1506.00976 | id:1506.00976 author:Gautier Marti, Philippe Very, Philippe Donnat category:cs.LG stat.ML  published:2015-06-02 summary:This paper presents a pre-processing and a distance which improve the performance of machine learning algorithms working on independent and identically distributed stochastic processes. We introduce a novel non-parametric approach to represent random variables which splits apart dependency and distribution without losing any information. We also propound an associated metric leveraging this representation and its statistical estimate. Besides experiments on synthetic datasets, the benefits of our contribution is illustrated through the example of clustering financial time series, for instance prices from the credit default swaps market. Results are available on the website www.datagrapple.com and an IPython Notebook tutorial is available at www.datagrapple.com/Tech for reproducible research. version:2
arxiv-1509-01173 | Community Detection in Networks with Node Features | http://arxiv.org/abs/1509.01173 | id:1509.01173 author:Yuan Zhang, Elizaveta Levina, Ji Zhu category:stat.ML cs.SI physics.soc-ph  published:2015-09-03 summary:Many methods have been proposed for community detection in networks, but most of them do not take into account additional information on the nodes that is often available in practice. In this paper, we propose a new joint community detection criterion that uses both the network edge information and the node features to detect community structures. One advantage our method has over existing joint detection approaches is the flexibility of learning the impact of different features which may differ across communities. Another advantage is the flexibility of choosing the amount of influence the feature information has on communities. The method is asymptotically consistent under the block model with additional assumptions on the feature distributions, and performs well on simulated and real networks. version:1
arxiv-1509-01168 | Semi-described and semi-supervised learning with Gaussian processes | http://arxiv.org/abs/1509.01168 | id:1509.01168 author:Andreas Damianou, Neil D. Lawrence category:stat.ML cs.AI cs.LG math.PR 60G15  58E30 G.3; I.2.6  published:2015-09-03 summary:Propagating input uncertainty through non-linear Gaussian process (GP) mappings is intractable. This hinders the task of training GPs using uncertain and partially observed inputs. In this paper we refer to this task as "semi-described learning". We then introduce a GP framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in GPs, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regression/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks. version:1
arxiv-1509-01126 | Training of CC4 Neural Network with Spread Unary Coding | http://arxiv.org/abs/1509.01126 | id:1509.01126 author:Pushpa Sree Potluri category:cs.NE  published:2015-09-03 summary:This paper adapts the corner classification algorithm (CC4) to train the neural networks using spread unary inputs. This is an important problem as spread unary appears to be at the basis of data representation in biological learning. The modified CC4 algorithm is tested using the pattern classification experiment and the results are found to be good. Specifically, we show that the number of misclassified points is not particularly sensitive to the chosen radius of generalization. version:1
arxiv-1509-01122 | Vision-Based Road Detection using Contextual Blocks | http://arxiv.org/abs/1509.01122 | id:1509.01122 author:Caio César Teodoro Mendes, Vincent Frémont, Denis Fernando Wolf category:cs.CV  published:2015-09-03 summary:Road detection is a fundamental task in autonomous navigation systems. In this paper, we consider the case of monocular road detection, where images are segmented into road and non-road regions. Our starting point is the well-known machine learning approach, in which a classifier is trained to distinguish road and non-road regions based on hand-labeled images. We proceed by introducing the use of "contextual blocks" as an efficient way of providing contextual information to the classifier. Overall, the proposed methodology, including its image feature selection and classifier, was conceived with computational cost in mind, leaving room for optimized implementations. Regarding experiments, we perform a sensible evaluation of each phase and feature subset that composes our system. The results show a great benefit from using contextual blocks and demonstrate their computational efficiency. Finally, we submit our results to the KITTI road detection benchmark achieving scores comparable with state of the art methods. version:1
arxiv-1509-01116 | A tree-based kernel for graphs with continuous attributes | http://arxiv.org/abs/1509.01116 | id:1509.01116 author:Giovanni Da San Martino, Nicolò Navarin, Alessandro Sperduti category:cs.LG  published:2015-09-03 summary:The availability of graph data with node attributes that can be either discrete or real-valued is constantly increasing. While existing kernel methods are effective techniques for dealing with graphs having discrete node labels, their adaptation to non-discrete or continuous node attributes has been limited, mainly for computational issues. Recently, a few kernels especially tailored for this domain, have been proposed. In order to alleviate the computational problems, the size of the feature space of such kernels tend to be smaller than the ones of the kernels for discrete node attributes. However, such choice might have a negative impact on the predictive performance. In this paper, we propose a graph kernel for complex and continuous nodes' attributes, whose features are tree structures extracted from specific graph visits. Experimental results obtained on real-world datasets show that the (approximated version of the) proposed kernel is comparable with current state-of-the-art kernels in terms of classification accuracy while requiring shorter running times. version:1
arxiv-1509-01074 | A Novice Guide towards Human Motion Analysis and Understanding | http://arxiv.org/abs/1509.01074 | id:1509.01074 author:Ahmed Nabil Mohamed category:cs.CV  published:2015-09-03 summary:Human motion analysis and understanding has been, and is still, the focus of attention of many disciplines which is considered an obvious indicator of the wide and massive importance of the subject. The purpose of this article is to shed some light on this very important subject, so it can be a good insight for a novice computer vision researcher in this field by providing him/her with a wealth of knowledge about the subject covering many directions. There are two main contributions of this article. The first one investigates various aspects of some disciplines (e.g., arts, philosophy, psychology, and neuroscience) that are interested in the subject and review some of their contributions stressing on those that can be useful for computer vision researchers. Moreover, many examples are illustrated to indicate the benefits of integrating concepts and results among different disciplines. The second contribution is concerned with the subject from the computer vision aspect where we discuss the following issues. First, we explore many demanding and promising applications to reveal the wide and massive importance of the field. Second, we list various types of sensors that may be used for acquiring various data. Third, we review different taxonomies used for classifying motions. Fourth, we review various processes involved in motion analysis. Fifth, we exhibit how different surveys are structured. Sixth, we examine many of the most cited and recent reviews in the field that have been published during the past two decades to reveal various approaches used for implementing different stages of the problem and refer to various algorithms and their suitability for different situations. Moreover, we provide a long list of public datasets and discuss briefly some examples of these datasets. Finally, we provide a general discussion of the subject from the aspect of computer vision. version:1
arxiv-1509-01053 | Training a Restricted Boltzmann Machine for Classification by Labeling Model Samples | http://arxiv.org/abs/1509.01053 | id:1509.01053 author:Malte Probst, Franz Rothlauf category:cs.LG  published:2015-09-03 summary:We propose an alternative method for training a classification model. Using the MNIST set of handwritten digits and Restricted Boltzmann Machines, it is possible to reach a classification performance competitive to semi-supervised learning if we first train a model in an unsupervised fashion on unlabeled data only, and then manually add labels to model samples instead of training data samples with the help of a GUI. This approach can benefit from the fact that model samples can be presented to the human labeler in a video-like fashion, resulting in a higher number of labeled examples. Also, after some initial training, hard-to-classify examples can be distinguished from easy ones automatically, saving manual work. version:1
arxiv-1507-02186 | Extending local features with contextual information in graph kernels | http://arxiv.org/abs/1507.02186 | id:1507.02186 author:Nicolò Navarin, Alessandro Sperduti, Riccardo Tesselli category:cs.LG  published:2015-07-08 summary:Graph kernels are usually defined in terms of simpler kernels over local substructures of the original graphs. Different kernels consider different types of substructures. However, in some cases they have similar predictive performances, probably because the substructures can be interpreted as approximations of the subgraphs they induce. In this paper, we propose to associate to each feature a piece of information about the context in which the feature appears in the graph. A substructure appearing in two different graphs will match only if it appears with the same context in both graphs. We propose a kernel based on this idea that considers trees as substructures, and where the contexts are features too. The kernel is inspired from the framework in [6], even if it is not part of it. We give an efficient algorithm for computing the kernel and show promising results on real-world graph classification datasets. version:2
arxiv-1509-01023 | Generating Weather Forecast Texts with Case Based Reasoning | http://arxiv.org/abs/1509.01023 | id:1509.01023 author:Ibrahim Adeyanju category:cs.AI cs.CL  published:2015-09-03 summary:Several techniques have been used to generate weather forecast texts. In this paper, case based reasoning (CBR) is proposed for weather forecast text generation because similar weather conditions occur over time and should have similar forecast texts. CBR-METEO, a system for generating weather forecast texts was developed using a generic framework (jCOLIBRI) which provides modules for the standard components of the CBR architecture. The advantage in a CBR approach is that systems can be built in minimal time with far less human effort after initial consultation with experts. The approach depends heavily on the goodness of the retrieval and revision components of the CBR process. We evaluated CBRMETEO with NIST, an automated metric which has been shown to correlate well with human judgements for this domain. The system shows comparable performance with other NLG systems that perform the same task. version:1
arxiv-1509-00998 | Sampling-based Causal Inference in Cue Combination and its Neural Implementation | http://arxiv.org/abs/1509.00998 | id:1509.00998 author:Zhaofei Yu, Feng Chen, Jianwu Dong, Qionghai Dai category:cs.NE q-bio.NC  published:2015-09-03 summary:Causal inference in cue combination is to decide whether the cues have a single cause or multiple causes. Although the Bayesian causal inference model explains the problem of causal inference in cue combination successfully, how causal inference in cue combination could be implemented by neural circuits, is unclear. The existing method based on calculating log posterior ratio with variable elimination has the problem of being unrealistic and task-specific. In this paper, we take advantages of the special structure of the Bayesian causal inference model and propose a hierarchical inference algorithm based on importance sampling. A simple neural circuit is designed to implement the proposed inference algorithm. Theoretical analyses and experimental results demonstrate that our algorithm converges to the accurate value as the sample size goes to infinite. Moreover, the neural circuit we design can be easily generalized to implement inference for other problems, such as the multi-stimuli cause inference and the same-different judgment. version:1
arxiv-1509-00980 | Sequential Design for Ranking Response Surfaces | http://arxiv.org/abs/1509.00980 | id:1509.00980 author:Ruimeng Hu, Mike Ludkovski category:stat.ML q-fin.CP stat.CO  published:2015-09-03 summary:We propose and analyze sequential design methods for the problem of ranking several response surfaces. Namely, given $L \ge 2$ response surfaces over a continuous input space $\cal X$, the aim is to efficiently find the index of the minimal response across the entire $\cal X$. The response surfaces are not known and have to be noisily sampled one-at-a-time. This setting is motivated by stochastic control applications and requires joint experimental design both in space and response-index dimensions. To generate sequential design heuristics we investigate stepwise uncertainty reduction approaches, as well as sampling based on posterior classification complexity. We also make connections between our continuous-input formulation and the discrete framework of pure regret in multi-armed bandits. To model the response surfaces we utilize kriging surrogates. Several numerical examples using both synthetic data and an epidemics control problem are provided to illustrate our approach and the efficacy of respective adaptive designs. version:1
arxiv-1509-00967 | A Reconfigurable Mixed-signal Implementation of a Neuromorphic ADC | http://arxiv.org/abs/1509.00967 | id:1509.00967 author:Ying Xu, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Runchun Wang, Andre van Schaik category:cs.NE  published:2015-09-03 summary:We present a neuromorphic Analogue-to-Digital Converter (ADC), which uses integrate-and-fire (I&F) neurons as the encoders of the analogue signal, with modulated inhibitions to decohere the neuronal spikes trains. The architecture consists of an analogue chip and a control module. The analogue chip comprises two scan chains and a twodimensional integrate-and-fire neuronal array. Individual neurons are accessed via the chains one by one without any encoder decoder or arbiter. The control module is implemented on an FPGA (Field Programmable Gate Array), which sends scan enable signals to the scan chains and controls the inhibition for individual neurons. Since the control module is implemented on an FPGA, it can be easily reconfigured. Additionally, we propose a pulse width modulation methodology for the lateral inhibition, which makes use of different pulse widths indicating different strengths of inhibition for each individual neuron to decohere neuronal spikes. Software simulations in this paper tested the robustness of the proposed ADC architecture to fixed random noise. A circuit simulation using ten neurons shows the performance and the feasibility of the architecture. version:1
arxiv-1509-00963 | On TimeML-Compliant Temporal Expression Extraction in Turkish | http://arxiv.org/abs/1509.00963 | id:1509.00963 author:Dilek Küçük, Doğan Küçük category:cs.CL  published:2015-09-03 summary:It is commonly acknowledged that temporal expression extractors are important components of larger natural language processing systems like information retrieval and question answering systems. Extraction and normalization of temporal expressions in Turkish has not been given attention so far except the extraction of some date and time expressions within the course of named entity recognition. As TimeML is the current standard of temporal expression and event annotation in natural language texts, in this paper, we present an analysis of temporal expressions in Turkish based on the related TimeML classification (i.e., date, time, duration, and set expressions). We have created a lexicon for Turkish temporal expressions and devised considerably wide-coverage patterns using the lexical classes as the building blocks. We believe that the proposed patterns, together with convenient normalization rules, can be readily used by prospective temporal expression extraction tools for Turkish. version:1
arxiv-1509-00962 | A compact aVLSI conductance-based silicon neuron | http://arxiv.org/abs/1509.00962 | id:1509.00962 author:Runchun Wang, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik category:cs.NE  published:2015-09-03 summary:We present an analogue Very Large Scale Integration (aVLSI) implementation that uses first-order lowpass filters to implement a conductance-based silicon neuron for high-speed neuromorphic systems. The aVLSI neuron consists of a soma (cell body) and a single synapse, which is capable of linearly summing both the excitatory and inhibitory postsynaptic potentials (EPSP and IPSP) generated by the spikes arriving from different sources. Rather than biasing the silicon neuron with different parameters for different spiking patterns, as is typically done, we provide digital control signals, generated by an FPGA, to the silicon neuron to obtain different spiking behaviours. The proposed neuron is only ~26.5 um2 in the IBM 130nm process and thus can be integrated at very high density. Circuit simulations show that this neuron can emulate different spiking behaviours observed in biological neurons. version:1
arxiv-1501-05427 | Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE) | http://arxiv.org/abs/1501.05427 | id:1501.05427 author:Maurizio Filippone, Raphael Engler category:stat.ME stat.CO stat.ML  published:2015-01-22 summary:In applications of Gaussian processes where quantification of uncertainty is of primary interest, it is necessary to accurately characterize the posterior distribution over covariance parameters. This paper proposes an adaptation of the Stochastic Gradient Langevin Dynamics algorithm to draw samples from the posterior distribution over covariance parameters with negligible bias and without the need to compute the marginal likelihood. In Gaussian process regression, this has the enormous advantage that stochastic gradients can be computed by solving linear systems only. A novel unbiased linear systems solver based on parallelizable covariance matrix-vector products is developed to accelerate the unbiased estimation of gradients. The results demonstrate the possibility to enable scalable and exact (in a Monte Carlo sense) quantification of uncertainty in Gaussian processes without imposing any special structure on the covariance or reducing the number of input vectors. version:4
arxiv-1409-6813 | Histogram of Oriented Principal Components for Cross-View Action Recognition | http://arxiv.org/abs/1409.6813 | id:1409.6813 author:Hossein Rahmani, Arif Mahmood, Du Huynh, Ajmal Mian category:cs.CV  published:2014-09-24 summary:Existing techniques for 3D action recognition are sensitive to viewpoint variations because they extract features from depth images which are viewpoint dependent. In contrast, we directly process pointclouds for cross-view action recognition from unknown and unseen views. We propose the Histogram of Oriented Principal Components (HOPC) descriptor that is robust to noise, viewpoint, scale and action speed variations. At a 3D point, HOPC is computed by projecting the three scaled eigenvectors of the pointcloud within its local spatio-temporal support volume onto the vertices of a regular dodecahedron. HOPC is also used for the detection of Spatio-Temporal Keypoints (STK) in 3D pointcloud sequences so that view-invariant STK descriptors (or Local HOPC descriptors) at these key locations only are used for action recognition. We also propose a global descriptor computed from the normalized spatio-temporal distribution of STKs in 4-D, which we refer to as STK-D. We have evaluated the performance of our proposed descriptors against nine existing techniques on two cross-view and three single-view human action recognition datasets. The Experimental results show that our techniques provide significant improvement over state-of-the-art methods. version:2
arxiv-1406-1922 | Nonparametric Independence Testing for Small Sample Sizes | http://arxiv.org/abs/1406.1922 | id:1406.1922 author:Aaditya Ramdas, Leila Wehbe category:stat.ML  published:2014-06-07 summary:This paper deals with the problem of nonparametric independence testing, a fundamental decision-theoretic problem that asks if two arbitrary (possibly multivariate) random variables $X,Y$ are independent or not, a question that comes up in many fields like causality and neuroscience. While quantities like correlation of $X,Y$ only test for (univariate) linear independence, natural alternatives like mutual information of $X,Y$ are hard to estimate due to a serious curse of dimensionality. A recent approach, avoiding both issues, estimates norms of an \textit{operator} in Reproducing Kernel Hilbert Spaces (RKHSs). Our main contribution is strong empirical evidence that by employing \textit{shrunk} operators when the sample size is small, one can attain an improvement in power at low false positive rates. We analyze the effects of Stein shrinkage on a popular test statistic called HSIC (Hilbert-Schmidt Independence Criterion). Our observations provide insights into two recently proposed shrinkage estimators, SCOSE and FCOSE - we prove that SCOSE is (essentially) the optimal linear shrinkage method for \textit{estimating} the true operator; however, the non-linearly shrunk FCOSE usually achieves greater improvements in \textit{test power}. This work is important for more powerful nonparametric detection of subtle nonlinear dependencies for small samples. version:2
arxiv-1509-00816 | Depth Fields: Extending Light Field Techniques to Time-of-Flight Imaging | http://arxiv.org/abs/1509.00816 | id:1509.00816 author:Suren Jayasuriya, Adithya Pediredla, Sriram Sivaramakrishnan, Alyosha Molnar, Ashok Veeraraghavan category:cs.CV  published:2015-09-02 summary:A variety of techniques such as light field, structured illumination, and time-of-flight (TOF) are commonly used for depth acquisition in consumer imaging, robotics and many other applications. Unfortunately, each technique suffers from its individual limitations preventing robust depth sensing. In this paper, we explore the strengths and weaknesses of combining light field and time-of-flight imaging, particularly the feasibility of an on-chip implementation as a single hybrid depth sensor. We refer to this combination as depth field imaging. Depth fields combine light field advantages such as synthetic aperture refocusing with TOF imaging advantages such as high depth resolution and coded signal processing to resolve multipath interference. We show applications including synthesizing virtual apertures for TOF imaging, improved depth mapping through partial and scattering occluders, and single frequency TOF phase unwrapping. Utilizing space, angle, and temporal coding, depth fields can improve depth sensing in the wild and generate new insights into the dimensions of light's plenoptic function. version:1
arxiv-1508-05133 | Steps Toward Deep Kernel Methods from Infinite Neural Networks | http://arxiv.org/abs/1508.05133 | id:1508.05133 author:Tamir Hazan, Tommi Jaakkola category:cs.LG cs.NE  published:2015-08-20 summary:Contemporary deep neural networks exhibit impressive results on practical problems. These networks generalize well although their inherent capacity may extend significantly beyond the number of training examples. We analyze this behavior in the context of deep, infinite neural networks. We show that deep infinite layers are naturally aligned with Gaussian processes and kernel methods, and devise stochastic kernels that encode the information of these networks. We show that stability results apply despite the size, offering an explanation for their empirical success. version:2
arxiv-1508-05154 | Posterior calibration and exploratory analysis for natural language processing models | http://arxiv.org/abs/1508.05154 | id:1508.05154 author:Khanh Nguyen, Brendan O'Connor category:cs.CL  published:2015-08-21 summary:Many models in natural language processing define probabilistic distributions over linguistic structures. We argue that (1) the quality of a model' s posterior distribution can and should be directly evaluated, as to whether probabilities correspond to empirical frequencies, and (2) NLP uncertainty can be projected not only to pipeline components, but also to exploratory data analysis, telling a user when to trust and not trust the NLP analysis. We present a method to analyze calibration, and apply it to compare the miscalibration of several commonly used models. We also contribute a coreference sampling algorithm that can create confidence intervals for a political event extraction task. version:2
arxiv-1509-00764 | Finding Near-Optimal Independent Sets at Scale | http://arxiv.org/abs/1509.00764 | id:1509.00764 author:Sebastian Lamm, Peter Sanders, Christian Schulz, Darren Strash, Renato F. Werneck category:cs.DS cs.NE cs.SI F.2.2; G.2.2  published:2015-09-02 summary:The independent set problem is NP-hard and particularly difficult to solve in large sparse graphs. In this work, we develop an advanced evolutionary algorithm, which incorporates kernelization techniques to compute large independent sets in huge sparse networks. A recent exact algorithm has shown that large networks can be solved exactly by employing a branch-and-reduce technique that recursively kernelizes the graph and performs branching. However, one major drawback of their algorithm is that, for huge graphs, branching still can take exponential time. To avoid this problem, we recursively choose vertices that are likely to be in a large independent set (using an evolutionary approach), then further kernelize the graph. We show that identifying and removing vertices likely to be in large independent sets opens up the reduction space---which not only speeds up the computation of large independent sets drastically, but also enables us to compute high-quality independent sets on much larger instances than previously reported in the literature. version:1
arxiv-1406-6288 | Reliable ABC model choice via random forests | http://arxiv.org/abs/1406.6288 | id:1406.6288 author:Pierre Pudlo, Jean-Michel Marin, Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier, Christian P. Robert category:stat.ML q-bio.PE stat.CO stat.ME  published:2014-06-24 summary:Approximate Bayesian computation (ABC) methods provide an elaborate approach to Bayesian inference on complex models, including model choice. Both theoretical arguments and simulation experiments indicate, however, that model posterior probabilities may be poorly evaluated by standard ABC techniques. We propose a novel approach based on a machine learning tool named random forests to conduct selection among the highly complex models covered by ABC algorithms. We thus modify the way Bayesian model selection is both understood and operated, in that we rephrase the inferential goal as a classification problem, first predicting the model that best fits the data with random forests and postponing the approximation of the posterior probability of the predicted MAP for a second stage also relying on random forests. Compared with earlier implementations of ABC model choice, the ABC random forest approach offers several potential improvements: (i) it often has a larger discriminative power among the competing models, (ii) it is more robust against the number and choice of statistics summarizing the data, (iii) the computing effort is drastically reduced (with a gain in computation efficiency of at least fifty), and (iv) it includes an approximation of the posterior probability of the selected model. The call to random forests will undoubtedly extend the range of size of datasets and complexity of models that ABC can handle. We illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population genetics datasets. The proposed methodologies are implemented in the R package abcrf available on the CRAN. version:3
arxiv-1509-00728 | On Transitive Consistency for Linear Invertible Transformations between Euclidean Coordinate Systems | http://arxiv.org/abs/1509.00728 | id:1509.00728 author:Johan Thunberg, Florian Bernard, Jorge Goncalves category:math.OC cs.CV cs.MA cs.NA stat.ML  published:2015-09-02 summary:Transitive consistency is an intrinsic property for collections of linear invertible transformations between Euclidean coordinate frames. In practice, when the transformations are estimated from data, this property is lacking. This work addresses the problem of synchronizing transformations that are not transitively consistent. Once the transformations have been synchronized, they satisfy the transitive consistency condition - a transformation from frame $A$ to frame $C$ is equal to the composite transformation of first transforming A to B and then transforming B to C. The coordinate frames correspond to nodes in a graph and the transformations correspond to edges in the same graph. Two direct or centralized synchronization methods are presented for different graph topologies; the first one for quasi-strongly connected graphs, and the second one for connected graphs. As an extension of the second method, an iterative Gauss-Newton method is presented, which is later adapted to the case of affine and Euclidean transformations. Two distributed synchronization methods are also presented for orthogonal matrices, which can be seen as distributed versions of the two direct or centralized methods; they are similar in nature to standard consensus protocols used for distributed averaging. When the transformations are orthogonal matrices, a bound on the optimality gap can be computed. Simulations show that the gap is almost right, even for noise large in magnitude. This work also contributes on a theoretical level by providing linear algebraic relationships for transitively consistent transformations. One of the benefits of the proposed methods is their simplicity - basic linear algebraic methods are used, e.g., the Singular Value Decomposition (SVD). For a wide range of parameter settings, the methods are numerically validated. version:1
arxiv-1509-00727 | Heavy-tailed Independent Component Analysis | http://arxiv.org/abs/1509.00727 | id:1509.00727 author:Joseph Anderson, Navin Goyal, Anupama Nandi, Luis Rademacher category:cs.LG math.ST stat.CO stat.ML stat.TH  published:2015-09-02 summary:Independent component analysis (ICA) is the problem of efficiently recovering a matrix $A \in \mathbb{R}^{n\times n}$ from i.i.d. observations of $X=AS$ where $S \in \mathbb{R}^n$ is a random vector with mutually independent coordinates. This problem has been intensively studied, but all existing efficient algorithms with provable guarantees require that the coordinates $S_i$ have finite fourth moments. We consider the heavy-tailed ICA problem where we do not make this assumption, about the second moment. This problem also has received considerable attention in the applied literature. In the present work, we first give a provably efficient algorithm that works under the assumption that for constant $\gamma > 0$, each $S_i$ has finite $(1+\gamma)$-moment, thus substantially weakening the moment requirement condition for the ICA problem to be solvable. We then give an algorithm that works under the assumption that matrix $A$ has orthogonal columns but requires no moment assumptions. Our techniques draw ideas from convex geometry and exploit standard properties of the multivariate spherical Gaussian distribution in a novel way. version:1
arxiv-1502-03656 | Quasi-Newton particle Metropolis-Hastings | http://arxiv.org/abs/1502.03656 | id:1502.03656 author:Johan Dahlin, Fredrik Lindsten, Thomas B. Schön category:stat.CO q-fin.CP stat.ML  published:2015-02-12 summary:Particle Metropolis-Hastings enables Bayesian parameter inference in general nonlinear state space models (SSMs). However, in many implementations a random walk proposal is used and this can result in poor mixing if not tuned correctly using tedious pilot runs. Therefore, we consider a new proposal inspired by quasi-Newton algorithms that may achieve similar (or better) mixing with less tuning. An advantage compared to other Hessian based proposals, is that it only requires estimates of the gradient of the log-posterior. A possible application is parameter inference in the challenging class of SSMs with intractable likelihoods. We exemplify this application and the benefits of the new proposal by modelling log-returns of future contracts on coffee by a stochastic volatility model with $\alpha$-stable observations. version:2
arxiv-1509-00714 | Dictionary based Approach to Edge Detection | http://arxiv.org/abs/1509.00714 | id:1509.00714 author:Nitish Chandra, Kedar Khare category:cs.CV  published:2015-09-02 summary:Edge detection is a very essential part of image processing, as quality and accuracy of detection determines the success of further processing. We have developed a new self learning technique for edge detection using dictionary comprised of eigenfilters constructed using features of the input image. The dictionary based method eliminates the need of pre or post processing of the image and accounts for noise, blurriness, class of image and variation of illumination during the detection process itself. Since, this method depends on the characteristics of the image, the new technique can detect edges more accurately and capture greater detail than existing algorithms such as Sobel, Prewitt Laplacian of Gaussian, Canny method etc which use generic filters and operators. We have demonstrated its application on various classes of images such as text, face, barcodes, traffic and cell images. An application of this technique to cell counting in a microscopic image is also presented. version:1
arxiv-1509-00705 | Analysis of Communication Pattern with Scammers in Enron Corpus | http://arxiv.org/abs/1509.00705 | id:1509.00705 author:Dinesh Balaji Sashikanth category:cs.CL  published:2015-09-02 summary:This paper is an exploratory analysis into fraud detection taking Enron email corpus as the case study. The paper posits conclusions like strict servitude and unquestionable faith among employees as breeding grounds for sham among higher executives. We also try to infer on the nature of communication between fraudulent employees and between non- fraudulent-fraudulent employees version:1
arxiv-1508-01306 | Replication and Generalization of PRECISE | http://arxiv.org/abs/1508.01306 | id:1508.01306 author:Michael Minock, Nils Everling category:cs.CL cs.AI cs.DB H.5.2; I.2.1; I.2.7  published:2015-08-06 summary:This report describes an initial replication study of the PRECISE system and develops a clearer, more formal description of the approach. Based on our evaluation, we conclude that the PRECISE results do not fully replicate. However the formalization developed here suggests a road map to further enhance and extend the approach pioneered by PRECISE. After a long, productive discussion with Ana-Maria Popescu (one of the authors of PRECISE) we got more clarity on the PRECISE approach and how the lexicon was authored for the GEO evaluation. Based on this we built a more direct implementation over a repaired formalism. Although our new evaluation is not yet complete, it is clear that the system is performing much better now. We will continue developing our ideas and implementation and generate a future report/publication that more accurately evaluates PRECISE like approaches. version:2
arxiv-1508-06576 | A Neural Algorithm of Artistic Style | http://arxiv.org/abs/1508.06576 | id:1508.06576 author:Leon A. Gatys, Alexander S. Ecker, Matthias Bethge category:cs.CV cs.NE q-bio.NC  published:2015-08-26 summary:In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery. version:2
arxiv-1509-00595 | A hybrid COA-DEA method for solving multi-objective problems | http://arxiv.org/abs/1509.00595 | id:1509.00595 author:Mahdi Gorjestani, Elham Shadkam, Mehdi Parvizi, Sajedeh Aminzadegan category:math.OC cs.NE  published:2015-09-02 summary:The Cuckoo optimization algorithm (COA) is developed for solving single-objective problems and it cannot be used for solving multi-objective problems. So the multi-objective cuckoo optimization algorithm based on data envelopment analysis (DEA) is developed in this paper and it can gain the efficient Pareto frontiers. This algorithm is presented by the CCR model of DEA and the output-oriented approach of it. The selection criterion is higher efficiency for next iteration of the proposed hybrid method. So the profit function of the COA is replaced by the efficiency value that is obtained from DEA. This algorithm is compared with other methods using some test problems. The results shows using COA and DEA approach for solving multi-objective problems increases the speed and the accuracy of the generated solutions. version:1
arxiv-1509-00568 | Exploring Online Ad Images Using a Deep Convolutional Neural Network Approach | http://arxiv.org/abs/1509.00568 | id:1509.00568 author:Michael Fire, Jonathan Schler category:cs.CV  published:2015-09-02 summary:Online advertising is a huge, rapidly growing advertising market in today's world. One common form of online advertising is using image ads. A decision is made (often in real time) every time a user sees an ad, and the advertiser is eager to determine the best ad to display. Consequently, many algorithms have been developed that calculate the optimal ad to show to the current user at the present time. Typically, these algorithms focus on variations of the ad, optimizing among different properties such as background color, image size, or set of images. However, there is a more fundamental layer. Our study looks at new qualities of ads that can be determined before an ad is shown (rather than online optimization) and defines which ads are most likely to be successful. We present a set of novel algorithms that utilize deep-learning image processing, machine learning, and graph theory to investigate online advertising and to construct prediction models which can foresee an image ad's success. We evaluated our algorithms on a dataset with over 260,000 ad images, as well as a smaller dataset specifically related to the automotive industry, and we succeeded in constructing regression models for ad image click rate prediction. The obtained results emphasize the great potential of using deep-learning algorithms to effectively and efficiently analyze image ads and to create better and more innovative online ads. Moreover, the algorithms presented in this paper can help predict ad success and can be applied to analyze other large-scale image corpora. version:1
arxiv-1405-2875 | Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems | http://arxiv.org/abs/1405.2875 | id:1405.2875 author:Chien-Ju Ho, Aleksandrs Slivkins, Jennifer Wortman Vaughan category:cs.DS cs.GT cs.LG  published:2014-05-12 summary:Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task's requester, and may be adjusted based on the quality of the completed work, for example, through the use of "bonus" payments. In this paper, we study the requester's problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each "arm" representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature). Our results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing. version:2
arxiv-1501-02627 | A fast numerical method for max-convolution and the application to efficient max-product inference in Bayesian networks | http://arxiv.org/abs/1501.02627 | id:1501.02627 author:Oliver Serang category:cs.NA math.NA stat.CO stat.ME stat.ML  published:2015-01-12 summary:Observations depending on sums of random variables are common throughout many fields; however, no efficient solution is currently known for performing max-product inference on these sums of general discrete distributions (max-product inference can be used to obtain maximum a posteriori estimates). The limiting step to max-product inference is the max-convolution problem (sometimes presented in log-transformed form and denoted as "infimal convolution", "min-convolution", or "convolution on the tropical semiring"), for which no O(k log(k)) method is currently known. Here I present a O(k log(k)) numerical method for estimating the max-convolution of two nonnegative vectors (e.g., two probability mass functions), where k is the length of the larger vector. This numerical max-convolution method is then demonstrated by performing fast max-product inference on a convolution tree, a data structure for performing fast inference given information on the sum of n discrete random variables in O(n k log(n k) log(n) ) steps (where each random variable has an arbitrary prior distribution on k contiguous possible states). The numerical max-convolution method can be applied to specialized classes of hidden Markov models to reduce the runtime of computing the Viterbi path from n k^2 to n k log(k), and has potential application to the all-pairs shortest paths problem. version:2
arxiv-1509-00533 | Enhancement and Recognition of Reverberant and Noisy Speech by Extending Its Coherence | http://arxiv.org/abs/1509.00533 | id:1509.00533 author:Scott Wisdom, Thomas Powers, Les Atlas, James Pitton category:cs.SD cs.CL stat.AP  published:2015-09-02 summary:Most speech enhancement algorithms make use of the short-time Fourier transform (STFT), which is a simple and flexible time-frequency decomposition that estimates the short-time spectrum of a signal. However, the duration of short STFT frames are inherently limited by the nonstationarity of speech signals. The main contribution of this paper is a demonstration of speech enhancement and automatic speech recognition in the presence of reverberation and noise by extending the length of analysis windows. We accomplish this extension by performing enhancement in the short-time fan-chirp transform (STFChT) domain, an overcomplete time-frequency representation that is coherent with speech signals over longer analysis window durations than the STFT. This extended coherence is gained by using a linear model of fundamental frequency variation of voiced speech signals. Our approach centers around using a single-channel minimum mean-square error log-spectral amplitude (MMSE-LSA) estimator proposed by Habets, which scales coefficients in a time-frequency domain to suppress noise and reverberation. In the case of multiple microphones, we preprocess the data with either a minimum variance distortionless response (MVDR) beamformer, or a delay-and-sum beamformer (DSB). We evaluate our algorithm on both speech enhancement and recognition tasks for the REVERB challenge dataset. Compared to the same processing done in the STFT domain, our approach achieves significant improvement in terms of objective enhancement metrics (including PESQ---the ITU-T standard measurement for speech quality). In terms of automatic speech recognition (ASR) performance as measured by word error rate (WER), our experiments indicate that the STFT with a long window is more effective for ASR. version:1
arxiv-1509-00498 | Sensor-Type Classification in Buildings | http://arxiv.org/abs/1509.00498 | id:1509.00498 author:Dezhi Hong, Jorge Ortiz, Arka Bhattacharya, Kamin Whitehouse category:cs.LG C.3  published:2015-09-01 summary:Many sensors/meters are deployed in commercial buildings to monitor and optimize their performance. However, because sensor metadata is inconsistent across buildings, software-based solutions are tightly coupled to the sensor metadata conventions (i.e. schemas and naming) for each building. Running the same software across buildings requires significant integration effort. Metadata normalization is critical for scaling the deployment process and allows us to decouple building-specific conventions from the code written for building applications. It also allows us to deal with missing metadata. One important aspect of normalization is to differentiate sensors by the typeof phenomena being observed. In this paper, we propose a general, simple, yet effective classification scheme to differentiate sensors in buildings by type. We perform ensemble learning on data collected from over 2000 sensor streams in two buildings. Our approach is able to achieve more than 92% accuracy for classification within buildings and more than 82% accuracy for across buildings. We also introduce a method for identifying potential misclassified streams. This is important because it allows us to identify opportunities to attain more input from experts -- input that could help improve classification accuracy when ground truth is unavailable. We show that by adjusting a threshold value we are able to identify at least 30% of the misclassified instances. version:1
arxiv-1411-3224 | On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence | http://arxiv.org/abs/1411.3224 | id:1411.3224 author:Nathaniel Korda, L. A. Prashanth category:cs.LG math.OC stat.ML  published:2014-11-12 summary:We provide non-asymptotic bounds for the well-known temporal difference learning algorithm TD(0) with linear function approximators. These include high-probability bounds as well as bounds in expectation. Our analysis suggests that a step-size inversely proportional to the number of iterations cannot guarantee optimal rate of convergence unless we assume (partial) knowledge of the stationary distribution for the Markov chain underlying the policy considered. We also provide bounds for the iterate averaged TD(0) variant, which gets rid of the step-size dependency while exhibiting the optimal rate of convergence. Furthermore, we propose a variant of TD(0) with linear approximators that incorporates a centering sequence, and establish that it exhibits an exponential rate of convergence in expectation. We demonstrate the usefulness of our bounds on two synthetic experimental settings. version:2
arxiv-1412-5126 | A Robust Regression Approach for Background/Foreground Segmentation | http://arxiv.org/abs/1412.5126 | id:1412.5126 author:Shervin Minaee, Haoping Yu, Yao Wang category:cs.CV  published:2014-12-16 summary:Background/foreground segmentation has a lot of applications in image and video processing. In this paper, a segmentation algorithm is proposed which is mainly designed for text and line extraction in screen content. The proposed method makes use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. The algorithm separates the background and foreground pixels by trying to fit pixel values in the block into a smooth function using a robust regression method. The inlier pixels that can fit well will be considered as background, while remaining outlier pixels will be considered foreground. This algorithm has been extensively tested on several images from HEVC standard test sequences for screen content coding, and is shown to have superior performance over other methods, such as the k-means clustering based segmentation algorithm in DjVu. This background/foreground segmentation can be used in different applications such as: text extraction, separate coding of background and foreground for compression of screen content and mixed content documents, principle line extraction from palmprint and crease detection in fingerprint images. version:2
arxiv-1506-06204 | Learning to Segment Object Candidates | http://arxiv.org/abs/1506.06204 | id:1506.06204 author:Pedro O. Pinheiro, Ronan Collobert, Piotr Dollar category:cs.CV  published:2015-06-20 summary:Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation. version:2
arxiv-1509-00313 | Iterative hypothesis testing for multi-object tracking in presence of features with variable reliability | http://arxiv.org/abs/1509.00313 | id:1509.00313 author:Amit Kumar K. C., Damien Delannay, Christophe De Vleeschouwer category:cs.CV  published:2015-09-01 summary:This paper assumes prior detections of multiple targets at each time instant, and uses a graph-based approach to connect those detections across time, based on their position and appearance estimates. In contrast to most earlier works in the field, our framework has been designed to exploit the appearance features, even when they are only sporadically available, or affected by a non-stationary noise, along the sequence of detections. This is done by implementing an iterative hypothesis testing strategy to progressively aggregate the detections into short trajectories, named tracklets. Specifically, each iteration considers a node, named key-node, and investigates how to link this key-node with other nodes in its neighborhood, under the assumption that the target appearance is defined by the key-node appearance estimate. This is done through shortest path computation in a temporal neighborhood of the key-node. The approach is conservative in that it only aggregates the shortest paths that are sufficiently better compared to alternative paths. It is also multi-scale in that the size of the investigated neighborhood is increased proportionally to the number of detections already aggregated into the key-node. The multi-scale nature of the process and the progressive relaxation of its conservativeness makes it both computationally efficient and effective. Experimental validations are performed extensively on a toy example, a 15 minutes long multi-view basketball dataset, and other monocular pedestrian datasets. version:1
arxiv-1509-00296 | Fast Randomized Singular Value Thresholding for Nuclear Norm Minimization | http://arxiv.org/abs/1509.00296 | id:1509.00296 author:Tae-Hyun Oh, Yasuyuki Matsushita, Yu-Wing Tai, In So Kweon category:cs.CV  published:2015-09-01 summary:Rank minimization can be boiled down to tractable surrogate problems, such as Nuclear Norm Minimization (NNM) and Weighted NNM (WNNM). The problems related to NNM (or WNNM) can be solved iteratively by applying a closed-form proximal operator, called Singular Value Thresholding (SVT) (or Weighted SVT), but they suffer from high computational cost of computing Singular Value Decomposition (SVD) at each iteration. We propose a fast and accurate approximation method for SVT, that we call fast randomized SVT (FRSVT), where we avoid direct computation of SVD. The key idea is to extract an approximate basis for the range of a matrix from its compressed matrix. Given the basis, we compute the partial singular values of the original matrix from a small factored matrix. In addition, by adopting a range propagation technique, our method further speeds up the extraction of approximate basis at each iteration. Our theoretical analysis shows the relationship between the approximation bound of SVD and its effect to NNM via SVT. Along with the analysis, our empirical results quantitatively and qualitatively show that our approximation rarely harms the convergence of the host algorithms. We assess the efficiency and accuracy of our method on various vision problems, e.g., subspace clustering, weather artifact removal, and simultaneous multi-image alignment and rectification. version:1
arxiv-1509-00244 | Robust Face Recognition via Multimodal Deep Face Representation | http://arxiv.org/abs/1509.00244 | id:1509.00244 author:Changxing Ding, Dacheng Tao category:cs.CV  published:2015-09-01 summary:Face images appeared in multimedia applications, e.g., social networks and digital entertainment, usually exhibit dramatic pose, illumination, and expression variations, resulting in considerable performance degradation for traditional face recognition algorithms. This paper proposes a comprehensive deep learning framework to jointly learn face representation using multimodal information. The proposed deep learning structure is composed of a set of elaborately designed convolutional neural networks (CNNs) and a three-layer stacked auto-encoder (SAE). The set of CNNs extracts complementary facial features from multimodal data. Then, the extracted features are concatenated to form a high-dimensional feature vector, whose dimension is compressed by SAE. All the CNNs are trained using a subset of 9,000 subjects from the publicly available CASIA-WebFace database, which ensures the reproducibility of this work. Using the proposed single CNN architecture and limited training data, 98.43% verification rate is achieved on the LFW database. Benefited from the complementary information contained in multimodal data, our small ensemble system achieves higher than 99.0% recognition rate on LFW using publicly available training set. version:1
arxiv-1509-00202 | Fingerprinting-Based Positioning in Distributed Massive MIMO Systems | http://arxiv.org/abs/1509.00202 | id:1509.00202 author:Vladimir Savic, Erik G. Larsson category:cs.IT cs.LG math.IT  published:2015-09-01 summary:Location awareness in wireless networks may enable many applications such as emergency services, autonomous driving and geographic routing. Although there are many available positioning techniques, none of them is adapted to work with massive multiple-in-multiple-out (MIMO) systems, which represent a leading 5G technology candidate. In this paper, we discuss possible solutions for positioning of mobile stations using a vector of signals at the base station, equipped with many antennas distributed over deployment area. Our main proposal is to use fingerprinting techniques based on a vector of received signal strengths. This kind of methods are able to work in highly-cluttered multipath environments, and require just one base station, in contrast to standard range-based and angle-based techniques. We also provide a solution for fingerprinting-based positioning based on Gaussian process regression, and discuss main applications and challenges. version:1
arxiv-1507-02592 | Fast rates in statistical and online learning | http://arxiv.org/abs/1507.02592 | id:1507.02592 author:Tim van Erven, Peter D. Grünwald, Nishant A. Mehta, Mark D. Reid, Robert C. Williamson category:cs.LG stat.ML  published:2015-07-09 summary:The speed with which a learning algorithm converges as it is presented with more data is a central problem in machine learning --- a fast rate of convergence means less data is needed for the same level of performance. The pursuit of fast rates in online and statistical learning has led to the discovery of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for 'proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that under surprisingly weak assumptions both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov margin condition, both of which have played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying conditions thus provide a substantial step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting. version:2
arxiv-1509-00692 | Discovery of Web Usage Profiles Using Various Clustering Techniques | http://arxiv.org/abs/1509.00692 | id:1509.00692 author:Zahid Ansari, Waseem Ahmed, M. F. Azeem, A. Vinaya Babu category:cs.DB cs.IR cs.LG  published:2015-09-01 summary:The explosive growth of World Wide Web (WWW) has necessitated the development of Web personalization systems in order to understand the user preferences to dynamically serve customized content to individual users. To reveal information about user preferences from Web usage data, Web Usage Mining (WUM) techniques are extensively being applied to the Web log data. Clustering techniques are widely used in WUM to capture similar interests and trends among users accessing a Web site. Clustering aims to divide a data set into groups or clusters where inter-cluster similarities are minimized while the intra cluster similarities are maximized. This paper reviews four of the popularly used clustering techniques: k-Means, k-Medoids, Leader and DBSCAN. These techniques are implemented and tested against the Web user navigational data. Performance and validity results of each technique are presented and compared. version:1
arxiv-1410-5358 | Remote sensing image classification exploiting multiple kernel learning | http://arxiv.org/abs/1410.5358 | id:1410.5358 author:Claudio Cusano, Paolo Napoletano, Raimondo Schettini category:cs.CV  published:2014-10-20 summary:We propose a strategy for land use classification which exploits Multiple Kernel Learning (MKL) to automatically determine a suitable combination of a set of features without requiring any heuristic knowledge about the classification task. We present a novel procedure that allows MKL to achieve good performance in the case of small training sets. Experimental results on publicly available datasets demonstrate the feasibility of the proposed approach. version:3
arxiv-1509-00174 | A Telescopic Binary Learning Machine for Training Neural Networks | http://arxiv.org/abs/1509.00174 | id:1509.00174 author:Mauro Brunato, Roberto Battiti category:cs.NE I.2.6  published:2015-09-01 summary:This paper proposes a new algorithm based on multi-scale stochastic local search with binary representation for training neural networks. In particular, we study the effects of neighborhood evaluation strategies, the effect of the number of bits per weight and that of the maximum weight range used for mapping binary strings to real values. Following this preliminary investigation, we propose a telescopic multi-scale version of local search where the number of bits is increased in an adaptive manner, leading to a faster search and to local minima of better quality. An analysis related to adapting the number of bits in a dynamic way is also presented. The control on the number of bits, which happens in a natural manner in the proposed method, is effective to increase the generalization performance. Benchmark tasks include a highly non-linear artificial problem, a control problem requiring either feed-forward or recurrent architectures for feedback control, and challenging real-world tasks in different application domains. The results demonstrate the effectiveness of the proposed method. version:1
arxiv-1412-0620 | Low-Rank Approximation and Completion of Positive Tensors | http://arxiv.org/abs/1412.0620 | id:1412.0620 author:Anil Aswani category:math.ST cs.LG stat.TH  published:2014-12-01 summary:Unlike the matrix case, computing low-rank approximations of tensors is NP-hard and numerically ill-posed in general. Even the best rank-1 approximation of a tensor is NP-hard. In this paper, we use convex optimization to develop polynomial-time algorithms for low-rank approximation and completion of positive tensors. Our approach is to use algebraic topology to define a new (numerically well-posed) decomposition for positive tensors, which we show is equivalent to the standard tensor decomposition in important cases. Though computing this decomposition is a nonconvex optimization problem, we prove it can be exactly reformulated as a convex optimization problem. This allows us to construct polynomial-time randomized algorithms for computing this decomposition and for solving low-rank tensor approximation problems. Among the consequences is that best rank-1 approximations of positive tensors can be computed in polynomial time. Our framework is next extended to the tensor completion problem, where noisy entries of a tensor are observed and then used to estimate missing entries. We provide a polynomial-time algorithm that requires a polynomial (in tensor order) number of measurements, in contrast to existing approaches that require an exponential number of measurements for specific cases. These algorithms are extended to exploit sparsity in the tensor to reduce the number of measurements needed. We conclude by providing a novel interpretation of statistical regression problems with categorical variables as tensor completion problems, and numerical examples with synthetic data and data from a bioengineered metabolic network show the improved performance of our approach on this problem. version:4
arxiv-1509-00137 | Online Supervised Subspace Tracking | http://arxiv.org/abs/1509.00137 | id:1509.00137 author:Yao Xie, Ruiyang Song, Hanjun Dai, Qingbin Li, Le Song category:cs.LG math.ST stat.ML stat.TH  published:2015-09-01 summary:We present a framework for supervised subspace tracking, when there are two time series $x_t$ and $y_t$, one being the high-dimensional predictors and the other being the response variables and the subspace tracking needs to take into consideration of both sequences. It extends the classic online subspace tracking work which can be viewed as tracking of $x_t$ only. Our online sufficient dimensionality reduction (OSDR) is a meta-algorithm that can be applied to various cases including linear regression, logistic regression, multiple linear regression, multinomial logistic regression, support vector machine, the random dot product model and the multi-scale union-of-subspace model. OSDR reduces data-dimensionality on-the-fly with low-computational complexity and it can also handle missing data and dynamic data. OSDR uses an alternating minimization scheme and updates the subspace via gradient descent on the Grassmannian manifold. The subspace update can be performed efficiently utilizing the fact that the Grassmannian gradient with respect to the subspace in many settings is rank-one (or low-rank in certain cases). The optimization problem for OSDR is non-convex and hard to analyze in general; we provide convergence analysis of OSDR in a simple linear regression setting. The good performance of OSDR compared with the conventional unsupervised subspace tracking are demonstrated via numerical examples on simulated and real data. version:1
arxiv-1509-00130 | Sequential Information Guided Sensing | http://arxiv.org/abs/1509.00130 | id:1509.00130 author:Ruiyang Song, Yao Xie, Sebastian Pokutta category:cs.IT math.IT math.ST stat.ML stat.TH  published:2015-09-01 summary:We study the value of information in sequential compressed sensing by characterizing the performance of sequential information guided sensing in practical scenarios when information is inaccurate. In particular, we assume the signal distribution is parameterized through Gaussian or Gaussian mixtures with estimated mean and covariance matrices, and we can measure compressively through a noisy linear projection or using one-sparse vectors, i.e., observing one entry of the signal each time. We establish a set of performance bounds for the bias and variance of the signal estimator via posterior mean, by capturing the conditional entropy (which is also related to the size of the uncertainty), and the additional power required due to inaccurate information to reach a desired precision. Based on this, we further study how to estimate covariance based on direct samples or covariance sketching. Numerical examples also demonstrate the superior performance of Info-Greedy Sensing algorithms compared with their random and non-adaptive counterparts. version:1
arxiv-1212-3913 | Group Component Analysis for Multi-block Data: Common and Individual Feature Extraction | http://arxiv.org/abs/1212.3913 | id:1212.3913 author:Guoxu Zhou, Andrzej Cichocki, Yu Zhang, Danilo Mandic category:cs.CV cs.LG  published:2012-12-17 summary:Very often data we encounter in practice is a collection of matrices rather than a single matrix. These multi-block data are naturally linked and hence often share some common features and at the same time they have their own individual features, due to the background in which they are measured and collected. In this study we proposed a new scheme of common and individual feature analysis (CIFA) that processes multi-block data in a linked way aiming at discovering and separating their common and individual features. According to whether the number of common features is given or not, two efficient algorithms were proposed to extract the common basis which is shared by all data. Then feature extraction is performed on the common and the individual spaces separately by incorporating the techniques such as dimensionality reduction and blind source separation. We also discussed how the proposed CIFA can significantly improve the performance of classification and clustering tasks by exploiting common and individual features of samples respectively. Our experimental results show some encouraging features of the proposed methods in comparison to the state-of-the-art methods on synthetic and real data. version:3
arxiv-1509-00105 | Evolving Unipolar Memristor Spiking Neural Networks | http://arxiv.org/abs/1509.00105 | id:1509.00105 author:David Howard, Larry Bull, Ben De Lacy Costello category:cs.NE  published:2015-09-01 summary:Neuromorphic computing --- brainlike computing in hardware --- typically requires myriad CMOS spiking neurons interconnected by a dense mesh of nanoscale plastic synapses. Memristors are frequently citepd as strong synapse candidates due to their statefulness and potential for low-power implementations. To date, plentiful research has focused on the bipolar memristor synapse, which is capable of incremental weight alterations and can provide adaptive self-organisation under a Hebbian learning scheme. In this paper we consider the Unipolar memristor synapse --- a device capable of non-Hebbian switching between only two states (conductive and resistive) through application of a suitable input voltage --- and discuss its suitability for neuromorphic systems. A self-adaptive evolutionary process is used to autonomously find highly fit network configurations. Experimentation on a two robotics tasks shows that unipolar memristor networks evolve task-solving controllers faster than both bipolar memristor networks and networks containing constant nonplastic connections whilst performing at least comparably. version:1
arxiv-1509-00083 | Metastatic liver tumour segmentation from discriminant Grassmannian manifolds | http://arxiv.org/abs/1509.00083 | id:1509.00083 author:Samuel Kadoury, Eugene Vorontsov, An Tang category:cs.LG cs.CV  published:2015-08-31 summary:The early detection, diagnosis and monitoring of liver cancer progression can be achieved with the precise delineation of metastatic tumours. However, accurate automated segmentation remains challenging due to the presence of noise, inhomogeneity and the high appearance variability of malignant tissue. In this paper, we propose an unsupervised metastatic liver tumour segmentation framework using a machine learning approach based on discriminant Grassmannian manifolds which learns the appearance of tumours with respect to normal tissue. First, the framework learns within-class and between-class similarity distributions from a training set of images to discover the optimal manifold discrimination between normal and pathological tissue in the liver. Second, a conditional optimisation scheme computes nonlocal pairwise as well as pattern-based clique potentials from the manifold subspace to recognise regions with similar labelings and to incorporate global consistency in the segmentation process. The proposed framework was validated on a clinical database of 43 CT images from patients with metastatic liver cancer. Compared to state-of-the-art methods, our method achieves a better performance on two separate datasets of metastatic liver tumours from different clinical sites, yielding an overall mean Dice similarity coefficient of 90.7 +/- 2.4 in over 50 tumours with an average volume of 27.3 mm3. version:1
arxiv-1509-00061 | Value function approximation via low-rank models | http://arxiv.org/abs/1509.00061 | id:1509.00061 author:Hao Yi Ong category:cs.LG cs.AI  published:2015-08-31 summary:We propose a novel value function approximation technique for Markov decision processes. We consider the problem of compactly representing the state-action value function using a low-rank and sparse matrix model. The problem is to decompose a matrix that encodes the true value function into low-rank and sparse components, and we achieve this using Robust Principal Component Analysis (PCA). Under minimal assumptions, this Robust PCA problem can be solved exactly via the Principal Component Pursuit convex optimization problem. We experiment the procedure on several examples and demonstrate that our method yields approximations essentially identical to the true function. version:1
arxiv-1509-00028 | Pure and Hybrid Evolutionary Computing in Global Optimization of Chemical Structures: from Atoms and Molecules to Clusters and Crystals | http://arxiv.org/abs/1509.00028 | id:1509.00028 author:Kanchan Sarkar, S. P. Bhattacharyya category:cond-mat.mtrl-sci cs.NE physics.chem-ph  published:2015-08-31 summary:The growth of evolutionary computing (EC) methods in the exploration of complex potential energy landscapes of atomic and molecular clusters, as well as crystals over the last decade or so is reviewed. The trend of growth indicates that pure as well as hybrid evolutionary computing techniques in conjunction of DFT has been emerging as a powerful tool, although work on molecular clusters has been rather limited so far. Some attempts to solve the atomic/molecular Schrodinger Equation (SE) directly by genetic algorithms (GA) are available in literature. At the Born-Oppenheimer level of approximation GA-density methods appear to be a viable tool which could be more extensively explored in the coming years, specially in the context of designing molecules and materials with targeted properties. version:1
arxiv-1508-07953 | Approximate Nearest Neighbor Fields in Video | http://arxiv.org/abs/1508.07953 | id:1508.07953 author:Nir Ben-Zrihem, Lihi Zelnik-Manor category:cs.CV  published:2015-08-31 summary:We introduce RIANN (Ring Intersection Approximate Nearest Neighbor search), an algorithm for matching patches of a video to a set of reference patches in real-time. For each query, RIANN finds potential matches by intersecting rings around key points in appearance space. Its search complexity is reversely correlated to the amount of temporal change, making it a good fit for videos, where typically most patches change slowly with time. Experiments show that RIANN is up to two orders of magnitude faster than previous ANN methods, and is the only solution that operates in real-time. We further demonstrate how RIANN can be used for real-time video processing and provide examples for a range of real-time video applications, including colorization, denoising, and several artistic effects. version:1
arxiv-1310-7300 | Relax but stay in control: from value to algorithms for online Markov decision processes | http://arxiv.org/abs/1310.7300 | id:1310.7300 author:Peng Guan, Maxim Raginsky, Rebecca Willett category:cs.LG math.OC stat.ML  published:2013-10-28 summary:Online learning algorithms are designed to perform in non-stationary environments, but generally there is no notion of a dynamic state to model constraints on current and future actions as a function of past actions. State-based models are common in stochastic control settings, but commonly used frameworks such as Markov Decision Processes (MDPs) assume a known stationary environment. In recent years, there has been a growing interest in combining the above two frameworks and considering an MDP setting in which the cost function is allowed to change arbitrarily after each time step. However, most of the work in this area has been algorithmic: given a problem, one would develop an algorithm almost from scratch. Moreover, the presence of the state and the assumption of an arbitrarily varying environment complicate both the theoretical analysis and the development of computationally efficient methods. This paper describes a broad extension of the ideas proposed by Rakhlin et al. to give a general framework for deriving algorithms in an MDP setting with arbitrarily changing costs. This framework leads to a unifying view of existing methods and provides a general procedure for constructing new ones. Several new methods are presented, and one of them is shown to have important advantages over a similar method developed from scratch via an online version of approximate dynamic programming. version:2
arxiv-1508-07859 | Multi-Projector Color Structured-Light Vision | http://arxiv.org/abs/1508.07859 | id:1508.07859 author:Changsoo Je, Kwang Hee Lee, Sang Wook Lee category:cs.CV cs.GR physics.optics I.2.10; I.4.8  published:2015-08-31 summary:Research interest in rapid structured-light imaging has grown increasingly for the modeling of moving objects, and a number of methods have been suggested for the range capture in a single video frame. The imaging area of a 3D object using a single projector is restricted since the structured light is projected only onto a limited area of the object surface. Employing additional projectors to broaden the imaging area is a challenging problem since simultaneous projection of multiple patterns results in their superposition in the light-intersected areas and the recognition of original patterns is by no means trivial. This paper presents a novel method of multi-projector color structured-light vision based on projector-camera triangulation. By analyzing the behavior of superposed-light colors in a chromaticity domain, we show that the original light colors cannot be properly extracted by the conventional direct estimation. We disambiguate multiple projectors by multiplexing the orientations of projector patterns so that the superposed patterns can be separated by explicit derivative computations. Experimental studies are carried out to demonstrate the validity of the presented method. The proposed method increases the efficiency of range acquisition compared to conventional active stereo using multiple projectors. version:1
arxiv-1507-02971 | Scalable MCMC for Large Data Problems using Data Subsampling and the Difference Estimator | http://arxiv.org/abs/1507.02971 | id:1507.02971 author:Matias Quiroz, Mattias Villani, Robert Kohn category:stat.ME stat.CO stat.ML  published:2015-07-10 summary:We propose a generic Markov Chain Monte Carlo (MCMC) algorithm to speed up computations for datasets with many observations. A key feature of our approach is the use of the highly efficient difference estimator from the survey sampling literature to estimate the log-likelihood accurately using only a small fraction of the data. Our algorithm improves on the $O(n)$ complexity of regular MCMC by operating over local data clusters instead of the full sample when computing the likelihood. The likelihood estimate is used in a Pseudo-marginal framework to sample from a perturbed posterior which is within $O(m^{-1/2})$ of the true posterior, where $m$ is the subsample size. The method is applied to a logistic regression model to predict firm bankruptcy for a large data set. We document a significant speed up in comparison to the standard MCMC on the full dataset. version:2
arxiv-1508-07753 | Bayesian Networks for Variable Groups | http://arxiv.org/abs/1508.07753 | id:1508.07753 author:Pekka Parviainen, Samuel Kaski category:stat.ML cs.AI  published:2015-08-31 summary:Bayesian networks, and especially their structures, are powerful tools for representing conditional independencies and dependencies between random variables. In applications where related variables form a priori known groups, chosen to represent different "views" to or aspects of the same entities, one may be more interested in modeling dependencies between groups of variables rather than between individual variables. Motivated by this, we study prospects of representing relationships between variable groups using Bayesian network structures. We show that for dependency structures between groups to be learnable, the data have to satisfy the so-called groupwise faithfulness assumption. We also show that one cannot learn causal relations between groups using only groupwise conditional independencies, but also variable-wise relations are needed. Additionally, we present algorithms for finding the groupwise dependency structures. version:1
arxiv-1508-07741 | Model Guided Sampling Optimization for Low-dimensional Problems | http://arxiv.org/abs/1508.07741 | id:1508.07741 author:Lukas Bajer, Martin Holena category:cs.NE stat.ML  published:2015-08-31 summary:Optimization of very expensive black-box functions requires utilization of maximum information gathered by the process of optimization. Model Guided Sampling Optimization (MGSO) forms a more robust alternative to Jones' Gaussian-process-based EGO algorithm. Instead of EGO's maximizing expected improvement, the MGSO uses sampling the probability of improvement which is shown to be helpful against trapping in local minima. Further, the MGSO can reach close-to-optimum solutions faster than standard optimization algorithms on low dimensional or smooth problems. version:1
arxiv-1508-07700 | A Cognitive Architecture Based on a Learning Classifier System with Spiking Classifiers | http://arxiv.org/abs/1508.07700 | id:1508.07700 author:David Howard, Larry Bull, Pier-Luca Lanzi category:cs.NE  published:2015-08-31 summary:Learning Classifier Systems (LCS) are population-based reinforcement learners that were originally designed to model various cognitive phenomena. This paper presents an explicitly cognitive LCS by using spiking neural networks as classifiers, providing each classifier with a measure of temporal dynamism. We employ a constructivist model of growth of both neurons and synaptic connections, which permits a Genetic Algorithm (GA) to automatically evolve sufficiently-complex neural structures. The spiking classifiers are coupled with a temporally-sensitive reinforcement learning algorithm, which allows the system to perform temporal state decomposition by appropriately rewarding "macro-actions," created by chaining together multiple atomic actions. The combination of temporal reinforcement learning and neural information processing is shown to outperform benchmark neural classifier systems, and successfully solve a robotic navigation task. version:1
arxiv-1508-07680 | Domain Generalization for Object Recognition with Multi-task Autoencoders | http://arxiv.org/abs/1508.07680 | id:1508.07680 author:Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, David Balduzzi category:cs.CV  published:2015-08-31 summary:The problem of domain generalization is to take knowledge acquired from a number of related domains where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition. Our algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter-domain variability in the appearance of objects. Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learnt features are then used as inputs to a classifier. We evaluated the performance of the algorithm on benchmark image recognition datasets, where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets. We found that (denoising) MTAE outperforms alternative autoencoder-based models as well as the current state-of-the-art algorithms for domain generalization. version:1
arxiv-1508-07678 | Online Model Evaluation in a Large-Scale Computational Advertising Platform | http://arxiv.org/abs/1508.07678 | id:1508.07678 author:Shahriar Shariat, Burkay Orten, Ali Dasdan category:cs.AI stat.ME stat.ML  published:2015-08-31 summary:Online media provides opportunities for marketers through which they can deliver effective brand messages to a wide range of audiences. Advertising technology platforms enable advertisers to reach their target audience by delivering ad impressions to online users in real time. In order to identify the best marketing message for a user and to purchase impressions at the right price, we rely heavily on bid prediction and optimization models. Even though the bid prediction models are well studied in the literature, the equally important subject of model evaluation is usually overlooked. Effective and reliable evaluation of an online bidding model is crucial for making faster model improvements as well as for utilizing the marketing budgets more efficiently. In this paper, we present an experimentation framework for bid prediction models where our focus is on the practical aspects of model evaluation. Specifically, we outline the unique challenges we encounter in our platform due to a variety of factors such as heterogeneous goal definitions, varying budget requirements across different campaigns, high seasonality and the auction-based environment for inventory purchasing. Then, we introduce return on investment (ROI) as a unified model performance (i.e., success) metric and explain its merits over more traditional metrics such as click-through rate (CTR) or conversion rate (CVR). Most importantly, we discuss commonly used evaluation and metric summarization approaches in detail and propose a more accurate method for online evaluation of new experimental models against the baseline. Our meta-analysis-based approach addresses various shortcomings of other methods and yields statistically robust conclusions that allow us to conclude experiments more quickly in a reliable manner. We demonstrate the effectiveness of our evaluation strategy on real campaign data through some experiments. version:1
arxiv-1508-07654 | Action Recognition by Hierarchical Mid-level Action Elements | http://arxiv.org/abs/1508.07654 | id:1508.07654 author:Tian Lan, Yuke Zhu, Amir Roshan Zamir, Silvio Savarese category:cs.CV  published:2015-08-31 summary:Realistic videos of human actions exhibit rich spatiotemporal structures at multiple levels of granularity: an action can always be decomposed into multiple finer-grained elements in both space and time. To capture this intuition, we propose to represent videos by a hierarchy of mid-level action elements (MAEs), where each MAE corresponds to an action-related spatiotemporal segment in the video. We introduce an unsupervised method to generate this representation from videos. Our method is capable of distinguishing action-related segments from background segments and representing actions at multiple spatiotemporal resolutions. Given a set of spatiotemporal segments generated from the training data, we introduce a discriminative clustering algorithm that automatically discovers MAEs at multiple levels of granularity. We develop structured models that capture a rich set of spatial, temporal and hierarchical relations among the segments, where the action label and multiple levels of MAE labels are jointly inferred. The proposed model achieves state-of-the-art performance in multiple action recognition benchmarks. Moreover, we demonstrate the effectiveness of our model in real-world applications such as action recognition in large-scale untrimmed videos and action parsing. version:1
arxiv-1508-07648 | Dictionary Learning for Blind One Bit Compressed Sensing | http://arxiv.org/abs/1508.07648 | id:1508.07648 author:Hadi Zayyani, Mehdi Korki, Farrokh Marvasti category:stat.ML cs.IT math.IT  published:2015-08-30 summary:This letter proposes a dictionary learning algorithm for blind one bit compressed sensing. In the blind one bit compressed sensing framework, the original signal to be reconstructed from one bit linear random measurements is sparse in an unknown domain. In this context, the multiplication of measurement matrix $\Ab$ and sparse domain matrix $\Phi$, \ie $\Db=\Ab\Phi$, should be learned. Hence, we use dictionary learning to train this matrix. Towards that end, an appropriate continuous convex cost function is suggested for one bit compressed sensing and a simple steepest-descent method is exploited to learn the rows of the matrix $\Db$. Experimental results show the effectiveness of the proposed algorithm against the case of no dictionary learning, specially with increasing the number of training signals and the number of sign measurements. version:1
arxiv-1405-6642 | Stabilized Nearest Neighbor Classifier and Its Statistical Properties | http://arxiv.org/abs/1405.6642 | id:1405.6642 author:Wei Sun, Xingye Qiao, Guang Cheng category:stat.ML cs.LG  published:2014-05-26 summary:The stability of statistical analysis is an important indicator for reproducibility, which is one main principle of scientific method. It entails that similar statistical conclusions can be reached based on independent samples from the same underlying population. In this paper, we introduce a general measure of classification instability (CIS) to quantify the sampling variability of the prediction made by a classification method. Interestingly, the asymptotic CIS of any weighted nearest neighbor classifier turns out to be proportional to the Euclidean norm of its weight vector. Based on this concise form, we propose a stabilized nearest neighbor (SNN) classifier, which distinguishes itself from other nearest neighbor classifiers, by taking the stability into consideration. In theory, we prove that SNN attains the minimax optimal convergence rate in risk, and a sharp convergence rate in CIS. The latter rate result is established for general plug-in classifiers under a low-noise condition. Extensive simulated and real examples demonstrate that SNN achieves a considerable improvement in CIS over existing nearest neighbor classifiers, with comparable classification accuracy. We implement the algorithm in a publicly available R package snn. version:2
arxiv-1402-3427 | Maximum Entropy Discrimination Denoising Autoencoders | http://arxiv.org/abs/1402.3427 | id:1402.3427 author:Sotirios P. Chatzis category:cs.LG  published:2014-02-14 summary:Denoising autoencoders (DAs) are typically applied to relatively large datasets for unsupervised learning of representative data encodings, they rely on the idea of making the learned representations robust to partial corruption of the input pattern, and perform learning using stochastic gradient descent with relatively large datasets. In this paper, we present a fully Bayesian DA architecture that allows for the application of DAs even when data is scarce. Our novel approach formulates the signal encoding problem under a nonparametric Bayesian regard, considering a Gaussian process prior over the latent input encodings generated given the (corrupt) input observations. Subsequently, the decoder modules of our model are formulated as large-margin regression models, treated under the Bayesian inference paradigm, by exploiting the maximum entropy discrimination (MED) framework. We exhibit the effectiveness of our approach using several datasets, dealing with both classification and transfer learning applications. version:2
arxiv-1508-07555 | An Event Network for Exploring Open Information | http://arxiv.org/abs/1508.07555 | id:1508.07555 author:Yanping Chen category:cs.CL  published:2015-08-30 summary:In this paper, an event network is presented for exploring open information, where linguistic units about an event are organized for analysing. The process is divided into three steps: document event detection, event network construction and event network analysis. First, by implementing event detection or tracking, documents are retrospectively (or on-line) organized into document events. Secondly, for each of the document event, linguistic units are extracted and combined into event networks. Thirdly, various analytic methods are proposed for event network analysis. In our application methodologies are presented for exploring open information. version:1
arxiv-1508-07551 | X-TREPAN: a multi class regression and adapted extraction of comprehensible decision tree in artificial neural networks | http://arxiv.org/abs/1508.07551 | id:1508.07551 author:Awudu Karim, Shangbo Zhou category:cs.LG cs.NE  published:2015-08-30 summary:In this work, the TREPAN algorithm is enhanced and extended for extracting decision trees from neural networks. We empirically evaluated the performance of the algorithm on a set of databases from real world events. This benchmark enhancement was achieved by adapting Single-test TREPAN and C4.5 decision tree induction algorithms to analyze the datasets. The models are then compared with X-TREPAN for comprehensibility and classification accuracy. Furthermore, we validate the experimentations by applying statistical methods. Finally, the modified algorithm is extended to work with multi-class regression problems and the ability to comprehend generalized feed forward networks is achieved. version:1
arxiv-1508-07535 | Calibration of One-Class SVM for MV set estimation | http://arxiv.org/abs/1508.07535 | id:1508.07535 author:Albert Thomas, Vincent Feuillard, Alexandre Gramfort category:stat.ML  published:2015-08-30 summary:A general approach for anomaly detection or novelty detection consists in estimating high density regions or Minimum Volume (MV) sets. The One-Class Support Vector Machine (OCSVM) is a state-of-the-art algorithm for estimating such regions from high dimensional data. Yet it suffers from practical limitations. When applied to a limited number of samples it can lead to poor performance even when picking the best hyperparameters. Moreover the solution of OCSVM is very sensitive to the selection of hyperparameters which makes it hard to optimize in an unsupervised setting. We present a new approach to estimate MV sets using the OCSVM with a different choice of the parameter controlling the proportion of outliers. The solution function of the OCSVM is learnt on a training set and the desired probability mass is obtained by adjusting the offset on a test set to prevent overfitting. Models learnt on different train/test splits are then aggregated to reduce the variance induced by such random splits. Our approach makes it possible to tune the hyperparameters automatically and obtain nested set estimates. Experimental results show that our approach outperforms the standard OCSVM formulation while suffering less from the curse of dimensionality than kernel density estimates. Results on actual data sets are also presented. version:1
arxiv-1508-00641 | Staged Multi-armed Bandits | http://arxiv.org/abs/1508.00641 | id:1508.00641 author:Cem Tekin, Mihaela van der Schaar category:cs.LG stat.ML  published:2015-08-04 summary:In conventional multi-armed bandits (MAB) and other reinforcement learning methods, the learner sequentially chooses actions and obtains a reward (which can be possibly missing, delayed or erroneous) after each taken action. This reward is then used by the learner to improve its future decisions. However, in numerous applications, ranging from personalized patient treatment to personalized web-based education, the learner does not obtain rewards after each action, but only after sequences of actions are taken, intermediate feedbacks are observed, and a final decision is made based on which a reward is obtained. In this paper, we introduce a new class of reinforcement learning methods which can operate in such settings. We refer to this class as staged multi-armed bandits (S-MAB). S-MAB proceeds in rounds, each composed of several stages; in each stage, the learner chooses an action and observes a feedback signal. Upon each action selection a feedback signal is observed, whilst the reward of the selected sequence of actions is only revealed after the learner selects a stop action that ends the current round. The reward of the round depends both on the sequence of actions and the sequence of observed feedbacks. The goal of the learner is to maximize its total expected reward over all rounds by learning to choose the best sequence of actions based on the feedback it gets about these actions. First, we define an oracle benchmark, which sequentially selects the actions that maximize the expected immediate reward. This benchmark is known to be approximately optimal when the reward sequence associated with the selected actions is adaptive submodular. Then, we propose our online learning algorithm, for which we prove that the regret is logarithmic in the number of rounds and linear in the number of stages with respect to the oracle benchmark. version:2
arxiv-1502-04390 | Equilibrated adaptive learning rates for non-convex optimization | http://arxiv.org/abs/1502.04390 | id:1502.04390 author:Yann N. Dauphin, Harm de Vries, Yoshua Bengio category:cs.LG cs.NA  published:2015-02-15 summary:Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes. We show that the popular Jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the so-called equilibration preconditioner is comparatively better suited to non-convex problems. We introduce a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner. Our experiments show that ESGD performs as well or better than RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent. version:2
arxiv-1502-04635 | Parameter estimation in softmax decision-making models with linear objective functions | http://arxiv.org/abs/1502.04635 | id:1502.04635 author:Paul Reverdy, Naomi E. Leonard category:math.OC cs.LG stat.ML 93E10  published:2015-02-16 summary:With an eye towards human-centered automation, we contribute to the development of a systematic means to infer features of human decision-making from behavioral data. Motivated by the common use of softmax selection in models of human decision-making, we study the maximum likelihood parameter estimation problem for softmax decision-making models with linear objective functions. We present conditions under which the likelihood function is convex. These allow us to provide sufficient conditions for convergence of the resulting maximum likelihood estimator and to construct its asymptotic distribution. In the case of models with nonlinear objective functions, we show how the estimator can be applied by linearizing about a nominal parameter value. We apply the estimator to fit the stochastic UCL (Upper Credible Limit) model of human decision-making to human subject data. We show statistically significant differences in behavior across related, but distinct, tasks. version:2
arxiv-1412-6575 | Embedding Entities and Relations for Learning and Inference in Knowledge Bases | http://arxiv.org/abs/1412.6575 | id:1412.6575 author:Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, Li Deng category:cs.CL  published:2014-12-20 summary:We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning. version:4
arxiv-1504-06063 | Multimodal Convolutional Neural Networks for Matching Image and Sentence | http://arxiv.org/abs/1504.06063 | id:1504.06063 author:Lin Ma, Zhengdong Lu, Lifeng Shang, Hang Li category:cs.CV cs.CL cs.NE  published:2015-04-23 summary:In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learning the joint representation of image and sentence. The matching CNN composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirectional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and Microsoft COCO databases achieve the state-of-the-art performances. version:5
arxiv-1508-07416 | Linked Component Analysis from Matrices to High Order Tensors: Applications to Biomedical Data | http://arxiv.org/abs/1508.07416 | id:1508.07416 author:Guoxu Zhou, Qibin Zhao, Yu Zhang, Tülay Adalı, Shengli Xie, Andrzej Cichocki category:cs.CE cs.LG cs.NA  published:2015-08-29 summary:With the increasing availability of various sensor technologies, we now have access to large amounts of multi-block (also called multi-set, multi-relational, or multi-view) data that need to be jointly analyzed to explore their latent connections. Various component analysis methods have played an increasingly important role for the analysis of such coupled data. In this paper, we first provide a brief review of existing matrix-based (two-way) component analysis methods for the joint analysis of such data with a focus on biomedical applications. Then, we discuss their important extensions and generalization to multi-block multiway (tensor) data. We show how constrained multi-block tensor decomposition methods are able to extract similar or statistically dependent common features that are shared by all blocks, by incorporating the multiway nature of data. Special emphasis is given to the flexible common and individual feature analysis of multi-block data with the aim to simultaneously extract common and individual latent components with desired properties and types of diversity. Illustrative examples are given to demonstrate their effectiveness for biomedical data analysis. version:1
arxiv-1502-05243 | SA-CNN: Dynamic Scene Classification using Convolutional Neural Networks | http://arxiv.org/abs/1502.05243 | id:1502.05243 author:Aalok Gangopadhyay, Shivam Mani Tripathi, Ishan Jindal, Shanmuganathan Raman category:cs.CV I.5.4; I.4.8  published:2015-02-17 summary:The task of classifying videos of natural dynamic scenes into appropriate classes has gained lot of attention in recent years. The problem especially becomes challenging when the camera used to capture the video is dynamic. In this paper, we analyse the performance of statistical aggregation (SA) techniques on various pre-trained convolutional neural network(CNN) models to address this problem. The proposed approach works by extracting CNN activation features for a number of frames in a video and then uses an aggregation scheme in order to obtain a robust feature descriptor for the video. We show through results that the proposed approach performs better than the-state-of-the arts for the Maryland and YUPenn dataset. The final descriptor obtained is powerful enough to distinguish among dynamic scenes and is even capable of addressing the scenario where the camera motion is dominant and the scene dynamics are complex. Further, this paper shows an extensive study on the performance of various aggregation methods and their combinations. We compare the proposed approach with other dynamic scene classification algorithms on two publicly available datasets - Maryland and YUPenn to demonstrate the superior performance of the proposed approach. version:2
arxiv-1406-2082 | Fast and Flexible ADMM Algorithms for Trend Filtering | http://arxiv.org/abs/1406.2082 | id:1406.2082 author:Aaditya Ramdas, Ryan J. Tibshirani category:stat.ML cs.LG cs.NA math.OC stat.AP  published:2014-06-09 summary:This paper presents a fast and robust algorithm for trend filtering, a recently developed nonparametric regression tool. It has been shown that, for estimating functions whose derivatives are of bounded variation, trend filtering achieves the minimax optimal error rate, while other popular methods like smoothing splines and kernels do not. Standing in the way of a more widespread practical adoption, however, is a lack of scalable and numerically stable algorithms for fitting trend filtering estimates. This paper presents a highly efficient, specialized ADMM routine for trend filtering. Our algorithm is competitive with the specialized interior point methods that are currently in use, and yet is far more numerically robust. Furthermore, the proposed ADMM implementation is very simple, and importantly, it is flexible enough to extend to many interesting related problems, such as sparse trend filtering and isotonic trend filtering. Software for our method is freely available, in both the C and R languages. version:4
