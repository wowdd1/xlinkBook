arxiv-1602-06349 | The Segmented iHMM: A Simple, Efficient Hierarchical Infinite HMM | http://arxiv.org/abs/1602.06349 | id:1602.06349 author:Ardavan Saeedi, Matthew Hoffman, Matthew Johnson, Ryan Adams category:stat.ML  published:2016-02-20 summary:We propose the segmented iHMM (siHMM), a hierarchical infinite hidden Markov model (iHMM) that supports a simple, efficient inference scheme. The siHMM is well suited to segmentation problems, where the goal is to identify points at which a time series transitions from one relatively stable regime to a new regime. Conventional iHMMs often struggle with such problems, since they have no mechanism for distinguishing between high- and low-level dynamics. Hierarchical HMMs (HHMMs) can do better, but they require much more complex and expensive inference algorithms. The siHMM retains the simplicity and efficiency of the iHMM, but outperforms it on a variety of segmentation problems, achieving performance that matches or exceeds that of a more complicated HHMM. version:1
arxiv-1602-06346 | Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models | http://arxiv.org/abs/1602.06346 | id:1602.06346 author:Bernardo Ávila Pires, Csaba Szepesvári category:stat.ML cs.LG  published:2016-02-19 summary:In this paper we study a model-based approach to calculating approximately optimal policies in Markovian Decision Processes. In particular, we derive novel bounds on the loss of using a policy derived from a factored linear model, a class of models which generalize virtually all previous models that come with strong computational guarantees. For the first time in the literature, we derive performance bounds for model-based techniques where the model inaccuracy is measured in weighted norms. Moreover, our bounds show a decreased sensitivity to the discount factor and, unlike similar bounds derived for other approaches, they are insensitive to measure mismatch. Similarly to previous works, our proofs are also based on contraction arguments, but with the main differences that we use carefully constructed norms building on Banach lattices, and the contraction property is only assumed for operators acting on "compressed" spaces, thus weakening previous assumptions, while strengthening previous results. version:1
arxiv-1412-7392 | Theoretical guarantees for approximate sampling from smooth and log-concave densities | http://arxiv.org/abs/1412.7392 | id:1412.7392 author:Arnak S. Dalalyan category:stat.CO math.ST stat.ML stat.TH  published:2014-12-23 summary:Sampling from various kinds of distributions is an issue of paramount importance in statistics since it is often the key ingredient for constructing estimators, test procedures or confidence intervals. In many situations, the exact sampling from a given distribution is impossible or computationally expensive and, therefore, one needs to resort to approximate sampling strategies. However, there is no well-developed theory providing meaningful nonasymptotic guarantees for the approximate sampling procedures, especially in the high-dimensional problems. This paper makes some progress in this direction by considering the problem of sampling from a distribution having a smooth and log-concave density defined on $\mathbb R^p$, for some integer $p>0$. We establish nonasymptotic bounds for the error of approximating the true distribution by the one obtained by the Langevin Monte Carlo method and its variants. We illustrate the effectiveness of the established guarantees with various experiments. Underlying our analysis are insights from the theory of continuous-time diffusion processes, which may be of interest beyond the framework of distributions with log-concave densities considered in the present work. version:5
arxiv-1406-7842 | Learning Laplacian Matrix in Smooth Graph Signal Representations | http://arxiv.org/abs/1406.7842 | id:1406.7842 author:Xiaowen Dong, Dorina Thanou, Pascal Frossard, Pierre Vandergheynst category:cs.LG cs.SI stat.ML  published:2014-06-30 summary:The construction of a meaningful graph plays a crucial role in the success of many graph-based representations and algorithms for handling structured data, especially in the emerging field of graph signal processing. However, a meaningful graph is not always readily available from the data, nor easy to define depending on the application domain. In particular, it is often desirable in graph signal processing applications that a graph is chosen such that the data admit certain regularity or smoothness on the graph. In this paper, we address the problem of learning graph Laplacians, which is equivalent to learning graph topologies, such that the input data form graph signals with smooth variations on the resulting topology. To this end, we adopt a factor analysis model for the graph signals and impose a Gaussian probabilistic prior on the latent variables that control these signals. We show that the Gaussian prior leads to an efficient representation that favors the smoothness property of the graph signals. We then propose an algorithm for learning graphs that enforces such property and is based on minimizing the variations of the signals on the learned graph. Experiments on both synthetic and real world data demonstrate that the proposed graph learning framework can efficiently infer meaningful graph topologies from signal observations under the smoothness prior. version:3
arxiv-1602-06289 | Learning to SMILE(S) | http://arxiv.org/abs/1602.06289 | id:1602.06289 author:Stanisław Jastrzębski, Damian Leśniak, Wojciech Marian Czarnecki category:cs.CL  published:2016-02-19 summary:This paper shows how one can directly apply natural language processing (NLP) methods to classification problems in cheminformatics. Connection between these seemingly separate fields is shown by considering standard textual representation of compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of computer aided drug design process. Conducted experiments show that this way one can not only outrank state of the art results of hand crafted representations but also gets direct structural insights into the way decisions are made. version:1
arxiv-1602-06276 | Semi-parametric Order-based Generalized Multivariate Regression | http://arxiv.org/abs/1602.06276 | id:1602.06276 author:Milad Kharratzadeh, Mark Coates category:stat.ML math.ST stat.TH  published:2016-02-19 summary:In this paper, we consider a generalized multivariate regression problem where the responses are monotonic functions of linear transformations of predictors. We propose a semi-parametric algorithm based on the ordering of the responses which is invariant to the functional form of the transformation function. We prove that our algorithm, which maximizes the rank correlation of responses and linear transformations of predictors, is a consistent estimator of the true coefficient matrix. We also identify the rate of convergence and show that the squared estimation error decays with a rate of $o(1/\sqrt{n})$. We then propose a greedy algorithm to maximize the highly non-smooth objective function of our model and examine its performance through extensive simulations. Finally, we compare our algorithm with traditional multivariate regression algorithms over synthetic and real data. version:1
arxiv-1602-06235 | A Mutual Contamination Analysis of Mixed Membership and Partial Label Models | http://arxiv.org/abs/1602.06235 | id:1602.06235 author:Julian Katz-Samuels, Clayton Scott category:stat.ML  published:2016-02-19 summary:Many machine learning problems can be characterized by mutual contamination models. In these problems, one observes several random samples from different convex combinations of a set of unknown base distributions. It is of interest to decontaminate mutual contamination models, i.e., to recover the base distributions either exactly or up to a permutation. This paper considers the general setting where the base distributions are defined on arbitrary probability spaces. We examine the decontamination problem in two mutual contamination models that describe popular machine learning tasks: recovering the base distributions up to a permutation in a mixed membership model, and recovering the base distributions exactly in a partial label model for classification. We give necessary and sufficient conditions for identifiability of both mutual contamination models, algorithms for both problems in the infinite and finite sample cases, and introduce novel proof techniques based on affine geometry. version:1
arxiv-1602-06225 | GAP Safe Screening Rules for Sparse-Group-Lasso | http://arxiv.org/abs/1602.06225 | id:1602.06225 author:Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon category:stat.ML cs.LG math.OC stat.CO  published:2016-02-19 summary:In high dimensional settings, sparse structures are crucial for efficiency, either in term of memory, computation or performance. In some contexts, it is natural to handle more refined structures than pure sparsity, such as for instance group sparsity. Sparse-Group Lasso has recently been introduced in the context of linear regression to enforce sparsity both at the feature level and at the group level. We adapt to the case of Sparse-Group Lasso recent safe screening rules that discard early in the solver irrelevant features/groups. Such rules have led to important speed-ups for a wide range of iterative methods. Thanks to dual gap computations, we provide new safe screening rules for Sparse-Group Lasso and show significant gains in term of computing time for a coordinate descent implementation. version:1
arxiv-1506-05865 | LCSTS: A Large Scale Chinese Short Text Summarization Dataset | http://arxiv.org/abs/1506.05865 | id:1506.05865 author:Baotian Hu, Qingcai Chen, Fangze Zhu category:cs.CL cs.IR cs.LG  published:2015-06-19 summary:Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public {http://icrc.hitsz.edu.cn/Article/show/139.html}. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic. version:4
arxiv-1511-04561 | 8-Bit Approximations for Parallelism in Deep Learning | http://arxiv.org/abs/1511.04561 | id:1511.04561 author:Tim Dettmers category:cs.NE cs.LG  published:2015-11-14 summary:The creation of practical deep learning data-products often requires parallelization across processors and computers to make deep learning feasible on large data sets, but bottlenecks in communication bandwidth make it difficult to attain good speedups through parallelism. Here we develop and test 8-bit approximation algorithms which make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not decrease predictive performance on MNIST, CIFAR10, and ImageNet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism. We build a predictive model for speedups based on our experimental data, verify its validity on known speedup data, and show that we can obtain a speedup of 50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism. Thus 8-bit approximation is an efficient method to parallelize convolutional networks on very large systems of GPUs. version:4
arxiv-1602-06183 | Node-By-Node Greedy Deep Learning for Interpretable Features | http://arxiv.org/abs/1602.06183 | id:1602.06183 author:Ke Wu, Malik Magdon-Ismail category:cs.LG  published:2016-02-19 summary:Multilayer networks have seen a resurgence under the umbrella of deep learning. Current deep learning algorithms train the layers of the network sequentially, improving algorithmic performance as well as providing some regularization. We present a new training algorithm for deep networks which trains \emph{each node in the network} sequentially. Our algorithm is orders of magnitude faster, creates more interpretable internal representations at the node level, while not sacrificing on the ultimate out-of-sample performance. version:1
arxiv-1511-01032 | TribeFlow: Mining & Predicting User Trajectories | http://arxiv.org/abs/1511.01032 | id:1511.01032 author:Flavio Figueiredo, Bruno Ribeiro, Jussara Almeida, Christos Faloutsos category:cs.SI physics.data-an physics.soc-ph stat.ML  published:2015-11-03 summary:Which song will Smith listen to next? Which restaurant will Alice go to tomorrow? Which product will John click next? These applications have in common the prediction of user trajectories that are in a constant state of flux over a hidden network (e.g. website links, geographic location). What users are doing now may be unrelated to what they will be doing in an hour from now. Mindful of these challenges we propose TribeFlow, a method designed to cope with the complex challenges of learning personalized predictive models of non-stationary, transient, and time-heterogeneous user trajectories. TribeFlow is a general method that can perform next product recommendation, next song recommendation, next location prediction, and general arbitrary-length user trajectory prediction without domain-specific knowledge. TribeFlow is more accurate and up to 413x faster than top competitors. version:2
arxiv-1602-00489 | Real Time Video Quality Representation Classification of Encrypted HTTP Adaptive Video Streaming - the Case of Safari | http://arxiv.org/abs/1602.00489 | id:1602.00489 author:Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar, Itay Richman, Ofir Trabelsi category:cs.MM cs.CR cs.LG cs.NI  published:2016-02-01 summary:The increasing popularity of HTTP adaptive video streaming services has dramatically increased bandwidth requirements on operator networks, which attempt to shape their traffic through Deep Packet Inspection (DPI). However, Google and certain content providers have started to encrypt their video services. As a result, operators often encounter difficulties in shaping their encrypted video traffic via DPI. This highlights the need for new traffic classification methods for encrypted HTTP adaptive video streaming to enable smart traffic shaping. These new methods will have to effectively estimate the quality representation layer and playout buffer. We present a new method and show for the first time that video quality representation classification for (YouTube) encrypted HTTP adaptive streaming is possible. We analyze the performance of this classification method with Safari over HTTPS. Based on a large number of offline and online traffic classification experiments, we demonstrate that it can independently classify, in real time, every video segment into one of the quality representation layers with 97.18% average accuracy. version:2
arxiv-1511-06422 | All you need is a good init | http://arxiv.org/abs/1511.06422 | id:1511.06422 author:Dmytro Mishkin, Jiri Matas category:cs.LG  published:2015-11-19 summary:Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets. version:7
arxiv-1602-06157 | Depth-Based Object Tracking Using a Robust Gaussian Filter | http://arxiv.org/abs/1602.06157 | id:1602.06157 author:Jan Issac, Manuel Wüthrich, Cristina Garcia Cifuentes, Jeannette Bohg, Sebastian Trimpe, Stefan Schaal category:cs.RO cs.CV  published:2016-02-19 summary:We consider the problem of model-based 3D-tracking of objects given dense depth images as input. Two difficulties preclude the application of a standard Gaussian filter to this problem. First of all, depth sensors are characterized by fat-tailed measurement noise. To address this issue, we show how a recently published robustification method for Gaussian filters can be applied to the problem at hand. Thereby, we avoid using heuristic outlier detection methods that simply reject measurements if they do not match the model. Secondly, the computational cost of the standard Gaussian filter is prohibitive due to the high-dimensional measurement, i.e. the depth image. To address this problem, we propose an approximation to reduce the computational complexity of the filter. In quantitative experiments on real data we show how our method clearly outperforms the standard Gaussian filter. Furthermore, we compare its performance to a particle-filter-based tracking method, and observe comparable computational efficiency and improved accuracy and smoothness of the estimates. version:1
arxiv-1602-06149 | Large age-gap face verification by feature injection in deep networks | http://arxiv.org/abs/1602.06149 | id:1602.06149 author:Simone Bianco category:cs.CV  published:2016-02-19 summary:This paper introduces a new method for face verification across large age gaps and also a dataset containing variations of age in the wild, the Large Age-Gap (LAG) dataset, with images ranging from child/young to adult/old. The proposed method exploits a deep convolutional neural network (DCNN) pre-trained for the face recognition task on a large dataset and then fine-tuned for the large age-gap face verification task. Finetuning is performed in a Siamese architecture using a contrastive loss function. A feature injection layer is introduced to boost verification accuracy, showing the ability of the DCNN to learn a similarity metric leveraging external features. Experimental results on the LAG dataset show that our method is able to outperform the face verification solutions in the state of the art considered. version:1
arxiv-1507-06838 | Descriptors and regions of interest fusion for gender classification in the wild. Comparison and combination with Convolutional Neural Networks | http://arxiv.org/abs/1507.06838 | id:1507.06838 author:M. Castrillón-Santana, J. Lorenzo-Navarro, E. Ramón-Balmaseda category:cs.CV  published:2015-07-24 summary:Gender classification (GC) has achieved high accuracy in different experimental evaluations based mostly on inner facial details. However, these results do not generalize well in unrestricted datasets and particularly in cross-database experiments, where the performance drops drastically. In this paper, we analyze the state-of-the-art GC accuracy on three large datasets: MORPH, LFW and GROUPS. We discuss their respective difficulties and bias, concluding that the most challenging and wildest complexity is present in GROUPS. This dataset covers hard conditions such as low resolution imagery and cluttered background. Firstly, we analyze in depth the performance of different descriptors extracted from the face and its local context on this dataset. Selecting the bests and studying their most suitable combination allows us to design a solution that beats any previously published results for GROUPS with the Dago's protocol, reaching an accuracy over 94.2%, reducing the gap with other simpler datasets. The chosen solution based on local descriptors is later evaluated in a cross-database scenario with the three mentioned datasets, and full dataset 5-fold cross validation. The achieved results are compared with a Convolutional Neural Network approach, achieving rather similar marks. Finally, a solution is proposed combining both focuses, exhibiting great complementarity, boosting GC performance to beat previously published results in GC both cross-database, and full in-database evaluations. version:2
arxiv-1602-05703 | Least Mean Squares Estimation of Graph Signals | http://arxiv.org/abs/1602.05703 | id:1602.05703 author:Paolo Di Lorenzo, Sergio Barbarossa, Paolo Banelli, Stefania Sardellitti category:cs.LG cs.SY  published:2016-02-18 summary:In many applications spanning from sensor to social networks, transportation systems, gene regulatory networks or big data, the signals of interest are defined over the vertices of a graph. The aim of this paper is to propose a least mean square (LMS) strategy for adaptive estimation of signals defined over graphs. Assuming the graph signal to be band-limited, over a known bandwidth, the method enables reconstruction, with guaranteed performance in terms of mean-square error, and tracking from a limited number of observations over a subset of vertices. A detailed mean square analysis provides the performance of the proposed method, and leads to several insights for designing useful sampling strategies for graph signals. Numerical results validate our theoretical findings, and illustrate the performance of the proposed method. Furthermore, to cope with the case where the bandwidth is not known beforehand, we propose a method that performs a sparse online estimation of the signal support in the (graph) frequency domain, which enables online adaptation of the graph sampling strategy. Finally, we apply the proposed method to build the power spatial density cartography of a given operational region in a cognitive network environment. version:2
arxiv-1511-05835 | Alternative Markov and Causal Properties for Acyclic Directed Mixed Graphs | http://arxiv.org/abs/1511.05835 | id:1511.05835 author:Jose M. Peña category:stat.ML cs.AI  published:2015-11-18 summary:We extend Andersson-Madigan-Perlman chain graphs by (i) relaxing the semidirected acyclity constraint so that only directed cycles are forbidden, and (ii) allowing up to two edges between any pair of nodes. We introduce global, and ordered local and pairwise Markov properties for the new models. We show the equivalence of these properties for strictly positive probability distributions. We also show that when the random variables are continuous, the new models can be interpreted as systems of structural equations with correlated errors. This enables us to adapt Pearl's do-calculus to them. Finally, we describe an exact algorithm for learning the new models from observational and interventional data via answer set programming. version:4
arxiv-1602-06057 | Uniresolution representations of white-matter data from CoCoMac | http://arxiv.org/abs/1602.06057 | id:1602.06057 author:Raghavendra Singh category:cs.NE q-bio.NC  published:2016-02-19 summary:Tracing data as collated by CoCoMac, a seminal neuroinformatics database, is at multiple resolutions -- white matter tracts were studied for areas and their subdivisions by different reports. Network theoretic analysis of this multi-resolution data often assumes that the data at various resolutions is equivalent, which may not be correct. In this paper we propose three methods to resolve the multi-resolution issue such that the resultant networks have connectivity data at only one resolution. The different resultant networks are compared in terms of their network analysis metrics and degree distributions. version:1
arxiv-1510-05492 | Modularity Component Analysis versus Principal Component Analysis | http://arxiv.org/abs/1510.05492 | id:1510.05492 author:Hansi Jiang, Carl Meyer category:stat.ML  published:2015-10-19 summary:In this paper the exact linear relation between the leading eigenvectors of the modularity matrix and the singular vectors of an uncentered data matrix is developed. Based on this analysis the concept of a modularity component is defined, and its properties are developed. It is shown that modularity component analysis can be used to cluster data similar to how traditional principal component analysis is used except that modularity component analysis does not require data centering. version:2
arxiv-1602-06053 | First-order Methods for Geodesically Convex Optimization | http://arxiv.org/abs/1602.06053 | id:1602.06053 author:Hongyi Zhang, Suvrit Sra category:math.OC cs.LG stat.ML  published:2016-02-19 summary:Geodesic convexity generalizes the notion of (vector space) convexity to nonlinear metric spaces. But unlike convex optimization, geodesically convex (g-convex) optimization is much less developed. In this paper we contribute to the understanding of g-convex optimization by developing iteration complexity analysis for several first-order algorithms on Hadamard manifolds. Specifically, we prove upper bounds for the global complexity of deterministic and stochastic (sub)gradient methods for optimizing smooth and nonsmooth g-convex functions, both with and without strong g-convexity. Our analysis also reveals how the manifold geometry, especially \emph{sectional curvature}, impacts convergence rates. To the best of our knowledge, our work is the first to provide global complexity analysis for first-order algorithms for general g-convex optimization. version:1
arxiv-1509-02597 | Asynchronous Distributed ADMM for Large-Scale Optimization- Part I: Algorithm and Convergence Analysis | http://arxiv.org/abs/1509.02597 | id:1509.02597 author:Tsung-Hui Chang, Mingyi Hong, Wei-Cheng Liao, Xiangfeng Wang category:cs.DC cs.LG cs.SY  published:2015-09-09 summary:Aiming at solving large-scale learning problems, this paper studies distributed optimization methods based on the alternating direction method of multipliers (ADMM). By formulating the learning problem as a consensus problem, the ADMM can be used to solve the consensus problem in a fully parallel fashion over a computer network with a star topology. However, traditional synchronized computation does not scale well with the problem size, as the speed of the algorithm is limited by the slowest workers. This is particularly true in a heterogeneous network where the computing nodes experience different computation and communication delays. In this paper, we propose an asynchronous distributed ADMM (AD-AMM) which can effectively improve the time efficiency of distributed optimization. Our main interest lies in analyzing the convergence conditions of the AD-ADMM, under the popular partially asynchronous model, which is defined based on a maximum tolerable delay of the network. Specifically, by considering general and possibly non-convex cost functions, we show that the AD-ADMM is guaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points as long as the algorithm parameters are chosen appropriately according to the network delay. We further illustrate that the asynchrony of the ADMM has to be handled with care, as slightly modifying the implementation of the AD-ADMM can jeopardize the algorithm convergence, even under a standard convex setting. version:2
arxiv-1602-06049 | Scaling up Dynamic Topic Models | http://arxiv.org/abs/1602.06049 | id:1602.06049 author:Arnab Bhadury, Jianfei Chen, Jun Zhu, Shixia Liu category:stat.ML H.4; G.3  published:2016-02-19 summary:Dynamic topic models (DTMs) are very effective in discovering topics and capturing their evolution trends in time series data. To do posterior inference of DTMs, existing methods are all batch algorithms that scan the full dataset before each update of the model and make inexact variational approximations with mean-field assumptions. Due to a lack of a more scalable inference algorithm, despite the usefulness, DTMs have not captured large topic dynamics. This paper fills this research void, and presents a fast and parallelizable inference algorithm using Gibbs Sampling with Stochastic Gradient Langevin Dynamics that does not make any unwarranted assumptions. We also present a Metropolis-Hastings based $O(1)$ sampler for topic assignments for each word token. In a distributed environment, our algorithm requires very little communication between workers during sampling (almost embarrassingly parallel) and scales up to large-scale applications. We are able to learn the largest Dynamic Topic Model to our knowledge, and learned the dynamics of 1,000 topics from 2.6 million documents in less than half an hour, and our empirical results show that our algorithm is not only orders of magnitude faster than the baselines but also achieves lower perplexity. version:1
arxiv-1602-06025 | Spectral Learning for Supervised Topic Models | http://arxiv.org/abs/1602.06025 | id:1602.06025 author:Yong Ren, Yining Wang, Jun Zhu category:cs.LG cs.CL cs.IR stat.ML  published:2016-02-19 summary:Supervised topic models simultaneously model the latent topic structure of large collections of documents and a response variable associated with each document. Existing inference methods are based on variational approximation or Monte Carlo sampling, which often suffers from the local minimum defect. Spectral methods have been applied to learn unsupervised topic models, such as latent Dirichlet allocation (LDA), with provable guarantees. This paper investigates the possibility of applying spectral methods to recover the parameters of supervised LDA (sLDA). We first present a two-stage spectral method, which recovers the parameters of LDA followed by a power update method to recover the regression model parameters. Then, we further present a single-phase spectral algorithm to jointly recover the topic distribution matrix as well as the regression weights. Our spectral algorithms are provably correct and computationally efficient. We prove a sample complexity bound for each algorithm and subsequently derive a sufficient condition for the identifiability of sLDA. Thorough experiments on synthetic and real-world datasets verify the theory and demonstrate the practical effectiveness of the spectral algorithms. In fact, our results on a large-scale review rating dataset demonstrate that our single-phase spectral algorithm alone gets comparable or even better performance than state-of-the-art methods, while previous work on spectral methods has rarely reported such promising performance. version:1
arxiv-1602-02697 | Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples | http://arxiv.org/abs/1602.02697 | id:1602.02697 author:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami category:cs.CR cs.LG  published:2016-02-08 summary:Advances in deep learning have led to the broad adoption of Deep Neural Networks (DNNs) to a range of important machine learning problems, e.g., guiding autonomous vehicles, speech recognition, malware detection. Yet, machine learning models, including DNNs, were shown to be vulnerable to adversarial samples-subtly (and often humanly indistinguishably) modified malicious inputs crafted to compromise the integrity of their outputs. Adversarial examples thus enable adversaries to manipulate system behaviors. Potential attacks include attempts to control the behavior of vehicles, have spam content identified as legitimate content, or have malware identified as legitimate software. Adversarial examples are known to transfer from one model to another, even if the second model has a different architecture or was trained on a different set. We introduce the first practical demonstration that this cross-model transfer phenomenon enables attackers to control a remotely hosted DNN with no access to the model, its parameters, or its training data. In our demonstration, we only assume that the adversary can observe outputs from the target DNN given inputs chosen by the adversary. We introduce the attack strategy of fitting a substitute model to the input-output pairs in this manner, then crafting adversarial examples based on this auxiliary model. We evaluate the approach on existing DNN datasets and real-world settings. In one experiment, we force a DNN supported by MetaMind (one of the online APIs for DNN classifiers) to mis-classify inputs at a rate of 84.24%. We conclude with experiments exploring why adversarial samples transfer between DNNs, and a discussion on the applicability of our attack when targeting machine learning algorithms distinct from DNNs. version:2
arxiv-1602-06904 | Structured illumination microscopy image reconstruction algorithm | http://arxiv.org/abs/1602.06904 | id:1602.06904 author:Amit Lal, Chunyan Shan, Peng Xi category:cs.CV  published:2016-02-19 summary:Structured illumination microscopy (SIM) is a very important super-resolution microscopy technique, which provides high speed super-resolution with about two-fold spatial resolution enhancement. Several attempts aimed at improving the performance of SIM reconstruction algorithm have been reported. However, most of these highlight only one specific aspect of the SIM reconstruction -- such as the determination of the illumination pattern phase shift accurately -- whereas other key elements -- such as determination of modulation factor, estimation of object power spectrum, Wiener filtering frequency components with inclusion of object power spectrum information, translocating and the merging of the overlapping frequency components -- are usually glossed over superficially. In addition, most of the work reported lie scattered throughout the literature and a comprehensive review of the theoretical background is found lacking. The purpose of the present work is two-fold: 1) to collect the essential theoretical details of SIM algorithm at one place, thereby making them readily accessible to readers for the first time; and 2) to provide an open source SIM reconstruction code (named OpenSIM), which enables users to interactively vary the code parameters and study it's effect on reconstructed SIM image. version:1
arxiv-1511-06830 | Ground-truth dataset and baseline evaluations for image base-detail separation algorithms | http://arxiv.org/abs/1511.06830 | id:1511.06830 author:Xuan Dong, Boyan Bonev, Weixin Li, Weichao Qiu, Xianjie Chen, Alan Yuille category:cs.CV  published:2015-11-21 summary:Base-detail separation is a fundamental computer vision problem consisting of modeling a smooth base layer with the coarse structures, and a detail layer containing the texture-like structures. One of the challenges of estimating the base is to preserve sharp boundaries between objects or parts to avoid halo artifacts. Many methods have been proposed to address this problem, but there is no ground-truth dataset of real images for quantitative evaluation. We proposed a procedure to construct such a dataset, and provide two datasets: Pascal Base-Detail and Fashionista Base-Detail, containing 1000 and 250 images, respectively. Our assumption is that the base is piecewise smooth and we label the appearance of each piece by a polynomial model. The pieces are objects and parts of objects, obtained from human annotations. Finally, we proposed a way to evaluate methods with our base-detail ground-truth and we compared the performances of seven state-of-the-art algorithms. version:2
arxiv-1602-05996 | A Nonparametric Framework for Quantifying Generative Inference on Neuromorphic Systems | http://arxiv.org/abs/1602.05996 | id:1602.05996 author:Ojash Neopane, Srinjoy Das, Ery Arias-Castro, Kenneth Kreutz-Delgado category:cs.NE  published:2016-02-18 summary:Restricted Boltzmann Machines and Deep Belief Networks have been successfully used in probabilistic generative model applications such as image occlusion removal, pattern completion and motion synthesis. Generative inference in such algorithms can be performed very efficiently on hardware using a Markov Chain Monte Carlo procedure called Gibbs sampling, where stochastic samples are drawn from noisy integrate and fire neurons implemented on neuromorphic substrates. Currently, no satisfactory metrics exist for evaluating the generative performance of such algorithms implemented on high-dimensional data for neuromorphic platforms. This paper demonstrates the application of nonparametric goodness-of-fit testing to both quantify the generative performance as well as provide decision-directed criteria for choosing the parameters of the neuromorphic Gibbs sampler and optimizing usage of hardware resources used during sampling. version:1
arxiv-1602-05990 | Plücker Correction Problem: Analysis and Improvements in Efficiency | http://arxiv.org/abs/1602.05990 | id:1602.05990 author:João R. Cardoso, Pedro Miraldo, Helder Araujo category:cs.CV cs.RO  published:2016-02-18 summary:A given six dimensional vector represents a 3D straight line in Plucker coordinates if its coordinates satisfy the Klein quadric constraint. In many problems aiming to find the Plucker coordinates of lines, noise in the data and other type of errors contribute for obtaining 6D vectors that do not correspond to lines, because of that constraint. A common procedure to overcome this drawback is to find the Plucker coordinates of the lines that are closest to those vectors. This is known as the Plucker correction problem. In this article we propose a simple, closed-form, and global solution for this problem. When compared with the state-of-the-art method, one can conclude that our algorithm is easier and requires much less operations than previous techniques (it does not require Singular Value Decomposition techniques). version:1
arxiv-1602-05944 | The Interaction of Memory and Attention in Novel Word Generalization: A Computational Investigation | http://arxiv.org/abs/1602.05944 | id:1602.05944 author:Erin Grant, Aida Nematzadeh, Suzanne Stevenson category:cs.CL  published:2016-02-18 summary:People exhibit a tendency to generalize a novel noun to the basic-level in a hierarchical taxonomy -- a cognitively salient category such as "dog" -- with the degree of generalization depending on the number and type of exemplars. Recently, a change in the presentation timing of exemplars has also been shown to have an effect, surprisingly reversing the prior observed pattern of basic-level generalization. We explore the precise mechanisms that could lead to such behavior by extending a computational model of word learning and word generalization to integrate cognitive processes of memory and attention. Our results show that the interaction of forgetting and attention to novelty, as well as sensitivity to both type and token frequencies of exemplars, enables the model to replicate the empirical results from different presentation timings. Our results reinforce the need to incorporate general cognitive processes within word learning models to better understand the range of observed behaviors in vocabulary acquisition. version:1
arxiv-1602-05941 | Multi-resolution Compressive Sensing Reconstruction | http://arxiv.org/abs/1602.05941 | id:1602.05941 author:Adriana Gonzalez, Hong Jiang, Gang Huang, Laurent Jacques category:cs.CV  published:2016-02-18 summary:We consider the problem of reconstructing an image from compressive measurements using a multi-resolution grid. In this context, the reconstructed image is divided into multiple regions, each one with a different resolution. This problem arises in situations where the image to reconstruct contains a certain region of interest (RoI) that is more important than the rest. Through a theoretical analysis and simulation experiments we show that the multi-resolution reconstruction provides a higher quality of the RoI compared to the traditional single-resolution approach. version:1
arxiv-1412-3121 | Multimodal Transfer Deep Learning with Applications in Audio-Visual Recognition | http://arxiv.org/abs/1412.3121 | id:1412.3121 author:Seungwhan Moon, Suyoun Kim, Haohan Wang category:cs.NE cs.LG  published:2014-12-09 summary:We propose a transfer deep learning (TDL) framework that can transfer the knowledge obtained from a single-modal neural network to a network with a different modality. Specifically, we show that we can leverage speech data to fine-tune the network trained for video recognition, given an initial set of audio-video parallel dataset within the same semantics. Our approach first learns the analogy-preserving embeddings between the abstract representations learned from intermediate layers of each network, allowing for semantics-level transfer between the source and target modalities. We then apply our neural network operation that fine-tunes the target network with the additional knowledge transferred from the source network, while keeping the topology of the target network unchanged. While we present an audio-visual recognition task as an application of our approach, our framework is flexible and thus can work with any multimodal dataset, or with any already-existing deep networks that share the common underlying semantics. In this work in progress report, we aim to provide comprehensive results of different configurations of the proposed approach on two widely used audio-visual datasets, and we discuss potential applications of the proposed approach. version:2
arxiv-1602-05925 | Encoding Data for HTM Systems | http://arxiv.org/abs/1602.05925 | id:1602.05925 author:Scott Purdy category:cs.NE q-bio.NC  published:2016-02-18 summary:Hierarchical Temporal Memory (HTM) is a biologically inspired machine intelligence technology that mimics the architecture and processes of the neocortex. In this white paper we describe how to encode data as Sparse Distributed Representations (SDRs) for use in HTM systems. We explain several existing encoders, which are available through the open source project called NuPIC, and we discuss requirements for creating encoders for new types of data. version:1
arxiv-1602-05920 | Weighted Unsupervised Learning for 3D Object Detection | http://arxiv.org/abs/1602.05920 | id:1602.05920 author:Kamran Kowsari, Manal H. Alassaf category:cs.CV  published:2016-02-18 summary:This paper introduces a novel weighted unsupervised learning for object detection using an RGB-D camera. This technique is feasible for detecting the moving objects in the noisy environments that are captured by an RGB-D camera. The main contribution of this paper is a real-time algorithm for detecting each object using weighted clustering as a separate cluster. In a preprocessing step, the algorithm calculates the pose 3D position X, Y, Z and RGB color of each data point and then it calculates each data point's normal vector using the point's neighbor. After preprocessing, our algorithm calculates k-weights for each data point; each weight indicates membership. Resulting in clustered objects of the scene. version:1
arxiv-1602-05916 | Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning | http://arxiv.org/abs/1602.05916 | id:1602.05916 author:Niloofar Yousefi, Yunwen Lei, Marius Kloft, Mansooreh Mollaghasemi, Georgios Anagnastapolous category:cs.LG  published:2016-02-18 summary:We show a Talagrand-type of concentration inequality for Multi-Task Learning (MTL), using which we establish sharp excess risk bounds for MTL in terms of distribution- and data-dependent versions of the Local Rademacher Complexity (LRC). We also give a new bound on the LRC for strongly convex hypothesis classes, which applies not only to MTL but also to the standard i.i.d. setting. Combining both results, one can now easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including---as we demonstrate---Schatten-norm, group-norm, and graph-regularized MTL. The derived bounds reflect a relationship akeen to a conservation law of asymptotic convergence rates. This very relationship allows for trading o? slower rates w.r.t. the number of tasks for faster rates with respect to the number of available samples per task, when compared to the rates obtained via a traditional, global Rademacher analysis. version:1
arxiv-1508-01720 | Mismatch in the Classification of Linear Subspaces: Sufficient Conditions for Reliable Classification | http://arxiv.org/abs/1508.01720 | id:1508.01720 author:Jure Sokolic, Francesco Renna, Robert Calderbank, Miguel R. D. Rodrigues category:cs.IT cs.CV math.IT stat.ML  published:2015-08-07 summary:This paper considers the classification of linear subspaces with mismatched classifiers. In particular, we assume a model where one observes signals in the presence of isotropic Gaussian noise and the distribution of the signals conditioned on a given class is Gaussian with a zero mean and a low-rank covariance matrix. We also assume that the classifier knows only a mismatched version of the parameters of input distribution in lieu of the true parameters. By constructing an asymptotic low-noise expansion of an upper bound to the error probability of such a mismatched classifier, we provide sufficient conditions for reliable classification in the low-noise regime that are able to sharply predict the absence of a classification error floor. Such conditions are a function of the geometry of the true signal distribution, the geometry of the mismatched signal distributions as well as the interplay between such geometries, namely, the principal angles and the overlap between the true and the mismatched signal subspaces. Numerical results demonstrate that our conditions for reliable classification can sharply predict the behavior of a mismatched classifier both with synthetic data and in a motion segmentation and a hand-written digit classification applications. version:2
arxiv-1602-05908 | Efficient approaches for escaping higher order saddle points in non-convex optimization | http://arxiv.org/abs/1602.05908 | id:1602.05908 author:Anima Anandkumar, Rong Ge category:cs.LG stat.ML  published:2016-02-18 summary:Local search heuristics for non-convex optimizations are popular in applied machine learning. However, in general it is hard to guarantee that such algorithms even converge to a local minimum, due to the existence of complicated saddle point structures in high dimensions. Many functions have degenerate saddle points such that the first and second order derivatives cannot distinguish them with local optima. In this paper we use higher order derivatives to escape these saddle points: we design the first efficient algorithm guaranteed to converge to a third order local optimum (while existing techniques are at most second order). We also show that it is NP-hard to extend this further to finding fourth order local optima. version:1
arxiv-1602-05897 | Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity | http://arxiv.org/abs/1602.05897 | id:1602.05897 author:Amit Daniely, Roy Frostig, Yoram Singer category:cs.LG cs.AI cs.CC cs.DS stat.ML  published:2016-02-18 summary:We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power. version:1
arxiv-1501-06521 | Noisy Tensor Completion via the Sum-of-Squares Hierarchy | http://arxiv.org/abs/1501.06521 | id:1501.06521 author:Boaz Barak, Ankur Moitra category:cs.LG cs.DS stat.ML  published:2015-01-26 summary:In the noisy tensor completion problem we observe $m$ entries (whose location is chosen uniformly at random) from an unknown $n_1 \times n_2 \times n_3$ tensor $T$. We assume that $T$ is entry-wise close to being rank $r$. Our goal is to fill in its missing entries using as few observations as possible. Let $n = \max(n_1, n_2, n_3)$. We show that if $m = n^{3/2} r$ then there is a polynomial time algorithm based on the sixth level of the sum-of-squares hierarchy for completing it. Our estimate agrees with almost all of $T$'s entries almost exactly and works even when our observations are corrupted by noise. This is also the first algorithm for tensor completion that works in the overcomplete case when $r > n$, and in fact it works all the way up to $r = n^{3/2-\epsilon}$. Our proofs are short and simple and are based on establishing a new connection between noisy tensor completion (through the language of Rademacher complexity) and the task of refuting random constant satisfaction problems. This connection seems to have gone unnoticed even in the context of matrix completion. Furthermore, we use this connection to show matching lower bounds. Our main technical result is in characterizing the Rademacher complexity of the sequence of norms that arise in the sum-of-squares relaxations to the tensor nuclear norm. These results point to an interesting new direction: Can we explore computational vs. sample complexity tradeoffs through the sum-of-squares hierarchy? version:3
arxiv-1511-05789 | Metric learning approach for graph-based label propagation | http://arxiv.org/abs/1511.05789 | id:1511.05789 author:Pauline Wauquier, Mikaela Keller category:cs.LG  published:2015-11-18 summary:The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. We claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently. version:6
arxiv-1601-04798 | Scale-aware Pixel-wise Object Proposal Networks | http://arxiv.org/abs/1601.04798 | id:1601.04798 author:Zequn Jie, Xiaodan Liang, Jiashi Feng, Wen Feng Lu, Eng Hock Francis Tay, Shuicheng Yan category:cs.CV  published:2016-01-19 summary:Object proposal is essential for current state-of-the-art object detection pipelines. However, the existing proposal methods generally fail in producing results with satisfying localization accuracy. The case is even worse for small objects which however are quite common in practice. In this paper we propose a novel Scale-aware Pixel-wise Object Proposal (SPOP) network to tackle the challenges. The SPOP network can generate proposals with high recall rate and average best overlap (ABO), even for small objects. In particular, in order to improve the localization accuracy, a fully convolutional network is employed which predicts locations of object proposals for each pixel. The produced ensemble of pixel-wise object proposals enhances the chance of hitting the object significantly without incurring heavy extra computational cost. To solve the challenge of localizing objects at small scale, two localization networks which are specialized for localizing objects with different scales are introduced, following the divide-and-conquer philosophy. Location outputs of these two networks are then adaptively combined to generate the final proposals by a large-/small-size weighting network. Extensive evaluations on PASCAL VOC 2007 show the SPOP network is superior over the state-of-the-art models. The high-quality proposals from SPOP network also significantly improve the mean average precision (mAP) of object detection with Fast-RCNN framework. Finally, the SPOP network (trained on PASCAL VOC) shows great generalization performance when testing it on ILSVRC 2013 validation set. version:2
arxiv-1602-05822 | What is the distribution of the number of unique original items in a bootstrap sample? | http://arxiv.org/abs/1602.05822 | id:1602.05822 author:Alex F. Mendelson, Maria A. Zuluaga, Brian F. Hutton, Sébastien Ourselin category:stat.ML 62G09  published:2016-02-18 summary:Sampling with replacement occurs in many settings in machine learning, notably in the bagging ensemble technique and the .632+ validation scheme. The number of unique original items in a bootstrap sample can have an important role in the behaviour of prediction models learned on it. Indeed, there are uncontrived examples where duplicate items have no effect. The purpose of this report is to present the distribution of the number of unique original items in a bootstrap sample clearly and concisely, with a view to enabling other machine learning researchers to understand and control this quantity in existing and future resampling techniques. We describe the key characteristics of this distribution along with the generalisation for the case where items come from distinct categories, as in classification. In both cases we discuss the normal limit, and conduct an empirical investigation to derive a heuristic for when a normal approximation is permissible. version:1
arxiv-1505-07427 | PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization | http://arxiv.org/abs/1505.07427 | id:1505.07427 author:Alex Kendall, Matthew Grimes, Roberto Cipolla category:cs.CV cs.NE cs.RO  published:2015-05-27 summary:We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/ version:4
arxiv-1509-05909 | Modelling Uncertainty in Deep Learning for Camera Relocalization | http://arxiv.org/abs/1509.05909 | id:1509.05909 author:Alex Kendall, Roberto Cipolla category:cs.CV cs.RO  published:2015-09-19 summary:We present a robust and real-time monocular six degree of freedom visual relocalization system. We use a Bayesian convolutional neural network to regress the 6-DOF camera pose from a single RGB image. It is trained in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking under 6ms to compute. It obtains approximately 2m and 6 degrees accuracy for very large scale outdoor scenes and 0.5m and 10 degrees accuracy indoors. Using a Bayesian convolutional neural network implementation we obtain an estimate of the model's relocalization uncertainty and improve state of the art localization accuracy on a large scale outdoor dataset. We leverage the uncertainty measure to estimate metric relocalization error and to detect the presence or absence of the scene in the input image. We show that the model's uncertainty is caused by images being dissimilar to the training dataset in either pose or appearance. version:2
arxiv-1602-05772 | Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure | http://arxiv.org/abs/1602.05772 | id:1602.05772 author:Stefan Gerdjikov, Klaus U. Schulz category:cs.CL  published:2016-02-18 summary:When looking at the structure of natural language, "phrases" and "words" are central notions. We consider the problem of identifying such "meaningful subparts" of language of any length and underlying composition principles in a completely corpus-based and language-independent way without using any kind of prior linguistic knowledge. Unsupervised methods for identifying "phrases", mining subphrase structure and finding words in a fully automated way are described. This can be considered as a step towards automatically computing a "general dictionary and grammar of the corpus". We hope that in the long run variants of our approach turn out to be useful for other kind of sequence data as well, such as, e.g., speech, genom sequences, or music annotation. Even if we are not primarily interested in immediate applications, results obtained for a variety of languages show that our methods are interesting for many practical tasks in text mining, terminology extraction and lexicography, search engine technology, and related fields. version:1
arxiv-1602-05765 | Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning | http://arxiv.org/abs/1602.05765 | id:1602.05765 author:Shoaib Jameel, Steven Schockaert category:cs.AI cs.CL  published:2016-02-18 summary:Conceptual spaces are geometric representations of conceptual knowledge, in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. While conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence. To address this issue, we propose a method which learns a vector-space embedding of entities from Wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace. We experimentally demonstrate the usefulness of these subspaces as (approximate) conceptual space representations by showing, among others, that important features can be modelled as directions and that natural properties tend to correspond to convex regions. version:1
arxiv-1602-05753 | Overview of Annotation Creation: Processes & Tools | http://arxiv.org/abs/1602.05753 | id:1602.05753 author:Mark A. Finlayson, Tomaž Erjavec category:cs.CL cs.HC  published:2016-02-18 summary:Creating linguistic annotations requires more than just a reliable annotation scheme. Annotation can be a complex endeavour potentially involving many people, stages, and tools. This chapter outlines the process of creating end-to-end linguistic annotations, identifying specific tasks that researchers often perform. Because tool support is so central to achieving high quality, reusable annotations with low cost, the focus is on identifying capabilities that are necessary or useful for annotation tools, as well as common problems these tools present that reduce their utility. Although examples of specific tools are provided in many cases, this chapter concentrates more on abstract capabilities and problems because new tools appear continuously, while old tools disappear into disuse or disrepair. The two core capabilities tools must have are support for the chosen annotation scheme and the ability to work on the language under study. Additional capabilities are organized into three categories: those that are widely provided; those that often useful but found in only a few tools; and those that have as yet little or no available tool support. version:1
arxiv-1602-05719 | An improved analysis of the ER-SpUD dictionary learning algorithm | http://arxiv.org/abs/1602.05719 | id:1602.05719 author:Jarosław Błasiok, Jelani Nelson category:cs.LG cs.DS cs.IT math.IT math.PR I.2.6; F.2.0  published:2016-02-18 summary:In "dictionary learning" we observe $Y = AX + E$ for some $Y\in\mathbb{R}^{n\times p}$, $A \in\mathbb{R}^{m\times n}$, and $X\in\mathbb{R}^{m\times p}$. The matrix $Y$ is observed, and $A, X, E$ are unknown. Here $E$ is "noise" of small norm, and $X$ is column-wise sparse. The matrix $A$ is referred to as a {\em dictionary}, and its columns as {\em atoms}. Then, given some small number $p$ of samples, i.e.\ columns of $Y$, the goal is to learn the dictionary $A$ up to small error, as well as $X$. The motivation is that in many applications data is expected to sparse when represented by atoms in the "right" dictionary $A$ (e.g.\ images in the Haar wavelet basis), and the goal is to learn $A$ from the data to then use it for other applications. Recently, [SWW12] proposed the dictionary learning algorithm ER-SpUD with provable guarantees when $E = 0$ and $m = n$. They showed if $X$ has independent entries with an expected $s$ non-zeroes per column for $1 \lesssim s \lesssim \sqrt{n}$, and with non-zero entries being subgaussian, then for $p\gtrsim n^2\log^2 n$ with high probability ER-SpUD outputs matrices $A', X'$ which equal $A, X$ up to permuting and scaling columns (resp.\ rows) of $A$ (resp.\ $X$). They conjectured $p\gtrsim n\log n$ suffices, which they showed was information theoretically necessary for {\em any} algorithm to succeed when $s \simeq 1$. Significant progress was later obtained in [LV15]. We show that for a slight variant of ER-SpUD, $p\gtrsim n\log(n/\delta)$ samples suffice for successful recovery with probability $1-\delta$. We also show that for the unmodified ER-SpUD, $p\gtrsim n^{1.99}$ samples are required even to learn $A, X$ with polynomially small success probability. This resolves the main conjecture of [SWW12], and contradicts the main result of [LV15], which claimed that $p\gtrsim n\log^4 n$ guarantees success whp. version:1
arxiv-1602-05702 | EEG-informed attended speaker extraction from recorded speech mixtures with application in neuro-steered hearing prostheses | http://arxiv.org/abs/1602.05702 | id:1602.05702 author:Simon Van Eyndhoven, Tom Francart, Alexander Bertrand category:cs.SD cs.SY stat.ML  published:2016-02-18 summary:OBJECTIVE: We aim to extract and denoise the attended speaker in a noisy, two-speaker acoustic scenario, relying on microphone array recordings from a binaural hearing aid, which are complemented with electroencephalography (EEG) recordings to infer the speaker of interest. METHODS: In this study, we propose a modular processing flow that first extracts the two speech envelopes from the microphone recordings, then selects the attended speech envelope based on the EEG, and finally uses this envelope to inform a multi-channel speech separation and denoising algorithm. RESULTS: Strong suppression of interfering (unattended) speech and background noise is achieved, while the attended speech is preserved. Furthermore, EEG-based auditory attention detection (AAD) is shown to be robust to the use of noisy speech signals. CONCLUSIONS: Our results show that AAD-based speaker extraction from microphone array recordings is feasible and robust, even in noisy acoustic environments, and without access to the clean speech signals to perform EEG-based AAD. SIGNIFICANCE: Current research on AAD always assumes the availability of the clean speech signals, which limits the applicability in real settings. We have extended this research to detect the attended speaker even when only microphone recordings with noisy speech mixtures are available. This is an enabling ingredient for new brain-computer interfaces and effective filtering schemes in neuro-steered hearing prostheses. Here, we provide a first proof of concept for EEG-informed attended speaker extraction and denoising. version:1
arxiv-1509-00114 | Multi-Sensor Slope Change Detection | http://arxiv.org/abs/1509.00114 | id:1509.00114 author:Yang Cao, Yao Xie, Nagi Gebraeel category:stat.ML cs.LG math.ST stat.TH  published:2015-09-01 summary:We develop a mixture procedure for multi-sensor systems to monitor data streams for a change-point that causes a gradual degradation to a subset of the streams. Observations are assumed to be initially normal random variables with known constant means and variances. After the change-point, observations in the subset will have increasing or decreasing means. The subset and the rate-of-changes are unknown. Our procedure uses a mixture statistics, which assumes that each sensor is affected by the change-point with probability $p_0$. Analytic expressions are obtained for the average run length (ARL) and the expected detection delay (EDD) of the mixture procedure, which are demonstrated to be quite accurate numerically. We establish the asymptotic optimality of the mixture procedure. Numerical examples demonstrate the good performance of the proposed procedure. We also discuss an adaptive mixture procedure using empirical Bayes. This paper extends our earlier work on detecting an abrupt change-point that causes a mean-shift, by tackling the challenges posed by the non-stationarity of the slope-change problem. version:2
arxiv-1602-05660 | Feature-Area Optimization: A Novel SAR Image Registration Method | http://arxiv.org/abs/1602.05660 | id:1602.05660 author:Fuqiang Liu, Fukun Bi, Liang Chen, Hao Shi, Wei Liu category:cs.CV  published:2016-02-18 summary:This letter proposes a synthetic aperture radar (SAR) image registration method named Feature-Area Optimization (FAO). First, the traditional area-based optimization model is reconstructed and decomposed into three key but uncertain factors: initialization, slice set and regularization. Next, structural features are extracted by scale invariant feature transform (SIFT) in dual-resolution space (SIFT-DRS), a novel SIFT-Like method dedicated to FAO. Then, the three key factors are determined based on these features. Finally, solving the factor-determined optimization model can get the registration result. A series of experiments demonstrate that the proposed method can register multi-temporal SAR images accurately and efficiently. version:1
arxiv-1602-05629 | Federated Learning of Deep Networks using Model Averaging | http://arxiv.org/abs/1602.05629 | id:1602.05629 author:H. Brendan McMahan, Eider Moore, Daniel Ramage, Blaise Agüera y Arcas category:cs.LG  published:2016-02-17 summary:Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data-center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks that proves robust to the unbalanced and non-IID data distributions that naturally arise. This method allows high-quality models to be trained in relatively few rounds of communication, the principal constraint for federated learning. The key insight is that despite the non-convex loss functions we optimize, parameter averaging over updates from multiple clients produces surprisingly good results, for example decreasing the communication needed to train an LSTM language model by two orders of magnitude. version:1
arxiv-1602-01887 | Visual Tracking via Reliable Memories | http://arxiv.org/abs/1602.01887 | id:1602.01887 author:Shu Wang, Shaoting Zhang, Wei Liu, Dimitris N. Metaxas category:cs.CV  published:2016-02-04 summary:In this paper, we propose a novel visual tracking framework that intelligently discovers reliable patterns from a wide range of video to resist drift error for long-term tracking tasks. First, we design a Discrete Fourier Transform (DFT) based tracker which is able to exploit a large number of tracked samples while still ensures real-time performance. Second, we propose a clustering method with temporal constraints to explore and memorize consistent patterns from previous frames, named as reliable memories. By virtue of this method, our tracker can utilize uncontaminated information to alleviate drifting issues. Experimental results show that our tracker performs favorably against other state of-the-art methods on benchmark datasets. Furthermore, it is significantly competent in handling drifts and able to robustly track challenging long videos over 4000 frames, while most of others lose track at early frames. version:2
arxiv-1601-01102 | A Survey on Social Media Anomaly Detection | http://arxiv.org/abs/1601.01102 | id:1601.01102 author:Rose Yu, Huida Qiu, Zhen Wen, Ching-Yung Lin, Yan Liu category:cs.LG cs.SI  published:2016-01-06 summary:Social media anomaly detection is of critical importance to prevent malicious activities such as bullying, terrorist attack planning, and fraud information dissemination. With the recent popularity of social media, new types of anomalous behaviors arise, causing concerns from various parties. While a large amount of work have been dedicated to traditional anomaly detection problems, we observe a surge of research interests in the new realm of social media anomaly detection. In this paper, we present a survey on existing approaches to address this problem. We focus on the new type of anomalous phenomena in the social media and review the recent developed techniques to detect those special types of anomalies. We provide a general overview of the problem domain, common formulations, existing methodologies and potential directions. With this work, we hope to call out the attention from the research community on this challenging problem and open up new directions that we can contribute in the future. version:2
arxiv-1602-05568 | Multi-layer Representation Learning for Medical Concepts | http://arxiv.org/abs/1602.05568 | id:1602.05568 author:Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coffey, Jimeng Sun category:cs.LG  published:2016-02-17 summary:Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification. Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit. This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit. In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation. version:1
arxiv-1602-05563 | Robust Kernel (Cross-) Covariance Operators in Reproducing Kernel Hilbert Space toward Kernel Methods | http://arxiv.org/abs/1602.05563 | id:1602.05563 author:Md. Ashad Alam, Kenji Fukumizu, Yu-Ping Wang category:stat.ML  published:2016-02-17 summary:To the best of our knowledge, there are no general well-founded robust methods for statistical unsupervised learning. Most of the unsupervised methods explicitly or implicitly depend on the kernel covariance operator (kernel CO) or kernel cross-covariance operator (kernel CCO). They are sensitive to contaminated data, even when using bounded positive definite kernels. First, we propose robust kernel covariance operator (robust kernel CO) and robust kernel crosscovariance operator (robust kernel CCO) based on a generalized loss function instead of the quadratic loss function. Second, we propose influence function of classical kernel canonical correlation analysis (classical kernel CCA). Third, using this influence function, we propose a visualization method to detect influential observations from two sets of data. Finally, we propose a method based on robust kernel CO and robust kernel CCO, called robust kernel CCA, which is designed for contaminated data and less sensitive to noise than classical kernel CCA. The principles we describe also apply to many kernel methods which must deal with the issue of kernel CO or kernel CCO. Experiments on synthesized and imaging genetics analysis demonstrate that the proposed visualization and robust kernel CCA can be applied effectively to both ideal data and contaminated data. The robust methods show the superior performance over the state-of-the-art methods. version:1
arxiv-1602-02658 | Graying the black box: Understanding DQNs | http://arxiv.org/abs/1602.02658 | id:1602.02658 author:Tom Zahavy, Nir Ben Zrihem, Shie Mannor category:cs.LG cs.AI cs.NE  published:2016-02-08 summary:In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize of deep neural networks in Reinforcement Learning. version:3
arxiv-1511-00758 | High-Performance and Tunable Stereo Reconstruction | http://arxiv.org/abs/1511.00758 | id:1511.00758 author:Sudeep Pillai, Srikumar Ramalingam, John J. Leonard category:cs.RO cs.CV  published:2015-11-03 summary:Traditional stereo algorithms have focused their efforts on reconstruction quality and have largely avoided prioritizing for run time performance. Robots, on the other hand, require quick maneuverability and effective computation to observe its immediate environment and perform tasks within it. In this work, we propose a high-performance and tunable stereo disparity estimation method, with a peak frame-rate of 120Hz (VGA resolution, on a single CPU-thread), that can potentially enable robots to quickly reconstruct their immediate surroundings and maneuver at high-speeds. Our key contribution is a disparity estimation algorithm that iteratively approximates the scene depth via a piece-wise planar mesh from stereo imagery, with a fast depth validation step for semi-dense reconstruction. The mesh is initially seeded with sparsely matched keypoints, and is recursively tessellated and refined as needed (via a resampling stage), to provide the desired stereo disparity accuracy. The inherent simplicity and speed of our approach, with the ability to tune it to a desired reconstruction quality and runtime performance makes it a compelling solution for applications in high-speed vehicles. version:2
arxiv-1511-04402 | Lass-0: sparse non-convex regression by local search | http://arxiv.org/abs/1511.04402 | id:1511.04402 author:William Herlands, Maria De-Arteaga, Daniel Neill, Artur Dubrawski category:stat.ML  published:2015-11-13 summary:We compute approximate solutions to L0 regularized linear regression using L1 regularization, also known as the Lasso, as an initialization step. Our algorithm, the Lass-0 ("Lass-zero"), uses a computationally efficient stepwise search to determine a locally optimal L0 solution given any L1 regularization solution. We present theoretical results of consistency under orthogonality and appropriate handling of redundant features. Empirically, we use synthetic data to demonstrate that Lass-0 solutions are closer to the true sparse support than L1 regularization models. Additionally, in real-world data Lass-0 finds more parsimonious solutions than L1 regularization while maintaining similar predictive accuracy. version:2
arxiv-1602-05450 | Inverse Reinforcement Learning in Swarm Systems | http://arxiv.org/abs/1602.05450 | id:1602.05450 author:Adrian Šošić, Wasiur R. KhudaBukhsh, Abdelhak M. Zoubir, Heinz Koeppl category:stat.ML cs.AI cs.MA cs.SY  published:2016-02-17 summary:Inverse reinforcement learning (IRL) is the problem of recovering a system's latent reward function from observed system behavior. In this paper, we concentrate on IRL in homogeneous large-scale systems, which we refer to as swarms. We show that, by exploiting the inherent homogeneity of a swarm, the IRL objective can be reduced to an equivalent single-agent formulation of constant complexity, which allows us to decompose a global system objective into local subgoals at the agent-level. Based on this finding, we reformulate the corresponding optimal control problem as a fix-point problem pointing towards a symmetric Nash equilibrium, which we solve using a novel heterogeneous learning scheme particularly tailored to the swarm setting. Results on the Vicsek model and the Ising model demonstrate that the proposed framework is able to produce meaningful reward models from which we can learn near-optimal local controllers that replicate the observed system dynamics. version:1
arxiv-1602-05439 | Cell segmentation with random ferns and graph-cuts | http://arxiv.org/abs/1602.05439 | id:1602.05439 author:Arnaud Browet, Christophe De Vleeschouwer, Laurent Jacques, Navrita Mathiah, Bechara Saykali, Isabelle Migeotte category:cs.CV cs.LG  published:2016-02-17 summary:The progress in imaging techniques have allowed the study of various aspect of cellular mechanisms. To isolate individual cells in live imaging data, we introduce an elegant image segmentation framework that effectively extracts cell boundaries, even in the presence of poor edge details. Our approach works in two stages. First, we estimate pixel interior/border/exterior class probabilities using random ferns. Then, we use an energy minimization framework to compute boundaries whose localization is compliant with the pixel class probabilities. We validate our approach on a manually annotated dataset. version:1
arxiv-1602-05436 | Low-Rank Factorization of Determinantal Point Processes for Recommendation | http://arxiv.org/abs/1602.05436 | id:1602.05436 author:Mike Gartrell, Ulrich Paquet, Noam Koenigstein category:stat.ML cs.LG  published:2016-02-17 summary:Determinantal point processes (DPPs) have garnered attention as an elegant probabilistic model of set diversity. They are useful for a number of subset selection tasks, including product recommendation. DPPs are parametrized by a positive semi-definite kernel matrix. In this work we present a new method for learning the DPP kernel from observed data using a low-rank factorization of this kernel. We show that this low-rank factorization enables a learning algorithm that is nearly an order of magnitude faster than previous approaches, while also providing for a method for computing product recommendation predictions that is far faster (up to 20x faster or more for large item catalogs) than previous techniques that involve a full-rank DPP kernel. Furthermore, we show that our method provides equivalent or sometimes better predictive performance than prior full-rank DPP approaches, and better performance than several other competing recommendation methods in many cases. We conduct an extensive experimental evaluation using several real-world datasets in the domain of product recommendation to demonstrate the utility of our method, along with its limitations. version:1
arxiv-1602-05350 | Relative Error Embeddings for the Gaussian Kernel Distance | http://arxiv.org/abs/1602.05350 | id:1602.05350 author:Di Chen, Jeff M. Phillips category:cs.LG  published:2016-02-17 summary:A reproducing kernel can define an embedding of a data point into an infinite dimensional reproducing kernel Hilbert space (RKHS). The norm in this space describes a distance, which we call the kernel distance. The random Fourier features (of Rahimi and Recht) describe an oblivious approximate mapping into finite dimensional Euclidean space that behaves similar to the RKHS. We show in this paper that for the Gaussian kernel the Euclidean norm between these mapped to features has $(1+\epsilon)$-relative error with respect to the kernel distance. When there are $n$ data points, we show that $O((1/\epsilon^2) \log(n))$ dimensions of the approximate feature space are sufficient and necessary. Without a bound on $n$, but when the original points lie in $\mathbb{R}^d$ and have diameter bounded by $\mathcal{M}$, then we show that $O((d/\epsilon^2) \log(\mathcal{M}))$ dimensions are sufficient, and that this many are required, up to $\log(1/\epsilon)$ factors. version:1
arxiv-1602-05332 | Image Restoration: A General Wavelet Frame Based Model and Its Asymptotic Analysis | http://arxiv.org/abs/1602.05332 | id:1602.05332 author:Bin Dong, Zuowei Shen, Peichu Xie category:math.FA cs.CV 42C40  68U10  65D15  published:2016-02-17 summary:Image restoration is one of the most important areas in imaging science. Mathematical tools have been widely used in image restoration, where wavelet frame based approach is one of the successful examples. In this paper, we introduce a generic wavelet frame based image restoration model, called the "general model", which includes most of the existing wavelet frame based models as special cases. Moreover, the general model also includes examples that are new to the literature. Motivated by our earlier studies [1-3], We provide an asymptotic analysis of the general model as image resolution goes to infinity, which establishes a connection between the general model in discrete setting and a new variatonal model in continuum setting. The variational model also includes some of the existing variational models as special cases, such as the total generalized variational model proposed by [4]. In the end, we introduce an algorithm solving the general model and present one numerical simulation as an example. version:1
arxiv-1511-04707 | Deep Linear Discriminant Analysis | http://arxiv.org/abs/1511.04707 | id:1511.04707 author:Matthias Dorfer, Rainer Kelz, Gerhard Widmer category:cs.LG  published:2015-11-15 summary:We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns linearly separable latent representations in an end-to-end fashion. Classic LDA extracts features which preserve class separability and is used for dimensionality reduction for many classification problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear extension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which: (a) have low variance within the same class and (b) high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and CIFAR-10 and outperforms a network trained with categorical cross entropy (same architecture) on a supervised setting of STL-10. version:5
arxiv-1508-00506 | A variational approach to path estimation and parameter inference of hidden diffusion processes | http://arxiv.org/abs/1508.00506 | id:1508.00506 author:Tobias Sutter, Arnab Ganguly, Heinz Koeppl category:math.OC cs.LG cs.SY math.PR math.ST stat.TH  published:2015-08-03 summary:We consider a hidden Markov model, where the signal process, given by a diffusion, is only indirectly observed through some noisy measurements. The article develops a variational method for approximating the hidden states of the signal process given the full set of observations. This, in particular, leads to systematic approximations of the smoothing densities of the signal process. The paper then demonstrates how an efficient inference scheme, based on this variational approach to the approximation of the hidden states, can be designed to estimate the unknown parameters of stochastic differential equations. Two examples at the end illustrate the efficacy and the accuracy of the presented method. version:2
arxiv-1602-05314 | PlaNet - Photo Geolocation with Convolutional Neural Networks | http://arxiv.org/abs/1602.05314 | id:1602.05314 author:Tobias Weyand, Ilya Kostrikov, James Philbin category:cs.CV  published:2016-02-17 summary:Is it possible to build a system to determine the location where a photo was taken using just its pixels? In general, the problem seems exceptionally difficult: it is trivial to construct situations where no location can be inferred. Yet images often contain informative cues such as landmarks, weather patterns, vegetation, road markings, and architectural details, which in combination may allow one to determine an approximate location and occasionally an exact location. Websites such as GeoGuessr and View from your Window suggest that humans are relatively good at integrating these cues to geolocate images, especially en-masse. In computer vision, the photo geolocation problem is usually approached using image retrieval methods. In contrast, we pose the problem as one of classification by subdividing the surface of the earth into thousands of multi-scale geographic cells, and train a deep network using millions of geotagged images. While previous approaches only recognize landmarks or perform approximate matching using global image descriptors, our model is able to use and integrate multiple visible cues. We show that the resulting model, called PlaNet, outperforms previous approaches and even attains superhuman levels of accuracy in some cases. Moreover, we extend our model to photo albums by combining it with a long short-term memory (LSTM) architecture. By learning to exploit temporal coherence to geolocate uncertain photos, we demonstrate that this model achieves a 50% performance improvement over the single-image model. version:1
arxiv-1602-05312 | Density-based Denoising of Point Cloud | http://arxiv.org/abs/1602.05312 | id:1602.05312 author:Faisal Zaman, Ya Ping Wong, Boon Yian Ng category:cs.CV  published:2016-02-17 summary:Point cloud source data for surface reconstruction is usually contaminated with noise and outliers. To overcome this deficiency, a density-based point cloud denoising method is presented to remove outliers and noisy points. First, particle-swam optimization technique is employed for automatically approximating optimal bandwidth of multivariate kernel density estimation to ensure the robust performance of density estimation. Then, mean-shift based clustering technique is used to remove outliers through a thresholding scheme. After removing outliers from the point cloud, bilateral mesh filtering is applied to smooth the remaining points. The experimental results show that this approach, comparably, is robust and efficient. version:1
arxiv-1509-01692 | Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning | http://arxiv.org/abs/1509.01692 | id:1509.01692 author:Ekaterina Vylomova, Laura Rimell, Trevor Cohn, Timothy Baldwin category:cs.CL  published:2015-09-05 summary:Recent work on word embeddings has shown that simple vector subtraction over pre-trained embeddings is surprisingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word relations, and (2) supervised learning to classify vector differences into relation types. We find that word embeddings capture a surprising amount of information, and that, under suitable supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items. version:3
arxiv-1602-05310 | Large Scale Kernel Learning using Block Coordinate Descent | http://arxiv.org/abs/1602.05310 | id:1602.05310 author:Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, Benjamin Recht category:cs.LG math.OC stat.ML  published:2016-02-17 summary:We demonstrate that distributed block coordinate descent can quickly solve kernel regression and classification problems with millions of data points. Armed with this capability, we conduct a thorough comparison between the full kernel, the Nystr\"om method, and random features on three large classification tasks from various domains. Our results suggest that the Nystr\"om method generally achieves better statistical accuracy than random features, but can require significantly more iterations of optimization. Lastly, we derive new rates for block coordinate descent which support our experimental findings when specialized to kernel methods. version:1
arxiv-1602-05307 | Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding | http://arxiv.org/abs/1602.05307 | id:1602.05307 author:Xiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Jiawei Han category:cs.CL cs.LG  published:2016-02-17 summary:Current systems of fine-grained entity typing use distant supervision in conjunction with existing knowledge bases to assign categories (type labels) to entity mentions. However, the type labels so obtained from knowledge bases are often noisy (i.e., incorrect for the entity mention's local context). We define a new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic identification of correct type labels (type-paths) for training examples, given the set of candidate type labels obtained by distant supervision with a given type hierarchy. The unknown type labels for individual entity mentions and the semantic similarity between entity types pose unique challenges for solving the LNR task. We propose a general framework, called PLE, to jointly embed entity mentions, text features and entity types into the same low-dimensional space where, in that space, objects whose types are semantically close have similar representations. Then we estimate the type-path for each training example in a top-down manner using the learned embeddings. We formulate a global objective for learning the embeddings from text corpora and knowledge bases, which adopts a novel margin-based loss that is robust to noisy labels and faithfully models type correlation derived from knowledge bases. Our experiments on three public typing datasets demonstrate the effectiveness and robustness of PLE, with an average of 25% improvement in accuracy compared to next best method. version:1
arxiv-1602-04889 | Multi-Source Domain Adaptation Using Approximate Label Matching | http://arxiv.org/abs/1602.04889 | id:1602.04889 author:Jordan T. Ash, Robert E. Schapire category:cs.LG cs.AI  published:2016-02-16 summary:Domain adaptation, and transfer learning more generally, seeks to remedy the problem created when training and testing datasets are generated by different distributions. In this work, we introduce a new unsupervised domain adaptation algorithm for when there are multiple sources available to a learner. Our technique assigns a rough labeling on the target samples, then uses it to learn a transformation that aligns the two datasets before final classification. In this article we give a convenient implementation of our method, show several experiments using it, and compare it to other methods commonly used in the field. version:2
arxiv-1602-05292 | Authorship Attribution Using a Neural Network Language Model | http://arxiv.org/abs/1602.05292 | id:1602.05292 author:Zhenhao Ge, Yufang Sun, Mark J. T. Smith category:cs.CL cs.AI  published:2016-02-17 summary:In practice, training language models for individual authors is often expensive because of limited data resources. In such cases, Neural Network Language Models (NNLMs), generally outperform the traditional non-parametric N-gram models. Here we investigate the performance of a feed-forward NNLM on an authorship attribution problem, with moderate author set size and relatively limited data. We also consider how the text topics impact performance. Compared with a well-constructed N-gram baseline method with Kneser-Ney smoothing, the proposed method achieves nearly 2:5% reduction in perplexity and increases author classification accuracy by 3:43% on average, given as few as 5 test sentences. The performance is very competitive with the state of the art in terms of accuracy and demand on test data. The source code, preprocessed datasets, a detailed description of the methodology and results are available at https://github.com/zge/authorship-attribution. version:1
arxiv-1602-05285 | Choice by Elimination via Deep Neural Networks | http://arxiv.org/abs/1602.05285 | id:1602.05285 author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.IR cs.LG  published:2016-02-17 summary:We introduce Neural Choice by Elimination, a new framework that integrates deep neural networks into probabilistic sequential choice models for learning to rank. Given a set of items to chose from, the elimination strategy starts with the whole item set and iteratively eliminates the least worthy item in the remaining subset. We prove that the choice by elimination is equivalent to marginalizing out the random Gompertz latent utilities. Coupled with the choice model is the recently introduced Neural Highway Networks for approximating arbitrarily complex rank functions. We evaluate the proposed framework on a large-scale public dataset with over 425K items, drawn from the Yahoo! learning to rank challenge. It is demonstrated that the proposed method is competitive against state-of-the-art learning to rank methods. version:1
arxiv-1509-08333 | High-dimensional Time Series Prediction with Missing Values | http://arxiv.org/abs/1509.08333 | id:1509.08333 author:Hsiang-Fu Yu, Nikhil Rao, Inderjit S. Dhillon category:cs.LG stat.ML  published:2015-09-28 summary:High-dimensional time series prediction is needed in applications as diverse as demand forecasting and climatology. Often, such applications require methods that are both highly scalable, and deal with noisy data in terms of corruptions or missing values. Classical time series methods usually fall short of handling both these issues. In this paper, we propose to adapt matrix matrix completion approaches that have previously been successfully applied to large scale noisy data, but which fail to adequately model high-dimensional time series due to temporal dependencies. We present a novel temporal regularized matrix factorization (TRMF) framework which supports data-driven temporal dependency learning and enables forecasting ability to our new matrix factorization approach. TRMF is highly general, and subsumes many existing matrix factorization approaches for time series data. We make interesting connections to graph regularized matrix factorization methods in the context of learning the dependencies. Experiments on both real and synthetic data show that TRMF outperforms several existing approaches for common time series tasks. version:3
arxiv-1602-05572 | A landmark-based algorithm for automatic pattern recognition and abnormality detection | http://arxiv.org/abs/1602.05572 | id:1602.05572 author:S. Huzurbazar, Long Lee, Dongyang Kuang category:cs.CV  published:2016-02-17 summary:We study a class of mathematical and statistical algorithms with the aim of establishing a computer-based framework for fast and reliable automatic pattern recognition and abnormality detection. Under this framework, we propose a numerical algorithm for finding group averages where an average of a group is an estimator that is said to best represent the properties of interest of that group. A novelty of the proposed landmark-based algorithm is that the algorithm tracks information of the momentum field through the geodesic shooting process. The momentum field provides a local template-based coordinate system and is linear in nature. It is also a dual of the velocity field with respect to an assigned base template, yielding advantages for statistical analyses. We apply this framework to a small brain image database for detecting structure abnormality. The brain structure changes identified by our framework are highly consistent with studies in the literature. version:1
arxiv-1602-05264 | Anomaly Detection in Clutter using Spectrally Enhanced Ladar | http://arxiv.org/abs/1602.05264 | id:1602.05264 author:Puneet S Chhabra, Andrew M Wallace, James R Hopgood category:physics.optics cs.LG physics.ins-det stat.AP stat.ML  published:2016-02-17 summary:Discrete return (DR) Laser Detection and Ranging (Ladar) systems provide a series of echoes that reflect from objects in a scene. These can be first, last or multi-echo returns. In contrast, Full-Waveform (FW)-Ladar systems measure the intensity of light reflected from objects continuously over a period of time. In a camouflaged scenario, e.g., objects hidden behind dense foliage, a FW-Ladar penetrates such foliage and returns a sequence of echoes including buried faint echoes. The aim of this paper is to learn local-patterns of co-occurring echoes characterised by their measured spectra. A deviation from such patterns defines an abnormal event in a forest/tree depth profile. As far as the authors know, neither DR or FW-Ladar, along with several spectral measurements, has not been applied to anomaly detection. This work presents an algorithm that allows detection of spectral and temporal anomalies in FW-Multi Spectral Ladar (FW-MSL) data samples. An anomaly is defined as a full waveform temporal and spectral signature that does not conform to a prior expectation, represented using a learnt subspace (dictionary) and set of coefficients that capture co-occurring local-patterns using an overlapping temporal window. A modified optimization scheme is proposed for subspace learning based on stochastic approximations. The objective function is augmented with a discriminative term that represents the subspace's separability properties and supports anomaly characterisation. The algorithm detects several man-made objects and anomalous spectra hidden in a dense clutter of vegetation and also allows tree species classification. version:1
arxiv-1602-05256 | 2D SEM images turn into 3D object models | http://arxiv.org/abs/1602.05256 | id:1602.05256 author:Wichai Shanklin category:cs.CV cs.GR  published:2016-02-17 summary:The scanning electron microscopy (SEM) is probably one the most fascinating examination approach that has been used since more than two decades to detailed inspection of micro scale objects. Most of the scanning electron microscopes could only produce 2D images that could not assist operational analysis of microscopic surface properties. Computer vision algorithms combined with very advanced geometry and mathematical approaches turn any SEM into a full 3D measurement device. This work focuses on a methodical literature review for automatic 3D surface reconstruction of scanning electron microscope images. version:1
arxiv-1506-08230 | Convolutional networks and learning invariant to homogeneous multiplicative scalings | http://arxiv.org/abs/1506.08230 | id:1506.08230 author:Mark Tygert, Arthur Szlam, Soumith Chintala, Marc'Aurelio Ranzato, Yuandong Tian, Wojciech Zaremba category:cs.LG cs.NE  published:2015-06-26 summary:The conventional classification schemes -- notably multinomial logistic regression -- used in conjunction with convolutional networks (convnets) are classical in statistics, designed without consideration for the usual coupling with convnets, stochastic gradient descent, and backpropagation. In the specific application to supervised learning for convnets, a simple scale-invariant classification stage turns out to be more robust than multinomial logistic regression, appears to result in slightly lower errors on several standard test sets, has similar computational costs, and features precise control over the actual rate of learning. "Scale-invariant" means that multiplying the input values by any nonzero scalar leaves the output unchanged. version:4
arxiv-1602-05236 | A Sparse PCA Approach to Clustering | http://arxiv.org/abs/1602.05236 | id:1602.05236 author:T. Tony Cai, Linjun Zhang category:stat.ME stat.ML  published:2016-02-16 summary:We discuss a clustering method for Gaussian mixture model based on the sparse principal component analysis (SPCA) method and compare it with the IF-PCA method. We also discuss the dependent case where the covariance matrix $\Sigma$ is not necessarily diagonal. version:1
arxiv-1407-0581 | Support Consistency of Direct Sparse-Change Learning in Markov Networks | http://arxiv.org/abs/1407.0581 | id:1407.0581 author:Song Liu, Taiji Suzuki, Raissa Relator, Jun Sese, Masashi Sugiyama, Kenji Fukumizu category:stat.ML  published:2014-07-02 summary:We study the problem of learning sparse structure changes between two Markov networks $P$ and $Q$. Rather than fitting two Markov networks separately to two sets of data and figuring out their differences, a recent work proposed to learn changes \emph{directly} via estimating the ratio between two Markov network models. In this paper, we give sufficient conditions for \emph{successful change detection} with respect to the sample size $n_p, n_q$, the dimension of data $m$, and the number of changed edges $d$. When using an unbounded density ratio model we prove that the true sparse changes can be consistently identified for $n_p = \Omega(d^2 \log \frac{m^2+m}{2})$ and $n_q = \Omega({n_p^2})$, with an exponentially decaying upper-bound on learning error. Such sample complexity can be improved to $\min(n_p, n_q) = \Omega(d^2 \log \frac{m^2+m}{2})$ when the boundedness of the density ratio model is assumed. Our theoretical guarantee can be applied to a wide range of discrete/continuous Markov networks. version:10
arxiv-1602-05168 | An Approach for Noise Removal on Depth Images | http://arxiv.org/abs/1602.05168 | id:1602.05168 author:Rashi Chaudhary, Himanshu Dasgupta category:cs.CV  published:2016-02-16 summary:Image based rendering is a fundamental problem in computer vision and graphics. Modern techniques often rely on depth image for the 3D construction. However for most of the existing depth cameras, the large and unpredictable noises can be problematic, which can cause noticeable artifacts in the rendered results. In this paper, we proposed an efficacious method for depth image noise removal that can be applied for most RGBD systems. The proposed solution will benefit many subsequent vision problems such as 3D reconstruction, novel view rendering, object recognition. Our experimental results demonstrate the efficacy and accuracy. version:1
arxiv-1602-05161 | Fast Learning Requires Good Memory: A Time-Space Lower Bound for Parity Learning | http://arxiv.org/abs/1602.05161 | id:1602.05161 author:Ran Raz category:cs.LG cs.CC cs.CR  published:2016-02-16 summary:We prove that any algorithm for learning parities requires either a memory of quadratic size or an exponential number of samples. This proves a recent conjecture of Steinhardt, Valiant and Wager and shows that for some learning problems a large storage space is crucial. More formally, in the problem of parity learning, an unknown string $x \in \{0,1\}^n$ was chosen uniformly at random. A learner tries to learn $x$ from a stream of samples $(a_1, b_1), (a_2, b_2) \ldots$, where each~$a_t$ is uniformly distributed over $\{0,1\}^n$ and $b_t$ is the inner product of $a_t$ and $x$, modulo~2. We show that any algorithm for parity learning, that uses less than $\frac{n^2}{25}$ bits of memory, requires an exponential number of samples. Previously, there was no non-trivial lower bound on the number of samples needed, for any learning problem, even if the allowed memory size is $O(n)$ (where $n$ is the space needed to store one sample). We also give an application of our result in the field of bounded-storage cryptography. We show an encryption scheme that requires a private key of length $n$, as well as time complexity of $n$ per encryption/decription of each bit, and is provenly and unconditionally secure as long as the attacker uses less than $\frac{n^2}{25}$ memory bits and the scheme is used at most an exponential number of times. Previous works on bounded-storage cryptography assumed that the memory size used by the attacker is at most linear in the time needed for encryption/decription. version:1
arxiv-1510-03105 | Kernel Sequential Monte Carlo | http://arxiv.org/abs/1510.03105 | id:1510.03105 author:Ingmar Schuster, Heiko Strathmann, Brooks Paige, Dino Sejdinovic category:stat.CO stat.ML  published:2015-10-11 summary:Bayesian posterior inference with Monte Carlo methods has a fundamental role in statistics and probabilistic machine learning. Target posterior distributions arising in increasingly complex models often exhibit high degrees of nonlinearity and multimodality and pose substantial challenges to traditional samplers. We propose the Kernel Sequential Monte Carlo (KSMC) framework for building emulator models of the current particle system in a Reproducing Kernel Hilbert Space and use the emulator's geometry to inform local proposals. KSMC is applicable when gradients are unknown or prohibitively expensive and inherits the superior performance of SMC on multi-modal targets and its ability to estimate model evidence. Strengths of the proposed methodology are demonstrated on a series of challenging synthetic and real-world examples. version:3
arxiv-1602-05128 | Interacting Particle Markov Chain Monte Carlo | http://arxiv.org/abs/1602.05128 | id:1602.05128 author:Tom Rainforth, Christian A. Naesseth, Fredrik Lindsten, Brooks Paige, Jan-Willem van de Meent, Arnaud Doucet, Frank Wood category:stat.CO stat.ML  published:2016-02-16 summary:We introduce interacting particle Markov chain Monte Carlo (iPMCMC), a PMCMC method that introduces a coupling between multiple standard and conditional sequential Monte Carlo samplers. Like related methods, iPMCMC is a Markov chain Monte Carlo sampler on an extended space. We present empirical results that show significant improvements in mixing rates relative to both non-interacting PMCMC samplers and a single PMCMC sampler with an equivalent total computational budget. An additional advantage of the iPMCMC method is that it is suitable for distributed and multi-core architectures. version:1
arxiv-1602-05127 | A Harmonic Extension Approach for Collaborative Ranking | http://arxiv.org/abs/1602.05127 | id:1602.05127 author:Da Kuang, Zuoqiang Shi, Stanley Osher, Andrea Bertozzi category:cs.LG  published:2016-02-16 summary:We present a new perspective on graph-based methods for collaborative ranking for recommender systems. Unlike user-based or item-based methods that compute a weighted average of ratings given by the nearest neighbors, or low-rank approximation methods using convex optimization and the nuclear norm, we formulate matrix completion as a series of semi-supervised learning problems, and propagate the known ratings to the missing ones on the user-user or item-item graph globally. The semi-supervised learning problems are expressed as Laplace-Beltrami equations on a manifold, or namely, harmonic extension, and can be discretized by a point integral method. We show that our approach does not impose a low-rank Euclidean subspace on the data points, but instead minimizes the dimension of the underlying manifold. Our method, named LDM (low dimensional manifold), turns out to be particularly effective in generating rankings of items, showing decent computational efficiency and robust ranking quality compared to state-of-the-art methods. version:1
arxiv-1602-05124 | Practical Introduction to Clustering Data | http://arxiv.org/abs/1602.05124 | id:1602.05124 author:Alexander K. Hartmann category:physics.data-an astro-ph.IM cond-mat.stat-mech cs.LG  published:2016-02-16 summary:Data clustering is an approach to seek for structure in sets of complex data, i.e., sets of "objects". The main objective is to identify groups of objects which are similar to each other, e.g., for classification. Here, an introduction to clustering is given and three basic approaches are introduced: the k-means algorithm, neighbour-based clustering, and an agglomerative clustering method. For all cases, C source code examples are given, allowing for an easy implementation. version:1
arxiv-1511-06241 | Convolutional Clustering for Unsupervised Learning | http://arxiv.org/abs/1511.06241 | id:1511.06241 author:Aysegul Dundar, Jonghoon Jin, Eugenio Culurciello category:cs.LG cs.CV  published:2015-11-19 summary:The task of labeling data for training deep neural networks is daunting and tedious, requiring millions of labels to achieve the current state-of-the-art results. Such reliance on large amounts of labeled data can be relaxed by exploiting hierarchical features via unsupervised learning techniques. In this work, we propose to train a deep convolutional network based on an enhanced version of the k-means clustering algorithm, which reduces the number of correlated parameters in the form of similar filters, and thus increases test categorization accuracy. We call our algorithm convolutional k-means clustering. We further show that learning the connection between the layers of a deep convolutional neural network improves its ability to be trained on a smaller amount of labeled data. Our experiments show that the proposed algorithm outperforms other techniques that learn filters unsupervised. Specifically, we obtained a test accuracy of 74.1% on STL-10 and a test error of 0.5% on MNIST. version:2
arxiv-1307-7521 | Union of Low-Rank Subspaces Detector | http://arxiv.org/abs/1307.7521 | id:1307.7521 author:Mohsen Joneidi, Parvin Ahmadi, Mostafa Sadeghi, Nazanin Rahnavard category:cs.IT cs.CV math.IT  published:2013-07-29 summary:The problem of signal detection using a flexible and general model is considered. Due to applicability and flexibility of sparse signal representation and approximation, it has attracted a lot of attention in many signal processing areas. In this paper, we propose a new detection method based on sparse decomposition in a union of subspaces (UoS) model. Our proposed detector uses a dictionary that can be interpreted as a bank of matched subspaces. This improves the performance of signal detection, as it is a generalization for detectors. Low-rank assumption for the desired signals implies that the representations of these signals in terms of some proper bases would be sparse. Our proposed detector exploits sparsity in its decision rule. We demonstrate the high efficiency of our method in the cases of voice activity detection in speech processing. version:6
arxiv-1511-04143 | Deep Reinforcement Learning in Parameterized Action Space | http://arxiv.org/abs/1511.04143 | id:1511.04143 author:Matthew Hausknecht, Peter Stone category:cs.AI cs.LG cs.MA cs.NE  published:2015-11-13 summary:Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agent can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs. version:4
arxiv-1511-00296 | Limiting fitness distributions in evolutionary dynamics | http://arxiv.org/abs/1511.00296 | id:1511.00296 author:Matteo Smerlak, Ahmed Youssef category:q-bio.PE cond-mat.stat-mech cs.NE  published:2015-11-01 summary:Darwinian evolution can be modeled in general terms as a flow in the space of fitness (i.e. reproductive rate) distributions. In the diffusion approximation, Tsimring et al. have showed that this flow admits "fitness wave" solutions: Gaussian-shape fitness distributions moving towards higher fitness values at constant speed. Here we show more generally that evolving fitness distributions are attracted to a one-parameter family of distributions with a fixed parabolic relationship between skewness and kurtosis. Unlike fitness waves, this statistical pattern encompasses both positive and negative (a.k.a. purifying) selection and is not restricted to rapidly adapting populations. Moreover we find that the mean fitness of a population under the selection of pre-existing variation is a power-law function of time, as observed in microbiological evolution experiments but at variance with fitness wave theory. At the conceptual level, our results can be viewed as the resolution of the "dynamic insufficiency" of Fisher's fundamental theorem of natural selection. Our predictions are in good agreement with numerical simulations. version:2
arxiv-1511-03483 | An Analytic Expression of Performance Rate, Fitness Value and Average Convergence Rate for a Class of Evolutionary Algorithms | http://arxiv.org/abs/1511.03483 | id:1511.03483 author:Jun He category:cs.NE  published:2015-11-11 summary:An important theoretical question in evolutionary computation is how good solutions evolutionary algorithms can produce. This paper aims to provide an analytic analysis of solution quality of evolutionary algorithms in terms of the performance rate, which is defined by the difference between 1 and the approximation ratio of the best solution found in each generation. The performance rate can be represented by a function of time. With the help of matrix analysis, it is possible to obtain an exact expression of such a function. For the first time, an analytic expression for calculating the performance rate is presented in this paper for a class of evolutionary algorithms, that is, (1+1) strictly elitist evolution algorithms. Furthermore, analytic expressions for calculate the fitness value and the average convergence rate in each generation are also derived for this class of evolutionary algorithms. The approach is promising, and it can be extended to non-elitist or population-based algorithms too. version:2
arxiv-1602-05012 | A Subsequence Interleaving Model for Sequential Pattern Mining | http://arxiv.org/abs/1602.05012 | id:1602.05012 author:Jaroslav Fowkes, Charles Sutton category:stat.ML cs.AI cs.LG  published:2016-02-16 summary:Recent sequential pattern mining methods have used the minimum description length (MDL) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database. We present a novel subsequence interleaving model based on a probabilistic model of the sequence database, which allows us to search for the most compressing set of patterns without designing a specific encoding scheme. Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness. The efficient inference in our model is a direct result of our use of a structural expectation-maximization framework, in which the expectation-step takes the form of a submodular optimization problem subject to a coverage constraint. We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spuriousness and redundancy, high interpretability and usefulness in real-world applications. Furthermore, we demonstrate that the quality of the patterns from our approach is comparable to, if not better than, existing state of the art sequential pattern mining algorithms. version:1
arxiv-1412-2620 | Cells in Multidimensional Recurrent Neural Networks | http://arxiv.org/abs/1412.2620 | id:1412.2620 author:G. Leifert, T. Strauß, T. Grüning, R. Labahn category:cs.AI cs.NE 68T10  68T05  published:2014-12-08 summary:The transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi-dimensional recurrent neural networks (MDRNN) with connectionist temporal classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multi-dimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells. version:2
arxiv-1602-04983 | Contextual Media Retrieval Using Natural Language Queries | http://arxiv.org/abs/1602.04983 | id:1602.04983 author:Sreyasi Nag Chowdhury, Mateusz Malinowski, Andreas Bulling, Mario Fritz category:cs.IR cs.AI cs.CL cs.CV cs.HC  published:2016-02-16 summary:The widespread integration of cameras in hand-held and head-worn devices as well as the ability to share content online enables a large and diverse visual capture of the world that millions of users build up collectively every day. We envision these images as well as associated meta information, such as GPS coordinates and timestamps, to form a collective visual memory that can be queried while automatically taking the ever-changing context of mobile users into account. As a first step towards this vision, in this work we present Xplore-M-Ego: a novel media retrieval system that allows users to query a dynamic database of images and videos using spatio-temporal natural language queries. We evaluate our system using a new dataset of real user queries as well as through a usability study. One key finding is that there is a considerable amount of inter-user variability, for example in the resolution of spatial relations in natural language utterances. We show that our retrieval system can cope with this variability using personalisation through an online learning-based retrieval formulation. version:1
arxiv-1602-04981 | Optimizing Gaze Direction in a Visual Navigation Task | http://arxiv.org/abs/1602.04981 | id:1602.04981 author:Tuomas Välimäki, Risto Ritala category:cs.RO cs.CV  published:2016-02-16 summary:Navigation in an unknown environment consists of multiple separable subtasks, such as collecting information about the surroundings and navigating to the current goal. In the case of pure visual navigation, all these subtasks need to utilize the same vision system, and therefore a way to optimally control the direction of focus is needed. We present a case study, where we model the active sensing problem of directing the gaze of a mobile robot with three machine vision cameras as a partially observable Markov decision process (POMDP) using a mutual information (MI) based reward function. The key aspect of the solution is that the cameras are dynamically used either in monocular or stereo configuration. The benefits of using the proposed active sensing implementation are demonstrated with simulations and experiments on a real robot. version:1
arxiv-1602-04976 | Stochastic Process Bandits: Upper Confidence Bounds Algorithms via Generic Chaining | http://arxiv.org/abs/1602.04976 | id:1602.04976 author:Emile Contal, Nicolas Vayatis category:stat.ML cs.LG  published:2016-02-16 summary:The paper considers the problem of global optimization in the setup of stochastic process bandits. We introduce an UCB algorithm which builds a cascade of discretization trees based on generic chaining in order to render possible his operability over a continuous domain. The theoretical framework applies to functions under weak probabilistic smoothness assumptions and also extends significantly the spectrum of application of UCB strategies. Moreover generic regret bounds are derived which are then specialized to Gaussian processes indexed on infinite-dimensional spaces as well as to quadratic forms of Gaussian processes. Lower bounds are also proved in the case of Gaussian processes to assess the optimality of the proposed algorithm. version:1
arxiv-1512-04086 | Deep Learning-Based Image Kernel for Inductive Transfer | http://arxiv.org/abs/1512.04086 | id:1512.04086 author:Neeraj Kumar, Animesh Karmakar, Ranti Dev Sharma, Abhinav Mittal, Amit Sethi category:cs.CV  published:2015-12-13 summary:We propose a method to classify images from target classes with a small number of training examples based on transfer learning from non-target classes. Without using any more information than class labels for samples from non-target classes, we train a Siamese net to estimate the probability of two images to belong to the same class. With some post-processing, output of the Siamese net can be used to form a gram matrix of a Mercer kernel. Coupled with a support vector machine (SVM), such a kernel gave reasonable classification accuracy on target classes without any fine-tuning. When the Siamese net was only partially fine-tuned using a small number of samples from the target classes, the resulting classifier outperformed the state-of-the-art and other alternatives. We share class separation capabilities and insights into the learning process of such a kernel on MNIST, Dogs vs. Cats, and CIFAR-10 datasets. version:3
arxiv-1602-04951 | Q($λ$) with Off-Policy Corrections | http://arxiv.org/abs/1602.04951 | id:1602.04951 author:Anna Harutyunyan, Marc G. Bellemare, Tom Stepleton, Remi Munos category:cs.AI cs.LG stat.ML  published:2016-02-16 summary:We propose and analyze an alternate approach to off-policy multi-step temporal difference learning, in which off-policy returns are corrected with the current Q-function in terms of rewards, rather than with the target policy in terms of transition probabilities. We prove that such approximate corrections are sufficient for off-policy convergence both in policy evaluation and control, provided certain conditions. These conditions relate the distance between the target and behavior policies, the eligibility trace parameter and the discount factor, and formalize an underlying tradeoff in off-policy TD($\lambda$). We illustrate this theoretical relationship empirically on a continuous-state control task. version:1
arxiv-1602-04938 | "Why Should I Trust You?": Explaining the Predictions of Any Classifier | http://arxiv.org/abs/1602.04938 | id:1602.04938 author:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin category:cs.LG cs.AI stat.ML  published:2016-02-16 summary:Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. Such understanding further provides insights into the model, which can be used to turn an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We further propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). The usefulness of explanations is shown via novel experiments, both simulated and with human subjects. Our explanations empower users in various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and detecting why a classifier should not be trusted. version:1
arxiv-1602-04933 | Greedy Ants Colony Optimization Strategy for Solving the Curriculum Based University Course Timetabling Problem | http://arxiv.org/abs/1602.04933 | id:1602.04933 author:Patrick Kenekayoro, Godswill Zipamone category:cs.NE  published:2016-02-16 summary:Timetabling is a problem faced in all higher education institutions. The International Timetabling Competition (ITC) has published a dataset that can be used to test the quality of methods used to solve this problem. A number of meta-heuristic approaches have obtained good results when tested on the ITC dataset, however few have used the ant colony optimization technique, particularly on the ITC 2007 curriculum based university course timetabling problem. This study describes an ant system that solves the curriculum based university course timetabling problem and the quality of the algorithm is tested on the ITC 2007 dataset. The ant system was able to find feasible solutions in all instances of the dataset and close to optimal solutions in some instances. The ant system performs better than some published approaches, however results obtained are not as good as those obtained by the best published approaches. This study may be used as a benchmark for ant based algorithms that solve the curriculum based university course timetabling problem. version:1
arxiv-1505-03504 | Loop-corrected belief propagation for lattice spin models | http://arxiv.org/abs/1505.03504 | id:1505.03504 author:Hai-Jun Zhou, Wei-Mou Zheng category:cond-mat.stat-mech cond-mat.dis-nn cs.CV  published:2015-05-13 summary:Belief propagation (BP) is a message-passing method for solving probabilistic graphical models. It is very successful in treating disordered models (such as spin glasses) on random graphs. On the other hand, finite-dimensional lattice models have an abundant number of short loops, and the BP method is still far from being satisfactory in treating the complicated loop-induced correlations in these systems. Here we propose a loop-corrected BP method to take into account the effect of short loops in lattice spin models. We demonstrate, through an application to the square-lattice Ising model, that loop-corrected BP improves over the naive BP method significantly. We also implement loop-corrected BP at the coarse-grained region graph level to further boost its performance. version:3
arxiv-1602-04930 | Generalized minimum dominating set and application in automatic text summarization | http://arxiv.org/abs/1602.04930 | id:1602.04930 author:Yi-Zhi Xu, Hai-Jun Zhou category:cs.IR cond-mat.stat-mech cs.CL physics.soc-ph  published:2016-02-16 summary:For a graph formed by vertices and weighted edges, a generalized minimum dominating set (MDS) is a vertex set of smallest cardinality such that the summed weight of edges from each outside vertex to vertices in this set is equal to or larger than certain threshold value. This generalized MDS problem reduces to the conventional MDS problem in the limiting case of all the edge weights being equal to the threshold value. We treat the generalized MDS problem in the present paper by a replica-symmetric spin glass theory and derive a set of belief-propagation equations. As a practical application we consider the problem of extracting a set of sentences that best summarize a given input text document. We carry out a preliminary test of the statistical physics-inspired method to this automatic text summarization problem. version:1
arxiv-1511-06362 | Efficient inference in occlusion-aware generative models of images | http://arxiv.org/abs/1511.06362 | id:1511.06362 author:Jonathan Huang, Kevin Murphy category:cs.LG cs.CV  published:2015-11-19 summary:We present a generative model of images based on layering, in which image layers are individually generated, then composited from front to back. We are thus able to factor the appearance of an image into the appearance of individual objects within the image --- and additionally for each individual object, we can factor content from pose. Unlike prior work on layered models, we learn a shape prior for each object/layer, allowing the model to tease out which object is in front by looking for a consistent shape, without needing access to motion cues or any labeled data. We show that ordinary stochastic gradient variational bayes (SGVB), which optimizes our fully differentiable lower-bound on the log-likelihood, is sufficient to learn an interpretable representation of images. Finally we present experiments demonstrating the effectiveness of the model for inferring foreground and background objects in images. version:2
arxiv-1602-04924 | Personalized Federated Search at LinkedIn | http://arxiv.org/abs/1602.04924 | id:1602.04924 author:Dhruv Arya, Viet Ha-Thuc, Shakti Sinha category:cs.IR cs.LG  published:2016-02-16 summary:LinkedIn has grown to become a platform hosting diverse sources of information ranging from member profiles, jobs, professional groups, slideshows etc. Given the existence of multiple sources, when a member issues a query like "software engineer", the member could look for software engineer profiles, jobs or professional groups. To tackle this problem, we exploit a data-driven approach that extracts searcher intents from their profile data and recent activities at a large scale. The intents such as job seeking, hiring, content consuming are used to construct features to personalize federated search experience. We tested the approach on the LinkedIn homepage and A/B tests show significant improvements in member engagement. As of writing this paper, the approach powers all of federated search on LinkedIn homepage. version:1
arxiv-1509-06791 | Learning Deep Control Policies for Autonomous Aerial Vehicles with MPC-Guided Policy Search | http://arxiv.org/abs/1509.06791 | id:1509.06791 author:Tianhao Zhang, Gregory Kahn, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO  published:2015-09-22 summary:Model predictive control (MPC) is an effective method for controlling robotic systems, particularly autonomous aerial vehicles such as quadcopters. However, application of MPC can be computationally demanding, and typically requires estimating the state of the system, which can be challenging in complex, unstructured environments. Reinforcement learning can in principle forego the need for explicit state estimation and acquire a policy that directly maps sensor readings to actions, but is difficult to apply to unstable systems that are liable to fail catastrophically during training before an effective policy has been found. We propose to combine MPC with reinforcement learning in the framework of guided policy search, where MPC is used to generate data at training time, under full state observations provided by an instrumented training environment. This data is used to train a deep neural network policy, which is allowed to access only the raw observations from the vehicle's onboard sensors. After training, the neural network policy can successfully control the robot without knowledge of the full state, and at a fraction of the computational cost of MPC. We evaluate our method by learning obstacle avoidance policies for a simulated quadrotor, using simulated onboard sensors and no explicit state estimation at test time. version:2
arxiv-1602-04921 | A diffusion and clustering-based approach for finding coherent motions and understanding crowd scenes | http://arxiv.org/abs/1602.04921 | id:1602.04921 author:Weiyao Lin, Yang Mi, Weiyue Wang, Jianxin Wu, Jingdong Wang, Tao Mei category:cs.CV cs.AI cs.MM  published:2016-02-16 summary:This paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding: semantic region detection and recurrent activity mining. It processes input motion fields (e.g., optical flow fields) and produces a coherent motion filed, named as thermal energy field. The thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them. We further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions. These semantic regions can be used to recognize pre-defined activities in crowd scenes. Finally, we introduce a cluster-and-merge process which automatically discovers recurrent activities in crowd scenes by clustering and merging the extracted coherent motions. Experiments on various videos demonstrate the effectiveness of our approach. version:1
arxiv-1602-04398 | Dimensionality Reduction for Nonlinear Regression with Two Predictor Vectors | http://arxiv.org/abs/1602.04398 | id:1602.04398 author:Yanjun Li, Yoram Bresler category:stat.ML cs.IT cs.LG math.IT  published:2016-02-13 summary:Many variables that we would like to predict depend nonlinearly on two types of attributes. For example, prices are influenced by supply and demand. Movie ratings are determined by demographic attributes and genre attributes. This paper addresses the dimensionality reduction problem in such regression problems with two predictor vectors. In particular, we assume a discriminative model where low-dimensional linear embeddings of the two predictor vectors are sufficient statistics for predicting a dependent variable. We show that a simple algorithm involving singular value decomposition can accurately estimate the embeddings provided that certain sample complexities are satisfied, surprisingly, without specifying the nonlinear regression model. These embeddings improve the efficiency and robustness of subsequent training, and can serve as a pre-training algorithm for neural networks. The main results establish sample complexities under multiple settings. Sample complexities for different regression models only differ by constant factors. version:2
arxiv-1602-04910 | Bayesian generalized fused lasso modeling via NEG distribution | http://arxiv.org/abs/1602.04910 | id:1602.04910 author:Kaito Shimamura, Masao Ueki, Shuichi Kawano, Sadanori Konishi category:stat.ME stat.ML  published:2016-02-16 summary:The fused lasso penalizes a loss function by the $L_1$ norm for both the regression coefficients and their successive differences to encourage sparsity of both. In this paper, we propose a Bayesian generalized fused lasso modeling based on a normal-exponential-gamma (NEG) prior distribution. The NEG prior is assumed into the difference of successive regression coefficients. The proposed method enables us to construct a more versatile sparse model than the ordinary fused lasso by using a flexible regularization term. We also propose a sparse fused algorithm to produce exact sparse solutions. Simulation studies and real data analyses show that the proposed method has superior performance to the ordinary fused lasso. version:1
arxiv-1602-04906 | Segmentation Rectification for Video Cutout via One-Class Structured Learning | http://arxiv.org/abs/1602.04906 | id:1602.04906 author:Junyan Wang, Sai-kit Yeung, Jue Wang, Kun Zhou category:cs.CV cs.GR cs.LG  published:2016-02-16 summary:Recent works on interactive video object cutout mainly focus on designing dynamic foreground-background (FB) classifiers for segmentation propagation. However, the research on optimally removing errors from the FB classification is sparse, and the errors often accumulate rapidly, causing significant errors in the propagated frames. In this work, we take the initial steps to addressing this problem, and we call this new task \emph{segmentation rectification}. Our key observation is that the possibly asymmetrically distributed false positive and false negative errors were handled equally in the conventional methods. We, alternatively, propose to optimally remove these two types of errors. To this effect, we propose a novel bilayer Markov Random Field (MRF) model for this new task. We also adopt the well-established structured learning framework to learn the optimal model from data. Additionally, we propose a novel one-class structured SVM (OSSVM) which greatly speeds up the structured learning process. Our method naturally extends to RGB-D videos as well. Comprehensive experiments on both RGB and RGB-D data demonstrate that our simple and effective method significantly outperforms the segmentation propagation methods adopted in the state-of-the-art video cutout systems, and the results also suggest the potential usefulness of our method in image cutout system. version:1
arxiv-1602-02358 | NED: An Inter-Graph Node Metric Based On Edit Distance | http://arxiv.org/abs/1602.02358 | id:1602.02358 author:Haohan Zhu, Xianrui Meng, George Kollios category:cs.DB cs.LG cs.SI  published:2016-02-07 summary:Node similarity is a fundamental problem in graph analytics. However, node similarity between nodes in different graphs (inter-graph nodes) has not received a lot of attention yet. The inter-graph node similarity is important in learning a new graph based on the knowledge of an existing graph (transfer learning on graphs) and has applications in biological, communication, and social networks. In this paper, we propose a novel distance function for measuring inter-graph node similarity with edit distance, called NED. In NED, two nodes are compared according to their local neighborhood structures which are represented as unordered k-adjacent trees, without relying on labels or other assumptions. Since the computation problem of tree edit distance on unordered trees is NP-Complete, we propose a modified tree edit distance, called TED*, for comparing neighborhood trees. TED* is a metric distance, as the original tree edit distance, but more importantly, TED* is polynomially computable. As a metric distance, NED admits efficient indexing, provides interpretable results, and shows to perform better than existing approaches on a number of data analysis tasks, including graph de-anonymization. Finally, the efficiency and effectiveness of NED are empirically demonstrated using real-world graphs. version:3
arxiv-1509-07979 | Anomaly Detection in Unstructured Environments using Bayesian Nonparametric Scene Modeling | http://arxiv.org/abs/1509.07979 | id:1509.07979 author:Yogesh Girdhar, Walter Cho, Matthew Campbell, Jesus Pineda, Elizabeth Clarke, Hanumant Singh category:cs.CV cs.RO  published:2015-09-26 summary:This paper explores the use of a Bayesian non-parametric topic modeling technique for the purpose of anomaly detection in video data. We present results from two experiments. The first experiment shows that the proposed technique is automatically able characterize the underlying terrain, and detect anomalous flora in image data collected by an underwater robot. The second experiment shows that the same technique can be used on images from a static camera in a dynamic unstructured environment. In the second dataset, consisting of video data from a static seafloor camera capturing images of a busy coral reef, the proposed technique was able to detect all three instances of an underwater vehicle passing in front of the camera, amongst many other observations of fishes, debris, lighting changes due to surface waves, and benthic flora. version:2
arxiv-1602-04886 | Fast, Robust, Continuous Monocular Egomotion Computation | http://arxiv.org/abs/1602.04886 | id:1602.04886 author:Andrew Jaegle, Stephen Phillips, Kostas Daniilidis category:cs.CV cs.RO  published:2016-02-16 summary:We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently proposed optimization framework for joint parameter and confidence weight estimation with good empirical properties. We incorporate these strategies into a motion estimation pipeline that avoids falling into local minima. We find that ERL outperforms the lifted kernel method and baseline monocular egomotion estimation strategies on the challenging KITTI dataset, while adding almost no runtime cost over baseline egomotion methods. version:1
arxiv-1602-04874 | Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation | http://arxiv.org/abs/1602.04874 | id:1602.04874 author:Yushi Yao, Zheng Huang category:cs.LG cs.CL  published:2016-02-16 summary:Recurrent neural network(RNN) has been broadly applied to natural language processing(NLP) problems. This kind of neural network is designed for modeling sequential data and has been testified to be quite efficient in sequential tagging tasks. In this paper, we propose to use bi-directional RNN with long short-term memory(LSTM) units for Chinese word segmentation, which is a crucial preprocess task for modeling Chinese sentences and articles. Classical methods focus on designing and combining hand-craft features from context, whereas bi-directional LSTM network(BLSTM) does not need any prior knowledge or pre-designing, and it is expert in keeping the contextual information in both directions. Experiment result shows that our approach gets state-of-the-art performance in word segmentation on both traditional Chinese datasets and simplified Chinese datasets. version:1
arxiv-1602-04868 | Deep Feature-based Face Detection on Mobile Devices | http://arxiv.org/abs/1602.04868 | id:1602.04868 author:Sayantan Sarkar, Vishal M. Patel, Rama Chellappa category:cs.CV  published:2016-02-16 summary:We propose a deep feature-based face detector for mobile devices to detect user's face acquired by the front facing camera. The proposed method is able to detect faces in images containing extreme pose and illumination variations as well as partial faces. The main challenge in developing deep feature-based algorithms for mobile devices is the constrained nature of the mobile platform and the non-availability of CUDA enabled GPUs on such devices. Our implementation takes into account the special nature of the images captured by the front-facing camera of mobile devices and exploits the GPUs present in mobile devices without CUDA-based frameorks, to meet these challenges. version:1
arxiv-1506-04322 | Graphlet Decomposition: Framework, Algorithms, and Applications | http://arxiv.org/abs/1506.04322 | id:1506.04322 author:Nesreen K. Ahmed, Jennifer Neville, Ryan A. Rossi, Nick Duffield, Theodore L. Willke category:cs.SI cs.DC cs.IR stat.ML  published:2015-06-13 summary:From social science to biology, numerous applications often rely on graphlets for intuitive and meaningful characterization of networks at both the global macro-level as well as the local micro-level. While graphlets have witnessed a tremendous success and impact in a variety of domains, there has yet to be a fast and efficient approach for computing the frequencies of these subgraph patterns. However, existing methods are not scalable to large networks with millions of nodes and edges, which impedes the application of graphlets to new problems that require large-scale network analysis. To address these problems, we propose a fast, efficient, and parallel algorithm for counting graphlets of size k={3,4}-nodes that take only a fraction of the time to compute when compared with the current methods used. The proposed graphlet counting algorithms leverages a number of proven combinatorial arguments for different graphlets. For each edge, we count a few graphlets, and with these counts along with the combinatorial arguments, we obtain the exact counts of others in constant time. On a large collection of 300+ networks from a variety of domains, our graphlet counting strategies are on average 460x faster than current methods. This brings new opportunities to investigate the use of graphlets on much larger networks and newer applications as we show in the experiments. To the best of our knowledge, this paper provides the largest graphlet computations to date as well as the largest systematic investigation on over 300+ networks from a variety of domains. version:2
arxiv-1501-01266 | The Quadrifocal Variety | http://arxiv.org/abs/1501.01266 | id:1501.01266 author:Luke Oeding category:math.AG cs.CV  published:2015-01-06 summary:Multi-view Geometry is reviewed from an Algebraic Geometry perspective and multi-focal tensors are constructed as equivariant projections of the Grassmannian. A connection to the principal minor assignment problem is made by considering several flatlander cameras. The ideal of the quadrifocal variety is computed up to degree 8 (and partially in degree 9) using the representations of $\operatorname{GL}(3)^{\times 4}$ in the polynomial ring on the space of $3 \times 3 \times 3 \times 3$ tensors. Further representation-theoretic analysis gives a lower bound for the number of minimal generators. We conjecture that the ideal of the quadrifocal variety is minimally generated in degree at most 9. version:2
arxiv-1602-04847 | Black-box optimization with a politician | http://arxiv.org/abs/1602.04847 | id:1602.04847 author:Sébastien Bubeck, Yin-Tat Lee category:math.OC cs.DS cs.LG cs.NA  published:2016-02-15 summary:We propose a new framework for black-box convex optimization which is well-suited for situations where gradient computations are expensive. We derive a new method for this framework which leverages several concepts from convex optimization, from standard first-order methods (e.g. gradient descent or quasi-Newton methods) to analytical centers (i.e. minimizers of self-concordant barriers). We demonstrate empirically that our new technique compares favorably with state of the art algorithms (such as BFGS). version:1
arxiv-1602-04805 | DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression | http://arxiv.org/abs/1602.04805 | id:1602.04805 author:Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh category:stat.ML cs.LG stat.CO stat.ME  published:2016-02-15 summary:Performing exact posterior inference in complex generative models is often difficult or impossible due to an expensive to evaluate or intractable likelihood function. Approximate Bayesian computation (ABC) is an inference framework that constructs an approximation to the true likelihood based on the similarity between the observed and simulated data as measured by a predefined set of summary statistics. Although the choice of appropriate problem-specific summary statistics crucially influences the quality of the likelihood approximation and hence also the quality of the posterior sample in ABC, there are only few principled general-purpose approaches to the selection or construction of such summary statistics. In this paper, we develop a novel framework for this task using kernel-based distribution regression. We model the functional relationship between data distributions and the optimal choice (with respect to a loss function) of summary statistics using kernel-based distribution regression. We show that our approach can be implemented in a computationally and statistically efficient way using the random Fourier features framework for large-scale kernel learning. In addition to that, our framework shows superior performance when compared to related methods on toy and real-world problems. version:1
arxiv-1602-04799 | Quantum Perceptron Models | http://arxiv.org/abs/1602.04799 | id:1602.04799 author:Nathan Wiebe, Ashish Kapoor, Krysta M Svore category:quant-ph cs.LG stat.ML  published:2016-02-15 summary:We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points $N$, namely $O(\sqrt{N})$. The second algorithm illustrates how the classical mistake bound of $O(\frac{1}{\gamma^2})$ can be further improved to $O(\frac{1}{\sqrt{\gamma}})$ through quantum means, where $\gamma$ denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model. version:1
arxiv-1204-4107 | Towards the Evolution of Vertical-Axis Wind Turbines using Supershapes | http://arxiv.org/abs/1204.4107 | id:1204.4107 author:Richard J. Preen, Larry Bull category:cs.NE cs.CG  published:2012-04-18 summary:We have recently presented an initial study of evolutionary algorithms used to design vertical-axis wind turbines (VAWTs) wherein candidate prototypes are evaluated under approximated wind tunnel conditions after being physically instantiated by a 3D printer. That is, unlike other approaches such as computational fluid dynamics simulations, no mathematical formulations are used and no model assumptions are made. However, the representation used significantly restricted the range of morphologies explored. In this paper, we present initial explorations into the use of a simple generative encoding, known as Gielis superformula, that produces a highly flexible 3D shape representation to design VAWT. First, the target-based evolution of 3D artefacts is investigated and subsequently initial design experiments are performed wherein each VAWT candidate is physically instantiated and evaluated under approximated wind tunnel conditions. It is shown possible to produce very closely matching designs of a number of 3D objects through the evolution of supershapes produced by Gielis superformula. Moreover, it is shown possible to use artificial physical evolution to identify novel and increasingly efficient supershape VAWT designs. version:4
arxiv-1602-03220 | Discriminative Regularization for Generative Models | http://arxiv.org/abs/1602.03220 | id:1602.03220 author:Alex Lamb, Vincent Dumoulin, Aaron Courville category:stat.ML cs.LG  published:2016-02-09 summary:We explore the question of whether the representations learned by classifiers can be used to enhance the quality of generative models. Our conjecture is that labels correspond to characteristics of natural data which are most salient to humans: identity in faces, objects in images, and utterances in speech. We propose to take advantage of this by using the representations from discriminative classifiers to augment the objective function corresponding to a generative model. In particular we enhance the objective function of the variational autoencoder, a popular generative model, with a discriminative regularization term. We show that enhancing the objective function in this way leads to samples that are clearer and have higher visual quality than the samples from the standard variational autoencoders. version:4
arxiv-1602-04742 | Training of spiking neural networks based on information theoretic costs | http://arxiv.org/abs/1602.04742 | id:1602.04742 author:Oleg Y. Sinyavskiy category:cs.NE q-bio.NC  published:2016-02-15 summary:Spiking neural network is a type of artificial neural network in which neurons communicate between each other with spikes. Spikes are identical Boolean events characterized by the time of their arrival. A spiking neuron has internal dynamics and responds to the history of inputs as opposed to the current inputs only. Because of such properties a spiking neural network has rich intrinsic capabilities to process spatiotemporal data. However, because the spikes are discontinuous 'yes or no' events, it is not trivial to apply traditional training procedures such as gradient descend to the spiking neurons. In this thesis we propose to use stochastic spiking neuron models in which probability of a spiking output is a continuous function of parameters. We formulate several learning tasks as minimization of certain information-theoretic cost functions that use spiking output probability distributions. We develop a generalized description of the stochastic spiking neuron and a new spiking neuron model that allows to flexibly process rich spatiotemporal data. We formulate and derive learning rules for the following tasks: - a supervised learning task of detecting a spatiotemporal pattern as a minimization of the negative log-likelihood (the surprisal) of the neuron's output - an unsupervised learning task of increasing the stability of neurons output as a minimization of the entropy - a reinforcement learning task of controlling an agent as a modulated optimization of filtered surprisal of the neuron's output. We test the derived learning rules in several experiments such as spatiotemporal pattern detection, spatiotemporal data storing and recall with autoassociative memory, combination of supervised and unsupervised learning to speed up the learning process, adaptive control of simple virtual agents in changing environments. version:1
arxiv-1602-07614 | A Model of Selective Advantage for the Efficient Inference of Cancer Clonal Evolution | http://arxiv.org/abs/1602.07614 | id:1602.07614 author:Daniele Ramazzotti category:cs.LG  published:2016-02-15 summary:Recently, there has been a resurgence of interest in rigorous algorithms for the inference of cancer progression from genomic data. The motivations are manifold: (i) growing NGS and single cell data from cancer patients, (ii) need for novel Data Science and Machine Learning algorithms to infer models of cancer progression, and (iii) a desire to understand the temporal and heterogeneous structure of tumor to tame its progression by efficacious therapeutic intervention. This thesis presents a multi-disciplinary effort to model tumor progression involving successive accumulation of genetic alterations, each resulting populations manifesting themselves in a cancer phenotype. The framework presented in this work along with algorithms derived from it, represents a novel approach for inferring cancer progression, whose accuracy and convergence rates surpass the existing techniques. The approach derives its power from several fields including algorithms in machine learning, theory of causality and cancer biology. Furthermore, a modular pipeline to extract ensemble-level progression models from sequenced cancer genomes is proposed. The pipeline combines state-of-the-art techniques for sample stratification, driver selection, identification of fitness-equivalent exclusive alterations and progression model inference. Furthermore, the results are validated by synthetic data with realistic generative models, and empirically interpreted in the context of real cancer datasets; in the later case, biologically significant conclusions are also highlighted. Specifically, it demonstrates the pipeline's ability to reproduce much of the knowledge on colorectal cancer, as well as to suggest novel hypotheses. Lastly, it also proves that the proposed framework can be applied to reconstruct the evolutionary history of cancer clones in single patients, as illustrated by an example from clear cell renal carcinomas. version:1
arxiv-1602-04723 | Efficient Representation of Low-Dimensional Manifolds using Deep Networks | http://arxiv.org/abs/1602.04723 | id:1602.04723 author:Ronen Basri, David Jacobs category:cs.NE cs.LG stat.ML  published:2016-02-15 summary:We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space. We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data. We first show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. We then extend these results to more general manifolds. version:1
arxiv-1511-04581 | A Test of Relative Similarity For Model Selection in Generative Models | http://arxiv.org/abs/1511.04581 | id:1511.04581 author:Wacha Bounliphone, Eugene Belilovsky, Matthew B. Blaschko, Ioannis Antonoglou, Arthur Gretton category:stat.ML cs.LG  published:2015-11-14 summary:Probabilistic generative models provide a powerful framework for representing data that avoids the expense of manual annotation typically needed by discriminative approaches. Model selection in this generative setting can be challenging, however, particularly when likelihoods are not easily accessible. To address this issue, we introduce a statistical test of relative similarity, which is used to determine which of two models generates samples that are significantly closer to a real-world reference dataset of interest. We use as our test statistic the difference in maximum mean discrepancies (MMDs) between the reference dataset and each model dataset, and derive a powerful, low-variance test based on the joint asymptotic distribution of the MMDs between each reference-model pair. In experiments on deep generative models, including the variational auto-encoder and generative moment matching network, the tests provide a meaningful ranking of model performance as a function of parameter and training settings. version:4
arxiv-1506-08781 | On Design Mining: Coevolution and Surrogate Models | http://arxiv.org/abs/1506.08781 | id:1506.08781 author:Richard J. Preen, Larry Bull category:cs.NE cs.AI cs.CE  published:2015-06-29 summary:Design mining is the use of computational intelligence techniques to iteratively search and model the attribute space of physical objects evaluated directly through rapid prototyping to meet given objectives. It enables the exploitation of novel materials and processes without formal models or complex simulation. In this paper, we focus upon the coevolutionary nature of the design process when it is decomposed into concurrent sub-design threads due to the overall complexity of the task. Using an abstract, tuneable model of coevolution we consider strategies to sample sub-thread designs for whole system testing, how best to construct and use surrogate models within the coevolutionary scenario, and the effects of access to multiple whole system (physical) testing equipment on performance. Drawing on our findings, the paper then describes the effective design of an array of six heterogeneous vertical-axis wind turbines. version:4
arxiv-1602-04676 | Maximin Action Identification: A New Bandit Framework for Games | http://arxiv.org/abs/1602.04676 | id:1602.04676 author:Aurélien Garivier, Emilie Kaufmann, Wouter Koolen category:math.ST cs.GT stat.ML stat.TH  published:2016-02-15 summary:We study an original problem of pure exploration in a strategic bandit model motivated by Monte Carlo Tree Search. It consists in identifying the best action in a game, when the player may sample random outcomes of sequentially chosen pairs of actions. We propose two strategies for the fixed-confidence setting: Maximin-LUCB, based on lower-and upper-confidence bounds; and Maximin-Racing, which operates by successively eliminating the sub-optimal actions. We discuss the sample complexity of both methods and compare their performance empirically. We sketch a lower bound analysis, and possible connections to an optimal algorithm. version:1
arxiv-1602-04621 | Deep Exploration via Bootstrapped DQN | http://arxiv.org/abs/1602.04621 | id:1602.04621 author:Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy category:cs.LG cs.AI cs.SY stat.ML  published:2016-02-15 summary:Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games. version:1
arxiv-1402-0635 | Generalization and Exploration via Randomized Value Functions | http://arxiv.org/abs/1402.0635 | id:1402.0635 author:Ian Osband, Benjamin Van Roy, Zheng Wen category:stat.ML cs.AI cs.LG cs.SY  published:2014-02-04 summary:We propose randomized least-squares value iteration (RLSVI) -- a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization. version:3
arxiv-1602-04593 | Edge Detection for Pattern Recognition: A Survey | http://arxiv.org/abs/1602.04593 | id:1602.04593 author:Alex Pappachen James category:cs.CV  published:2016-02-15 summary:This review provides an overview of the literature on the edge detection methods for pattern recognition that inspire from the understanding of human vision. We note that edge detection is one of the most fundamental process within the low level vision and provides the basis for the higher level visual intelligence in primates. The recognition of the patterns within the images relate closely to the spatiotemporal processes of edge formations, and its implementation needs a crossdisciplanry approach in neuroscience, computing and pattern recognition. In this review, the edge detectors are grouped in as edge features, gradients and sketch models, and some example applications are provided for reference. We note a significant increase in the amount of published research in the last decade that utilizes edge features in a wide range of problems in computer vision and image understanding having a direct implication to pattern recognition with images. version:1
arxiv-1506-00473 | An Efficient Algorithm for Video Super-Resolution Based On a Sequential Model | http://arxiv.org/abs/1506.00473 | id:1506.00473 author:Patrick Héas, Angélique Drémeau, Cédric Herzet category:cs.CV  published:2015-06-01 summary:In this work, we propose a novel procedure for video super-resolution, that is the recovery of a sequence of high-resolution images from its low-resolution counterpart. Our approach is based on a "sequential" model (i.e., each high-resolution frame is supposed to be a displaced version of the preceding one) and considers the use of sparsity-enforcing priors. Both the recovery of the high-resolution images and the motion fields relating them is tackled. This leads to a large-dimensional, non-convex and non-smooth problem. We propose an algorithmic framework to address the latter. Our approach relies on fast gradient evaluation methods and modern optimization techniques for non-differentiable/non-convex problems. Unlike some other previous works, we show that there exists a provably-convergent method with a complexity linear in the problem dimensions. We assess the proposed optimization method on {several video benchmarks and emphasize its good performance with respect to the state of the art.} version:3
arxiv-1602-04579 | Secure Approximation Guarantee for Cryptographically Private Empirical Risk Minimization | http://arxiv.org/abs/1602.04579 | id:1602.04579 author:Toshiyuki Takada, Hiroyuki Hanada, Yoshiji Yamada, Jun Sakuma, Ichiro Takeuchi category:stat.ML cs.CR cs.LG  published:2016-02-15 summary:Privacy concern has been increasingly important in many machine learning (ML) problems. We study empirical risk minimization (ERM) problems under secure multi-party computation (MPC) frameworks. Main technical tools for MPC have been developed based on cryptography. One of limitations in current cryptographically private ML is that it is computationally intractable to evaluate non-linear functions such as logarithmic functions or exponential functions. Therefore, for a class of ERM problems such as logistic regression in which non-linear function evaluations are required, one can only obtain approximate solutions. In this paper, we introduce a novel cryptographically private tool called secure approximation guarantee (SAG) method. The key property of SAG method is that, given an arbitrary approximate solution, it can provide a non-probabilistic assumption-free bound on the approximation quality under cryptographically secure computation framework. We demonstrate the benefit of the SAG method by applying it to several problems including a practical privacy-preserving data analysis task on genomic and clinical information. version:1
arxiv-1602-04572 | Personalized Expertise Search at LinkedIn | http://arxiv.org/abs/1602.04572 | id:1602.04572 author:Viet Ha-Thuc, Ganesh Venkataraman, Mario Rodriguez, Shakti Sinha, Senthil Sundaram, Lin Guo category:cs.IR cs.LG cs.SI  published:2016-02-15 summary:LinkedIn is the largest professional network with more than 350 million members. As the member base increases, searching for experts becomes more and more challenging. In this paper, we propose an approach to address the problem of personalized expertise search on LinkedIn, particularly for exploratory search queries containing {\it skills}. In the offline phase, we introduce a collaborative filtering approach based on matrix factorization. Our approach estimates expertise scores for both the skills that members list on their profiles as well as the skills they are likely to have but do not explicitly list. In the online phase (at query time) we use expertise scores on these skills as a feature in combination with other features to rank the results. To learn the personalized ranking function, we propose a heuristic to extract training data from search logs while handling position and sample selection biases. We tested our models on two products - LinkedIn homepage and LinkedIn recruiter. A/B tests showed significant improvements in click through rates - 31% for CTR@1 for recruiter (18% for homepage) as well as downstream messages sent from search - 37% for recruiter (20% for homepage). As of writing this paper, these models serve nearly all live traffic for skills search on LinkedIn homepage as well as LinkedIn recruiter. version:1
arxiv-1510-00149 | Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding | http://arxiv.org/abs/1510.00149 | id:1510.00149 author:Song Han, Huizi Mao, William J. Dally category:cs.CV cs.NE  published:2015-10-01 summary:Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency. version:5
arxiv-1305-6526 | Adaptive estimation of the copula correlation matrix for semiparametric elliptical copulas | http://arxiv.org/abs/1305.6526 | id:1305.6526 author:Marten Wegkamp, Yue Zhao category:stat.ML  published:2013-05-28 summary:We study the adaptive estimation of copula correlation matrix $\Sigma$ for the semi-parametric elliptical copula model. In this context, the correlations are connected to Kendall's tau through a sine function transformation. Hence, a natural estimate for $\Sigma$ is the plug-in estimator $\hat{\Sigma}$ with Kendall's tau statistic. We first obtain a sharp bound on the operator norm of $\hat{\Sigma}-\Sigma$. Then we study a factor model of $\Sigma$, for which we propose a refined estimator $\widetilde{\Sigma}$ by fitting a low-rank matrix plus a diagonal matrix to $\hat{\Sigma}$ using least squares with a nuclear norm penalty on the low-rank matrix. The bound on the operator norm of $\hat{\Sigma}-\Sigma$ serves to scale the penalty term, and we obtain finite sample oracle inequalities for $\widetilde{\Sigma}$. We also consider an elementary factor copula model of $\Sigma$, for which we propose closed-form estimators. All of our estimation procedures are entirely data-driven. version:3
arxiv-1602-04567 | Adversarial Top-$K$ Ranking | http://arxiv.org/abs/1602.04567 | id:1602.04567 author:Changho Suh, Vincent Y. F. Tan, Renbo Zhao category:cs.IR cs.IT cs.LG math.IT stat.ML  published:2016-02-15 summary:We study the top-$K$ ranking problem where the goal is to recover the set of top-$K$ ranked items out of a large collection of items based on partially revealed preferences. We consider an adversarial crowdsourced setting where there are two population sets, and pairwise comparison samples drawn from one of the populations follow the standard Bradley-Terry-Luce model (i.e., the chance of item $i$ beating item $j$ is proportional to the relative score of item $i$ to item $j$), while in the other population, the corresponding chance is inversely proportional to the relative score. When the relative size of the two populations is known, we characterize the minimax limit on the sample size required (up to a constant) for reliably identifying the top-$K$ items, and demonstrate how it scales with the relative size. Moreover, by leveraging a tensor decomposition method for disambiguating mixture distributions, we extend our result to the more realistic scenario in which the relative population size is unknown, thus establishing an upper bound on the fundamental limit of the sample size for recovering the top-$K$ set. version:1
arxiv-1601-03778 | Trust from the past: Bayesian Personalized Ranking based Link Prediction in Knowledge Graphs | http://arxiv.org/abs/1601.03778 | id:1601.03778 author:Baichuan Zhang, Sutanay Choudhury, Mohammad Al Hasan, Xia Ning, Khushbu Agarwal, Sumit Purohit, Paola Pesntez Cabrera category:cs.LG cs.AI cs.IR  published:2016-01-14 summary:Link prediction, or predicting the likelihood of a link in a knowledge graph based on its existing state is a key research task. It differs from a traditional link prediction task in that the links in a knowledge graph are categorized into different predicates and the link prediction performance of different predicates in a knowledge graph generally varies widely. In this work, we propose a latent feature embedding based link prediction model which considers the prediction task for each predicate disjointly. To learn the model parameters it utilizes a Bayesian personalized ranking based optimization technique. Experimental results on large-scale knowledge bases such as YAGO2 show that our link prediction approach achieves substantially higher performance than several state-of-art approaches. We also show that for a given predicate the topological properties of the knowledge graph induced by the given predicate edges are key indicators of the link prediction performance of that predicate in the knowledge graph. version:2
arxiv-1602-04548 | Safe Pattern Pruning: An Efficient Approach for Predictive Pattern Mining | http://arxiv.org/abs/1602.04548 | id:1602.04548 author:Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda, Ichiro Takeuchi category:stat.ML  published:2016-02-15 summary:In this paper we study predictive pattern mining problems where the goal is to construct a predictive model based on a subset of predictive patterns in the database. Our main contribution is to introduce a novel method called safe pattern pruning (SPP) for a class of predictive pattern mining problems. The SPP method allows us to efficiently find a superset of all the predictive patterns in the database that are needed for the optimal predictive model. The advantage of the SPP method over existing boosting-type method is that the former can find the superset by a single search over the database, while the latter requires multiple searches. The SPP method is inspired by recent development of safe feature screening. In order to extend the idea of safe feature screening into predictive pattern mining, we derive a novel pruning rule called safe pattern pruning (SPP) rule that can be used for searching over the tree defined among patterns in the database. The SPP rule has a property that, if a node corresponding to a pattern in the database is pruned out by the SPP rule, then it is guaranteed that all the patterns corresponding to its descendant nodes are never needed for the optimal predictive model. We apply the SPP method to graph mining and item-set mining problems, and demonstrate its computational advantage. version:1
arxiv-1602-05112 | ICU Patient Flow Prediction via Discriminative Learning of Mutually-Correcting Processes | http://arxiv.org/abs/1602.05112 | id:1602.05112 author:Hongteng Xu, Weichang Wu, Shamim Nemati, Hongyuan Zha category:cs.LG  published:2016-02-14 summary:Over the past decade the rate of intensive care unit (ICU) use in the United States has been increasing, with a recent study reporting almost one in three Medicare beneficiaries experiencing an ICU visit during the last month of their lives. With an aging population and ever-growing demand for critical care, effective management of patient flow and transition among different care facilities will prove indispensible for shortening lengths of hospital stays, improving patient outcomes, allocating critical resources, and reducing preventable re-admissions. In this paper, we focus on a new problem of predicting the so-called ICU patient flow from longitudinal electronic health records (EHRs), which is not explored via existing machine learning techniques. By treating a sequence of transition events as a point process, we develop a novel framework for modeling patient flow through various ICU care units and predict patients' destination ICUs and duration days jointly. Instead of learning a generative point process model via maximum likelihood estimation, we propose a novel discriminative learning algorithm aiming at improving the prediction of transition events. By parameterizing the proposed model as a mutually-correcting process, we formulate the problem as a generalized linear model, i.e., multinomial logistic regression, which yields itself to efficient learning via alternating direction method of multipliers (ADMM). Furthermore, we achieve simultaneous feature selection and learning by adding a group-lasso regularizer to the ADMM algorithm. Using real-world data of ICU patients, we show that our method obtains superior performance in terms of accuracy of predicting the destination ICU transition and duration of each ICU occupancy. version:1
arxiv-1602-04513 | Validity and reliability of free software for bidimensional gait analysis | http://arxiv.org/abs/1602.04513 | id:1602.04513 author:Ana Paula Quixadá, Andrea Naomi Onodera, Norberto Peña, José Garcia Vivas Miranda, Katia Nunes Sá category:q-bio.QM cs.CV physics.med-ph  published:2016-02-14 summary:Despite the evaluation systems of human movement that have been advancing in recent decades, their use are not feasible for clinical practice because it has a high cost and scarcity of trained operators to interpret their results. An ideal videogrammetry system should be easy to use, low cost, with minimal equipment, and fast realization. The CvMob is a free tool for dynamic evaluation of human movements that express measurements in figures, tables, and graphics. This paper aims to determine if CvMob is a reliable tool for the evaluation of two dimensional human gait. This is a validity and reliability study. The sample was composed of 56 healthy individuals who walked on a 9-meterlong walkway and were simultaneously filmed by CvMob and Vicon system cameras. Linear trajectories and angular measurements were compared to validate the CvMob system, and inter and intrarater findings of the same measurements were used to determine reliability. A strong correlation (rs mean = 0.988) of the linear trajectories between systems and inter and intrarater analysis were found. According to the Bland-Altman method, the angles that had good agreement between systems were maximum flexion and extension (stance and swing) of the knee and dorsiflexion range of motion and stride length. The CvMob is a reliable tool for analysis of linear motion and lengths in two-dimensional evaluations of human gait. The angular measurements demonstrate high agreement for the knee joint; however, the hip and ankle measurements were limited by differences between systems. version:1
arxiv-1506-02351 | Stacked What-Where Auto-encoders | http://arxiv.org/abs/1506.02351 | id:1506.02351 author:Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun category:stat.ML cs.LG cs.NE  published:2015-06-08 summary:We present a novel architecture, the "stacked what-where auto-encoders" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the "what" which are fed to the next layer, and its complementary variable "where" that are fed to the corresponding layer in the generative decoder. version:8
arxiv-1602-04506 | Embracing Error to Enable Rapid Crowdsourcing | http://arxiv.org/abs/1602.04506 | id:1602.04506 author:Ranjay Krishna, Kenji Hata, Stephanie Chen, Joshua Kravitz, David A. Shamma, Li Fei-Fei, Michael S. Bernstein category:cs.HC cs.CV H.5.m  published:2016-02-14 summary:Microtask crowdsourcing has enabled dataset advances in social science and machine learning, but existing crowdsourcing schemes are too expensive to scale up with the expanding volume of data. To scale and widen the applicability of crowdsourcing, we present a technique that produces extremely rapid judgments for binary and categorical labels. Rather than punishing all errors, which causes workers to proceed slowly and deliberately, our technique speeds up workers' judgments to the point where errors are acceptable and even expected. We demonstrate that it is possible to rectify these errors by randomizing task order and modeling response latency. We evaluate our technique on a breadth of common labeling tasks such as image verification, word similarity, sentiment analysis and topic classification. Where prior work typically achieves a 0.25x to 1x speedup over fixed majority vote, our approach often achieves an order of magnitude (10x) speedup. version:1
arxiv-1602-04504 | Can we still avoid automatic face detection? | http://arxiv.org/abs/1602.04504 | id:1602.04504 author:Michael J. Wilber, Vitaly Shmatikov, Serge Belongie category:cs.CV  published:2016-02-14 summary:After decades of study, automatic face detection and recognition systems are now accurate and widespread. Naturally, this means users who wish to avoid automatic recognition are becoming less able to do so. Where do we stand in this cat-and-mouse race? We currently live in a society where everyone carries a camera in their pocket. Many people willfully upload most or all of the pictures they take to social networks which invest heavily in automatic face recognition systems. In this setting, is it still possible for privacy-conscientious users to avoid automatic face detection and recognition? If so, how? Must evasion techniques be obvious to be effective, or are there still simple measures that users can use to protect themselves? In this work, we find ways to evade face detection on Facebook, a representative example of a popular social network that uses automatic face detection to enhance their service. We challenge widely-held beliefs about evading face detection: do our old techniques such as blurring the face region or wearing "privacy glasses" still work? We show that in general, state-of-the-art detectors can often find faces even if the subject wears occluding clothing or even if the uploader damages the photo to prevent faces from being detected. version:1
arxiv-1602-04502 | Do We Need Binary Features for 3D Reconstruction? | http://arxiv.org/abs/1602.04502 | id:1602.04502 author:Bin Fan, Qingqun Kong, Wei Sui, Zhiheng Wang, Xinchao Wang, Shiming Xiang, Chunhong Pan, Pascal Fua category:cs.CV  published:2016-02-14 summary:Binary features have been incrementally popular in the past few years due to their low memory footprints and the efficient computation of Hamming distance between binary descriptors. They have been shown with promising results on some real time applications, e.g., SLAM, where the matching operations are relative few. However, in computer vision, there are many applications such as 3D reconstruction requiring lots of matching operations between local features. Therefore, a natural question is that is the binary feature still a promising solution to this kind of applications? To get the answer, this paper conducts a comparative study of binary features and their matching methods on the context of 3D reconstruction in a recently proposed large scale mutliview stereo dataset. Our evaluations reveal that not all binary features are capable of this task. Most of them are inferior to the classical SIFT based method in terms of reconstruction accuracy and completeness with a not significant better computational performance. version:1
arxiv-1602-04489 | Convolutional Tables Ensemble: classification in microseconds | http://arxiv.org/abs/1602.04489 | id:1602.04489 author:Aharon Bar-Hillel, Eyal Krupka, Noam Bloom category:cs.CV cs.LG 68T45  published:2016-02-14 summary:We study classifiers operating under severe classification time constraints, corresponding to 1-1000 CPU microseconds, using Convolutional Tables Ensemble (CTE), an inherently fast architecture for object category recognition. The architecture is based on convolutionally-applied sparse feature extraction, using trees or ferns, and a linear voting layer. Several structure and optimization variants are considered, including novel decision functions, tree learning algorithm, and distillation from CNN to CTE architecture. Accuracy improvements of 24-45% over related art of similar speed are demonstrated on standard object recognition benchmarks. Using Pareto speed-accuracy curves, we show that CTE can provide better accuracy than Convolutional Neural Networks (CNN) for a certain range of classification time constraints, or alternatively provide similar error rates with 5-200X speedup. version:1
arxiv-1511-04747 | Learning Representations of Affect from Speech | http://arxiv.org/abs/1511.04747 | id:1511.04747 author:Sayan Ghosh, Eugene Laksana, Louis-Philippe Morency, Stefan Scherer category:cs.CL cs.LG  published:2015-11-15 summary:There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes i.e. categorical and dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative of activation intensity and at separating out negative valence (sadness and anger) from positive valence (happiness). We experiment with different input speech features (such as FFT and log-mel spectrograms with temporal context windows), and different autoencoder architectures (such as stacked and deep autoencoders). We also learn utterance specific representations by a combination of denoising autoencoders and BLSTM based recurrent autoencoders. Emotion classification is performed with the learnt temporal/dynamic representations to evaluate the quality of the representations. Experiments on a well-established real-life speech dataset (IEMOCAP) show that the learnt representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) and are competitive with state-of-the-art approaches at emotion and dimensional affect recognition. version:6
arxiv-1511-04119 | Action Recognition using Visual Attention | http://arxiv.org/abs/1511.04119 | id:1511.04119 author:Shikhar Sharma, Ryan Kiros, Ruslan Salakhutdinov category:cs.LG cs.CV  published:2015-11-12 summary:We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed. version:3
arxiv-1602-04450 | Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning in Robotics | http://arxiv.org/abs/1602.04450 | id:1602.04450 author:Felix Berkenkamp, Andreas Krause, Angela P. Schoellig category:cs.RO cs.LG cs.SY  published:2016-02-14 summary:Robotics algorithms typically depend on various parameters, the choice of which significantly affects the robot's performance. While an initial guess for the parameters may be obtained from dynamic models of the robot, parameters are usually tuned manually on the real system to achieve the best performance. Optimization algorithms, such as Bayesian optimization, have been used to automate this process. However, these methods may evaluate parameters during the optimization process that lead to safety-critical system failures. Recently, a safe Bayesian optimization algorithm, called SafeOpt, has been developed and applied in robotics, which guarantees that the performance of the system never falls below a critical value; that is, safety is defined based on the performance function. However, coupling performance and safety is not desirable in most cases. In this paper, we define separate functions for performance and safety. We present a generalized SafeOpt algorithm that, given an initial safe guess for the parameters, maximizes performance but only evaluates parameters that satisfy all safety constraints with high probability. It achieves this by modeling the underlying and unknown performance and constraint functions as Gaussian processes. We provide a theoretical analysis and demonstrate in experiments on a quadrotor vehicle that the proposed algorithm enables fast, automatic, and safe optimization of tuning parameters. Moreover, we show an extension to context- or environment-dependent, safe optimization in the experiments. version:1
arxiv-1506-02620 | Distributed Training of Structured SVM | http://arxiv.org/abs/1506.02620 | id:1506.02620 author:Ching-pei Lee, Kai-Wei Chang, Shyam Upadhyay, Dan Roth category:stat.ML cs.DC cs.LG  published:2015-06-08 summary:Training structured prediction models is time-consuming. However, most existing approaches only use a single machine, thus, the advantage of computing power and the capacity for larger data sets of multiple machines have not been exploited. In this work, we propose an efficient algorithm for distributedly training structured support vector machines based on a distributed block-coordinate descent method. Both theoretical and experimental results indicate that our method is efficient. version:2
arxiv-1602-04436 | Distributed Time-Varying Graph Filtering | http://arxiv.org/abs/1602.04436 | id:1602.04436 author:Elvin Isufi, Andreas Loukas, Andrea Simonetto, Geert Leus category:cs.LG cs.SY stat.ML  published:2016-02-14 summary:One of the cornerstones of the field of signal processing on graphs are graph filters, direct analogues of classical filters, but intended for signals defined on graphs. This work brings forth new insights on the distributed graph filtering problem. We design a family of autoregressive moving average (ARMA) recursions, which (i) are able to approximate any desired graph frequency response, and (ii) give exact solutions for tasks such as graph signal denoising and interpolation. The design philosophy, which allows us to design the ARMA coefficients independently from the underlying graph, renders the ARMA graph filters suitable in static and, particularly, time-varying settings. The latter occur when the graph signal and/or graph are changing over time. We show that in case of a time-varying graph signal our approach extends naturally to a two-dimensional filter, operating concurrently in the graph and regular time domains. We also derive sufficient conditions for filter stability when the graph and signal are time-varying. The analytical and numerical results presented in this paper illustrate that ARMA graph filters are practically appealing for static and time-varying settings, accompanied by strong theoretical guarantees. version:1
arxiv-1602-04435 | Random Forest Based Approach for Concept Drift Handling | http://arxiv.org/abs/1602.04435 | id:1602.04435 author:A. Zhukov, D. Sidorov, A. Foley category:cs.AI cs.LG math.ST stat.TH  published:2016-02-14 summary:Concept drift has potential in smart grid analysis because the socio-economic behaviour of consumers is not governed by the laws of physics. Likewise there are also applications in wind power forecasting. In this paper we present decision tree ensemble classification method based on the Random Forest algorithm for concept drift. The weighted majority voting ensemble aggregation rule is employed based on the ideas of Accuracy Weighted Ensemble (AWE) method. Base learner weight in our case is computed for each sample evaluation using base learners accuracy and intrinsic proximity measure of Random Forest. Our algorithm exploits both temporal weighting of samples and ensemble pruning as a forgetting strategy. We present results of empirical comparison of our method with original random forest with incorporated "replace-the-looser" forgetting andother state-of-the-art concept-drfit classifiers like AWE2. version:1
arxiv-1602-04434 | Frequency Analysis of Temporal Graph Signals | http://arxiv.org/abs/1602.04434 | id:1602.04434 author:Andreas Loukas, Damien Foucard category:cs.LG cs.SY stat.ML  published:2016-02-14 summary:This letter extends the concept of graph-frequency to graph signals that evolve with time. Our goal is to generalize and, in fact, unify the familiar concepts from time- and graph-frequency analysis. To this end, we study a joint temporal and graph Fourier transform (JFT) and demonstrate its attractive properties. We build on our results to create filters which act on the joint (temporal and graph) frequency domain, and show how these can be used to perform interference cancellation. The proposed algorithms are distributed, have linear complexity, and can approximate any desired joint filtering objective. version:1
arxiv-1602-04433 | Unsupervised Domain Adaptation with Residual Transfer Networks | http://arxiv.org/abs/1602.04433 | id:1602.04433 author:Mingsheng Long, Jianmin Wang, Michael I. Jordan category:cs.LG  published:2016-02-14 summary:The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can simultaneously learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into the deep network to explicitly learn the residual function with reference to the target classifier. We embed features of multiple layers into reproducing kernel Hilbert spaces (RKHSs) and match feature distributions for feature adaptation. The adaptation behaviors can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently using standard back-propagation. Empirical evidence exhibits that the approach outperforms state of art methods on standard domain adaptation datasets. version:1
arxiv-1602-04422 | Hi Detector, What's Wrong with that Object? Identifying Irregular Object From Images by Modelling the Detection Score Distribution | http://arxiv.org/abs/1602.04422 | id:1602.04422 author:Peng Wang, Lingqiao Liu, Chunhua Shen, Anton van den Hengel, Heng Tao Shen category:cs.CV  published:2016-02-14 summary:In this work, we study the challenging problem of identifying the irregular status of objects from images in an "open world" setting, that is, distinguishing the irregular status of an object category from its regular status as well as objects from other categories in the absence of "irregular object" training data. To address this problem, we propose a novel approach by inspecting the distribution of the detection scores at multiple image regions based on the detector trained from the "regular object" and "other objects". The key observation motivating our approach is that for "regular object" images as well as "other objects" images, the region-level scores follow their own essential patterns in terms of both the score values and the spatial distributions while the detection scores obtained from an "irregular object" image tend to break these patterns. To model this distribution, we propose to use Gaussian Processes (GP) to construct two separate generative models for the case of the "regular object" and the "other objects". More specifically, we design a new covariance function to simultaneously model the detection score at a single region and the score dependencies at multiple regions. We finally demonstrate the superior performance of our method on a large dataset newly proposed in this paper. version:1
arxiv-1602-04418 | Identifiability assumptions for directed graphical models with feedback | http://arxiv.org/abs/1602.04418 | id:1602.04418 author:Gunwoong Park, Garvesh Raskutti category:stat.ML cs.LG  published:2016-02-14 summary:Directed graphical models provide a useful framework for modeling causal or directional relationships for multivariate data. Prior work has largely focused on identifiability and search algorithms for directed acyclic graphical (DAG) models. In many applications, feedback naturally arises and directed graphical models that permit cycles arise. However theory and methodology for directed graphical models with feedback are considerably less developed since graphs with cycles pose a number of additional challenges. In this paper we address the issue of identifiability for general directed cyclic graphical (DCG) models satisfying only the Markov assumption. In particular, in addition to the faithfulness assumption which has already been introduced for cyclic models, we introduce two new identifiability assumptions, one based on selecting the model with the fewest edges and the other based on selecting the DCG model that entails the maximum d-separation rules. We provide theoretical results comparing these assumptions which shows that: (1) selecting models with the largest number of d-separation rules is strictly weaker than the faithfulness assumption; (2) unlike for DAG models, selecting models with the fewest edges do not necessarily result in a milder assumption than the faithfulness assumption. We also provide connections between our two new principles and minimality assumptions which lead to a ranking of how strong and weak various identifiability and minimality assumptions are for both DAG and DCG models. We use our identifiability assumptions to develop search algorithms for small-scale DCG models. Our simulations results using our search algorithms support our theoretical results, showing that our two new principles generally out-perform the faithfulness assumption in terms of selecting the true skeleton for DCG models. version:1
arxiv-1511-06067 | Convolutional neural networks with low-rank regularization | http://arxiv.org/abs/1511.06067 | id:1511.06067 author:Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, Weinan E category:cs.LG cs.CV stat.ML  published:2015-11-19 summary:Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problematic for deploying these models on mobile devices. Recently, tensor decompositions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolution kernels. The algorithm finds the exact global optimizer of the decomposition and is more effective than iterative methods. Based on the decomposition, we further propose a new method for training low-rank constrained CNNs from scratch. Interestingly, while achieving a significant speedup, sometimes the low-rank constrained CNNs delivers significantly better performance than their non-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank NIN model achieves $91.31\%$ accuracy (without data augmentation), which also improves upon state-of-the-art result. We evaluated the proposed method on CIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet, NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is reduced by half while the performance is still comparable. Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speeding up large CNNs. version:3
arxiv-1602-04409 | Convex Optimization For Non-Convex Problems via Column Generation | http://arxiv.org/abs/1602.04409 | id:1602.04409 author:Julian Yarkony, Kamalika Chaudhuri category:cs.LG  published:2016-02-14 summary:We apply column generation to approximating complex structured objects via a set of primitive structured objects under either the cross entropy or L2 loss. We use L1 regularization to encourage the use of few structured primitive objects. We attack approximation using convex optimization over an infinite number of variables each corresponding to a primitive structured object that are generated on demand by easy inference in the Lagrangian dual. We apply our approach to producing low rank approximations to large 3-way tensors. version:1
arxiv-1503-06858 | Communication Efficient Distributed Kernel Principal Component Analysis | http://arxiv.org/abs/1503.06858 | id:1503.06858 author:Maria-Florina Balcan, Yingyu Liang, Le Song, David Woodruff, Bo Xie category:cs.LG  published:2015-03-23 summary:Kernel Principal Component Analysis (KPCA) is a key machine learning algorithm for extracting nonlinear features from data. In the presence of a large volume of high dimensional data collected in a distributed fashion, it becomes very costly to communicate all of this data to a single data center and then perform kernel PCA. Can we perform kernel PCA on the entire dataset in a distributed and communication efficient fashion while maintaining provable and strong guarantees in solution quality? In this paper, we give an affirmative answer to the question by developing a communication efficient algorithm to perform kernel PCA in the distributed setting. The algorithm is a clever combination of subspace embedding and adaptive sampling techniques, and we show that the algorithm can take as input an arbitrary configuration of distributed datasets, and compute a set of global kernel principal components with relative error guarantees independent of the dimension of the feature space or the total number of data points. In particular, computing $k$ principal components with relative error $\epsilon$ over $s$ workers has communication cost $\tilde{O}(s \rho k/\epsilon+s k^2/\epsilon^3)$ words, where $\rho$ is the average number of nonzero entries in each data point. Furthermore, we experimented the algorithm with large-scale real world datasets and showed that the algorithm produces a high quality kernel PCA solution while using significantly less communication than alternative approaches. version:4
arxiv-1412-7260 | Approximate Subspace-Sparse Recovery with Corrupted Data via Constrained $\ell_1$-Minimization | http://arxiv.org/abs/1412.7260 | id:1412.7260 author:Ehsan Elhamifar, Mahdi Soltanolkotabi, Shankar Sastry category:stat.ML  published:2014-12-23 summary:High-dimensional data often lie in low-dimensional subspaces corresponding to different classes they belong to. Finding sparse representations of data points in a dictionary built using the collection of data helps to uncover low-dimensional subspaces and address problems such as clustering, classification, subset selection and more. In this paper, we address the problem of recovering sparse representations for noisy data points in a dictionary whose columns correspond to corrupted data lying close to a union of subspaces. We consider a constrained $\ell_1$-minimization and study conditions under which the solution of the proposed optimization satisfies the approximate subspace-sparse recovery condition. More specifically, we show that each noisy data point, perturbed from a subspace by a noise of the magnitude of $\varepsilon$, will be reconstructed using data points from the same subspace with a small error of the order of $O(\varepsilon)$ and that the coefficients corresponding to data points in other subspaces will be sufficiently small, \ie, of the order of $O(\varepsilon)$. We do not impose any randomness assumption on the arrangement of subspaces or distribution of data points in each subspace. Our framework is based on a novel generalization of the null-space property to the setting where data lie in multiple subspaces, the number of data points in each subspace exceeds the dimension of the subspace, and all data points are corrupted by noise. Moreover, assuming a random distribution for data points, we further show that coefficients from the desired support not only reconstruct a given point with high accuracy, but also have sufficiently large values, \ie, of the order of $O(1)$. version:2
arxiv-1602-04393 | Semantic Scan: Detecting Subtle, Spatially Localized Events in Text Streams | http://arxiv.org/abs/1602.04393 | id:1602.04393 author:Abhinav Maurya, Kenton Murray, Yandong Liu, Chris Dyer, William W. Cohen, Daniel B. Neill category:cs.IR stat.ML  published:2016-02-13 summary:Early detection and precise characterization of emerging topics in text streams can be highly useful in applications such as timely and targeted public health interventions and discovering evolving regional business trends. Many methods have been proposed for detecting emerging events in text streams using topic modeling. However, these methods have numerous shortcomings that make them unsuitable for rapid detection of locally emerging events on massive text streams. In this paper, we describe Semantic Scan (SS) that has been developed specifically to overcome these shortcomings in detecting new spatially compact events in text streams. Semantic Scan integrates novel contrastive topic modeling with online document assignment and principled likelihood ratio-based spatial scanning to identify emerging events with unexpected patterns of keywords hidden in text streams. This enables more timely and accurate detection and characterization of anomalous, spatially localized emerging events. Semantic Scan does not require manual intervention or labeled training data, and is robust to noise in real-world text data since it identifies anomalous text patterns that occur in a cluster of new documents rather than an anomaly in a single new document. We compare Semantic Scan to alternative state-of-the-art methods such as Topics over Time, Online LDA, and Labeled LDA on two real-world tasks: (i) a disease surveillance task monitoring free-text Emergency Department chief complaints in Allegheny County, and (ii) an emerging business trend detection task based on Yelp reviews. On both tasks, we find that Semantic Scan provides significantly better event detection and characterization accuracy than competing approaches, while providing up to an order of magnitude speedup. version:1
arxiv-1602-04391 | Constrained Multi-Slot Optimization for Ranking Recommendations | http://arxiv.org/abs/1602.04391 | id:1602.04391 author:Kinjal Basu, Shaunak Chatterjee, Ankan Saha category:stat.ML math.OC stat.AP 90C20  11K36 G.1.6  published:2016-02-13 summary:Ranking items to be recommended to users is one of the main problems in large scale social media applications. This problem can be set up as a multi-objective optimization problem to allow for trading off multiple, potentially conflicting objectives (that are driven by those items) against each other. Most previous approaches to this problem optimize for a single slot without considering the interaction effect of these items on one another. In this paper, we develop a constrained multi-slot optimization formulation, which allows for modeling interactions among the items on the different slots. We characterize the solution in terms of problem parameters and identify conditions under which an efficient solution is possible. The problem formulation results in a quadratically constrained quadratic program (QCQP). We provide an algorithm that gives us an efficient solution by relaxing the constraints of the QCQP minimally. Through simulated experiments, we show the benefits of modeling interactions in a multi-slot ranking context, and the speed and accuracy of our QCQP approximate solver against other state of the art methods. version:1
arxiv-1509-07175 | Exploration and Exploitation of Victorian Science in Darwin's Reading Notebooks | http://arxiv.org/abs/1509.07175 | id:1509.07175 author:Jaimie Murdock, Colin Allen, Simon DeDeo category:cs.CL cs.AI cs.CY cs.DL physics.soc-ph  published:2015-09-23 summary:Search in an environment with an uncertain distribution of resources involves a trade-off between exploitation of past discoveries and further exploration. This extends to information foraging, where a knowledge-seeker shifts between reading in depth and studying new domains. We study this process in Charles Darwin by modeling the full-text of books listed in his chronologically-organized reading journals. We use the information-theoretic Kullback-Liebler Divergence, or relative surprise, between books for both his local (book-to-book) and global (book-to-past) reading decisions. Rather than a pattern of surprise-minimization, corresponding to a pure exploitation strategy, Darwin's behavior shifts from early exploitation to later exploration, seeking unusually high levels of cognitive surprise relative to previous eras. These shifts, detected by an unsupervised Bayesian model, correlate with major intellectual epochs of his career as identified both by traditional, qualitative scholarship and Darwin's own self-commentary. In addition to quantifying Darwin's individual-level foraging, our methods allow us to compare his consumption of texts with their publication order. We find Darwin's consumption more exploratory than the culture's production, suggesting that underneath gradual societal changes are the explorations of individual synthesis and discovery. version:3
arxiv-1602-04364 | Look, Listen and Learn - A Multimodal LSTM for Speaker Identification | http://arxiv.org/abs/1602.04364 | id:1602.04364 author:Jimmy Ren, Yongtao Hu, Yu-Wing Tai, Chuan Wang, Li Xu, Wenxiu Sun, Qiong Yan category:cs.LG  published:2016-02-13 summary:Speaker identification refers to the task of localizing the face of a person who has the same identity as the ongoing voice in a video. This task not only requires collective perception over both visual and auditory signals, the robustness to handle severe quality degradations and unconstrained content variations are also indispensable. In this paper, we describe a novel multimodal Long Short-Term Memory (LSTM) architecture which seamlessly unifies both visual and auditory modalities from the beginning of each sequence input. The key idea is to extend the conventional LSTM by not only sharing weights across time steps, but also sharing weights across modalities. We show that modeling the temporal dependency across face and voice can significantly improve the robustness to content quality degradations and variations. We also found that our multimodal LSTM is robustness to distractors, namely the non-speaking identities. We applied our multimodal LSTM to The Big Bang Theory dataset and showed that our system outperforms the state-of-the-art systems in speaker identification with lower false alarm rate and higher recognition accuracy. version:1
arxiv-1508-04028 | "Owl" and "Lizard": Patterns of Head Pose and Eye Pose in Driver Gaze Classification | http://arxiv.org/abs/1508.04028 | id:1508.04028 author:Lex Fridman, Joonbum Lee, Bryan Reimer, Trent Victor category:cs.CV cs.HC cs.LG  published:2015-08-17 summary:Accurate, robust, inexpensive gaze tracking in the car can help keep a driver safe by facilitating the more effective study of how to improve (1) vehicle interfaces and (2) the design of future Advanced Driver Assistance Systems. In this paper, we estimate head pose and eye pose from monocular video using methods developed extensively in prior work and ask two new interesting questions. First, how much better can we classify driver gaze using head and eye pose versus just using head pose? Second, are there individual-specific gaze strategies that strongly correlate with how much gaze classification improves with the addition of eye pose information? We answer these questions by evaluating data drawn from an on-road study of 40 drivers. The main insight of the paper is conveyed through the analogy of an "owl" and "lizard" which describes the degree to which the eyes and the head move when shifting gaze. When the head moves a lot ("owl"), not much classification improvement is attained by estimating eye pose on top of head pose. On the other hand, when the head stays still and only the eyes move ("lizard"), classification accuracy increases significantly from adding in eye pose. We characterize how that accuracy varies between people, gaze strategies, and gaze regions. version:2
arxiv-1602-04358 | Machine olfaction using time scattering of sensor multiresolution graphs | http://arxiv.org/abs/1602.04358 | id:1602.04358 author:Leonid Gugel, Yoel Shkolnisky, Shai Dekel category:cs.AI cs.DS stat.ML  published:2016-02-13 summary:In this paper we construct a learning architecture for high dimensional time series sampled by sensor arrangements. Using a redundant wavelet decomposition on a graph constructed over the sensor locations, our algorithm is able to construct discriminative features that exploit the mutual information between the sensors. The algorithm then applies scattering networks to the time series graphs to create the feature space. We demonstrate our method on a machine olfaction problem, where one needs to classify the gas type and the location where it originates from data sampled by an array of sensors. Our experimental results clearly demonstrate that our method outperforms classical machine learning techniques used in previous studies. version:1
arxiv-1602-04348 | Character Proposal Network for Robust Text Extraction | http://arxiv.org/abs/1602.04348 | id:1602.04348 author:Shuye Zhang, Mude Lin, Tianshui Chen, Lianwen Jin, Liang Lin category:cs.CV  published:2016-02-13 summary:Maximally stable extremal regions (MSER), which is a popular method to generate character proposals/candidates, has shown superior performance in scene text detection. However, the pixel-level operation limits its capability for handling some challenging cases (e.g., multiple connected characters, separated parts of one character and non-uniform illumination). To better tackle these cases, we design a character proposal network (CPN) by taking advantage of the high capacity and fast computing of fully convolutional network (FCN). Specifically, the network simultaneously predicts characterness scores and refines the corresponding locations. The characterness scores can be used for proposal ranking to reject non-character proposals and the refining process aims to obtain the more accurate locations. Furthermore, considering the situation that different characters have different aspect ratios, we propose a multi-template strategy, designing a refiner for each aspect ratio. The extensive experiments indicate our method achieves recall rates of 93.88%, 93.60% and 96.46% on ICDAR 2013, SVT and Chinese2k datasets respectively using less than 1000 proposals, demonstrating promising performance of our character proposal network. version:1
arxiv-1602-04341 | Attention-Based Convolutional Neural Network for Machine Comprehension | http://arxiv.org/abs/1602.04341 | id:1602.04341 author:Wenpeng Yin, Sebastian Ebert, Hinrich Schütze category:cs.CL  published:2016-02-13 summary:Understanding open-domain text is one of the primary challenges in natural language processing (NLP). Machine comprehension benchmarks evaluate the system's ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features. Specifically, we explore HABCNN for this task by two routes, one is through traditional joint modeling of passage, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin. version:1
arxiv-1505-02065 | Improving Gibbs Sampling Predictions on Unseen Data for Latent Dirichlet Allocation | http://arxiv.org/abs/1505.02065 | id:1505.02065 author:Yannis Papanikolaou, Timothy N. Rubin, Grigorios Tsoumakas category:stat.ML  published:2015-05-08 summary:Latent Dirichlet Allocation (LDA) is a model for discovering the underlying structure of a given data set. LDA and its extensions have been used in unsupervised and supervised learning tasks across a variety of data types including textual, image and biological data. Several methods have been presented for approximate inference of LDA parameters, including Variational Bayes (VB), Collapsed Gibbs Sampling (CGS) and Collapsed Variational Bayes (CVB) techniques. This work explores three novel methods for generating LDA predictions on unobserved data, given a model trained by CGS. We present extensive experiments on real-world data sets for both standard unsupervised LDA and Prior LDA, one of the supervised variants of LDA for multi-label data. In both supervised and unsupervised settings, we perform extensive empirical comparison of our prediction methods with the standard predictions generated by CGS and CVB0 (a variant of CVB). The results show a consistent advantage of one of our methods over CGS under all experimental conditions, and over CVB0 under the majority of conditions. version:2
arxiv-1602-04335 | Learning Over Long Time Lags | http://arxiv.org/abs/1602.04335 | id:1602.04335 author:Hojjat Salehinejad category:cs.NE  published:2016-02-13 summary:The advantage of recurrent neural networks (RNNs) in learning dependencies between time-series data has distinguished RNNs from other deep learning models. Recently, many advances are proposed in this emerging field. However, there is a lack of comprehensive review on memory models in RNNs in the literature. This paper provides a fundamental review on RNNs and long short term memory (LSTM) model. Then, provides a surveys of recent advances in different memory enhancements and learning techniques for capturing long term dependencies in RNNs. version:1
arxiv-1511-06660 | Using Deep Learning to Predict Demographics from Mobile Phone Metadata | http://arxiv.org/abs/1511.06660 | id:1511.06660 author:Bjarke Felbo, Pål Sundsøy, Alex 'Sandy' Pentland, Sune Lehmann, Yves-Alexandre de Montjoye category:cs.LG  published:2015-11-20 summary:Mobile phone metadata are increasingly used to study human behavior at large-scale. There has recently been a growing interest in predicting demographic information from metadata. Previous approaches relied on hand-engineered features. We here apply, for the first time, deep learning methods to mobile phone metadata using a convolutional network. Our method provides high accuracy on both age and gender prediction. These results show great potential for deep learning approaches for prediction tasks using standard mobile phone metadata. version:4
arxiv-1504-05776 | Combining local regularity estimation and total variation optimization for scale-free texture segmentation | http://arxiv.org/abs/1504.05776 | id:1504.05776 author:Nelly Pustelnik, Herwig Wendt, Patrice Abry, Nicolas Dobigeon category:cs.CV  published:2015-04-22 summary:Texture segmentation constitutes a standard image processing task, crucial to many applications. The present contribution focuses on the particular subset of scale-free textures and its originality resides in the combination of three key ingredients: First, texture characterization relies on the concept of local regularity ; Second, estimation of local regularity is based on new multiscale quantities referred to as wavelet leaders ; Third, segmentation from local regularity faces a fundamental bias variance trade-off: In nature, local regularity estimation shows high variability that impairs the detection of changes, while a posteriori smoothing of regularity estimates precludes from locating correctly changes. Instead, the present contribution proposes several variational problem formulations based on total variation and proximal resolutions that effectively circumvent this trade-off. Estimation and segmentation performance for the proposed procedures are quantified and compared on synthetic as well as on real-world textures. version:2
arxiv-1602-04330 | On the Topology of Projective Shape Spaces | http://arxiv.org/abs/1602.04330 | id:1602.04330 author:Florian Kelma, John T. Kent, Thomas Hotz category:math.ST cs.CV math.GT stat.TH I.4.1; I.4.7  published:2016-02-13 summary:The projective shape of a configuration consists of the information that is invariant under projective transformations. It encodes the information about an object reconstructable from uncalibrated camera views. The space of projective shapes of k points in d-dimensional real projective space is by definition the quotient space of k copies of that projective space modulo the action of the projective linear group. A detailed examination of the topology of projective shape space is given, and it is shown how to derive subsets that are maximal Hausdorff manifolds. A special case are Tyler regular shapes for which one can construct a Riemannian metric. version:1
arxiv-1602-03131 | Large scale multi-objective optimization: Theoretical and practical challenges | http://arxiv.org/abs/1602.03131 | id:1602.03131 author:Kinjal Basu, Ankan Saha, Shaunak Chatterjee category:stat.AP math.OC stat.ML 90C29  90C20  90C90  published:2016-02-09 summary:Multi-objective optimization (MOO) is a well-studied problem for several important recommendation problems. While multiple approaches have been proposed, in this work, we focus on using constrained optimization formulations (e.g., quadratic and linear programs) to formulate and solve MOO problems. This approach can be used to pick desired operating points on the trade-off curve between multiple objectives. It also works well for internet applications which serve large volumes of online traffic, by working with Lagrangian duality formulation to connect dual solutions (computed offline) with the primal solutions (computed online). We identify some key limitations of this approach -- namely the inability to handle user and item level constraints, scalability considerations and variance of dual estimates introduced by sampling processes. We propose solutions for each of the problems and demonstrate how through these solutions we significantly advance the state-of-the-art in this realm. Our proposed methods can exactly handle user and item (and other such local) constraints, achieve a $100\times$ scalability boost over existing packages in R and reduce variance of dual estimates by two orders of magnitude. version:2
arxiv-1409-6086 | Parallel and Distributed Block-Coordinate Frank-Wolfe Algorithms | http://arxiv.org/abs/1409.6086 | id:1409.6086 author:Yu-Xiang Wang, Veeranjaneyulu Sadhanala, Wei Dai, Willie Neiswanger, Suvrit Sra, Eric P. Xing category:stat.ML math.OC  published:2014-09-22 summary:We develop parallel and distributed Frank-Wolfe algorithms; the former on shared memory machines with mini-batching, and the latter in a delayed update framework. Whenever possible, we perform computations asynchronously, which helps attain speedups on multicore machines as well as in distributed environments. Moreover, instead of worst-case bounded delays, our methods only depend (mildly) on \emph{expected} delays, allowing them to be robust to stragglers and faulty worker threads. Our algorithms assume block-separable constraints, and subsume the recent Block-Coordinate Frank-Wolfe (BCFW) method~\citep{lacoste2013block}. Our analysis reveals problem-dependent quantities that govern the speedups of our methods over BCFW. We present experiments on structural SVM and Group Fused Lasso, obtaining significant speedups over competing state-of-the-art (and synchronous) methods. version:2
arxiv-1602-04287 | A Minimax Theory for Adaptive Data Analysis | http://arxiv.org/abs/1602.04287 | id:1602.04287 author:Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg category:stat.ML cs.LG  published:2016-02-13 summary:In adaptive data analysis, the user makes a sequence of queries on the data, where at each step the choice of query may depend on the results in previous steps. The releases are often randomized in order to reduce overfitting for such adaptively chosen queries. In this paper, we propose a minimax framework for adaptive data analysis. Assuming Gaussianity of queries, we establish the first sharp minimax lower bound on the squared error in the order of $O(\frac{\sqrt{k}\sigma^2}{n})$, where $k$ is the number of queries asked, and $\sigma^2/n$ is the ordinary signal-to-noise ratio for a single query. Our lower bound is based on the construction of an approximately least favorable adversary who picks a sequence of queries that are most likely to be affected by overfitting. This approximately least favorable adversary uses only one level of adaptivity, suggesting that the minimax risk for 1-step adaptivity with k-1 initial releases and that for $k$-step adaptivity are on the same order. The key technical component of the lower bound proof is a reduction to finding the convoluting distribution that optimally obfuscates the sign of a Gaussian signal. Our lower bound construction also reveals a transparent and elementary proof of the matching upper bound as an alternative approach to Russo and Zou (2015), who used information-theoretic tools to provide the same upper bound. We believe that the proposed framework opens up opportunities to obtain theoretical insights for many other settings of adaptive data analysis, which would extend the idea to more practical realms. version:1
arxiv-1502-00182 | A Subspace Learning Approach to High-Dimensional Matrix Decomposition with Efficient Information Sampling | http://arxiv.org/abs/1502.00182 | id:1502.00182 author:Mostafa Rahmani, George Atia category:cs.NA cs.DS cs.LG math.NA stat.ML  published:2015-02-01 summary:This paper is concerned with the problem of low-rank plus sparse matrix decomposition for big data. Conventional algorithms for matrix decomposition use the entire data to extract the low-rank and sparse components, and are based on optimization problems that scale with the dimension of the data, which limit their scalability. Furthermore, the existing randomized approaches mostly rely on uniform random sampling, which can be quite inefficient for many real world data matrices that exhibit additional structures (e.g. clustering). In this paper, a scalable subspace-pursuit approach that transforms the decomposition problem to a subspace learning problem is proposed. The decomposition is carried out using a small data sketch formed from sampled columns/rows. Even when the data is sampled uniformly at random, it is shown that the sufficient number of sampled columns/rows is roughly O(r \mu), where \mu is the coherency parameter and r the rank of the low-rank component. In addition, efficient sampling algorithms are proposed to address the problem of column/row sampling from structured data. The proposed sampling algorithms can be independently used for feature selection from high-dimensional data. The proposed approach is amenable to online implementation and an online scheme is proposed. version:2
arxiv-1505-05629 | Regulating Greed Over Time | http://arxiv.org/abs/1505.05629 | id:1505.05629 author:Stefano Tracà, Cynthia Rudin category:stat.ML cs.LG F.2.0; G.1.6; I.2.6  published:2015-05-21 summary:In retail, there are predictable yet dramatic time-dependent patterns in customer behavior, such as periodic changes in the number of visitors, or increases in visitors just before major holidays (e.g., Christmas). The current paradigm of multi-armed bandit analysis does not take these known patterns into account, which means that despite the firm theoretical foundation of these methods, they are fundamentally flawed when it comes to real applications. This work provides a remedy that takes the time-dependent patterns into account, and we show how this remedy is implemented in the UCB and {\epsilon}-greedy methods. In the corrected methods, exploitation (greed) is regulated over time, so that more exploitation occurs during higher reward periods, and more exploration occurs in periods of low reward. In order to understand why regret is reduced with the corrected methods, we present a set of bounds that provide insight into why we would want to exploit during periods of high reward, and discuss the impact on regret. Our proposed methods have excellent performance in experiments, and were inspired by a high-scoring entry in the Exploration and Exploitation 3 contest using data from Yahoo! Front Page. That entry heavily used time-series methods to regulate greed over time, which was substantially more effective than other contextual bandit methods. version:2
arxiv-1602-04283 | Deep Learning on FPGAs: Past, Present, and Future | http://arxiv.org/abs/1602.04283 | id:1602.04283 author:Griffin Lacey, Graham W. Taylor, Shawki Areibi category:cs.DC cs.LG stat.ML  published:2016-02-13 summary:The rapid growth of data size and accessibility in recent years has instigated a shift of philosophy in algorithm design for artificial intelligence. Instead of engineering algorithms by hand, the ability to learn composable systems automatically from massive amounts of data has led to ground-breaking performance in important domains such as computer vision, speech recognition, and natural language processing. The most popular class of techniques used in these domains is called deep learning, and is seeing significant attention from industry. However, these models require incredible amounts of data and compute power to train, and are limited by the need for better hardware acceleration to accommodate scaling beyond current data and model sizes. While the current solution has been to use clusters of graphics processing units (GPU) as general purpose processors (GPGPU), the use of field programmable gate arrays (FPGA) provide an interesting alternative. Current trends in design tools for FPGAs have made them more compatible with the high-level software practices typically practiced in the deep learning community, making FPGAs more accessible to those who build and deploy models. Since FPGA architectures are flexible, this could also allow researchers the ability to explore model-level optimizations beyond what is possible on fixed architectures such as GPUs. As well, FPGAs tend to provide high performance per watt of power consumption, which is of particular importance for application scientists interested in large scale server-based deployment or resource-limited embedded applications. This review takes a look at deep learning and FPGAs from a hardware acceleration perspective, identifying trends and innovations that make these technologies a natural fit, and motivates a discussion on how FPGAs may best serve the needs of the deep learning community moving forward. version:1
arxiv-1602-04282 | Conservative Bandits | http://arxiv.org/abs/1602.04282 | id:1602.04282 author:Yifan Wu, Roshan Shariff, Tor Lattimore, Csaba Szepesvári category:stat.ML cs.LG  published:2016-02-13 summary:We study a novel multi-armed bandit problem that models the challenge faced by a company wishing to explore new strategies to maximize revenue whilst simultaneously maintaining their revenue above a fixed baseline, uniformly over time. While previous work addressed the problem under the weaker requirement of maintaining the revenue constraint only at a given fixed time in the future, the algorithms previously proposed are unsuitable due to their design under the more stringent constraints. We consider both the stochastic and the adversarial settings, where we propose, natural, yet novel strategies and analyze the price for maintaining the constraints. Amongst other things, we prove both high probability and expectation bounds on the regret, while we also consider both the problem of maintaining the constraints with high probability or expectation. For the adversarial setting the price of maintaining the constraint appears to be higher, at least for the algorithm considered. A lower bound is given showing that the algorithm for the stochastic setting is almost optimal. Empirical results obtained in synthetic environments complement our theoretical findings. version:1
arxiv-1602-04278 | Signer-independent Fingerspelling Recognition with Deep Neural Network Adaptation | http://arxiv.org/abs/1602.04278 | id:1602.04278 author:Taehwan Kim, Weiran Wang, Hao Tang, Karen Livescu category:cs.CL cs.CV cs.NE  published:2016-02-13 summary:We study the problem of recognition of fingerspelled letter sequences in American Sign Language in a signer-independent setting. Fingerspelled sequences are both challenging and important to recognize, as they are used for many content words such as proper nouns and technical terms. Previous work has shown that it is possible to achieve almost 90% accuracies on fingerspelling recognition in a signer-dependent setting. However, the more realistic signer-independent setting presents challenges due to significant variations among signers, coupled with the dearth of available training data. We investigate this problem with approaches inspired by automatic speech recognition. We start with the best-performing approaches from prior work, based on tandem models and segmental conditional random fields (SCRFs), with features based on deep neural network (DNN) classifiers of letters and phonological features. Using DNN adaptation, we find that it is possible to bridge a large part of the gap between signer-dependent and signer-independent performance. Using only about 115 transcribed words for adaptation from the target signer, we obtain letter accuracies of up to 82.7% with frame-level adaptation labels and 69.7% with only word labels. version:1
arxiv-1510-06779 | Cascaded High Dimensional Histograms: A Generative Approach to Density Estimation | http://arxiv.org/abs/1510.06779 | id:1510.06779 author:Siong Thye Goh, Cynthia Rudin category:stat.ML 62  published:2015-10-22 summary:We present tree- and list- structured density estimation methods for high dimensional binary/categorical data. Our density estimation models are high dimensional analogies to variable bin width histograms. In each leaf of the tree (or list), the density is constant, similar to the flat density within the bin of a histogram. Histograms, however, cannot easily be visualized in higher dimensions, whereas our models can. The accuracy of histograms fades as dimensions increase, whereas our models have priors that help with generalization. Our models are sparse, unlike high-dimensional histograms. We present three generative models, where the first one allows the user to specify the number of desired leaves in the tree within a Bayesian prior. The second model allows the user to specify the desired number of branches within the prior. The third model returns lists (rather than trees) and allows the user to specify the desired number of rules and the length of rules within the prior. Our results indicate that the new approaches yield a better balance between sparsity and accuracy of density estimates than other methods for this task. version:3
arxiv-1602-04277 | Evaluation of Protein Structural Models Using Random Forests | http://arxiv.org/abs/1602.04277 | id:1602.04277 author:Renzhi Cao, Taeho Jo, Jianlin Cheng category:cs.LG q-bio.BM q-bio.QM stat.ML  published:2016-02-13 summary:Protein structure prediction has been a grand challenge problem in the structure biology over the last few decades. Protein quality assessment plays a very important role in protein structure prediction. In the paper, we propose a new protein quality assessment method which can predict both local and global quality of the protein 3D structural models. Our method uses both multi and single model quality assessment method for global quality assessment, and uses chemical, physical, geo-metrical features, and global quality score for local quality assessment. CASP9 targets are used to generate the features for local quality assessment. We evaluate the performance of our local quality assessment method on CASP10, which is comparable with two stage-of-art QA methods based on the average absolute distance between the real and predicted distance. In addition, we blindly tested our method on CASP11, and the good performance shows that combining single and multiple model quality assessment method could be a good way to improve the accuracy of model quality assessment, and the random forest technique could be used to train a good local quality assessment model. version:1
arxiv-1602-01198 | k-variates++: more pluses in the k-means++ | http://arxiv.org/abs/1602.01198 | id:1602.01198 author:Richard Nock, Raphaël Canyasse, Roksana Boreli, Frank Nielsen category:cs.LG H.3.3; I.5.3  published:2016-02-03 summary:k-means++ seeding has become a de facto standard for hard clustering algorithms. In this paper, our first contribution is a two-way generalisation of this seeding, k-variates++, that includes the sampling of general densities rather than just a discrete set of Dirac densities anchored at the point locations, and a generalisation of the well known Arthur-Vassilvitskii (AV) approximation guarantee, in the form of a bias+variance approximation bound of the global optimum. This approximation exhibits a reduced dependency on the "noise" component with respect to the optimal potential --- actually approaching the statistical lower bound. We show that k-variates++ reduces to efficient (biased seeding) clustering algorithms tailored to specific frameworks; these include distributed, streaming and on-line clustering, with direct approximation results for these algorithms. Finally, we present a novel application of k-variates++ to differential privacy. For either the specific frameworks considered here, or for the differential privacy setting, there is little to no prior results on the direct application of k-means++ and its approximation bounds --- state of the art contenders appear to be significantly more complex and / or display less favorable (approximation) properties. We stress that our algorithms can still be run in cases where there is \textit{no} closed form solution for the population minimizer. We demonstrate the applicability of our analysis via experimental evaluation on several domains and settings, displaying competitive performances vs state of the art. version:2
arxiv-1512-05244 | Learning Games and Rademacher Observations Losses | http://arxiv.org/abs/1512.05244 | id:1512.05244 author:Richard Nock category:cs.LG I.2.6  published:2015-12-16 summary:It has recently been shown that supervised learning with the popular logistic loss is equivalent to optimizing the exponential loss over sufficient statistics about the class: Rademacher observations (rados). We first show that this unexpected equivalence can actually be generalized to other example / rado losses, with necessary and sufficient conditions for the equivalence, exemplified on four losses that bear popular names in various fields: exponential (boosting), mean-variance (finance), Linear Hinge (on-line learning), ReLU (deep learning), and unhinged (statistics). Second, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (with Minkowski sums) in the equivalent rado loss. This brings simple and powerful rado-based learning algorithms for sparsity-controlling regularization, that we exemplify on a boosting algorithm for the regularized exponential rado-loss, which formally boosts over four types of regularization, including the popular ridge and lasso, and the recently coined slope --- we obtain the first proven boosting algorithm for this last regularization. Through our first contribution on the equivalence of rado and example-based losses, Omega-R.AdaBoost~appears to be an efficient proxy to boost the regularized logistic loss over examples using whichever of the four regularizers. Experiments display that regularization consistently improves performances of rado-based learning, and may challenge or beat the state of the art of example-based learning even when learning over small sets of rados. Finally, we connect regularization to differential privacy, and display how tiny budgets can be afforded on big domains while beating (protected) example-based learning. version:2
arxiv-1602-04265 | Regularized Estimation in High Dimensional Time Series under Mixing Conditions | http://arxiv.org/abs/1602.04265 | id:1602.04265 author:Kam Chung Wong, Ambuj Tewari, Zifan Li category:stat.ML cs.LG  published:2016-02-12 summary:The Lasso is one of the most popular methods in high dimensional statistical learning. Most existing theoretical results for the Lasso, however, require the samples to be iid. Recent work has provided guarantees for the Lasso assuming that the time series is generated by a sparse Vector Auto-Regressive (VAR) model with Gaussian innovations. Proofs of these results rely critically on the fact that the true data generating mechanism (DGM) is a finite-order Gaussian VAR. This assumption is quite brittle: linear transformations, including selecting a subset of variables, can lead to the violation of this assumption. In order to break free from such assumptions, we derive nonasymptotic inequalities for estimation error and prediction error of the Lasso estimate of the best linear predictor without assuming any special parametric form of the DGM. Instead, we rely only on (strict) stationarity and mixing conditions to establish consistency of the Lasso in the following two scenarios: (a) alpha-mixing Gaussian processes, and (b) beta-mixing sub-Gaussian random vectors. Our work provides an alternative proof of the consistency of the Lasso for sparse Gaussian VAR models. But the applicability of our results extends to non-Gaussian and non-linear times series models as the examples we provide demonstrate. In order to prove our results, we derive a novel Hanson-Wright type concentration inequality for beta-mixing sub-Gaussian random vectors that may be of independent interest. version:1
arxiv-1408-3686 | Motion Deblurring for Plenoptic Images | http://arxiv.org/abs/1408.3686 | id:1408.3686 author:Paramanand Chandramouli, Paolo Favaro, Daniele Perrone category:cs.CV  published:2014-08-16 summary:We address for the first time the issue of motion blur in light field images captured from plenoptic cameras. We propose a solution to the estimation of a sharp high resolution scene radiance given a blurry light field image, when the motion blur point spread function is unknown, i.e., the so-called blind deconvolution problem. In a plenoptic camera, the spatial sampling in each view is not only decimated but also defocused. Consequently, current blind deconvolution approaches for traditional cameras are not applicable. Due to the complexity of the imaging model, we investigate first the case of uniform (shift-invariant) blur of Lambertian objects, i.e., when objects are sufficiently far away from the camera to be approximately invariant to depth changes and their reflectance does not vary with the viewing direction. We introduce a highly parallelizable model for light field motion blur that is computationally and memory efficient. We then adapt a regularized blind deconvolution approach to our model and demonstrate its performance on both synthetic and real light field data. Our method handles practical issues in real cameras such as radial distortion correction and alignment within an energy minimization framework. version:2
arxiv-1602-04227 | Scale-free network optimization: foundations and algorithms | http://arxiv.org/abs/1602.04227 | id:1602.04227 author:Patrick Rebeschini, Sekhar Tatikonda category:stat.ML math.OC  published:2016-02-12 summary:We investigate the fundamental principles that drive the development of scalable algorithms for network optimization. Despite the significant amount of work on parallel and decentralized algorithms in the optimization community, the methods that have been proposed typically rely on strict separability assumptions for objective function and constraints. Beside sparsity, these methods typically do not exploit the strength of the interaction between variables in the system. We propose a notion of correlation in constrained optimization that is based on the sensitivity of the optimal solution upon perturbations of the constraints. We develop a general theory of sensitivity of optimizers the extends beyond the infinitesimal setting. We present instances in network optimization where the correlation decays exponentially fast with respect to the natural distance in the network, and we design algorithms that can exploit this decay to yield dimension-free optimization. Our results are the first of their kind, and open new possibilities in the theory of local algorithms. version:1
arxiv-1504-02206 | A Multiphase Image Segmentation Based on Fuzzy Membership Functions and L1-norm Fidelity | http://arxiv.org/abs/1504.02206 | id:1504.02206 author:Fang Li, Stanley Osher, Jing Qin, Ming Yan category:math.OC cs.CV  published:2015-04-09 summary:In this paper, we propose a variational multiphase image segmentation model based on fuzzy membership functions and L1-norm fidelity. Then we apply the alternating direction method of multipliers to solve an equivalent problem. All the subproblems can be solved efficiently. Specifically, we propose a fast method to calculate the fuzzy median. Experimental results and comparisons show that the L1-norm based method is more robust to outliers such as impulse noise and keeps better contrast than its L2-norm counterpart. Theoretically, we prove the existence of the minimizer and analyze the convergence of the algorithm. version:2
arxiv-1602-04208 | Pursuits in Structured Non-Convex Matrix Factorizations | http://arxiv.org/abs/1602.04208 | id:1602.04208 author:Rajiv Khanna, Michael Tschannen, Martin Jaggi category:cs.LG stat.ML  published:2016-02-12 summary:Efficiently representing real world data in a succinct and parsimonious manner is of central importance in many fields. We present a generalized greedy pursuit framework, allowing us to efficiently solve structured matrix factorization problems, where the factors are allowed to be from arbitrary sets of structured vectors. Such structure may include sparsity, non-negativeness, order, or a combination thereof. The algorithm approximates a given matrix by a linear combination of few rank-1 matrices, each factorized into an outer product of two vector atoms of the desired structure. For the non-convex subproblems of obtaining good rank-1 structured matrix atoms, we employ and analyze a general atomic power method. In addition to the above applications, we prove linear convergence for generalized pursuit variants in Hilbert spaces - for the task of approximation over the linear span of arbitrary dictionaries - which generalizes OMP and is useful beyond matrix problems. Our experiments on real datasets confirm both the efficiency and also the broad applicability of our framework in practice. version:1
arxiv-1602-04709 | Identifying Structures in Social Conversations in NSCLC Patients through the Semi-Automatic extraction of Topical Taxonomies | http://arxiv.org/abs/1602.04709 | id:1602.04709 author:Giancarlo Crocetti, Amir A. Delay, Fatemeh Seyedmendhi category:cs.IR cs.AI cs.CL H.3.1; H.3.3  published:2016-02-12 summary:The exploration of social conversations for addressing patient's needs is an important analytical task in which many scholarly publications are contributing to fill the knowledge gap in this area. The main difficulty remains the inability to turn such contributions into pragmatic processes the pharmaceutical industry can leverage in order to generate insight from social media data, which can be considered as one of the most challenging source of information available today due to its sheer volume and noise. This study is based on the work by Scott Spangler and Jeffrey Kreulen and applies it to identify structure in social media through the extraction of a topical taxonomy able to capture the latent knowledge in social conversations in health-related sites. The mechanism for automatically identifying and generating a taxonomy from social conversations is developed and pressured tested using public data from media sites focused on the needs of cancer patients and their families. Moreover, a novel method for generating the category's label and the determination of an optimal number of categories is presented which extends Scott and Jeffrey's research in a meaningful way. We assume the reader is familiar with taxonomies, what they are and how they are used. version:1
arxiv-1602-04186 | An Evolutionary Strategy based on Partial Imitation for Solving Optimization Problems | http://arxiv.org/abs/1602.04186 | id:1602.04186 author:Marco Alberto Javarone category:cond-mat.dis-nn cs.NE math.OC  published:2016-02-12 summary:In this work we introduce an evolutionary strategy to solve optimization tasks. In particular, we focus on the Travel Salesman Problem (TSP), i.e., a NP-hard problem with a discrete search space. The solutions of the TSP can be codified by arrays of cities, and can be evaluated by a fitness computed according to a cost function (e.g., the length of a path). Our method is based on the evolution of an agent population by means of a `partial imitation' mechanism. In particular, agents receive a random solution and then, interacting among themselves, imitate the solutions of agents with a higher fitness. Moreover, as stated above, the imitation is only partial, i.e., agents copy only one, randomly chosen, entry of better (array) solutions. In doing so, the population converges towards a shared solution, behaving like a spin system undergoing a cooling process, i.e., driven towards an ordered phase. We highlight that the adopted `partial imitation' mechanism allows the population to generate new solutions over time, before reaching the final equilibrium. Remarkably, results of numerical simulations show that our method is able to find the optimal solution in all considered search spaces. version:1
arxiv-1511-07837 | Generalized Conjugate Gradient Methods for $\ell_1$ Regularized Convex Quadratic Programming with Finite Convergence | http://arxiv.org/abs/1511.07837 | id:1511.07837 author:Zhaosong Lu, Xiaojun Chen category:math.OC cs.LG math.NA stat.CO stat.ML  published:2015-11-24 summary:The conjugate gradient (CG) method is an efficient iterative method for solving large-scale strongly convex quadratic programming (QP). In this paper we propose some generalized CG (GCG) methods for solving the $\ell_1$-regularized (possibly not strongly) convex QP that terminate at an optimal solution in a finite number of iterations. At each iteration, our methods first identify a face of an orthant and then either perform an exact line search along the direction of the negative projected minimum-norm subgradient of the objective function or execute a CG subroutine that conducts a sequence of CG iterations until a CG iterate crosses the boundary of this face or an approximate minimizer of over this face or a subface is found. We determine which type of step should be taken by comparing the magnitude of some components of the minimum-norm subgradient of the objective function to that of its rest components. Our analysis on finite convergence of these methods makes use of an error bound result and some key properties of the aforementioned exact line search and the CG subroutine. We also show that the proposed methods are capable of finding an approximate solution of the problem by allowing some inexactness on the execution of the CG subroutine. The overall arithmetic operation cost of our GCG methods for finding an $\epsilon$-optimal solution depends on $\epsilon$ in $O(\log(1/\epsilon))$, which is superior to the accelerated proximal gradient method [2,23] that depends on $\epsilon$ in $O(1/\sqrt{\epsilon})$. In addition, our GCG methods can be extended straightforwardly to solve box-constrained convex QP with finite convergence. Numerical results demonstrate that our methods are very favorable for solving ill-conditioned problems. version:3
arxiv-1506-03762 | Recovering metric from full ordinal information | http://arxiv.org/abs/1506.03762 | id:1506.03762 author:Thibaut Le Gouic category:stat.ML math.ST stat.TH  published:2015-06-11 summary:Given a geodesic space (E, d), we show that full ordinal knowledge on the metric d-i.e. knowledge of the function D d : (w, x, y, z) $\rightarrow$ 1 d(w,x)$\le$d(y,z) , determines uniquely-up to a constant factor-the metric d. For a subspace En of n points of E, converging in Hausdorff distance to E, we construct a metric dn on En, based only on the knowledge of D d on En and establish a sharp upper bound of the Gromov-Hausdorff distance between (En, dn) and (E, d). version:3
arxiv-1511-03229 | Learning Communities in the Presence of Errors | http://arxiv.org/abs/1511.03229 | id:1511.03229 author:Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan category:cs.DS cs.LG math.ST stat.TH  published:2015-11-10 summary:We study the problem of learning communities in the presence of modeling errors and give robust recovery algorithms for the Stochastic Block Model (SBM). This model, which is also known as the Planted Partition Model, is widely used for community detection and graph partitioning in various fields, including machine learning, statistics, and social sciences. Many algorithms exist for learning communities in the Stochastic Block Model, but they do not work well in the presence of errors. In this paper, we initiate the study of robust algorithms for partial recovery in SBM with modeling errors or noise. We consider graphs generated according to the Stochastic Block Model and then modified by an adversary. We allow two types of adversarial errors, Feige---Kilian or monotone errors, and edge outlier errors. Mossel, Neeman and Sly (STOC 2015) posed an open question about whether an almost exact recovery is possible when the adversary is allowed to add $o(n)$ edges. Our work answers this question affirmatively even in the case of $k>2$ communities. We then show that our algorithms work not only when the instances come from SBM, but also work when the instances come from any distribution of graphs that is $\epsilon m$ close to SBM in the Kullback---Leibler divergence. This result also works in the presence of adversarial errors. Finally, we present almost tight lower bounds for two communities. version:2
arxiv-1602-04133 | Deep Gaussian Processes for Regression using Approximate Expectation Propagation | http://arxiv.org/abs/1602.04133 | id:1602.04133 author:Thang D. Bui, Daniel Hernández-Lobato, Yingzhen Li, José Miguel Hernández-Lobato, Richard E. Turner category:stat.ML cs.LG  published:2016-02-12 summary:Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks. version:1
arxiv-1602-04128 | From Coin Betting to Parameter-Free Online Learning | http://arxiv.org/abs/1602.04128 | id:1602.04128 author:Francesco Orabona, Dávid Pál category:cs.LG  published:2016-02-12 summary:In the recent years a number of parameter-free algorithms for online linear optimization over Hilbert spaces and for learning with expert advice have been developed. While these two families of algorithms might seem different to a distract eye, the proof methods are indeed very similar, making the reader wonder if such a connection is only accidental. In this paper, we unify these two families, showing that both can be instantiated from online coin betting algorithms. We present two new reductions from online coin betting to online linear optimization over Hilbert spaces and to learning with expert advice. We instantiate our framework using a betting algorithm based on the Krichevsky-Trofimov estimator. We obtain a simple algorithm for online linear optimization over any Hilbert space with $O(\norm{u}\sqrt{T \log(1+T \norm{u}}))$ regret with respect to any competitor $u$. For learning with expert advice we obtain an algorithm that has $O(\sqrt{T (1 + \KL{u}{\pi})})$ regret against any competitor $u$ and where $\KL{u}{\pi}$ is the Kullback-Leibler divergence between algorithm's prior distribution $\pi$ and the competitor. In both cases, no parameters need to be tuned. version:1
arxiv-1602-04124 | Fast and Robust Hand Tracking Using Detection-Guided Optimization | http://arxiv.org/abs/1602.04124 | id:1602.04124 author:Srinath Sridhar, Franziska Mueller, Antti Oulasvirta, Christian Theobalt category:cs.CV  published:2016-02-12 summary:Markerless tracking of hands and fingers is a promising enabler for human-computer interaction. However, adoption has been limited because of tracking inaccuracies, incomplete coverage of motions, low framerate, complex camera setups, and high computational requirements. In this paper, we present a fast method for accurately tracking rapid and complex articulations of the hand using a single depth camera. Our algorithm uses a novel detection-guided optimization strategy that increases the robustness and speed of pose estimation. In the detection step, a randomized decision forest classifies pixels into parts of the hand. In the optimization step, a novel objective function combines the detected part labels and a Gaussian mixture representation of the depth to estimate a pose that best fits the depth. Our approach needs comparably less computational resources which makes it extremely fast (50 fps without GPU support). The approach also supports varying static, or moving, camera-to-scene arrangements. We show the benefits of our method by evaluating on public datasets and comparing against previous work. version:1
arxiv-1602-04101 | An Empirical Study on Academic Commentary and Its Implications on Reading and Writing | http://arxiv.org/abs/1602.04101 | id:1602.04101 author:Tai Wang, Xiangen Hu, Keith Shubeck, Zhiqiang Cai, Jie Tang category:cs.CY cs.CL  published:2016-02-12 summary:The relationship between reading and writing (RRW) is one of the major themes in learning science. One of its obstacles is that it is difficult to define or measure the latent background knowledge of the individual. However, in an academic research setting, scholars are required to explicitly list their background knowledge in the citation sections of their manuscripts. This unique opportunity was taken advantage of to observe RRW, especially in the published academic commentary scenario. RRW was visualized under a proposed topic process model by using a state of the art version of latent Dirichlet allocation (LDA). The empirical study showed that the academic commentary is modulated both by its target paper and the author's background knowledge. Although this conclusion was obtained in a unique environment, we suggest its implications can also shed light on other similar interesting areas, such as dialog and conversation, group discussion, and social media. version:1
arxiv-1602-04062 | Using Deep Q-Learning to Control Optimization Hyperparameters | http://arxiv.org/abs/1602.04062 | id:1602.04062 author:Samantha Hansen category:math.OC cs.LG  published:2016-02-12 summary:We present a novel definition of the reinforcement learning state, actions and reward function that allows a deep Q-network (DQN) to learn to control an optimization hyperparameter. Using Q-learning with experience replay, we train two DQNs to accept a state representation of an objective function as input and output the expected discounted return of rewards, or q-values, connected to the actions of either adjusting the learning rate or leaving it unchanged. The two DQNs learn a policy similar to a line search, but differ in the number of allowed actions. The trained DQNs in combination with a gradient-based update routine form the basis of the Q-gradient descent algorithms. To demonstrate the viability of this framework, we show that the DQN's q-values associated with optimal action converge and that the Q-gradient descent algorithms outperform gradient descent with an Armijo or nonomonotone line search. Unlike traditional optimization methods, Q-gradient descent can incorporate any objective statistic and by varying the actions we gain insight into the type of learning rate adjustment strategies that are successful for neural network optimization. version:1
arxiv-1602-02575 | DECOrrelated feature space partitioning for distributed sparse regression | http://arxiv.org/abs/1602.02575 | id:1602.02575 author:Xiangyu Wang, David Dunson, Chenlei Leng category:stat.ME cs.DC stat.CO stat.ML  published:2016-02-08 summary:Fitting statistical models is computationally challenging when the sample size or the dimension of the dataset is huge. An attractive approach for down-scaling the problem size is to first partition the dataset into subsets and then fit using distributed algorithms. The dataset can be partitioned either horizontally (in the sample space) or vertically (in the feature space). While the majority of the literature focuses on sample space partitioning, feature space partitioning is more effective when $p\gg n$. Existing methods for partitioning features, however, are either vulnerable to high correlations or inefficient in reducing the model dimension. In this paper, we solve these problems through a new embarrassingly parallel framework named DECO for distributed variable selection and parameter estimation. In DECO, variables are first partitioned and allocated to $m$ distributed workers. The decorrelated subset data within each worker are then fitted via any algorithm designed for high-dimensional problems. We show that by incorporating the decorrelation step, DECO can achieve consistent variable selection and parameter estimation on each subset with (almost) no assumptions. In addition, the convergence rate is nearly minimax optimal for both sparse and weakly sparse models and does NOT depend on the partition number $m$. Extensive numerical experiments are provided to illustrate the performance of the new framework. version:2
arxiv-1602-03670 | Online Low-Rank Subspace Learning from Incomplete Data: A Bayesian View | http://arxiv.org/abs/1602.03670 | id:1602.03670 author:Paris V. Giampouras, Athanasios A. Rontogiannis, Konstantinos E. Themelis, Konstantinos D. Koutroumbas category:stat.ML  published:2016-02-11 summary:Extracting the underlying low-dimensional space where high-dimensional signals often reside has long been at the center of numerous algorithms in the signal processing and machine learning literature during the past few decades. At the same time, working with incomplete (partly observed) large scale datasets has recently been commonplace for diverse reasons. This so called {\it big data era} we are currently living calls for devising online subspace learning algorithms that can suitably handle incomplete data. Their envisaged objective is to {\it recursively} estimate the unknown subspace by processing streaming data sequentially, thus reducing computational complexity, while obviating the need for storing the whole dataset in memory. In this paper, an online variational Bayes subspace learning algorithm from partial observations is presented. To account for the unawareness of the true rank of the subspace, commonly met in practice, low-rankness is explicitly imposed on the sought subspace data matrix by exploiting sparse Bayesian learning principles. Moreover, sparsity, {\it simultaneously} to low-rankness, is favored on the subspace matrix by the sophisticated hierarchical Bayesian scheme that is adopted. In doing so, the proposed algorithm becomes adept in dealing with applications whereby the underlying subspace may be also sparse, as, e.g., in sparse dictionary learning problems. As shown, the new subspace tracking scheme outperforms its state-of-the-art counterparts in terms of estimation accuracy, in a variety of experiments conducted on simulated and real data. version:2
arxiv-1602-03995 | An automatic method for segmentation of fission tracks in epidote crystal photomicrographs | http://arxiv.org/abs/1602.03995 | id:1602.03995 author:Alexandre Fioravante de Siqueira, Wagner Massayuki Nakasuga, Aylton Pagamisse, Carlos Alberto Tello Saenz, Aldo Eloizo Job category:cs.CV 65T60  published:2016-02-12 summary:Manual identification of fission tracks has practical problems, such as variation due to observer-observation efficiency. An automatic processing method that could identify fission tracks in a photomicrograph could solve this problem and improve the speed of track counting. However, separation of non-trivial images is one of the most difficult tasks in image processing. Several commercial and free softwares are available, but these softwares are meant to be used in specific images. In this paper, an automatic method based on starlet wavelets is presented in order to separate fission tracks in mineral photomicrographs. Automatization is obtained by Matthews correlation coefficient, and results are evaluated by precision, recall and accuracy. This technique is an improvement of a method aimed at segmentation of scanning electron microscopy images. This method is applied in photomicrographs of epidote phenocrystals, in which accuracy higher than 89% was obtained in fission track segmentation, even for difficult images. Algorithms corresponding to the proposed method are available for download. Using the method presented here, an user could easily determine fission tracks in photomicrographs of mineral samples. version:1
arxiv-1601-03890 | Stereo Matching by Joint Energy Minimization | http://arxiv.org/abs/1601.03890 | id:1601.03890 author:Hongyang Xue, Deng Cai category:cs.CV  published:2016-01-15 summary:In [18], Mozerov et al. propose to perform stereo matching as a two-step energy minimization problem. For the first step they solve a fully connected MRF model. And in the next step the marginal output is employed as the unary cost for a locally connected MRF model. In this paper we intend to combine the two steps of energy minimization in order to improve stereo matching results. We observe that the fully connected MRF leads to smoother disparity maps, while the locally connected MRF achieves superior results in fine-structured regions. Thus we propose to jointly solve the fully connected and locally connected models, taking both their advantages into account. The joint model is solved by mean field approximations. While remaining efficient, our joint model outperforms the two-step energy minimization approach in both time and estimation error on the Middlebury stereo benchmark v3. version:3
arxiv-1602-03992 | Orthogonal Sparse PCA and Covariance Estimation via Procrustes Reformulation | http://arxiv.org/abs/1602.03992 | id:1602.03992 author:Konstantinos Benidis, Ying Sun, Prabhu Babu, Daniel P. Palomar category:stat.ML cs.LG math.OC stat.AP  published:2016-02-12 summary:The problem of estimating sparse eigenvectors of a symmetric matrix attracts a lot of attention in many applications, especially those with high dimensional data set. While classical eigenvectors can be obtained as the solution of a maximization problem, existing approaches formulate this problem by adding a penalty term into the objective function that encourages a sparse solution. However, the resulting methods achieve sparsity at the expense of sacrificing the orthogonality property. In this paper, we develop a new method to estimate dominant sparse eigenvectors without trading off their orthogonality. The problem is highly non-convex and hard to handle. We apply the MM framework where we iteratively maximize a tight lower bound (surrogate function) of the objective function over the Stiefel manifold. The inner maximization problem turns out to be a rectangular Procrustes problem, which has a closed form solution. In addition, we propose a method to improve the covariance estimation problem when its underlying eigenvectors are known to be sparse. We use the eigenvalue decomposition of the covariance matrix to formulate an optimization problem where we impose sparsity on the corresponding eigenvectors. Numerical experiments show that the proposed eigenvector extraction algorithm matches or outperforms existing algorithms in terms of support recovery and explained variance, while the covariance estimation algorithms improve significantly the sample covariance estimator. version:1
arxiv-1105-5332 | Multidimensional Scaling in the Poincare Disk | http://arxiv.org/abs/1105.5332 | id:1105.5332 author:Andrej Cvetkovski, Mark Crovella category:stat.ML cs.SI  published:2011-05-26 summary:Multidimensional scaling (MDS) is a class of projective algorithms traditionally used in Euclidean space to produce two- or three-dimensional visualizations of datasets of multidimensional points or point distances. More recently however, several authors have pointed out that for certain datasets, hyperbolic target space may provide a better fit than Euclidean space. In this paper we develop PD-MDS, a metric MDS algorithm designed specifically for the Poincare disk (PD) model of the hyperbolic plane. Emphasizing the importance of proceeding from first principles in spite of the availability of various black box optimizers, our construction is based on an elementary hyperbolic line search and reveals numerous particulars that need to be carefully addressed when implementing this as well as more sophisticated iterative optimization methods in a hyperbolic space model. version:3
arxiv-1602-03960 | TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice Questions | http://arxiv.org/abs/1602.03960 | id:1602.03960 author:Sujay Kumar Jauhar, Peter Turney, Eduard Hovy category:cs.CL  published:2016-02-12 summary:We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling. version:1
arxiv-1511-08343 | The Automatic Statistician: A Relational Perspective | http://arxiv.org/abs/1511.08343 | id:1511.08343 author:Yunseong Hwang, Anh Tong, Jaesik Choi category:cs.LG stat.ML  published:2015-11-26 summary:Gaussian Processes (GPs) provide a general and analytically tractable way of modeling complex time-varying, nonparametric functions. The Automatic Bayesian Covariance Discovery (ABCD) system constructs natural-language description of time-series data by treating unknown time-series data nonparametrically using GP with a composite covariance kernel function. Unfortunately, learning a composite covariance kernel with a single time-series data set often results in less informative kernel that may not give qualitative, distinctive descriptions of data. We address this challenge by proposing two relational kernel learning methods which can model multiple time-series data sets by finding common, shared causes of changes. We show that the relational kernel learning methods find more accurate models for regression problems on several real-world data sets; US stock data, US house price index data and currency exchange rate data. version:2
arxiv-1602-03950 | General Vector Machine | http://arxiv.org/abs/1602.03950 | id:1602.03950 author:Hong Zhao category:stat.ML cs.LG  published:2016-02-12 summary:The support vector machine (SVM) is an important class of learning machines for function approach, pattern recognition, and time-serious prediction, etc. It maps samples into the feature space by so-called support vectors of selected samples, and then feature vectors are separated by maximum margin hyperplane. The present paper presents the general vector machine (GVM) to replace the SVM. The support vectors are replaced by general project vectors selected from the usual vector space, and a Monte Carlo (MC) algorithm is developed to find the general vectors. The general project vectors improves the feature-extraction ability, and the MC algorithm can control the width of the separation margin of the hyperplane. By controlling the separation margin, we show that the maximum margin hyperplane can usually induce the overlearning, and the best learning machine is achieved with a proper separation margin. Applications in function approach, pattern recognition, and classification indicate that the developed method is very successful, particularly for small-set training problems. Additionally, our algorithm may induce some particular applications, such as for the transductive inference. version:1
arxiv-1602-03935 | Face Attribute Prediction Using Off-The-Shelf Deep Learning Networks | http://arxiv.org/abs/1602.03935 | id:1602.03935 author:Yang Zhong, Josephine Sullivan, Haibo Li category:cs.CV  published:2016-02-12 summary:Attribute prediction from face images in the wild is a challenging problem. To automatically describe face attributes from face containing images, traditionally one needs to cascade three technical blocks --- face localization, facial feature extraction, and classification --- in a pipeline. As a typical classification problem, face attribute prediction has been addressed by using deep learning networks. Current state-of-the-art performance was achieved by using two cascaded CNNs, which were specifically trained to learn face localization and facial attribute prediction. In this paper we experiment in an alternative way of exploring the power of deep representations from the networks: we employ off-the-shelf CNNs trained for face recognition tasks to do facial feature extraction, combined with conventional face localization techniques. Recognizing that the describable face attributes are diverse, we select representations from different levels of the CNNs and investigate their utilities for attribute classification. Experiments on two large datasets, LFWA and CeleA, show that the performance is totally comparable to state-of-the-art approach. Our findings suggest two potentially important questions in using CNNs for face attribute prediction: 1) How to maximally leverage the power of CNN representations. 2) How to best combine traditional computer vision techniques with deep learning networks. version:1
arxiv-1602-03930 | Global Deconvolutional Networks for Semantic Segmentation | http://arxiv.org/abs/1602.03930 | id:1602.03930 author:Vladimir Nekrasov, Janghoon Ju, Jaesik Choi category:cs.CV  published:2016-02-12 summary:Semantic image segmentation is an important low-level computer vision problem aimed to correctly classify each individual pixel of the image. Recent empirical improvements achieved in this area have primarily been motivated by successful exploitation of Convolutional Neural Networks (CNNs) pre-trained for image classification and object recognition tasks. However, the pixel-wise labeling with CNNs has its own unique challenges: (1) an accurate deconvolution, or upsampling, of low-resolution output into a higher-resolution segmentation mask and (2) an inclusion of global information, or context, within locally extracted features. To address these issues, we propose a novel architecture to conduct the deconvolution operation and acquire dense predictions, and an additional refinement, which allows to incorporate global information into the network. We demonstrate that these alterations lead to improved performance of state-of-the-art semantic segmentation models on the PASCAL VOC 2012 benchmark. version:1
arxiv-1602-02410 | Exploring the Limits of Language Modeling | http://arxiv.org/abs/1602.02410 | id:1602.02410 author:Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu category:cs.CL  published:2016-02-07 summary:In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon. version:2
arxiv-1602-03860 | Real-Time Hand Tracking Using a Sum of Anisotropic Gaussians Model | http://arxiv.org/abs/1602.03860 | id:1602.03860 author:Srinath Sridhar, Helge Rhodin, Hans-Peter Seidel, Antti Oulasvirta, Christian Theobalt category:cs.CV  published:2016-02-11 summary:Real-time marker-less hand tracking is of increasing importance in human-computer interaction. Robust and accurate tracking of arbitrary hand motion is a challenging problem due to the many degrees of freedom, frequent self-occlusions, fast motions, and uniform skin color. In this paper, we propose a new approach that tracks the full skeleton motion of the hand from multiple RGB cameras in real-time. The main contributions include a new generative tracking method which employs an implicit hand shape representation based on Sum of Anisotropic Gaussians (SAG), and a pose fitting energy that is smooth and analytically differentiable making fast gradient based pose optimization possible. This shape representation, together with a full perspective projection model, enables more accurate hand modeling than a related baseline method from literature. Our method achieves better accuracy than previous methods and runs at 25 fps. We show these improvements both qualitatively and quantitatively on publicly available datasets. version:1
arxiv-1509-07636 | Validity of time reversal for testing Granger causality | http://arxiv.org/abs/1509.07636 | id:1509.07636 author:Irene Winkler, Danny Panknin, Daniel Bartz, Klaus-Robert Müller, Stefan Haufe category:math.ST stat.ML stat.TH  published:2015-09-25 summary:Inferring causal interactions from observed data is a challenging problem, especially in the presence of measurement noise. To alleviate the problem of spurious causality, Haufe et al. (2013) proposed to contrast measures of information flow obtained on the original data against the same measures obtained on time-reversed data. They show that this procedure, time-reversed Granger causality (TRGC), robustly rejects causal interpretations on mixtures of independent signals. While promising results have been achieved in simulations, it was so far unknown whether time reversal leads to valid measures of information flow in the presence of true interaction. Here we prove that, for linear finite-order autoregressive processes with unidirectional information flow, the application of time reversal for testing Granger causality indeed leads to correct estimates of information flow and its directionality. Using simulations, we further show that TRGC is able to infer correct directionality with similar statistical power as the net Granger causality between two variables, while being much more robust to the presence of measurement noise. version:2
arxiv-1602-03808 | Semi-supervised Learning with Explicit Relationship Regularization | http://arxiv.org/abs/1602.03808 | id:1602.03808 author:Kwang In Kim, James Tompkin, Hanspeter Pfister, Christian Theobalt category:cs.CV cs.LG  published:2016-02-11 summary:In many learning tasks, the structure of the target space of a function holds rich information about the relationships between evaluations of functions on different data points. Existing approaches attempt to exploit this relationship information implicitly by enforcing smoothness on function evaluations only. However, what happens if we explicitly regularize the relationships between function evaluations? Inspired by homophily, we regularize based on a smooth relationship function, either defined from the data or with labels. In experiments, we demonstrate that this significantly improves the performance of state-of-the-art algorithms in semi-supervised classification and in spectral data embedding for constrained clustering and dimensionality reduction. version:1
arxiv-1602-03807 | Bayesian Sparsity for Intractable Distributions | http://arxiv.org/abs/1602.03807 | id:1602.03807 author:John B. Ingraham, Debora S. Marks category:stat.ML cond-mat.dis-nn physics.data-an q-bio.QM  published:2016-02-11 summary:Bayesian approaches for single-variable and group-structured sparsity outperform L1 regularization, but are challenging to apply to large, potentially intractable models. Here we show how noncentered parameterizations, a common trick for improving the efficiency of exact inference in hierarchical models, can similarly improve the accuracy of variational approximations. We develop this with two contributions: First, we introduce Fadeout, an approach for variational inference that uses noncentered parameterizations to capture a posteriori correlations between parameters and hyperparameters. Second, we extend stochastic variational inference to undirected models, enabling efficient hierarchical Bayes without approximations of intractable normalizing constants. We find that this framework substantially improves inferences of undirected graphical models under both sparse and group-sparse priors. version:1
arxiv-1602-03805 | Local High-order Regularization on Data Manifolds | http://arxiv.org/abs/1602.03805 | id:1602.03805 author:Kwang In Kim, James Tompkin, Hanspeter Pfister, Christian Theobalt category:cs.CV  published:2016-02-11 summary:The common graph Laplacian regularizer is well-established in semi-supervised learning and spectral dimensionality reduction. However, as a first-order regularizer, it can lead to degenerate functions in high-dimensional manifolds. The iterated graph Laplacian enables high-order regularization, but it has a high computational complexity and so cannot be applied to large problems. We introduce a new regularizer which is globally high order and so does not suffer from the degeneracy of the graph Laplacian regularizer, but is also sparse for efficient computation in semi-supervised learning applications. We reduce computational complexity by building a local first-order approximation of the manifold as a surrogate geometry, and construct our high-order regularizer based on local derivative evaluations therein. Experiments on human body shape and pose analysis demonstrate the effectiveness and efficiency of our method. version:1
arxiv-1602-03742 | HMM and DTW for evaluation of therapeutical gestures using kinect | http://arxiv.org/abs/1602.03742 | id:1602.03742 author:Carlos Palma, Augusto Salazar, Francisco Vargas category:cs.HC cs.CV  published:2016-02-11 summary:Automatic recognition of the quality of movement in human beings is a challenging task, given the difficulty both in defining the constraints that make a movement correct, and the difficulty in using noisy data to determine if these constraints were satisfied. This paper presents a method for the detection of deviations from the correct form in movements from physical therapy routines based on Hidden Markov Models, which is compared to Dynamic Time Warping. The activities studied include upper an lower limbs movements, the data used comes from a Kinect sensor. Correct repetitions of the activities of interest were recorded, as well as deviations from these correct forms. The ability of the proposed approach to detect these deviations was studied. Results show that a system based on HMM is much more likely to determine if a certain movement has deviated from the specification. version:1
arxiv-1602-03725 | A Versatile Scene Model with Differentiable Visibility Applied to Generative Pose Estimation | http://arxiv.org/abs/1602.03725 | id:1602.03725 author:Helge Rhodin, Nadia Robertini, Christian Richardt, Hans-Peter Seidel, Christian Theobalt category:cs.CV  published:2016-02-11 summary:Generative reconstruction methods compute the 3D configuration (such as pose and/or geometry) of a shape by optimizing the overlap of the projected 3D shape model with images. Proper handling of occlusions is a big challenge, since the visibility function that indicates if a surface point is seen from a camera can often not be formulated in closed form, and is in general discrete and non-differentiable at occlusion boundaries. We present a new scene representation that enables an analytically differentiable closed-form formulation of surface visibility. In contrast to previous methods, this yields smooth, analytically differentiable, and efficient to optimize pose similarity energies with rigorous occlusion handling, fewer local minima, and experimentally verified improved convergence of numerical optimization. The underlying idea is a new image formation model that represents opaque objects by a translucent medium with a smooth Gaussian density distribution which turns visibility into a smooth phenomenon. We demonstrate the advantages of our versatile scene model in several generative pose estimation problems, namely marker-less multi-object pose estimation, marker-less human motion capture with few cameras, and image-based 3D geometry estimation. version:1
arxiv-1508-01774 | An End-to-End Neural Network for Polyphonic Piano Music Transcription | http://arxiv.org/abs/1508.01774 | id:1508.01774 author:Siddharth Sigtia, Emmanouil Benetos, Simon Dixon category:stat.ML cs.LG cs.SD  published:2015-08-07 summary:We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yields the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications. version:2
arxiv-1602-03686 | Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction | http://arxiv.org/abs/1602.03686 | id:1602.03686 author:Edward Choi, Andy Schuetz, Walter F. Stewart, Jimeng Sun category:cs.LG cs.NE  published:2016-02-11 summary:Objective: To transform heterogeneous clinical data from electronic health records into clinically meaningful constructed features using data driven method that rely, in part, on temporal relations among data. Materials and Methods: The clinically meaningful representations of medical concepts and patients are the key for health analytic applications. Most of existing approaches directly construct features mapped to raw data (e.g., ICD or CPT codes), or utilize some ontology mapping such as SNOMED codes. However, none of the existing approaches leverage EHR data directly for learning such concept representation. We propose a new way to represent heterogeneous medical concepts (e.g., diagnoses, medications and procedures) based on co-occurrence patterns in longitudinal electronic health records. The intuition behind the method is to map medical concepts that are co-occuring closely in time to similar concept vectors so that their distance will be small. We also derive a simple method to construct patient vectors from the related medical concept vectors. Results: We evaluate similar medical concepts across diagnosis, medication and procedure. The results show xx% relevancy between similar pairs of medical concepts. Our proposed representation significantly improves the predictive modeling performance for onset of heart failure (HF), where classification methods (e.g. logistic regression, neural network, support vector machine and K-nearest neighbors) achieve up to 23% improvement in area under the ROC curve (AUC) using this proposed representation. Conclusion: We proposed an effective method for patient and medical concept representation learning. The resulting representation can map relevant concepts together and also improves predictive modeling performance. version:1
arxiv-1602-03683 | A Universal Approximation Theorem for Mixture of Experts Models | http://arxiv.org/abs/1602.03683 | id:1602.03683 author:Hien D Nguyen, Luke R Lloyd-Jones, Geoffrey J McLachlan category:stat.ML  published:2016-02-11 summary:The mixture of experts (MoE) model is a popular neural network architecture for nonlinear regression and classification. The class of MoE mean functions is known to be uniformly convergent to any unknown target function, assuming that the target function is from Sobolev space that is sufficiently differentiable and that the domain of estimation is a compact unit hypercube. We provide an alternative result, which shows that the class of MoE mean functions is dense in the class of all continuous functions over arbitrary compact domains of estimation. Our result can be viewed as a universal approximation theorem for MoE models. version:1
arxiv-1602-03681 | Package equivalence in complex software network | http://arxiv.org/abs/1602.03681 | id:1602.03681 author:Tomislav Slijepčević category:cs.SI stat.ML  published:2016-02-11 summary:The public package registry npm is one of the biggest software registry. With its 216 911 software packages, it forms a big network of software dependencies. In this paper we evaluate various methods for finding similar packages in the npm network, using only the structure of the graph. Namely, we want to find a way of categorizing similar packages, which would be useful for recommendation systems. This size enables us to compute meaningful results, as it softened the particularities of the graph. Npm is also quite famous as it is the default package repository of Node.js. We believe that it will make our results interesting for more people than a less used package repository. This makes it a good subject of analysis of software networks. version:1
arxiv-1602-03661 | On the emergence of syntactic structures: quantifying and modelling duality of patterning | http://arxiv.org/abs/1602.03661 | id:1602.03661 author:Vittorio Loreto, Pietro Gravino, Vito D. P. Servedio, Francesca Tria category:physics.soc-ph cs.CL  published:2016-02-11 summary:The complex organization of syntax in hierarchical structures is one of the core design features of human language. Duality of patterning refers for instance to the organization of the meaningful elements in a language at two distinct levels: a combinatorial level where meaningless forms are combined into meaningful forms and a compositional level where meaningful forms are composed into larger lexical units. The question remains wide open regarding how such a structure could have emerged. Furthermore a clear mathematical framework to quantify this phenomenon is still lacking. The aim of this paper is that of addressing these two aspects in a self-consistent way. First, we introduce suitable measures to quantify the level of combinatoriality and compositionality in a language, and present a framework to estimate these observables in human natural languages. Second, we show that the theoretical predictions of a multi-agents modeling scheme, namely the Blending Game, are in surprisingly good agreement with empirical data. In the Blending Game a population of individuals plays language games aiming at success in communication. It is remarkable that the two sides of duality of patterning emerge simultaneously as a consequence of a pure cultural dynamics in a simulated environment that contains meaningful relations, provided a simple constraint on message transmission fidelity is also considered. version:1
arxiv-1602-03647 | On the Difficulty of Selecting Ising Models with Approximate Recovery | http://arxiv.org/abs/1602.03647 | id:1602.03647 author:Jonathan Scarlett, Volkan Cevher category:cs.IT cs.LG cs.SI math.IT stat.ML  published:2016-02-11 summary:In this paper, we consider the problem of estimating the underlying graphical model of an Ising distribution given a number of independent and identically distributed samples. We adopt an \emph{approximate recovery} criterion that allows for a number of missed edges or incorrectly-included edges, thus departing from the extensive literature considering the exact recovery problem. Our main results provide information-theoretic lower bounds on the required number of samples (i.e., the sample complexity) for graph classes imposing constraints on the number of edges, maximal degree, and sparse separation properties. We identify a broad range of scenarios where, either up to constant factors or logarithmic factors, our lower bounds match the best known lower bounds for the exact recovery criterion, several of which are known to be tight or near-tight. Hence, in these cases, we prove that the approximate recovery problem is not much easier than the exact recovery problem. Our bounds are obtained via a modification of Fano's inequality for handling the approximate recovery criterion, along with suitably-designed ensembles of graphs that can broadly be classed into two categories: (i) Those containing graphs that contain several isolated edges or cliques and are thus difficult to distinguish from the empty graph; (ii) Those containing graphs for which certain groups of nodes are highly correlated, thus making it difficult to determine precisely which edges connect them. We support our theoretical results on these ensembles with numerical experiments. version:1
arxiv-1601-04805 | Sparsity in Dynamics of Spontaneous Subtle Emotions: Analysis \& Application | http://arxiv.org/abs/1601.04805 | id:1601.04805 author:Anh Cat Le Ngo, John See, Raphael Chung-Wei Phan category:cs.CV  published:2016-01-19 summary:Spontaneous subtle emotions are expressed through micro-expressions, which are tiny, sudden and short-lived dynamics of facial muscles; thus poses a great challenge for visual recognition. The abrupt but significant dynamics for the recognition task are temporally sparse while the rest, irrelevant dynamics, are temporally redundant. In this work, we analyze and enforce sparsity constrains to learn significant temporal and spectral structures while eliminate irrelevant facial dynamics of micro-expressions, which would ease the challenge in the visual recognition of spontaneous subtle emotions. The hypothesis is confirmed through experimental results of automatic spontaneous subtle emotion recognition with several sparsity levels on CASME II and SMIC, the only two publicly available spontaneous subtle emotion databases. The overall performances of the automatic subtle emotion recognition are boosted when only significant dynamics are preserved from the original sequences. version:2
arxiv-1503-01436 | Class Probability Estimation via Differential Geometric Regularization | http://arxiv.org/abs/1503.01436 | id:1503.01436 author:Qinxun Bai, Steven Rosenberg, Zheng Wu, Stan Sclaroff category:cs.LG cs.CG stat.ML  published:2015-03-04 summary:We study the problem of supervised learning for both binary and multiclass classification from a unified geometric perspective. In particular, we propose a geometric regularization technique to find the submanifold corresponding to a robust estimator of the class probability $P(y \pmb{x})$. The regularization term measures the volume of this submanifold, based on the intuition that overfitting produces rapid local oscillations and hence large volume of the estimator. This technique can be applied to regularize any classification function that satisfies two requirements: firstly, an estimator of the class probability can be obtained; secondly, first and second derivatives of the class probability estimator can be calculated. In experiments, we apply our regularization technique to standard loss functions for classification, our RBF-based implementation compares favorably to widely used regularization methods for both binary and multiclass classification. version:7
arxiv-1602-03265 | Simple Search Algorithms on Semantic Networks Learned from Language Use | http://arxiv.org/abs/1602.03265 | id:1602.03265 author:Aida Nematzadeh, Filip Miscevic, Suzanne Stevenson category:cs.CL  published:2016-02-10 summary:Recent empirical and modeling research has focused on the semantic fluency task because it is informative about semantic memory. An interesting interplay arises between the richness of representations in semantic memory and the complexity of algorithms required to process it. It has remained an open question whether representations of words and their relations learned from language use can enable a simple search algorithm to mimic the observed behavior in the fluency task. Here we show that it is plausible to learn rich representations from naturalistic data for which a very simple search algorithm (a random walk) can replicate the human patterns. We suggest that explicitly structuring knowledge about words into a semantic network plays a crucial role in modeling human behavior in memory search and retrieval; moreover, this is the case across a range of semantic information sources. version:2
arxiv-1602-03609 | Attentive Pooling Networks | http://arxiv.org/abs/1602.03609 | id:1602.03609 author:Cicero dos Santos, Ming Tan, Bing Xiang, Bowen Zhou category:cs.CL cs.LG  published:2016-02-11 summary:In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks. version:1
arxiv-1602-03606 | Variations of the Similarity Function of TextRank for Automated Summarization | http://arxiv.org/abs/1602.03606 | id:1602.03606 author:Federico Barrios, Federico López, Luis Argerich, Rosa Wachenchauzer category:cs.CL cs.IR I.2.7  published:2016-02-11 summary:This article presents new alternatives to the similarity function for the TextRank algorithm for automatic summarization of texts. We describe the generalities of the algorithm and the different functions we propose. Some of these variants achieve a significative improvement using the same metrics and dataset as the original publication. version:1
arxiv-1602-03600 | Data-Driven Online Decision Making with Costly Information Acquisition | http://arxiv.org/abs/1602.03600 | id:1602.03600 author:Onur Atan, William Whoiles, Mihaela van der Schaar category:stat.ML cs.LG  published:2016-02-11 summary:Existing work on online learning for decision making takes the information available as a given and focuses solely on choosing best actions given this information. Instead, in this paper, the decision maker needs to simultaneously learn both what decisions to make and what source(s) of contextual information to gather data from in order to inform its decisions such that its reward is maximized. We propose algorithms that obtain costly source(s) of contextual information over time, while simultaneously learning what actions to take based on the contextual information revealed by the selected source(s). We prove that our algorithms achieve regret that is logarithmic in time. We demonstrate the performance of our algorithms using a medical dataset. The proposed algorithm can be applied in many applications including clinical decision assist systems for medical diagnosis, recommender systems, actionable intelligence etc., where observing the complete information in every instance or consulting all the available sources to gather intelligence before making decisions is costly. version:1
arxiv-1602-03585 | Generating Discriminative Object Proposals via Submodular Ranking | http://arxiv.org/abs/1602.03585 | id:1602.03585 author:Yangmuzi Zhang, Zhuolin Jiang, Xi Chen, Larry S. Davis category:cs.CV  published:2016-02-11 summary:A multi-scale greedy-based object proposal generation approach is presented. Based on the multi-scale nature of objects in images, our approach is built on top of a hierarchical segmentation. We first identify the representative and diverse exemplar clusters within each scale by using a diversity ranking algorithm. Object proposals are obtained by selecting a subset from the multi-scale segment pool via maximizing a submodular objective function, which consists of a weighted coverage term, a single-scale diversity term and a multi-scale reward term. The weighted coverage term forces the selected set of object proposals to be representative and compact; the single-scale diversity term encourages choosing segments from different exemplar clusters so that they will cover as many object patterns as possible; the multi-scale reward term encourages the selected proposals to be discriminative and selected from multiple layers generated by the hierarchical image segmentation. The experimental results on the Berkeley Segmentation Dataset and PASCAL VOC2012 segmentation dataset demonstrate the accuracy and efficiency of our object proposal model. Additionally, we validate our object proposals in simultaneous segmentation and detection and outperform the state-of-art performance. version:1
arxiv-1601-05350 | Disaggregation of SMAP L3 Brightness Temperatures to 9km using Kernel Machines | http://arxiv.org/abs/1601.05350 | id:1601.05350 author:Subit Chakrabarti, Tara Bongiovanni, Jasmeet Judge, Anand Rangarajan, Sanjay Ranka category:cs.CV  published:2016-01-20 summary:In this study, a machine learning algorithm is used for disaggregation of SMAP brightness temperatures (T$_{\textrm{B}}$) from 36km to 9km. It uses image segmentation to cluster the study region based on meteorological and land cover similarity, followed by a support vector machine based regression that computes the value of the disaggregated T$_{\textrm{B}}$ at all pixels. High resolution remote sensing products such as land surface temperature, normalized difference vegetation index, enhanced vegetation index, precipitation, soil texture, and land-cover were used for disaggregation. The algorithm was implemented in Iowa, United States, from April to July 2015, and compared with the SMAP L3_SM_AP T$_{\textrm{B}}$ product at 9km. It was found that the disaggregated T$_{\textrm{B}}$ were very similar to the SMAP-T$_{\textrm{B}}$ product, even for vegetated areas with a mean difference $\leq$ 5K. However, the standard deviation of the disaggregation was lower by 7K than that of the AP product. The probability density functions of the disaggregated T$_{\textrm{B}}$ were similar to the SMAP-T$_{\textrm{B}}$. The results indicate that this algorithm may be used for disaggregating T$_{\textrm{B}}$ using complex non-linear correlations on a grid. version:2
arxiv-1511-06566 | Acceleration of the PDHGM on strongly convex subspaces | http://arxiv.org/abs/1511.06566 | id:1511.06566 author:Tuomo Valkonen, Thomas Pock category:math.OC cs.CV 90C25  49M29  94A08  published:2015-11-20 summary:We propose several variants of the primal-dual method due to Chambolle and Pock. Without requiring full strong convexity of the objective functions, our methods are accelerated on subspaces with strong convexity. This yields mixed rates, $O(1/N^2)$ with respect to initialisation and $O(1/N)$ with respect to the dual sequence, and the residual part of the primal sequence. We demonstrate the efficacy of the proposed methods on image processing problems lacking strong convexity, such as total generalised variation denoising and total variation deblurring. version:2
arxiv-1602-03571 | High Dimensional Inference with Random Maximum A-Posteriori Perturbations | http://arxiv.org/abs/1602.03571 | id:1602.03571 author:Tamir Hazan, Francesco Orabona, Anand D. Sarwate, Subhransu Maji, Tommi Jaakkola category:cs.LG cs.IT math.IT stat.ML  published:2016-02-10 summary:In this work we present a new approach for high-dimensional statistical inference that is based on optimization and random perturbations. This framework injects randomness to maximum a-posteriori (MAP) predictors by randomly perturbing its potential function. When the perturbations are of low dimension, sampling the perturb-max prediction is as efficient as MAP optimization. A classic result from extreme value statistics asserts that perturb-max operations generate unbiased samples from the Gibbs distribution using high-dimensional perturbations. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. In this work we show that the expected value of perturb-max inference with low dimensional perturbations can be used sequentially to generate unbiased samples from the Gibbs distribution. We also show that the expected value of the maximal perturbations is a natural bound on the entropy of such perturb-max models. Finally we describe the measure concentration properties of perturb-max values while showing that the deviation of their sampled average from its expectation decays exponentially in the number of samples. version:1
arxiv-1501-07227 | A Neural Network Anomaly Detector Using the Random Cluster Model | http://arxiv.org/abs/1501.07227 | id:1501.07227 author:Robert A. Murphy category:cs.LG cs.NE stat.ML 60D05  published:2015-01-28 summary:The random cluster model is used to define an upper bound on a distance measure as a function of the number of data points to be classified and the expected value of the number of classes to form in a hybrid K-means and regression classification methodology, with the intent of detecting anomalies. Conditions are given for the identification of classes which contain anomalies and individual anomalies within identified classes. A neural network model describes the decision region-separating surface for offline storage and recall in any new anomaly detection. version:5
arxiv-1503-03488 | Estimating the Mean Number of K-Means Clusters to Form | http://arxiv.org/abs/1503.03488 | id:1503.03488 author:Robert A. Murphy category:cs.LG 60D05  published:2015-03-07 summary:Utilizing the sample size of a dataset, the random cluster model is employed in order to derive an estimate of the mean number of K-Means clusters to form during classification of a dataset. version:2
arxiv-1602-03552 | Learning Privately from Multiparty Data | http://arxiv.org/abs/1602.03552 | id:1602.03552 author:Jihun Hamm, Paul Cao, Mikhail Belkin category:cs.LG cs.CR  published:2016-02-10 summary:Learning a classifier from private data collected by multiple parties is an important problem that has many potential applications. How can we build an accurate and differentially private global classifier by combining locally-trained classifiers from different parties, without access to any party's private data? We propose to transfer the `knowledge' of the local classifier ensemble by first creating labeled data from auxiliary unlabeled data, and then train a global $\epsilon$-differentially private classifier. We show that majority voting is too sensitive and therefore propose a new risk weighted by class probabilities estimated from the ensemble. Relative to a non-private solution, our private solution has a generalization error bounded by $O(\epsilon^{-2}M^{-2})$ where $M$ is the number of parties. This allows strong privacy without performance loss when $M$ is large, such as in crowdsensing applications. We demonstrate the performance of our method with realistic tasks of activity recognition, network intrusion detection, and malicious URL detection. version:1
arxiv-1602-03551 | Knowledge Transfer with Medical Language Embeddings | http://arxiv.org/abs/1602.03551 | id:1602.03551 author:Stephanie L. Hyland, Theofanis Karaletsos, Gunnar Rätsch category:cs.CL stat.AP  published:2016-02-10 summary:Identifying relationships between concepts is a key aspect of scientific knowledge synthesis. Finding these links often requires a researcher to laboriously search through scien- tific papers and databases, as the size of these resources grows ever larger. In this paper we describe how distributional semantics can be used to unify structured knowledge graphs with unstructured text to predict new relationships between medical concepts, using a probabilistic generative model. Our approach is also designed to ameliorate data sparsity and scarcity issues in the medical domain, which make language modelling more challenging. Specifically, we integrate the medical relational database (SemMedDB) with text from electronic health records (EHRs) to perform knowledge graph completion. We further demonstrate the ability of our model to predict relationships between tokens not appearing in the relational database. version:1
arxiv-1512-09300 | Autoencoding beyond pixels using a learned similarity metric | http://arxiv.org/abs/1512.09300 | id:1512.09300 author:Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, Ole Winther category:cs.LG cs.CV stat.ML  published:2015-12-31 summary:We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic. version:2
arxiv-1602-03506 | Research Priorities for Robust and Beneficial Artificial Intelligence | http://arxiv.org/abs/1602.03506 | id:1602.03506 author:Stuart Russell, Daniel Dewey, Max Tegmark category:cs.AI stat.ML  published:2016-02-10 summary:Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial. version:1
arxiv-1601-04770 | Adaptive Image Denoising by Mixture Adaptation | http://arxiv.org/abs/1601.04770 | id:1601.04770 author:Enming Luo, Stanley H. Chan, Truong Q. Nguyen category:cs.CV stat.ME  published:2016-01-19 summary:We propose an adaptive learning procedure to learn effective image priors. The new algorithm, called the Expectation-Maximization (EM) adaptation, takes a generic prior learned from a generic external database and adapts it to the image of interest to generate a specific prior. Different from existing methods which combine internal and external statistics in an ad-hoc way, the proposed algorithm learns a single unified prior through an adaptive process. There are two major contributions in this paper. First, we rigorously derive the EM adaptation algorithm from the Bayesian hyper-prior perspective and show that it can be further simplified to improve the computational complexity. Second, in the absence of the latent clean image, we show how EM adaptation can be modified and applied on pre-filtered images. We discuss how to estimate internal parameters and demonstrate how to improve the denoising performance by running EM adaptation iteratively. Experimental results show that the adapted prior is consistently better than the originally un-adapted prior, and is superior than some state-of-the-art algorithms. version:2
arxiv-1602-03483 | Learning Distributed Representations of Sentences from Unlabelled Data | http://arxiv.org/abs/1602.03483 | id:1602.03483 author:Felix Hill, Kyunghyun Cho, Anna Korhonen category:cs.CL cs.LG  published:2016-02-10 summary:Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance. version:1
arxiv-1602-03481 | Reliable Crowdsourcing under the Generalized Dawid-Skene Model | http://arxiv.org/abs/1602.03481 | id:1602.03481 author:Ashish Khetan, Sewoong Oh category:cs.LG cs.HC cs.SI stat.ML  published:2016-02-10 summary:Crowdsourcing systems provide scalable and cost-effective human-powered solutions at marginal cost, for classification tasks where humans are significantly better than the machines. Although traditional approaches in aggregating crowdsourced labels have relied on the Dawid-Skene model, this fails to capture how some tasks are inherently more difficult than the others. Several generalizations have been proposed, but inference becomes intractable and typical solutions resort to heuristics. To bridge this gap, we study a recently proposed generalize Dawid-Skene model, and propose a linear-time algorithm based on spectral methods. We show near-optimality of the proposed approach, by providing an upper bound on the error and comparing it to a fundamental limit. We provide numerical experiments on synthetic data matching our analyses, and also on real datasets demonstrating that the spectral method significantly improves over simple majority voting and is comparable to other methods. version:1
arxiv-1602-02845 | Online Active Linear Regression via Thresholding | http://arxiv.org/abs/1602.02845 | id:1602.02845 author:Carlos Riquelme, Ramesh Johari, Baosen Zhang category:stat.ML cs.LG  published:2016-02-09 summary:We consider the problem of online active learning to collect data for regression modeling. Specifically, we consider a decision maker that faces a limited experimentation budget but must efficiently learn an underlying linear population model. Our goal is to develop algorithms that provide substantial gains over passive random sampling of observations. To that end, our main contribution is a novel threshold-based algorithm for selection of observations; we characterize its performance and related lower bounds. We also apply our approach successfully to regularized regression. Simulations suggest the algorithm is remarkably robust: it provides significant benefits over passive random sampling even in several real-world datasets that exhibit high nonlinearity and high dimensionality --- significantly reducing the mean and variance of the squared error. version:2
arxiv-1602-03458 | Super-Resolved Retinal Image Mosaicing | http://arxiv.org/abs/1602.03458 | id:1602.03458 author:Thomas Köhler, Axel Heinrich, Andreas Maier, Joachim Hornegger, Ralf P. Tornow category:cs.CV  published:2016-02-10 summary:The acquisition of high-resolution retinal fundus images with a large field of view (FOV) is challenging due to technological, physiological and economic reasons. This paper proposes a fully automatic framework to reconstruct retinal images of high spatial resolution and increased FOV from multiple low-resolution images captured with non-mydriatic, mobile and video-capable but low-cost cameras. Within the scope of one examination, we scan different regions on the retina by exploiting eye motion conducted by a patient guidance. Appropriate views for our mosaicing method are selected based on optic disk tracking to trace eye movements. For each view, one super-resolved image is reconstructed by fusion of multiple video frames. Finally, all super-resolved views are registered to a common reference using a novel polynomial registration scheme and combined by means of image mosaicing. We evaluated our framework for a mobile and low-cost video fundus camera. In our experiments, we reconstructed retinal images of up to 30{\deg} FOV from 10 complementary views of 15{\deg} FOV. An evaluation of the mosaics by human experts as well as a quantitative comparison to conventional color fundus images encourage the clinical usability of our framework. version:1
arxiv-1602-03442 | Stochastic Quasi-Newton Langevin Monte Carlo | http://arxiv.org/abs/1602.03442 | id:1602.03442 author:Umut Şimşekli, Roland Badeau, A. Taylan Cemgil, Gaël Richard category:stat.ML  published:2016-02-10 summary:Recently, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods have been proposed for scaling up Monte Carlo computations to large data problems. Whilst these approaches have proven useful in many applications, vanilla SG-MCMC might suffer from poor mixing rates when random variables exhibit strong couplings under the target densities or big scale differences. In this study, we propose a novel SG-MCMC method that takes the local geometry into account by using ideas from Quasi-Newton optimization methods. These second order methods directly approximate the inverse Hessian by using a limited history of samples and their gradients. Our method uses dense approximations of the inverse Hessian while keeping the time and memory complexities linear with the dimension of the problem. We provide a formal theoretical analysis where we show that the proposed method is asymptotically unbiased and consistent with the posterior expectations. We illustrate the effectiveness of the approach on both synthetic and real datasets. Our experiments on two challenging applications show that our method achieves fast convergence rates similar to Riemannian approaches while at the same time having low computational requirements similar to diagonal preconditioning approaches. version:1
arxiv-1506-01911 | Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video | http://arxiv.org/abs/1506.01911 | id:1506.01911 author:Lionel Pigou, Aäron van den Oord, Sander Dieleman, Mieke Van Herreweghe, Joni Dambre category:cs.CV cs.AI cs.LG cs.NE stat.ML  published:2015-06-05 summary:Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results. version:3
arxiv-1602-03426 | Automatic Sarcasm Detection: A Survey | http://arxiv.org/abs/1602.03426 | id:1602.03426 author:Aditya Joshi, Pushpak Bhattacharyya, Mark James Carman category:cs.CL  published:2016-02-10 summary:Automatic detection of sarcasm has witnessed interest from the sentiment analysis research community. With diverse approaches, datasets and analyses that have been reported, there is an essential need to have a collective understanding of the research in this area. In this survey of automatic sarcasm detection, we describe datasets, approaches (both supervised and rule-based), and trends in sarcasm detection research. We also present a research matrix that summarizes past work, and list pointers to future work. version:1
arxiv-1602-03409 | Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning | http://arxiv.org/abs/1602.03409 | id:1602.03409 author:Hoo-Chang Shin, Holger R. Roth, Mingchen Gao, Le Lu, Ziyue Xu, Isabella Nogues, Jianhua Yao, Daniel Mollura, Ronald M. Summers category:cs.CV  published:2016-02-10 summary:Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and the revival of deep CNN. CNNs enable learning data-driven, highly representative, layered hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, with 85% sensitivity at 3 false positive per patient, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks. version:1
arxiv-1506-00852 | Peer Grading in a Course on Algorithms and Data Structures: Machine Learning Algorithms do not Improve over Simple Baselines | http://arxiv.org/abs/1506.00852 | id:1506.00852 author:Mehdi S. M. Sajjadi, Morteza Alamgir, Ulrike von Luxburg category:cs.LG stat.ML  published:2015-06-02 summary:Peer grading is the process of students reviewing each others' work, such as homework submissions, and has lately become a popular mechanism used in massive open online courses (MOOCs). Intrigued by this idea, we used it in a course on algorithms and data structures at the University of Hamburg. Throughout the whole semester, students repeatedly handed in submissions to exercises, which were then evaluated both by teaching assistants and by a peer grading mechanism, yielding a large dataset of teacher and peer grades. We applied different statistical and machine learning methods to aggregate the peer grades in order to come up with accurate final grades for the submissions (supervised and unsupervised, methods based on numeric scores and ordinal rankings). Surprisingly, none of them improves over the baseline of using the mean peer grade as the final grade. We discuss a number of possible explanations for these results and present a thorough analysis of the generated dataset. version:2
arxiv-1602-03379 | Comparison of feature extraction and dimensionality reduction methods for single channel extracellular spike sorting | http://arxiv.org/abs/1602.03379 | id:1602.03379 author:Anupam Mitra, Anagh Pathak, Kaushik Majumdar category:q-bio.QM cs.CV q-bio.NC  published:2016-02-10 summary:Spikes in the membrane electrical potentials of neurons play a major role in the functioning of nervous systems of animals. Obtaining the spikes from different neurons has been a challenging problem for decades. Several schemes have been proposed for spike sorting to isolate the spikes of individual neurons from electrical recordings in extracellular media. However, there is much scope for improvement in the accuracies obtained using the prevailing methods of spike sorting. To determine more effective spike sorting strategies using well known methods, we compared different types of signal features and techniques for dimensionality reduction in feature space. We tried to determine an optimum or near optimum feature extraction and dimensionality reduction methods and an optimum or near optimum number of features for spike sorting. We assessed relative performance of well known methods on simulated recordings specially designed for development and benchmarking of spike sorting schemes, with varying number of spike classes and the well established method of $k$-means clustering of selected features. We found that almost all well known methods performed quite well. Nevertheless, from spike waveforms of 64 samples, sampled at 24 kHz, using principal component analysis (PCA) to select around 46 to 55 features led to the better spike sorting performance than most other methods (Wilcoxon signed rank sum test, $p < 0.001$). version:1
arxiv-1602-03368 | Fast model selection by limiting SVM training times | http://arxiv.org/abs/1602.03368 | id:1602.03368 author:Aydin Demircioglu, Daniel Horn, Tobias Glasmachers, Bernd Bischl, Claus Weihs category:stat.ML cs.LG  published:2016-02-10 summary:Kernelized Support Vector Machines (SVMs) are among the best performing supervised learning methods. But for optimal predictive performance, time-consuming parameter tuning is crucial, which impedes application. To tackle this problem, the classic model selection procedure based on grid-search and cross-validation was refined, e.g. by data subsampling and direct search heuristics. Here we focus on a different aspect, the stopping criterion for SVM training. We show that by limiting the training time given to the SVM solver during parameter tuning we can reduce model selection times by an order of magnitude. version:1
arxiv-1602-03346 | DAP3D-Net: Where, What and How Actions Occur in Videos? | http://arxiv.org/abs/1602.03346 | id:1602.03346 author:Li Liu, Yi Zhou, Ling Shao category:cs.CV  published:2016-02-10 summary:Action parsing in videos with complex scenes is an interesting but challenging task in computer vision. In this paper, we propose a generic 3D convolutional neural network in a multi-task learning manner for effective Deep Action Parsing (DAP3D-Net) in videos. Particularly, in the training phase, action localization, classification and attributes learning can be jointly optimized on our appearancemotion data via DAP3D-Net. For an upcoming test video, we can describe each individual action in the video simultaneously as: Where the action occurs, What the action is and How the action is performed. To well demonstrate the effectiveness of the proposed DAP3D-Net, we also contribute a new Numerous-category Aligned Synthetic Action dataset, i.e., NASA, which consists of 200; 000 action clips of more than 300 categories and with 33 pre-defined action attributes in two hierarchical levels (i.e., low-level attributes of basic body part movements and high-level attributes related to action motion). We learn DAP3D-Net using the NASA dataset and then evaluate it on our collected Human Action Understanding (HAU) dataset. Experimental results show that our approach can accurately localize, categorize and describe multiple actions in realistic videos. version:1
arxiv-1511-06429 | Patterns for Learning with Side Information | http://arxiv.org/abs/1511.06429 | id:1511.06429 author:Rico Jonschkowski, Sebastian Höfer, Oliver Brock category:cs.LG stat.ML  published:2015-11-19 summary:Supervised, semi-supervised, and unsupervised learning estimate a function given input/output samples. Generalization of the learned function to unseen data can be improved by incorporating side information into learning. Side information are data that are neither from the input space nor from the output space of the function, but include useful information for learning it. In this paper we show that learning with side information subsumes a variety of related approaches, e.g. multi-task learning, multi-view learning and learning using privileged information. Our main contributions are (i) a new perspective that connects these previously isolated approaches, (ii) insights about how these methods incorporate different types of prior knowledge, and hence implement different patterns, (iii) facilitating the application of these methods in novel tasks, as well as (iv) a systematic experimental evaluation of these patterns in two supervised learning tasks. version:5
arxiv-1412-2487 | Word learning under infinite uncertainty | http://arxiv.org/abs/1412.2487 | id:1412.2487 author:Richard A. Blythe, Andrew D. M. Smith, Kenny Smith category:physics.soc-ph cs.CL  published:2014-12-08 summary:Language learners must learn the meanings of many thousands of words, despite those words occurring in complex environments in which infinitely many meanings might be inferred by the learner as a word's true meaning. This problem of infinite referential uncertainty is often attributed to Willard Van Orman Quine. We provide a mathematical formalisation of an ideal cross-situational learner attempting to learn under infinite referential uncertainty, and identify conditions under which word learning is possible. As Quine's intuitions suggest, learning under infinite uncertainty is in fact possible, provided that learners have some means of ranking candidate word meanings in terms of their plausibility; furthermore, our analysis shows that this ranking could in fact be exceedingly weak, implying that constraints which allow learners to infer the plausibility of candidate word meanings could themselves be weak. This approach lifts the burden of explanation from `smart' word learning constraints in learners, and suggests a programme of research into weak, unreliable, probabilistic constraints on the inference of word meaning in real word learners. version:2
arxiv-1602-03308 | Gabor Wavelets in Image Processing | http://arxiv.org/abs/1602.03308 | id:1602.03308 author:David Barina category:cs.CV cs.GR cs.MM  published:2016-02-10 summary:This work shows the use of a two-dimensional Gabor wavelets in image processing. Convolution with such a two-dimensional wavelet can be separated into two series of one-dimensional ones. The key idea of this work is to utilize a Gabor wavelet as a multiscale partial differential operator of a given order. Gabor wavelets are used here to detect edges, corners and blobs. A performance of such an interest point detector is compared to detectors utilizing a Haar wavelet and a derivative of a Gaussian function. The proposed approach may be useful when a fast implementation of the Gabor transform is available or when the transform is already precomputed. version:1
arxiv-1507-05870 | A statistical perspective of sampling scores for linear regression | http://arxiv.org/abs/1507.05870 | id:1507.05870 author:Siheng Chen, Rohan Varma, Aarti Singh, Jelena Kovačević category:stat.ML  published:2015-07-21 summary:In this paper, we consider a statistical problem of learning a linear model from noisy samples. Existing work has focused on approximating the least squares solution by using leverage-based scores as an importance sampling distribution. However, no finite sample statistical guarantees and no computationally efficient optimal sampling strategies have been proposed. To evaluate the statistical properties of different sampling strategies, we propose a simple yet effective estimator, which is easy for theoretical analysis and is useful in multitask linear regression. We derive the exact mean square error of the proposed estimator for any given sampling scores. Based on minimizing the mean square error, we propose the optimal sampling scores for both estimator and predictor, and show that they are influenced by the noise-to-signal ratio. Numerical simulations match the theoretical analysis well. version:2
arxiv-1602-03256 | Improved Eigenfeature Regularization for Face Identification | http://arxiv.org/abs/1602.03256 | id:1602.03256 author:Bappaditya Mandal category:cs.CV  published:2016-02-10 summary:In this work, we propose to divide each class (a person) into subclasses using spatial partition trees which helps in better capturing the intra-personal variances arising from the appearances of the same individual. We perform a comprehensive analysis on within-class and within-subclass eigenspectrums of face images and propose a novel method of eigenspectrum modeling which extracts discriminative features of faces from both within-subclass and total or between-subclass scatter matrices. Effective low-dimensional face discriminative features are extracted for face recognition (FR) after performing discriminant evaluation in the entire eigenspace. Experimental results on popular face databases (AR, FERET) and the challenging unconstrained YouTube Face database show the superiority of our proposed approach on all three databases. version:1
arxiv-1602-03253 | A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model Evaluation | http://arxiv.org/abs/1602.03253 | id:1602.03253 author:Qiang Liu, Jason D. Lee, Michael I. Jordan category:stat.ML  published:2016-02-10 summary:We derive a new discrepancy statistic for measuring differences between two probability distributions based on a novel combination of Stein's method and the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly. version:1
arxiv-1602-01608 | Appearance Based Robot and Human Activity Recognition System | http://arxiv.org/abs/1602.01608 | id:1602.01608 author:Bappaditya Mandal category:cs.RO cs.CV  published:2016-02-04 summary:In this work, we present an appearance based human activity recognition system. It uses background modeling to segment the foreground object and extracts useful discriminative features for representing activities performed by humans and robots. Subspace based method like principal component analysis is used to extract low dimensional features from large voluminous activity images. These low dimensional features are then used to classify an activity. An apparatus is designed using a webcam, which watches a robot replicating a human fall under indoor environment. In this apparatus, a robot performs various activities (like walking, bending, moving arms) replicating humans, which also includes a sudden fall. Experimental results on robot performing various activities and standard human activity recognition databases show the efficacy of our proposed method. version:2
arxiv-1602-02389 | Ensemble Robustness of Deep Learning Algorithms | http://arxiv.org/abs/1602.02389 | id:1602.02389 author:Jiashi Feng, Tom Zahavy, Bingyi Kang, Huan Xu, Shie Mannor category:cs.LG cs.CV stat.ML  published:2016-02-07 summary:The question why deep learning algorithms perform so well in practice has puzzled machine learning theoreticians and practitioners alike. However, most of well-established approaches, such as hypothesis capacity, robustness or sparseness, have not provided complete explanations, due to the high complexity of the deep learning algorithms and their inherent randomness. In this work, we introduce a new approach -- ensemble robustness -- towards characterizing the generalization performance of generic deep learning algorithms. Ensemble robustness concerns robustness of the population of the hypotheses that may be output by a learning algorithm. Through the lens of ensemble robustness, we reveal that a stochastic learning algorithm can generalize well as long as its sensitiveness to adversarial perturbation is bounded in average, or equivalently, the performance variance of the algorithm is small. Quantifying the ensemble robustness of various deep learning algorithms may be difficult analytically. However, extensive simulations for seven common deep learning algorithms for different network architectures provide supporting evidence for our claims. In addition, as an example for utilizing ensemble robustness, we propose a novel semi-supervised learning method that outperforms the state-of-the-art. Furthermore, our work explains the good performance of several published deep learning algorithms. version:2
arxiv-1602-02450 | Loss factorization, weakly supervised learning and label noise robustness | http://arxiv.org/abs/1602.02450 | id:1602.02450 author:Giorgio Patrini, Frank Nielsen, Richard Nock, Marcello Carioni category:cs.LG stat.ML  published:2016-02-08 summary:We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator --the focal quantity of this work-- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation. Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. version:2
arxiv-1602-03205 | Image encryption with dynamic chaotic Look-Up Table | http://arxiv.org/abs/1602.03205 | id:1602.03205 author:Med Karim Abdmouleh, Ali Khalfallah, Med Salim Bouhlel category:cs.CR cs.CV  published:2016-02-09 summary:In this paper we propose a novel image encryption scheme. The proposed method is based on the chaos theory. Our cryptosystem uses the chaos theory to define a dynamic chaotic Look-Up Table (LUT) to compute the new value of the current pixel to cipher. Applying this process on each pixel of the plain image, we generate the encrypted image. The results of different experimental tests, such as Key space analysis, Information Entropy and Histogram analysis, show that the proposed encryption image scheme seems to be protected against various attacks. A comparison between the plain and encrypted image, in terms of correlation coefficient, proves that the plain image is very different from the encrypted one. version:1
arxiv-1511-06392 | Neural Random-Access Machines | http://arxiv.org/abs/1511.06392 | id:1511.06392 author:Karol Kurach, Marcin Andrychowicz, Ilya Sutskever category:cs.LG cs.NE  published:2015-11-19 summary:In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions. version:3
arxiv-1511-06428 | A Controller-Recognizer Framework: How necessary is recognition for control? | http://arxiv.org/abs/1511.06428 | id:1511.06428 author:Marcin Moczulski, Kelvin Xu, Aaron Courville, Kyunghyun Cho category:cs.LG cs.CV  published:2015-11-19 summary:Recently there has been growing interest in building active visual object recognizers, as opposed to the usual passive recognizers which classifies a given static image into a predefined set of object categories. In this paper we propose to generalize these recently proposed end-to-end active visual recognizers into a controller-recognizer framework. A model in the controller-recognizer framework consists of a controller, which interfaces with an external manipulator, and a recognizer which classifies the visual input adjusted by the manipulator. We describe two most recently proposed controller-recognizer models: recurrent attention model and spatial transformer network as representative examples of controller-recognizer models. Based on this description we observe that most existing end-to-end controller-recognizers tightly, or completely, couple a controller and recognizer. We ask a question whether this tight coupling is necessary, and try to answer this empirically by building a controller-recognizer model with a decoupled controller and recognizer. Our experiments revealed that it is not always necessary to tightly couple them and that by decoupling a controller and recognizer, there is a possibility of building a generic controller that is pretrained and works together with any subsequent recognizer. version:4
arxiv-1601-07213 | Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization | http://arxiv.org/abs/1601.07213 | id:1601.07213 author:Alexander G. Ororbia II, C. Lee Giles, Daniel Kifer category:cs.LG cs.NE  published:2016-01-26 summary:We present DataGrad, a general back-propagation style training procedure for deep neural architectures that uses regularization of a deep Jacobian-based penalty. It can be viewed as a deep extension of the layerwise contractive auto-encoder penalty. More importantly, it unifies previous proposals for adversarial training of deep neural nets -- this list includes directly modifying the gradient, training on a mix of original and adversarial examples, using contractive penalties, and approximately optimizing constrained adversarial objective functions. In an experiment using a Deep Sparse Rectifier Network, we find that the deep Jacobian regularization of DataGrad (which also has L1 and L2 flavors of regularization) outperforms traditional L1 and L2 regularization both on the original dataset as well as on adversarial examples. version:2
arxiv-1602-03145 | A New Spatio-Spectral Morphological Segmentation For Multi-Spectral Remote-Sensing Images | http://arxiv.org/abs/1602.03145 | id:1602.03145 author:Guillaume Noyel, Jesus Angulo, Dominique Jeulin category:cs.CV  published:2016-02-09 summary:A general framework of spatio-spectral segmentation for multi-spectral images is introduced in this paper. The method is based on classification-driven stochastic watershed (WS) by Monte Carlo simulations, and it gives more regular and reliable contours than standard WS. The present approach is decomposed into several sequential steps. First, a dimensionality-reduction stage is performed using the factor-correspondence analysis method. In this context, a new way to select the factor axes (eigenvectors) according to their spatial information is introduced. Then, a spectral classification produces a spectral pre-segmentation of the image. Subsequently, a probability density function (pdf) of contours containing spatial and spectral information is estimated by simulation using a stochastic WS approach driven by the spectral classification. The pdf of the contours is finally segmented by a WS controlled by markers from a regularization of the initial classification. version:1
arxiv-1602-03105 | Graphical Model Sketch | http://arxiv.org/abs/1602.03105 | id:1602.03105 author:Branislav Kveton, Hung Bui, Mohammad Ghavamzadeh, Georgios Theocharous, S. Muthukrishnan, Siqi Sun category:cs.DS cs.LG stat.ML  published:2016-02-09 summary:Structured high-cardinality data arises in many domains and poses a major challenge for both modeling and inference, which is beyond current graphical model frameworks. We view these data as a stream $(x^{(t)})_{t = 1}^n$ of $n$ observations from an unknown distribution $P$, where $x^{(t)} \in [M]^K$ is a $K$-dimensional vector and $M$ is the cardinality of its entries, which is very large. Suppose that the graphical model $\mathcal{G}$ of $P$ is known, and let $\bar{P}$ be the maximum-likelihood estimate (MLE) of $P$ from $(x^{(t)})_{t = 1}^n$ conditioned on $\mathcal{G}$. In this work, we design and analyze algorithms that approximate $\bar{P}$ with $\hat{P}$, such that $\hat{P}(x) \approx \bar{P}(x)$ for any $x \in [M]^K$ with a high probability, and crucially in the space independent of $M$. The key idea of our approximations is to use the structure of $\mathcal{G}$ and approximately estimate its factors by "sketches". The sketches hash high-cardinality variables using random projections. Our approximations are computationally and space efficient, being independent of $M$. Our error bounds are multiplicative and provably improve upon those of the count-min (CM) sketch, a state-of-the-art approach to estimating the frequency of values in a stream, in a class of naive Bayes models. We evaluate our algorithms on synthetic and real-world problems, and report an order of magnitude improvements over the CM sketch. version:1
arxiv-1501-04537 | Coupled Depth Learning | http://arxiv.org/abs/1501.04537 | id:1501.04537 author:Mohammad Haris Baig, Lorenzo Torresani category:cs.CV  published:2015-01-19 summary:In this paper we propose a method for estimating depth from a single image using a coarse to fine approach. We argue that modeling the fine depth details is easier after a coarse depth map has been computed. We express a global (coarse) depth map of an image as a linear combination of a depth basis learned from training examples. The depth basis captures spatial and statistical regularities and reduces the problem of global depth estimation to the task of predicting the input-specific coefficients in the linear combination. This is formulated as a regression problem from a holistic representation of the image. Crucially, the depth basis and the regression function are {\bf coupled} and jointly optimized by our learning scheme. We demonstrate that this results in a significant improvement in accuracy compared to direct regression of depth pixel values or approaches learning the depth basis disjointly from the regression function. The global depth estimate is then used as a guidance by a local refinement method that introduces depth details that were not captured at the global level. Experiments on the NYUv2 and KITTI datasets show that our method outperforms the existing state-of-the-art at a considerably lower computational cost for both training and testing. version:6
arxiv-1602-03048 | Bayesian nonparametric image segmentation using a generalized Swendsen-Wang algorithm | http://arxiv.org/abs/1602.03048 | id:1602.03048 author:Richard Yi Da Xu, Francois Caron, Arnaud Doucet category:stat.ML  published:2016-02-09 summary:Unsupervised image segmentation aims at clustering the set of pixels of an image into spatially homogeneous regions. We introduce here a class of Bayesian nonparametric models to address this problem. These models are based on a combination of a Potts-like spatial smoothness component and a prior on partitions which is used to control both the number and size of clusters. This class of models is flexible enough to include the standard Potts model and the more recent Potts-Dirichlet Process model \cite{Orbanz2008}. More importantly, any prior on partitions can be introduced to control the global clustering structure so that it is possible to penalize small or large clusters if necessary. Bayesian computation is carried out using an original generalized Swendsen-Wang algorithm. Experiments demonstrate that our method is competitive in terms of RAND\ index compared to popular image segmentation methods, such as mean-shift, and recent alternative Bayesian nonparametric models. version:1
arxiv-1511-06267 | Asymmetrically Weighted CCA And Hierarchical Kernel Sentence Embedding For Multimodal Retrieval | http://arxiv.org/abs/1511.06267 | id:1511.06267 author:Youssef Mroueh, Etienne Marcheret, Vaibhava Goel category:cs.LG  published:2015-11-19 summary:Joint modeling of language and vision has been drawing increasing interest. A multimodal data representation allowing for bidirectional retrieval of images by sentences and vice versa is a key aspect. In this paper we present three contributions in canonical correlation analysis (CCA) based multimodal retrieval. Firstly, we show that an asymmetric weighting of the canonical weights, while achieving a cross-view mapping from the search to the query space, it improves the retrieval performance. Secondly, we devise a computationally efficient model selection - crucial to generalization and stability - in the framework of the Bjork Golub algorithm for regularized CCA via spectral filtering. Finally, we introduce a Hierarchical Kernel Sentence Embedding (HKSE) that approximates Kernel CCA for a special similarity kernel between words distributions. State of the art results are obtained on MSCOCO and Flickr benchmarks when these three techniques are used in conjunction. version:4
arxiv-1602-03027 | Minimax Lower Bounds for Realizable Transductive Classification | http://arxiv.org/abs/1602.03027 | id:1602.03027 author:Ilya Tolstikhin, David Lopez-Paz category:stat.ML cs.LG  published:2016-02-09 summary:Transductive learning considers a training set of $m$ labeled samples and a test set of $u$ unlabeled samples, with the goal of best labeling that particular test set. Conversely, inductive learning considers a training set of $m$ labeled samples drawn iid from $P(X,Y)$, with the goal of best labeling any future samples drawn iid from $P(X)$. This comparison suggests that transduction is a much easier type of inference than induction, but is this really the case? This paper provides a negative answer to this question, by proving the first known minimax lower bounds for transductive, realizable, binary classification. Our lower bounds show that $m$ should be at least $\Omega(d/\epsilon + \log(1/\delta)/\epsilon)$ when $\epsilon$-learning a concept class $\mathcal{H}$ of finite VC-dimension $d<\infty$ with confidence $1-\delta$, for all $m \leq u$. This result draws three important conclusions. First, general transduction is as hard as general induction, since both problems have $\Omega(d/m)$ minimax values. Second, the use of unlabeled data does not help general transduction, since supervised learning algorithms such as ERM and (Hanneke, 2015) match our transductive lower bounds while ignoring the unlabeled test set. Third, our transductive lower bounds imply lower bounds for semi-supervised learning, which add to the important discussion about the role of unlabeled data in machine learning. version:1
arxiv-1602-02999 | Face Recognition: Perspectives from the Real-World | http://arxiv.org/abs/1602.02999 | id:1602.02999 author:Bappaditya Mandal category:cs.CV  published:2016-02-09 summary:In this paper, we analyze some of our real-world deployment of face recognition (FR) systems for various applications and discuss the gaps between expectations of the user and what the system can deliver. We evaluate some of our proposed algorithms with ad-hoc modifications for applications such as FR on wearable devices (like Google Glass), monitoring of elderly people in senior citizens centers, FR of children in child care centers and face matching between a scanned IC/passport face image and a few live webcam images for automatic hotel/resort checkouts. We describe each of these applications, the challenges involved and proposed solutions. Since FR is intuitive in nature and we human beings use it for interactions with the outside world, people have high expectations of its performance in real-world scenarios. However, we analyze and discuss here that it is not the case, machine recognition of faces for each of these applications poses unique challenges and demands specific research components so as to adapt in the actual sites. version:1
arxiv-1505-02142 | Porting HTM Models to the Heidelberg Neuromorphic Computing Platform | http://arxiv.org/abs/1505.02142 | id:1505.02142 author:Sebastian Billaudelle, Subutai Ahmad category:q-bio.NC cs.NE  published:2015-05-08 summary:Hierarchical Temporal Memory (HTM) is a computational theory of machine intelligence based on a detailed study of the neocortex. The Heidelberg Neuromorphic Computing Platform, developed as part of the Human Brain Project (HBP), is a mixed-signal (analog and digital) large-scale platform for modeling networks of spiking neurons. In this paper we present the first effort in porting HTM networks to this platform. We describe a framework for simulating key HTM operations using spiking network models. We then describe specific spatial pooling and temporal memory implementations, as well as simulations demonstrating that the fundamental properties are maintained. We discuss issues in implementing the full set of plasticity rules using Spike-Timing Dependent Plasticity (STDP), and rough place and route calculations. Although further work is required, our initial studies indicate that it should be possible to run large-scale HTM networks (including plasticity rules) efficiently on the Heidelberg platform. More generally the exercise of porting high level HTM algorithms to biophysical neuron models promises to be a fruitful area of investigation for future studies. version:2
arxiv-1602-02950 | Spoofing detection under noisy conditions: a preliminary investigation and an initial database | http://arxiv.org/abs/1602.02950 | id:1602.02950 author:Xiaohai Tian, Zhizheng Wu, Xiong Xiao, Eng Siong Chng, Haizhou Li category:cs.LG cs.SD  published:2016-02-09 summary:Spoofing detection for automatic speaker verification (ASV), which is to discriminate between live speech and attacks, has received increasing attentions recently. However, all the previous studies have been done on the clean data without significant additive noise. To simulate the real-life scenarios, we perform a preliminary investigation of spoofing detection under additive noisy conditions, and also describe an initial database for this task. The noisy database is based on the ASVspoof challenge 2015 database and generated by artificially adding background noises at different signal-to-noise ratios (SNRs). Five different additive noises are included. Our preliminary results show that using the model trained from clean data, the system performance degrades significantly in noisy conditions. Phase-based feature is more noise robust than magnitude-based features. And the systems perform significantly differ under different noise scenarios. version:1
arxiv-1507-05333 | Causal Transfer in Machine Learning | http://arxiv.org/abs/1507.05333 | id:1507.05333 author:Mateo Rojas-Carulla, Bernhard Schölkopf, Richard Turner, Jonas Peters category:stat.ML  published:2015-07-19 summary:Methods of domain adaptation try to combine knowledge from several related domains (or tasks) to improve performance on a test domain. Inspired by causal methodology, we assume that the covariate shift assumption holds true for a subset of predictor variables: the conditional of the target variable given this subset of predictors is invariant over all tasks. We prove that in an adversarial setting using this subset for prediction is optimal if no examples from the test task are observed. For a specific scenario, in which tasks are drawn from a meta distribution, further optimality results are available. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set. version:2
arxiv-1602-02938 | Challenges of Integrating A Priori Information Efficiently in the Discovery of Spatio-Temporal Objects in Large Databases | http://arxiv.org/abs/1602.02938 | id:1602.02938 author:Benjamin Schott, Johannes Stegmaier, Masanari Takamiya, Ralf Mikut category:cs.CV  published:2016-02-09 summary:Using the knowledge discovery framework, it is possible to explore object databases and extract groups of objects with highly heterogeneous movement behavior by efficiently integrating a priori knowledge through interacting with the framework. The whole process is modular expandable and is therefore adaptive to any problem formulation. Further, the flexible use of different information allocation processes reveal a great potential to efficiently incorporate the a priori knowledge of different users in different ways. Therefore, the stepwise knowledge discovery process embedded in the knowledge discovery framework is described in detail to point out the flexibility of such a system incorporating object databases from different applications. The described framework can be used to gain knowledge out of object databases in many different fields. This knowledge can be used to gain further insights and improve the understanding of underlying phenomena. The functionality of the proposed framework is exemplarily demonstrated using a benchmark database based on real biological object data. version:1
arxiv-1405-6070 | Empirical Bayes Estimation for the Stochastic Blockmodel | http://arxiv.org/abs/1405.6070 | id:1405.6070 author:Shakira Suwan, Dominic S. Lee, Runze Tang, Daniel L. Sussman, Minh Tang, Carey E. Priebe category:stat.ME stat.ML  published:2014-05-23 summary:Inference for the stochastic blockmodel is currently of burgeoning interest in the statistical community, as well as in various application domains as diverse as social networks, citation networks, brain connectivity networks (connectomics), etc. Recent theoretical developments have shown that spectral embedding of graphs yields tractable distributional results; in particular, a random dot product latent position graph formulation of the stochastic blockmodel informs a mixture of normal distributions for the adjacency spectral embedding. We employ this new theory to provide an empirical Bayes methodology for estimation of block memberships of vertices in a random graph drawn from the stochastic blockmodel, and demonstrate its practical utility. The posterior inference is conducted using a Metropolis-within-Gibbs algorithm. The theory and methods are illustrated through Monte Carlo simulation studies, both within the stochastic blockmodel and beyond, and experimental results on a Wikipedia data set are presented. version:3
arxiv-1602-02915 | Calculus of the exponent of Kurdyka-Łojasiewicz inequality and its applications to linear convergence of first-order methods | http://arxiv.org/abs/1602.02915 | id:1602.02915 author:Guoyin Li, Ting Kei Pong category:math.OC stat.ML  published:2016-02-09 summary:In this paper, we study the Kurdyka-{\L}ojasiewicz (KL) exponent, an important quantity for analyzing the convergence rate of first-order methods. Specifically, we develop various calculus rules to deduce the KL exponent of new (possibly nonconvex and nonsmooth) functions formed from functions with known KL exponents. In addition, we show that the well-studied Luo-Tseng error bound together with a mild assumption on the separation of stationary values implies that the KL exponent is $\frac{1}{2}$. The Luo-Tseng error bound is known to hold for a large class of concrete structured optimization problems, and thus we deduce the KL exponent of a large class of functions whose exponents were previously unknown. Building upon this and the calculus rules, we are then able to show that for many convex or nonconvex optimization models for applications, such as sparse recovery, their objective function's KL exponent is $\frac{1}{2}$. This includes the least squares problem with smoothly clipped absolute deviation (SCAD) regularization or minimax concave penalty (MCP) regularization and the logistic regression problem with $\ell_1$ regularization. Since many existing local convergence rate analysis for first-order methods in the nonconvex scenario relies on the KL exponent, our results enable us to obtain explicit convergence rate for various first-order methods when they are applied to a large variety of practical optimization models. Finally, we further illustrate how our results can be applied to analyzing the local linear convergence rate of the proximal gradient algorithm and the inertial proximal algorithm for some specific models that arise in sparse recovery. version:1
arxiv-1602-02644 | Generating Images with Perceptual Similarity Metrics based on Deep Networks | http://arxiv.org/abs/1602.02644 | id:1602.02644 author:Alexey Dosovitskiy, Thomas Brox category:cs.LG cs.CV cs.NE  published:2016-02-08 summary:Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images. version:2
arxiv-1602-02899 | Secure Multi-Party Computation Based Privacy Preserving Extreme Learning Machine Algorithm Over Vertically Distributed Data | http://arxiv.org/abs/1602.02899 | id:1602.02899 author:Ferhat Özgür Çatak category:cs.CR cs.LG  published:2016-02-09 summary:Especially in the Big Data era, the usage of different classification methods is increasing day by day. The success of these classification methods depends on the effectiveness of learning methods. Extreme learning machine (ELM) classification algorithm is a relatively new learning method built on feed-forward neural-network. ELM classification algorithm is a simple and fast method that can create a model from high-dimensional data sets. Traditional ELM learning algorithm implicitly assumes complete access to whole data set. This is a major privacy concern in most of cases. Sharing of private data (i.e. medical records) is prevented because of security concerns. In this research, we propose an efficient and secure privacy-preserving learning algorithm for ELM classification over data that is vertically partitioned among several parties. The new learning method preserves the privacy on numerical attributes, builds a classification model without sharing private data without disclosing the data of each party to others. version:1
arxiv-1602-02888 | Robust Ensemble Classifier Combination Based on Noise Removal with One-Class SVM | http://arxiv.org/abs/1602.02888 | id:1602.02888 author:Ferhat Özgür Çatak category:cs.LG  published:2016-02-09 summary:In machine learning area, as the number of labeled input samples becomes very large, it is very difficult to build a classification model because of input data set is not fit in a memory in training phase of the algorithm, therefore, it is necessary to utilize data partitioning to handle overall data set. Bagging and boosting based data partitioning methods have been broadly used in data mining and pattern recognition area. Both of these methods have shown a great possibility for improving classification model performance. This study is concerned with the analysis of data set partitioning with noise removal and its impact on the performance of multiple classifier models. In this study, we propose noise filtering preprocessing at each data set partition to increment classifier model performance. We applied Gini impurity approach to find the best split percentage of noise filter ratio. The filtered sub data set is then used to train individual ensemble models. version:1
arxiv-1602-02887 | Classification with Boosting of Extreme Learning Machine Over Arbitrarily Partitioned Data | http://arxiv.org/abs/1602.02887 | id:1602.02887 author:Ferhat Özgür Çatak category:cs.LG  published:2016-02-09 summary:Machine learning based computational intelligence methods are widely used to analyze large scale data sets in this age of big data. Extracting useful predictive modeling from these types of data sets is a challenging problem due to their high complexity. Analyzing large amount of streaming data that can be leveraged to derive business value is another complex problem to solve. With high levels of data availability (\textit{i.e. Big Data}) automatic classification of them has become an important and complex task. Hence, we explore the power of applying MapReduce based Distributed AdaBoosting of Extreme Learning Machine (ELM) to build a predictive bag of classification models. Accordingly, (i) data set ensembles are created; (ii) ELM algorithm is used to build weak learners (classifier functions); and (iii) builds a strong learner from a set of weak learners. We applied this training model to the benchmark knowledge discovery and data mining data sets. version:1
arxiv-1602-02885 | Joint Defogging and Demosaicking | http://arxiv.org/abs/1602.02885 | id:1602.02885 author:Y. J. Lee, K. Hirakawa, T. Q. Nguyen category:cs.CV  published:2016-02-09 summary:Image defogging is a technique used extensively for enhancing visual quality of images in bad weather condition. Even though defogging algorithms have been well studied, defogging performance is degraded by demosaicking artifacts and sensor noise amplification in distant scenes. In order to improve visual quality of restored images, we propose a novel approach to perform defogging and demosaicking simultaneously. We conclude that better defogging performance with fewer artifacts can be achieved when a defogging algorithm is combined with a demosaicking algorithm simultaneously. We also demonstrate that the proposed joint algorithm has the benefit of suppressing noise amplification in distant scene. In addition, we validate our theoretical analysis and observations for both synthesized datasets with ground truth fog-free images and natural scene datasets captured in a raw format. version:1
arxiv-1602-02881 | Detection and Visualization of Endoleaks in CT Data for Monitoring of Thoracic and Abdominal Aortic Aneurysm Stents | http://arxiv.org/abs/1602.02881 | id:1602.02881 author:Jing Lu, Jan Egger, Andreas Wimmer, Stefan Großkopf, Bernd Freisleben category:cs.CV cs.CG cs.GR  published:2016-02-09 summary:In this paper we present an efficient algorithm for the segmentation of the inner and outer boundary of thoratic and abdominal aortic aneurysms (TAA & AAA) in computed tomography angiography (CTA) acquisitions. The aneurysm segmentation includes two steps: first, the inner boundary is segmented based on a grey level model with two thresholds; then, an adapted active contour model approach is applied to the more complicated outer boundary segmentation, with its initialization based on the available inner boundary segmentation. An opacity image, which aims at enhancing important features while reducing spurious structures, is calculated from the CTA images and employed to guide the deformation of the model. In addition, the active contour model is extended by a constraint force that prevents intersections of the inner and outer boundary and keeps the outer boundary at a distance, given by the thrombus thickness, to the inner boundary. Based upon the segmentation results, we can measure the aneurysm size at each centerline point on the centerline orthogonal multiplanar reformatting (MPR) plane. Furthermore, a 3D TAA or AAA model is reconstructed from the set of segmented contours, and the presence of endoleaks is detected and highlighted. The implemented method has been evaluated on nine clinical CTA data sets with variations in anatomy and location of the pathology and has shown promising results. version:1
arxiv-1602-02865 | The Role of Typicality in Object Classification: Improving The Generalization Capacity of Convolutional Neural Networks | http://arxiv.org/abs/1602.02865 | id:1602.02865 author:Babak Saleh, Ahmed Elgammal, Jacob Feldman category:cs.CV cs.LG cs.NE  published:2016-02-09 summary:Deep artificial neural networks have made remarkable progress in different tasks in the field of computer vision. However, the empirical analysis of these models and investigation of their failure cases has received attention recently. In this work, we show that deep learning models cannot generalize to atypical images that are substantially different from training images. This is in contrast to the superior generalization ability of the visual system in the human brain. We focus on Convolutional Neural Networks (CNN) as the state-of-the-art models in object recognition and classification; investigate this problem in more detail, and hypothesize that training CNN models suffer from unstructured loss minimization. We propose computational models to improve the generalization capacity of CNNs by considering how typical a training image looks like. By conducting an extensive set of experiments we show that involving a typicality measure can improve the classification results on a new set of images by a large margin. More importantly, this significant improvement is achieved without fine-tuning the CNN model on the target image set. version:1
arxiv-1602-02862 | A Feature-Based Prediction Model of Algorithm Selection for Constrained Continuous Optimisation | http://arxiv.org/abs/1602.02862 | id:1602.02862 author:Shayan Poursoltan, Frank Neumann category:cs.NE  published:2016-02-09 summary:With this paper, we contribute to the growing research area of feature-based analysis of bio-inspired computing. In this research area, problem instances are classified according to different features of the underlying problem in terms of their difficulty of being solved by a particular algorithm. We investigate the impact of different sets of evolved instances for building prediction models in the area of algorithm selection. Building on the work of Poursoltan and Neumann [11,10], we consider how evolved instances can be used to predict the best performing algorithm for constrained continuous optimisation from a set of bio-inspired computing methods, namely high performing variants of differential evolution, particle swarm optimization, and evolution strategies. Our experimental results show that instances evolved with a multi-objective approach in combination with random instances of the underlying problem allow to build a model that accurately predicts the best performing algorithm for a wide range of problem instances. version:1
arxiv-1602-02852 | Compliance-Aware Bandits | http://arxiv.org/abs/1602.02852 | id:1602.02852 author:Nicolás Della Penna, Mark D. Reid, David Balduzzi category:stat.ML cs.LG  published:2016-02-09 summary:Motivated by clinical trials, we study bandits with observable non-compliance. At each step, the learner chooses an arm, after, instead of observing only the reward, it also observes the action that took place. We show that such noncompliance can be helpful or hurtful to the learner in general. Unfortunately, naively incorporating compliance information into bandit algorithms loses guarantees on sublinear regret. We present hybrid algorithms that maintain regret bounds up to a multiplicative factor and can incorporate compliance information. Simulations based on real data from the International Stoke Trial show the practical potential of these algorithms. version:1
arxiv-1602-02850 | Toward Optimal Feature Selection in Naive Bayes for Text Categorization | http://arxiv.org/abs/1602.02850 | id:1602.02850 author:Bo Tang, Steven Kay, Haibo He category:stat.ML cs.CL cs.IR cs.LG  published:2016-02-09 summary:Automated feature selection is important for text categorization to reduce the feature size and to speed up the learning process of classifiers. In this paper, we present a novel and efficient feature selection framework based on the Information Theory, which aims to rank the features with their discriminative capacity for classification. We first revisit two information measures: Kullback-Leibler divergence and Jeffreys divergence for binary hypothesis testing, and analyze their asymptotic properties relating to type I and type II errors of a Bayesian classifier. We then introduce a new divergence measure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure multi-distribution divergence for multi-class classification. Based on the JMH-divergence, we develop two efficient feature selection methods, termed maximum discrimination ($MD$) and $MD-\chi^2$ methods, for text categorization. The promising results of extensive experiments demonstrate the effectiveness of the proposed approaches. version:1
arxiv-1602-02842 | Collaborative filtering via sparse Markov random fields | http://arxiv.org/abs/1602.02842 | id:1602.02842 author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.IR cs.LG  published:2016-02-09 summary:Recommender systems play a central role in providing individualized access to information and services. This paper focuses on collaborative filtering, an approach that exploits the shared structure among mind-liked users and similar items. In particular, we focus on a formal probabilistic framework known as Markov random fields (MRF). We address the open problem of structure learning and introduce a sparsity-inducing algorithm to automatically estimate the interaction structures between users and between items. Item-item and user-user correlation networks are obtained as a by-product. Large-scale experiments on movie recommendation and date matching datasets demonstrate the power of the proposed method. version:1
arxiv-1509-01228 | Machine Learning Model of the Swift/BAT Trigger Algorithm for Long GRB Population Studies | http://arxiv.org/abs/1509.01228 | id:1509.01228 author:Philip B Graff, Amy Y Lien, John G Baker, Takanori Sakamoto category:astro-ph.HE physics.data-an stat.ML  published:2015-09-03 summary:To draw inferences about gamma-ray burst (GRB) source populations based on Swift observations, it is essential to understand the detection efficiency of the Swift burst alert telescope (BAT). This study considers the problem of modeling the Swift/BAT triggering algorithm for long GRBs, a computationally expensive procedure, and models it using machine learning algorithms. A large sample of simulated GRBs from Lien 2014 is used to train various models: random forests, boosted decision trees (with AdaBoost), support vector machines, and artificial neural networks. The best models have accuracies of $\gtrsim97\%$ ($\lesssim 3\%$ error), which is a significant improvement on a cut in GRB flux which has an accuracy of $89.6\%$ ($10.4\%$ error). These models are then used to measure the detection efficiency of Swift as a function of redshift $z$, which is used to perform Bayesian parameter estimation on the GRB rate distribution. We find a local GRB rate density of $n_0 \sim 0.48^{+0.41}_{-0.23} \ {\rm Gpc}^{-3} {\rm yr}^{-1}$ with power-law indices of $n_1 \sim 1.7^{+0.6}_{-0.5}$ and $n_2 \sim -5.9^{+5.7}_{-0.1}$ for GRBs above and below a break point of $z_1 \sim 6.8^{+2.8}_{-3.2}$. This methodology is able to improve upon earlier studies by more accurately modeling Swift detection and using this for fully Bayesian model fitting. The code used in this is analysis is publicly available online (https://github.com/PBGraff/SwiftGRB_PEanalysis). version:2
arxiv-1602-02823 | Poor starting points in machine learning | http://arxiv.org/abs/1602.02823 | id:1602.02823 author:Mark Tygert category:cs.LG cs.NE math.OC stat.ML  published:2016-02-09 summary:Poor (even random) starting points for learning/training/optimization are common in machine learning. In many settings, the method of Robbins and Monro (online stochastic gradient descent) is known to be optimal for good starting points, but may not be optimal for poor starting points -- indeed, for poor starting points Nesterov acceleration can help during the initial iterations, even though Nesterov methods not designed for stochastic approximation could hurt during later iterations. The common practice of training with nontrivial minibatches enhances the advantage of Nesterov acceleration. version:1
arxiv-1602-02822 | Parameterizing Region Covariance: An Efficient Way To Apply Sparse Codes On Second Order Statistics | http://arxiv.org/abs/1602.02822 | id:1602.02822 author:Xiyang Dai, Sameh Khamis, Yangmuzi Zhang, Larry S. Davis category:cs.CV  published:2016-02-09 summary:Sparse representations have been successfully applied to signal processing, computer vision and machine learning. Currently there is a trend to learn sparse models directly on structure data, such as region covariance. However, such methods when combined with region covariance often require complex computation. We present an approach to transform a structured sparse model learning problem to a traditional vectorized sparse modeling problem by constructing a Euclidean space representation for region covariance matrices. Our new representation has multiple advantages. Experiments on several vision tasks demonstrate competitive performance with the state-of-the-art methods. version:1
arxiv-1407-6267 | Learning in games via reinforcement and regularization | http://arxiv.org/abs/1407.6267 | id:1407.6267 author:Panayotis Mertikopoulos, William H. Sandholm category:math.OC cs.GT cs.LG  published:2014-07-23 summary:We investigate a class of reinforcement learning dynamics where players adjust their strategies based on their actions' cumulative payoffs over time - specifically, by playing mixed strategies that maximize their expected cumulative payoff minus a regularization term. A widely studied example is exponential reinforcement learning, a process induced by an entropic regularization term which leads mixed strategies to evolve according to the replicator dynamics. However, in contrast to the class of regularization functions used to define smooth best responses in models of stochastic fictitious play, the functions used in this paper need not be infinitely steep at the boundary of the simplex; in fact, dropping this requirement gives rise to an important dichotomy between steep and nonsteep cases. In this general framework, we extend several properties of exponential learning, including the elimination of dominated strategies, the asymptotic stability of strict Nash equilibria, and the convergence of time-averaged trajectories in zero-sum games with an interior Nash equilibrium. version:2
arxiv-1602-02726 | Local and Global Convergence of a General Inertial Proximal Splitting Scheme | http://arxiv.org/abs/1602.02726 | id:1602.02726 author:Patrick R. Johnstone, Pierre Moulin category:math.OC cs.LG math.NA  published:2016-02-08 summary:This paper is concerned with convex composite minimization problems in a Hilbert space. In these problems, the objective is the sum of two closed, proper, and convex functions where one is smooth and the other admits a computationally inexpensive proximal operator. We analyze a general family of inertial proximal splitting algorithms (GIPSA) for solving such problems. We establish finiteness of the sum of squared increments of the iterates and optimality of the accumulation points. Weak convergence of the entire sequence then follows if the minimum is attained. Our analysis unifies and extends several previous results. We then focus on $\ell_1$-regularized optimization, which is the ubiquitous special case where the nonsmooth term is the $\ell_1$-norm. For certain parameter choices, GIPSA is amenable to a local analysis for this problem. For these choices we show that GIPSA achieves finite "active manifold identification", i.e. convergence in a finite number of iterations to the optimal support and sign, after which GIPSA reduces to minimizing a local smooth function. Local linear convergence then holds under certain conditions. We determine the rate in terms of the inertia, stepsize, and local curvature. Our local analysis is applicable to certain recent variants of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), for which we establish active manifold identification and local linear convergence. Our analysis motivates the use of a momentum restart scheme in these FISTA variants to obtain the optimal local linear convergence rate. version:1
arxiv-1602-02701 | Compressed Online Dictionary Learning for Fast fMRI Decomposition | http://arxiv.org/abs/1602.02701 | id:1602.02701 author:Arthur Mensch, Gaël Varoquaux, Bertrand Thirion category:stat.ML cs.LG  published:2016-02-08 summary:We present a method for fast resting-state fMRI spatial decomposi-tions of very large datasets, based on the reduction of the temporal dimension before applying dictionary learning on concatenated individual records from groups of subjects. Introducing a measure of correspondence between spatial decompositions of rest fMRI, we demonstrates that time-reduced dictionary learning produces result as reliable as non-reduced decompositions. We also show that this reduction significantly improves computational scalability. version:1
arxiv-1602-02685 | Predicting Clinical Events by Combining Static and Dynamic Information Using Recurrent Neural Networks | http://arxiv.org/abs/1602.02685 | id:1602.02685 author:Cristóbal Esteban, Oliver Staeck, Yinchong Yang, Volker Tresp category:cs.LG cs.AI cs.NE  published:2016-02-08 summary:In clinical data sets we often find static information (e.g. gender of the patients, blood type, etc.) combined with sequences of data that are recorded during multiple hospital visits (e.g. medications prescribed, tests performed, etc.). Recurrent Neural Networks (RNNs) have proven to be very successful for modelling sequences of data in many areas of Machine Learning. In this work we present an approach based on RNNs that is specifically designed for the clinical domain and that combines static and dynamic information in order to predict future events. We work with a database collected in the Charit\'{e} Hospital in Berlin that contains all the information concerning patients that underwent a kidney transplantation. After the transplantation three main endpoints can occur: rejection of the kidney, loss of the kidney and death of the patient. Our goal is to predict, given the Electronic Health Record of each patient, whether any of those endpoints will occur within the next six or twelve months after each visit to the clinic. We compared different types of RNNs that we developed for this work, a model based on a Feedforward Neural Network and a Logistic Regression model. We found that the RNN that we developed based on Gated Recurrent Units provides the best performance for this task. We also performed an additional experiment using these models to predict next actions and found that for such use case the model based on a Feedforward Neural Network outperformed the other models. Our hypothesis is that long-term dependencies are not as relevant in this task. version:1
arxiv-1602-02672 | Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks | http://arxiv.org/abs/1602.02672 | id:1602.02672 author:Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, Shimon Whiteson category:cs.AI cs.LG  published:2016-02-08 summary:We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success. version:1
