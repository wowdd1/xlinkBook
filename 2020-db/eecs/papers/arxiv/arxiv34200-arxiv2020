arxiv-1710-00629 | Adaptive Smoothing in fMRI Data Processing Neural Networks | http://arxiv.org/abs/1710.00629 | id:1710.00629 author:Albert Vilamala, Kristoffer Hougaard Madsen, Lars Kai Hansen category:cs.CV stat.ML  published:2017-10-02 summary:Functional Magnetic Resonance Imaging (fMRI) relies on multi-step data processing pipelines to accurately determine brain activity; among them, the crucial step of spatial smoothing. These pipelines are commonly suboptimal, given the local optimisation strategy they use, treating each step in isolation. With the advent of new tools for deep learning, recent work has proposed to turn these pipelines into end-to-end learning networks. This change of paradigm offers new avenues to improvement as it allows for a global optimisation. The current work aims at benefitting from this paradigm shift by defining a smoothing step as a layer in these networks able to adaptively modulate the degree of smoothing required by each brain volume to better accomplish a given data analysis task. The viability is evaluated on real fMRI data where subjects did alternate between left and right finger tapping tasks. version:1
arxiv-1710-00620 | Out-of-focus Blur: Image De-blurring | http://arxiv.org/abs/1710.00620 | id:1710.00620 author:Yuzhen Lu category:cs.CV cs.NA  published:2017-10-02 summary:Image de-blurring is important in many cases of imaging a real scene or object by a camera. This project focuses on de-blurring an image distorted by an out-of-focus blur through a simulation study. A pseudo-inverse filter is first explored but it fails because of severe noise amplification. Then Tikhonov regularization methods are employed, which produce greatly improved results compared to the pseudo-inverse filter. In Tikhonov regularization, the choice of the regularization parameter plays a critical rule in obtaining a high-quality image, and the regularized solutions possess a semi-convergence property. The best result, with the relative restoration error of 8.49%, is achieved when the prescribed discrepancy principle is used to decide an optimal value. Furthermore, an iterative method, Conjugated Gradient, is employed for image de-blurring, which is fast in computation and leads to an even better result with the relative restoration error of 8.22%. The number of iteration in CG acts as a regularization parameter, and the iterates have a semi-convergence property as well. version:1
arxiv-1710-00598 | Lasso Regularization Paths for NARMAX Models via Coordinate Descent | http://arxiv.org/abs/1710.00598 | id:1710.00598 author:Antônio H. Ribeiro, Luis A. Aguirre category:cs.SY stat.ML  published:2017-10-02 summary:We propose a new algorithm for estimating NARMAX models with L1 regularization for models represented as a linear combination of basis functions. Due to the L1-norm penalty the Lasso estimation tends to produce some coefficients that are exactly zero and hence gives interpretable models. The proposed algorithm uses cyclical coordinate descent to compute the parameters of the NARMAX models for the entire regularization path and, to the best of the authors knowledge, it is first the algorithm to allow the inclusion of error regressors in the Lasso estimation. This is made possible by updating the regressor matrix along with the parameter vector. In comparative timings we find that this modification does not harm the global efficiency of the algorithm and can provide the most important regressors in very few inexpensive iterations. The method is illustrated for linear and polynomial models by means of two examples. version:1
arxiv-1710-00578 | sgmcmc: An R Package for Stochastic Gradient Markov Chain Monte Carlo | http://arxiv.org/abs/1710.00578 | id:1710.00578 author:Jack Baker, Paul Fearnhead, Emily B. Fox, Christopher Nemeth category:stat.CO stat.AP stat.ML  published:2017-10-02 summary:This paper introduces the R package sgmcmc; which can be used for Bayesian inference on problems with large datasets using stochastic gradient Markov chain Monte Carlo (SGMCMC). Traditional Markov chain Monte Carlo (MCMC) methods, such as Metropolis-Hastings, are known to run prohibitively slowly as the dataset size increases. SGMCMC solves this issue by only using a subset of data at each iteration. SGMCMC requires calculating gradients of the log likelihood and log priors, which can be time consuming and error prone to perform by hand. The sgmcmc package calculates these gradients itself using automatic differentiation, making the implementation of these methods much easier. To do this, the package uses the software library TensorFlow, which has a variety of statistical distributions and mathematical operations as standard, meaning a wide class of models can be built using this framework. SGMCMC has become widely adopted in the machine learning literature, but less so in the statistics community. We believe this may be partly due to lack of software; this package aims to bridge this gap. version:1
arxiv-1708-01547 | Lifelong Learning with Dynamically Expandable Networks | http://arxiv.org/abs/1708.01547 | id:1708.01547 author:Jaehong Yoon, Eunho Yang, Sung ju Hwang category:cs.LG I.2.6  I.2.10  published:2017-08-04 summary:We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. version:6
arxiv-1710-00569 | Learning Predictive Leading Indicators for Forecasting Time Series Systems with Unknown Clusters of Forecast Tasks | http://arxiv.org/abs/1710.00569 | id:1710.00569 author:Magda Gregorova, Alexandros Kalousis, Stephane Marchand-Maillet category:stat.ML  published:2017-10-02 summary:We present a new method for forecasting systems of multiple interrelated time series. The method learns the forecast models together with discovering leading indicators from within the system that serve as good predictors improving the forecast accuracy and a cluster structure of the predictive tasks around these. The method is based on the classical linear vector autoregressive model (VAR) and links the discovery of the leading indicators to inferring sparse graphs of Granger causality. We formulate a new constrained optimisation problem to promote the desired sparse structures across the models and the sharing of information amongst the learning tasks in a multi-task manner. We propose an algorithm for solving the problem and document on a battery of synthetic and real-data experiments the advantages of our new method over baseline VAR models as well as the state-of-the-art sparse VAR learning methods. version:1
arxiv-1710-00568 | Indirect Match Highlights Detection with Deep Convolutional Neural Networks | http://arxiv.org/abs/1710.00568 | id:1710.00568 author:Marco Godi, Paolo Rota, Francesco Setti category:cs.CV  published:2017-10-02 summary:Highlights in a sport video are usually referred as actions that stimulate excitement or attract attention of the audience. A big effort is spent in designing techniques which find automatically highlights, in order to automatize the otherwise manual editing process. Most of the state-of-the-art approaches try to solve the problem by training a classifier using the information extracted on the tv-like framing of players playing on the game pitch, learning to detect game actions which are labeled by human observers according to their perception of highlight. Obviously, this is a long and expensive work. In this paper, we reverse the paradigm: instead of looking at the gameplay, inferring what could be exciting for the audience, we directly analyze the audience behavior, which we assume is triggered by events happening during the game. We apply deep 3D Convolutional Neural Network (3D-CNN) to extract visual features from cropped video recordings of the supporters that are attending the event. Outputs of the crops belonging to the same frame are then accumulated to produce a value indicating the Highlight Likelihood (HL) which is then used to discriminate between positive (i.e. when a highlight occurs) and negative samples (i.e. standard play or time-outs). Experimental results on a public dataset of ice-hockey matches demonstrate the effectiveness of our method and promote further research in this new exciting direction. version:1
arxiv-1710-00519 | Attentive Convolution | http://arxiv.org/abs/1710.00519 | id:1710.00519 author:Wenpeng Yin, Hinrich Schütze category:cs.CL  published:2017-10-02 summary:In NLP, convolution neural networks (CNNs) have benefited less than recurrent neural networks (RNNs) from attention mechanisms. We hypothesize that this is because attention in CNNs has been mainly implemented as attentive pooling (i.e., it is applied to pooling) rather than as attentive convolution (i.e., it is integrated into convolution). Convolution is the differentiator of CNNs in that it can powerfully model the higher-level representation of a word by taking into account its local fixed-size context in input text $t^x$. In this work, we propose an attentive convolution network, AttentiveConvNet. It extends the context scope of the convolution operation, deriving higher-level features for a word not only from local context, but also from information extracted from nonlocal context by the attention mechanism commonly used in RNNs. This nonlocal context can come (i) from parts of the input text $t^x$ that are distant or (ii) from a second input text, the context text $t^y$. In an evaluation on sentence relation classification (textual entailment and answer sentence selection) and text classification, experiments demonstrate that AttentiveConvNet has state-of-the-art performance and outperforms RNN/CNN variants with and without attention. version:1
arxiv-1710-00517 | Temporal shape super-resolution by intra-frame motion encoding using high-fps structured light | http://arxiv.org/abs/1710.00517 | id:1710.00517 author:Yuki Shiba, Satoshi Ono, Ryo Furukawa, Shinsaku Hiura, Hiroshi Kawasaki category:cs.CV  published:2017-10-02 summary:One of the solutions of depth imaging of moving scene is to project a static pattern on the object and use just a single image for reconstruction. However, if the motion of the object is too fast with respect to the exposure time of the image sensor, patterns on the captured image are blurred and reconstruction fails. In this paper, we impose multiple projection patterns into each single captured image to realize temporal super resolution of the depth image sequences. With our method, multiple patterns are projected onto the object with higher fps than possible with a camera. In this case, the observed pattern varies depending on the depth and motion of the object, so we can extract temporal information of the scene from each single image. The decoding process is realized using a learning-based approach where no geometric calibration is needed. Experiments confirm the effectiveness of our method where sequential shapes are reconstructed from a single image. Both quantitative evaluations and comparisons with recent techniques were also conducted. version:1
arxiv-1710-00513 | Depth estimation using structured light flow -- analysis of projected pattern flow on an object's surface -- | http://arxiv.org/abs/1710.00513 | id:1710.00513 author:Ryo Furukawa, Ryusuke Sagawa, Hiroshi Kawasaki category:cs.CV  published:2017-10-02 summary:Shape reconstruction techniques using structured light have been widely researched and developed due to their robustness, high precision, and density. Because the techniques are based on decoding a pattern to find correspondences, it implicitly requires that the projected patterns be clearly captured by an image sensor, i.e., to avoid defocus and motion blur of the projected pattern. Although intensive researches have been conducted for solving defocus blur, few researches for motion blur and only solution is to capture with extremely fast shutter speed. In this paper, unlike the previous approaches, we actively utilize motion blur, which we refer to as a light flow, to estimate depth. Analysis reveals that minimum two light flows, which are retrieved from two projected patterns on the object, are required for depth estimation. To retrieve two light flows at the same time, two sets of parallel line patterns are illuminated from two video projectors and the size of motion blur of each line is precisely measured. By analyzing the light flows, i.e. lengths of the blurs, scene depth information is estimated. In the experiments, 3D shapes of fast moving objects, which are inevitably captured with motion blur, are successfully reconstructed by our technique. version:1
arxiv-1710-00499 | Online control of the false discovery rate with decaying memory | http://arxiv.org/abs/1710.00499 | id:1710.00499 author:Aaditya Ramdas, Fanny Yang, Martin J. Wainwright, Michael I. Jordan category:stat.ME cs.LG math.ST stat.ML stat.TH  published:2017-10-02 summary:In the online multiple testing problem, p-values corresponding to different null hypotheses are observed one by one, and the decision of whether or not to reject the current hypothesis must be made immediately, after which the next p-value is observed. Alpha-investing algorithms to control the false discovery rate (FDR), formulated by Foster and Stine, have been generalized and applied to many settings, including quality-preserving databases in science and multiple A/B or multi-armed bandit tests for internet commerce. This paper improves the class of generalized alpha-investing algorithms (GAI) in four ways: (a) we show how to uniformly improve the power of the entire class of monotone GAI procedures by awarding more alpha-wealth for each rejection, giving a win-win resolution to a recent dilemma raised by Javanmard and Montanari, (b) we demonstrate how to incorporate prior weights to indicate domain knowledge of which hypotheses are likely to be non-null, (c) we allow for differing penalties for false discoveries to indicate that some hypotheses may be more important than others, (d) we define a new quantity called the decaying memory false discovery rate (mem-FDR) that may be more meaningful for truly temporal applications, and which alleviates problems that we describe and refer to as "piggybacking" and "alpha-death". Our GAI++ algorithms incorporate all four generalizations simultaneously, and reduce to more powerful variants of earlier algorithms when the weights and decay are all set to unity. Finally, we also describe a simple method to derive new online FDR rules based on an estimated false discovery proportion. version:1
arxiv-1710-00486 | DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in Neural Networks | http://arxiv.org/abs/1710.00486 | id:1710.00486 author:Divya Gopinath, Guy Katz, Corina S. Pasareanu, Clark Barrett category:cs.NE cs.LG stat.ML  published:2017-10-02 summary:Deep neural networks have become widely used, obtaining remarkable results in domains such as computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, and bio-informatics, where they have produced results comparable to human experts. However, these networks can be easily fooled by adversarial perturbations: minimal changes to correctly-classified inputs, that cause the network to mis-classify them. This phenomenon represents a concern for both safety and security, but it is currently unclear how to measure a network's robustness against such perturbations. Existing techniques are limited to checking robustness around a few individual input points, providing only very limited guarantees. We propose a novel approach for automatically identifying safe regions of the input space, within which the network is robust against adversarial perturbations. The approach is data-guided, relying on clustering to identify well-defined geometric regions as candidate safe regions. We then utilize verification techniques to confirm that these regions are safe or to provide counter-examples showing that they are not safe. We also introduce the notion of targeted robustness which, for a given target label and region, ensures that a NN does not map any input in the region to the target label. We evaluated our technique on the MNIST dataset and on a neural network implementation of a controller for the next-generation Airborne Collision Avoidance System for unmanned aircraft (ACAS Xu). For these networks, our approach identified multiple regions which were completely safe as well as some which were only safe for specific labels. It also discovered several adversarial perturbations of interest. version:1
arxiv-1710-00482 | Weighted-SVD: Matrix Factorization with Weights on the Latent Factors | http://arxiv.org/abs/1710.00482 | id:1710.00482 author:Hung-Hsuan Chen category:cs.IR cs.LG stat.ML  published:2017-10-02 summary:The Matrix Factorization models, sometimes called the latent factor models, are a family of methods in the recommender system research area to (1) generate the latent factors for the users and the items and (2) predict users' ratings on items based on their latent factors. However, current Matrix Factorization models presume that all the latent factors are equally weighted, which may not always be a reasonable assumption in practice. In this paper, we propose a new model, called Weighted-SVD, to integrate the linear regression model with the SVD model such that each latent factor accompanies with a corresponding weight parameter. This mechanism allows the latent factors have different weights to influence the final ratings. The complexity of the Weighted-SVD model is slightly larger than the SVD model but much smaller than the SVD++ model. We compared the Weighted-SVD model with several latent factor models on five public datasets based on the Root-Mean-Squared-Errors (RMSEs). The results show that the Weighted-SVD model outperforms the baseline methods in all the experimental datasets under almost all settings. version:1
arxiv-1704-07085 | Unified Framework for Automated Person Re-identification and Camera Network Topology Inference in Camera Networks | http://arxiv.org/abs/1704.07085 | id:1704.07085 author:Yeong-Jun Cho, Jae-Han Park, Su-A Kim, Kyuewang Lee, Kuk-Jin Yoon category:cs.CV  published:2017-04-24 summary:Person re-identification in large-scale multi-camera networks is a challenging task because of the spatio-temporal uncertainty and high complexity due to large numbers of cameras and people. To handle these difficulties, additional information such as camera network topology should be provided, which is also difficult to automatically estimate. In this paper, we propose a unified framework which jointly solves both person re-id and camera network topology inference problems. The proposed framework takes general multi-camera network environments into account. To effectively show the superiority of the proposed framework, we also provide a new person re-id dataset with full annotations, named SLP, captured in the synchronized multi-camera network. Experimental results show that the proposed methods are promising for both person re-id and camera topology inference tasks. version:5
arxiv-1710-01163 | Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences | http://arxiv.org/abs/1710.01163 | id:1710.01163 author:Kinjal Basu, Ankan Saha, Shaunak Chatterjee category:stat.ML  published:2017-10-02 summary:We consider the problem of solving a large-scale Quadratically Constrained Quadratic Program. Such problems occur naturally in many scientific and web applications. Although there are efficient methods which tackle this problem, they are mostly not scalable. In this paper, we develop a method that transforms the quadratic constraint into a linear form by sampling a set of low-discrepancy points. The transformed problem can then be solved by applying any state-of-the-art large-scale quadratic programming solvers. We show the convergence of our approximate solution to the true solution as well as some finite sample error bounds. Experimental results are also shown to prove scalability as well as improved quality of approximation in practice. version:1
arxiv-1710-00453 | Visual Reasoning with Natural Language | http://arxiv.org/abs/1710.00453 | id:1710.00453 author:Stephanie Zhou, Alane Suhr, Yoav Artzi category:cs.CL  published:2017-10-02 summary:Natural language provides a widely accessible and expressive interface for robotic agents. To understand language in complex environments, agents must reason about the full range of language inputs and their correspondence to the world. Such reasoning over language and vision is an open problem that is receiving increasing attention. While existing data sets focus on visual diversity, they do not display the full range of natural language expressions, such as counting, set reasoning, and comparisons. We propose a simple task for natural language visual reasoning, where images are paired with descriptive statements. The task is to predict if a statement is true for the given scene. This abstract describes our existing synthetic images corpus and our current work on collecting real vision data. version:1
arxiv-1710-00447 | An Estimation-Theoretic View of Privacy | http://arxiv.org/abs/1710.00447 | id:1710.00447 author:Hao Wang, Flavio P. Calmon category:cs.IT cs.LG math.IT  published:2017-10-02 summary:We study the central problem in data privacy: how to share data with an analyst while providing both privacy and utility guarantees to the user that owns the data. We present an estimation-theoretic analysis of the privacy-utility trade-off (PUT) in this setting. Here, an analyst is allowed to reconstruct (in a mean-squared error sense) certain functions of the data (utility), while other private functions should not be reconstructed with distortion below a certain threshold (privacy). We demonstrate how a $\chi^2$-based information measure captures the fundamental PUT, and characterize several properties of this function. In particular, we give a sharp bound for the PUT. We then propose a convex program to compute privacy-assuring mappings when the functions to be disclosed and hidden are known a priori. Finally, we evaluate the robustness of our approach to finite samples. version:1
arxiv-1709-08055 | Feature-based time-series analysis | http://arxiv.org/abs/1709.08055 | id:1709.08055 author:Ben D. Fulcher category:cs.LG  published:2017-09-23 summary:This work presents an introduction to feature-based time-series analysis. The time series as a data type is first described, along with an overview of the interdisciplinary time-series analysis literature. I then summarize the range of feature-based representations for time series that have been developed to aid interpretable insights into time-series structure. Particular emphasis is given to emerging research that facilitates wide comparison of feature-based representations that allow us to understand the properties of a time-series dataset that make it suited to a particular feature-based representation or analysis algorithm. The future of time-series analysis is likely to embrace approaches that exploit machine learning methods to partially automate human learning to aid understanding of the complex dynamical patterns in the time series we measure from the world. version:2
arxiv-1709-08610 | Numerical optimization for Artificial Retina Algorithm | http://arxiv.org/abs/1709.08610 | id:1709.08610 author:Maxim Borisyak, Andrey Ustyuzhanin, Denis Derkach, Mikhail Belous category:cs.CV hep-ex physics.data-an  published:2017-09-25 summary:High-energy physics experiments rely on reconstruction of the trajectories of particles produced at the interaction point. This is a challenging task, especially in the high track multiplicity environment generated by p-p collisions at the LHC energies. A typical event includes hundreds of signal examples (interesting decays) and a significant amount of noise (uninteresting examples). This work describes a modification of the Artificial Retina algorithm for fast track finding: numerical optimization methods were adopted for fast local track search. This approach allows for considerable reduction of the total computational time per event. Test results on simplified simulated model of LHCb VELO (VErtex LOcator) detector are presented. Also this approach is well-suited for implementation of paralleled computations as GPGPU which look very attractive in the context of upcoming detector upgrades. version:2
arxiv-1710-01695 | DeepTFP: Mobile Time Series Data Analytics based Traffic Flow Prediction | http://arxiv.org/abs/1710.01695 | id:1710.01695 author:Yuanfang Chen, Falin Chen, Yizhi Ren, Ting Wu, Ye Yao category:cs.LG  published:2017-10-01 summary:Traffic flow prediction is an important research issue to avoid traffic congestion in transportation systems. Traffic congestion avoiding can be achieved by knowing traffic flow and then conducting transportation planning. Achieving traffic flow prediction is challenging as the prediction is affected by many complex factors such as inter-region traffic, vehicles' relations, and sudden events. However, as the mobile data of vehicles has been widely collected by sensor-embedded devices in transportation systems, it is possible to predict the traffic flow by analysing mobile data. This study proposes a deep learning based prediction algorithm, DeepTFP, to collectively predict the traffic flow on each and every traffic road of a city. This algorithm uses three deep residual neural networks to model temporal closeness, period, and trend properties of traffic flow. Each residual neural network consists of a branch of residual convolutional units. DeepTFP aggregates the outputs of the three residual neural networks to optimize the parameters of a time series prediction model. Contrast experiments on mobile time series data from the transportation system of England demonstrate that the proposed DeepTFP outperforms the Long Short-Term Memory (LSTM) architecture based method in prediction accuracy. version:1
arxiv-1709-08571 | On Noisy Negative Curvature Descent: Competing with Gradient Descent for Faster Non-convex Optimization | http://arxiv.org/abs/1709.08571 | id:1709.08571 author:Mingrui Liu, Tianbao Yang category:math.OC stat.ML  published:2017-09-25 summary:The Hessian-vector product has been utilized to find a second-order stationary solution with strong complexity guarantee (e.g., almost linear time complexity in the problem's dimensionality). In this paper, we propose to further reduce the number of Hessian-vector products for faster non-convex optimization. Previous algorithms need to approximate the smallest eigen-value with a sufficient precision (e.g., $\epsilon_2\ll 1$) in order to achieve a sufficiently accurate second-order stationary solution (i.e., $\lambda_{\min}(\nabla^2 f(\x))\geq -\epsilon_2)$. In contrast, the proposed algorithms only need to compute the smallest eigen-vector approximating the corresponding eigen-value up to a small power of current gradient's norm. As a result, it can dramatically reduce the number of Hessian-vector products during the course of optimization before reaching first-order stationary points (e.g., saddle points). The key building block of the proposed algorithms is a novel updating step named the NCG step, which lets a noisy negative curvature descent compete with the gradient descent. We show that the worst-case time complexity of the proposed algorithms with their favorable prescribed accuracy requirements can match the best in literature for achieving a second-order stationary point but with an arguably smaller per-iteration cost. We also show that the proposed algorithms can benefit from inexact Hessian by developing their variants accepting inexact Hessian under a mild condition for achieving the same goal. Moreover, we develop a stochastic algorithm for a finite or infinite sum non-convex optimization problem. To the best of our knowledge, the proposed stochastic algorithm is the first one that converges to a second-order stationary point in {\it high probability} with a time complexity independent of the sample size and almost linear in dimensionality. version:2
arxiv-1707-01203 | Estimating the Fundamental Limits is Easier than Achieving the Fundamental Limits | http://arxiv.org/abs/1707.01203 | id:1707.01203 author:Jiantao Jiao, Yanjun Han, Irena Fischer-Hwang, Tsachy Weissman category:cs.IT math.IT stat.ML  published:2017-07-05 summary:We show through case studies that it is easier to estimate the fundamental limits of data processing than to construct explicit algorithms to achieve those limits. Focusing on binary classification, data compression, and prediction under logarithmic loss, we show that in the finite space setting, when it is possible to construct an estimator of the limits with vanishing error with $n$ samples, it may require at least $n\ln n$ samples to construct an explicit algorithm to achieve the limits. version:2
arxiv-1710-00387 | Efficient Preconditioning for Noisy Separable NMFs by Successive Projection Based Low-Rank Approximations | http://arxiv.org/abs/1710.00387 | id:1710.00387 author:Tomohiko Mizutani, Mirai Tanaka category:cs.NA stat.ML  published:2017-10-01 summary:The successive projection algorithm (SPA) can quickly solve a nonnegative matrix factorization problem under a separability assumption. Even if noise is added to the problem, SPA is robust as long as the perturbations caused by the noise are small. In particular, robustness against noise should be high when handling the problems arising from real applications. The preconditioner proposed by Gillis and Vavasis (2015) makes it possible to enhance the noise robustness of SPA. Meanwhile, an additional computational cost is required. The construction of the preconditioner contains a step to compute the top-$k$ truncated singular value decomposition of an input matrix. It is known that the decomposition provides the best rank-$k$ approximation to the input matrix; in other words, a matrix with the smallest approximation error among all matrices of rank less than $k$. This step is an obstacle to an efficient implementation of the preconditioned SPA. To address the cost issue, we propose a modification of the algorithm for constructing the preconditioner. Although the original algorithm uses the best rank-$k$ approximation, instead of it, our modification uses an alternative. Ideally, this alternative should have high approximation accuracy and low computational cost. To ensure this, our modification employs a rank-$k$ approximation produced by an SPA based algorithm. We analyze the accuracy of the approximation and evaluate the computational cost of the algorithm. We then present an empirical study revealing the actual performance of the SPA based rank-$k$ approximation algorithm and the modified preconditioned SPA. version:1
arxiv-1708-02276 | Parallelizing Over Artificial Neural Network Training Runs with Multigrid | http://arxiv.org/abs/1708.02276 | id:1708.02276 author:Jacob B. Schroder category:cs.NA cs.LG  published:2017-08-07 summary:Artificial neural networks are a popular and effective machine learning technique. Great progress has been made parallelizing the expensive training phase of an individual network, leading to highly specialized pieces of hardware, many based on GPU-type architectures, and more concurrent algorithms such as synthetic gradients. However, the training phase continues to be a bottleneck, where the training data must be processed serially over thousands of individual training runs. This work considers a multigrid reduction in time (MGRIT) algorithm that is able to parallelize over the thousands of training runs and converge to the exact same solution as traditional training would provide. MGRIT was originally developed to provide parallelism for time evolution problems that serially step through a finite number of time-steps. This work recasts the training of a neural network similarly, treating neural network training as an evolution equation that evolves the network weights from one step to the next. Thus, this work concerns distributed computing approaches for neural networks, but is distinct from other approaches which seek to parallelize only over individual training runs. The work concludes with supporting numerical results for two model problems. version:2
arxiv-1710-00379 | libact: Pool-based Active Learning in Python | http://arxiv.org/abs/1710.00379 | id:1710.00379 author:Yao-Yuan Yang, Shao-Chuan Lee, Yu-An Chung, Tung-En Wu, Si-An Chen, Hsuan-Tien Lin category:cs.LG  published:2017-10-01 summary:libact is a Python package designed to make active learning easier for general users. The package not only implements several popular active learning strategies, but also features the active-learning-by-learning meta-algorithm that assists the users to automatically select the best strategy on the fly. Furthermore, the package provides a unified interface for implementing more strategies, models and application-specific labelers. The package is open-source on Github, and can be easily installed from Python Package Index repository. version:1
arxiv-1710-00372 | Mathematical foundations of matrix syntax | http://arxiv.org/abs/1710.00372 | id:1710.00372 author:Roman Orus, Roger Martin, Juan Uriagereka category:cs.CL quant-ph  published:2017-10-01 summary:Matrix syntax is a formal model of syntactic relations in language. The purpose of this paper is to explain its mathematical foundations, for an audience with some formal background. We make an axiomatic presentation, motivating each axiom on linguistic and practical grounds. The resulting mathematical structure resembles some aspects of quantum mechanics. Matrix syntax allows us to describe a number of language phenomena that are otherwise very difficult to explain, such as linguistic chains, and is arguably a more economical theory of language than most of the theories proposed in the context of the minimalist program in linguistics. In particular, sentences are naturally modeled as vectors in a Hilbert space with a tensor product structure, built from 2x2 matrices belonging to some specific group. version:1
arxiv-1709-05188 | Masquer Hunter: Adversarial Occlusion-aware Face Detection | http://arxiv.org/abs/1709.05188 | id:1709.05188 author:Yujia Chen, Lingxiao Song, Ran He category:cs.CV  published:2017-09-15 summary:Occluded face detection is a challenging detection task due to the large appearance variations incurred by various real-world occlusions. This paper introduces an Adversarial Occlusion-aware Face Detector (AOFD) by simultaneously detecting occluded faces and segmenting occluded areas. Specifically, we employ an adversarial training strategy to generate occlusion-like face features that are difficult for a face detector to recognize. Occlusion mask is predicted simultaneously while detecting occluded faces and the occluded area is utilized as an auxiliary instead of being regarded as a hindrance. Moreover, the supervisory signals from the segmentation branch will reversely affect the features, aiding in detecting heavily-occluded faces accordingly. Consequently, AOFD is able to find the faces with few exposed facial landmarks with very high confidences and keeps high detection accuracy even for masked faces. Extensive experiments demonstrate that AOFD not only significantly outperforms state-of-the-art methods on the MAFA occluded face detection dataset, but also achieves competitive detection accuracy on benchmark dataset for general face detection such as FDDB. version:2
arxiv-1710-00346 | Robust Tuning Datasets for Statistical Machine Translation | http://arxiv.org/abs/1710.00346 | id:1710.00346 author:Preslav Nakov, Stephan Vogel category:cs.CL 68T50 I.2.7  published:2017-10-01 summary:We explore the idea of automatically crafting a tuning dataset for Statistical Machine Translation (SMT) that makes the hyper-parameters of the SMT system more robust with respect to some specific deficiencies of the parameter tuning algorithms. This is an under-explored research direction, which can allow better parameter tuning. In this paper, we achieve this goal by selecting a subset of the available sentence pairs, which are more suitable for specific combinations of optimizers, objective functions, and evaluation measures. We demonstrate the potential of the idea with the pairwise ranking optimization (PRO) optimizer, which is known to yield too short translations. We show that the learning problem can be alleviated by tuning on a subset of the development set, selected based on sentence length. In particular, using the longest 50% of the tuning sentences, we achieve two-fold tuning speedup, and improvements in BLEU score that rival those of alternatives, which fix BLEU+1's smoothing instead. version:1
arxiv-1710-00341 | Fully Automated Fact Checking Using External Sources | http://arxiv.org/abs/1710.00341 | id:1710.00341 author:Georgi Karadzhov, Preslav Nakov, Lluis Marquez, Alberto Barron-Cedeno, Ivan Koychev category:cs.CL 68T50 I.2.7  published:2017-10-01 summary:Given the constantly growing proliferation of false claims online in recent years, there has been also a growing research interest in automatically distinguishing false rumors from factually true claims. Here, we propose a general-purpose framework for fully-automatic fact checking using external sources, tapping the potential of the entire Web as a knowledge source to confirm or reject a claim. Our framework uses a deep neural network with LSTM text encoding to combine semantic kernels with task-specific embeddings that encode a claim together with pieces of potentially-relevant text fragments from the Web, taking the source reliability into account. The evaluation results show good performance on two different tasks and datasets: (i) rumor detection and (ii) fact checking of the answers to a question in community question answering forums. version:1
arxiv-1709-01867 | Neural Networks Regularization Through Class-wise Invariant Representation Learning | http://arxiv.org/abs/1709.01867 | id:1709.01867 author:Soufiane Belharbi, Clément Chatelain, Romain Hérault, Sébastien Adam category:cs.LG stat.ML  published:2017-09-06 summary:Training deep neural networks is known to require a large number of training samples. However, in many applications only few training samples are available. In this work, we tackle the issue of training neural networks for classification task when few training samples are available. We attempt to solve this issue by proposing a new regularization term that constrains the hidden layers of a network to learn class-wise invariant representations. In our regularization framework, learning invariant representations is generalized to the class membership where samples with the same class should have the same representation. Numerical experiments over MNIST and its variants showed that our proposal helps improving the generalization of neural network particularly when trained with few samples. We provide the source code of our framework https://github.com/sbelharbi/learning-class-invariant-features . version:2
arxiv-1710-00307 | Pyramidal RoR for Image Classification | http://arxiv.org/abs/1710.00307 | id:1710.00307 author:Ke Zhang, Liru Guo, Ce Gao, Zhenbing Zhao category:cs.CV  published:2017-10-01 summary:The Residual Networks of Residual Networks (RoR) exhibits excellent performance in the image classification task, but sharply increasing the number of feature map channels makes the characteristic information transmission incoherent, which losses a certain of information related to classification prediction, limiting the classification performance. In this paper, a Pyramidal RoR network model is proposed by analysing the performance characteristics of RoR and combining with the PyramidNet. Firstly, based on RoR, the Pyramidal RoR network model with channels gradually increasing is designed. Secondly, we analysed the effect of different residual block structures on performance, and chosen the residual block structure which best favoured the classification performance. Finally, we add an important principle to further optimize Pyramidal RoR networks, drop-path is used to avoid over-fitting and save training time. In this paper, image classification experiments were performed on CIFAR-10/100 and SVHN datasets, and we achieved the current lowest classification error rates were 2.96%, 16.40% and 1.59%, respectively. Experiments show that the Pyramidal RoR network optimization method can improve the network performance for different data sets and effectively suppress the gradient disappearance problem in DCNN training. version:1
arxiv-1710-00286 | DTATG: An Automatic Title Generator based on Dependency Trees | http://arxiv.org/abs/1710.00286 | id:1710.00286 author:Liqun Shao, Jie Wang category:cs.IR cs.CL  published:2017-10-01 summary:We study automatic title generation for a given block of text and present a method called DTATG to generate titles. DTATG first extracts a small number of central sentences that convey the main meanings of the text and are in a suitable structure for conversion into a title. DTATG then constructs a dependency tree for each of these sentences and removes certain branches using a Dependency Tree Compression Model we devise. We also devise a title test to determine if a sentence can be used as a title. If a trimmed sentence passes the title test, then it becomes a title candidate. DTATG selects the title candidate with the highest ranking score as the final title. Our experiments showed that DTATG can generate adequate titles. We also showed that DTATG-generated titles have higher F1 scores than those generated by the previous methods. version:1
arxiv-1612-04112 | Upper Bound of Bayesian Generalization Error in Non-negative Matrix Factorization | http://arxiv.org/abs/1612.04112 | id:1612.04112 author:Naoki Hayashi, Sumio Watanabe category:math.ST cs.LG stat.ML stat.TH  published:2016-12-13 summary:Non-negative matrix factorization (NMF) is a new knowledge discovery method that is used for text mining, signal processing, bioinformatics, and consumer analysis. However, its basic property as a learning machine is not yet clarified, as it is not a regular statistical model, resulting that theoretical optimization method of NMF has not yet established. In this paper, we study the real log canonical threshold of NMF and give an upper bound of the generalization error in Bayesian learning. The results show that the generalization error of the matrix factorization can be made smaller than regular statistical models if Bayesian learning is applied. version:5
arxiv-1710-00284 | Efficient and Effective Single-Document Summarizations and A Word-Embedding Measurement of Quality | http://arxiv.org/abs/1710.00284 | id:1710.00284 author:Liqun Shao, Hao Zhang, Ming Jia, Jie Wang category:cs.IR cs.CL  published:2017-10-01 summary:Our task is to generate an effective summary for a given document with specific realtime requirements. We use the softplus function to enhance keyword rankings to favor important sentences, based on which we present a number of summarization algorithms using various keyword extraction and topic clustering methods. We show that our algorithms meet the realtime requirements and yield the best ROUGE recall scores on DUC-02 over all previously-known algorithms. We show that our algorithms meet the realtime requirements and yield the best ROUGE recall scores on DUC-02 over all previously-known algorithms. To evaluate the quality of summaries without human-generated benchmarks, we define a measure called WESM based on word-embedding using Word Mover's Distance. We show that the orderings of the ROUGE and WESM scores of our algorithms are highly comparable, suggesting that WESM may serve as a viable alternative for measuring the quality of a summary. version:1
arxiv-1710-00279 | Image Dehazing using Bilinear Composition Loss Function | http://arxiv.org/abs/1710.00279 | id:1710.00279 author:Hui Yang, Jinshan Pan, Qiong Yan, Wenxiu Sun, Jimmy Ren, Yu-Wing Tai category:cs.CV  published:2017-10-01 summary:In this paper, we introduce a bilinear composition loss function to address the problem of image dehazing. Previous methods in image dehazing use a two-stage approach which first estimate the transmission map followed by clear image estimation. The drawback of a two-stage method is that it tends to boost local image artifacts such as noise, aliasing and blocking. This is especially the case for heavy haze images captured with a low quality device. Our method is based on convolutional neural networks. Unique in our method is the bilinear composition loss function which directly model the correlations between transmission map, clear image, and atmospheric light. This allows errors to be back-propagated to each sub-network concurrently, while maintaining the composition constraint to avoid overfitting of each sub-network. We evaluate the effectiveness of our proposed method using both synthetic and real world examples. Extensive experiments show that our method outperfoms state-of-the-art methods especially for haze images with severe noise level and compressions. version:1
arxiv-1710-00273 | What Words Do We Use to Lie?: Word Choice in Deceptive Messages | http://arxiv.org/abs/1710.00273 | id:1710.00273 author:Jason Dou, Michelle Liu, Haaris Muneer, Adam Schlussel category:cs.CL  published:2017-10-01 summary:Text messaging is the most widely used form of computer- mediated communication (CMC). Previous findings have shown that linguistic factors can reliably indicate messages as deceptive. For example, users take longer and use more words to craft deceptive messages than they do truthful messages. Existing research has also examined how factors, such as student status and gender, affect rates of deception and word choice in deceptive messages. However, this research has been limited by small sample sizes and has returned contradicting findings. This paper aims to address these issues by using a dataset of text messages collected from a large and varied set of participants using an Android messaging application. The results of this paper show significant differences in word choice and frequency of deceptive messages between male and female participants, as well as between students and non-students. version:1
arxiv-1710-00264 | Bayesian estimation from few samples: community detection and related problems | http://arxiv.org/abs/1710.00264 | id:1710.00264 author:Samuel B. Hopkins, David Steurer category:cs.DS cs.CC cs.LG math.PR stat.ML  published:2017-09-30 summary:We propose an efficient meta-algorithm for Bayesian estimation problems that is based on low-degree polynomials, semidefinite programming, and tensor decomposition. The algorithm is inspired by recent lower bound constructions for sum-of-squares and related to the method of moments. Our focus is on sample complexity bounds that are as tight as possible (up to additive lower-order terms) and often achieve statistical thresholds or conjectured computational thresholds. Our algorithm recovers the best known bounds for community detection in the sparse stochastic block model, a widely-studied class of estimation problems for community detection in graphs. We obtain the first recovery guarantees for the mixed-membership stochastic block model (Airoldi et el.) in constant average degree graphs---up to what we conjecture to be the computational threshold for this model. We show that our algorithm exhibits a sharp computational threshold for the stochastic block model with multiple communities beyond the Kesten--Stigum bound---giving evidence that this task may require exponential time. The basic strategy of our algorithm is strikingly simple: we compute the best-possible low-degree approximation for the moments of the posterior distribution of the parameters and use a robust tensor decomposition algorithm to recover the parameters from these approximate posterior moments. version:1
arxiv-1710-01260 | Gaussian Three-Dimensional kernel SVM for Edge Detection Applications | http://arxiv.org/abs/1710.01260 | id:1710.01260 author:Safar Irandoust-Pakchin, Aydin Ayanzadeh, Siamak Beikzadeh category:cs.CV  published:2017-09-30 summary:This paper presents a novel and uniform algorithm for edge detection based on SVM (support vector machine) with Three-dimensional Gaussian radial basis function with kernel. Because of disadvantages in traditional edge detection such as inaccurate edge location, rough edge and careless on detect soft edge. The experimental results indicate how the SVM can detect edge in efficient way. The performance of the proposed algorithm is compared with existing methods, including Sobel and canny detectors. The results show that this method is better than classical algorithm such as canny and Sobel detector. version:1
arxiv-1710-00241 | DeepWheat: Estimating Phenotypic Traits From Images of Crops Using Deep Learning | http://arxiv.org/abs/1710.00241 | id:1710.00241 author:Shubhra Aich, Imran Ahmed, Ilya Obsyannikov, Ian Stavness, Anique Josuttes, Keegan Strueby, Hema Sudhakar Duddu, Curtis Pozniak, Steve Shirtliffe category:cs.CV  published:2017-09-30 summary:In this paper, we investigate the problem of estimating the phenotypic traits of plants from color images and elevation maps of field plots. We focus on emergence and biomass traits - two important indicators of crop growth and health. We employ a state-of-the-art deconvolutional network for segmentation and convolutional architectures, with residual learning in the final stages, for trait estimation. Our intention was to design estimation architectures that behave like high dimensional nonlinear regression models. To the best of our knowledge, this is the first work on emergence counting and biomass estimation based on deep learning. Evaluation was performed on two different species of wheat, grown in field plots for an experimental plant breeding study. Our framework achieves satisfactory performance with mean and standard deviation of absolute difference of 1.20 and 1.53 counts for emergence and 1.45 and 2.05 for biomass estimation. Our results for counting wheat plants from field images is comparable to the accuracy reported for the similar, but arguably less difficult, task of counting leaves from pictures of rosettes grown in pots. Our results for biomass estimation improve upon all previously proposed approaches in the literature. version:1
arxiv-1710-01168 | Fast Fine-grained Image Classification via Weakly Supervised Discriminative Localization | http://arxiv.org/abs/1710.01168 | id:1710.01168 author:Xiangteng He, Yuxin Peng, Junjie Zhao category:cs.CV  published:2017-09-30 summary:Fine-grained image classification is to recognize hundreds of subcategories in each basic-level category. Existing methods employ discriminative localization to find the key distinctions among subcategories. However, they generally have two limitations: (1) Discriminative localization relies on region proposal methods to hypothesize the locations of discriminative regions, which are time-consuming. (2) The training of discriminative localization depends on object or part annotations, which are heavily labor-consuming. It is highly challenging to address the two key limitations simultaneously, and existing methods only focus on one of them. Therefore, we propose a weakly supervised discriminative localization approach (WSDL) for fast fine-grained image classification to address the two limitations at the same time, and its main advantages are: (1) n-pathway end-to-end discriminative localization network is designed to improve classification speed, which simultaneously localizes multiple different discriminative regions for one image to boost classification accuracy, and shares full-image convolutional features generated by region proposal network to accelerate the process of generating region proposals as well as reduce the computation of convolutional operation. (2) Multi-level attention guided localization learning is proposed to localize discriminative regions with different focuses automatically, without using object and part annotations, avoiding the labor consumption. Different level attentions focus on different characteristics of the image, which are complementary and boost the classification accuracy. Both are jointly employed to simultaneously improve classification speed and eliminate dependence on object and part annotations. Compared with state-of-the-art methods on 2 widely-used fine-grained image classification datasets, our WSDL approach achieves the best performance. version:1
arxiv-1710-00230 | Robust Surface Reconstruction from Gradients via Adaptive Dictionary Regularization | http://arxiv.org/abs/1710.00230 | id:1710.00230 author:Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi category:cs.CV stat.ML  published:2017-09-30 summary:This paper introduces a novel approach to robust surface reconstruction from photometric stereo normal vector maps that is particularly well-suited for reconstructing surfaces from noisy gradients. Specifically, we propose an adaptive dictionary learning based approach that attempts to simultaneously integrate the gradient fields while sparsely representing the spatial patches of the reconstructed surface in an adaptive dictionary domain. We show that our formulation learns the underlying structure of the surface, effectively acting as an adaptive regularizer that enforces a smoothness constraint on the reconstructed surface. Our method is general and may be coupled with many existing approaches in the literature to improve the integrity of the reconstructed surfaces. We demonstrate the performance of our method on synthetic data as well as real photometric stereo data and evaluate its robustness to noise. version:1
arxiv-1710-00002 | Robust Photometric Stereo Using Learned Image and Gradient Dictionaries | http://arxiv.org/abs/1710.00002 | id:1710.00002 author:Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi category:cs.CV stat.ML  published:2017-09-30 summary:Photometric stereo is a method for estimating the normal vectors of an object from images of the object under varying lighting conditions. Motivated by several recent works that extend photometric stereo to more general objects and lighting conditions, we study a new robust approach to photometric stereo that utilizes dictionary learning. Specifically, we propose and analyze two approaches to adaptive dictionary regularization for the photometric stereo problem. First, we propose an image preprocessing step that utilizes an adaptive dictionary learning model to remove noise and other non-idealities from the image dataset before estimating the normal vectors. We also propose an alternative model where we directly apply the adaptive dictionary regularization to the normal vectors themselves during estimation. We study the practical performance of both methods through extensive simulations, which demonstrate the state-of-the-art performance of both methods in the presence of noise. version:1
arxiv-1707-02131 | SigNet: Convolutional Siamese Network for Writer Independent Offline Signature Verification | http://arxiv.org/abs/1707.02131 | id:1707.02131 author:Sounak Dey, Anjan Dutta, J. Ignacio Toledo, Suman K. Ghosh, Josep Llados, Umapada Pal category:cs.CV  published:2017-07-07 summary:Offline signature verification is one of the most challenging tasks in biometrics and document forensics. Unlike other verification problems, it needs to model minute but critical details between genuine and forged signatures, because a skilled falsification might often resembles the real signature with small deformation. This verification task is even harder in writer independent scenarios which is undeniably fiscal for realistic cases. In this paper, we model an offline writer independent signature verification task with a convolutional Siamese network. Siamese networks are twin networks with shared weights, which can be trained to learn a feature space where similar observations are placed in proximity. This is achieved by exposing the network to a pair of similar and dissimilar observations and minimizing the Euclidean distance between similar pairs while simultaneously maximizing it between dissimilar pairs. Experiments conducted on cross-domain datasets emphasize the capability of our network to model forgery in different languages (scripts) and handwriting styles. Moreover, our designed Siamese network, named SigNet, exceeds the state-of-the-art results on most of the benchmark signature datasets, which paves the way for further research in this direction. version:2
arxiv-1710-01167 | Decontamination of Mutual Contamination Models | http://arxiv.org/abs/1710.01167 | id:1710.01167 author:Julian Katz-Samuels, Gilles Blanchard, Clayton Scott category:stat.ML  published:2017-09-30 summary:Many machine learning problems can be characterized by mutual contamination models. In these problems, one observes several random samples from different convex combinations of a set of unknown base distributions and the goal is to infer these base distributions. This paper considers the general setting where the base distributions are defined on arbitrary probability spaces. We examine three popular machine learning problems that arise in this general setting: multiclass classification with label noise, demixing of mixed membership models, and classification with partial labels. In each case, we give sufficient conditions for identifiability and present algorithms for the infinite and finite sample settings, with associated performance guarantees. version:1
arxiv-1709-05006 | Two-sample Statistics Based on Anisotropic Kernels | http://arxiv.org/abs/1709.05006 | id:1709.05006 author:Xiuyuan Cheng, Alexander Cloninger, Ronald R. Coifman category:stat.ML cs.LG stat.AP stat.CO  published:2017-09-14 summary:The paper introduces a new kernel-based Maximum Mean Discrepancy (MMD) statistic for measuring the distance between two distributions given finitely-many multivariate samples. When the distributions are locally low-dimensional, the proposed test can be made more powerful to distinguish certain alternatives by incorporating local covariance matrices and constructing an anisotropic kernel. The kernel matrix is asymmetric; it computes the affinity between $n$ data points and a set of $n_R$ reference points, where $n_R$ can be drastically smaller than $n$. While the proposed statistic can be viewed as a special class of Reproducing Kernel Hilbert Space MMD, the consistency of the test is proved, under mild assumptions of the kernel, as long as $\ p-q\ \sim O(n^{-1/2+\delta})$ for any $\delta>0$, based on a result of convergence in distribution of the test statistic. Applications to flow cytometry and diffusion MRI datasets are demonstrated, which motivate the proposed approach to compare distributions. version:2
arxiv-1710-00211 | The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems | http://arxiv.org/abs/1710.00211 | id:1710.00211 author:Weinan E, Bing Yu category:cs.LG stat.ML 35Q68  published:2017-09-30 summary:We propose a deep learning based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems. version:1
arxiv-1710-00210 | Testing for Feature Relevance: The HARVEST Algorithm | http://arxiv.org/abs/1710.00210 | id:1710.00210 author:Herbert Weisberg, Victor Pontes category:stat.ML  published:2017-09-30 summary:Feature selection with high-dimensional data and a very small proportion of relevant features poses a severe challenge to standard statistical methods. We have developed a new approach (HARVEST) that is straightforward to apply, albeit somewhat computer-intensive. This algorithm can be used to pre-screen a large number of features to identify those that are potentially useful. The basic idea is to evaluate each feature in the context of many random subsets of other features. HARVEST is predicated on the assumption that an irrelevant feature can add no real predictive value, regardless of which other features are included in the subset. Motivated by this idea, we have derived a simple statistical test for feature relevance. Empirical analyses and simulations produced so far indicate that the HARVEST algorithm is highly effective in predictive analytics, both in science and business. version:1
arxiv-1710-00209 | Improved Training for Self-Training | http://arxiv.org/abs/1710.00209 | id:1710.00209 author:Gal Hyams, Daniel Greenfeld, Dor Bank category:cs.LG  published:2017-09-30 summary:It is well known that for some tasks, labeled data sets may be hard to gather. Therefore, we wished to tackle here the problem of having insufficient training data. We examined learning methods from unlabeled data after an initial training on a limited labeled data set. The suggested approach can be used as an online learning method on the unlabeled test set. In the general classification task, whenever we predict a label with high enough confidence, we treat it as a true label and train the data accordingly. For the semantic segmentation task, a classic example for an expensive data labeling process, we do so pixel-wise. Our suggested approaches were applied on the MNIST data-set as a proof of concept for a vision classification task and on the ADE20K data-set in order to tackle the semi-supervised semantic segmentation problem. version:1
arxiv-1710-00205 | Bag-of-Vector Embeddings of Dependency Graphs for Semantic Induction | http://arxiv.org/abs/1710.00205 | id:1710.00205 author:Diana Nicoleta Popa, James Henderson category:cs.CL  published:2017-09-30 summary:Vector-space models, from word embeddings to neural network parsers, have many advantages for NLP. But how to generalise from fixed-length word vectors to a vector space for arbitrary linguistic structures is still unclear. In this paper we propose bag-of-vector embeddings of arbitrary linguistic graphs. A bag-of-vector space is the minimal nonparametric extension of a vector space, allowing the representation to grow with the size of the graph, but not tying the representation to any specific tree or graph structure. We propose efficient training and inference algorithms based on tensor factorisation for embedding arbitrary graphs in a bag-of-vector space. We demonstrate the usefulness of this representation by training bag-of-vector embeddings of dependency graphs and evaluating them on unsupervised semantic induction for the Semantic Textual Similarity and Natural Language Inference tasks. version:1
arxiv-1708-09630 | Optimal Distributed Control of Multi-agent Systems in Contested Environments via Reinforcement Learning | http://arxiv.org/abs/1708.09630 | id:1708.09630 author:Rohollah Moghadam, Hamidreza Modares category:cs.MA cs.LG cs.SY  published:2017-08-31 summary:This paper presents a model-free reinforcement learning (RL) based distributed control protocol for leader-follower multi-agent systems. Although RL has been successfully used to learn optimal control protocols for multi-agent systems, the effects of adversarial inputs are ignored. It is shown in this paper, however, that their adverse effects can propagate across the network and impact the learning outcome of other intact agents. To alleviate this problem, a unified RL-based distributed control frameworks is developed for both homogeneous and heterogeneous multi-agent systems to prevent corrupted sensory data from propagating across the network. To this end, only the leader communicates its actual sensory information and other agents estimate the leader state using a distributed observer and communicate this estimation to their neighbors to achieve consensus on the leader state. The observer cannot be physically affected by any adversarial input. To further improve resiliency, distributed H-infinity control protocols are designed to attenuate the effect of the adversarial inputs on the compromised agent itself. An off-policy RL algorithm is developed to learn the solutions of the game algebraic Riccati equations arising from solving the H-infinity control problem. No knowledge of the agent dynamics is required and it is shown that the proposed RL-based H-infinity control protocol is resilient against adversarial inputs. version:2
arxiv-1706-04138 | An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing | http://arxiv.org/abs/1706.04138 | id:1706.04138 author:Marcin Junczys-Dowmunt, Roman Grundkiewicz category:cs.CL  published:2017-06-13 summary:In this work, we explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs $mt$ (raw MT output) and $src$ (source language input) in a single neural architecture, modeling $\{mt, src\} \rightarrow pe$ directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT-2016 shared task on automatic post-editing and can demonstrate that dual-attention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Dual-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input. version:2
arxiv-1710-00194 | Where computer vision can aid physics: dynamic cloud motion forecasting from satellite images | http://arxiv.org/abs/1710.00194 | id:1710.00194 author:Sergiy Zhuk, Tigran Tchrakian, Albert Akhriev, Siyuan Lu, Hendrik Hamann category:math.OC cs.CV physics.flu-dyn physics.geo-ph  published:2017-09-30 summary:This paper describes a new algorithm for solar energy forecasting from a sequence of Cloud Optical Depth (COD) images. The algorithm is based on the following simple observation: the dynamics of clouds represented by COD images resembles the motion (transport) of a density in a fluid flow. This suggests that, to forecast the motion of COD images, it is sufficient to forecast the flow. The latter, in turn, can be accomplished by fitting a parametric model of the fluid flow to the COD images observed in the past. Namely, the learning phase of the algorithm is composed of the following steps: (i) given a sequence of COD images, the snapshots of the optical flow are estimated from two consecutive COD images; (ii) these snapshots are then assimilated into a Navier-Stokes Equation (NSE), i.e. an initial velocity field for NSE is selected so that the corresponding NSE' solution is as close as possible to the optical flow snapshots. The prediction phase consists of utilizing a linear transport equation, which describes the propagation of COD images in the fluid flow predicted by NSE, to estimate the future motion of the COD images. The algorithm has been tested on COD images provided by two geostationary operational environmental satellites from NOAA serving the west-hemisphere. version:1
arxiv-1710-00189 | Unsupervised Classification of Intrusive Igneous Rock Thin Section Images using Edge Detection and Colour Analysis | http://arxiv.org/abs/1710.00189 | id:1710.00189 author:S. Joseph, H. Ujir, I. Hipiny category:cs.CV  published:2017-09-30 summary:Classification of rocks is one of the fundamental tasks in a geological study. The process requires a human expert to examine sampled thin section images under a microscope. In this study, we propose a method that uses microscope automation, digital image acquisition, edge detection and colour analysis (histogram). We collected 60 digital images from 20 standard thin sections using a digital camera mounted on a conventional microscope. Each image is partitioned into a finite number of cells that form a grid structure. Edge and colour profile of pixels inside each cell determine its classification. The individual cells then determine the thin section image classification via a majority voting scheme. Our method yielded successful results as high as 90% to 100% precision. version:1
arxiv-1710-00187 | Unsupervised Segmentation of Action Segments in Egocentric Videos using Gaze | http://arxiv.org/abs/1710.00187 | id:1710.00187 author:I. Hipiny, H. Ujir, J. L. Minoi, S. F. Samson Juan, M. A. Khairuddin, M. S. Sunar category:cs.CV  published:2017-09-30 summary:Unsupervised segmentation of action segments in egocentric videos is a desirable feature in tasks such as activity recognition and content-based video retrieval. Reducing the search space into a finite set of action segments facilitates a faster and less noisy matching. However, there exist a substantial gap in machine understanding of natural temporal cuts during a continuous human activity. This work reports on a novel gaze-based approach for segmenting action segments in videos captured using an egocentric camera. Gaze is used to locate the region-of-interest inside a frame. By tracking two simple motion-based parameters inside successive regions-of-interest, we discover a finite set of temporal cuts. We present several results using combinations (of the two parameters) on a dataset, i.e., BRISGAZE-ACTIONS. The dataset contains egocentric videos depicting several daily-living activities. The quality of the temporal cuts is further improved by implementing two entropy measures. version:1
arxiv-1710-00175 | A Many-Objective Evolutionary Algorithm with Angle-Based Selection and Shift-Based Density Estimation | http://arxiv.org/abs/1710.00175 | id:1710.00175 author:Zhi-Zhong Liu, Yong Wang, Pei-Qiu Huang category:cs.NE  published:2017-09-30 summary:Evolutionary many-objective optimization has been gaining increasing attention from the evolutionary computation research community. Much effort has been devoted to addressing this issue by improving the scalability of multiobjective evolutionary algorithms, such as Pareto-based, decomposition-based, and indicator-based approaches. Different from current work, we propose a novel algorithm in this paper called AnD, which consists of an angle-based selection strategy and a shift-based density estimation strategy. These two strategies are employed in the environmental selection to delete the poor individuals one by one. Specifically, the former is devised to find a pair of individuals with the minimum vector angle, which means that these two individuals share the most similar search direction. The latter, which takes both the diversity and convergence into account, is adopted to compare these two individuals and to delete the worse one. AnD has a simple structure, few parameters, and no complicated operators. The performance of AnD is compared with that of seven state-of-the-art many-objective evolutionary algorithms on a variety of benchmark test problems with up to 15 objectives. The experimental results suggest that AnD can achieve highly competitive performance. In addition, we also verify that AnD can be readily extended to solve constrained many-objective optimization problems. version:1
arxiv-1710-00166 | PCANet-II: When PCANet Meets the Second Order Pooling | http://arxiv.org/abs/1710.00166 | id:1710.00166 author:Lei Tian, Xiaopeng Hong, Guoying Zhao, Chunxiao Fan, Yue Ming, Matti Pietikäinen category:cs.CV  published:2017-09-30 summary:PCANet, as one noticeable shallow network, employs the histogram representation for feature pooling. However, there are three main problems about this kind of pooling method. First, the histogram-based pooling method binarizes the feature maps and leads to inevitable discriminative information loss. Second, it is difficult to effectively combine other visual cues into a compact representation, because the simple concatenation of various visual cues leads to feature representation inefficiency. Third, the dimensionality of histogram-based output grows exponentially with the number of feature maps used. In order to overcome these problems, we propose a novel shallow network model, named as PCANet-II. Compared with the histogram-based output, the second order pooling not only provides more discriminative information by preserving both the magnitude and sign of convolutional responses, but also dramatically reduces the size of output features. Thus we combine the second order statistical pooling method with the shallow network, i.e., PCANet. Moreover, it is easy to combine other discriminative and robust cues by using the second order pooling. So we introduce the binary feature difference encoding scheme into our PCANet-II to further improve robustness. Experiments demonstrate the effectiveness and robustness of our proposed PCANet-II method. version:1
arxiv-1710-00818 | Continuous-Time Relationship Prediction in Dynamic Heterogeneous Information Networks | http://arxiv.org/abs/1710.00818 | id:1710.00818 author:Sina Sajadmanesh, Jiawei Zhang, Hamid R. Rabiee category:cs.SI cs.LG  published:2017-09-30 summary:Online social networks, World Wide Web, media and technological networks, and other types of so-called information networks are ubiquitous nowadays. These information networks are inherently heterogeneous and dynamic. They are heterogeneous as they consist of multi-typed objects and relations, and they are dynamic as they are constantly evolving over time. One of the challenging issues in such heterogeneous and dynamic environments is to forecast those relationships in the network that will appear in the future. In this paper, we try to solve the problem of continuous-time relationship prediction in dynamic and heterogeneous information networks. This implies predicting the time it takes for a relationship to appear in the future, given its features that have been extracted by considering both the heterogeneity and the temporal dynamics of the underlying network. To this end, we first introduce a meta-path-based feature extraction framework to effectively extract features suitable for relationship prediction regarding the heterogeneity and dynamicity of the network. Next, we propose a supervised nonparametric approach, called Non-Parametric Generalized Linear Model (NP-GLM), which infers the hidden underlying probability distribution of the relationship building time given its features. We then present a learning algorithm to train NP-GLM and an inference method to answer time-related queries. Extensive experiments conducted on both synthetic dataset and real-world DBLP bibliographic citation network dataset demonstrate the effectiveness of Np-Glm in solving continuous-time relationship prediction problem vis-a-vis alternative baselines. version:1
arxiv-1710-00165 | Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken Language Understanding | http://arxiv.org/abs/1710.00165 | id:1710.00165 author:Po-Chun Chen, Ta-Chung Chi, Shang-Yu Su, Yun-Nung Chen category:cs.CL  published:2017-09-30 summary:Spoken language understanding (SLU) is an essential component in conversational systems. Most SLU component treats each utterance independently, and then the following components aggregate the multi-turn information in the separate phases. In order to avoid error propagation and effectively utilize contexts, prior work leveraged history for contextual SLU. However, the previous model only paid attention to the content in history utterances without considering their temporal information and speaker roles. In the dialogues, the most recent utterances should be more important than the least recent ones. Furthermore, users usually pay attention to 1) self history for reasoning and 2) others' utterances for listening, the speaker of the utterances may provides informative cues to help understanding. Therefore, this paper proposes an attention-based network that additionally leverages temporal information and speaker role for better SLU, where the attention to contexts and speaker roles can be automatically learned in an end-to-end manner. The experiments on the benchmark Dialogue State Tracking Challenge 4 (DSTC4) dataset show that the time-aware dynamic role attention networks significantly improve the understanding performance. version:1
arxiv-1710-00164 | Speaker Role Contextual Modeling for Language Understanding and Dialogue Policy Learning | http://arxiv.org/abs/1710.00164 | id:1710.00164 author:Ta-Chung Chi, Po-Chun Chen, Shang-Yu Su, Yun-Nung Chen category:cs.CL  published:2017-09-30 summary:Language understanding (LU) and dialogue policy learning are two essential components in conversational systems. Human-human dialogues are not well-controlled and often random and unpredictable due to their own goals and speaking habits. This paper proposes a role-based contextual model to consider different speaker roles independently based on the various speaking patterns in the multi-turn dialogues. The experiments on the benchmark dataset show that the proposed role-based model successfully learns role-specific behavioral patterns for contextual encoding and then significantly improves language understanding and dialogue policy learning tasks. version:1
arxiv-1709-01643 | Learning to Compose Domain-Specific Transformations for Data Augmentation | http://arxiv.org/abs/1709.01643 | id:1709.01643 author:Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hussain, Jared Dunnmon, Christopher Ré category:stat.ML cs.CV cs.LG  published:2017-09-06 summary:Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches. version:3
arxiv-1706-04277 | AFIF4: Deep Gender Classification based on AdaBoost-based Fusion of Isolated Facial Features and Foggy Faces | http://arxiv.org/abs/1706.04277 | id:1706.04277 author:Mahmoud Afifi, Abdelrahman Abdelhamed category:cs.CV  published:2017-06-13 summary:Gender classification aims at recognizing a person's gender. Despite the high accuracy achieved by state-of-the-art methods for this task, there is still room for improvement in generalized and unrestricted datasets. In this paper, we advocate a new strategy inspired by the behavior of humans in gender recognition. Instead of dealing with the face image as a sole feature, we rely on the combination of isolated facial features and a holistic feature which we call the foggy face. Then, we use these features to train deep convolutional neural networks followed by an AdaBoost-based score fusion to infer the final gender class. We evaluate our method on four challenging datasets to demonstrate its efficacy in achieving better or on-par accuracy with state-of-the-art methods. In addition, we present a new face dataset that intensifies the challenges of occluded faces and illumination changes, which we believe to be a much-needed resource for gender classification research. version:4
arxiv-1710-00109 | Reconstruction from Periodic Nonlinearities, With Applications to HDR Imaging | http://arxiv.org/abs/1710.00109 | id:1710.00109 author:Viraj Shah, Mohammadreza Soltani, Chinmay Hegde category:stat.ML  published:2017-09-29 summary:We consider the problem of reconstructing signals and images from periodic nonlinearities. For such problems, we design a measurement scheme that supports efficient reconstruction; moreover, our method can be adapted to extend to compressive sensing-based signal and image acquisition systems. Our techniques can be potentially useful for reducing the measurement complexity of high dynamic range (HDR) imaging systems, with little loss in reconstruction quality. Several numerical experiments on real data demonstrate the effectiveness of our approach. version:1
arxiv-1710-00095 | User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient | http://arxiv.org/abs/1710.00095 | id:1710.00095 author:Arnak S. Dalalyan, Avetik G. Karagulyan category:math.ST cs.LG math.PR stat.CO stat.ML stat.TH  published:2017-09-29 summary:In this paper, we revisit the recently established theoretical guarantees for the convergence of the Langevin Monte Carlo algorithm of sampling from a smooth and (strongly) log-concave density. We improve, in terms of constants, the existing results when the accuracy of sampling is measured in the Wasserstein distance and provide further insights on relations between, on the one hand, the Langevin Monte Carlo for sampling and, on the other hand, the gradient descent for optimization. More importantly, we establish non-asymptotic guarantees for the accuracy of a version of the Langevin Monte Carlo algorithm that is based on inaccurate evaluations of the gradient. Finally, we propose a variable-step version of the Langevin Monte Carlo algorithm that has two advantages. First, its step-sizes are independent of the target accuracy and, second, its rate provides a logarithmic improvement over the constant-step Langevin Monte Carlo algorithm. version:1
arxiv-1710-00085 | Language-depedent I-Vectors for LRE15 | http://arxiv.org/abs/1710.00085 | id:1710.00085 author:Niko Brümmer, Albert Swart category:stat.ML cs.LG  published:2017-09-29 summary:A standard recipe for spoken language recognition is to apply a Gaussian back-end to i-vectors. This ignores the uncertainty in the i-vector extraction, which could be important especially for short utterances. A recent paper by Cumani, Plchot and Fer proposes a solution to propagate that uncertainty into the backend. We propose an alternative method of propagating the uncertainty. version:1
arxiv-1710-01693 | Model-free prediction of noisy chaotic time series by deep learning | http://arxiv.org/abs/1710.01693 | id:1710.01693 author:Kyongmin Yeo category:cs.LG physics.comp-ph physics.data-an  published:2017-09-29 summary:We present a deep neural network for a model-free prediction of a chaotic dynamical system from noisy observations. The proposed deep learning model aims to predict the conditional probability distribution of a state variable. The Long Short-Term Memory network (LSTM) is employed to model the nonlinear dynamics and a softmax layer is used to approximate a probability distribution. The LSTM model is trained by minimizing a regularized cross-entropy function. The LSTM model is validated against delay-time chaotic dynamical systems, Mackey-Glass and Ikeda equations. It is shown that the present LSTM makes a good prediction of the nonlinear dynamics by effectively filtering out the noise. It is found that the prediction uncertainty of a multiple-step forecast of the LSTM model is not a monotonic function of time; the predicted standard deviation may increase or decrease dynamically in time. version:1
arxiv-1710-00075 | A Gaussian mixture model representation of endmember variability in hyperspectral unmixing | http://arxiv.org/abs/1710.00075 | id:1710.00075 author:Yuan Zhou, Anand Rangarajan, Paul D. Gader category:cs.CV  published:2017-09-29 summary:Hyperspectral unmixing while considering endmember variability is usually performed by the normal compositional model (NCM), where the endmembers for each pixel are assumed to be sampled from unimodal Gaussian distributions. However, in real applications, the distribution of a material is often not Gaussian. In this paper, we use Gaussian mixture models (GMM) to represent the endmember variability. We show, given the GMM starting premise, that the distribution of the mixed pixel (under the linear mixing model) is also a GMM (and this is shown from two perspectives). The first perspective originates from the random variable transformation and gives a conditional density function of the pixels given the abundances and GMM parameters. With proper smoothness and sparsity prior constraints on the abundances, the conditional density function leads to a standard maximum a posteriori (MAP) problem which can be solved using generalized expectation maximization. The second perspective originates from marginalizing over the endmembers in the GMM, which provides us with a foundation to solve for the endmembers at each pixel. Hence, our model can not only estimate the abundances and distribution parameters, but also the distinct endmember set for each pixel. We tested the proposed GMM on several synthetic and real datasets, and showed its potential by comparing it to current popular methods. version:1
arxiv-1710-00032 | Learning the Exact Topology of Undirected Consensus Networks | http://arxiv.org/abs/1710.00032 | id:1710.00032 author:Saurav Talukdar, Deepjyoti Deka, Sandeep Attree, Donatello Materassi, Murti V. Salapaka category:cs.SY cs.LG  published:2017-09-29 summary:In this article, we present a method to learn the interaction topology of a network of agents undergoing linear consensus updates in a non invasive manner. Our approach is based on multivariate Wiener filtering, which is known to recover spurious edges apart from the true edges in the topology. The main contribution of this work is to show that in the case of undirected consensus networks, all spurious links obtained using Wiener filtering can be identified using frequency response of the Wiener filters. Thus, the exact interaction topology of the agents is unveiled. The method presented requires time series measurements of the state of the agents and does not require any knowledge of link weights. To the best of our knowledge this is the first approach that provably reconstructs the structure of undirected consensus networks with correlated noise. We illustrate the effectiveness of the method developed through numerical simulations as well as experiments on a five node network of Raspberry Pis. version:1
arxiv-1710-00018 | Unsupervised Domain Adaptation with Copula Models | http://arxiv.org/abs/1710.00018 | id:1710.00018 author:Cuong D. Tran, Ognjen Rudovic, Vladimir Pavlovic category:cs.LG cs.CV stat.ML  published:2017-09-29 summary:We study the task of unsupervised domain adaptation, where no labeled data from the target domain is provided during training time. To deal with the potential discrepancy between the source and target distributions, both in features and labels, we exploit a copula-based regression framework. The benefits of this approach are two-fold: (a) it allows us to model a broader range of conditional predictive densities beyond the common exponential family, (b) we show how to leverage Sklar's theorem, the essence of the copula formulation relating the joint density to the copula dependency functions, to find effective feature mappings that mitigate the domain mismatch. By transforming the data to a copula domain, we show on a number of benchmark datasets (including human emotion estimation), and using different regression models for prediction, that we can achieve a more robust and accurate estimation of target labels, compared to recently proposed feature transformation (adaptation) methods. version:1
arxiv-1710-00017 | Hierarchical modeling of molecular energies using a deep neural network | http://arxiv.org/abs/1710.00017 | id:1710.00017 author:Nicholas Lubbers, Justin S. Smith, Kipton Barros category:stat.ML physics.chem-ph  published:2017-09-29 summary:We introduce the Hierarchically Interacting Particle Neural Network (HIP-NN) to model molecular properties from datasets of quantum calculations. Inspired by a many-body expansion, HIP-NN decomposes properties, such as energy, as a sum over hierarchical terms. These terms are generated from a neural network--a composition of many nonlinear transformations--acting on a representation of the molecule. HIP-NN achieves state-of-the-art performance on a dataset of 131k ground state organic molecules, and predicts energies with 0.26 kcal/mol mean absolute error. With minimal tuning, our model is also competitive on a dataset of molecular dynamics trajectories. In addition to enabling accurate energy predictions, the hierarchical structure of HIP-NN helps to identify regions of model uncertainty. version:1
arxiv-1709-10498 | A generalization of the Jensen divergence: The chord gap divergence | http://arxiv.org/abs/1709.10498 | id:1709.10498 author:Frank Nielsen category:cs.LG cs.IT math.IT  published:2017-09-29 summary:We introduce a novel family of distances, called the chord gap divergences, that generalizes the Jensen divergences (also called the Burbea-Rao distances), and study its properties. It follows a generalization of the celebrated statistical Bhattacharyya distance that is frequently met in applications. We report an iterative concave-convex procedure for computing centroids, and analyze the performance of the $k$-means++ clustering with respect to that new dissimilarity measure by introducing the Taylor-Lagrange remainder form of the skew Jensen divergences. version:1
arxiv-1709-10486 | Symbol, Conversational, and Societal Grounding with a Toy Robot | http://arxiv.org/abs/1709.10486 | id:1709.10486 author:Casey Kennington, Sarah Plane category:cs.CL  published:2017-09-29 summary:Essential to meaningful interaction is grounding at the symbolic, conversational, and societal levels. We present ongoing work with Anki's Cozmo toy robot as a research platform where we leverage the recent words-as-classifiers model of lexical semantics in interactive reference resolution tasks for language grounding. version:1
arxiv-1709-10459 | Improving image generative models with human interactions | http://arxiv.org/abs/1709.10459 | id:1709.10459 author:Andrew Kyle Lampinen, David So, Douglas Eck, Fred Bertsch category:cs.CV cs.LG cs.NE  published:2017-09-29 summary:GANs provide a framework for training generative models which mimic a data distribution. However, in many cases we wish to train these generative models to optimize some auxiliary objective function within the data it generates, such as making more aesthetically pleasing images. In some cases, these objective functions are difficult to evaluate, e.g. they may require human interaction. Here, we develop a system for efficiently improving a GAN to target an objective involving human interaction, specifically generating images that increase rates of positive user interactions. To improve the generative model, we build a model of human behavior in the targeted domain from a relatively small set of interactions, and then use this behavioral model as an auxiliary loss function to improve the generative model. We show that this system is successful at improving positive interaction rates, at least on simulated data, and characterize some of the factors that affect its performance. version:1
arxiv-1709-10443 | Adaptive Generation-Based Evolution Control for Gaussian Process Surrogate Models | http://arxiv.org/abs/1709.10443 | id:1709.10443 author:Jakub Repicky, Lukas Bajer, Zbynek Pitra, Martin Holena category:cs.NE  published:2017-09-29 summary:The interest in accelerating black-box optimizers has resulted in several surrogate model-assisted version of the Covariance Matrix Adaptation Evolution Strategy, a state-of-the-art continuous black-box optimizer. The version called Surrogate CMA-ES uses Gaussian processes or random forests surrogate models with a generation-based evolution control. This paper presents an adaptive improvement for S-CMA-ES based on a general procedure introduced with the s*ACM-ES algorithm, in which the number of generations using the surrogate model before retraining is adjusted depending on the performance of the last instance of the surrogate. Three algorithms that differ in the measure of the surrogate model's performance are evaluated on the COCO/BBOB framework. The results show a minor improvement on S-CMA-ES with constant model lifelengths, especially when larger lifelengths are considered. version:1
arxiv-1709-10437 | Optimisation of photometric stereo methods by non-convex variational minimisation | http://arxiv.org/abs/1709.10437 | id:1709.10437 author:Georg Radow, Laurent Hoeltgen, Yvain Quéau, Michael Breuß category:cs.CV 65K10  published:2017-09-29 summary:Estimating shape and appearance of a three dimensional object from a given set of images is a classic research topic that is still actively pursued. Among the various techniques available, PS is distinguished by the assumption that the underlying input images are taken from the same point of view but under different lighting conditions. The most common techniques provide the shape information in terms of surface normals. In this work, we instead propose to minimise a much more natural objective function, namely the reprojection error in terms of depth. Minimising the resulting non-trivial variational model for PS allows to recover the depth of the photographed scene directly. As a solving strategy, we follow an approach based on a recently published optimisation scheme for non-convex and non-smooth cost functions. The main contributions of our paper are of theoretical nature. A technical novelty in our framework is the usage of matrix differential calculus. We supplement our approach by a detailed convergence analysis of the resulting optimisation algorithm and discuss possibilities to ease the computational complexity. At hand of an experimental evaluation we discuss important properties of the method. Overall, our strategy achieves more accurate results than competing approaches. The experiments also highlights some practical aspects of the underlying optimisation algorithm that may be of interest in a more general context. version:1
arxiv-1709-10433 | On the Capacity of Face Representation | http://arxiv.org/abs/1709.10433 | id:1709.10433 author:Sixue Gong, Vishnu Naresh Boddeti, Anil K. Jain category:cs.CV stat.ML  published:2017-09-29 summary:Face recognition is a widely used technology with numerous large-scale applications, such as surveillance, social media and law enforcement. There has been tremendous progress in face recognition accuracy over the past few decades, much of which can be attributed to deep learning based approaches during the last five years. Indeed, automated face recognition systems are now believed to surpass human performance in some scenarios. Despite this progress, a crucial question still remains unanswered: given a face representation, how many identities can it resolve? In other words, what is the capacity of the face representation? A scientific basis for estimating the capacity of a given face representation will not only benefit the evaluation and comparison of different face representation methods, but will also establish an upper bound on the scalability of an automatic face recognition system. We cast the face capacity estimation problem under the information theoretic framework of capacity of a Gaussian noise channel. By explicitly accounting for two sources of representational noise: epistemic (model) uncertainty and aleatoric (data) variability, our approach is able to estimate the capacity of any given face representation. To demonstrate the efficacy of our approach, we estimate the capacity of a 128-dimensional state-of-the-art deep neural network based face representation, FaceNet. Our numerical experiments indicate that, (a) our capacity estimation model yields a capacity upper bound of $1\times10^{12}$ for the FaceNet representation at a false acceptance rate (FAR) of 5%, (b) the capacity reduces drastically as you lower the desired FAR with an estimate of $2\times10^{7}$ and $6\times10^{3}$ at FAR of 0.1% and 0.001%, respectively), and (c) the performance of the FaceNet representation is significantly below the theoretical limit. version:1
arxiv-1709-10432 | Convergence Analysis of Distributed Stochastic Gradient Descent with Shuffling | http://arxiv.org/abs/1709.10432 | id:1709.10432 author:Qi Meng, Wei Chen, Yue Wang, Zhi-Ming Ma, Tie-Yan Liu category:stat.ML cs.LG  published:2017-09-29 summary:When using stochastic gradient descent to solve large-scale machine learning problems, a common practice of data processing is to shuffle the training data, partition the data across multiple machines if needed, and then perform several epochs of training on the re-shuffled (either locally or globally) data. The above procedure makes the instances used to compute the gradients no longer independently sampled from the training data set. Then does the distributed SGD method have desirable convergence properties in this practical situation? In this paper, we give answers to this question. First, we give a mathematical formulation for the practical data processing procedure in distributed machine learning, which we call data partition with global/local shuffling. We observe that global shuffling is equivalent to without-replacement sampling if the shuffling operations are independent. We prove that SGD with global shuffling has convergence guarantee in both convex and non-convex cases. An interesting finding is that, the non-convex tasks like deep learning are more suitable to apply shuffling comparing to the convex tasks. Second, we conduct the convergence analysis for SGD with local shuffling. The convergence rate for local shuffling is slower than that for global shuffling, since it will lose some information if there's no communication between partitioned data. Finally, we consider the situation when the permutation after shuffling is not uniformly distributed (insufficient shuffling), and discuss the condition under which this insufficiency will not influence the convergence rate. Our theoretical results provide important insights to large-scale machine learning, especially in the selection of data processing methods in order to achieve faster convergence and good speedup. Our theoretical findings are verified by extensive experiments on logistic regression and deep neural networks. version:1
arxiv-1708-02286 | Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification | http://arxiv.org/abs/1708.02286 | id:1708.02286 author:Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, Pan Zhou category:cs.CV cs.LG stat.ML  published:2017-08-03 summary:Person Re-Identification (person re-id) is a crucial task as its applications in visual surveillance and human-computer interaction. In this work, we present a novel joint Spatial and Temporal Attention Pooling Network (ASTPN) for video-based person re-identification, which enables the feature extractor to be aware of the current input video sequences, in a way that interdependency from the matching items can directly influence the computation of each other's representation. Specifically, the spatial pooling layer is able to select regions from each frame, while the attention temporal pooling performed can select informative frames over the sequence, both pooling guided by the information from distance matching. Experiments are conduced on the iLIDS-VID, PRID-2011 and MARS datasets and the results demonstrate that this approach outperforms existing state-of-art methods. We also analyze how the joint pooling in both dimensions can boost the person re-id performance more effectively than using either of them separately. version:2
arxiv-1710-00755 | A Study of Cross-domain Generative Models applied to Cartoon Series | http://arxiv.org/abs/1710.00755 | id:1710.00755 author:Eman T. Hassan, David J. Crandall category:cs.CV  published:2017-09-29 summary:We investigate Generative Adversarial Networks (GANs) to model one particular kind of image: frames from TV cartoons. Cartoons are particularly interesting because their visual appearance emphasizes the important semantic information about a scene while abstracting out the less important details, but each cartoon series has a distinctive artistic style that performs this abstraction in different ways. We consider a dataset consisting of images from two popular television cartoon series, Family Guy and The Simpsons. We examine the ability of GANs to generate images from each of these two domains, when trained independently as well as on both domains jointly. We find that generative models may be capable of finding semantic-level correspondences between these two image domains despite the unsupervised setting, even when the training data does not give labeled alignments between them. version:1
arxiv-1709-10381 | Towards Universal Semantic Tagging | http://arxiv.org/abs/1709.10381 | id:1709.10381 author:Lasha Abzianidze, Johan Bos category:cs.CL  published:2017-09-29 summary:The paper proposes the task of universal semantic tagging---tagging word tokens with language-neutral, semantically informative tags. We argue that the task, with its independent nature, contributes to better semantic analysis for wide-coverage multilingual text. We present the initial version of the semantic tagset and show that (a) the tags provide semantically fine-grained information, and (b) they are suitable for cross-lingual semantic parsing. An application of the semantic tagging in the Parallel Meaning Bank supports both of these points as the tags contribute to formal lexical semantics and their cross-lingual projection. As a part of the application, we annotate a small corpus with the semantic tags and present new baseline result for universal semantic tagging. version:1
arxiv-1706-02829 | Time Series Using Exponential Smoothing Cells | http://arxiv.org/abs/1706.02829 | id:1706.02829 author:Avner Abrami, Aleksandr Y. Aravkin, Younghun Kim category:stat.ML 62F35  65K10  49M15  published:2017-06-09 summary:Time series analysis is used to understand and predict dynamic processes, including evolving demands in business, weather, markets, and biological rhythms. Exponential smoothing is used in all these domains to obtain simple interpretable models of time series and to forecast future values. Despite its popularity, exponential smoothing fails dramatically in the presence of outliers, large amounts of noise, or when the underlying time series changes. We propose a flexible model for time series analysis, using exponential smoothing cells for overlapping time windows. The approach can detect and remove outliers, denoise data, fill in missing observations, and provide meaningful forecasts in challenging situations. In contrast to classic exponential smoothing, which solves a nonconvex optimization problem over the smoothing parameters and initial state, the proposed approach requires solving a single structured convex optimization problem. Recent developments in efficient convex optimization of large-scale dynamic models make the approach tractable. We illustrate new capabilities using synthetic examples, and then use the approach to analyze and forecast noisy real-world time series. Code for the approach and experiments is publicly available. version:4
arxiv-1709-06901 | De-identification of medical records using conditional random fields and long short-term memory networks | http://arxiv.org/abs/1709.06901 | id:1709.06901 author:Zhipeng Jiang, Chao Zhao, Bin He, Yi Guan, Jingchi Jiang category:cs.CL  published:2017-09-20 summary:The CEGS N-GRID 2016 Shared Task 1 in Clinical Natural Language Processing focuses on the de-identification of psychiatric evaluation records. This paper describes two participating systems of our team, based on conditional random fields (CRFs) and long short-term memory networks (LSTMs). A pre-processing module was introduced for sentence detection and tokenization before de-identification. For CRFs, manually extracted rich features were utilized to train the model. For LSTMs, a character-level bi-directional LSTM network was applied to represent tokens and classify tags for each token, following which a decoding layer was stacked to decode the most probable protected health information (PHI) terms. The LSTM-based system attained an i2b2 strict micro-F_1 measure of 89.86%, which was higher than that of the CRF-based system. version:2
arxiv-1709-10354 | A Variational Approach to Shape-from-shading Under Natural Illumination | http://arxiv.org/abs/1709.10354 | id:1709.10354 author:Yvain Quéau, Jean Mélou, Fabien Castan, Daniel Cremers, Jean-Denis Durou category:cs.CV  published:2017-09-29 summary:A numerical solution to shape-from-shading under natural illumination is presented. It builds upon a generic PDE-based formulation which handles directional or spherical harmonics lighting, orthographic or perspective projection, and greylevel or multi-channel images. An augmented Lagrangian solver is proposed, which separates the global, nonlinear optimization problem into a sequence of simpler (either local or linear) ones. Real-world applications to shading-aware depth map denoising, refinement and completion are presented. version:1
arxiv-1709-10323 | A Nonlinear Orthogonal Non-Negative Matrix Factorization Approach to Subspace Clustering | http://arxiv.org/abs/1709.10323 | id:1709.10323 author:Dijana Tolic, Nino Antulov-Fantulin, Ivica Kopriva category:stat.ML cs.LG  published:2017-09-29 summary:A recent theoretical analysis shows the equivalence between non-negative matrix factorization (NMF) and spectral clustering based approach to subspace clustering. As NMF and many of its variants are essentially linear, we introduce a nonlinear NMF with explicit orthogonality and derive general kernel-based orthogonal multiplicative update rules to solve the subspace clustering problem. In nonlinear orthogonal NMF framework, we propose two subspace clustering algorithms, named kernel-based non-negative subspace clustering KNSC-Ncut and KNSC-Rcut and establish their connection with spectral normalized cut and ratio cut clustering. We further extend the nonlinear orthogonal NMF framework and introduce a graph regularization to obtain a factorization that respects a local geometric structure of the data after the nonlinear mapping. The proposed NMF-based approach to subspace clustering takes into account the nonlinear nature of the manifold, as well as its intrinsic local geometry, which considerably improves the clustering performance when compared to the several recently proposed state-of-the-art methods. version:1
arxiv-1709-10297 | Privacy Preserving Identification Using Sparse Approximation with Ambiguization | http://arxiv.org/abs/1709.10297 | id:1709.10297 author:Behrooz Razeghi, Slava Voloshynovskiy, Dimche Kostadinov, Olga Taran category:cs.CR cs.LG stat.ML  published:2017-09-29 summary:In this paper, we consider a privacy preserving encoding framework for identification applications covering biometrics, physical object security and the Internet of Things (IoT). The proposed framework is based on a sparsifying transform, which consists of a trained linear map, an element-wise nonlinearity, and privacy amplification. The sparsifying transform and privacy amplification are not symmetric for the data owner and data user. We demonstrate that the proposed approach is closely related to sparse ternary codes (STC), a recent information-theoretic concept proposed for fast approximate nearest neighbor (ANN) search in high dimensional feature spaces that being machine learning in nature also offers significant benefits in comparison to sparse approximation and binary embedding approaches. We demonstrate that the privacy of the database outsourced to a server as well as the privacy of the data user are preserved at a low computational cost, storage and communication burdens. version:1
arxiv-1709-03124 | A Detail Based Method for Linear Full Reference Image Quality Prediction | http://arxiv.org/abs/1709.03124 | id:1709.03124 author:Elio D. Di Claudio, Giovanni Jacovitti category:cs.CV  published:2017-09-10 summary:In this paper, a novel Full Reference method is proposed for image quality assessment, using the combination of two separate metrics to measure the perceptually distinct impact of detail losses and of spurious details. To this purpose, the gradient of the impaired image is locally decomposed as a predicted version of the original gradient, plus a gradient residual. It is assumed that the detail attenuation identifies the detail loss, whereas the gradient residuals describe the spurious details. It turns out that the perceptual impact of detail losses is roughly linear with the loss of the positional Fisher information, while the perceptual impact of the spurious details is roughly proportional to a logarithmic measure of the signal to residual ratio. The affine combination of these two metrics forms a new index strongly correlated with the empirical Differential Mean Opinion Score (DMOS) for a significant class of image impairments, as verified for three independent popular databases. The method allowed alignment and merging of DMOS data coming from these different databases to a common DMOS scale by affine transformations. Unexpectedly, the DMOS scale setting is possible by the analysis of a single image affected by additive noise. version:2
arxiv-1709-10282 | Deep Competitive Pathway Networks | http://arxiv.org/abs/1709.10282 | id:1709.10282 author:Jia-Ren Chang, Yong-Sheng Chen category:cs.CV  published:2017-09-29 summary:In the design of deep neural architectures, recent studies have demonstrated the benefits of grouping subnetworks into a larger network. For examples, the Inception architecture integrates multi-scale subnetworks and the residual network can be regarded that a residual unit combines a residual subnetwork with an identity shortcut. In this work, we embrace this observation and propose the Competitive Pathway Network (CoPaNet). The CoPaNet comprises a stack of competitive pathway units and each unit contains multiple parallel residual-type subnetworks followed by a max operation for feature competition. This mechanism enhances the model capability by learning a variety of features in subnetworks. The proposed strategy explicitly shows that the features propagate through pathways in various routing patterns, which is referred to as pathway encoding of category information. Moreover, the cross-block shortcut can be added to the CoPaNet to encourage feature reuse. We evaluated the proposed CoPaNet on four object recognition benchmarks: CIFAR-10, CIFAR-100, SVHN, and ImageNet. CoPaNet obtained the state-of-the-art or comparable results using similar amounts of parameters. The code of CoPaNet is available at: https://github.com/JiaRenChang/CoPaNet. version:1
arxiv-1709-10276 | Fast online low-rank tensor subspace tracking by CP decomposition using recursive least squares from incomplete observations | http://arxiv.org/abs/1709.10276 | id:1709.10276 author:Hiroyuki Kasai category:cs.NA stat.ML  published:2017-09-29 summary:We consider the problem of online subspace tracking of a partially observed high-dimensional data stream corrupted by noise, where we assume that the data lie in a low-dimensional linear subspace. This problem is cast as an online low-rank tensor completion problem. We propose a novel online tensor subspace tracking algorithm based on the CANDECOMP/PARAFAC (CP) decomposition, dubbed OnLine Low-rank Subspace tracking by TEnsor CP Decomposition (OLSTEC). The proposed algorithm especially addresses the case in which the subspace of interest is dynamically time-varying. To this end, we build up our proposed algorithm exploiting the recursive least squares (RLS), which is the second-order gradient algorithm. Numerical evaluations on synthetic datasets and real-world datasets such as communication network traffic, environmental data, and surveillance videos, show that the proposed OLSTEC algorithm outperforms state-of-the-art online algorithms in terms of the convergence rate per iteration. version:1
arxiv-1706-03335 | Exploring Automated Essay Scoring for Nonnative English Speakers | http://arxiv.org/abs/1706.03335 | id:1706.03335 author:Amber Nigam category:cs.CL I.2.7  published:2017-06-11 summary:Automated Essay Scoring (AES) has been quite popular and is being widely used. However, lack of appropriate methodology for rating nonnative English speakers' essays has meant a lopsided advancement in this field. In this paper, we report initial results of our experiments with nonnative AES that learns from manual evaluation of nonnative essays. For this purpose, we conducted an exercise in which essays written by nonnative English speakers in test environment were rated both manually and by the automated system designed for the experiment. In the process, we experimented with a few features to learn about nuances linked to nonnative evaluation. The proposed methodology of automated essay evaluation has yielded a correlation coefficient of 0.750 with the manual evaluation. version:3
arxiv-1709-09676 | Lower Bounds on the Bayes Risk of the Bayesian BTL Model with Applications to Random Graphs | http://arxiv.org/abs/1709.09676 | id:1709.09676 author:Mine Alsan, Ranjitha Prasad, Vincent Y. F. Tan category:cs.IT cs.LG math.IT  published:2017-09-27 summary:We consider the problem of aggregating pairwise comparisons to obtain a consensus ranking order over a collection of objects. We employ the popular Bradley-Terry-Luce (BTL) model in which each object is associated with a skill parameter which allows us to probabilistically describe the pairwise comparisons between objects. In particular, we employ the Bayesian BTL model which allows for meaningful prior assumptions and to cope with situations where the number of objects is large and the number of comparisons between some players is small or even zero. For the conventional Bayesian BTL model and the same model with home advantage, we derive information-theoretic lower bounds on the Bayes risk of estimators for norm-based distortion functions. We compare the information-theoretic lower bounds with the Bayesian Cram\'{e}r-Rao lower bounds we derive for the case when the Bayes risk is the mean square error. We draw parallels between pairwise comparisons in the BTL model and inter-player games represented as edges in an Erd\H{o}s-R\'{e}nyi graph and analyze the effect of connectedness and various tree structures on the lower bounds. We illustrate the utility of the bounds through simulations by comparing them with the error performance of an expectation maximization-based inference algorithm for the Bayesian BTL model. version:2
arxiv-1709-10230 | Light Cascaded Convolutional Neural Networks for Accurate Player Detection | http://arxiv.org/abs/1709.10230 | id:1709.10230 author:Keyu Lu, Jianhui Chen, James J. Little, Hangen He category:cs.CV  published:2017-09-29 summary:Vision based player detection is important in sports applications. Accuracy, efficiency, and low memory consumption are desirable for real-time tasks such as intelligent broadcasting and automatic event classification. In this paper, we present a cascaded convolutional neural network (CNN) that satisfies all three of these requirements. Our method first trains a binary (player/non-player) classification network from labeled image patches. Then, our method efficiently applies the network to a whole image in testing. We conducted experiments on basketball and soccer games. Experimental results demonstrate that our method can accurately detect players under challenging conditions such as varying illumination, highly dynamic camera movements and motion blur. Comparing with conventional CNNs, our approach achieves state-of-the-art accuracy on both games with 1000x fewer parameters (i.e., it is light}. version:1
arxiv-1709-10222 | Comparison of PCA with ICA from data distribution perspective | http://arxiv.org/abs/1709.10222 | id:1709.10222 author:Miron Ivanov category:stat.ML cs.LG  published:2017-09-29 summary:We performed an empirical comparison of ICA and PCA algorithms by applying them on two simulated noisy time series with varying distribution parameters and level of noise. In general, ICA shows better results than PCA because it takes into account higher moments of data distribution. On the other hand, PCA remains quite sensitive to the level of correlations among signals. version:1
arxiv-1709-10217 | The First Evaluation of Chinese Human-Computer Dialogue Technology | http://arxiv.org/abs/1709.10217 | id:1709.10217 author:Wei-Nan Zhang, Zhigang Chen, Wanxiang Che, Guoping Hu, Ting Liu category:cs.CL  published:2017-09-29 summary:In this paper, we introduce the first evaluation of Chinese human-computer dialogue technology. We detail the evaluation scheme, tasks, metrics and how to collect and annotate the data for training, developing and test. The evaluation includes two tasks, namely user intent classification and online testing of task-oriented dialogue. To consider the different sources of the data for training and developing, the first task can also be divided into two sub tasks. Both the two tasks are coming from the real problems when using the applications developed by industry. The evaluation data is provided by the iFLYTEK Corporation. Meanwhile, in this paper, we publish the evaluation results to present the current performance of the participants in the two tasks of Chinese human-computer dialogue technology. Moreover, we analyze the existing problems of human-computer dialogue as well as the evaluation scheme itself. version:1
arxiv-1709-09885 | Sentiment Classification with Word Attention based on Weakly Supervised Learning with a Convolutional Neural Network | http://arxiv.org/abs/1709.09885 | id:1709.09885 author:Gichang Lee, Jaeyun Jeong, Seungwan Seo, CzangYeob Kim, Pilsung Kang category:cs.CL  published:2017-09-28 summary:In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is no information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. In order to verify the proposed methodology, we evaluated the classification accuracy and inclusion rate of polarity words using two movie review datasets. Experimental result show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores. version:2
arxiv-1709-10211 | Reservoir Computing using Stochastic p-Bits | http://arxiv.org/abs/1709.10211 | id:1709.10211 author:Samiran Ganguly, Kerem Y. Camsari, Avik W. Ghosh category:cs.ET cond-mat.mes-hall cs.NE  published:2017-09-29 summary:We present a general hardware framework for building networks that directly implement Reservoir Computing, a popular software method for implementing and training Recurrent Neural Networks and are particularly suited for temporal inferencing and pattern recognition. We provide a specific example of a candidate hardware unit based on a combination of soft-magnets, spin-orbit materials and CMOS transistors that can implement these networks. Efficient non von-Neumann hardware implementation of reservoir computers can open up a pathway for integration of temporal Neural Networks in a wide variety of emerging systems such as Internet of Things (IoTs), industrial controls, bio- and photo-sensors, and self-driving automotives. version:1
arxiv-1709-10197 | Fast Barcode Retrieval for Consensus Contouring | http://arxiv.org/abs/1709.10197 | id:1709.10197 author:H. R. Tizhoosh, G. J. Czarnota category:cs.CV  published:2017-09-28 summary:Marking tumors and organs is a challenging task suffering from both inter- and intra-observer variability. The literature quantifies observer variability by generating consensus among multiple experts when they mark the same image. Automatically building consensus contours to establish quality assurance for image segmentation is presently absent in the clinical practice. As the \emph{big data} becomes more and more available, techniques to access a large number of existing segments of multiple experts becomes possible. Fast algorithms are, hence, required to facilitate the search for similar cases. The present work puts forward a potential framework that tested with small datasets (both synthetic and real images) displays the reliability of finding similar images. In this paper, the idea of content-based barcodes is used to retrieve similar cases in order to build consensus contours in medical image segmentation. This approach may be regarded as an extension of the conventional atlas-based segmentation that generally works with rather small atlases due to required computational expenses. The fast segment-retrieval process via barcodes makes it possible to create and use large atlases, something that directly contributes to the quality of the consensus building. Because the accuracy of experts' contours must be measured, we first used 500 synthetic prostate images with their gold markers and delineations by 20 simulated users. The fast barcode-guided computed consensus delivered an average error of $8\%\!\pm\!5\%$ compared against the gold standard segments. Furthermore, we used magnetic resonance images of prostates from 15 patients delineated by 5 oncologists and selected the best delineations to serve as the gold-standard segments. The proposed barcode atlas achieved a Jaccard overlap of $87\%\!\pm\!9\%$ with the contours of the gold-standard segments. version:1
arxiv-1709-10191 | Jointly Trained Sequential Labeling and Classification by Sparse Attention Neural Networks | http://arxiv.org/abs/1709.10191 | id:1709.10191 author:Mingbo Ma, Kai Zhao, Liang Huang, Bing Xiang, Bowen Zhou category:cs.CL  published:2017-09-28 summary:Sentence-level classification and sequential labeling are two fundamental tasks in language understanding. While these two tasks are usually modeled separately, in reality, they are often correlated, for example in intent classification and slot filling, or in topic classification and named-entity recognition. In order to utilize the potential benefits from their correlations, we propose a jointly trained model for learning the two tasks simultaneously via Long Short-Term Memory (LSTM) networks. This model predicts the sentence-level category and the word-level label sequence from the stepwise output hidden representations of LSTM. We also introduce a novel mechanism of "sparse attention" to weigh words differently based on their semantic relevance to sentence-level classification. The proposed method outperforms baseline models on ATIS and TREC datasets. version:1
arxiv-1709-10190 | Unified Deep Supervised Domain Adaptation and Generalization | http://arxiv.org/abs/1709.10190 | id:1709.10190 author:Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, Gianfranco Doretto category:cs.CV  published:2017-09-28 summary:This work provides a unified framework for addressing the problem of visual supervised domain adaptation and generalization with deep models. The main idea is to exploit the Siamese architecture to learn an embedding subspace that is discriminative, and where mapped visual domains are semantically aligned and yet maximally separated. The supervised setting becomes attractive especially when only few target data samples need to be labeled. In this scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by reverting to point-wise surrogates of distribution distances and similarities provides an effective solution. In addition, the approach has a high speed of adaptation, which requires an extremely low number of labeled target training samples, even one per category can be effective. The approach is extended to domain generalization. For both applications the experiments show very promising results. version:1
arxiv-1709-10180 | Possibilistic Fuzzy Local Information C-Means for Sonar Image Segmentation | http://arxiv.org/abs/1709.10180 | id:1709.10180 author:Alina Zare, Nicholas Young, Daniel Suen, Thomas Nabelek, Aquila Galusha, James Keller category:cs.CV  published:2017-09-28 summary:Side-look synthetic aperture sonar (SAS) can produce very high quality images of the sea-floor. When viewing this imagery, a human observer can often easily identify various sea-floor textures such as sand ripple, hard-packed sand, sea grass and rock. In this paper, we present the Possibilistic Fuzzy Local Information C-Means (PFLICM) approach to segment SAS imagery into sea-floor regions that exhibit these various natural textures. The proposed PFLICM method incorporates fuzzy and possibilistic clustering methods and leverages (local) spatial information to perform soft segmentation. Results are shown on several SAS scenes and compared to alternative segmentation approaches. version:1
arxiv-1709-10177 | Recognition of feature curves on 3D shapes using an algebraic approach to Hough transforms | http://arxiv.org/abs/1709.10177 | id:1709.10177 author:Maria-Laura Torrente, Silvia Biasotti, Bianca Falcidieno category:cs.CV 8U05  65D18  published:2017-09-28 summary:Feature curves are largely adopted to highlight shape features, such as sharp lines, or to divide surfaces into meaningful segments, like convex or concave regions. Extracting these curves is not sufficient to convey prominent and meaningful information about a shape. We have first to separate the curves belonging to features from those caused by noise and then to select the lines, which describe non-trivial portions of a surface. The automatic detection of such features is crucial for the identification and/or annotation of relevant parts of a given shape. To do this, the Hough transform (HT) is a feature extraction technique widely used in image analysis, computer vision and digital image processing, while, for 3D shapes, the extraction of salient feature curves is still an open problem. Thanks to algebraic geometry concepts, the HT technique has been recently extended to include a vast class of algebraic curves, thus proving to be a competitive tool for yielding an explicit representation of the diverse feature lines equations. In the paper, for the first time we apply this novel extension of the HT technique to the realm of 3D shapes in order to identify and localize semantic features like patterns, decorations or anatomical details on 3D objects (both complete and fragments), even in the case of features partially damaged or incomplete. The method recognizes various features, possibly compound, and it selects the most suitable feature profiles among families of algebraic curves. version:1
arxiv-1708-05932 | Fundamental Limits of Weak Recovery with Applications to Phase Retrieval | http://arxiv.org/abs/1708.05932 | id:1708.05932 author:Marco Mondelli, Andrea Montanari category:stat.ML cs.IT math.IT  published:2017-08-20 summary:In phase retrieval we want to recover an unknown signal $\boldsymbol x\in\mathbb C^d$ from $n$ quadratic measurements of the form $y_i = \langle{\boldsymbol a}_i,{\boldsymbol x}\rangle ^2+w_i$ where $\boldsymbol a_i\in \mathbb C^d$ are known sensing vectors and $w_i$ is measurement noise. We ask the following weak recovery question: what is the minimum number of measurements $n$ needed to produce an estimator $\hat{\boldsymbol x}(\boldsymbol y)$ that is positively correlated with the signal $\boldsymbol x$? We consider the case of Gaussian vectors $\boldsymbol a_i$. We prove that - in the high-dimensional limit - a sharp phase transition takes place, and we locate the threshold in the regime of vanishingly small noise. For $n\le d-o(d)$ no estimator can do significantly better than random and achieve a strictly positive correlation. For $n\ge d+o(d)$ a simple spectral estimator achieves a positive correlation. Surprisingly, numerical simulations with the same spectral estimator demonstrate promising performance with realistic sensing matrices. Spectral methods are used to initialize non-convex optimization algorithms in phase retrieval, and our approach can boost the performance in this setting as well. Our impossibility result is based on classical information-theory arguments. The spectral algorithm computes the leading eigenvector of a weighted empirical covariance matrix. We obtain a sharp characterization of the spectral properties of this random matrix using tools from free probability and generalizing a recent result by Lu and Li. Both the upper and lower bound generalize beyond phase retrieval to measurements $y_i$ produced according to a generalized linear model. As a byproduct of our analysis, we compare the threshold of the proposed spectral method with that of a message passing algorithm. version:2
arxiv-1705-08244 | On the mathematics of beauty: beautiful images | http://arxiv.org/abs/1705.08244 | id:1705.08244 author:A. M. Khalili category:cs.CV  published:2017-05-13 summary:In this paper, we will study the simplest kind of beauty that can be found in a simple visual pattern and can be appreciated universally. The proposed model suggest that there is a link between beautiful pattern and a deeper optimisation process between randomness and regularity. Then we show that beautiful patterns need to satisfy a more fundamental law that seeks to deliver the highest amount of information using the same amount of energy. The proposed model is used to classify and generate beautiful patterns. version:2
arxiv-1709-10159 | A Web of Hate: Tackling Hateful Speech in Online Social Spaces | http://arxiv.org/abs/1709.10159 | id:1709.10159 author:Haji Mohammad Saleem, Kelly P Dillon, Susan Benesch, Derek Ruths category:cs.CL  published:2017-09-28 summary:Online social platforms are beset with hateful speech - content that expresses hatred for a person or group of people. Such content can frighten, intimidate, or silence platform users, and some of it can inspire other users to commit violence. Despite widespread recognition of the problems posed by such content, reliable solutions even for detecting hateful speech are lacking. In the present work, we establish why keyword-based methods are insufficient for detection. We then propose an approach to detecting hateful speech that uses content produced by self-identifying hateful communities as training data. Our approach bypasses the expensive annotation process often required to train keyword systems and performs well across several established platforms, making substantial improvements over current state-of-the-art approaches. version:1
arxiv-1709-10152 | L1-norm Kernel PCA | http://arxiv.org/abs/1709.10152 | id:1709.10152 author:Cheolmin Kim, Diego Klabjan category:stat.ML cs.LG  published:2017-09-28 summary:We present the first model and algorithm for L1-norm kernel PCA. While L2-norm kernel PCA has been widely studied, there has been no work on L1-norm kernel PCA. For this non-convex and non-smooth problem, we offer geometric understandings through reformulations and present an efficient algorithm where the kernel trick is applicable. To attest the efficiency of the algorithm, we provide a convergence analysis including linear rate of convergence. Moreover, we prove that the output of our algorithm is a local optimal solution to the L1-norm kernel PCA problem. We also numerically show its robustness when extracting principal components in the presence of influential outliers, as well as its runtime comparability to L2-norm kernel PCA. Lastly, we introduce its application to outlier detection and show that the L1-norm kernel PCA based model outperforms especially for high dimensional data. version:1
arxiv-1709-10142 | Resilient Learning-Based Control for Synchronization of Passive Multi-Agent Systems under Attack | http://arxiv.org/abs/1709.10142 | id:1709.10142 author:Arash Rahnama, Panos J. Antsaklis category:cs.SY math.OC stat.ML  published:2017-09-28 summary:In this paper, we show synchronization for a group of output passive agents that communicate with each other according to an underlying communication graph to achieve a common goal. We propose a distributed event-triggered control framework that will guarantee synchronization and considerably decrease the required communication load on the band-limited network. We define a general Byzantine attack on the event-triggered multi-agent network system and characterize its negative effects on synchronization. The Byzantine agents are capable of intelligently falsifying their data and manipulating the underlying communication graph by altering their respective control feedback weights. We introduce a decentralized detection framework and analyze its steady-state and transient performances. We propose a way of identifying individual Byzantine neighbors and a learning-based method of estimating the attack parameters. Lastly, we propose learning-based control approaches to mitigate the negative effects of the adversarial attack. version:1
arxiv-1709-10128 | Optimal Online Learning with Randomized Feedback Graphs with Application in PUE Attacks in CRN | http://arxiv.org/abs/1709.10128 | id:1709.10128 author:Monireh Dabaghchian, Amir Alipour-Fanid, Kai Zeng, Qingsi Wang, Peter Auer category:cs.NI cs.CR cs.LG  published:2017-09-28 summary:In a cognitive radio network, a secondary user learns the spectrum environment and dynamically accesses the channel where the primary user is inactive. At the same time, a primary user emulation (PUE) attacker can send falsified primary user signals and prevent the secondary user from utilizing the available channel. The best attacking strategies that an attacker can apply have not been well studied. In this paper, for the first time, we study optimal PUE attack strategies by formulating an online learning problem where the attacker needs to dynamically decide the attacking channel in each time slot based on its attacking experience. The challenge in our problem is that since the PUE attack happens in the spectrum sensing phase, the attacker cannot observe the reward on the attacked channel. To address this challenge, we utilize the attacker's observation capability. We propose online learning-based attacking strategies based on the attacker's observation capabilities. Through our analysis, we show that with no observation within the attacking slot, the attacker loses on the regret order, and with the observation of at least one channel, there is a significant improvement on the attacking performance. Observation of multiple channels does not give additional benefit to the attacker (only a constant scaling) though it gives insight on the number of observations required to achieve the minimum constant factor. Our proposed algorithms are optimal in the sense that their regret upper bounds match their corresponding regret lower-bounds. We show consistency between simulation and analytical results under various system parameters. version:1
arxiv-1709-10106 | What is the Machine Learning? | http://arxiv.org/abs/1709.10106 | id:1709.10106 author:Spencer Chang, Timothy Cohen, Bryan Ostdiek category:hep-ph physics.data-an stat.ML  published:2017-09-28 summary:Applications of machine learning tools to problems of physical interest are often criticized for producing sensitivity at the expense of transparency. To address this concern, we explore a data planing procedure for identifying combinations of variables -- aided by physical intuition -- that can discriminate signal from background. Weights are introduced to smooth away the features in a given variable(s). New networks are then trained on this modified data. Observed decreases in sensitivity diagnose the variable's discriminating power. Planing also allows the investigation of the linear versus non-linear nature of the boundaries between signal and background. We demonstrate the efficacy of this approach using a toy example, followed by an application to an idealized heavy resonance scenario at the Large Hadron Collider. By unpacking the information being utilized by these algorithms, this method puts in context what it means for a machine to learn. version:1
arxiv-1709-10056 | Introducing DeepBalance: Random Deep Belief Network Ensembles to Address Class Imbalance | http://arxiv.org/abs/1709.10056 | id:1709.10056 author:Peter Xenopoulos category:stat.ML cs.LG  published:2017-09-28 summary:Class imbalance problems manifest in domains such as financial fraud detection or network intrusion analysis, where the prevalence of one class is much higher than another. Typically, practitioners are more interested in predicting the minority class than the majority class as the minority class may carry a higher misclassification cost. However, classifier performance deteriorates in the face of class imbalance as oftentimes classifiers may predict every point as the majority class. Methods for dealing with class imbalance include cost-sensitive learning or resampling techniques. In this paper, we introduce DeepBalance, an ensemble of deep belief networks trained with balanced bootstraps and random feature selection. We demonstrate that our proposed method outperforms baseline resampling methods such as SMOTE and under- and over-sampling in metrics such as AUC and sensitivity when applied to a highly imbalanced financial transaction data. Additionally, we explore performance and training time implications of various model parameters. Furthermore, we show that our model is easily parallelizable, which can reduce training times. Finally, we present an implementation of DeepBalance in R. version:1
arxiv-1709-10053 | Graph Convolutional Networks for Named Entity Recognition | http://arxiv.org/abs/1709.10053 | id:1709.10053 author:A. Cetoli, S. Bragaglia, A. D. O'Harney, M. Sloan category:cs.CL  published:2017-09-28 summary:In this paper we investigate the role of the dependency tree in a named entity recognizer upon using a set of GCN. We perform a comparison among different NER architectures and show that the grammar of a sentence positively influences the results. Experiments on the ontonotes dataset demonstrate consistent performance improvements, without requiring heavy feature engineering nor additional language-specific knowledge. version:1
arxiv-1709-10041 | Bayesian Multi Plate High Throughput Screening of Compounds | http://arxiv.org/abs/1709.10041 | id:1709.10041 author:Ivo D. Shterev, David B. Dunson, Cliburn Chan, Gregory D. Sempowski category:stat.ML  published:2017-09-28 summary:High throughput screening of compounds (chemicals) is an essential part of drug discovery [7], involving thousands to millions of compounds, with the purpose of identifying candidate hits. Most statistical tools, including the industry standard B-score method, work on individual compound plates and do not exploit cross-plate correlation or statistical strength among plates. We present a new statistical framework for high throughput screening of compounds based on Bayesian nonparametric modeling. The proposed approach is able to identify candidate hits from multiple plates simultaneously, sharing statistical strength among plates and providing more robust estimates of compound activity. It can flexibly accommodate arbitrary distributions of compound activities and is applicable to any plate geometry. The algorithm provides a principled statistical approach for hit identification and false discovery rate control. Experiments demonstrate significant improvements in hit identification sensitivity and specificity over the B-score method, which is highly sensitive to threshold choice. The framework is implemented as an efficient R extension package BHTSpack and is suitable for large scale data sets. version:1
arxiv-1709-10030 | Sparse Hierarchical Regression with Polynomials | http://arxiv.org/abs/1709.10030 | id:1709.10030 author:Dimitris Bertsimas, Bart Van Parys category:math.OC cs.LG stat.ML  published:2017-09-28 summary:We present a novel method for exact hierarchical sparse polynomial regression. Our regressor is that degree $r$ polynomial which depends on at most $k$ inputs, counting at most $\ell$ monomial terms, which minimizes the sum of the squares of its prediction errors. The previous hierarchical sparse specification aligns well with modern big data settings where many inputs are not relevant for prediction purposes and the functional complexity of the regressor needs to be controlled as to avoid overfitting. We present a two-step approach to this hierarchical sparse regression problem. First, we discard irrelevant inputs using an extremely fast input ranking heuristic. Secondly, we take advantage of modern cutting plane methods for integer optimization to solve our resulting reduced hierarchical $(k, \ell)$-sparse problem exactly. The ability of our method to identify all $k$ relevant inputs and all $\ell$ monomial terms is shown empirically to experience a phase transition. Crucially, the same transition also presents itself in our ability to reject all irrelevant features and monomials as well. In the regime where our method is statistically powerful, its computational complexity is interestingly on par with Lasso based heuristics. The presented work fills a void in terms of a lack of powerful disciplined nonlinear sparse regression methods in high-dimensional settings. Our method is shown empirically to scale to regression problems with $n\approx 10,000$ observations for input dimension $p\approx 1,000$. version:1
arxiv-1709-10029 | Sparse High-Dimensional Regression: Exact Scalable Algorithms and Phase Transitions | http://arxiv.org/abs/1709.10029 | id:1709.10029 author:Dimitris Bertsimas, Bart Van Parys category:math.OC stat.ML  published:2017-09-28 summary:We present a novel binary convex reformulation of the sparse regression problem that constitutes a new duality perspective. We devise a new cutting plane method and provide evidence that it can solve to provable optimality the sparse regression problem for sample sizes n and number of regressors p in the 100,000s, that is two orders of magnitude better than the current state of the art, in seconds. The ability to solve the problem for very high dimensions allows us to observe new phase transition phenomena. Contrary to traditional complexity theory which suggests that the difficulty of a problem increases with problem size, the sparse regression problem has the property that as the number of samples $n$ increases the problem becomes easier in that the solution recovers 100% of the true signal, and our approach solves the problem extremely fast (in fact faster than Lasso), while for small number of samples n, our approach takes a larger amount of time to solve the problem, but importantly the optimal solution provides a statistically more relevant regressor. We argue that our exact sparse regression approach presents a superior alternative over heuristic methods available at present. version:1
arxiv-1707-09835 | Meta-SGD: Learning to Learn Quickly for Few-Shot Learning | http://arxiv.org/abs/1707.09835 | id:1707.09835 author:Zhenguo Li, Fengwei Zhou, Fei Chen, Hang Li category:cs.LG  published:2017-07-31 summary:Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning. version:2
arxiv-1704-06960 | Translating Neuralese | http://arxiv.org/abs/1704.06960 | id:1704.06960 author:Jacob Andreas, Anca Dragan, Dan Klein category:cs.CL cs.NE  published:2017-04-23 summary:Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language. version:3
arxiv-1709-08728 | Stochastic Nonconvex Optimization with Large Minibatches | http://arxiv.org/abs/1709.08728 | id:1709.08728 author:Weiran Wang, Nathan Srebro category:cs.LG  published:2017-09-25 summary:We study stochastic optimization of nonconvex loss functions, which are typical objectives for training neural networks. We propose stochastic approximation algorithms which optimize a series of regularized, nonlinearized losses on large minibatches of samples, using only first-order gradient information. Our algorithms provably converge to an approximate critical point of the expected objective with faster rates than minibatch stochastic gradient descent, and facilitate better parallelization by allowing larger minibatches. version:2
arxiv-1709-10367 | Structured Embedding Models for Grouped Data | http://arxiv.org/abs/1709.10367 | id:1709.10367 author:Maja Rudolph, Francisco Ruiz, Susan Athey, David Blei category:cs.CL cs.LG stat.ML  published:2017-09-28 summary:Word embeddings are a powerful approach for analyzing language, and exponential family embeddings (EFE) extend them to other types of data. Here we develop structured exponential family embeddings (S-EFE), a method for discovering embeddings that vary across related groups of data. We study how the word usage of U.S. Congressional speeches varies across states and party affiliation, how words are used differently across sections of the ArXiv, and how the co-purchase patterns of groceries can vary across seasons. Key to the success of our method is that the groups share statistical information. We develop two sharing strategies: hierarchical modeling and amortization. We demonstrate the benefits of this approach in empirical studies of speeches, abstracts, and shopping baskets. We show how S-EFE enables group-specific interpretation of word usage, and outperforms EFE in predicting held-out data. version:1
arxiv-1705-00581 | Query-adaptive Video Summarization via Quality-aware Relevance Estimation | http://arxiv.org/abs/1705.00581 | id:1705.00581 author:Arun Balajee Vasudevan, Michael Gygli, Anna Volokitin, Luc Van Gool category:cs.CV cs.CL cs.MM  published:2017-05-01 summary:Although the problem of automatic video summarization has recently received a lot of attention, the problem of creating a video summary that also highlights elements relevant to a search query has been less studied. We address this problem by posing query-relevant summarization as a video frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, representative of the entire video, and relevant to a text query. We quantify relevance by measuring the distance between frames and queries in a common textual-visual semantic embedding space induced by a neural network. In addition, we extend the model to capture query-independent properties, such as frame quality. We compare our method against previous state of the art on textual-visual embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. Furthermore, we introduce a new dataset, annotated with diversity and query-specific relevance labels. On this dataset, we train and test our complete model for video summarization and show that it outperforms standard baselines such as Maximal Marginal Relevance. version:2
arxiv-1709-09930 | HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis | http://arxiv.org/abs/1709.09930 | id:1709.09930 author:Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng, Jing Shao, Shuai Yi, Junjie Yan, Xiaogang Wang category:cs.CV  published:2017-09-28 summary:Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems. Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem. In this study, we propose a new attention-based deep neural network, named as HydraPlus-Net (HP-net), that multi-directionally feeds the multi-level attention maps to different feature layers. The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person re-identification. Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-the-art methods on various datasets. version:1
arxiv-1706-06759 | Comicolorization: Semi-Automatic Manga Colorization | http://arxiv.org/abs/1706.06759 | id:1706.06759 author:Chie Furusawa, Kazuyuki Hiroshiba, Keisuke Ogaki, Yuri Odagiri category:cs.CV cs.GR  published:2017-06-21 summary:We developed "Comicolorization", a semi-automatic colorization system for manga images. Given a monochrome manga and reference images as inputs, our system generates a plausible color version of the manga. This is the first work to address the colorization of an entire manga title (a set of manga pages). Our method colorizes a whole page (not a single panel) semi-automatically, with the same color for the same character across multiple panels. To colorize the target character by the color from the reference image, we extract a color feature from the reference and feed it to the colorization network to help the colorization. Our approach employs adversarial loss to encourage the effect of the color features. Optionally, our tool allows users to revise the colorization result interactively. By feeding the color features to our deep colorization network, we accomplish colorization of the entire manga using the desired colors for each panel. version:4
arxiv-1709-09927 | Inference of Personal Attributes from Tweets Using Machine Learning | http://arxiv.org/abs/1709.09927 | id:1709.09927 author:Take Yo, Kazutoshi Sasahara category:cs.CY cs.CL cs.SI  published:2017-09-28 summary:Using machine learning algorithms, including deep learning, we studied the prediction of personal attributes from the text of tweets, such as gender, occupation, and age groups. We applied word2vec to construct word vectors, which were then used to vectorize tweet blocks. The resulting tweet vectors were used as inputs for training models, and the prediction accuracy of those models was examined as a function of the dimension of the tweet vectors and the size of the tweet blacks. The results showed that the machine learning algorithms could predict the three personal attributes of interest with 60-70% accuracy. version:1
arxiv-1709-09888 | Efficient Convolutional Neural Network For Audio Event Detection | http://arxiv.org/abs/1709.09888 | id:1709.09888 author:Matthias Meyer, Lukas Cavigelli, Lothar Thiele category:cs.CV  published:2017-09-28 summary:Wireless distributed systems as used in sensor networks, Internet-of-Things and cyber-physical systems, impose high requirements on resource efficiency. Advanced preprocessing and classification of data at the network edge can help to decrease the communication demand and to reduce the amount of data to be processed centrally. In the area of distributed acoustic sensing, the combination of algorithms with a high classification rate and resource-constraint embedded systems is essential. Unfortunately, algorithms for acoustic event detection have a high memory and computational demand and are not suited for execution at the network edge. This paper addresses these aspects by applying structural optimizations to a convolutional neural network for audio event detection to reduce the memory requirement by a factor of more than 500 and the computational effort by a factor of 2.1 while performing 9.2% better. version:1
arxiv-1709-09883 | The prototype of the HL-LHC magnets monitoring system based on Recurrent Neural Networks and adaptive quantization | http://arxiv.org/abs/1709.09883 | id:1709.09883 author:Maciej Wielgosz, Matej Mertik, Andrzej Skoczeń category:cs.LG physics.acc-ph physics.ins-det  published:2017-09-28 summary:This paper focuses on an examination of an applicability of Recurrent Neural Network models for detecting anomalous behavior of the CERN superconducting magnets. In order to conduct the experiments, the authors designed and implemented an adaptive signal quantization algorithm and a custom GRU-based detector and developed a method for the detector parameters selection. Three different datasets were used for testing the detector. Two artificially generated datasets were used to assess the raw performance of the system whereas the 231 MB dataset composed of the signals acquired from HiLumi magnets was intended for real-life experiments and model training. Several different setups of the developed anomaly detection system were evaluated and compared with state-of-the-art OC-SVM reference model operating on the same data. The OC-SVM model was equipped with a rich set of feature extractors accounting for a range of the input signal properties. It was determined in the course of the experiments that the detector, along with its supporting design methodology, reaches F1 equal or very close to 1 for almost all test sets. Due to the profile of the data, the best_length setup of the detector turned out to perform the best among all five tested configuration schemes of the detection system. The quantization parameters have the biggest impact on the overall performance of the detector with the best values of input/output grid equal to 16 and 8, respectively. The proposed solution of the detection significantly outperformed OC-SVM-based detector in most of the cases, with much more stable performance across all the datasets. version:1
arxiv-1709-09875 | Recognition of Documents in Braille | http://arxiv.org/abs/1709.09875 | id:1709.09875 author:Jomy John category:cs.CV  published:2017-09-28 summary:Visually impaired people are integral part of the society and it has been a must to provide them with means and system through which they may communicate with the world. In this work, I would like to address how computers can be made useful to read the scripts in Braille. The importance of this work is to reduce communication gap between visually impaired people and the society. Braille remains the most popular tactile reading code even in this century. There are numerous amount of literature locked up in Braille. Braille recognition not only reduces time in reading or extracting information from Braille document but also helps people engaged in special education for correcting papers and other school related works. The availability of such a system will enhance communication and collaboration possibilities with visually impaired people. Existing works supports only documents in white either bright or dull in colour. Hardly any work could be traced on hand printed ordinary documents in Braille. version:1
arxiv-1709-09868 | A Generative Model for Score Normalization in Speaker Recognition | http://arxiv.org/abs/1709.09868 | id:1709.09868 author:Albert Swart, Niko Brummer category:stat.ML cs.LG cs.SD eess.AS  published:2017-09-28 summary:We propose a theoretical framework for thinking about score normalization, which confirms that normalization is not needed under (admittedly fragile) ideal conditions. If, however, these conditions are not met, e.g. under data-set shift between training and runtime, our theory reveals dependencies between scores that could be exploited by strategies such as score normalization. Indeed, it has been demonstrated over and over experimentally, that various ad-hoc score normalization recipes do work. We present a first attempt at using probability theory to design a generative score-space normalization model which gives similar improvements to ZT-norm on the text-dependent RSR 2015 database. version:1
arxiv-1709-09843 | Soft Correspondences in Multimodal Scene Parsing | http://arxiv.org/abs/1709.09843 | id:1709.09843 author:Sarah Taghavi Namin, Mohammad Najafi, Mathieu Salzmann, Lars Petersson category:cs.CV  published:2017-09-28 summary:Exploiting multiple modalities for semantic scene parsing has been shown to improve accuracy over the singlemodality scenario. However multimodal datasets often suffer from problems such as data misalignment and label inconsistencies, where the existing methods assume that corresponding regions in two modalities must have identical labels. We propose to address this issue, by formulating multimodal semantic labeling as inference in a CRF and introducing latent nodes to explicitly model inconsistencies between two modalities. These latent nodes allow us not only to leverage information from both domains to improve their labeling, but also to cut the edges between inconsistent regions. We propose to learn intradomain and inter-domain potential functions from training data to avoid hand-tuning of the model parameters. We evaluate our approach on two publicly available datasets containing 2D and 3D data. Thanks to our latent nodes and our learning strategy, our method outperforms the state-of-the-art in both cases. Moreover, in order to highlight the benefits of the geometric information and the potential of our method in simultaneous 2D/3D semantic and geometric inference, we performed simultaneous inference of semantic and geometric classes both in 2D and 3D that led to satisfactory improvements of the labeling results in both datasets. version:1
arxiv-1709-09840 | PSA: A novel optimization algorithm based on survival rules of porcellio scaber | http://arxiv.org/abs/1709.09840 | id:1709.09840 author:Yinyan Zhang, Shuai Li category:cs.NE  published:2017-09-28 summary:Bio-inspired algorithms have received a significant amount of attention in both academic and engineering societies. In this paper, based on the observation of two major survival rules of a species of woodlice, i.e., porcellio scaber, we design and propose an algorithm called the porcellio scaber algorithm (PSA) for solving optimization problems, including differentiable and non-differential ones as well as the case with local optimums. Numerical results based on benchmark problems are presented to validate the efficacy of PSA. version:1
arxiv-1709-09828 | Photorealistic Style Transfer with Screened Poisson Equation | http://arxiv.org/abs/1709.09828 | id:1709.09828 author:Roey Mechrez, Eli Shechtman, Lihi Zelnik-Manor category:cs.CV  published:2017-09-28 summary:Recent work has shown impressive success in transferring painterly style to images. These approaches, however, fall short of photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. In this paper we propose an approach that takes as input a stylized image and makes it more photorealistic. It relies on the Screened Poisson Equation, maintaining the fidelity of the stylized image while constraining the gradients to those of the original input image. Our method is fast, simple, fully automatic and shows positive progress in making a stylized image photorealistic. Our results exhibit finer details and are less prone to artifacts than the state-of-the-art. version:1
arxiv-1709-09820 | Generative Adversarial Mapping Networks | http://arxiv.org/abs/1709.09820 | id:1709.09820 author:Jianbo Guo, Guangxiang Zhu, Jian Li category:cs.LG  published:2017-09-28 summary:Generative Adversarial Networks (GANs) have shown impressive performance in generating photo-realistic images. They fit generative models by minimizing certain distance measure between the real image distribution and the generated data distribution. Several distance measures have been used, such as Jensen-Shannon divergence, $f$-divergence, and Wasserstein distance, and choosing an appropriate distance measure is very important for training the generative network. In this paper, we choose to use the maximum mean discrepancy (MMD) as the distance metric, which has several nice theoretical guarantees. In fact, generative moment matching network (GMMN) (Li, Swersky, and Zemel 2015) is such a generative model which contains only one generator network $G$ trained by directly minimizing MMD between the real and generated distributions. However, it fails to generate meaningful samples on challenging benchmark datasets, such as CIFAR-10 and LSUN. To improve on GMMN, we propose to add an extra network $F$, called mapper. $F$ maps both real data distribution and generated data distribution from the original data space to a feature representation space $\mathcal{R}$, and it is trained to maximize MMD between the two mapped distributions in $\mathcal{R}$, while the generator $G$ tries to minimize the MMD. We call the new model generative adversarial mapping networks (GAMNs). We demonstrate that the adversarial mapper $F$ can help $G$ to better capture the underlying data distribution. We also show that GAMN significantly outperforms GMMN, and is also superior to or comparable with other state-of-the-art GAN based methods on MNIST, CIFAR-10 and LSUN-Bedrooms datasets. version:1
arxiv-1709-08907 | Input-to-Output Gate to Improve RNN Language Models | http://arxiv.org/abs/1709.08907 | id:1709.08907 author:Sho Takase, Jun Suzuki, Masaaki Nagata category:cs.CL  published:2017-09-26 summary:This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models. We refer to our proposed method as Input-to-Output Gate (IOG). IOG has an extremely simple structure, and thus, can be easily combined with any RNN language models. Our experiments on the Penn Treebank and WikiText-2 datasets demonstrate that IOG consistently boosts the performance of several different types of current topline RNN language models. version:2
arxiv-1704-06977 | Sparse Latent Factor Models with Pure Variables for Overlapping Clustering | http://arxiv.org/abs/1704.06977 | id:1704.06977 author:Xin Bing, Florentina Bunea, Yang Ning, Marten Wegkamp category:stat.ME math.ST stat.ML stat.TH  published:2017-04-23 summary:The problem of overlapping variable clustering, ubiquitous in data science, is that of finding overlapping sub-groups of a p-dimensional random vector X, from a sample of size n of observations on X. Typical solutions are algorithmic in nature, and little is known about the statistical guarantees of the estimated clusters, as most algorithms are not model-based. This work introduces a novel method, LOVE, based on a sparse Latent factor model, with correlated factors, and with pure variables, for OVErlapping clustering with statistical guarantees. The model is used to define the population level clusters as groups of those components of X that are associated, via a sparse allocation matrix, with the same unobservable latent factor, and multi-factor association is allowed. Clusters are respectively anchored by components of X, called pure variables, that are associated with only one latent factor. We prove that the existence of pure variables is a sufficient, and almost necessary, assumption for the identifiability of the allocation matrix, in sparse latent factor models. Consequently, model-based clusters can be uniquely defined, and provide a bona fide estimation target. LOVE estimates first the set of pure variables, and the number of clusters, via a novel method that has low computational complexity of order p2. Each cluster, anchored by pure variables, is then further populated with components of X according to the sparse estimates of the allocation matrix. The latter are obtained via a new, computationally efficient, estimation method tailored to the structure of this problem. The combined procedure yields rate-optimal estimates of the allocation matrix and consistent estimators of the number of clusters. version:2
arxiv-1707-09971 | Spectral Method and Regularized MLE Are Both Optimal for Top-$K$ Ranking | http://arxiv.org/abs/1707.09971 | id:1707.09971 author:Yuxin Chen, Jianqing Fan, Cong Ma, Kaizheng Wang category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH  published:2017-07-31 summary:This paper is concerned with the problem of top-$K$ ranking from pairwise comparisons. Given a collection of $n$ items and a few pairwise binary comparisons across them, one wishes to identify the set of $K$ items that receive the highest ranks. To tackle this problem, we adopt the logistic parametric model---the Bradley-Terry-Luce model, where each item is assigned a latent preference score, and where the outcome of each pairwise comparison depends solely on the relative scores of the two items involved. Recent works have made significant progress towards characterizing the performance (e.g. the mean square error for estimating the scores) of several classical methods, including the spectral method and the maximum likelihood estimator (MLE). However, where they stand regarding top-$K$ ranking remains unsettled. We demonstrate that under a random sampling model, the spectral method alone, or the regularized MLE alone, is minimax optimal in terms of the sample complexity---the number of paired comparisons needed to ensure exact top-$K$ identification. This is accomplished via optimal control of the entrywise error of the score estimates. We complement our theoretical studies by numerical experiments, confirming that both methods yield low entrywise errors for estimating the underlying scores. Our theory is established based on a novel leave-one-out trick, which proves effective for analyzing both iterative and non-iterative optimization procedures. Along the way, we derive an elementary eigenvector perturbation bound for probability transition matrices, which parallels the Davis-Kahan $\sin\Theta$ theorem for symmetric matrices. This further allows us to close the gap between the $\ell_2$ error upper bound for the spectral method and the minimax lower limit. version:2
arxiv-1709-09783 | A Deep Neural Network Approach To Parallel Sentence Extraction | http://arxiv.org/abs/1709.09783 | id:1709.09783 author:Francis Grégoire, Philippe Langlais category:cs.CL  published:2017-09-28 summary:Parallel sentence extraction is a task addressing the data sparsity problem found in multilingual natural language processing applications. We propose an end-to-end deep neural network approach to detect translational equivalence between sentences in two different languages. In contrast to previous approaches, which typically rely on multiples models and various word alignment features, by leveraging continuous vector representation of sentences we remove the need of any domain specific feature engineering. Using a siamese bidirectional recurrent neural networks, our results against a strong baseline based on a state-of-the-art parallel sentence extraction system show a significant improvement in both the quality of the extracted parallel sentences and the translation performance of statistical machine translation systems. We believe this study is the first one to investigate deep learning for the parallel sentence extraction task. version:1
arxiv-1709-09780 | Improving Dermoscopic Image Segmentation with Enhanced Convolutional-Deconvolutional Networks | http://arxiv.org/abs/1709.09780 | id:1709.09780 author:Yading Yuan, Yeh-Chi Lo category:cs.CV  published:2017-09-28 summary:Automatic skin lesion segmentation on dermoscopic images is an essential step in computer-aided diagnosis of melanoma. However, this task is challenging due to significant variations of lesion appearances across different patients. This challenge is further exacerbated when dealing with a large amount of image data. In this paper, we extended our previous work by developing a deeper network architecture with smaller kernels to enhance its discriminant capacity. In addition, we explicitly included color information from multiple color spaces to facilitate network training and thus to further improve the segmentation performance. We extensively evaluated our method on the ISBI 2017 skin lesion segmentation challenge. By training with the 2000 challenge training images, our method achieved an average Jaccard Index (JA) of 0.765 on the 600 challenge testing images, which ranked itself in the first place in the challenge version:1
arxiv-1709-09778 | Sublinear-Time Adaptive Data Analysis | http://arxiv.org/abs/1709.09778 | id:1709.09778 author:Benjamin Fish, Lev Reyzin, Benjamin I. P. Rubinstein category:cs.LG cs.DS  published:2017-09-28 summary:The central aim of most fields of data analysis and experimental scientific investigation is to draw valid conclusions from a given data set. But when past inferences guide future inquiries into the same dataset, reaching valid inferences becomes significantly more difficult. In addition to avoiding the overfitting that can result from adaptive analysis, a data analyst often wants to use as little time and data as possible. A recent line of work in the theory community has established mechanisms that provide low generalization error on adaptive queries, yet there remain large gaps between established theoretical results and how data analysis is done in practice. Many practitioners, for instance, successfully employ bootstrapping and related sampling approaches in order to maintain validity and speed up analysis, but prior to this work, no theoretical analysis existed to justify employing such techniques in this adaptive setting. In this paper, we show how these techniques can be used to provably guarantee validity while speeding up analysis. Through this investigation, we initiate the study of sub-linear time mechanisms to answer adaptive queries into datasets. Perhaps surprisingly, we describe mechanisms that provide an exponential speed-up per query over previous mechanisms, without needing to increase the total amount of data needed for low generalization error. We also provide a method for achieving statistically-meaningful responses even when the mechanism is only allowed to see a constant number of samples from the data per query. version:1
arxiv-1709-02540 | The Expressive Power of Neural Networks: A View from the Width | http://arxiv.org/abs/1709.02540 | id:1709.02540 author:Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, Liwei Wang category:cs.LG  published:2017-09-08 summary:The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-$2$) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-$(n+4)$ ReLU networks, where $n$ is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-$n$ ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth is more effective than width for the expressiveness of ReLU networks. version:2
arxiv-1710-00683 | The Dependence of Frequency Distributions on Multiple Meanings of Words, Codes and Signs | http://arxiv.org/abs/1710.00683 | id:1710.00683 author:Xiaoyong Yan, Petter Minnhagen category:cs.CL eess.AS physics.soc-ph  published:2017-09-28 summary:The dependence of the frequency distributions due to multiple meanings of words in a text is investigated by deleting letters. By coding the words with fewer letters the number of meanings per coded word increases. This increase is measured and used as an input in a predictive theory. For a text written in English, the word-frequency distribution is broad and fat-tailed, whereas if the words are only represented by their first letter the distribution becomes exponential. Both distribution are well predicted by the theory, as is the whole sequence obtained by consecutively representing the words by the first L=6,5,4,3,2,1 letters. Comparisons of texts written by Chinese characters and the same texts written by letter-codes are made and the similarity of the corresponding frequency-distributions are interpreted as a consequence of the multiple meanings of Chinese characters. This further implies that the difference of the shape for word-frequencies for an English text written by letters and a Chinese text written by Chinese characters is due to the coding and not to the language per se. version:1
arxiv-1708-05473 | Dilated Deep Residual Network for Image Denoising | http://arxiv.org/abs/1708.05473 | id:1708.05473 author:Tianyang Wang, Mingxuan Sun, Kaoning Hu category:cs.CV  published:2017-08-18 summary:Variations of deep neural networks such as convolutional neural network (CNN) have been successfully applied to image denoising. The goal is to automatically learn a mapping from a noisy image to a clean image given training data consisting of pairs of noisy and clean images. Most existing CNN models for image denoising have many layers. In such cases, the models involve a large amount of parameters and are computationally expensive to train. In this paper, we develop a dilated residual CNN for Gaussian image denoising. Compared with the recently proposed residual denoiser, our method can achieve comparable performance with less computational cost. Specifically, we enlarge receptive field by adopting dilated convolution in residual network, and the dilation factor is set to a certain value. We utilize appropriate zero padding to make the dimension of the output the same as the input. It has been proven that the expansion of receptive field can boost the CNN performance in image classification, and we further demonstrate that it can also lead to competitive performance for denoising problem. Moreover, we present a formula to calculate receptive field size when dilated convolution is incorporated. Thus, the change of receptive field can be interpreted mathematically. To validate the efficacy of our approach, we conduct extensive experiments for both gray and color image denoising with specific or randomized noise levels. Both of the quantitative measurements and the visual results of denoising are promising comparing with state-of-the-art baselines. version:3
arxiv-1709-09761 | The detour problem in a stochastic environment: Tolman revisited | http://arxiv.org/abs/1709.09761 | id:1709.09761 author:Pegah Fakhari, Arash Khodadadi, Jerome Busemeyer category:stat.ML q-bio.NC  published:2017-09-27 summary:We designed a grid world task to study human planning and re-planning behavior in an unknown stochastic environment. In our grid world, participants were asked to travel from a random starting point to a random goal position while maximizing their reward. Because they were not familiar with the environment, they needed to learn its characteristics from experience to plan optimally. Later in the task, we randomly blocked the optimal path to investigate whether and how people adjust their original plans to find a detour. To this end, we developed and compared 12 different models. These models were different on how they learned and represented the environment and how they planned to catch the goal. The majority of our participants were able to plan optimally. We also showed that people were capable of revising their plans when an unexpected event occurred. The result from the model comparison showed that the model-based reinforcement learning approach provided the best account for the data and outperformed heuristics in explaining the behavioral data in the re-planning trials. version:1
arxiv-1710-01249 | A Comparative Study of CNN, BoVW and LBP for Classification of Histopathological Images | http://arxiv.org/abs/1710.01249 | id:1710.01249 author:Meghana Dinesh Kumar, Morteza Babaie, Shujin Zhu, Shivam Kalra, H. R. Tizhoosh category:cs.CV  published:2017-09-27 summary:Despite the progress made in the field of medical imaging, it remains a large area of open research, especially due to the variety of imaging modalities and disease-specific characteristics. This paper is a comparative study describing the potential of using local binary patterns (LBP), deep features and the bag-of-visual words (BoVW) scheme for the classification of histopathological images. We introduce a new dataset, \emph{KIMIA Path960}, that contains 960 histopathology images belonging to 20 different classes (different tissue types). We make this dataset publicly available. The small size of the dataset and its inter- and intra-class variability makes it ideal for initial investigations when comparing image descriptors for search and classification in complex medical imaging cases like histopathology. We investigate deep features, LBP histograms and BoVW to classify the images via leave-one-out validation. The accuracy of image classification obtained using LBP was 90.62\% while the highest accuracy using deep features reached 94.72\%. The dictionary approach (BoVW) achieved 96.50\%. Deep solutions may be able to deliver higher accuracies but they need extensive training with a large number of (balanced) image datasets. version:1
arxiv-1709-09754 | Combining Real-Valued and Binary Gabor-Radon Features for Classification and Search in Medical Imaging Archives | http://arxiv.org/abs/1709.09754 | id:1709.09754 author:Hamed Erfankhah, Mehran Yazdi, H. R. Tizhoosh category:cs.CV  published:2017-09-27 summary:Content-based image retrieval (CBIR) of medical images in large datasets to identify similar images when a query image is given can be very useful in improving the diagnostic decision of the clinical experts and as well in educational scenarios. In this paper, we used two stage classification and retrieval approach to retrieve similar images. First, the Gabor filters are applied to Radon-transformed images to extract features and to train a multi-class SVM. Then based on the classification results and using an extracted Gabor barcode, similar images are retrieved. The proposed method was tested on IRMA dataset which contains more than 14,000 images. Experimental results show the efficiency of our approach in retrieving similar images compared to other Gabor-Radon-oriented methods. version:1
arxiv-1710-01248 | Skin Lesion Segmentation: U-Nets versus Clustering | http://arxiv.org/abs/1710.01248 | id:1710.01248 author:Bill S. Lin, Kevin Michael, Shivam Kalra, H. R. Tizhoosh category:cs.CV  published:2017-09-27 summary:Many automatic skin lesion diagnosis systems use segmentation as a preprocessing step to diagnose skin conditions because skin lesion shape, border irregularity, and size can influence the likelihood of malignancy. This paper presents, examines and compares two different approaches to skin lesion segmentation. The first approach uses U-Nets and introduces a histogram equalization based preprocessing step. The second approach is a C-Means clustering based approach that is much simpler to implement and faster to execute. The Jaccard Index between the algorithm output and hand segmented images by dermatologists is used to evaluate the proposed algorithms. While many recently proposed deep neural networks to segment skin lesions require a significant amount of computational power for training (i.e., computer with GPUs), the main objective of this paper is to present methods that can be used with only a CPU. This severely limits, for example, the number of training instances that can be presented to the U-Net. Comparing the two proposed algorithms, U-Nets achieved a significantly higher Jaccard Index compared to the clustering approach. Moreover, using the histogram equalization for preprocessing step significantly improved the U-Net segmentation results. version:1
arxiv-1710-01247 | Learning Autoencoded Radon Projections | http://arxiv.org/abs/1710.01247 | id:1710.01247 author:Aditya Sriram, Shivam Kalra, H. R. Tizhoosh, Shahryar Rahnamayan category:cs.CV  published:2017-09-27 summary:Autoencoders have been recently used for encoding medical images. In this study, we design and validate a new framework for retrieving medical images by classifying Radon projections, compressed in the deepest layer of an autoencoder. As the autoencoder reduces the dimensionality, a multilayer perceptron (MLP) can be employed to classify the images. The integration of MLP promotes a rather shallow learning architecture which makes the training faster. We conducted a comparative study to examine the capabilities of autoencoders for different inputs such as raw images, Histogram of Oriented Gradients (HOG) and normalized Radon projections. Our framework is benchmarked on IRMA dataset containing $14,410$ x-ray images distributed across $57$ different classes. Experiments show an IRMA error of $313$ (equivalent to $\approx 82\%$ accuracy) outperforming state-of-the-art works on retrieval from IRMA dataset using autoencoders. version:1
arxiv-1709-09749 | KeyVec: Key-semantics Preserving Document Representations | http://arxiv.org/abs/1709.09749 | id:1709.09749 author:Bin Bi, Hao Ma category:cs.CL cs.LG cs.NE  published:2017-09-27 summary:Previous studies have demonstrated the empirical success of word embeddings in various applications. In this paper, we investigate the problem of learning distributed representations for text documents which many machine learning algorithms take as input for a number of NLP tasks. We propose a neural network model, KeyVec, which learns document representations with the goal of preserving key semantics of the input text. It enables the learned low-dimensional vectors to retain the topics and important information from the documents that will flow to downstream tasks. Our empirical evaluations show the superior quality of KeyVec representations in two different document understanding tasks. version:1
arxiv-1709-05584 | Representation Learning on Graphs: Methods and Applications | http://arxiv.org/abs/1709.05584 | id:1709.05584 author:William L. Hamilton, Rex Ying, Jure Leskovec category:cs.SI cs.LG  published:2017-09-17 summary:Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph convolutional networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work. version:2
arxiv-1709-00515 | A convergence analysis of the perturbed compositional gradient flow: averaging principle and normal deviations | http://arxiv.org/abs/1709.00515 | id:1709.00515 author:Wenqing Hu, Chris Junchi Li category:math.PR stat.ML  published:2017-09-02 summary:We consider in this work a system of two stochastic differential equations named the perturbed compositional gradient flow. By introducing a separation of fast and slow scales of the two equations, we show that the limit of the slow motion is given by an averaged ordinary differential equation. We then demonstrate that the deviation of the slow motion from the averaged equation, after proper rescaling, converges to a stochastic process with Gaussian inputs. This indicates that the slow motion can be approximated in the weak sense by a standard perturbed gradient flow or the continuous-time stochastic gradient descent algorithm that solves the optimization problem for a composition of two functions. As an application, the perturbed compositional gradient flow corresponds to the diffusion limit of the Stochastic Composite Gradient Descent (SCGD) algorithm for minimizing a composition of two expected-value functions in the optimization literatures. For the strongly convex case, such an analysis implies that the SCGD algorithm has the same convergence time asymptotic as the classical stochastic gradient descent algorithm. Thus it validates the effectiveness of using the SCGD algorithm in the strongly convex case. version:2
arxiv-1709-07903 | Ensemble Multi-task Gaussian Process Regression with Multiple Latent Processes | http://arxiv.org/abs/1709.07903 | id:1709.07903 author:Weitong Ruan, Eric L. Miller category:stat.ML cs.LG  published:2017-09-22 summary:Multi-task/Multi-output learning seeks to exploit correlation among tasks to enhance performance over learning or solving each task independently. In this paper, we investigate this problem in the context of Gaussian Processes (GPs) and propose a new model which learns a mixture of latent processes by decomposing the covariance matrix into a sum of structured hidden components each of which is controlled by a latent GP over input features and a "weight" over tasks. From this sum structure, we propose a parallelizable parameter learning algorithm with a predetermined initialization for the "weights". We also notice that an ensemble parameter learning approach using mini-batches of training data not only reduces the computation complexity of learning but also improves the regression performance. We evaluate our model on two datasets, the smaller Swiss Jura dataset and another relatively larger ATMS dataset from NOAA. Substantial improvements are observed compared with established alternatives. version:2
arxiv-1709-04024 | Discovering Potential Correlations via Hypercontractivity | http://arxiv.org/abs/1709.04024 | id:1709.04024 author:Hyeji Kim, Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath category:stat.ML  published:2017-09-12 summary:Discovering a correlation from one variable to another variable is of fundamental scientific and practical interest. While existing correlation measures are suitable for discovering average correlation, they fail to discover hidden or potential correlations. To bridge this gap, (i) we postulate a set of natural axioms that we expect a measure of potential correlation to satisfy; (ii) we show that the rate of information bottleneck, i.e., the hypercontractivity coefficient, satisfies all the proposed axioms; (iii) we provide a novel estimator to estimate the hypercontractivity coefficient from samples; and (iv) we provide numerical experiments demonstrating that this proposed estimator discovers potential correlations among various indicators of WHO datasets, is robust in discovering gene interactions from gene expression time series data, and is statistically more powerful than the estimators for other correlation measures in binary hypothesis testing of canonical potential correlations. version:2
arxiv-1709-09641 | Neural Multi-Atlas Label Fusion: Application to Cardiac MR Images | http://arxiv.org/abs/1709.09641 | id:1709.09641 author:Heran Yang, Jian Sun, Huibin Li, Lisheng Wang, Zongben Xu category:cs.CV  published:2017-09-27 summary:Multi-atlas segmentation approach is one of the most widely-used image segmentation techniques in biomedical applications. There are two major challenges in this category of methods, i.e., atlas selection and label fusion. In this paper, we propose a novel multi-atlas segmentation method that formulates multi-atlas segmentation in a deep learning framework for better solving these challenges. The proposed method, dubbed deep fusion net (DFN), is a deep architecture that integrates a feature extraction subnet and a non-local patch-based label fusion (NL-PLF) subnet in a single network. The network parameters are learned by end-to-end training strategy for automatically learning deep features that enable optimal performance in a NL-PLF framework. Besides, the learned deep features are further utilized in defining a similarity measure for atlas selection. We evaluate our proposed method on two public cardiac MR databases of SATA-13 and LV-09 for left ventricle segmentation, and our learned DFNs with extracted deep features for atlas selection at testing phase achieve state-of-the-art accuracies, e.g., 0.833 in averaged Dice metric (ADM) on SATA-13 database and 0.95 in ADM for epicardium segmentation on LV-09 database. Besides, our method is robust to the cross-database evaluation, e.g., the DFN learned on LV-09 database achieves 0.815 in ADM on SATA-13 database. We also test our proposed method on Cardiac Atlas Project (CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, and our method achieves 0.815 in Dice metric, ranking as the highest result on this dataset. version:1
arxiv-1709-09625 | How regularization affects the critical points in linear networks | http://arxiv.org/abs/1709.09625 | id:1709.09625 author:Amirhossein Taghvaei, Jin W. Kim, Prashant G. Mehta category:math.OC cs.LG  published:2017-09-27 summary:This paper is concerned with the problem of representing and learning a linear transformation using a linear neural network. In recent years, there has been a growing interest in the study of such networks in part due to the successes of deep learning. The main question of this body of research and also of this paper pertains to the existence and optimality properties of the critical points of the mean-squared loss function. The primary concern here is the robustness of the critical points with regularization of the loss function. An optimal control model is introduced for this purpose and a learning algorithm (regularized form of backprop) derived for the same using the Hamilton's formulation of optimal control. The formulation is used to provide a complete characterization of the critical points in terms of the solutions of a nonlinear matrix-valued equation, referred to as the characteristic equation. Analytical and numerical tools from bifurcation theory are used to compute the critical points via the solutions of the characteristic equation. The main conclusion is that the critical point diagram can be fundamentally different even with arbitrary small amounts of regularization. version:1
arxiv-1709-09603 | Riemannian approach to batch normalization | http://arxiv.org/abs/1709.09603 | id:1709.09603 author:Minhyung Cho, Jaehyung Lee category:cs.LG  published:2017-09-27 summary:Batch Normalization (BN) has proven to be an effective algorithm for deep neural network training by normalizing the input to each neuron and reducing the internal covariate shift. The space of weight vectors in the BN layer can be naturally interpreted as a Riemannian manifold, which is invariant to linear scaling of weights. Following the intrinsic geometry of this manifold provides a new learning rule that is more efficient and easier to analyze. We also propose intuitive and effective gradient clipping and regularization methods for the proposed algorithm by utilizing the geometry of the manifold. The resulting algorithm consistently outperforms the original BN on various types of network architectures and datasets. version:1
arxiv-1709-01424 | Towards social pattern characterization in egocentric photo-streams | http://arxiv.org/abs/1709.01424 | id:1709.01424 author:Maedeh Aghaei, Mariella Dimiccoli, Cristian Canton Ferrer, Petia Radeva category:cs.CV  published:2017-09-05 summary:Following the increasingly popular trend of social interaction analysis in egocentric vision, this manuscript proposes a new pipeline for automatic social pattern characterization of a wearable photo-camera user, relying on visual analysis of captured egocentric photos. The proposed framework consists of three major steps. The first step is dedicated to social interaction detection where the impact of several social signals is explored. Detected social events are inspected in the second step for categorization into different social meetings. These two steps act at event-level where each potential social event is modeled as a multi-dimensional time-series, whose dimensions correspond to a set of relevant features for each task, and LSTM is employed for time-series classification. The last step of the framework corresponds to the social pattern characterization of the user, where recurrences of the same person across the whole set of social events of the user are clustered to achieve a comprehensive understanding of the diversity and frequency of the social relations of the user. Experimental evaluation over a dataset acquired by a user wearing a photo-camera during a month demonstrates the relevance of the considered features for social interaction analysis, and show promising results on the task of social pattern characterization from egocentric photo-streams. version:2
arxiv-1709-09590 | An attentive neural architecture for joint segmentation and parsing and its application to real estate ads | http://arxiv.org/abs/1709.09590 | id:1709.09590 author:Giannis Bekoulis, Johannes Deleu, Thomas Demeester, Chris Develder category:cs.CL  published:2017-09-27 summary:In this paper we develop a relatively simple and effective neural joint model that performs both segmentation and dependency parsing. While the model arose in the context of a particular application, we believe the general idea could be translated to other settings where both (1) entities have to be identified from text, and (2) dependency relationships between them. The application we focus on here is the recently introduced problem of extracting the structured description, which we name property tree, of a real estate property based on the textual advertisement: convert an ad to a tree-structure indicating the buildings, floors, rooms, etc. and how one is part of another. Previous work on the problem focused on a hand-crafted feature-based pipeline approach comprising two different modules, solving the two subtasks of the structured prediction problem: (1) identify important entities of a property (e.g., rooms) from classifieds and (2) structure them into a tree format. In this work, we propose a new joint model that is able to tackle the two tasks simultaneously and construct the property tree by (i) avoiding the error propagation and (ii) exploiting the interactions between the subtasks. For this purpose, we perform an extensive comparative study of the pipeline methods and the new proposed joint model, reporting an improvement of over three percentage points in the overall edge $F_1$ score of the property tree. Also, we have considered several attention methods, to encourage our model to focus on salient tokens during the construction of the property tree. Thus we experimentally demonstrate the usefulness of attentive neural architectures for the proposed joint model, showcasing a further improvement of two percentage points in edge $F_1$ score for our application. version:1
arxiv-1709-09582 | Connectivity Learning in Multi-Branch Networks | http://arxiv.org/abs/1709.09582 | id:1709.09582 author:Karim Ahmed, Lorenzo Torresani category:cs.LG cs.CV  published:2017-09-27 summary:While much of the work in the design of convolutional networks over the last five years has revolved around the empirical investigation of the importance of depth, filter sizes, and number of feature channels, recent studies have shown that branching, i.e., splitting the computation along parallel but distinct threads and then aggregating their outputs, represents a new promising dimension for significant improvements in performance. To combat the complexity of design choices in multi-branch architectures, prior work has adopted simple strategies, such as a fixed branching factor, the same input being fed to all parallel branches, and an additive combination of the outputs produced by all branches at aggregation points. In this work we remove these predefined choices and propose an algorithm to learn the connections between branches in the network. Instead of being chosen a priori by the human designer, the multi-branch connectivity is learned simultaneously with the weights of the network by optimizing a single loss function defined with respect to the end task. We demonstrate our approach on the problem of multi-class image classification using three different datasets where it yields consistently higher accuracy compared to the state-of-the-art "ResNeXt" multi-branch network given the same learning capacity. version:1
arxiv-1709-09578 | Neural networks for topology optimization | http://arxiv.org/abs/1709.09578 | id:1709.09578 author:Ivan Sosnovik, Ivan Oseledets category:cs.LG math.NA  published:2017-09-27 summary:In this research, we propose a deep learning based approach for speeding up the topology optimization methods. The problem we seek to solve is the layout problem. The main novelty of this work is to state the problem as an image segmentation task. We leverage the power of deep learning methods as the efficient pixel-wise image labeling technique to perform the topology optimization. We introduce convolutional encoder-decoder architecture and the overall approach of solving the above-described problem with high performance. The conducted experiments demonstrate the significant acceleration of the optimization process. The proposed approach has excellent generalization properties. We demonstrate the ability of the application of the proposed model to other problems. The successful results, as well as the drawbacks of the current method, are discussed. version:1
arxiv-1708-05969 | Applying Data Augmentation to Handwritten Arabic Numeral Recognition Using Deep Learning Neural Networks | http://arxiv.org/abs/1708.05969 | id:1708.05969 author:Akm Ashiquzzaman, Abdul Kawsar Tushar, Ashiqur Rahman category:cs.CV  published:2017-08-20 summary:Handwritten character recognition has been the center of research and a benchmark problem in the sector of pattern recognition and artificial intelligence, and it continues to be a challenging research topic. Due to its enormous application many works have been done in this field focusing on different languages. Arabic, being a diversified language has a huge scope of research with potential challenges. A convolutional neural network model for recognizing handwritten numerals in Arabic language is proposed in this paper, where the dataset is subject to various augmentation in order to add robustness needed for deep learning approach. The proposed method is empowered by the presence of dropout regularization to do away with the problem of data overfitting. Moreover, suitable change is introduced in activation function to overcome the problem of vanishing gradient. With these modifications, the proposed system achieves an accuracy of 99.4\% which performs better than every previous work on the dataset. version:4
arxiv-1709-09559 | ANSAC: Adaptive Non-minimal Sample and Consensus | http://arxiv.org/abs/1709.09559 | id:1709.09559 author:Victor Fragoso, Chris Sweeney, Pradeep Sen, Matthew Turk category:cs.CV  published:2017-09-27 summary:While RANSAC-based methods are robust to incorrect image correspondences (outliers), their hypothesis generators are not robust to correct image correspondences (inliers) with positional error (noise). This slows down their convergence because hypotheses drawn from a minimal set of noisy inliers can deviate significantly from the optimal model. This work addresses this problem by introducing ANSAC, a RANSAC-based estimator that accounts for noise by adaptively using more than the minimal number of correspondences required to generate a hypothesis. ANSAC estimates the inlier ratio (the fraction of correct correspondences) of several ranked subsets of candidate correspondences and generates hypotheses from them. Its hypothesis-generation mechanism prioritizes the use of subsets with high inlier ratio to generate high-quality hypotheses. ANSAC uses an early termination criterion that keeps track of the inlier ratio history and terminates when it has not changed significantly for a period of time. The experiments show that ANSAC finds good homography and fundamental matrix estimates in a few iterations, consistently outperforming state-of-the-art methods. version:1
arxiv-1709-09503 | Modeling the Resource Requirements of Convolutional Neural Networks on Mobile Devices | http://arxiv.org/abs/1709.09503 | id:1709.09503 author:Zongqing Lu, Swati Rallapalli, Kevin Chan, Thomas La Porta category:cs.CV  published:2017-09-27 summary:Convolutional Neural Networks (CNNs) have revolutionized the research in computer vision, due to their ability to capture complex patterns, resulting in high inference accuracies. However, the increasingly complex nature of these neural networks means that they are particularly suited for server computers with powerful GPUs. We envision that deep learning applications will be eventually and widely deployed on mobile devices, e.g., smartphones, self-driving cars, and drones. Therefore, in this paper, we aim to understand the resource requirements (time, memory) of CNNs on mobile devices. First, by deploying several popular CNNs on mobile CPUs and GPUs, we measure and analyze the performance and resource usage for every layer of the CNNs. Our findings point out the potential ways of optimizing the performance on mobile devices. Second, we model the resource requirements of the different CNN computations. Finally, based on the measurement, pro ling, and modeling, we build and evaluate our modeling tool, Augur, which takes a CNN configuration (descriptor) as the input and estimates the compute time and resource usage of the CNN, to give insights about whether and how e ciently a CNN can be run on a given mobile platform. In doing so Augur tackles several challenges: (i) how to overcome pro ling and measurement overhead; (ii) how to capture the variance in different mobile platforms with different processors, memory, and cache sizes; and (iii) how to account for the variance in the number, type and size of layers of the different CNN configurations. version:1
arxiv-1709-09500 | Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets | http://arxiv.org/abs/1709.09500 | id:1709.09500 author:Rotem Dror, Gili Baumer, Marina Bogomolov, Roi Reichart category:cs.CL  published:2017-09-27 summary:With the ever-growing amounts of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper, we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction. version:1
arxiv-1709-09490 | Scene Parsing by Weakly Supervised Learning with Image Descriptions | http://arxiv.org/abs/1709.09490 | id:1709.09490 author:Ruimao Zhang, Liang Lin, Guangrun Wang, Meng Wang, Wangmeng Zuo category:cs.CV  published:2017-09-27 summary:This paper investigates a fundamental problem of scene understanding: how to parse a scene image into a structured configuration (i.e., a semantic object hierarchy with object interaction relations). We propose a deep architecture consisting of two networks: i) a convolutional neural network (CNN) extracting the image representation for pixel-wise object labeling and ii) a recursive neural network (RsNN) discovering the hierarchical object structure and the inter-object relations. Rather than relying on elaborative annotations (e.g., manually labeled semantic maps and relations), we train our deep model in a weakly-supervised learning manner by leveraging the descriptive sentences of the training images. Specifically, we decompose each sentence into a semantic tree consisting of nouns and verb phrases, and apply these tree structures to discover the configurations of the training images. Once these scene configurations are determined, then the parameters of both the CNN and RsNN are updated accordingly by back propagation. The entire model training is accomplished through an Expectation-Maximization method. Extensive experiments show that our model is capable of producing meaningful and structured scene configurations, and achieving more favorable scene labeling results on PASCAL VOC 2012 and SYSU-Scenes datasets compared to other state-of-the-art weakly-supervised deep learning methods. In particular, SYSU-Scenes is a dedicated dataset released by us to facilitate further research on scene parsing, which contains more than 5000 scene images with their sentence-based semantic descriptions. version:1
arxiv-1709-09479 | Fast Convolutional Sparse Coding in the Dual Domain | http://arxiv.org/abs/1709.09479 | id:1709.09479 author:Lama Affara, Bernard Ghanem, Peter Wonka category:cs.CV  published:2017-09-27 summary:Convolutional sparse coding (CSC) is an important building block of many computer vision applications ranging from image and video compression to deep learning. We present two contributions to the state of the art in CSC. First, we significantly speed up the computation by proposing a new optimization framework that tackles the problem in the dual domain. Second, we extend the original formulation to higher dimensions in order to process a wider range of inputs, such as color inputs, or HOG features. Our results show a significant speedup compared to the current state of the art in CSC. version:1
arxiv-1709-09443 | Prosodic Features from Large Corpora of Child-Directed Speech as Predictors of the Age of Acquisition of Words | http://arxiv.org/abs/1709.09443 | id:1709.09443 author:Lea Frermann, Michael C. Frank category:cs.CL  published:2017-09-27 summary:The impressive ability of children to acquire language is a widely studied phenomenon, and the factors influencing the pace and patterns of word learning remains a subject of active research. Although many models predicting the age of acquisition of words have been proposed, little emphasis has been directed to the raw input children achieve. In this work we present a comparatively large-scale multi-modal corpus of prosody-text aligned child directed speech. Our corpus contains automatically extracted word-level prosodic features, and we investigate the utility of this information as predictors of age of acquisition. We show that prosody features boost predictive power in a regularized regression, and demonstrate their utility in the context of a multi-modal factorized language models trained and tested on child-directed speech. version:1
arxiv-1709-09429 | FoodNet: Recognizing Foods Using Ensemble of Deep Networks | http://arxiv.org/abs/1709.09429 | id:1709.09429 author:Paritosh Pandey, Akella Deepthi, Bappaditya Mandal, N. B. Puhan category:cs.CV  published:2017-09-27 summary:In this work we propose a methodology for an automatic food classification system which recognizes the contents of the meal from the images of the food. We developed a multi-layered deep convolutional neural network (CNN) architecture that takes advantages of the features from other deep networks and improves the efficiency. Numerous classical handcrafted features and approaches are explored, among which CNNs are chosen as the best performing features. Networks are trained and fine-tuned using preprocessed images and the filter outputs are fused to achieve higher accuracy. Experimental results on the largest real-world food recognition database ETH Food-101 and newly contributed Indian food image database demonstrate the effectiveness of the proposed methodology as compared to many other benchmark deep learned CNN frameworks. version:1
arxiv-1705-05311 | Using Titles vs. Full-text as Source for Automated Semantic Document Annotation | http://arxiv.org/abs/1705.05311 | id:1705.05311 author:Lukas Galke, Florian Mai, Alan Schelten, Dennis Brunsch, Ansgar Scherp category:cs.DL cs.CL  published:2017-05-15 summary:A significant part of the largest Knowledge Graph today, the Linked Open Data cloud, consists of metadata about documents such as publications, news reports, and other media articles. While the widespread access to the document metadata is a tremendous advancement, it is yet not so easy to assign semantic annotations and organize the documents along semantic concepts. Providing semantic annotations like concepts in SKOS thesauri is a classical research topic, but typically it is conducted on the full-text of the documents. For the first time, we offer a systematic comparison of classification approaches to investigate how far semantic annotations can be conducted using just the metadata of the documents such as titles published as labels on the Linked Open Data cloud. We compare the classifications obtained from analyzing the documents' titles with semantic annotations obtained from analyzing the full-text. Apart from the prominent text classification baselines kNN and SVM, we also compare recent techniques of Learning to Rank and neural networks and revisit the traditional methods logistic regression, Rocchio, and Naive Bayes. The results show that across three of our four datasets, the performance of the classifications using only titles reaches over 90% of the quality compared to the classification performance when using the full-text. Thus, conducting document classification by just using the titles is a reasonable approach for automated semantic annotation and opens up new possibilities for enriching Knowledge Graphs. version:2
arxiv-1709-09426 | Leveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction | http://arxiv.org/abs/1709.09426 | id:1709.09426 author:Charles Corbière, Hedi Ben-Younes, Alexandre Ramé, Charles Ollion category:cs.CV  published:2017-09-27 summary:In this paper, we present a method to learn a visual representation adapted for e-commerce products. Based on weakly supervised learning, our model learns from noisy datasets crawled on e-commerce website catalogs and does not require any manual labeling. We show that our representation can be used for downward classification tasks over clothing categories with different levels of granularity. We also demonstrate that the learnt representation is suitable for image retrieval. We achieve nearly state-of-art results on the DeepFashion In-Shop Clothes Retrieval and Categories Attributes Prediction tasks, without using the provided training set. version:1
arxiv-1709-09422 | Light field super resolution through controlled micro-shifts of light field sensor | http://arxiv.org/abs/1709.09422 | id:1709.09422 author:M. Umair Mukati, Bahadir K. Gunturk category:cs.CV  published:2017-09-27 summary:Light field cameras presents new capabilities, such as post-capture refocusing and aperture control, through capturing directional and spatial distribution of light rays in space. Among different light field camera implementations, micro-lens array based light field cameras is a cost-effective and compact approach to capture light field. One drawback of the micro-lens array based light field cameras is low spatial resolution due to the fact that a single sensor is shared to capture both spatial and angular information. To address the low spatial resolution issue, we present a light field imaging approach that captures and fuses multiple light fields to improve the spatial resolution. For each capture, the light field sensor is shifted in fractional micro-lens steps by means of an XY stage for optimal performance. version:1
arxiv-1709-09404 | A Preliminary Study for Building an Arabic Corpus of Pair Questions-Texts from the Web: AQA-Webcorp | http://arxiv.org/abs/1709.09404 | id:1709.09404 author:Wided Bakari, Patrice Bellot, Mahmoud Neji category:cs.CL cs.IR  published:2017-09-27 summary:With the development of electronic media and the heterogeneity of Arabic data on the Web, the idea of building a clean corpus for certain applications of natural language processing, including machine translation, information retrieval, question answer, become more and more pressing. In this manuscript, we seek to create and develop our own corpus of pair's questions-texts. This constitution then will provide a better base for our experimentation step. Thus, we try to model this constitution by a method for Arabic insofar as it recovers texts from the web that could prove to be answers to our factual questions. To do this, we had to develop a java script that can extract from a given query a list of html pages. Then clean these pages to the extent of having a data base of texts and a corpus of pair's question-texts. In addition, we give preliminary results of our proposal method. Some investigations for the construction of Arabic corpus are also presented in this document. version:1
arxiv-1709-09389 | Human Detection for Night Surveillance using Adaptive Background Subtracted Image | http://arxiv.org/abs/1709.09389 | id:1709.09389 author:Yash Khandhediya, Karishma Sav, Vandit Gajjar category:cs.CV  published:2017-09-27 summary:Surveillance based on Computer Vision has become a major necessity in current era. Most of the surveillance systems operate on visible light imaging, but performance based on visible light imaging is limited due to some factors like variation in light intensity during the daytime. The matter of concern lies in the need for processing images in low light, such as in the need of nighttime surveillance. In this paper, we have proposed a novel approach for human detection using FLIR(Forward Looking Infrared) camera. As the principle involves sensing based on thermal radiation in the Near IR Region, it is possible to detect Humans from an image captured using a FLIR camera even in low light. The proposed method for human detection involves processing of Thermal images by using HOG (Histogram of Oriented Gradients) feature extraction technique along with some enhancements. The principle of the proposed technique lies in an adaptive background subtraction algorithm, which works in association with the HOG technique. By means of this method, we are able to reduce execution time, precision and some other parameters, which result in improvement of overall accuracy of the human detection system. version:1
arxiv-1709-01662 | Unsupervised Generative Modeling Using Matrix Product States | http://arxiv.org/abs/1709.01662 | id:1709.01662 author:Zhao-Yu Han, Jun Wang, Heng Fan, Lei Wang, Pan Zhang category:cond-mat.stat-mech cs.LG quant-ph stat.ML  published:2017-09-06 summary:Generative modeling, which learns joint probability distribution from training data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning by utilizing the density matrix renormalization group method which allows dynamic adjusting dimensions of the tensors, and offers an efficient direct sampling approach, Zipper, for generative tasks. We apply our method to generative modeling of several standard datasets including the principled Bars and Stripes, random binary patterns and the MNIST handwritten digits, to illustrate ability of our model, and discuss features as well as drawbacks of our model over popular generative models such as Hopfield model, Boltzmann machines and generative adversarial networks. Our work shed light on many interesting directions for future exploration on the development of quantum-inspired algorithms for unsupervised machine learning, which is of possibility of being realized by a quantum device. version:2
arxiv-1709-09373 | A Bimodal Network Approach to Model Topic Dynamics | http://arxiv.org/abs/1709.09373 | id:1709.09373 author:Luigi Di Caro, Marco Guerzoni, Massimiliano Nuccio, Giovanni Siragusa category:cs.CL  published:2017-09-27 summary:This paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the Latent Dirichlet Allocation (LDA). The main contribution is the conceptualization of the topic dynamics and its formalization and codification into an algorithm. To benchmark the effectiveness of this approach, we propose three indexes which track the transformation of topics over time, their rate of birth and death, and the novelty of their content. Applying the LDA, we test the algorithm both on a controlled experiment and on a corpus of several thousands of scientific papers over a period of more than 100 years which account for the history of the economic thought. version:1
arxiv-1709-02517 | Extreme Sparse Multinomial Logistic Regression: A Fast and Robust Framework for Hyperspectral Image Classification | http://arxiv.org/abs/1709.02517 | id:1709.02517 author:Faxian Cao, Zhijing Yang, Jinchang Ren, Wing-Kuen Ling category:cs.CV  published:2017-09-08 summary:Although the sparse multinomial logistic regression (SMLR) has provided a useful tool for sparse classification, it suffers from inefficacy in dealing with high dimensional features and manually set initial regressor values. This has significantly constrained its applications for hyperspectral image (HSI) classification. In order to tackle these two drawbacks, an extreme sparse multinomial logistic regression (ESMLR) is proposed for effective classification of HSI. First, the HSI dataset is projected to a new feature space with randomly generated weight and bias. Second, an optimization model is established by the Lagrange multiplier method and the dual principle to automatically determine a good initial regressor for SMLR via minimizing the training error and the regressor value. Furthermore, the extended multi-attribute profiles (EMAPs) are utilized for extracting both the spectral and spatial features. A combinational linear multiple features learning (MFL) method is proposed to further enhance the features extracted by ESMLR and EMAPs. Finally, the logistic regression via the variable splitting and the augmented Lagrangian (LORSAL) is adopted in the proposed framework for reducing the computational time. Experiments are conducted on two well-known HSI datasets, namely the Indian Pines dataset and the Pavia University dataset, which have shown the fast and robust performance of the proposed ESMLR framework. version:2
arxiv-1709-09360 | Learning Distributions of Meant Color | http://arxiv.org/abs/1709.09360 | id:1709.09360 author:Lyndon White, Roberto Togneri, Wei Liu, Mohammed Bennamoun category:cs.CL  published:2017-09-27 summary:When a speaker says the name of a color, the color they picture is not necessarily the same as the listener imagines. Color is a grounded semantic task, but that grounding is not a single value as the most recent works on color-generation do. Rather proper understanding of color language requires the capacity to map a sequence of words is to a distribution in color space. To achieve this, we propose a novel GRU based model which when given a color description such as "light greenish blue" produces an estimated probability distribution across HSV color space. This model learns the compositional functioning of each term: "light", "green", "ish" and "blue" in the color name, and show together they determine the shape of the distribution. This allows the prediction of distributions for color description phrases never seen in training. Such capacity is crucial in allowing human computer interaction systems to interpret vague color statements, with particular use in recognising objects described by color. version:1
arxiv-1709-09354 | Generative Adversarial Networks with Inverse Transformation Unit | http://arxiv.org/abs/1709.09354 | id:1709.09354 author:Zhifeng Kong, Shuo Ding category:cs.CV  published:2017-09-27 summary:In this paper we introduce a new structure to Generative Adversarial Networks by adding an inverse transformation unit behind the generator. We present two theorems to claim the convergence of the model, and two conjectures to nonideal situations when the transformation is not bijection. A general survey on models with different transformations was done on the MNIST dataset and the Fashion-MNIST dataset, which shows the transformation does not necessarily need to be bijection. Also, with certain transformations that blurs an image, our model successfully learned to sharpen the images and recover blurred images, which was additionally verified by our measurement of sharpness. version:1
arxiv-1709-09348 | Signature Verification Approach using Fusion of Hybrid Texture Features | http://arxiv.org/abs/1709.09348 | id:1709.09348 author:Ankan Kumar Bhunia, Alireza Alaei, Partha Pratim Roy category:cs.CV  published:2017-09-27 summary:In this paper, a writer-dependent signature verification method is proposed. Two different types of texture features, namely Wavelet and Local Quantized Patterns (LQP) features, are employed to extract two kinds of transform and statistical based information from signature images. For each writer two separate one-class support vector machines (SVMs) corresponding to each set of LQP and Wavelet features are trained to obtain two different authenticity scores for a given signature. Finally, a score level classifier fusion method is used to integrate the scores obtained from the two one-class SVMs to achieve the verification score. In the proposed method only genuine signatures are used to train the one-class SVMs. The proposed signature verification method has been tested using four different publicly available datasets and the results demonstrate the generality of the proposed method. The proposed system outperforms other existing systems in the literature. version:1
arxiv-1709-08855 | Learning to Inpaint for Image Compression | http://arxiv.org/abs/1709.08855 | id:1709.08855 author:Mohammad Haris Baig, Vladlen Koltun, Lorenzo Torresani category:cs.CV  published:2017-09-26 summary:We study the design of deep architectures for lossy image compression. We present two architectural recipes in the context of multi-stage progressive encoders and empirically demonstrate their importance on compression performance. Specifically, we show that: (a) predicting the original image data from residuals in a multi-stage progressive architecture facilitates learning and leads to improved performance at approximating the original content and (b) learning to inpaint (from neighboring image pixels) before performing compression reduces the amount of information that must be stored to achieve a high-quality approximation. Incorporating these design choices in a baseline progressive encoder yields an average reduction of over $60\%$ in file size with similar quality compared to the original residual encoder. version:2
arxiv-1709-09328 | Augmented Robust PCA For Foreground-Background Separation on Noisy, Moving Camera Video | http://arxiv.org/abs/1709.09328 | id:1709.09328 author:Chen Gao, Brian E. Moore, Raj Rao Nadakuditi category:stat.ML cs.CV  published:2017-09-27 summary:This work presents a novel approach for robust PCA with total variation regularization for foreground-background separation and denoising on noisy, moving camera video. Our proposed algorithm registers the raw (possibly corrupted) frames of a video and then jointly processes the registered frames to produce a decomposition of the scene into a low-rank background component that captures the static components of the scene, a smooth foreground component that captures the dynamic components of the scene, and a sparse component that can isolate corruptions and other non-idealities. Unlike existing methods, our proposed algorithm produces a panoramic low-rank component that spans the entire field of view, automatically stitching together corrupted data from partially overlapping scenes. The low-rank portion of our robust PCA model is based on a recently discovered optimal low-rank matrix estimator (OptShrink) that requires no parameter tuning. We demonstrate the performance of our algorithm on both static and moving camera videos corrupted by noise and outliers. version:1
arxiv-1709-09002 | A physical model for efficient ranking in networks | http://arxiv.org/abs/1709.09002 | id:1709.09002 author:Caterina De Bacco, Daniel B. Larremore, Cristopher Moore category:physics.soc-ph cs.LG cs.SI physics.data-an  published:2017-09-03 summary:We present a principled model and algorithm to infer a hierarchical ranking of nodes in directed networks. Unlike other methods such as minimum violation ranking, it assigns real-valued scores to nodes rather than simply ordinal ranks, and it formalizes the assumption that interactions are more likely to occur between individuals with similar ranks. It provides a natural framework for a statistical significance test for distinguishing when the inferred hierarchy is due to the network topology or is instead due to random chance, and it can be used to perform inference tasks such as predicting the existence or direction of edges. The ranking is inferred by solving a linear system of equations, which is sparse if the network is; thus the resulting algorithm is extremely efficient and scalable. We illustrate these findings by analyzing real and synthetic data and show that our method outperforms others, in both speed and accuracy, in recovering the underlying ranks and predicting edge directions. version:2
arxiv-1709-09323 | Pseudo-labels for supervised learning on event-based data | http://arxiv.org/abs/1709.09323 | id:1709.09323 author:Nicholas F. Y. Chen category:cs.CV  published:2017-09-27 summary:In recent years, event-based cameras have seen increased use because it has a high temporal resolution which overcomes motion blurring, high dynamic range which overcomes extreme illumination conditions and low power consumption, making it suitable for platforms such as drones and self-driving cars. While frame-based computer vision is mature due to the large amounts of data and ground truth available, event-based computer vision is still a work in progress as data sets are not as aplenty and ground truth is scarce for tasks such as object detection. In this paper, we suggest a way to overcome the lack of data by introducing a simple method to generate pseudo-labels for event-based data, assuming that the corresponding frame-based data is also available. These pseudo-labels can be treated as ground truth when training on supervised learning algorithms, and we show, for the first time, event-based car detection in a realistic environment at 100 frames per second with an average precision of 40.8% relative to pseudo-ground truth. version:1
arxiv-1709-09304 | Effective Image Retrieval via Multilinear Multi-index Fusion | http://arxiv.org/abs/1709.09304 | id:1709.09304 author:Zhizhong Zhang, Yuan Xie, Wensheng Zhang, Qi Tian category:cs.CV  published:2017-09-27 summary:Multi-index fusion has demonstrated impressive performances in retrieval task by integrating different visual representations in a unified framework. However, previous works mainly consider propagating similarities via neighbor structure, ignoring the high order information among different visual representations. In this paper, we propose a new multi-index fusion scheme for image retrieval. By formulating this procedure as a multilinear based optimization problem, the complementary information hidden in different indexes can be explored more thoroughly. Specially, we first build our multiple indexes from various visual representations. Then a so-called index-specific functional matrix, which aims to propagate similarities, is introduced for updating the original index. The functional matrices are then optimized in a unified tensor space to achieve a refinement, such that the relevant images can be pushed more closer. The optimization problem can be efficiently solved by the augmented Lagrangian method with theoretical convergence guarantee. Unlike the traditional multi-index fusion scheme, our approach embeds the multi-index subspace structure into the new indexes with sparse constraint, thus it has little additional memory consumption in online query stage. Experimental evaluation on three benchmark datasets reveals that the proposed approach achieves the state-of-the-art performance, i.e., N-score 3.94 on UKBench, mAP 94.1\% on Holiday and 62.39\% on Market-1501. version:1
arxiv-1709-09301 | Multi-way Interacting Regression via Factorization Machines | http://arxiv.org/abs/1709.09301 | id:1709.09301 author:Mikhail Yurochkin, XuanLong Nguyen, Nikolaos Vasiloglou category:stat.ML  published:2017-09-27 summary:We propose a Bayesian regression method that accounts for multi-way interactions of arbitrary orders among the predictor variables. Our model makes use of a factorization mechanism for representing the regression coefficients of interactions among the predictors, while the interaction selection is guided by a prior distribution on random hypergraphs, a construction which generalizes the Finite Feature Model. We present a posterior inference algorithm based on Gibbs sampling, and establish posterior consistency of our regression model. Our method is evaluated with extensive experiments on simulated data and demonstrated to be able to identify meaningful interactions in applications in genetics and retail demand forecasting. version:1
arxiv-1709-07109 | Deconvolutional Latent-Variable Model for Text Sequence Matching | http://arxiv.org/abs/1709.07109 | id:1709.07109 author:Dinghan Shen, Yizhe Zhang, Ricardo Henao, Qinliang Su, Lawrence Carin category:cs.CL cs.LG stat.ML  published:2017-09-21 summary:A latent-variable model is introduced for text matching, inferring sentence representations by jointly optimizing generative and discriminative objectives. To alleviate typical optimization challenges in latent-variable models for text, we employ deconvolutional networks as the sequence decoder (generator), providing learned latent codes with more semantic information and better generalization. Our model, trained in an unsupervised manner, yields stronger empirical predictive performance than a decoder based on Long Short-Term Memory (LSTM), with less parameters and considerably faster training. Further, we apply it to text sequence-matching problems. The proposed model significantly outperforms several strong sentence-encoding baselines, especially in the semi-supervised setting. version:2
arxiv-1709-09297 | Dynamic Label Graph Matching for Unsupervised Video Re-Identification | http://arxiv.org/abs/1709.09297 | id:1709.09297 author:Mang Ye, Andy J Ma, Liang Zheng, Jiawei Li, P C Yuen category:cs.CV  published:2017-09-27 summary:Label estimation is an important component in an unsupervised person re-identification (re-ID) system. This paper focuses on cross-camera label estimation, which can be subsequently used in feature learning to learn robust re-ID models. Specifically, we propose to construct a graph for samples in each camera, and then graph matching scheme is introduced for cross-camera labeling association. While labels directly output from existing graph matching methods may be noisy and inaccurate due to significant cross-camera variations, this paper proposes a dynamic graph matching (DGM) method. DGM iteratively updates the image graph and the label estimation process by learning a better feature space with intermediate estimated labels. DGM is advantageous in two aspects: 1) the accuracy of estimated labels is improved significantly with the iterations; 2) DGM is robust to noisy initial training data. Extensive experiments conducted on three benchmarks including the large-scale MARS dataset show that DGM yields competitive performance to fully supervised baselines, and outperforms competing unsupervised learning methods. version:1
arxiv-1709-09283 | Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network | http://arxiv.org/abs/1709.09283 | id:1709.09283 author:Sepideh Hosseinzadeh, Moein Shakeri, Hong Zhang category:cs.CV  published:2017-09-26 summary:In recent years, various shadow detection methods from a single image have been proposed and used in vision systems; however, most of them are not appropriate for the robotic applications due to the expensive time complexity. This paper introduces a fast shadow detection method using a deep learning framework, with a time cost that is appropriate for robotic applications. In our solution, we first obtain a shadow prior map with the help of multi-class support vector machine using statistical features. Then, we use a semantic- aware patch level Convolutional Neural Network architecture that efficiently trains on patch level shadow examples by combining the original image and the shadow prior map. Experiments on benchmark datasets demonstrate the proposed method significantly decreases the time complexity of shadow detection without losing accuracy. version:1
arxiv-1709-09929 | SUBIC: A Supervised Bi-Clustering Approach for Precision Medicine | http://arxiv.org/abs/1709.09929 | id:1709.09929 author:Milad Zafar Nezhad, Dongxiao Zhu, Najibesadat Sadati, Kai Yang, Phillip Levy category:cs.LG stat.ML  published:2017-09-26 summary:Traditional medicine typically applies one-size-fits-all treatment for the entire patient population whereas precision medicine develops tailored treatment schemes for different patient subgroups. The fact that some factors may be more significant for a specific patient subgroup motivates clinicians and medical researchers to develop new approaches to subgroup detection and analysis, which is an effective strategy to personalize treatment. In this study, we propose a novel patient subgroup detection method, called Supervised Biclustring (SUBIC) using convex optimization and apply our approach to detect patient subgroups and prioritize risk factors for hypertension (HTN) in a vulnerable demographic subgroup (African-American). Our approach not only finds patient subgroups with guidance of a clinically relevant target variable but also identifies and prioritizes risk factors by pursuing sparsity of the input variables and encouraging similarity among the input variables and between the input and target variables version:1
arxiv-1709-09274 | Symbolic Analysis-based Reduced Order Markov Modeling of Time Series Data | http://arxiv.org/abs/1709.09274 | id:1709.09274 author:Devesh K Jha, Nurali Virani, Jan Reimann, Abhishek Srivastav, Asok Ray category:stat.ML  published:2017-09-26 summary:This paper presents a technique for reduced-order Markov modeling for compact representation of time-series data. In this work, symbolic dynamics-based tools have been used to infer an approximate generative Markov model. The time-series data are first symbolized by partitioning the continuous measurement space of the signal and then, the discrete sequential data are modeled using symbolic dynamics. In the proposed approach, the size of temporal memory of the symbol sequence is estimated from spectral properties of the resulting stochastic matrix corresponding to a first-order Markov model of the symbol sequence. Then, hierarchical clustering is used to represent the states of the corresponding full-state Markov model to construct a reduced-order or size Markov model with a non-deterministic algebraic structure. Subsequently, the parameters of the reduced-order Markov model are identified from the original model by making use of a Bayesian inference rule. The final model is selected using information-theoretic criteria. The proposed concept is elucidated and validated on two different data sets as examples. The first example analyzes a set of pressure data from a swirl-stabilized combustor, where controlled protocols are used to induce flame instabilities. Variations in the complexity of the derived Markov model represent how the system operating condition changes from a stable to an unstable combustion regime. In the second example, the data set is taken from NASA's data repository for prognostics of bearings on rotating shafts. We show that, even with a very small state-space, the reduced-order models are able to achieve comparable performance and that the proposed approach provides flexibility in the selection of a final model for representation and learning. version:1
arxiv-1709-09254 | Learning to Explain Non-Standard English Words and Phrases | http://arxiv.org/abs/1709.09254 | id:1709.09254 author:Ke Ni, William Yang Wang category:cs.CL  published:2017-09-26 summary:We describe a data-driven approach for automatically explaining new, non-standard English expressions in a given sentence, building on a large dataset that includes 15 years of crowdsourced examples from UrbanDictionary.com. Unlike prior studies that focus on matching keywords from a slang dictionary, we investigate the possibility of learning a neural sequence-to-sequence model that generates explanations of unseen non-standard English expressions given context. We propose a dual encoder approach---a word-level encoder learns the representation of context, and a second character-level encoder to learn the hidden representation of the target non-standard expression. Our model can produce reasonable definitions of new non-standard English expressions given their context with certain confidence. version:1
arxiv-1709-09239 | Predicting Disease-Gene Associations using Cross-Document Graph-based Features | http://arxiv.org/abs/1709.09239 | id:1709.09239 author:Hendrik ter Horst, Matthias Hartung, Roman Klinger, Matthias Zwick, Philipp Cimiano category:cs.CL  published:2017-09-26 summary:In the context of personalized medicine, text mining methods pose an interesting option for identifying disease-gene associations, as they can be used to generate novel links between diseases and genes which may complement knowledge from structured databases. The most straightforward approach to extract such links from text is to rely on a simple assumption postulating an association between all genes and diseases that co-occur within the same document. However, this approach (i) tends to yield a number of spurious associations, (ii) does not capture different relevant types of associations, and (iii) is incapable of aggregating knowledge that is spread across documents. Thus, we propose an approach in which disease-gene co-occurrences and gene-gene interactions are represented in an RDF graph. A machine learning-based classifier is trained that incorporates features extracted from the graph to separate disease-gene pairs into valid disease-gene associations and spurious ones. On the manually curated Genetic Testing Registry, our approach yields a 30 points increase in F1 score over a plain co-occurrence baseline. version:1
arxiv-1709-09227 | Optimizing PID parameters with machine learning | http://arxiv.org/abs/1709.09227 | id:1709.09227 author:Adam Nyberg category:cs.NE  published:2017-09-26 summary:This paper examines the Evolutionary programming (EP) method for optimizing PID parameters. PID is the most common type of regulator within control theory, partly because it's relatively simple and yields stable results for most applications. The p, i and d parameters vary for each application; therefore, choosing the right parameters is crucial for obtaining good results but also somewhat difficult. EP is a derivative-free optimization algorithm which makes it suitable for PID optimization. The experiments in this paper demonstrate the power of EP to solve the problem of optimizing PID parameters without getting stuck in local minimums. version:1
arxiv-1709-09220 | Dataset Construction via Attention for Aspect Term Extraction with Distant Supervision | http://arxiv.org/abs/1709.09220 | id:1709.09220 author:Athanasios Giannakopoulos, Diego Antognini, Claudiu Musat, Andreea Hossmann, Michael Baeriswyl category:cs.CL  published:2017-09-26 summary:Aspect Term Extraction (ATE) detects opinionated aspect terms in sentences or text spans, with the end goal of performing aspect-based sentiment analysis. The small amount of available datasets for supervised ATE and the fact that they cover only a few domains raise the need for exploiting other data sources in new and creative ways. Publicly available review corpora contain a plethora of opinionated aspect terms and cover a larger domain spectrum. In this paper, we first propose a method for using such review corpora for creating a new dataset for ATE. Our method relies on an attention mechanism to select sentences that have a high likelihood of containing actual opinionated aspects. We thus improve the quality of the extracted aspects. We then use the constructed dataset to train a model and perform ATE with distant supervision. By evaluating on human annotated datasets, we prove that our method achieves a significantly improved performance over various unsupervised and supervised baselines. Finally, we prove that sentence selection matters when it comes to creating new datasets for ATE. Specifically, we show that, using a set of selected sentences leads to higher ATE performance compared to using the whole sentence set. version:1
arxiv-1709-09216 | PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference | http://arxiv.org/abs/1709.09216 | id:1709.09216 author:Jonathan H. Huggins, Ryan P. Adams, Tamara Broderick category:stat.CO stat.ML  published:2017-09-26 summary:Generalized linear models (GLMs) -- such as logistic regression, Poisson regression, and robust regression -- provide interpretable models for diverse data types. Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian methods necessary for inference have either failed to scale to large data sets or failed to provide theoretical guarantees on the quality of inference. We propose a new approach based on constructing polynomial approximate sufficient statistics for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound error across computations. We provide theoretical guarantees on the quality of point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic regression using a quadratic approximation and show competitive performance with stochastic gradient descent, MCMC, and the Laplace approximation in terms of speed and multiple measures of accuracy -- including on an advertising data set with 40 million data points and 20,000 covariates. version:1
arxiv-1709-09215 | Understanding Infographics through Textual and Visual Tag Prediction | http://arxiv.org/abs/1709.09215 | id:1709.09215 author:Zoya Bylinskii, Sami Alsheikh, Spandan Madan, Adria Recasens, Kimberli Zhong, Hanspeter Pfister, Fredo Durand, Aude Oliva category:cs.CV  published:2017-09-26 summary:We introduce the problem of visual hashtag discovery for infographics: extracting visual elements from an infographic that are diagnostic of its topic. Given an infographic as input, our computational approach automatically outputs textual and visual elements predicted to be representative of the infographic content. Concretely, from a curated dataset of 29K large infographic images sampled across 26 categories and 391 tags, we present an automated two step approach. First, we extract the text from an infographic and use it to predict text tags indicative of the infographic content. And second, we use these predicted text tags as a supervisory signal to localize the most diagnostic visual elements from within the infographic i.e. visual hashtags. We report performances on a categorization and multi-label tag prediction problem and compare our proposed visual hashtags to human annotations. version:1
arxiv-1709-09161 | EDEN: Evolutionary Deep Networks for Efficient Machine Learning | http://arxiv.org/abs/1709.09161 | id:1709.09161 author:Emmanuel Dufourq, Bruce A. Bassett category:stat.ML cs.LG cs.NE  published:2017-09-26 summary:Deep neural networks continue to show improved performance with increasing depth, an encouraging trend that implies an explosion in the possible permutations of network architectures and hyperparameters for which there is little intuitive guidance. To address this increasing complexity, we propose Evolutionary DEep Networks (EDEN), a computationally efficient neuro-evolutionary algorithm which interfaces to any deep neural network platform, such as TensorFlow. We show that EDEN evolves simple yet successful architectures built from embedding, 1D and 2D convolutional, max pooling and fully connected layers along with their hyperparameters. Evaluation of EDEN across seven image and sentiment classification datasets shows that it reliably finds good networks -- and in three cases achieves state-of-the-art results -- even on a single GPU, in just 6-24 hours. Our study provides a first attempt at applying neuro-evolution to the creation of 1D convolutional networks for sentiment analysis including the optimisation of the embedding layer. version:1
arxiv-1709-09130 | Output Range Analysis for Deep Neural Networks | http://arxiv.org/abs/1709.09130 | id:1709.09130 author:Souradeep Dutta, Susmit Jha, Sriram Sanakaranarayanan, Ashish Tiwari category:cs.LG stat.ML  published:2017-09-26 summary:Deep neural networks (NN) are extensively used for machine learning tasks such as image classification, perception and control of autonomous systems. Increasingly, these deep NNs are also been deployed in high-assurance applications. Thus, there is a pressing need for developing techniques to verify neural networks to check whether certain user-expected properties are satisfied. In this paper, we study a specific verification problem of computing a guaranteed range for the output of a deep neural network given a set of inputs represented as a convex polyhedron. Range estimation is a key primitive for verifying deep NNs. We present an efficient range estimation algorithm that uses a combination of local search and linear programming problems to efficiently find the maximum and minimum values taken by the outputs of the NN over the given input set. In contrast to recently proposed "monolithic" optimization approaches, we use local gradient descent to repeatedly find and eliminate local minima of the function. The final global optimum is certified using a mixed integer programming instance. We implement our approach and compare it with Reluplex, a recently proposed solver for deep neural networks. We demonstrate the effectiveness of the proposed approach for verification of NNs used in automated control as well as those used in classification. version:1
arxiv-1709-01112 | Optimal deep neural networks for sparse recovery via Laplace techniques | http://arxiv.org/abs/1709.01112 | id:1709.01112 author:Steffen Limmer, Slawomir Stanczak category:cs.IT math.IT stat.ML  published:2017-09-04 summary:This paper introduces Laplace techniques for designing a neural network, with the goal of estimating simplex-constraint sparse vectors from compressed measurements. To this end, we recast the problem of MMSE estimation (w.r.t. a pre-defined uniform input distribution) as the problem of computing the centroid of some polytope that results from the intersection of the simplex and an affine subspace determined by the measurements. Owing to the specific structure, it is shown that the centroid can be computed analytically by extending a recent result that facilitates the volume computation of polytopes via Laplace transformations. A main insight of this paper is that the desired volume and centroid computations can be performed by a classical deep neural network comprising threshold functions, rectified linear (ReLU) and rectified polynomial (ReP) activation functions. The proposed construction of a deep neural network for sparse recovery is completely analytic so that time-consuming training procedures are not necessary. Furthermore, we show that the number of layers in our construction is equal to the number of measurements which might enable novel low-latency sparse recovery algorithms for a larger class of signals than that assumed in this paper. To assess the applicability of the proposed uniform input distribution, we showcase the recovery performance on samples that are soft-classification vectors generated by two standard datasets. As both volume and centroid computation are known to be computationally hard, the network width grows exponentially in the worst-case. It can be, however, decreased by inducing sparse connectivity in the neural network via a well-suited basis of the affine subspace. Finally, the presented analytical construction may serve as a viable initialization to be further optimized and trained using particular input datasets at hand. version:2
arxiv-1709-09121 | Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge | http://arxiv.org/abs/1709.09121 | id:1709.09121 author:Ryota Hinami, Tao Mei, Shin'ichi Satoh category:cs.CV  published:2017-09-26 summary:This paper addresses the problem of joint detection and recounting of abnormal events in videos. Recounting of abnormal events, i.e., explaining why they are judged to be abnormal, is an unexplored but critical task in video surveillance, because it helps human observers quickly judge if they are false alarms or not. To describe the events in the human-understandable form for event recounting, learning generic knowledge about visual concepts (e.g., object and action) is crucial. Although convolutional neural networks (CNNs) have achieved promising results in learning such concepts, it remains an open question as to how to effectively use CNNs for abnormal event detection, mainly due to the environment-dependent nature of the anomaly detection. In this paper, we tackle this problem by integrating a generic CNN model and environment-dependent anomaly detectors. Our approach first learns CNN with multiple visual tasks to exploit semantic information that is useful for detecting and recounting abnormal events. By appropriately plugging the model into anomaly detectors, we can detect and recount abnormal events while taking advantage of the discriminative power of CNNs. Our approach outperforms the state-of-the-art on Avenue and UCSD Ped2 benchmarks for abnormal event detection and also produces promising results of abnormal event recounting. version:1
arxiv-1709-09119 | Integration of Japanese Papers Into the DBLP Data Set | http://arxiv.org/abs/1709.09119 | id:1709.09119 author:Paul Christian Sommerhoff category:cs.CL cs.DL  published:2017-09-26 summary:If someone is looking for a certain publication in the field of computer science, the searching person is likely to use the DBLP to find the desired publication. The DBLP data set is continuously extended with new publications, or rather their metadata, for example the names of involved authors, the title and the publication date. While the size of the data set is already remarkable, specific areas can still be improved. The DBLP offers a huge collection of English papers because most papers concerning computer science are published in English. Nevertheless, there are official publications in other languages which are supposed to be added to the data set. One kind of these are Japanese papers. This diploma thesis will show a way to automatically process publication lists of Japanese papers and to make them ready for an import into the DBLP data set. Especially important are the problems along the way of processing, such as transcription handling and Personal Name Matching with Japanese names. version:1
arxiv-1709-09106 | Region-Based Image Retrieval Revisited | http://arxiv.org/abs/1709.09106 | id:1709.09106 author:Ryota Hinami, Yusuke Matsui, Shin'ichi Satoh category:cs.MM cs.CV  published:2017-09-26 summary:Region-based image retrieval (RBIR) technique is revisited. In early attempts at RBIR in the late 90s, researchers found many ways to specify region-based queries and spatial relationships; however, the way to characterize the regions, such as by using color histograms, were very poor at that time. Here, we revisit RBIR by incorporating semantic specification of objects and intuitive specification of spatial relationships. Our contributions are the following. First, to support multiple aspects of semantic object specification (category, instance, and attribute), we propose a multitask CNN feature that allows us to use deep learning technique and to jointly handle multi-aspect object specification. Second, to help users specify spatial relationships among objects in an intuitive way, we propose recommendation techniques of spatial relationships. In particular, by mining the search results, a system can recommend feasible spatial relationships among the objects. The system also can recommend likely spatial relationships by assigned object category names based on language prior. Moreover, object-level inverted indexing supports very fast shortlist generation, and re-ranking based on spatial constraints provides users with instant RBIR experiences. version:1
arxiv-1709-09102 | Adaptive Nonparametric Clustering | http://arxiv.org/abs/1709.09102 | id:1709.09102 author:Kirill Efimov, Larisa Adamyan, Vladimir Spokoiny category:stat.ML  published:2017-09-26 summary:This paper presents a new approach to non-parametric cluster analysis called Adaptive Weights Clustering (AWC). The idea is to identify the clustering structure by checking at different points and for different scales on departure from local homogeneity. The proposed procedure describes the clustering structure in terms of weights \( w_{ij} \) each of them measures the degree of local inhomogeneity for two neighbor local clusters using statistical tests of "no gap" between them. % The procedure starts from very local scale, then the parameter of locality grows by some factor at each step. The method is fully adaptive and does not require to specify the number of clusters or their structure. The clustering results are not sensitive to noise and outliers, the procedure is able to recover different clusters with sharp edges or manifold structure. The method is scalable and computationally feasible. An intensive numerical study shows a state-of-the-art performance of the method in various artificial examples and applications to text data. Our theoretical study states optimal sensitivity of AWC to local inhomogeneity. version:1
arxiv-1709-04849 | Global-Context Neural Machine Translation through Target-Side Attentive Residual Connections | http://arxiv.org/abs/1709.04849 | id:1709.04849 author:Lesly Miculicich Werlen, Nikolaos Pappas, Dhananjay Ram, Andrei Popescu-Belis category:cs.CL  published:2017-09-14 summary:Neural sequence-to-sequence models achieve remarkable performance not only in machine translation (MT) but also in other language processing tasks. One of the reasons for their effectiveness is the ability of their decoder to capture contextual information through its recurrent layer. However, its sequential modeling over-emphasizes the local context, i.e. the previously translated word in the case of MT. As a result, the model ignores important information from the global context of the translation. In this paper, we address this limitation by introducing attentive residual connections from the previously translated words to the output of the decoder, which enable the learning of longer-range dependencies between words. The proposed model can emphasize any of the previously translated words, as opposed to only the last one, gaining access to the global context of the translated text. The model outperforms strong neural MT baselines on three language pairs, as well as a neural language modeling baseline. The analysis of the attention learned by the decoder confirms that it emphasizes a wide context, and reveals resemblance to syntactic-like structures. version:2
arxiv-1709-09550 | Scale Adaptive Clustering of Multiple Structures | http://arxiv.org/abs/1709.09550 | id:1709.09550 author:Xiang Yang, Peter Meer category:cs.CV  published:2017-09-26 summary:We propose the segmentation of noisy datasets into Multiple Inlier Structures with a new Robust Estimator (MISRE). The scale of each individual structure is estimated adaptively from the input data and refined by mean shift, without tuning any parameter in the process, or manually specifying thresholds for different estimation problems. Once all the data points were classified into separate structures, these structures are sorted by their densities with the strongest inlier structures coming out first. Several 2D and 3D synthetic and real examples are presented to illustrate the efficiency, robustness and the limitations of the MISRE algorithm. version:1
arxiv-1709-09077 | Multi-Person Brain Activity Recognition via Comprehensive EEG Signal Analysis | http://arxiv.org/abs/1709.09077 | id:1709.09077 author:Xiang Zhang, Lina Yao, Dalin Zhang, Xianzhi Wang, Quan Z. Sheng, Tao Gu category:cs.HC cs.CV  published:2017-09-26 summary:An electroencephalography (EEG) based brain activity recognition is a fundamental field of study for a number of significant applications such as intention prediction, appliance control, and neurological disease diagnosis in smart home and smart healthcare domains. Existing techniques mostly focus on binary brain activity recognition for a single person, which limits their deployment in wider and complex practical scenarios. Therefore, multi-person and multi-class brain activity recognition has obtained popularity recently. Another challenge faced by brain activity recognition is the low recognition accuracy due to the massive noises and the low signal-to-noise ratio in EEG signals. Moreover, the feature engineering in EEG processing is time-consuming and highly re- lies on the expert experience. In this paper, we attempt to solve the above challenges by proposing an approach which has better EEG interpretation ability via raw Electroencephalography (EEG) signal analysis for multi-person and multi-class brain activity recognition. Specifically, we analyze inter-class and inter-person EEG signal characteristics, based on which to capture the discrepancy of inter-class EEG data. Then, we adopt an Autoencoder layer to automatically refine the raw EEG signals by eliminating various artifacts. We evaluate our approach on both a public and a local EEG datasets and conduct extensive experiments to explore the effect of several factors (such as normalization methods, training data size, and Autoencoder hidden neuron size) on the recognition results. The experimental results show that our approach achieves a high accuracy comparing to competitive state-of-the-art methods, indicating its potential in promoting future research on multi-person EEG recognition. version:1
arxiv-1709-09075 | Automated sub-cortical brain structure segmentation combining spatial and deep convolutional features | http://arxiv.org/abs/1709.09075 | id:1709.09075 author:Kaisar Kushibar, Sergi Valverde, Sandra Gonzalez-Villa, Jose Bernal, Mariano Cabezas, Arnau Oliver, Xavier Llado category:cs.CV  published:2017-09-26 summary:Sub-cortical brain structure segmentation in Magnetic Resonance Images (MRI) has attracted the interest of the research community for a long time because morphological changes in these structures are related to different neurodegenerative disorders. However, manual segmentation of these structures can be tedious and prone to variability, highlighting the need for robust automated segmentation methods. In this paper, we present a novel convolutional neural network based approach for accurate segmentation of the sub-cortical brain structures that combines both convolutional and prior spatial features for improving the segmentation accuracy. In order to increase the accuracy of the automated segmentation, we propose to train the network using a restricted sample selection to force the network to learn the most difficult parts of the structures. We evaluate the accuracy of the proposed method on the public MICCAI 2012 challenge and IBSR 18 datasets, comparing it with different available state-of-the-art methods and other recently proposed deep learning approaches. On the MICCAI 2012 dataset, our method shows an excellent performance comparable to the best challenge participant strategy, while performing significantly better than state-of-the-art techniques such as FreeSurfer and FIRST. On the IBSR 18 dataset, our method also exhibits a significant increase in the performance with respect to not only FreeSurfer and FIRST, but also comparable or better results than other recent deep learning approaches. Moreover, our experiments show that both the addition of the spatial priors and the restricted sampling strategy have a significant effect on the accuracy of the proposed method. In order to encourage the reproducibility and the use of the proposed method, a public version of our approach is available to download for the neuroimaging community. version:1
arxiv-1709-03856 | StarSpace: Embed All The Things! | http://arxiv.org/abs/1709.03856 | id:1709.03856 author:Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, Jason Weston category:cs.CL  published:2017-09-12 summary:We present StarSpace, a general-purpose neural embedding model that can solve a wide variety of problems: labeling tasks such as text classification, ranking tasks such as information retrieval/web search, collaborative filtering-based or content-based recommendation, embedding of multi-relational graphs, and learning word, sentence or document level embeddings. In each case the model works by embedding those entities comprised of discrete features and comparing them against each other -- learning similarities dependent on the task. Empirical results on a number of tasks show that StarSpace is highly competitive with existing methods, whilst also being generally applicable to new cases where those methods are not. version:4
arxiv-1709-09069 | MDP environments for the OpenAI Gym | http://arxiv.org/abs/1709.09069 | id:1709.09069 author:Andreas Kirsch category:cs.LG  published:2017-09-26 summary:The OpenAI Gym provides researchers and enthusiasts with simple to use environments for reinforcement learning. Even the simplest environment have a level of complexity that can obfuscate the inner workings of RL approaches and make debugging difficult. This whitepaper describes a Python framework that makes it very easy to create simple Markov-Decision-Process environments programmatically by specifying state transitions and rewards of deterministic and non-deterministic MDPs in a domain-specific language in Python. It then presents results and visualizations created with this MDP framework. version:1
arxiv-1709-09018 | AutoEncoder by Forest | http://arxiv.org/abs/1709.09018 | id:1709.09018 author:Ji Feng, Zhi-Hua Zhou category:cs.LG stat.ML  published:2017-09-26 summary:Auto-encoding is an important task which is typically realized by deep neural networks (DNNs) such as convolutional neural networks (CNN). In this paper, we propose EncoderForest (abbrv. eForest), the first tree ensemble based auto-encoder. We present a procedure for enabling forests to do backward reconstruction by utilizing the equivalent classes defined by decision paths of the trees, and demonstrate its usage in both supervised and unsupervised setting. Experiments show that, compared with DNN autoencoders, eForest is able to obtain lower reconstruction error with fast training speed, while the model itself is reusable and damage-tolerable. version:1
arxiv-1709-08962 | Multi-layer Visualization for Medical Mixed Reality | http://arxiv.org/abs/1709.08962 | id:1709.08962 author:Séverine Habert, Ma Meng, Pascal Fallavollita, Nassir Navab category:cs.CV  published:2017-09-26 summary:Medical Mixed Reality helps surgeons to contextualize intraoperative data with video of the surgical scene. Nonetheless, the surgical scene and anatomical target are often occluded by surgical instruments and surgeon hands. In this paper and to our knowledge, we propose a multi-layer visualization in Medical Mixed Reality solution which subtly improves a surgeon's visualization by making transparent the occluding objects. As an example scenario, we use an augmented reality C-arm fluoroscope device. A video image is created using a volumetric-based image synthesization technique and stereo-RGBD cameras mounted on the C-arm. From this synthesized view, the background which is occluded by the surgical instruments and surgeon hands is recovered by modifying the volumetric-based image synthesization technique. The occluding objects can, therefore, become transparent over the surgical scene. Experimentation with different augmented reality scenarios yield results demonstrating that the background of the surgical scenes can be recovered with accuracy between 45%-99%. In conclusion, we presented a solution that a Mixed Reality solution for medicine, providing transparency to objects occluding the surgical scene. This work is also the first application of volumetric field for Diminished Reality/ Mixed Reality. version:1
arxiv-1705-00885 | Quantifying the relation between performance and success in soccer | http://arxiv.org/abs/1705.00885 | id:1705.00885 author:Luca Pappalardo, Paolo Cintia category:stat.AP stat.ML H.2.8; J.3  published:2017-05-02 summary:The availability of massive data about sports activities offers nowadays the opportunity to quantify the relation between performance and success. In this study, we analyze more than 6,000 games and 10 million events in six European leagues and investigate this relation in soccer competitions. We discover that a team's position in a competition's final ranking is significantly related to its typical performance, as described by a set of technical features extracted from the soccer data. Moreover we find that, while victory and defeats can be explained by the team's performance during a game, it is difficult to detect draws by using a machine learning approach. We then simulate the outcomes of an entire season of each league only relying on technical data, i.e. excluding the goals scored, exploiting a machine learning model trained on data from past seasons. The simulation produces a team ranking (the PC ranking) which is close to the actual ranking, suggesting that a complex systems' view on soccer has the potential of revealing hidden patterns regarding the relation between performance and success. version:3
arxiv-1709-08924 | UBSegNet: Unified Biometric Region of Interest Segmentation Network | http://arxiv.org/abs/1709.08924 | id:1709.08924 author:Ranjeet Ranjan Jha, Daksh Thapar, Shreyas Malakarjun Patil, Aditya Nigam category:cs.CV  published:2017-09-26 summary:Digital human identity management, can now be seen as a social necessity, as it is essentially required in almost every public sector such as, financial inclusions, security, banking, social networking e.t.c. Hence, in today's rampantly emerging world with so many adversarial entities, relying on a single biometric trait is being too optimistic. In this paper, we have proposed a novel end-to-end, Unified Biometric ROI Segmentation Network (UBSegNet), for extracting region of interest from five different biometric traits viz. face, iris, palm, knuckle and 4-slap fingerprint. The architecture of the proposed UBSegNet consists of two stages: (i) Trait classification and (ii) Trait localization. For these stages, we have used a state of the art region based convolutional neural network (RCNN), comprising of three major parts namely convolutional layers, region proposal network (RPN) along with classification and regression heads. The model has been evaluated over various huge publicly available biometric databases. To the best of our knowledge this is the first unified architecture proposed, segmenting multiple biometric traits. It has been tested over around 5000 * 5 = 25,000 images (5000 images per trait) and produces very good results. Our work on unified biometric segmentation, opens up the vast opportunities in the field of multiple biometric traits based authentication systems. version:1
arxiv-1709-08915 | Telling Cause from Effect using MDL-based Local and Global Regression | http://arxiv.org/abs/1709.08915 | id:1709.08915 author:Alexander Marx, Jilles Vreeken category:stat.ML  published:2017-09-26 summary:We consider the fundamental problem of inferring the causal direction between two univariate numeric random variables $X$ and $Y$ from observational data. The two-variable case is especially difficult to solve since it is not possible to use standard conditional independence tests between the variables. To tackle this problem, we follow an information theoretic approach based on Kolmogorov complexity and use the Minimum Description Length (MDL) principle to provide a practical solution. In particular, we propose a compression scheme to encode local and global functional relations using MDL-based regression. We infer $X$ causes $Y$ in case it is shorter to describe $Y$ as a function of $X$ than the inverse direction. In addition, we introduce Slope, an efficient linear-time algorithm that through thorough empirical evaluation on both synthetic and real world data we show outperforms the state of the art by a wide margin. version:1
arxiv-1709-08898 | Improving a Multi-Source Neural Machine Translation Model with Corpus Extension for Low-Resource Languages | http://arxiv.org/abs/1709.08898 | id:1709.08898 author:Gyu-Hyeon Choi, Jong-Hun Shin, Young-Kil Kim category:cs.CL  published:2017-09-26 summary:In machine translation, we often try to collect resources to improve its performance. However, most of the language pairs don't have enough resources to train machine translation systems. In this paper, we propose to use synthetic methods for extending a low resource corpus and apply it to a multi source neural machine translation model. We showed the improvement of machine translation performance through the corpus extension using the synthetic method. Especially, we focused on how to create source sentences that can make better target sentences, even using synthetic methods. And we found that the corpus extension could also improve the performance of a multi source neural machine translation. We showed the corpus extension and multi source model to be an efficient method for a low-resource language pair. Furthermore, when both methods were used together, we found better machine translation performance. version:1
arxiv-1709-08894 | On the regularization of Wasserstein GANs | http://arxiv.org/abs/1709.08894 | id:1709.08894 author:Henning Petzka, Asja Fischer, Denis Lukovnicov category:stat.ML cs.LG  published:2017-09-26 summary:Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. It was proposed that training can be improved by instead augmenting the loss by a regularization term that penalizes the deviation of the gradient of the critic (as a function of the network's input) from one. We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on toy data sets. version:1
arxiv-1709-08603 | Dense scale selection over space, time and space-time | http://arxiv.org/abs/1709.08603 | id:1709.08603 author:Tony Lindeberg category:cs.CV  published:2017-09-25 summary:Scale selection methods based on local extrema over scale of scale-normalized derivatives have been primarily developed to be applied sparsely --- at image points where the magnitude of a scale-normalized differential expression additionally assumes local extrema over the domain where the data are defined. This paper presents a methodology for performing dense scale selection, so that hypotheses about local characteristic scales in images, temporal signals and video can be computed at every image point and every time moment. A critical problem when designing mechanisms for dense scale selection is that the scale at which scale-normalized differential entities assume local extrema over scale can be strongly dependent on the local order of the locally dominant differential structure. To address this problem, we propose a methodology where local extrema over scale are detected of a quasi quadrature measure involving scale-space derivatives up to order two and propose two independent mechanisms to reduce the phase dependency of the local scale estimates by: (i) introducing a second layer of post-smoothing prior to the detection of local extrema over scale and (ii) performing local phase compensation based on a model of the phase dependency of the local scale estimates depending on the relative strengths between first- vs. second-order differential structure. This general methodology is applied over three types of domains: (i) spatial images, (ii) temporal signals and (iii) spatio-temporal video. Experiments show that the proposed methodology leads to intuitively reasonable results with local scale estimates that reflect variations in the characteristic scale of locally dominant structures over space and time. version:2
arxiv-1709-08868 | Learning Multi-grid Generative ConvNets by Minimal Contrastive Divergence | http://arxiv.org/abs/1709.08868 | id:1709.08868 author:Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, Ying Nian Wu category:stat.ML cs.CV  published:2017-09-26 summary:This paper proposes a minimal contrastive divergence method for learning energy-based generative ConvNet models of images at multiple grids (or scales) simultaneously. For each grid, we learn an energy-based probabilistic model where the energy function is defined by a bottom-up convolutional neural network (ConvNet or CNN). Learning such a model requires generating synthesized examples from the model. Within each iteration of our learning algorithm, for each observed training image, we generate synthesized images at multiple grids by initializing the finite-step MCMC sampling from a minimal 1 x 1 version of the training image. The synthesized image at each subsequent grid is obtained by a finite-step MCMC initialized from the synthesized image generated at the previous coarser grid. After obtaining the synthesized examples, the parameters of the models at multiple grids are updated separately and simultaneously based on the differences between synthesized and observed examples. We call this learning method the multi-grid minimal contrastive divergence. We show that this method can learn realistic energy-based generative ConvNet models, and it outperforms the original contrastive divergence (CD) and persistent CD. version:1
arxiv-1710-01245 | Robust non-local means filter for ultrasound image denoising | http://arxiv.org/abs/1710.01245 | id:1710.01245 author:Hamid Reza Shahdoosti category:cs.CV  published:2017-09-26 summary:This paper introduces a new approach to non-local means image denoising. Instead of using all pixels located in the search window for estimating the value of a pixel, we identify the highly corrupted pixels and assign less weight to these pixels. This method is called robust non-local means. Numerical and subjective evaluations using ultrasound images show good performances of the proposed denoising method in recovering the shape of edges and important detailed components, in comparison to traditional ultrasound image denoising methods version:1
arxiv-1708-07853 | The Parallel Algorithm for the 2-D Discrete Wavelet Transform | http://arxiv.org/abs/1708.07853 | id:1708.07853 author:David Barina, Pavel Najman, Petr Kleparnik, Michal Kula, Pavel Zemcik category:cs.CV  published:2017-08-25 summary:The discrete wavelet transform can be found at the heart of many image-processing algorithms. Until now, the transform on general-purpose processors (CPUs) was mostly computed using a separable lifting scheme. As the lifting scheme consists of a small number of operations, it is preferred for processing using single-core CPUs. However, considering a parallel processing using multi-core processors, this scheme is inappropriate due to a large number of steps. On such architectures, the number of steps corresponds to the number of points that represent the exchange of data. Consequently, these points often form a performance bottleneck. Our approach appropriately rearranges calculations inside the transform, and thereby reduces the number of steps. In other words, we propose a new scheme that is friendly to parallel environments. When evaluating on multi-core CPUs, we consistently overcome the original lifting scheme. The evaluation was performed on 61-core Intel Xeon Phi and 8-core Intel Xeon processors. version:3
arxiv-1709-08398 | Morphable Face Models - An Open Framework | http://arxiv.org/abs/1709.08398 | id:1709.08398 author:Thomas Gerig, Andreas Morel-Forster, Clemens Blumer, Bernhard Egger, Marcel Lüthi, Sandro Schönborn, Thomas Vetter category:cs.CV  published:2017-09-25 summary:In this paper, we present a novel open-source pipeline for face registration based on Gaussian processes as well as an application to face image analysis. Non-rigid registration of faces is significant for many applications in computer vision, such as the construction of 3D Morphable face models (3DMMs). Gaussian Process Morphable Models (GPMMs) unify a variety of non-rigid deformation models with B-splines and PCA models as examples. GPMM separate problem specific requirements from the registration algorithm by incorporating domain-specific adaptions as a prior model. The novelties of this paper are the following: (i) We present a strategy and modeling technique for face registration that considers symmetry, multi-scale and spatially-varying details. The registration is applied to neutral faces and facial expressions. (ii) We release an open-source software framework for registration and model-building, demonstrated on the publicly available BU3D-FE database. The released pipeline also contains an implementation of an Analysis-by-Synthesis model adaption of 2D face images, tested on the Multi-PIE and LFW database. This enables the community to reproduce, evaluate and compare the individual steps of registration to model-building and 3D/2D model fitting. (iii) Along with the framework release, we publish a new version of the Basel Face Model (BFM-2017) with an improved age distribution and an additional facial expression model. version:2
arxiv-1709-08862 | Bayesian Inference of Network Epidemics | http://arxiv.org/abs/1709.08862 | id:1709.08862 author:Ritabrata Dutta, Antonietta Mira, Jukka-Pekka Onnela category:stat.AP stat.CO stat.ML  published:2017-09-26 summary:Infectious diseases are studied to understand their spreading mechanisms, to evaluate control strategies and to predict the risk and course of future outbreaks. Because people only interact with a small number of individuals, and because the structure of these interactions matters for spreading processes, the pairwise relationships between individuals in a population can be usefully represented by a network. Although the underlying processes of transmission are different, the network approach can be used to study the spread of pathogens in a contact network or the spread of rumors in an online social network. We study simulated simple and complex epidemics on synthetic networks and on two empirical networks, a social / contact network in an Indian village and an online social network in the U.S. Our goal is to learn simultaneously about the spreading process parameters and the source node (first infected node) of the epidemic, given a fixed and known network structure, and observations about state of nodes at several points in time. Our inference scheme is based on approximate Bayesian computation (ABC), an inference technique for complex models with likelihood functions that are either expensive to evaluate or analytically intractable. ABC enables us to adopt a Bayesian approach to the problem despite the posterior distribution being very complex. The proposed methodology generally performs well and, somewhat counter-intuitively, the inference problem appears to be easier on more heterogeneous network topologies, which enhances its future applicability to real-world settings where few networks have homogeneous topologies. version:1
arxiv-1709-08858 | Polysemy Detection in Distributed Representation of Word Sense | http://arxiv.org/abs/1709.08858 | id:1709.08858 author:Kana Oomoto, Haruka Oikawa, Eiko Yamamoto, Mitsuo Yoshida, Masayuki Okabe, Kyoji Umemura category:cs.DS cs.CL  published:2017-09-26 summary:In this paper, we propose a statistical test to determine whether a given word is used as a polysemic word or not. The statistic of the word in this test roughly corresponds to the fluctuation in the senses of the neighboring words a nd the word itself. Even though the sense of a word corresponds to a single vector, we discuss how polysemy of the words affects the position of vectors. Finally, we also explain the method to detect this effect. version:1
arxiv-1709-08842 | Learning a Predictive Model for Music Using PULSE | http://arxiv.org/abs/1709.08842 | id:1709.08842 author:Jonas Langhabel category:cs.LG cs.SD eess.AS  published:2017-09-26 summary:Predictive models for music are studied by researchers of algorithmic composition, the cognitive sciences and machine learning. They serve as base models for composition, can simulate human prediction and provide a multidisciplinary application domain for learning algorithms. A particularly well established and constantly advanced subtask is the prediction of monophonic melodies. As melodies typically involve non-Markovian dependencies their prediction requires a capable learning algorithm. In this thesis, I apply the recent feature discovery and learning method PULSE to the realm of symbolic music modeling. PULSE is comprised of a feature generating operation and L1-regularized optimization. These are used to iteratively expand and cull the feature set, effectively exploring feature spaces that are too large for common feature selection approaches. I design a general Python framework for PULSE, propose task-optimized feature generating operations and various music-theoretically motivated features that are evaluated on a standard corpus of monophonic folk and chorale melodies. The proposed method significantly outperforms comparable state-of-the-art models. I further discuss the free parameters of the learning algorithm and analyze the feature composition of the learned models. The models learned by PULSE afford an easy inspection and are musicologically interpreted for the first time. version:1
arxiv-1709-08830 | Catching Anomalous Distributed Photovoltaics: An Edge-based Multi-modal Anomaly Detection | http://arxiv.org/abs/1709.08830 | id:1709.08830 author:Devu Manikantan Shilay, Kin Gwn Lorey, Tianshu Weiz, Teems Lovetty, Yu Cheng category:cs.SY cs.CR cs.LG  published:2017-09-26 summary:A significant challenge in energy system cyber security is the current inability to detect cyber-physical attacks targeting and originating from distributed grid-edge devices such as photovoltaics (PV) panels, smart flexible loads, and electric vehicles. We address this concern by designing and developing a distributed, multi-modal anomaly detection approach that can sense the health of the device and the electric power grid from the edge. This is realized by exploiting unsupervised machine learning algorithms on multiple sources of time-series data, fusing these multiple local observations and flagging anomalies when a deviation from the normal behavior is observed. We particularly focus on the cyber-physical threats to the distributed PVs that has the potential to cause local disturbances or grid instabilities by creating supply-demand mismatch, reverse power flow conditions etc. We use an open source power system simulation tool called GridLAB-D, loaded with real smart home and solar datasets to simulate the smart grid scenarios and to illustrate the impact of PV attacks on the power system. Various attacks targeting PV panels that create voltage fluctuations, reverse power flow etc were designed and performed. We observe that while individual unsupervised learning algorithms such as OCSVMs, Corrupt RF and PCA surpasses in identifying particular attack type, PCA with Convex Hull outperforms all algorithms in identifying all designed attacks with a true positive rate of 83.64% and an accuracy of 95.78%. Our key insight is that due to the heterogeneous nature of the distribution grid and the uncertainty in the type of the attack being launched, relying on single mode of information for defense can lead to increased false alarms and missed detection rates as one can design attacks to hide within those uncertainties and remain stealthy. version:1
arxiv-1709-08828 | Towards End-to-End Car License Plates Detection and Recognition with Deep Neural Networks | http://arxiv.org/abs/1709.08828 | id:1709.08828 author:Hui Li, Peng Wang, Chunhua Shen category:cs.CV  published:2017-09-26 summary:In this work, we tackle the problem of car license plate detection and recognition in natural scene images. We propose a unified deep neural network which can localize license plates and recognize the letters simultaneously in a single forward pass. The whole network can be trained end-to-end. In contrast to existing approaches which take license plate detection and recognition as two separate tasks and settle them step by step, our method jointly solves these two tasks by a single network. It not only avoids intermediate error accumulation, but also accelerates the processing speed. For performance evaluation, three datasets including images captured from various scenes under different conditions are tested. Extensive experiments show the effectiveness and efficiency of our proposed approach. version:1
arxiv-1709-08820 | Converting Your Thoughts to Texts: Enabling Brain Typing via Deep Feature Learning of EEG Signals | http://arxiv.org/abs/1709.08820 | id:1709.08820 author:Xiang Zhang, Lina Yao, Quan Z. Sheng, Salil S. Kanhere, Tao Gu, Dalin Zhang category:cs.HC cs.CV  published:2017-09-26 summary:An electroencephalography (EEG) based Brain Computer Interface (BCI) enables people to communicate with the outside world by interpreting the EEG signals of their brains to interact with devices such as wheelchairs and intelligent robots. More specifically, motor imagery EEG (MI-EEG), which reflects a subjects active intent, is attracting increasing attention for a variety of BCI applications. Accurate classification of MI-EEG signals while essential for effective operation of BCI systems, is challenging due to the significant noise inherent in the signals and the lack of informative correlation between the signals and brain activities. In this paper, we propose a novel deep neural network based learning framework that affords perceptive insights into the relationship between the MI-EEG data and brain activities. We design a joint convolutional recurrent neural network that simultaneously learns robust high-level feature presentations through low-dimensional dense embeddings from raw MI-EEG signals. We also employ an Autoencoder layer to eliminate various artifacts such as background activities. The proposed approach has been evaluated extensively on a large- scale public MI-EEG dataset and a limited but easy-to-deploy dataset collected in our lab. The results show that our approach outperforms a series of baselines and the competitive state-of-the- art methods, yielding a classification accuracy of 95.53%. The applicability of our proposed approach is further demonstrated with a practical BCI system for typing. version:1
arxiv-1709-08802 | A Deep Learning Model for Traffic Flow State Classification Based on Smart Phone Sensor Data | http://arxiv.org/abs/1709.08802 | id:1709.08802 author:Wenwen Tu, Feng Xiao, Liping Fu, Guangyuan Pan category:cs.LG  published:2017-09-26 summary:This study proposes a Deep Belief Network model to classify traffic flow states. The model is capable of processing massive, high-density, and noise-contaminated data sets generated from smartphone sensors. The statistical features of Vehicle acceleration, angular acceleration, and GPS speed data, recorded by smartphone software, are analyzed, and then used as input for traffic flow state classification. Data from a five-day experiment is used to train and test the proposed model. A total of 747,856 sets of data are generated and used for both traffic flow states classification and sensitivity analysis of input variables. The result shows that the proposed Deep Belief Network model is superior to traditional machine learning methods in both classification performance and computational efficiency. version:1
arxiv-1709-08795 | On Stein's Identity and Near-Optimal Estimation in High-dimensional Index Models | http://arxiv.org/abs/1709.08795 | id:1709.08795 author:Zhuoran Yang, Krishnakumar Balasubramanian, Han Liu category:math.ST stat.ME stat.ML stat.TH  published:2017-09-26 summary:We consider estimating the parametric components of semi-parametric multiple index models in a high-dimensional non-Gaussian setting. Our estimators leverage the score function based second-order Stein's lemma and do not require Gaussian or elliptical symmetry assumptions made in the literature. Moreover, to handle score functions and response variables that are heavy-tailed, our estimators are constructed via carefully thresholding their empirical counterparts. We show that our estimator achieves near- optimal statistical rate of convergence in several settings. We supplement our theoretical results via simulation experiments that confirm the theory. version:1
arxiv-1709-08770 | On the Model Shrinkage Effect of Gamma Process Edge Partition Models | http://arxiv.org/abs/1709.08770 | id:1709.08770 author:Iku Ohama, Issei Sato, Takuya Kida, Hiroki Arimura category:stat.ML  published:2017-09-26 summary:The edge partition model (EPM) is a fundamental Bayesian nonparametric model for extracting an overlapping structure from binary matrix. The EPM adopts a gamma process ($\Gamma$P) prior to automatically shrink the number of active atoms. However, we empirically found that the model shrinkage of the EPM does not typically work appropriately and leads to an overfitted solution. An analysis of the expectation of the EPM's intensity function suggested that the gamma priors for the EPM hyperparameters disturb the model shrinkage effect of the internal $\Gamma$P. In order to ensure that the model shrinkage effect of the EPM works in an appropriate manner, we proposed two novel generative constructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM incorporating Dirichlet priors instead of the gamma priors. Furthermore, all DEPM's model parameters including the infinite atoms of the $\Gamma$P prior could be marginalized out, and thus it was possible to derive a truly infinite DEPM (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler. We experimentally confirmed that the model shrinkage of the proposed models works well and that the IDEPM indicated state-of-the-art performance in generalization ability, link prediction accuracy, mixing efficiency, and convergence speed. version:1
arxiv-1709-08761 | Image similarity using Deep CNN and Curriculum Learning | http://arxiv.org/abs/1709.08761 | id:1709.08761 author:Srikar Appalaraju, Vineet Chaoji category:cs.CV  published:2017-09-26 summary:Image similarity involves fetching similar looking images given a reference image. Our solution called SimNet, is a deep siamese network which is trained on pairs of positive and negative images using a novel online pair mining strategy inspired by Curriculum learning. We also created a multi-scale CNN, where the final image embedding is a joint representation of top as well as lower layer embedding's. We go on to show that this multi-scale siamese network is better at capturing fine grained image similarities than traditional CNN's. version:1
arxiv-1709-08739 | Camera-Aware Multi-Resolution Analysis (CAMRA) for Raw Sensor Data Compression | http://arxiv.org/abs/1709.08739 | id:1709.08739 author:Y. Lee, K. Hirakawa, T. Nguyen category:cs.CV  published:2017-09-25 summary:We propose a novel lossless and lossy compression scheme for color filter array~(CFA) sampled images based on the wavelet transform of them. Our analysis suggests that the wavelet coefficients of HL and LH subbands are highly correlated. Hence, we decorrelate Mallat wavelet packet decomposition to further sparsify the coefficients. In addition, we develop a camera processing pipeline for compressing CFA sampled images aimed at maximizing the quality of the color images constructed from the compressed CFA sampled images. We validated our theoretical analysis and the performance of the proposed compression scheme using images of natural scenes captured in a raw format. The experimental results verify that our proposed method improves coding efficiency relative to the standard and the state-of-the-art compression schemes CFA sampled images. version:1
arxiv-1706-05349 | Active learning in annotating micro-blogs dealing with e-reputation | http://arxiv.org/abs/1706.05349 | id:1706.05349 author:Jean-Valère Cossu, Alejandro Molina-Villegas, Mariana Tello-Signoret category:cs.SI cs.CL  published:2017-06-16 summary:Elections unleash strong political views on Twitter, but what do people really think about politics? Opinion and trend mining on micro blogs dealing with politics has recently attracted researchers in several fields including Information Retrieval and Machine Learning (ML). Since the performance of ML and Natural Language Processing (NLP) approaches are limited by the amount and quality of data available, one promising alternative for some tasks is the automatic propagation of expert annotations. This paper intends to develop a so-called active learning process for automatically annotating French language tweets that deal with the image (i.e., representation, web reputation) of politicians. Our main focus is on the methodology followed to build an original annotated dataset expressing opinion from two French politicians over time. We therefore review state of the art NLP-based ML algorithms to automatically annotate tweets using a manual initiation step as bootstrap. This paper focuses on key issues about active learning while building a large annotated data set from noise. This will be introduced by human annotators, abundance of data and the label distribution across data and entities. In turn, we show that Twitter characteristics such as the author's name or hashtags can be considered as the bearing point to not only improve automatic systems for Opinion Mining (OM) and Topic Classification but also to reduce noise in human annotations. However, a later thorough analysis shows that reducing noise might induce the loss of crucial information. version:4
arxiv-1709-07502 | A Multimodal, Full-Surround Vehicular Testbed for Naturalistic Studies and Benchmarking: Design, Calibration and Deployment | http://arxiv.org/abs/1709.07502 | id:1709.07502 author:Akshay Rangesh, Kevan Yuen, Ravi Kumar Satzoda, Rakesh Nattoji Rajaram, Pujitha Gunaratne, Mohan M. Trivedi category:cs.CV  published:2017-09-21 summary:Recent progress in autonomous and semi-autonomous driving has been made possible in part through an assortment of sensors that provide the intelligent agent with an enhanced perception of its surroundings. It has been clear for quite some while now that for intelligent vehicles to function effectively in all situations and conditions, a fusion of different sensor technologies is essential. Consequently, the availability of synchronized multi-sensory data streams are necessary to promote the development of fusion based algorithms for low, mid and high level semantic tasks. In this paper, we provide a comprehensive description of our heavily sensorized, panoramic testbed capable of providing high quality data from a slew of synchronized and calibrated sensors such as cameras, LIDARs, radars, and the IMU/GPS. The vehicle has recorded over 100 hours of real world data for a very diverse set of weather, traffic and daylight conditions. All captured data is accurately calibrated and synchronized using timestamps, and stored safely in high performance servers mounted inside the vehicle itself. Details on the testbed instrumentation, sensor layout, sensor outputs, calibration and synchronization are described in this paper. version:2
arxiv-1709-08730 | Understanding a Version of Multivariate Symmetric Uncertainty to assist in Feature Selection | http://arxiv.org/abs/1709.08730 | id:1709.08730 author:Gustavo Sosa-Cabrera, Miguel García-Torres, Santiago Gómez, Christian Schaerer, Federico Divina category:cs.LG stat.ML  published:2017-09-25 summary:In this paper, we analyze the behavior of the multivariate symmetric uncertainty (MSU) measure through the use of statistical simulation techniques under various mixes of informative and non-informative randomly generated features. Experiments show how the number of attributes, their cardinalities, and the sample size affect the MSU. We discovered a condition that preserves good quality in the MSU under different combinations of these three factors, providing a new useful criterion to help drive the process of dimension reduction. version:1
arxiv-1709-08716 | DOC: Deep Open Classification of Text Documents | http://arxiv.org/abs/1709.08716 | id:1709.08716 author:Lei Shu, Hu Xu, Bing Liu category:cs.CL  published:2017-09-25 summary:Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training. This also applies to text learning or text classification. As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem. This problem is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically. version:1
arxiv-1709-08680 | Multimodal Image Super-Resolution via Joint Sparse Representations induced by Coupled Dictionaries | http://arxiv.org/abs/1709.08680 | id:1709.08680 author:Pingfan Song, Xin Deng, João F. C. Mota, Nikos Deligiannis, Pier Luigi Dragotti, Miguel R. D. Rodrigues category:cs.CV  published:2017-09-25 summary:Real-world data processing problems often involve various image modalities associated with a certain scene, including RGB images, infrared images or multispectral images. The fact that different image modalities often share diverse attributes, such as certain edges, textures and other structure primitives, represents an opportunity to enhance various image processing tasks. This paper proposes a new approach to construct a high-resolution (HR) version of a low-resolution (LR) image given another HR image modality as reference, based on joint sparse representations induced by coupled dictionaries. Our approach, which captures the similarities and disparities between different image modalities in a learned sparse feature domain in lieu of the original image domain, consists of two phases. The coupled dictionary learning phase is used to learn a set of dictionaries that couple different image modalities in the sparse feature domain given a set of training data. In turn, the coupled super-resolution phase leverages such coupled dictionaries to construct a HR version of the LR target image given another related image modality. One of the merits of our sparsity-driven approach relates to the fact that it overcomes drawbacks such as the texture copying artifacts commonly resulting from inconsistency between the guidance and target images. Experiments on both synthetic data and real multimodal images demonstrate that incorporating appropriate guidance information via joint sparse representation induced by coupled dictionary learning brings notable benefits in the super-resolution task with respect to the state-of-the-art. version:1
arxiv-1709-08666 | Fast Vehicle Detection in Aerial Imagery | http://arxiv.org/abs/1709.08666 | id:1709.08666 author:Jennifer Carlet, Bernard Abayowa category:cs.CV  published:2017-09-25 summary:In recent years, several real-time or near real-time object detectors have been developed. However these object detectors are typically designed for first-person view images where the subject is large in the image and do not directly apply well to detecting vehicles in aerial imagery. Though some detectors have been developed for aerial imagery, these are either slow or do not handle multi-scale imagery very well. Here the popular YOLOv2 detector is modified to vastly improve it's performance on aerial data. The modified detector is compared to Faster RCNN on several aerial imagery datasets. The proposed detector gives near state of the art performance at more than 4x the speed. version:1
arxiv-1709-08605 | Muon Trigger for Mobile Phones | http://arxiv.org/abs/1709.08605 | id:1709.08605 author:Maxim Borisyak, Michail Usvyatsov, Michael Mulhearn, Chase Shimmin, Andrey Ustyuzhanin category:cs.CV astro-ph.IM physics.ins-det  published:2017-09-25 summary:The CRAYFIS experiment proposes to use privately owned mobile phones as a ground detector array for Ultra High Energy Cosmic Rays. Upon interacting with Earth's atmosphere, these events produce extensive particle showers which can be detected by cameras on mobile phones. A typical shower contains minimally-ionizing particles such as muons. As these particles interact with CMOS image sensors, they may leave tracks of faintly-activated pixels that are sometimes hard to distinguish from random detector noise. Triggers that rely on the presence of very bright pixels within an image frame are not efficient in this case. We present a trigger algorithm based on Convolutional Neural Networks which selects images containing such tracks and are evaluated in a lazy manner: the response of each successive layer is computed only if activation of the current layer satisfies a continuation criterion. Usage of neural networks increases the sensitivity considerably comparable with image thresholding, while the lazy evaluation allows for execution of the trigger under the limited computational power of mobile phones. version:1
arxiv-1709-08600 | EZLearn: Exploiting Organic Supervision in Large-Scale Data Annotation | http://arxiv.org/abs/1709.08600 | id:1709.08600 author:Maxim Grechkin, Hoifung Poon, Bill Howe category:cs.CL cs.LG  published:2017-09-25 summary:We propose Extreme Zero-shot Learning (EZLearn) for classifying data into potentially thousands of classes, with zero labeled examples. The key insight is to leverage the abundant unlabeled data together with two sources of organic supervision: a lexicon for the annotation classes, and text descriptions that often accompany unlabeled data. Such indirect supervision is readily available in science and other high-value applications. The classes represent the consensus conceptualization of a given domain, and their standard references can be easily obtained, often readily available in an existing domain ontology. Likewise, to facilitate reuse, public datasets typically include text descriptions, some of which mention the relevant classes. To exploit such organic supervision, EZLearn introduces an auxiliary natural language processing system, which uses the lexicon to generate initial noisy labels from the text descriptions, and then co-teaches the main classifier until convergence. Effectively, EZLearn combines distant supervision and co-training into a new learning paradigm for leveraging unlabeled data. Because no hand-labeled examples are required, EZLearn is naturally applicable to domains with a long tail of classes and/or frequent updates. We evaluated EZLearn on applications in functional genomics and scientific figure comprehension. In both cases, using text descriptions as the pivot, EZLearn learned to accurately annotate data samples without direct supervision, even substantially outperforming the state-of-the-art supervised methods trained on tens of thousands of annotated examples. version:1
arxiv-1709-08563 | Evolutionary Acyclic Graph Partitioning | http://arxiv.org/abs/1709.08563 | id:1709.08563 author:Orlando Moreira, Merten Popp, Christian Schulz category:cs.DS cs.NE  published:2017-09-25 summary:Directed graphs are widely used to model data flow and execution dependencies in streaming applications. This enables the utilization of graph partitioning algorithms for the problem of parallelizing computation for multiprocessor architectures. However due to resource restrictions, an acyclicity constraint on the partition is necessary when mapping streaming applications to an embedded multiprocessor. Here, we contribute a multi-level algorithm for the acyclic graph partitioning problem. Based on this, we engineer an evolutionary algorithm to further reduce communication cost, as well as to improve load balancing and the scheduling makespan on embedded multiprocessor architectures. version:1
arxiv-1709-08553 | Attribute Recognition by Joint Recurrent Learning of Context and Correlation | http://arxiv.org/abs/1709.08553 | id:1709.08553 author:Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li category:cs.CV  published:2017-09-25 summary:Recognising semantic pedestrian attributes in surveillance images is a challenging task for computer vision, particularly when the imaging quality is poor with complex background clutter and uncontrolled viewing conditions, and the number of labelled training data is small. In this work, we formulate a Joint Recurrent Learning (JRL) model for exploring attribute context and correlation in order to improve attribute recognition given small sized training data with poor quality images. The JRL model learns jointly pedestrian attribute correlations in a pedestrian image and in particular their sequential ordering dependencies (latent high-order correlation) in an end-to-end encoder/decoder recurrent network. We demonstrate the performance advantage and robustness of the JRL model over a wide range of state-of-the-art deep models for pedestrian attribute recognition, multi-label image classification, and multi-person image annotation on two largest pedestrian attribute benchmarks PETA and RAP. version:1
arxiv-1709-08527 | Multi-view pose estimation with mixtures-of-parts and adaptive viewpoint selection | http://arxiv.org/abs/1709.08527 | id:1709.08527 author:Emre Dogan, Gonen Eren, Christian Wolf, Eric Lombardi, Atilla Baskurt category:cs.CV  published:2017-09-25 summary:We propose a new method for human pose estimation which leverages information from multiple views to impose a strong prior on articulated pose. The novelty of the method concerns the types of coherence modelled. Consistency is maximised over the different views through different terms modelling classical geometric information (coherence of the resulting poses) as well as appearance information which is modelled as latent variables in the global energy function. Moreover, adequacy of each view is assessed and their contributions are adjusted accordingly. Experiments on the HumanEva and UMPM datasets show that the proposed method significantly decreases the estimation error compared to single-view results. version:1
arxiv-1709-08524 | Generative learning for deep networks | http://arxiv.org/abs/1709.08524 | id:1709.08524 author:Boris Flach, Alexander Shekhovtsov, Ondrej Fikar category:cs.LG cs.CV cs.NE stat.ML  published:2017-09-25 summary:Learning, taking into account full distribution of the data, referred to as generative, is not feasible with deep neural networks (DNNs) because they model only the conditional distribution of the outputs given the inputs. Current solutions are either based on joint probability models facing difficult estimation problems or learn two separate networks, mapping inputs to outputs (recognition) and vice-versa (generation). We propose an intermediate approach. First, we show that forward computation in DNNs with logistic sigmoid activations corresponds to a simplified approximate Bayesian inference in a directed probabilistic multi-layer model. This connection allows to interpret DNN as a probabilistic model of the output and all hidden units given the input. Second, we propose that in order for the recognition and generation networks to be more consistent with the joint model of the data, weights of the recognition and generator network should be related by transposition. We demonstrate in a tentative experiment that such a coupled pair can be learned generatively, modelling the full distribution of the data, and has enough capacity to perform well in both recognition and generation. version:1
arxiv-1709-08521 | Using objective words in the reviews to improve the colloquial arabic sentiment analysis | http://arxiv.org/abs/1709.08521 | id:1709.08521 author:Omar Al-Harbi category:cs.CL  published:2017-09-25 summary:One of the main difficulties in sentiment analysis of the Arabic language is the presence of the colloquialism. In this paper, we examine the effect of using objective words in conjunction with sentimental words on sentiment classification for the colloquial Arabic reviews, specifically Jordanian colloquial reviews. The reviews often include both sentimental and objective words, however, the most existing sentiment analysis models ignore the objective words as they are considered useless. In this work, we created two lexicons: the first includes the colloquial sentimental words and compound phrases, while the other contains the objective words associated with values of sentiment tendency based on a particular estimation method. We used these lexicons to extract sentiment features that would be training input to the Support Vector Machines (SVM) to classify the sentiment polarity of the reviews. The reviews dataset have been collected manually from JEERAN website. The results of the experiments show that the proposed approach improves the polarity classification in comparison to two baseline models, with accuracy 95.6%. version:1
arxiv-1709-08520 | Predictive-State Decoders: Encoding the Future into Recurrent Networks | http://arxiv.org/abs/1709.08520 | id:1709.08520 author:Arun Venkatraman, Nicholas Rhinehart, Wen Sun, Lerrel Pinto, Martial Hebert, Byron Boots, Kris M. Kitani, J. Andrew Bagnell category:stat.ML cs.LG  published:2017-09-25 summary:Recurrent neural networks (RNNs) are a vital modeling technique that rely on internal states learned indirectly by optimization of a supervised, unsupervised, or reinforcement training loss. RNNs are used to model dynamic processes that are characterized by underlying latent states whose form is often unknown, precluding its analytic representation inside an RNN. In the Predictive-State Representation (PSR) literature, latent state processes are modeled by an internal state representation that directly models the distribution of future observations, and most recent work in this area has relied on explicitly representing and targeting sufficient statistics of this probability distribution. We seek to combine the advantages of RNNs and PSRs by augmenting existing state-of-the-art recurrent neural networks with Predictive-State Decoders (PSDs), which add supervision to the network's internal state representation to target predicting future observations. Predictive-State Decoders are simple to implement and easily incorporated into existing training pipelines via additional loss regularization. We demonstrate the effectiveness of PSDs with experimental results in three different domains: probabilistic filtering, Imitation Learning, and Reinforcement Learning. In each, our method improves statistical performance of state-of-the-art recurrent baselines and does so with fewer iterations and less data. version:1
arxiv-1709-02984 | Sentiment Polarity Detection for Software Development | http://arxiv.org/abs/1709.02984 | id:1709.02984 author:Fabio Calefato, Filippo Lanubile, Federico Maiorano, Nicole Novielli category:cs.SE cs.CL  published:2017-09-09 summary:The role of sentiment analysis is increasingly emerging to study software developers' emotions by mining crowd-generated content within social software engineering tools. However, off-the-shelf sentiment analysis tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. Here, we present Senti4SD, a classifier specifically trained to support sentiment analysis in developers' communication channels. Senti4SD is trained and validated using a gold standard of Stack Overflow questions, answers, and comments manually annotated for sentiment polarity. It exploits a suite of both lexicon- and keyword-based features, as well as semantic features based on word embedding. With respect to a mainstream off-the-shelf tool, which we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. To encourage replications, we release a lab package including the classifier, the word embedding space, and the gold standard with annotation guidelines. version:2
arxiv-1709-08491 | Statistical learning of spatiotemporal patterns from longitudinal manifold-valued networks | http://arxiv.org/abs/1709.08491 | id:1709.08491 author:Igor Koval, Jean-Baptiste Schiratti, Alexandre Routier, Michael Bacci, Olivier Colliot, Stéphanie Allassonnière, Stanley Durrleman category:stat.ML cs.CV q-bio.NC q-bio.QM  published:2017-09-25 summary:We introduce a mixed-effects model to learn spatiotempo-ral patterns on a network by considering longitudinal measures distributed on a fixed graph. The data come from repeated observations of subjects at different time points which take the form of measurement maps distributed on a graph such as an image or a mesh. The model learns a typical group-average trajectory characterizing the propagation of measurement changes across the graph nodes. The subject-specific trajectories are defined via spatial and temporal transformations of the group-average scenario, thus estimating the variability of spatiotemporal patterns within the group. To estimate population and individual model parameters, we adapted a stochastic version of the Expectation-Maximization algorithm, the MCMC-SAEM. The model is used to describe the propagation of cortical atrophy during the course of Alzheimer's Disease. Model parameters show the variability of this average pattern of atrophy in terms of trajectories across brain regions, age at disease onset and pace of propagation. We show that the personalization of this model yields accurate prediction of maps of cortical thickness in patients. version:1
arxiv-1709-08432 | House Price Prediction Using LSTM | http://arxiv.org/abs/1709.08432 | id:1709.08432 author:Xiaochen Chen, Lai Wei, Jiaxin Xu category:cs.LG stat.ML  published:2017-09-25 summary:In this paper, we use the house price data ranging from January 2004 to October 2016 to predict the average house price of November and December in 2016 for each district in Beijing, Shanghai, Guangzhou and Shenzhen. We apply Autoregressive Integrated Moving Average model to generate the baseline while LSTM networks to build prediction model. These algorithms are compared in terms of Mean Squared Error. The result shows that the LSTM model has excellent properties with respect to predict time series. Also, stateful LSTM networks and stack LSTM networks are employed to further study the improvement of accuracy of the house prediction model. version:1
arxiv-1709-08421 | Summarization of User-Generated Sports Video by Using Deep Action Recognition Features | http://arxiv.org/abs/1709.08421 | id:1709.08421 author:Antonio Tejero-de-Pablos, Yuta Nakashima, Tomokazu Sato, Naokazu Yokoya, Marko Linna, Esa Rahtu category:cs.CV 68T45  published:2017-09-25 summary:Automatically generating a summary of sports video poses the challenge of detecting interesting moments, or highlights, of a game. Traditional sports video summarization methods leverage editing conventions of broadcast sports video that facilitate the extraction of high-level semantics. However, user-generated videos are not edited, and thus traditional methods are not suitable to generate a summary. In order to solve this problem, this work proposes a novel video summarization method that uses players' actions as a cue to determine the highlights of the original video. A deep neural network-based approach is used to extract two types of action-related features and to classify video segments into interesting or uninteresting parts. The proposed method can be applied to any sports in which games consist of a succession of actions. Especially, this work considers the case of Kendo (Japanese fencing) as an example of a sport to evaluate the proposed method. The method is trained using Kendo videos with ground truth labels that indicate the video highlights. The labels are provided by annotators possessing different experience with respect to Kendo to demonstrate how the proposed method adapts to different needs. The performance of the proposed method is compared with several combinations of different features, and the results show that it outperforms previous summarization methods. version:1
arxiv-1709-05820 | Toward a full-scale neural machine translation in production: the Booking.com use case | http://arxiv.org/abs/1709.05820 | id:1709.05820 author:Pavel Levin, Nishikant Dhanuka, Talaat Khalil, Fedor Kovalev, Maxim Khalilov category:cs.CL  published:2017-09-18 summary:While some remarkable progress has been made in neural machine translation (NMT) research, there have not been many reports on its development and evaluation in practice. This paper tries to fill this gap by presenting some of our findings from building an in-house travel domain NMT system in a large scale E-commerce setting. The three major topics that we cover are optimization and training (including different optimization strategies and corpus sizes), handling real-world content and evaluating results. version:2
arxiv-1709-08393 | Multi-view Registration Based on Weighted Low Rank and Sparse Matrix Decomposition of Motions | http://arxiv.org/abs/1709.08393 | id:1709.08393 author:Congcong Jin, Jihua Zhu, Yaochen Li, Shanmin Pang, Lei Chen, Jun Wang category:cs.CV  published:2017-09-25 summary:Recently, the low rank and sparse (LRS) matrix decomposition has been introduced as an effective mean to solve the multi-view registration. However, this method presents two notable disadvantages: the registration result is quite sensitive to the sparsity of the LRS matrix; besides, the decomposition process treats each block element equally in spite of their reliability. Therefore, this paper firstly proposes a matrix completion method based on the overlap percentage of scan pairs. By completing the LRS matrix with reliable block elements as much as possible, more synchronization constraints of relative motions can be utilized for registration. Furthermore, it is observed that the reliability of each element in the LRS matrix can be weighed by the relationship between its corresponding model and data shapes. Therefore, a weight matrix is designed to measure the contribution of each element to decomposition and accordingly, the decomposition result is closer to the ground truth than before. Benefited from the more informative LRS matrix as well as the weight matrix, experimental results conducted on several public datasets demonstrate the superiority of the proposed approach over other methods on both accuracy and robustness. version:1
arxiv-1709-04875 | Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting | http://arxiv.org/abs/1709.04875 | id:1709.04875 author:Bing Yu, Haoteng Yin, Zhanxing Zhu category:cs.LG stat.ML  published:2017-09-14 summary:The goal of traffic forecasting is to predict the future vital indicators (such as speed, volume and density) of the local traffic network in reasonable response time. Due to the dynamics and complexity of traffic network flow, typical simulation experiments and classic statistical methods cannot satisfy the requirements of mid-and-long term forecasting. In this work, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Neural Network (ST-GCNN), to tackle this spatio-temporal sequence forecasting task. Instead of applying recurrent models to sequence learning, we build our model entirely on convolutional neural networks (CNNs) with gated linear units (GLU) and highway networks. The proposed architecture fully employs the graph structure of the road networks and enables faster training. Experiments show that our ST-GCNN network captures comprehensive spatio-temporal correlations throughout complex traffic network and consistently outperforms state-of-the-art baseline algorithms on several real-world traffic datasets. version:2
arxiv-1709-08378 | Variational Reflectance Estimation from Multi-view Images | http://arxiv.org/abs/1709.08378 | id:1709.08378 author:Jean Mélou, Yvain Quéau, Jean-Denis Durou, Fabien Castan, Daniel Cremers category:cs.CV  published:2017-09-25 summary:We tackle the problem of reflectance estimation from a set of multi-view images, assuming known geometry. The approach we put forward turns the input images into reflectance maps, through a robust variational method. The variational model comprises an image-driven fidelity term and a term which enforces consistency of the reflectance estimates with respect to each view. If illumination is fixed across the views, then reflectance estimation remains under-constrained: a regularization term, which ensures piecewise-smoothness of the reflectance, is thus used. Reflectance is parameterized in the image domain, rather than on the surface, which makes the numerical solution much easier, by resorting to an alternating majorization-minimization approach. Experiments on both synthetic and real datasets are carried out to validate the proposed strategy. version:1
arxiv-1709-08374 | Deep Sparse Subspace Clustering | http://arxiv.org/abs/1709.08374 | id:1709.08374 author:Xi Peng, Jiashi Feng, Shijie Xiao, Jiwen Lu, Zhang Yi, Shuicheng Yan category:cs.CV  published:2017-09-25 summary:In this paper, we present a deep extension of Sparse Subspace Clustering, termed Deep Sparse Subspace Clustering (DSSC). Regularized by the unit sphere distribution assumption for the learned deep features, DSSC can infer a new data affinity matrix by simultaneously satisfying the sparsity principle of SSC and the nonlinearity given by neural networks. One of the appealing advantages brought by DSSC is: when original real-world data do not meet the class-specific linear subspace distribution assumption, DSSC can employ neural networks to make the assumption valid with its hierarchical nonlinear transformations. To the best of our knowledge, this is among the first deep learning based subspace clustering methods. Extensive experiments are conducted on four real-world datasets to show the proposed DSSC is significantly superior to 12 existing methods for subspace clustering. version:1
arxiv-1709-08367 | Robust Associative Memories Naturally Occuring From Recurrent Hebbian Networks Under Noise | http://arxiv.org/abs/1709.08367 | id:1709.08367 author:Eliott Coyac, Vincent Gripon, Charlotte Langlais, Claude Berrou category:cs.NE  published:2017-09-25 summary:The brain is a noisy system subject to energy constraints. These facts are rarely taken into account when modelling artificial neural networks. In this paper, we are interested in demonstrating that those factors can actually lead to the appearance of robust associative memories. We first propose a simplified model of noise in the brain, taking into account synaptic noise and interference from neurons external to the network. When coarsely quantized, we show that this noise can be reduced to insertions and erasures. We take a neural network with recurrent modifiable connections, and subject it to noisy external inputs. We introduce an energy usage limitation principle in the network as well as consolidated Hebbian learning, resulting in an incremental processing of inputs. We show that the connections naturally formed correspond to state-of-the-art binary sparse associative memories. version:1
arxiv-1709-08364 | 3D Textured Model Encryption via 3D Lu Chaotic Mapping | http://arxiv.org/abs/1709.08364 | id:1709.08364 author:Xin Jin, Shuyun Zhu, Chaoen Xiao, Hongbo Sun, Xiaodong Li, Geng Zhao, Shiming Ge category:cs.CV cs.CR  published:2017-09-25 summary:In the coming Virtual/Augmented Reality (VR/AR) era, 3D contents will be popularized just as images and videos today. The security and privacy of these 3D contents should be taken into consideration. 3D contents contain surface models and solid models. The surface models include point clouds, meshes and textured models. Previous work mainly focus on encryption of solid models, point clouds and meshes. This work focuses on the most complicated 3D textured model. We propose a 3D Lu chaotic mapping based encryption method of 3D textured model. We encrypt the vertexes, the polygons and the textures of 3D models separately using the 3D Lu chaotic mapping. Then the encrypted vertices, edges and texture maps are composited together to form the final encrypted 3D textured model. The experimental results reveal that our method can encrypt and decrypt 3D textured models correctly. In addition, our method can resistant several attacks such as brute-force attack and statistic attack. version:1
arxiv-1709-08362 | An Evolutionary Computing Enriched RS Attack Resilient Medical Image Steganography Model for Telemedicine Applications | http://arxiv.org/abs/1709.08362 | id:1709.08362 author:Romany F. Mansour, Elsaid MD. Abdelrahim category:cs.CV  published:2017-09-25 summary:The recent advancement in computing technologies and resulting vision based applications have gives rise to a novel practice called telemedicine that requires patient diagnosis images or allied information to recommend or even perform diagnosis practices being located remotely. However, to ensure accurate and optimal telemedicine there is the requirement of seamless or flawless biomedical information about patient. On the contrary, medical data transmitted over insecure channel often remains prone to get manipulated or corrupted by attackers. The existing cryptosystems alone are not sufficient to deal with these issues and hence in this paper a highly robust reversible image steganography model has been developed for secret information hiding. Unlike traditional wavelet transform techniques, we incorporated Discrete Ripplet Transformation (DRT) technique for message embedding in the medical cover images. In addition, to assure seamless communication over insecure channel, a dual cryptosystem model containing proposed steganography scheme and RSA cryptosystem has been developed. One of the key novelties of the proposed research work is the use of adaptive genetic algorithm (AGA) for optimal pixel adjustment process (OPAP) that enriches data hiding capacity as well as imperceptibility features. The performance assessment reveals that the proposed steganography model outperforms other wavelet transformation based approaches in terms of high PSNR, embedding capacity, imperceptibility etc. version:1
arxiv-1709-08340 | Realizing Half-Diminished Reality from Video Stream of Manipulating Objects | http://arxiv.org/abs/1709.08340 | id:1709.08340 author:Hayato Okumoto, Mitsuo Yoshida, Kyoji Umemura category:cs.CV  published:2017-09-25 summary:When we watch a video, in which human hands manipulate objects, these hands may obscure some parts of those objects. We are willing to make clear how the objects are manipulated by making the image of hands semi-transparent, and showing the complete images of the hands and the object. By carefully choosing a Half-Diminished Reality method, this paper proposes a method that can process the video in real time and verifies that the proposed method works well. version:1
arxiv-1709-05342 | Anomaly Detection for a Water Treatment System Using Unsupervised Machine Learning | http://arxiv.org/abs/1709.05342 | id:1709.05342 author:Jun Inoue, Yoriyuki Yamagata, Yuqi Chen, Christopher M. Poskitt, Jun Sun category:cs.LG  published:2017-09-15 summary:In this paper, we propose and evaluate the application of unsupervised machine learning to anomaly detection for a Cyber-Physical System (CPS). We compare two methods: Deep Neural Networks (DNN) adapted to time series data generated by a CPS, and one-class Support Vector Machines (SVM). These methods are evaluated against data from the Secure Water Treatment (SWaT) testbed, a scaled-down but fully operational raw water purification plant. For both methods, we first train detectors using a log generated by SWaT operating under normal conditions. Then, we evaluate the performance of both methods using a log generated by SWaT operating under 36 different attack scenarios. We find that our DNN generates fewer false positives than our one-class SVM while our SVM detects slightly more anomalies. Overall, our DNN has a slightly better F measure than our SVM. We discuss the characteristics of the DNN and one-class SVM used in this experiment, and compare the advantages and disadvantages of the two methods. version:2
arxiv-1709-08325 | Pose-driven Deep Convolutional Model for Person Re-identification | http://arxiv.org/abs/1709.08325 | id:1709.08325 author:Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, Qi Tian category:cs.CV  published:2017-09-25 summary:Feature extraction and matching are two crucial components in person Re-Identification (ReID). The large pose deformations and the complex view variations exhibited by the captured person images significantly increase the difficulty of learning and matching of the features from person images. To overcome these difficulties, in this work we propose a Pose-driven Deep Convolutional (PDC) model to learn improved feature extraction and matching models from end to end. Our deep architecture explicitly leverages the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts. To match the features from global human body and local body parts, a pose driven feature weighting sub-network is further designed to learn adaptive feature fusions. Extensive experimental analyses and results on three popular datasets demonstrate significant performance improvements of our model over all published state-of-the-art methods. version:1
arxiv-1709-06316 | Predicting Video Saliency with Object-to-Motion CNN and Two-layer Convolutional LSTM | http://arxiv.org/abs/1709.06316 | id:1709.06316 author:Lai Jiang, Mai Xu, Zulin Wang category:cs.CV  published:2017-09-19 summary:Over the past few years, deep neural networks (DNNs) have exhibited great success in predicting the saliency of images. However, there are few works that apply DNNs to predict the saliency of generic videos. In this paper, we propose a novel DNN-based video saliency prediction method. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which provides sufficient data to train the DNN models for predicting video saliency. Through the statistical analysis of our LEDOV database, we find that human attention is normally attracted by objects, particularly moving objects or the moving parts of objects. Accordingly, we propose an object-to-motion convolutional neural network (OM-CNN) to learn spatio-temporal features for predicting the intra-frame saliency via exploring the information of both objectness and object motion. We further find from our database that there exists a temporal correlation of human attention with a smooth saliency transition across video frames. Therefore, we develop a two-layer convolutional long short-term memory (2C-LSTM) network in our DNN-based method, using the extracted features of OM-CNN as the input. Consequently, the inter-frame saliency maps of videos can be generated, which consider the transition of attention across video frames. Finally, the experimental results show that our method advances the state-of-the-art in video saliency prediction. version:2
arxiv-1709-08299 | Dataset for the First Evaluation on Chinese Machine Reading Comprehension | http://arxiv.org/abs/1709.08299 | id:1709.08299 author:Yiming Cui, Ting Liu, Zhipeng Chen, Wentao Ma, Shijin Wang, Guoping Hu category:cs.CL  published:2017-09-25 summary:Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attentions. However, existing reading comprehension datasets are mostly in English. To add diversity in reading comprehension datasets, in this paper we propose a new Chinese reading comprehension dataset for accelerating related research in the community. The proposed dataset contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set. Along with this dataset, we also hosted the first Evaluation on Chinese Machine Reading Comprehension (CMRC-2017) and successfully attracted tens of participants, which suggest the potential impact of this dataset. version:1
arxiv-1709-08295 | Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN | http://arxiv.org/abs/1709.08295 | id:1709.08295 author:Xiangteng He, Yuxin Peng, Junjie Zhao category:cs.CV  published:2017-09-25 summary:Discriminative localization is essential for fine-grained image classification task, which devotes to recognizing hundreds of subcategories in the same basic-level category. Reflecting on discriminative regions of objects, key differences among different subcategories are subtle and local. Existing methods generally adopt a two-stage learning framework: The first stage is to localize the discriminative regions of objects, and the second is to encode the discriminative features for training classifiers. However, these methods generally have two limitations: (1) Separation of the two-stage learning is time-consuming. (2) Dependence on object and parts annotations for discriminative localization learning leads to heavily labor-consuming labeling. It is highly challenging to address these two important limitations simultaneously. Existing methods only focus on one of them. Therefore, this paper proposes the discriminative localization approach via saliency-guided Faster R-CNN to address the above two limitations at the same time, and our main novelties and advantages are: (1) End-to-end network based on Faster R-CNN is designed to simultaneously localize discriminative regions and encode discriminative features, which accelerates classification speed. (2) Saliency-guided localization learning is proposed to localize the discriminative region automatically, avoiding labor-consuming labeling. Both are jointly employed to simultaneously accelerate classification speed and eliminate dependence on object and parts annotations. Comparing with the state-of-the-art methods on the widely-used CUB-200-2011 dataset, our approach achieves both the best classification accuracy and efficiency. version:1
arxiv-1709-08294 | Adaptive Convolutional Filter Generation for Natural Language Understanding | http://arxiv.org/abs/1709.08294 | id:1709.08294 author:Dinghan Shen, Martin Renqiang Min, Yitong Li, Lawrence Carin category:cs.CL cs.LG stat.ML  published:2017-09-25 summary:Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP are not expressive enough, in the sense that all input sentences share the same learned (and static) set of filters. Motivated by this problem, we propose an adaptive convolutional filter generation framework for natural language understanding, by leveraging a meta network to generate input-aware filters. We further generalize our framework to model question-answer sentence pairs and propose an adaptive question answering (AdaQA) model; a novel two-way feature abstraction mechanism is introduced to encapsulate co-dependent sentence representations. We investigate the effectiveness of our framework on document categorization and answer sentence-selection tasks, achieving state-of-the-art performance on several benchmark datasets. version:1
arxiv-1709-08274 | Learning Semantic Maps with Topological Spatial Relations Using Graph-Structured Sum-Product Networks | http://arxiv.org/abs/1709.08274 | id:1709.08274 author:Kaiyu Zheng, Andrzej Pronobis, Rajesh P. N. Rao category:cs.LG  published:2017-09-24 summary:We introduce Graph-Structured Sum-Product Networks (GraphSPNs), a probabilistic approach to structured prediction for problems where dependencies between latent variables are expressed in terms of arbitrary, dynamic graphs. While many approaches to structured prediction place strict constraints on the interactions between inferred variables, many real-world problems can be only characterized using complex graph structures of varying size, often contaminated with noise when obtained from real data. Here, we focus on one such problem in the domain of robotics. We demonstrate how GraphSPNs can be used to bolster inference about semantic, conceptual place descriptions using noisy topological relations discovered by a robot exploring large-scale office spaces. Through experiments, we show that GraphSPNs consistently outperform the traditional approach based on undirected graphical models, successfully disambiguating information in global semantic maps built from uncertain, noisy local evidence. We further exploit the probabilistic nature of the model to infer marginal distributions over semantic descriptions of as yet unexplored places and detect spatial environment configurations that are novel and incongruent with the known evidence. version:1
arxiv-1709-08271 | 3D Camouflaging Object using RGB-D Sensors | http://arxiv.org/abs/1709.08271 | id:1709.08271 author:Ahmed M. Siddek, Mohsen A. Rashwan, Islam A. Eshrah category:cs.CV  published:2017-09-24 summary:This paper proposes a new optical camouflage system that uses RGB-D cameras, for acquiring point cloud of background scene, and tracking observers eyes. This system enables a user to conceal an object located behind a display that surrounded by 3D objects. If we considered here the tracked point of observer s eyes is a light source, the system will work on estimating shadow shape of the display device that falls on the objects in background. The system uses the 3d observer s eyes and the locations of display corners to predict their shadow points which have nearest neighbors in the constructed point cloud of background scene. version:1
arxiv-1710-01214 | Calligraphic Stylisation Learning with a Physiologically Plausible Model of Movement and Recurrent Neural Networks | http://arxiv.org/abs/1710.01214 | id:1710.01214 author:Daniel Berio, Memo Akten, Frederic Fol Leymarie, Mick Grierson, Réjean Plamondon category:cs.CV I.2.6; I.6.8; D.2.2  published:2017-09-24 summary:We propose a computational framework to learn stylisation patterns from example drawings or writings, and then generate new trajectories that possess similar stylistic qualities. We particularly focus on the generation and stylisation of trajectories that are similar to the ones that can be seen in calligraphy and graffiti art. Our system is able to extract and learn dynamic and visual qualities from a small number of user defined examples which can be recorded with a digitiser device, such as a tablet, mouse or motion capture sensors. Our system is then able to transform new user drawn traces to be kinematically and stylistically similar to the training examples. We implement the system using a Recurrent Mixture Density Network (RMDN) combined with a representation given by the parameters of the Sigma Lognormal model, a physiologically plausible model of movement that has been shown to closely reproduce the velocity and trace of human handwriting gestures. version:1
arxiv-1709-08248 | Discovery Radiomics via Deep Multi-Column Radiomic Sequencers for Skin Cancer Detection | http://arxiv.org/abs/1709.08248 | id:1709.08248 author:Mohammad Javad Shafiee, Alexander Wong category:cs.CV cs.NE  published:2017-09-24 summary:While skin cancer is the most diagnosed form of cancer in men and women, with more cases diagnosed each year than all other cancers combined, sufficiently early diagnosis results in very good prognosis and as such makes early detection crucial. While radiomics have shown considerable promise as a powerful diagnostic tool for significantly improving oncological diagnostic accuracy and efficiency, current radiomics-driven methods have largely rely on pre-defined, hand-crafted quantitative features, which can greatly limit the ability to fully characterize unique cancer phenotype that distinguish it from healthy tissue. Recently, the notion of discovery radiomics was introduced, where a large amount of custom, quantitative radiomic features are directly discovered from the wealth of readily available medical imaging data. In this study, we present a novel discovery radiomics framework for skin cancer detection, where we leverage novel deep multi-column radiomic sequencers for high-throughput discovery and extraction of a large amount of custom radiomic features tailored for characterizing unique skin cancer tissue phenotype. The discovered radiomic sequencer was tested against 9,152 biopsy-proven clinical images comprising of different skin cancers such as melanoma and basal cell carcinoma, and demonstrated sensitivity and specificity of 91% and 75%, respectively, thus achieving dermatologist-level performance and \break hence can be a powerful tool for assisting general practitioners and dermatologists alike in improving the efficiency, consistency, and accuracy of skin cancer diagnosis. version:1
arxiv-1709-08225 | Learning crystal plasticity using digital image correlation: Examples from discrete dislocation dynamics | http://arxiv.org/abs/1709.08225 | id:1709.08225 author:Stefanos Papanikolaou, Michail Tzimas, Hengxu Song, Andrew C. E. Reid, Stephen A. Langer category:cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.stat-mech cs.CE cs.LG  published:2017-09-24 summary:Digital image correlation (DIC) is a well-established, non-invasive technique for tracking and quantifying the deformation of mechanical samples under strain. While it provides an obvious way to observe incremental and aggregate displacement information, it seems likely that DIC data sets, which after all reflect the spatially-resolved response of a microstructure to loads, contain much richer information than has generally been extracted from them. In this paper, we demonstrate a machine-learning approach to quantifying the prior deformation history of a crystalline sample based on its response to a subsequent DIC test. This prior deformation history is encoded in the microstructure through the inhomogeneity of the dislocation microstructure, and in the spatial correlations of the dislocation patterns, which mediate the system's response to the DIC test load. Our domain consists of deformed crystalline thin films generated by a discrete dislocation plasticity simulation. We explore the range of applicability of machine learning (ML) for typical experimental protocols, and as a function of possible size effects and stochasticity. Plasticity size effects may directly influence the data, rendering unsupervised techniques unable to distinguish different plasticity regimes. version:1
arxiv-1709-02974 | A Deep Structured Learning Approach Towards Automating Connectome Reconstruction from 3D Electron Micrographs | http://arxiv.org/abs/1709.02974 | id:1709.02974 author:Jan Funke, Fabian David Tschopp, William Grisaitis, Arlo Sheridan, Chandan Singh, Stephan Saalfeld, Srinivas C. Turaga category:cs.CV  published:2017-09-09 summary:We present a deep structured learning method for neuron segmentation from 3D electron microscopy (EM) which improves significantly upon the state of the art in terms of accuracy and scalability. Our method consists of a 3D U-Net classifier predicting affinity graphs on voxels, followed by iterative region agglomeration. We train the U-Net using a new structured loss based on MALIS that encourages topological correctness. Our extension consists of two parts: First, an $O(n\log(n))$ method to compute the loss gradient, improving over the originally proposed $O(n^2)$ algorithm. Second, we compute the gradient in two separate passes to avoid spurious contributions in early training stages. Our affinity predictions are accurate enough that simple agglomeration outperforms more involved methods used earlier on inferior predictions. We present results on three datasets (CREMI, FIB, and SegEM) of different imaging techniques and animals and achieve improvements over previous results of 27%, 15%, and 250%. Our findings suggest that a single 3D segmentation strategy can be applied to both isotropic and anisotropic EM data. The runtime of our method scales with $O(n)$ in the size of the volume and achieves a throughput of about 2.6 seconds per megavoxel, allowing processing of very large datasets. version:3
arxiv-1709-01591 | Improving Landmark Localization with Semi-Supervised Learning | http://arxiv.org/abs/1709.01591 | id:1709.01591 author:Sina Honari, Pavlo Molchanov, Stephen Tyree, Pascal Vincent, Christopher Pal, Jan Kautz category:cs.CV  published:2017-09-05 summary:We present two techniques to improve landmark localization from partially annotated datasets. Our primary goal is to leverage the common situation where precise landmark locations are only provided for a small data subset, but where class labels for classification tasks related to the landmarks are more abundantly available. We propose a new architecture for landmark localization, where training with class labels acts as an auxiliary signal to guide the landmark localization on unlabeled data. A key aspect of our approach is that errors can be backpropagated through a complete landmark localization model. We also propose and explore an unsupervised learning technique for landmark localization based on having a model predict equivariant landmarks with respect to transformations applied to the image. We show that this technique, used as additional regularization, improves landmark prediction considerably and can learn effective detectors even when only a small fraction of the dataset has labels for landmarks. We present results on two toy datasets and three real datasets, with hands and faces, respectively, showing the performance gain of our method on each. version:2
arxiv-1709-08203 | Survey of Recent Advances in Visual Question Answering | http://arxiv.org/abs/1709.08203 | id:1709.08203 author:Supriya Pandhre, Shagun Sodhani category:cs.CV  published:2017-09-24 summary:Visual Question Answering (VQA) presents a unique challenge as it requires the ability to understand and encode the multi-modal inputs - in terms of image processing and natural language processing. The algorithm further needs to learn how to perform reasoning over this multi-modal representation so it can answer the questions correctly. This paper presents a survey of different approaches proposed to solve the problem of Visual Question Answering. We also describe the current state of the art model in later part of paper. In particular, the paper describes the approaches taken by various algorithms to extract image features, text features and the way these are employed to predict answers. We also briefly discuss the experiments performed to evaluate the VQA models and report their performances on diverse datasets including newly released VQA2.0[8]. version:1
arxiv-1708-00790 | Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition | http://arxiv.org/abs/1708.00790 | id:1708.00790 author:Yong Jiang, Wenjuan Han, Kewei Tu category:cs.CL  published:2017-08-02 summary:Unsupervised dependency parsing aims to learn a dependency parser from unannotated sentences. Existing work focuses on either learning generative models using the expectation-maximization algorithm and its variants, or learning discriminative models using the discriminative clustering algorithm. In this paper, we propose a new learning strategy that learns a generative model and a discriminative model jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-of-the-art performance on thirty languages. version:2
arxiv-1709-08196 | Identifying Phrasemes via Interlingual Association Measures -- A Data-driven Approach on Dependency-parsed and Word-aligned Parallel Corpora | http://arxiv.org/abs/1709.08196 | id:1709.08196 author:Johannes Graën category:cs.CL  published:2017-09-24 summary:This is a preprint of the article "Identifying Phrasemes via Interlingual Association Measures" that was presented in February 2016 at the LeKo (Lexical combinations and typified speech in a multilingual context) conference in Innsbruck. version:1
arxiv-1709-02656 | Deep Packet: A Novel Approach For Encrypted Traffic Classification Using Deep Learning | http://arxiv.org/abs/1709.02656 | id:1709.02656 author:Mohammad Lotfollahi, Ramin Shirali Hossein Zade, Mahdi Jafari Siavoshani, Mohammdsadegh Saberian category:cs.LG cs.CR cs.NI  published:2017-09-08 summary:Network traffic classification has become significantly important with rapid growth of current Internet network and online applications. There have been numerous studies on this topic which have led to many different approaches. Most of these approaches use predefined features extracted by an expert in order to classify network traffic. In contrast, in this study, we propose a \emph{deep learning} based approach which integrates both feature extraction and classification phases into one system. Our proposed scheme, called "Deep Packet," can handle both \emph{traffic characterization}, in which the network traffic is categorized into major classes (\eg, FTP and P2P), and \emph{application identification} in which identification of end-user applications (\eg, BitTorrent and Skype) is desired. Contrary to the most of current methods, Deep Packet can identify encrypted traffic and also distinguishes between VPN and non-VPN network traffic. After an initial pre-processing phase on data, packets are fed into Deep Packet framework that embeds \emph{stacked autoencoder} and \emph{convolution neural network} (CNN) in order to classify network traffic. Deep packet with CNN as its classification model achieved $F_{1}$ score of $0.95$ in application identification task and it also accomplished $F_{1}$ score of $0.97$ in traffic characterization task. To the best of our knowledge, Deep Packet outperforms all of the proposed classification methods on UNB ISCX VPN-nonVPN dataset. version:2
arxiv-1709-08174 | Function approximation with ReLU-like zonal function networks | http://arxiv.org/abs/1709.08174 | id:1709.08174 author:Hrushikesh N. Mhaskar category:cs.LG math.NA  published:2017-09-24 summary:A zonal function (ZF) network on the $q$ dimensional sphere $\mathbb{S}^q$ is a network of the form $\mathbf{x}\mapsto \sum_{k=1}^n a_k\phi(\mathbf{x}\cdot\mathbf{x}_k)$ where $\phi :[-1,1]\to\mathbf{R}$ is the activation function, $\mathbf{x}_k\in\mathbb{S}^q$ are the centers, and $a_k\in\mathbb{R}$. While the approximation properties of such networks are well studied in the context of positive definite activation functions, recent interest in deep and shallow networks motivate the study of activation functions of the form $\phi(t)= t $, which are not positive definite. In this paper, we define an appropriate smoothess class and establish approximation properties of such networks for functions in this class. The centers can be chosen independently of the target function, and the coefficients are linear combinations of the training data. The constructions preserve rotational symmetries. version:1
arxiv-1709-08172 | Can Image Retrieval help Visual Saliency Detection? | http://arxiv.org/abs/1709.08172 | id:1709.08172 author:Shuang Li, Peter Mathews category:cs.CV  published:2017-09-24 summary:We propose a novel image retrieval framework for visual saliency detection using information about salient objects contained within bounding box annotations for similar images. For each test image, we train a customized SVM from similar example images to predict the saliency values of its object proposals and generate an external saliency map (ES) by aggregating the regional scores. To overcome limitations caused by the size of the training dataset, we also propose an internal optimization module which computes an internal saliency map (IS) by measuring the low-level contrast information of the test image. The two maps, ES and IS, have complementary properties so we take a weighted combination to further improve the detection performance. Experimental results on several challenging datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods. version:1
arxiv-1709-08164 | Tensor-Based Classifiers for Hyperspectral Data Analysis | http://arxiv.org/abs/1709.08164 | id:1709.08164 author:Konstantinos Makantasis, Anastasios Doulamis, Nikolaos Doulamis, Antonis Nikitakis category:cs.CV  published:2017-09-24 summary:In this work, we present tensor-based linear and nonlinear models for hyperspectral data classification and analysis. By exploiting principles of tensor algebra, we introduce new classification architectures, the weight parameters of which satisfies the {\it rank}-1 canonical decomposition property. Then, we introduce learning algorithms to train both the linear and the non-linear classifier in a way to i) to minimize the error over the training samples and ii) the weight coefficients satisfies the {\it rank}-1 canonical decomposition property. The advantages of the proposed classification model is that i) it reduces the number of parameters required and thus reduces the respective number of training samples required to properly train the model, ii) it provides a physical interpretation regarding the model coefficients on the classification output and iii) it retains the spatial and spectral coherency of the input samples. To address issues related with linear classification, characterizing by low capacity, since it can produce rules that are linear in the input space, we introduce non-linear classification models based on a modification of a feedforward neural network. We call the proposed architecture {\it rank}-1 Feedfoward Neural Network (FNN), since their weights satisfy the {\it rank}-1 caconical decomposition property. Appropriate learning algorithms are also proposed to train the network. Experimental results and comparisons with state of the art classification methods, either linear (e.g., SVM) and non-linear (e.g., deep learning) indicates the outperformance of the proposed scheme, especially in cases where a small number of training samples are available. Furthermore, the proposed tensor-based classfiers are evaluated against their capabilities in dimensionality reduction. version:1
arxiv-1709-00308 | A Comprehensive Survey of Deep Learning in Remote Sensing: Theories, Tools and Challenges for the Community | http://arxiv.org/abs/1709.00308 | id:1709.00308 author:John E. Ball, Derek T. Anderson, Chee Seng Chan category:cs.CV  published:2017-09-01 summary:In recent years, deep learning (DL), a re-branding of neural networks (NNs), has risen to the top in numerous areas, namely computer vision (CV), speech recognition, natural language processing, etc. Whereas remote sensing (RS) possesses a number of unique challenges, primarily related to sensors and applications, inevitably RS draws from many of the same theories as CV; e.g., statistics, fusion, and machine learning, to name a few. This means that the RS community should be aware of, if not at the leading edge of, of advancements like DL. Herein, we provide the most comprehensive survey of state-of-the-art RS DL research. We also review recent new developments in the DL field that can be used in DL for RS. Namely, we focus on theories, tools and challenges for the RS community. Specifically, we focus on unsolved challenges and opportunities as it relates to (i) inadequate data sets, (ii) human-understandable solutions for modelling physical phenomena, (iii) Big Data, (iv) non-traditional heterogeneous data sources, (v) DL architectures and learning algorithms for spectral, spatial and temporal data, (vi) transfer learning, (vii) an improved theoretical understanding of DL systems, (viii) high barriers to entry, and (ix) training and optimizing the DL. version:2
arxiv-1709-08154 | Rapid and Robust Automated Macroscopic Wood Identification System using Smartphone with Macro-lens | http://arxiv.org/abs/1709.08154 | id:1709.08154 author:Xin Jie Tang, Yong Haur Tay, Nordahlia Abdullah Siam, Seng Choon Lim category:cs.CY cs.CV  published:2017-09-24 summary:Wood Identification has never been more important to serve the purpose of global forest species protection and timber regulation. Macroscopic level wood identification practiced by wood anatomists can identify wood up to genus level. This is sufficient to serve as a frontline identification to fight against illegal wood logging and timber trade for law enforcement authority. However, frontline enforcement official may lack of the accuracy and confidence of a well trained wood anatomist. Hence, computer assisted method such as machine vision methods are developed to do rapid field identification for law enforcement official. In this paper, we proposed a rapid and robust macroscopic wood identification system using machine vision method with off-the-shelf smartphone and retrofitted macro-lens. Our system is cost effective, easily accessible, fast and scalable at the same time provides human-level accuracy on identification. Camera-enabled smartphone with Internet connectivity coupled with a macro-lens provides a simple and effective digital acquisition of macroscopic wood images which are essential for macroscopic wood identification. The images are immediately streamed to a cloud server via Internet connection for identification which are done within seconds. version:1
arxiv-1709-08148 | On the Optimality of Kernel-Embedding Based Goodness-of-Fit Tests | http://arxiv.org/abs/1709.08148 | id:1709.08148 author:Krishnakumar Balasubramanian, Tong Li, Ming Yuan category:stat.ML math.ST stat.ME stat.TH  published:2017-09-24 summary:The reproducing kernel Hilbert space (RKHS) embedding of distributions offers a general and flexible framework for testing problems in arbitrary domains and has attracted considerable amount of attention in recent years. To gain insights into their operating characteristics, we study here the statistical performance of such approaches within a minimax framework. Focusing on the case of goodness-of-fit tests, our analyses show that a vanilla version of the kernel-embedding based test could be suboptimal, and suggest a simple remedy by moderating the embedding. We prove that the moderated approach provides optimal tests for a wide range of deviations from the null and can also be made adaptive over a large collection of interpolation spaces. Numerical experiments are presented to further demonstrate the merits of our approach. version:1
arxiv-1709-08142 | Domain Adaptation from Synthesis to Reality in Single-model Detector for Video Smoke Detection | http://arxiv.org/abs/1709.08142 | id:1709.08142 author:Gao Xu, Yongming Zhang, Qixing Zhang, Gaohua Lin, Jinjun Wang category:cs.CV  published:2017-09-24 summary:This paper proposes a method for video smoke detection using synthetic smoke samples. The virtual data can automatically offer precise and rich annotated samples. However, the learning of smoke representations will be hurt by the appearance gap between real and synthetic smoke samples. The existed researches mainly work on the adaptation to samples extracted from original annotated samples. These methods take the object detection and domain adaptation as two independent parts. To train a strong detector with rich synthetic samples, we construct the adaptation to the detection layer of state-of-the-art single-model detectors (SSD and MS-CNN). The training procedure is an end-to-end stage. The classification, location and adaptation are combined in the learning. The performance of the proposed model surpasses the original baseline in our experiments. Meanwhile, our results show that the detectors based on the adversarial adaptation are superior to the detectors based on the discrepancy adaptation. Code will be made publicly available on http://smoke.ustc.edu.cn. version:1
arxiv-1708-05482 | A Question Answering Approach to Emotion Cause Extraction | http://arxiv.org/abs/1708.05482 | id:1708.05482 author:Lin Gui, Jiannan Hu, Yulan He, Ruifeng Xu, Qin Lu, Jiachen Du category:cs.CL  published:2017-08-18 summary:Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure. version:2
arxiv-1709-08135 | Weather Forecasting Error in Solar Energy Forecasting | http://arxiv.org/abs/1709.08135 | id:1709.08135 author:Hossein Sangrody, Morteza Sarailoo, Ning Zhou, Nhu Tran, Mahdi Motalleb, Elham Foruzan category:stat.ML  published:2017-09-24 summary:As renewable distributed energy resources (DERs) penetrate the power grid at an accelerating speed, it is essential for operators to have accurate solar photovoltaic (PV) energy forecasting for efficient operations and planning. Generally, observed weather data are applied in the solar PV generation forecasting model while in practice the energy forecasting is based on forecasted weather data. In this paper, a study on the uncertainty in weather forecasting for the most commonly used weather variables is presented. The forecasted weather data for six days ahead is compared with the observed data and the results of analysis are quantified by statistical metrics. In addition, the most influential weather predictors in energy forecasting model are selected. The performance of historical and observed weather data errors is assessed using a solar PV generation forecasting model. Finally, a sensitivity test is performed to identify the influential weather variables whose accurate values can significantly improve the results of energy forecasting. version:1
arxiv-1709-08130 | Simultaneous Facial Landmark Detection, Pose and Deformation Estimation under Facial Occlusion | http://arxiv.org/abs/1709.08130 | id:1709.08130 author:Yue Wu, Chao Gou, Qiang Ji category:cs.CV  published:2017-09-23 summary:Facial landmark detection, head pose estimation, and facial deformation analysis are typical facial behavior analysis tasks in computer vision. The existing methods usually perform each task independently and sequentially, ignoring their interactions. To tackle this problem, we propose a unified framework for simultaneous facial landmark detection, head pose estimation, and facial deformation analysis, and the proposed model is robust to facial occlusion. Following a cascade procedure augmented with model-based head pose estimation, we iteratively update the facial landmark locations, facial occlusion, head pose and facial de- formation until convergence. The experimental results on benchmark databases demonstrate the effectiveness of the proposed method for simultaneous facial landmark detection, head pose and facial deformation estimation, even if the images are under facial occlusion. version:1
arxiv-1709-08129 | Constrained Joint Cascade Regression Framework for Simultaneous Facial Action Unit Recognition and Facial Landmark Detection | http://arxiv.org/abs/1709.08129 | id:1709.08129 author:Yue Wu, Qiang Ji category:cs.CV  published:2017-09-23 summary:Cascade regression framework has been shown to be effective for facial landmark detection. It starts from an initial face shape and gradually predicts the face shape update from the local appearance features to generate the facial landmark locations in the next iteration until convergence. In this paper, we improve upon the cascade regression framework and propose the Constrained Joint Cascade Regression Framework (CJCRF) for simultaneous facial action unit recognition and facial landmark detection, which are two related face analysis tasks, but are seldomly exploited together. In particular, we first learn the relationships among facial action units and face shapes as a constraint. Then, in the proposed constrained joint cascade regression framework, with the help from the constraint, we iteratively update the facial landmark locations and the action unit activation probabilities until convergence. Experimental results demonstrate that the intertwined relationships of facial action units and face shapes boost the performances of both facial action unit recognition and facial landmark detection. The experimental results also demonstrate the effectiveness of the proposed method comparing to the state-of-the-art works. version:1
arxiv-1709-08128 | Constrained Deep Transfer Feature Learning and its Applications | http://arxiv.org/abs/1709.08128 | id:1709.08128 author:Yue Wu, Qiang Ji category:cs.CV  published:2017-09-23 summary:Feature learning with deep models has achieved impressive results for both data representation and classification for various vision tasks. Deep feature learning, however, typically requires a large amount of training data, which may not be feasible for some application domains. Transfer learning can be one of the approaches to alleviate this problem by transferring data from data-rich source domain to data-scarce target domain. Existing transfer learning methods typically perform one-shot transfer learning and often ignore the specific properties that the transferred data must satisfy. To address these issues, we introduce a constrained deep transfer feature learning method to perform simultaneous transfer learning and feature learning by performing transfer learning in a progressively improving feature space iteratively in order to better narrow the gap between the target domain and the source domain for effective transfer of the data from the source domain to target domain. Furthermore, we propose to exploit the target domain knowledge and incorporate such prior knowledge as a constraint during transfer learning to ensure that the transferred data satisfies certain properties of the target domain. To demonstrate the effectiveness of the proposed constrained deep transfer feature learning method, we apply it to thermal feature learning for eye detection by transferring from the visible domain. We also applied the proposed method for cross-view facial expression recognition as a second application. The experimental results demonstrate the effectiveness of the proposed method for both applications. version:1
arxiv-1709-08127 | Robust Facial Landmark Detection under Significant Head Poses and Occlusion | http://arxiv.org/abs/1709.08127 | id:1709.08127 author:Yue Wu, Qiang Ji category:cs.CV  published:2017-09-23 summary:There have been tremendous improvements for facial landmark detection on general "in-the-wild" images. However, it is still challenging to detect the facial landmarks on images with severe occlusion and images with large head poses (e.g. profile face). In fact, the existing algorithms usually can only handle one of them. In this work, we propose a unified robust cascade regression framework that can handle both images with severe occlusion and images with large head poses. Specifically, the method iteratively predicts the landmark occlusions and the landmark locations. For occlusion estimation, instead of directly predicting the binary occlusion vectors, we introduce a supervised regression method that gradually updates the landmark visibility probabilities in each iteration to achieve robustness. In addition, we explicitly add occlusion pattern as a constraint to improve the performance of occlusion prediction. For landmark detection, we combine the landmark visibility probabilities, the local appearances, and the local shapes to iteratively update their positions. The experimental results show that the proposed method is significantly better than state-of-the-art works on images with severe occlusion and images with large head poses. It is also comparable to other methods on general "in-the-wild" images. version:1
arxiv-1709-08114 | Nonconvex Low-Rank Matrix Recovery with Arbitrary Outliers via Median-Truncated Gradient Descent | http://arxiv.org/abs/1709.08114 | id:1709.08114 author:Yuanxin Li, Yuejie Chi, Huishuai Zhang, Yingbin Liang category:cs.IT math.IT stat.ML  published:2017-09-23 summary:Recent work has demonstrated the effectiveness of gradient descent for directly recovering the factors of low-rank matrices from random linear measurements in a globally convergent manner when initialized properly. However, the performance of existing algorithms is highly sensitive in the presence of outliers that may take arbitrary values. In this paper, we propose a truncated gradient descent algorithm to improve the robustness against outliers, where the truncation is performed to rule out the contributions of samples that deviate significantly from the {\em sample median} of measurement residuals adaptively in each iteration. We demonstrate that, when initialized in a basin of attraction close to the ground truth, the proposed algorithm converges to the ground truth at a linear rate for the Gaussian measurement model with a near-optimal number of measurements, even when a constant fraction of the measurements are arbitrarily corrupted. In addition, we propose a new truncated spectral method that ensures an initialization in the basin of attraction at slightly higher requirements. We finally provide numerical experiments to validate the superior performance of the proposed approach. version:1
arxiv-1709-08103 | Compact Environment-Invariant Codes for Robust Visual Place Recognition | http://arxiv.org/abs/1709.08103 | id:1709.08103 author:Unnat Jain, Vinay P. Namboodiri, Gaurav Pandey category:cs.CV  published:2017-09-23 summary:Robust visual place recognition (VPR) requires scene representations that are invariant to various environmental challenges such as seasonal changes and variations due to ambient lighting conditions during day and night. Moreover, a practical VPR system necessitates compact representations of environmental features. To satisfy these requirements, in this paper we suggest a modification to the existing pipeline of VPR systems to incorporate supervised hashing. The modified system learns (in a supervised setting) compact binary codes from image feature descriptors. These binary codes imbibe robustness to the visual variations exposed to it during the training phase, thereby, making the system adaptive to severe environmental changes. Also, incorporating supervised hashing makes VPR computationally more efficient and easy to implement on simple hardware. This is because binary embeddings can be learned over simple-to-compute features and the distance computation is also in the low-dimensional hamming space of binary codes. We have performed experiments on several challenging data sets covering seasonal, illumination and viewpoint variations. We also compare two widely used supervised hashing methods of CCAITQ and MLH and show that this new pipeline out-performs or closely matches the state-of-the-art deep learning VPR methods that are based on high-dimensional features extracted from pre-trained deep convolutional neural networks. version:1
arxiv-1709-08101 | Classical and Quantum Factors of Channels | http://arxiv.org/abs/1709.08101 | id:1709.08101 author:J. R. Mahoney, C. Aghamohammadi, J. P. Crutchfield category:quant-ph cond-mat.stat-mech cs.IT math.IT stat.ML  published:2017-09-23 summary:Given a classical channel, a stochastic map from inputs to outputs, can we replace the input with a simple intermediate variable that still yields the correct conditional output distribution? We examine two cases: first, when the intermediate variable is classical; second, when the intermediate variable is quantum. We show that the quantum variable's size is generically smaller than the classical, according to two different measures---cardinality and entropy. We demonstrate optimality conditions for a special case. We end with several related results: a proposal for extending the special case, a demonstration of the impact of quantum phases, and a case study concerning pure versus mixed states. version:1
arxiv-1709-08074 | Language Independent Acquisition of Abbreviations | http://arxiv.org/abs/1709.08074 | id:1709.08074 author:Michael R. Glass, Md Faisal Mahbub Chowdhury, Alfio M. Gliozzo category:cs.CL  published:2017-09-23 summary:This paper addresses automatic extraction of abbreviations (encompassing acronyms and initialisms) and corresponding long-form expansions from plain unstructured text. We create and are going to release a multilingual resource for abbreviations and their corresponding expansions, built automatically by exploiting Wikipedia redirect and disambiguation pages, that can be used as a benchmark for evaluation. We address a shortcoming of previous work where only the redirect pages were used, and so every abbreviation had only a single expansion, even though multiple different expansions are possible for many of the abbreviations. We also develop a principled machine learning based approach to scoring expansion candidates using different techniques such as indicators of near synonymy, topical relatedness, and surface similarity. We show improved performance over seven languages, including two with a non-Latin alphabet, relative to strong baselines. version:1
arxiv-1709-08068 | A Generic Regression Framework for Pose Recognition on Color and Depth Images | http://arxiv.org/abs/1709.08068 | id:1709.08068 author:Wenye He category:cs.CV  published:2017-09-23 summary:Cascaded regression method is a fast and accurate method on finding 2D pose of objects in RGB images. It is able to find the accurate pose of objects in an image by a great number of corrections on the good initial guess of the pose of objects. This paper explains the algorithm and shows the result of two experiments carried by the researchers. The presented new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing. Finally, we generate confidence-scored 3D proposals of several body parts by re-projecting the classification result and finding local modes. version:1
arxiv-1710-01244 | Adaptive Measurement Network for CS Image Reconstruction | http://arxiv.org/abs/1710.01244 | id:1710.01244 author:Xuemei Xie, Yuxiang Wang, Guangming Shi, Chenye Wang, Jiang Du, Zhifu Zhao category:cs.CV  published:2017-09-23 summary:Conventional compressive sensing (CS) reconstruction is very slow for its characteristic of solving an optimization problem. Convolu- tional neural network can realize fast processing while achieving compa- rable results. While CS image recovery with high quality not only de- pends on good reconstruction algorithms, but also good measurements. In this paper, we propose an adaptive measurement network in which measurement is obtained by learning. The new network consists of a fully-connected layer and ReconNet. The fully-connected layer which has low-dimension output acts as measurement. We train the fully-connected layer and ReconNet simultaneously and obtain adaptive measurement. Because the adaptive measurement fits dataset better, in contrast with random Gaussian measurement matrix, under the same measuremen- t rate, it can extract the information of scene more efficiently and get better reconstruction results. Experiments show that the new network outperforms the original one. version:1
arxiv-1709-08041 | Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks | http://arxiv.org/abs/1709.08041 | id:1709.08041 author:Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari category:cs.SD cs.LG eess.AS  published:2017-09-23 summary:A method for statistical parametric speech synthesis incorporating generative adversarial networks (GANs) is proposed. Although powerful deep neural networks (DNNs) techniques can be applied to artificially synthesize speech waveform, the synthetic speech quality is low compared with that of natural speech. One of the issues causing the quality degradation is an over-smoothing effect often observed in the generated speech parameters. A GAN introduced in this paper consists of two neural networks: a discriminator to distinguish natural and generated samples, and a generator to deceive the discriminator. In the proposed framework incorporating the GANs, the discriminator is trained to distinguish natural and generated speech parameters, while the acoustic models are trained to minimize the weighted sum of the conventional minimum generation loss and an adversarial loss for deceiving the discriminator. Since the objective of the GANs is to minimize the divergence (i.e., distribution difference) between the natural and generated speech parameters, the proposed method effectively alleviates the over-smoothing effect on the generated speech parameters. We evaluated the effectiveness for text-to-speech and voice conversion, and found that the proposed method can generate more natural spectral parameters and $F_0$ than conventional minimum generation error training algorithm regardless its hyper-parameter settings. Furthermore, we investigated the effect of the divergence of various GANs, and found that a Wasserstein GAN minimizing the Earth-Mover's distance works the best in terms of improving synthetic speech quality. version:1
arxiv-1709-08025 | Deep Learning for Secure Mobile Edge Computing | http://arxiv.org/abs/1709.08025 | id:1709.08025 author:Yuanfang Chen, Yan Zhang, Sabita Maharjan category:cs.CR cs.LG cs.NI  published:2017-09-23 summary:Mobile edge computing (MEC) is a promising approach for enabling cloud-computing capabilities at the edge of cellular networks. Nonetheless, security is becoming an increasingly important issue in MEC-based applications. In this paper, we propose a deep-learning-based model to detect security threats. The model uses unsupervised learning to automate the detection process, and uses location information as an important feature to improve the performance of detection. Our proposed model can be used to detect malicious applications at the edge of a cellular network, which is a serious security threat. Extensive experiments are carried out with 10 different datasets, the results of which illustrate that our deep-learning-based model achieves an average gain of 6% accuracy compared with state-of-the-art machine learning algorithms. version:1
arxiv-1708-06633 | Nonparametric regression using deep neural networks with ReLU activation function | http://arxiv.org/abs/1708.06633 | id:1708.06633 author:Johannes Schmidt-Hieber category:math.ST cs.LG stat.ML stat.TH 62G08  published:2017-08-22 summary:Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to log n-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While there is a lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large networks with number of potential parameters being much bigger than the sample size. The analysis gives some insights why multilayer feedforward neural networks perform well in practice. Interestingly, the depth (number of layers) of the neural network architectures plays an important role and our theory suggests that scaling the network depth with the logarithm of the sample size is natural. version:2
arxiv-1709-08015 | Combining Machine Learning and Physics to Understand Glassy Systems | http://arxiv.org/abs/1709.08015 | id:1709.08015 author:Samuel S. Schoenholz category:stat.ML cond-mat.soft cond-mat.stat-mech  published:2017-09-23 summary:Our understanding of supercooled liquids and glasses has lagged significantly behind that of simple liquids and crystalline solids. This is in part due to the many possibly relevant degrees of freedom that are present due to the disorder inherent to these systems and in part to non-equilibrium effects which are difficult to treat in the standard context of statistical physics. Together these issues have resulted in a field whose theories are under-constrained by experiment and where fundamental questions are still unresolved. Mean field results have been successful in infinite dimensions but it is unclear to what extent they apply to realistic systems and assume uniform local structure. At odds with this are theories premised on the existence of structural defects. However, until recently it has been impossible to find structural signatures that are predictive of dynamics. Here we summarize and recast the results from several recent papers offering a data driven approach to building a phenomenological theory of disordered materials by combining machine learning with physical intuition. version:1
arxiv-1709-08011 | Long Short-Term Memory for Japanese Word Segmentation | http://arxiv.org/abs/1709.08011 | id:1709.08011 author:Yoshiaki Kitagawa, Mamoru Komachi category:cs.CL  published:2017-09-23 summary:This study presents a Long Short-Term Memory (LSTM) neural network approach to Japanese word segmentation (JWS). Previous studies on Chinese word segmentation (CWS) succeeded in using recurrent neural networks such as LSTM and gated recurrent units (GRU). However, in contrast to Chinese, Japanese includes several character types, such as hiragana, katakana, and kanji, that produce orthographic variations and increase the difficulty of word segmentation. Additionally, it is important for JWS tasks to consider a global context, and yet traditional JWS approaches rely on local features. In order to address this problem, this study proposes employing an LSTM-based approach to JWS. The experimental results indicate that the proposed model achieves state-of-the-art accuracy with respect to various Japanese corpora. version:1
arxiv-1709-07993 | A semi-automated segmentation method for detection of pulmonary embolism in True-FISP MRI sequences | http://arxiv.org/abs/1709.07993 | id:1709.07993 author:Luis R Soenksen, Luis Jiménez-Angeles, Gabriela Melendez, Aloha Meave category:cs.CV  published:2017-09-23 summary:Pulmonary embolism (PE) is a highly mortal disease, currently assessed by pulmonary CT angiography. True-FISP MRI has emerged as an innocuous alternative that does not hold many of the limitations of x-ray imaging. However, True-FISP MRI is very sensitive to turbulent blood flow, generating artifacts that may resemble fake clots in the pulmonary vasculature. These misinterpretations reduce its overall diagnostic accuracy to 94%, limiting a wider use in clinical environments. A new segmentation algorithm is proposed to confirm the presence of real pulmonary clots in True-FISP MR images by quantitative means, measuring the shape, intensity, and solidity of the formation. The algorithm was evaluated in 37 patients. The developed method increased the diagnostic accuracy of expert observers assessing Pulmonary True-FISP MRI sequences by 6% without the use of ionizing radiation, achieving a diagnostic accuracy comparable to standard CT angiography. version:1
arxiv-1709-07992 | Visual Reference Resolution using Attention Memory for Visual Dialog | http://arxiv.org/abs/1709.07992 | id:1709.07992 author:Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, Leonid Sigal category:cs.CV  published:2017-09-23 summary:Visual dialog is a task of answering a series of inter-dependent questions given an input image, and often requires to resolve visual references among the questions. This problem is different from visual question answering (VQA), which relies on spatial attention (a.k.a. visual grounding) estimated from an image and question pair. We propose a novel attention mechanism that exploits visual attentions in the past to resolve the current reference in the visual dialog scenario. The proposed model is equipped with an associative attention memory storing a sequence of previous (attention, key) pairs. From this memory, the model retrieves previous attention, taking into account recency, that is most relevant for the current question, in order to resolve potentially ambiguous reference(s). The model then merges the retrieved attention with the tentative one to obtain the final attention for the current question; specifically, we use dynamic parameter prediction to combine the two attentions conditioned on the question. Through extensive experiments on a new synthetic visual dialog dataset, we show that our model significantly outperforms the state-of-the-art (by ~16 % points) in the situation where the visual reference resolution plays an important role. Moreover, the proposed model presents superior performance (~2 % points improvement) in the Visual Dialog dataset, despite having significantly fewer parameters than the baselines. version:1
arxiv-1709-07984 | A Grassmannian Approach to Zero-Shot Learning for Network Intrusion Detection | http://arxiv.org/abs/1709.07984 | id:1709.07984 author:Jorge Rivero, Bernardete Ribeiro, Ning Chen, Fátima Silva Leite category:cs.CR cs.LG  published:2017-09-23 summary:One of the main problems in Network Intrusion Detection comes from constant rise of new attacks, so that not enough labeled examples are available for the new classes of attacks. Traditional Machine Learning approaches hardly address such problem. This can be overcome with Zero-Shot Learning, a new approach in the field of Computer Vision, which can be described in two stages: the Attribute Learning and the Inference Stage. The goal of this paper is to propose a new Inference Stage algorithm for Network Intrusion Detection. In order to attain this objective, we firstly put forward an experimental setup for the evaluation of the Zero-Shot Learning in Network Intrusion Detection related tasks. Secondly, a decision tree based algorithm is applied to extract rules for generating the attributes in the AL stage. Finally, using a representation of a Zero-Shot Class as a point in the Grassmann manifold, an explicit formula for the shortest distance between points in that manifold can be used to compute the geodesic distance between the Zero-Shot Classes which represent the new attacks and the Known Classes corresponding to the attack categories. The experimental results in the datasets KDD Cup 99 and NSL-KDD show that our approach with Zero-Shot Learning successfully addresses the Network Intrusion Detection problem. version:1
arxiv-1708-09485 | Learning Invariant Riemannian Geometric Representations Using Deep Nets | http://arxiv.org/abs/1708.09485 | id:1708.09485 author:Suhas Lohit, Pavan Turaga category:cs.CV  published:2017-08-30 summary:Non-Euclidean constraints are inherent in many kinds of data in computer vision and machine learning, typically as a result of specific invariance requirements that need to be respected during high-level inference. Often, these geometric constraints can be expressed in the language of Riemannian geometry, where conventional vector space machine learning does not apply directly. The central question this paper deals with is: How does one train deep neural nets whose final outputs are elements on a Riemannian manifold? To answer this, we propose a general framework for manifold-aware training of deep neural networks -- we utilize tangent spaces and exponential maps in order to convert the proposed problem into a form that allows us to bring current advances in deep learning to bear upon this problem. We describe two specific applications to demonstrate this approach: prediction of probability distributions for multi-class image classification, and prediction of illumination-invariant subspaces from a single face-image via regression on the Grassmannian. These applications show the generality of the proposed framework, and result in improved performance over baselines that ignore the geometry of the output space. In addition to solving this specific problem, we believe this paper opens new lines of enquiry centered on the implications of Riemannian geometry on deep architectures. version:2
arxiv-1709-07875 | Elliptification of Rectangular Imagery | http://arxiv.org/abs/1709.07875 | id:1709.07875 author:Chamberlain Fong category:eess.IV cs.CV  published:2017-09-22 summary:We present and discuss different algorithms for converting rectangular imagery into elliptical regions. We will focus primarily on methods that use mathematical mappings with explicit and invertible equations. We will also present different post-processing effects that could be applied to enhance the resulting images and minimize distortion. version:1
arxiv-1709-07944 | MR Acquisition-Invariant Representation Learning | http://arxiv.org/abs/1709.07944 | id:1709.07944 author:Wouter M. Kouw, Marco Loog, Lambertus W. Bartels, Adriënne M. Mendrik category:cs.CV stat.ML  published:2017-09-22 summary:Voxelwise classification is a popular and effective method for tissue quantification in brain magnetic resonance imaging (MRI) scans. However, there are often large differences over sets of MRI scans due to how they were acquired (i.e. field strength, vendor, protocol), that lead to variation in, among others, pixel intensities, tissue contrast, signal-to-noise ratio, resolution, slice thickness and magnetic field inhomogeneities. Classifiers trained on data from a specific scanner fail or under-perform when applied to data that was differently acquired. In order to address this lack of generalization, we propose a Siamese neural network (MRAI-net) to learn a representation that minimizes the between-scanner variation, while maintaining the contrast between brain tissues necessary for brain tissue quantification. The proposed MRAI-net was evaluated on both simulated and real MRI data. After learning the MR acquisition invariant representation, any supervised classifier can be applied. In this paper we showed that applying a linear classifier on the MRAI representation outperforms supervised convolutional neural network classifiers for tissue classification when little target training data is available. version:1
arxiv-1705-07213 | MTDeep: Boosting the Security of Deep Neural Nets Against Adversarial Attacks with Moving Target Defense | http://arxiv.org/abs/1705.07213 | id:1705.07213 author:Sailik Sengupta, Tathagata Chakraborti, Subbarao Kambhampati category:cs.LG cs.CR cs.GT  published:2017-05-19 summary:Recent works on gradient-based attacks and universal perturbations can adversarially modify images to bring down the accuracy of state-of-the-art classification techniques based on deep neural networks to as low as 10\% on popular datasets like MNIST and ImageNet. The design of general defense strategies against a wide range of such attacks remains a challenging problem. In this paper, we derive inspiration from recent advances in the fields of cybersecurity and multi-agent systems and propose to use the concept of Moving Target Defense (MTD) for increasing the robustness of a set of deep networks against such adversarial attacks. To this end, we formalize and exploit the notion of differential immunity of an ensemble of networks to specific attacks. To classify an input image, a trained network is picked from this set of networks by formulating the interaction between a Defender (who hosts the classification networks) and their (Legitimate and Malicious) Users as a repeated Bayesian Stackelberg Game (BSG). We empirically show that our approach, MTDeep reduces misclassification on perturbed images for MNIST and ImageNet datasets while maintaining high classification accuracy on legitimate test images. Lastly, we demonstrate that our framework can be used in conjunction with any existing defense mechanism to provide more resilience to adversarial attacks than those defense mechanisms by themselves. version:2
