arxiv-1206-5277 | Accuracy Bounds for Belief Propagation | http://arxiv.org/abs/1206.5277 | id:1206.5277 author:Alexander T. Ihler category:cs.AI cs.LG stat.ML  published:2012-06-20 summary:The belief propagation (BP) algorithm is widely applied to perform approximate inference on arbitrary graphical models, in part due to its excellent empirical properties and performance. However, little is known theoretically about when this algorithm will perform well. Using recent analysis of convergence and stability properties in BP and new results on approximations in binary systems, we derive a bound on the error in BP's estimates for pairwise Markov random fields over discrete valued random variables. Our bound is relatively simple to compute, and compares favorably with a previous method of bounding the accuracy of BP. version:1
arxiv-1206-4522 | BADREX: In situ expansion and coreference of biomedical abbreviations using dynamic regular expressions | http://arxiv.org/abs/1206.4522 | id:1206.4522 author:Phil Gooch category:cs.CL I.2.7; I.1.2; J.3  published:2012-06-20 summary:BADREX uses dynamically generated regular expressions to annotate term definition-term abbreviation pairs, and corefers unpaired acronyms and abbreviations back to their initial definition in the text. Against the Medstract corpus BADREX achieves precision and recall of 98% and 97%, and against a much larger corpus, 90% and 85%, respectively. BADREX yields improved performance over previous approaches, requires no training data and allows runtime customisation of its input parameters. BADREX is freely available from https://github.com/philgooch/BADREX-Biomedical-Abbreviation-Expander as a plugin for the General Architecture for Text Engineering (GATE) framework and is licensed under the GPLv3. version:1
arxiv-1206-5274 | On Discarding, Caching, and Recalling Samples in Active Learning | http://arxiv.org/abs/1206.5274 | id:1206.5274 author:Ashish Kapoor, Eric J. Horvitz category:cs.LG stat.ML  published:2012-06-20 summary:We address challenges of active learning under scarce informational resources in non-stationary environments. In real-world settings, data labeled and integrated into a predictive model may become invalid over time. However, the data can become informative again with switches in context and such changes may indicate unmodeled cyclic or other temporal dynamics. We explore principles for discarding, caching, and recalling labeled data points in active learning based on computations of value of information. We review key concepts and study the value of the methods via investigations of predictive performance and costs of acquiring data for simulated and real-world data sets. version:1
arxiv-1206-5270 | Nonparametric Bayes Pachinko Allocation | http://arxiv.org/abs/1206.5270 | id:1206.5270 author:Wei Li, David Blei, Andrew McCallum category:cs.IR cs.LG stat.ML  published:2012-06-20 summary:Recent advances in topic models have explored complicated structured distributions to represent topic correlation. For example, the pachinko allocation model (PAM) captures arbitrary, nested, and possibly sparse correlations between topics using a directed acyclic graph (DAG). While PAM provides more flexibility and greater expressive power than previous models like latent Dirichlet allocation (LDA), it is also more difficult to determine the appropriate topic structure for a specific dataset. In this paper, we propose a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP). Although the HDP can capture topic correlations defined by nested data structure, it does not automatically discover such correlations from unstructured data. By assuming an HDP-based prior for PAM, we are able to learn both the number of topics and how the topics are correlated. We evaluate our model on synthetic and real-world text datasets, and show that nonparametric PAM achieves performance matching the best of PAM without manually tuning the number of topics. version:1
arxiv-1206-5267 | Collaborative Filtering and the Missing at Random Assumption | http://arxiv.org/abs/1206.5267 | id:1206.5267 author:Benjamin Marlin, Richard S. Zemel, Sam Roweis, Malcolm Slaney category:cs.LG cs.IR stat.ML  published:2012-06-20 summary:Rating prediction is an important application, and a popular research topic in collaborative filtering. However, both the validity of learning algorithms, and the validity of standard testing procedures rest on the assumption that missing ratings are missing at random (MAR). In this paper we present the results of a user study in which we collect a random sample of ratings from current users of an online radio service. An analysis of the rating data collected in the study shows that the sample of random ratings has markedly different properties than ratings of user-selected songs. When asked to report on their own rating behaviour, a large number of users indicate they believe their opinion of a song does affect whether they choose to rate that song, a violation of the MAR condition. Finally, we present experimental results showing that incorporating an explicit model of the missing data mechanism can lead to significant improvements in prediction performance on the random sample of ratings. version:1
arxiv-1206-5265 | Consensus ranking under the exponential model | http://arxiv.org/abs/1206.5265 | id:1206.5265 author:Marina Meila, Kapil Phadnis, Arthur Patterson, Jeff A. Bilmes category:cs.LG cs.AI stat.ML  published:2012-06-20 summary:We analyze the generalized Mallows model, a popular exponential model over rankings. Estimating the central (or consensus) ranking from data is NP-hard. We obtain the following new results: (1) We show that search methods can estimate both the central ranking pi0 and the model parameters theta exactly. The search is n! in the worst case, but is tractable when the true distribution is concentrated around its mode; (2) We show that the generalized Mallows model is jointly exponential in (pi0; theta), and introduce the conjugate prior for this model class; (3) The sufficient statistics are the pairwise marginal probabilities that item i is preferred to item j. Preliminary experiments confirm the theoretical predictions and compare the new algorithm and existing heuristics. version:1
arxiv-1206-5264 | Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods | http://arxiv.org/abs/1206.5264 | id:1206.5264 author:Gergely Neu, Csaba Szepesvari category:cs.LG stat.ML  published:2012-06-20 summary:In this paper we propose a novel gradient algorithm to learn a policy from an expert's observed behavior assuming that the expert behaves optimally with respect to some unknown reward function of a Markovian Decision Problem. The algorithm's aim is to find a reward function such that the resulting optimal policy matches well the expert's observed behavior. The main difficulty is that the mapping from the parameters to policies is both nonsmooth and highly redundant. Resorting to subdifferentials solves the first difficulty, while the second one is over- come by computing natural gradients. We tested the proposed method in two artificial domains and found it to be more reliable and efficient than some previous methods. version:1
arxiv-1206-5263 | Reading Dependencies from Polytree-Like Bayesian Networks | http://arxiv.org/abs/1206.5263 | id:1206.5263 author:Jose M. Pena category:cs.AI cs.LG stat.ML  published:2012-06-20 summary:We present a graphical criterion for reading dependencies from the minimal directed independence map G of a graphoid p when G is a polytree and p satisfies composition and weak transitivity. We prove that the criterion is sound and complete. We argue that assuming composition and weak transitivity is not too restrictive. version:1
arxiv-1206-5261 | Mixture-of-Parents Maximum Entropy Markov Models | http://arxiv.org/abs/1206.5261 | id:1206.5261 author:David S. Rosenberg, Dan Klein, Ben Taskar category:cs.LG cs.AI stat.ML  published:2012-06-20 summary:We present the mixture-of-parents maximum entropy Markov model (MoP-MEMM), a class of directed graphical models extending MEMMs. The MoP-MEMM allows tractable incorporation of long-range dependencies between nodes by restricting the conditional distribution of each node to be a mixture of distributions given the parents. We show how to efficiently compute the exact marginal posterior node distributions, regardless of the range of the dependencies. This enables us to model non-sequential correlations present within text documents, as well as between interconnected documents, such as hyperlinked web pages. We apply the MoP-MEMM to a named entity recognition task and a web page classification task. In each, our model shows significant improvement over the basic MEMM, and is competitive with other long-range sequence models that use approximate inference. version:1
arxiv-1206-5256 | Discovering Patterns in Biological Sequences by Optimal Segmentation | http://arxiv.org/abs/1206.5256 | id:1206.5256 author:Joseph Bockhorst, Nebojsa Jojic category:cs.CE cs.LG q-bio.QM stat.AP  published:2012-06-20 summary:Computational methods for discovering patterns of local correlations in sequences are important in computational biology. Here we show how to determine the optimal partitioning of aligned sequences into non-overlapping segments such that positions in the same segment are strongly correlated while positions in different segments are not. Our approach involves discovering the hidden variables of a Bayesian network that interact with observed sequences so as to form a set of independent mixture models. We introduce a dynamic program to efficiently discover the optimal segmentation, or equivalently the optimal set of hidden variables. We evaluate our approach on two computational biology tasks. One task is related to the design of vaccines against polymorphic pathogens and the other task involves analysis of single nucleotide polymorphisms (SNPs) in human DNA. We show how common tasks in these problems naturally correspond to inference procedures in the learned models. Error rates of our learned models for the prediction of missing SNPs are up to 1/3 less than the error rates of a state-of-the-art SNP prediction method. Source code is available at www.uwm.edu/~joebock/segmentation. version:1
arxiv-1206-5248 | Statistical Translation, Heat Kernels and Expected Distances | http://arxiv.org/abs/1206.5248 | id:1206.5248 author:Joshua Dillon, Yi Mao, Guy Lebanon, Jian Zhang category:cs.LG cs.CV cs.IR stat.ML  published:2012-06-20 summary:High dimensional structured data such as text and images is often poorly understood and misrepresented in statistical modeling. The standard histogram representation suffers from high variance and performs poorly in general. We explore novel connections between statistical translation, heat kernels on manifolds and graphs, and expected distances. These connections provide a new framework for unsupervised metric learning for text documents. Experiments indicate that the resulting distances are generally superior to their more standard counterparts. version:1
arxiv-1206-5247 | Bayesian structure learning using dynamic programming and MCMC | http://arxiv.org/abs/1206.5247 | id:1206.5247 author:Daniel Eaton, Kevin Murphy category:cs.LG stat.ML  published:2012-06-20 summary:MCMC methods for sampling from the space of DAGs can mix poorly due to the local nature of the proposals that are commonly used. It has been shown that sampling from the space of node orders yields better results [FK03, EW06]. Recently, Koivisto and Sood showed how one can analytically marginalize over orders using dynamic programming (DP) [KS04, Koi06]. Their method computes the exact marginal posterior edge probabilities, thus avoiding the need for MCMC. Unfortunately, there are four drawbacks to the DP technique: it can only use modular priors, it can only compute posteriors over modular features, it is difficult to compute a predictive density, and it takes exponential time and space. We show how to overcome the first three of these problems by using the DP algorithm as a proposal distribution for MCMC in DAG space. We show that this hybrid technique converges to the posterior faster than other methods, resulting in more accurate structure learning and higher predictive likelihoods on test data. version:1
arxiv-1206-5245 | A new parameter Learning Method for Bayesian Networks with Qualitative Influences | http://arxiv.org/abs/1206.5245 | id:1206.5245 author:Ad Feelders category:cs.AI cs.LG stat.ME  published:2012-06-20 summary:We propose a new method for parameter learning in Bayesian networks with qualitative influences. This method extends our previous work from networks of binary variables to networks of discrete variables with ordered values. The specified qualitative influences correspond to certain order restrictions on the parameters in the network. These parameters may therefore be estimated using constrained maximum likelihood estimation. We propose an alternative method, based on the isotonic regression. The constrained maximum likelihood estimates are fairly complicated to compute, whereas computation of the isotonic regression estimates only requires the repeated application of the Pool Adjacent Violators algorithm for linear orders. Therefore, the isotonic regression estimator is to be preferred from the viewpoint of computational complexity. Through experiments on simulated and real data, we show that the new learning method is competitive in performance to the constrained maximum likelihood estimator, and that both estimators improve on the standard estimator. version:1
arxiv-1206-5243 | Convergent Propagation Algorithms via Oriented Trees | http://arxiv.org/abs/1206.5243 | id:1206.5243 author:Amir Globerson, Tommi S. Jaakkola category:cs.LG stat.ML  published:2012-06-20 summary:Inference problems in graphical models are often approximated by casting them as constrained optimization problems. Message passing algorithms, such as belief propagation, have previously been suggested as methods for solving these optimization problems. However, there are few convergence guarantees for such algorithms, and the algorithms are therefore not guaranteed to solve the corresponding optimization problem. Here we present an oriented tree decomposition algorithm that is guaranteed to converge to the global optimum of the Tree-Reweighted (TRW) variational problem. Our algorithm performs local updates in the convex dual of the TRW problem - an unconstrained generalized geometric program. Primal updates, also local, correspond to oriented reparametrization operations that leave the distribution intact. version:1
arxiv-1206-5241 | Shift-Invariance Sparse Coding for Audio Classification | http://arxiv.org/abs/1206.5241 | id:1206.5241 author:Roger Grosse, Rajat Raina, Helen Kwong, Andrew Y. Ng category:cs.LG stat.ML  published:2012-06-20 summary:Sparse coding is an unsupervised learning algorithm that learns a succinct high-level representation of the inputs given only unlabeled data; it represents each input as a sparse linear combination of a set of basis functions. Originally applied to modeling the human visual cortex, sparse coding has also been shown to be useful for self-taught learning, in which the goal is to solve a supervised classification task given access to additional unlabeled data drawn from different classes than that in the supervised learning problem. Shift-invariant sparse coding (SISC) is an extension of sparse coding which reconstructs a (usually time-series) input using all of the basis functions in all possible shifts. In this paper, we present an efficient algorithm for learning SISC bases. Our method is based on iteratively solving two large convex optimization problems: The first, which computes the linear coefficients, is an L1-regularized linear least squares problem with potentially hundreds of thousands of variables. Existing methods typically use a heuristic to select a small subset of the variables to optimize, but we present a way to efficiently compute the exact solution. The second, which solves for bases, is a constrained linear least squares problem. By optimizing over complex-valued variables in the Fourier domain, we reduce the coupling between the different variables, allowing the problem to be solved efficiently. We show that SISC's learned high-level representations of speech and music provide useful features for classification tasks within those domains. When applied to classification, under certain conditions the learned features outperform state of the art spectral and cepstral features. version:1
arxiv-1206-5240 | Analysis of Semi-Supervised Learning with the Yarowsky Algorithm | http://arxiv.org/abs/1206.5240 | id:1206.5240 author:Gholam Reza Haffari, Anoop Sarkar category:cs.LG stat.ML  published:2012-06-20 summary:The Yarowsky algorithm is a rule-based semi-supervised learning algorithm that has been successfully applied to some problems in computational linguistics. The algorithm was not mathematically well understood until (Abney 2004) which analyzed some specific variants of the algorithm, and also proposed some new algorithms for bootstrapping. In this paper, we extend Abney's work and show that some of his proposed algorithms actually optimize (an upper-bound on) an objective function based on a new definition of cross-entropy which is based on a particular instantiation of the Bregman distance between probability distributions. Moreover, we suggest some new algorithms for rule-based semi-supervised learning and show connections with harmonic functions and minimum multi-way cuts in graph-based semi-supervised learning. version:1
arxiv-1101-5076 | Geometric representations for minimalist grammars | http://arxiv.org/abs/1101.5076 | id:1101.5076 author:Peter beim Graben, Sabrina Gerth category:cs.CL  published:2011-01-26 summary:We reformulate minimalist grammars as partial functions on term algebras for strings and trees. Using filler/role bindings and tensor product representations, we construct homomorphisms for these data structures into geometric vector spaces. We prove that the structure-building functions as well as simple processors for minimalist languages can be realized by piecewise linear operators in representation space. We also propose harmony, i.e. the distance of an intermediate processing step from the final well-formed state in representation space, as a measure of processing complexity. Finally, we illustrate our findings by means of two particular arithmetic and fractal representations. version:6
arxiv-1110-2136 | Active Learning Using Smooth Relative Regret Approximations with Applications | http://arxiv.org/abs/1110.2136 | id:1110.2136 author:Nir Ailon, Ron Begleiter, Esther Ezra category:cs.LG  published:2011-10-10 summary:The disagreement coefficient of Hanneke has become a central data independent invariant in proving active learning rates. It has been shown in various ways that a concept class with low complexity together with a bound on the disagreement coefficient at an optimal solution allows active learning rates that are superior to passive learning ones. We present a different tool for pool based active learning which follows from the existence of a certain uniform version of low disagreement coefficient, but is not equivalent to it. In fact, we present two fundamental active learning problems of significant interest for which our approach allows nontrivial active learning bounds. However, any general purpose method relying on the disagreement coefficient bounds only fails to guarantee any useful bounds for these problems. The tool we use is based on the learner's ability to compute an estimator of the difference between the loss of any hypotheses and some fixed "pivotal" hypothesis to within an absolute error of at most $\eps$ times the version:3
arxiv-1206-4391 | Gray Image extraction using Fuzzy Logic | http://arxiv.org/abs/1206.4391 | id:1206.4391 author:Koushik Mondal, Paramartha Dutta, Siddhartha Bhattacharyya category:cs.CV cs.AI  published:2012-06-20 summary:Fuzzy systems concern fundamental methodology to represent and process uncertainty and imprecision in the linguistic information. The fuzzy systems that use fuzzy rules to represent the domain knowledge of the problem are known as Fuzzy Rule Base Systems (FRBS). On the other hand image segmentation and subsequent extraction from a noise-affected background, with the help of various soft computing methods, are relatively new and quite popular due to various reasons. These methods include various Artificial Neural Network (ANN) models (primarily supervised in nature), Genetic Algorithm (GA) based techniques, intensity histogram based methods etc. providing an extraction solution working in unsupervised mode happens to be even more interesting problem. Literature suggests that effort in this respect appears to be quite rudimentary. In the present article, we propose a fuzzy rule guided novel technique that is functional devoid of any external intervention during execution. Experimental results suggest that this approach is an efficient one in comparison to different other techniques extensively addressed in literature. In order to justify the supremacy of performance of our proposed technique in respect of its competitors, we take recourse to effective metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), Peak Signal to Noise Ratio (PSNR). version:1
arxiv-1206-4326 | Joint Reconstruction of Multi-view Compressed Images | http://arxiv.org/abs/1206.4326 | id:1206.4326 author:Vijayaraghavan Thirumalai, Pascal Frossard category:cs.MM cs.CV  published:2012-06-19 summary:The distributed representation of correlated multi-view images is an important problem that arise in vision sensor networks. This paper concentrates on the joint reconstruction problem where the distributively compressed correlated images are jointly decoded in order to improve the reconstruction quality of all the compressed images. We consider a scenario where the images captured at different viewpoints are encoded independently using common coding solutions (e.g., JPEG, H.264 intra) with a balanced rate distribution among different cameras. A central decoder first estimates the underlying correlation model from the independently compressed images which will be used for the joint signal recovery. The joint reconstruction is then cast as a constrained convex optimization problem that reconstructs total-variation (TV) smooth images that comply with the estimated correlation model. At the same time, we add constraints that force the reconstructed images to be consistent with their compressed versions. We show by experiments that the proposed joint reconstruction scheme outperforms independent reconstruction in terms of image quality, for a given target bit rate. In addition, the decoding performance of our proposed algorithm compares advantageously to state-of-the-art distributed coding schemes based on disparity learning and on the DISCOVER. version:1
arxiv-1111-0654 | Distributed Lossy Source Coding Using Real-Number Codes | http://arxiv.org/abs/1111.0654 | id:1111.0654 author:Mojtaba Vaezi, Fabrice Labeau category:cs.IT cs.CV cs.NI math.IT  published:2011-11-02 summary:We show how real-number codes can be used to compress correlated sources, and establish a new framework for lossy distributed source coding, in which we quantize compressed sources instead of compressing quantized sources. This change in the order of binning and quantization blocks makes it possible to model correlation between continuous-valued sources more realistically and correct quantization error when the sources are completely correlated. The encoding and decoding procedures are described in detail, for discrete Fourier transform (DFT) codes. Reconstructed signal, in the mean squared error sense, is seen to be better than that in the conventional approach. version:2
arxiv-1101-3755 | Transductive-Inductive Cluster Approximation Via Multivariate Chebyshev Inequality | http://arxiv.org/abs/1101.3755 | id:1101.3755 author:Shriprakash Sinha category:cs.CV cs.AI  published:2011-01-19 summary:Approximating adequate number of clusters in multidimensional data is an open area of research, given a level of compromise made on the quality of acceptable results. The manuscript addresses the issue by formulating a transductive inductive learning algorithm which uses multivariate Chebyshev inequality. Considering clustering problem in imaging, theoretical proofs for a particular level of compromise are derived to show the convergence of the reconstruction error to a finite value with increasing (a) number of unseen examples and (b) the number of clusters, respectively. Upper bounds for these error rates are also proved. Non-parametric estimates of these error from a random sample of sequences empirically point to a stable number of clusters. Lastly, the generalization of algorithm can be applied to multidimensional data sets from different fields. version:2
arxiv-1206-4169 | Clustered Bandits | http://arxiv.org/abs/1206.4169 | id:1206.4169 author:Loc Bui, Ramesh Johari, Shie Mannor category:cs.LG  published:2012-06-19 summary:We consider a multi-armed bandit setting that is inspired by real-world applications in e-commerce. In our setting, there are a few types of users, each with a specific response to the different arms. When a user enters the system, his type is unknown to the decision maker. The decision maker can either treat each user separately ignoring the previously observed users, or can attempt to take advantage of knowing that only few types exist and cluster the users according to their response to the arms. We devise algorithms that combine the usual exploration-exploitation tradeoff with clustering of users and demonstrate the value of clustering. In the process of developing algorithms for the clustered setting, we propose and analyze simple algorithms for the setup where a decision maker knows that a user belongs to one of few types, but does not know which one. version:1
arxiv-1206-4116 | Dependence Maximizing Temporal Alignment via Squared-Loss Mutual Information | http://arxiv.org/abs/1206.4116 | id:1206.4116 author:Makoto Yamada, Leonid Sigal, Michalis Raptis, Masashi Sugiyama category:stat.ML cs.AI  published:2012-06-19 summary:The goal of temporal alignment is to establish time correspondence between two sequences, which has many applications in a variety of areas such as speech processing, bioinformatics, computer vision, and computer graphics. In this paper, we propose a novel temporal alignment method called least-squares dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes statistical dependency between sequences, measured by a squared-loss variant of mutual information. The benefit of this novel information-theoretic formulation is that LSDTW can align sequences with different lengths, different dimensionality, high non-linearity, and non-Gaussianity in a computationally efficient manner. In addition, model parameters such as an initial alignment matrix can be systematically optimized by cross-validation. We demonstrate the usefulness of LSDTW through experiments on synthetic and real-world Kinect action recognition datasets. version:1
arxiv-1206-4110 | ConeRANK: Ranking as Learning Generalized Inequalities | http://arxiv.org/abs/1206.4110 | id:1206.4110 author:Truyen T. Tran, Duc Son Pham category:cs.LG cs.IR  published:2012-06-19 summary:We propose a new data mining approach in ranking documents based on the concept of cone-based generalized inequalities between vectors. A partial ordering between two vectors is made with respect to a proper cone and thus learning the preferences is formulated as learning proper cones. A pairwise learning-to-rank algorithm (ConeRank) is proposed to learn a non-negative subspace, formulated as a polyhedral cone, over document-pair differences. The algorithm is regularized by controlling the `volume' of the cone. The experimental studies on the latest and largest ranking dataset LETOR 4.0 shows that ConeRank is competitive against other recent ranking approaches. version:1
arxiv-1206-3975 | The Ultrasound Visualization Pipeline - A Survey | http://arxiv.org/abs/1206.3975 | id:1206.3975 author:Åsmund Birkeland, Veronika Solteszova, Dieter Hönigmann, Odd Helge Gilja, Svein Brekke, Timo Ropinski, Ivan Viola category:cs.GR cs.CV  published:2012-06-18 summary:Ultrasound is one of the most frequently used imaging modality in medicine. The high spatial resolution, its interactive nature and non-invasiveness makes it the first choice in many examinations. Image interpretation is one of ultrasound's main challenges. Much training is required to obtain a confident skill level in ultrasound-based diagnostics. State-of-the-art graphics techniques is needed to provide meaningful visualizations of ultrasound in real-time. In this paper we present the process-pipeline for ultrasound visualization, including an overview of the tasks performed in the specific steps. To provide an insight into the trends of ultrasound visualization research, we have selected a set of significant publications and divided them into a technique-based taxonomy covering the topics pre-processing, segmentation, registration, rendering and augmented reality. For the different technique types we discuss the difference between ultrasound-based techniques and techniques for other modalities. version:1
arxiv-1206-4686 | Discriminative Probabilistic Prototype Learning | http://arxiv.org/abs/1206.4686 | id:1206.4686 author:Edwin Bonilla, Antonio Robles-Kelly category:cs.LG stat.ML  published:2012-06-18 summary:In this paper we propose a simple yet powerful method for learning representations in supervised learning scenarios where each original input datapoint is described by a set of vectors and their associated outputs may be given by soft labels indicating, for example, class probabilities. We represent an input datapoint as a mixture of probabilities over the corresponding set of feature vectors where each probability indicates how likely each vector is to belong to an unknown prototype pattern. We propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization. More importantly, both the model parameters and the prototype patterns can be learned from data in a discriminative way. We show that our model can be seen as a probabilistic generalization of learning vector quantization (LVQ). We apply our method to the problems of shape classification, hyperspectral imaging classification and people's work class categorization, showing the superior performance of our method compared to the standard prototype-based classification approach and other competitive benchmark methods. version:1
arxiv-1206-4685 | Sparse-GEV: Sparse Latent Space Model for Multivariate Extreme Value Time Serie Modeling | http://arxiv.org/abs/1206.4685 | id:1206.4685 author:Yan Liu, Taha Bahadori, Hongfei Li category:stat.ME cs.LG stat.AP  published:2012-06-18 summary:In many applications of time series models, such as climate analysis and social media analysis, we are often interested in extreme events, such as heatwave, wind gust, and burst of topics. These time series data usually exhibit a heavy-tailed distribution rather than a Gaussian distribution. This poses great challenges to existing approaches due to the significantly different assumptions on the data distributions and the lack of sufficient past data on extreme events. In this paper, we propose the Sparse-GEV model, a latent state model based on the theory of extreme value modeling to automatically learn sparse temporal dependence and make predictions. Our model is theoretically significant because it is among the first models to learn sparse temporal dependencies among multivariate extreme value time series. We demonstrate the superior performance of our algorithm to the state-of-art methods, including Granger causality, copula approach, and transfer entropy, on one synthetic dataset, one climate dataset and two Twitter datasets. version:1
arxiv-1206-4683 | Marginalized Denoising Autoencoders for Domain Adaptation | http://arxiv.org/abs/1206.4683 | id:1206.4683 author:Minmin Chen, Zhixiang Xu, Kilian Weinberger, Fei Sha category:cs.LG  published:2012-06-18 summary:Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters ? in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks. version:1
arxiv-1206-4682 | Copula-based Kernel Dependency Measures | http://arxiv.org/abs/1206.4682 | id:1206.4682 author:Barnabas Poczos, Zoubin Ghahramani, Jeff Schneider category:cs.LG math.ST stat.ML stat.TH  published:2012-06-18 summary:The paper presents a new copula based method for measuring dependence between random variables. Our approach extends the Maximum Mean Discrepancy to the copula of the joint distribution. We prove that this approach has several advantageous properties. Similarly to Shannon mutual information, the proposed dependence measure is invariant to any strictly increasing transformation of the marginal variables. This is important in many applications, for example in feature selection. The estimator is consistent, robust to outliers, and uses rank statistics only. We derive upper bounds on the convergence rate and propose independence tests too. We illustrate the theoretical contributions through a series of experiments in feature selection and low-dimensional embedding of distributions. version:1
arxiv-1206-4681 | LPQP for MAP: Putting LP Solvers to Better Use | http://arxiv.org/abs/1206.4681 | id:1206.4681 author:Patrick Pletscher, Sharon Wulff category:cs.LG stat.ML  published:2012-06-18 summary:MAP inference for general energy functions remains a challenging problem. While most efforts are channeled towards improving the linear programming (LP) based relaxation, this work is motivated by the quadratic programming (QP) relaxation. We propose a novel MAP relaxation that penalizes the Kullback-Leibler divergence between the LP pairwise auxiliary variables, and QP equivalent terms given by the product of the unaries. We develop two efficient algorithms based on variants of this relaxation. The algorithms minimize the non-convex objective using belief propagation and dual decomposition as building blocks. Experiments on synthetic and real-world data show that the solutions returned by our algorithms substantially improve over the LP relaxation. version:1
arxiv-1206-4680 | Fast Prediction of New Feature Utility | http://arxiv.org/abs/1206.4680 | id:1206.4680 author:Hoyt Koepke, Mikhail Bilenko category:cs.LG math.ST stat.TH  published:2012-06-18 summary:We study the new feature utility prediction problem: statistically testing whether adding a new feature to the data representation can improve predictive accuracy on a supervised learning task. In many applications, identifying new informative features is the primary pathway for improving performance. However, evaluating every potential feature by re-training the predictor with it can be costly. The paper describes an efficient, learner-independent technique for estimating new feature utility without re-training based on the current predictor's outputs. The method is obtained by deriving a connection between loss reduction potential and the new feature's correlation with the loss gradient of the current predictor. This leads to a simple yet powerful hypothesis testing procedure, for which we prove consistency. Our theoretical analysis is accompanied by empirical evaluation on standard benchmarks and a large-scale industrial dataset. version:1
arxiv-1206-4679 | Factorized Asymptotic Bayesian Hidden Markov Models | http://arxiv.org/abs/1206.4679 | id:1206.4679 author:Ryohei Fujimaki, Kohei Hayashi category:cs.LG stat.ML  published:2012-06-18 summary:This paper addresses the issue of model selection for hidden Markov models (HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has been recently developed for model selection on independent hidden variables (i.e., mixture models), for time-dependent hidden variables. As with FAB in mixture models, FAB for HMMs is derived as an iterative lower bound maximization algorithm of a factorized information criterion (FIC). It inherits, from FAB for mixture models, several desirable properties for learning HMMs, such as asymptotic consistency of FIC with marginal log-likelihood, a shrinkage effect for hidden state selection, monotonic increase of the lower FIC bound through the iterative optimization. Further, it does not have a tunable hyper-parameter, and thus its model selection process can be fully automated. Experimental results shows that FAB outperforms states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in terms of model selection accuracy and computational efficiency. version:1
arxiv-1206-4678 | Linear Regression with Limited Observation | http://arxiv.org/abs/1206.4678 | id:1206.4678 author:Elad Hazan, Tomer Koren category:cs.LG stat.ML  published:2012-06-18 summary:We consider the most common variants of linear regression, including Ridge, Lasso and Support-vector regression, in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time. We present simple and efficient algorithms for these problems: for Lasso and Ridge regression they need the same total number of attributes (up to constants) as do full-information algorithms, for reaching a certain accuracy. For Support-vector regression, we require exponentially less attributes compared to the state of the art. By that, we resolve an open problem recently posed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art. version:1
arxiv-1206-4677 | Semi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching | http://arxiv.org/abs/1206.4677 | id:1206.4677 author:Marthinus Du Plessis, Masashi Sugiyama category:cs.LG stat.ML  published:2012-06-18 summary:In real-world classification problems, the class balance in the training dataset does not necessarily reflect that of the test dataset, which can cause significant estimation bias. If the class ratio of the test dataset is known, instance re-weighting or resampling allows systematical bias correction. However, learning the class ratio of the test dataset is challenging when no labeled data is available from the test domain. In this paper, we propose to estimate the class ratio in the test dataset by matching probability distributions of training and test input data. We demonstrate the utility of the proposed approach through experiments. version:1
arxiv-1206-4676 | Clustering by Low-Rank Doubly Stochastic Matrix Decomposition | http://arxiv.org/abs/1206.4676 | id:1206.4676 author:Zhirong Yang, Erkki Oja category:cs.LG cs.CV cs.NA stat.ML  published:2012-06-18 summary:Clustering analysis by nonnegative low-rank approximations has achieved remarkable progress in the past decade. However, most approximation approaches in this direction are still restricted to matrix factorization. We propose a new low-rank learning method to improve the clustering performance, which is beyond matrix factorization. The approximation is based on a two-step bipartite random walk through virtual cluster nodes, where the approximation is formed by only cluster assigning probabilities. Minimizing the approximation error measured by Kullback-Leibler divergence is equivalent to maximizing the likelihood of a discriminative model, which endows our method with a solid probabilistic interpretation. The optimization is implemented by a relaxed Majorization-Minimization algorithm that is advantageous in finding good local minima. Furthermore, we point out that the regularized algorithm with Dirichlet prior only serves as initialization. Experimental results show that the new method has strong performance in clustering purity for various datasets, especially for large-scale manifold data. version:1
arxiv-1206-4675 | Finding Botnets Using Minimal Graph Clusterings | http://arxiv.org/abs/1206.4675 | id:1206.4675 author:Peter Haider, Tobias Scheffer category:cs.CR cs.DC cs.LG  published:2012-06-18 summary:We study the problem of identifying botnets and the IP addresses which they comprise, based on the observation of a fraction of the global email spam traffic. Observed mailing campaigns constitute evidence for joint botnet membership, they are represented by cliques in the graph of all messages. No evidence against an association of nodes is ever available. We reduce the problem of identifying botnets to a problem of finding a minimal clustering of the graph of messages. We directly model the distribution of clusterings given the input graph; this avoids potential errors caused by distributional assumptions of a generative model. We report on a case study in which we evaluate the model by its ability to predict the spam campaign that a given IP address is going to participate in. version:1
arxiv-1206-4674 | Comparison-Based Learning with Rank Nets | http://arxiv.org/abs/1206.4674 | id:1206.4674 author:Amin Karbasi, Stratis Ioannidis, laurent Massoulie category:cs.LG cs.DS stat.ML  published:2012-06-18 summary:We consider the problem of search through comparisons, where a user is presented with two candidate objects and reveals which is closer to her intended target. We study adaptive strategies for finding the target, that require knowledge of rank relationships but not actual distances between objects. We propose a new strategy based on rank nets, and show that for target distributions with a bounded doubling constant, it finds the target in a number of comparisons close to the entropy of the target distribution and, hence, of the optimum. We extend these results to the case of noisy oracles, and compare this strategy to prior art over multiple datasets. version:1
arxiv-1206-4673 | Group Sparse Additive Models | http://arxiv.org/abs/1206.4673 | id:1206.4673 author:Junming Yin, Xi Chen, Eric Xing category:cs.LG stat.ML  published:2012-06-18 summary:We consider the problem of sparse variable selection in nonparametric additive models, with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly. Previous works either study the group sparsity in the parametric setting (e.g., group lasso), or address the problem in the non-parametric setting without exploiting the structural information (e.g., sparse additive models). In this paper, we present a new method, called group sparse additive models (GroupSpAM), which can handle group sparsity in additive models. We generalize the l1/l2 norm to Hilbert spaces as the sparsity-inducing penalty in GroupSpAM. Moreover, we derive a novel thresholding condition for identifying the functional sparsity at the group level, and propose an efficient block coordinate descent algorithm for constructing the estimate. We demonstrate by simulation that GroupSpAM substantially outperforms the competing methods in terms of support recovery and prediction accuracy in additive models, and also conduct a comparative experiment on a real breast cancer dataset. version:1
arxiv-1206-4672 | Efficient Active Algorithms for Hierarchical Clustering | http://arxiv.org/abs/1206.4672 | id:1206.4672 author:Akshay Krishnamurthy, Sivaraman Balakrishnan, Min Xu, Aarti Singh category:cs.LG stat.ML  published:2012-06-18 summary:Advances in sensing technologies and the growth of the internet have resulted in an explosion in the size of modern datasets, while storage and processing power continue to lag behind. This motivates the need for algorithms that are efficient, both in terms of the number of measurements needed and running time. To combat the challenges associated with large datasets, we propose a general framework for active hierarchical clustering that repeatedly runs an off-the-shelf clustering algorithm on small subsets of the data and comes with guarantees on performance, measurement complexity and runtime complexity. We instantiate this framework with a simple spectral clustering algorithm and provide concrete results on its performance, showing that, under some assumptions, this algorithm recovers all clusters of size ?(log n) using O(n log^2 n) similarities and runs in O(n log^3 n) time for a dataset of n objects. Through extensive experimentation we also demonstrate that this framework is practically alluring. version:1
arxiv-1206-4671 | Dependent Hierarchical Normalized Random Measures for Dynamic Topic Modeling | http://arxiv.org/abs/1206.4671 | id:1206.4671 author:Changyou Chen, Nan Ding, Wray Buntine category:cs.LG stat.ML  published:2012-06-18 summary:We develop dependent hierarchical normalized random measures and apply them to dynamic topic modeling. The dependency arises via superposition, subsampling and point transition on the underlying Poisson processes of these measures. The measures used include normalised generalised Gamma processes that demonstrate power law properties, unlike Dirichlet processes used previously in dynamic topic modeling. Inference for the model includes adapting a recently developed slice sampler to directly manipulate the underlying Poisson process. Experiments performed on news, blogs, academic and Twitter collections demonstrate the technique gives superior perplexity over a number of previous models. version:1
arxiv-1206-4670 | State-Space Inference for Non-Linear Latent Force Models with Application to Satellite Orbit Prediction | http://arxiv.org/abs/1206.4670 | id:1206.4670 author:Jouni Hartikainen, Mari Seppanen, Simo Sarkka category:cs.IT astro-ph.EP cs.LG math.IT physics.data-an  published:2012-06-18 summary:Latent force models (LFMs) are flexible models that combine mechanistic modelling principles (i.e., physical models) with non-parametric data-driven components. Several key applications of LFMs need non-linearities, which results in analytically intractable inference. In this work we show how non-linear LFMs can be represented as non-linear white noise driven state-space models and present an efficient non-linear Kalman filtering and smoothing based method for approximate state and parameter inference. We illustrate the performance of the proposed methodology via two simulated examples, and apply it to a real-world problem of long-term prediction of GPS satellite orbits. version:1
arxiv-1206-4669 | Sparse Additive Functional and Kernel CCA | http://arxiv.org/abs/1206.4669 | id:1206.4669 author:Sivaraman Balakrishnan, Kriti Puniyani, John Lafferty category:cs.LG stat.ML  published:2012-06-18 summary:Canonical Correlation Analysis (CCA) is a classical tool for finding correlations among the components of two random vectors. In recent years, CCA has been widely applied to the analysis of genomic data, where it is common for researchers to perform multiple assays on a single set of patient samples. Recent work has proposed sparse variants of CCA to address the high dimensionality of such data. However, classical and sparse CCA are based on linear models, and are thus limited in their ability to find general correlations. In this paper, we present two approaches to high-dimensional nonparametric CCA, building on recent developments in high-dimensional nonparametric regression. We present estimation procedures for both approaches, and analyze their theoretical properties in the high-dimensional setting. We demonstrate the effectiveness of these procedures in discovering nonlinear correlations via extensive simulations, as well as through experiments with genomic data. version:1
arxiv-1206-4668 | Approximate Principal Direction Trees | http://arxiv.org/abs/1206.4668 | id:1206.4668 author:Mark McCartin-Lim, Andrew McGregor, Rui Wang category:cs.LG cs.DS stat.ML  published:2012-06-18 summary:We introduce a new spatial data structure for high dimensional data called the \emph{approximate principal direction tree} (APD tree) that adapts to the intrinsic dimension of the data. Our algorithm ensures vector-quantization accuracy similar to that of computationally-expensive PCA trees with similar time-complexity to that of lower-accuracy RP trees. APD trees use a small number of power-method iterations to find splitting planes for recursively partitioning the data. As such they provide a natural trade-off between the running-time and accuracy achieved by RP and PCA trees. Our theoretical results establish a) strong performance guarantees regardless of the convergence rate of the power-method and b) that $O(\log d)$ iterations suffice to establish the guarantee of PCA trees when the intrinsic dimension is $d$. We demonstrate this trade-off and the efficacy of our data structure on both the CPU and GPU. version:1
arxiv-1206-4666 | A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices | http://arxiv.org/abs/1206.4666 | id:1206.4666 author:Mingjun Zhong, Mark Girolami category:stat.CO cs.LG stat.ME  published:2012-06-18 summary:We present a Bayesian scheme for the approximate diagonalisation of several square matrices which are not necessarily symmetric. A Gibbs sampler is derived to simulate samples of the common eigenvectors and the eigenvalues for these matrices. Several synthetic examples are used to illustrate the performance of the proposed Gibbs sampler and we then provide comparisons to several other joint diagonalization algorithms, which shows that the Gibbs sampler achieves the state-of-the-art performance on the examples considered. As a byproduct, the output of the Gibbs sampler could be used to estimate the log marginal likelihood, however we employ the approximation based on the Bayesian information criterion (BIC) which in the synthetic examples considered correctly located the number of common eigenvectors. We then succesfully applied the sampler to the source separation problem as well as the common principal component analysis and the common spatial pattern analysis problems. version:1
arxiv-1206-4665 | Nonparametric variational inference | http://arxiv.org/abs/1206.4665 | id:1206.4665 author:Samuel Gershman, Matt Hoffman, David Blei category:cs.LG stat.ML  published:2012-06-18 summary:Variational methods are widely used for approximate posterior inference. However, their use is typically limited to families of distributions that enjoy particular conjugacy properties. To circumvent this limitation, we propose a family of variational approximations inspired by nonparametric kernel density estimation. The locations of these kernels and their bandwidth are treated as variational parameters and optimized to improve an approximate lower bound on the marginal likelihood of the data. Using multiple kernels allows the approximation to capture multiple modes of the posterior, unlike most other variational approximations. We demonstrate the efficacy of the nonparametric approximation with a hierarchical logistic regression model and a nonlinear matrix factorization model. We obtain predictive performance as good as or better than more specialized variational methods and sample-based approximations. The method is easy to apply to more general graphical models for which standard variational methods are difficult to derive. version:1
arxiv-1206-4664 | Tighter Variational Representations of f-Divergences via Restriction to Probability Measures | http://arxiv.org/abs/1206.4664 | id:1206.4664 author:Avraham Ruderman, Mark Reid, Dario Garcia-Garcia, James Petterson category:cs.LG stat.ML  published:2012-06-18 summary:We show that the variational representations for f-divergences currently used in the literature can be tightened. This has implications to a number of methods recently proposed based on this representation. As an example application we use our tighter representation to derive a general f-divergence estimator based on two i.i.d. samples and derive the dual program for this estimator that performs well empirically. We also point out a connection between our estimator and MMD. version:1
arxiv-1206-4663 | The Convexity and Design of Composite Multiclass Losses | http://arxiv.org/abs/1206.4663 | id:1206.4663 author:Mark Reid, Robert Williamson, Peng Sun category:cs.LG stat.ML  published:2012-06-18 summary:We consider composite loss functions for multiclass prediction comprising a proper (i.e., Fisher-consistent) loss over probability distributions and an inverse link function. We establish conditions for their (strong) convexity and explore the implications. We also show how the separation of concerns afforded by using this composite representation allows for the design of families of losses with the same Bayes risk. version:1
arxiv-1206-4662 | Bayesian Watermark Attacks | http://arxiv.org/abs/1206.4662 | id:1206.4662 author:Ivo Shterev, David Dunson category:cs.CR cs.LG cs.MM  published:2012-06-18 summary:This paper presents an application of statistical machine learning to the field of watermarking. We propose a new attack model on additive spread-spectrum watermarking systems. The proposed attack is based on Bayesian statistics. We consider the scenario in which a watermark signal is repeatedly embedded in specific, possibly chosen based on a secret message bitstream, segments (signals) of the host data. The host signal can represent a patch of pixels from an image or a video frame. We propose a probabilistic model that infers the embedded message bitstream and watermark signal, directly from the watermarked data, without access to the decoder. We develop an efficient Markov chain Monte Carlo sampler for updating the model parameters from their conjugate full conditional posteriors. We also provide a variational Bayesian solution, which further increases the convergence speed of the algorithm. Experiments with synthetic and real image signals demonstrate that the attack model is able to correctly infer a large part of the message bitstream and obtain a very accurate estimate of the watermark signal. version:1
arxiv-1206-4661 | Predicting accurate probabilities with a ranking loss | http://arxiv.org/abs/1206.4661 | id:1206.4661 author:Aditya Menon, Xiaoqian Jiang, Shankar Vembu, Charles Elkan, Lucila Ohno-Machado category:cs.LG stat.ML  published:2012-06-18 summary:In many real-world applications of machine learning classifiers, it is essential to predict the probability of an example belonging to a particular class. This paper proposes a simple technique for predicting probabilities based on optimizing a ranking loss, followed by isotonic regression. This semi-parametric technique offers both good ranking and regression performance, and models a richer set of probability distributions than statistical workhorses such as logistic regression. We provide experimental results that show the effectiveness of this technique on real-world applications of probability prediction. version:1
arxiv-1206-4660 | Learning with Augmented Features for Heterogeneous Domain Adaptation | http://arxiv.org/abs/1206.4660 | id:1206.4660 author:Lixin Duan, Dong Xu, Ivor Tsang category:cs.LG  published:2012-06-18 summary:We propose a new learning method for heterogeneous domain adaptation (HDA), in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains. We then propose two new feature mapping functions to augment the transformed data with their original features and zeros. The existing learning methods (e.g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA. Using the hinge loss function in SVM as an example, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions. Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method. Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods. version:1
arxiv-1206-4659 | Max-Margin Nonparametric Latent Feature Models for Link Prediction | http://arxiv.org/abs/1206.4659 | id:1206.4659 author:Jun Zhu category:cs.LG stat.ML  published:2012-06-18 summary:We present a max-margin nonparametric latent feature model, which unites the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction and automatically infer the unknown latent social dimension. By minimizing a hinge-loss using the linear expectation operator, we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function; by using a fully-Bayesian formulation, we can avoid tuning regularization constants. Experimental results on real datasets appear to demonstrate the benefits inherited from max-margin learning and fully-Bayesian nonparametric inference. version:1
arxiv-1206-4658 | Dirichlet Process with Mixed Random Measures: A Nonparametric Topic Model for Labeled Data | http://arxiv.org/abs/1206.4658 | id:1206.4658 author:Dongwoo Kim, Suin Kim, Alice Oh category:cs.LG stat.ML  published:2012-06-18 summary:We describe a nonparametric topic model for labeled data. The model uses a mixture of random measures (MRM) as a base distribution of the Dirichlet process (DP) of the HDP framework, so we call it the DP-MRM. To model labeled data, we define a DP distributed random measure for each label, and the resulting model generates an unbounded number of topics for each label. We apply DP-MRM on single-labeled and multi-labeled corpora of documents and compare the performance on label prediction with MedLDA, LDA-SVM, and Labeled-LDA. We further enhance the model by incorporating ddCRP and modeling multi-labeled images for image segmentation and object labeling, comparing the performance with nCuts and rddCRP. version:1
arxiv-1206-4657 | Projection-free Online Learning | http://arxiv.org/abs/1206.4657 | id:1206.4657 author:Elad Hazan, Satyen Kale category:cs.LG cs.DS  published:2012-06-18 summary:The computational bottleneck in applying online learning to massive data sets is usually the projection step. We present efficient online learning algorithms that eschew projections in favor of much more efficient linear optimization steps using the Frank-Wolfe technique. We obtain a range of regret bounds for online convex optimization, with better bounds for specific cases such as stochastic online smooth convex optimization. Besides the computational advantage, other desirable features of our algorithms are that they are parameter-free in the stochastic case and produce sparse decisions. We apply our algorithms to computationally intensive applications of collaborative filtering, and show the theoretical improvements to be clearly visible on standard datasets. version:1
arxiv-1206-4656 | Machine Learning that Matters | http://arxiv.org/abs/1206.4656 | id:1206.4656 author:Kiri Wagstaff category:cs.LG cs.AI stat.ML  published:2012-06-18 summary:Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters. version:1
arxiv-1206-4655 | Modelling transition dynamics in MDPs with RKHS embeddings | http://arxiv.org/abs/1206.4655 | id:1206.4655 author:Steffen Grunewalder, Guy Lever, Luca Baldassarre, Massi Pontil, Arthur Gretton category:cs.LG  published:2012-06-18 summary:We propose a new, nonparametric approach to learning and representing transition dynamics in Markov decision processes (MDPs), which can be combined easily with dynamic programming methods for policy optimisation and value estimation. This approach makes use of a recently developed representation of conditional distributions as \emph{embeddings} in a reproducing kernel Hilbert space (RKHS). Such representations bypass the need for estimating transition probabilities or densities, and apply to any domain on which kernels can be defined. This avoids the need to calculate intractable integrals, since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding. We provide guarantees for the proposed applications in MDPs: in the context of a value iteration algorithm, we prove convergence to either the optimal policy, or to the closest projection of the optimal policy in our model class (an RKHS), under reasonable assumptions. In experiments, we investigate a learning task in a typical classical control setting (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. For policy optimisation we compare with least-squares policy iteration where a Gaussian process is used for value function estimation. For value estimation we also compare to the NPDP method. Our approach achieves better performance in all experiments. version:1
arxiv-1206-4654 | A Generalized Loop Correction Method for Approximate Inference in Graphical Models | http://arxiv.org/abs/1206.4654 | id:1206.4654 author:Siamak Ravanbakhsh, Chun-Nam Yu, Russell Greiner category:cs.AI cs.LG stat.ML  published:2012-06-18 summary:Belief Propagation (BP) is one of the most popular methods for inference in probabilistic graphical models. BP is guaranteed to return the correct answer for tree structures, but can be incorrect or non-convergent for loopy graphical models. Recently, several new approximate inference algorithms based on cavity distribution have been proposed. These methods can account for the effect of loops by incorporating the dependency between BP messages. Alternatively, region-based approximations (that lead to methods such as Generalized Belief Propagation) improve upon BP by considering interactions within small clusters of variables, thus taking small loops within these clusters into account. This paper introduces an approach, Generalized Loop Correction (GLC), that benefits from both of these types of loop correction. We show how GLC relates to these two families of inference methods, then provide empirical evidence that GLC works effectively in general, and can be significantly more accurate than both correction schemes. version:1
arxiv-1206-4653 | Dimensionality Reduction by Local Discriminative Gaussians | http://arxiv.org/abs/1206.4653 | id:1206.4653 author:Nathan Parrish, Maya Gupta category:cs.LG cs.CV stat.ML  published:2012-06-18 summary:We present local discriminative Gaussian (LDG) dimensionality reduction, a supervised dimensionality reduction technique for classification. The LDG objective function is an approximation to the leave-one-out training error of a local quadratic discriminant analysis classifier, and thus acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data. While other state-of-the-art linear dimensionality reduction methods require gradient descent or iterative solution approaches, LDG is solved with a single eigen-decomposition. Thus, it scales better for datasets with a large number of feature dimensions or training examples. We also adapt LDG to the transfer learning setting, and show that it achieves good performance when the test data distribution differs from that of the training data. version:1
arxiv-1206-4652 | The Most Persistent Soft-Clique in a Set of Sampled Graphs | http://arxiv.org/abs/1206.4652 | id:1206.4652 author:Novi Quadrianto, Chao Chen, Christoph Lampert category:cs.LG cs.AI  published:2012-06-18 summary:When searching for characteristic subpatterns in potentially noisy graph data, it appears self-evident that having multiple observations would be better than having just one. However, it turns out that the inconsistencies introduced when different graph instances have different edge sets pose a serious challenge. In this work we address this challenge for the problem of finding maximum weighted cliques. We introduce the concept of most persistent soft-clique. This is subset of vertices, that 1) is almost fully or at least densely connected, 2) occurs in all or almost all graph instances, and 3) has the maximum weight. We present a measure of clique-ness, that essentially counts the number of edge missing to make a subset of vertices into a clique. With this measure, we show that the problem of finding the most persistent soft-clique problem can be cast either as: a) a max-min two person game optimization problem, or b) a min-min soft margin optimization problem. Both formulations lead to the same solution when using a partial Lagrangian method to solve the optimization problems. By experiments on synthetic data and on real social network data, we show that the proposed method is able to reliably find soft cliques in graph data, even if that is distorted by random noise or unreliable observations. version:1
arxiv-1206-4651 | Is margin preserved after random projection? | http://arxiv.org/abs/1206.4651 | id:1206.4651 author:Qinfeng Shi, Chunhua Shen, Rhys Hill, Anton van den Hengel category:cs.LG cs.CV stat.ML  published:2012-06-18 summary:Random projections have been applied in many machine learning algorithms. However, whether margin is preserved after random projection is non-trivial and not well studied. In this paper we analyse margin distortion after random projection, and give the conditions of margin preservation for binary classification problems. We also extend our analysis to margin for multiclass problems, and provide theoretical bounds on multiclass margin on the projected data. version:1
arxiv-1206-4650 | Analysis of Kernel Mean Matching under Covariate Shift | http://arxiv.org/abs/1206.4650 | id:1206.4650 author:Yaoliang Yu, Csaba Szepesvari category:cs.LG stat.ML  published:2012-06-18 summary:In real supervised learning scenarios, it is not uncommon that the training and test sample follow different probability distributions, thus rendering the necessity to correct the sampling bias. Focusing on a particular covariate shift problem, we derive high probability confidence bounds for the kernel mean matching (KMM) estimator, whose convergence rate turns out to depend on some regularity measure of the regression function and also on some capacity measure of the kernel. By comparing KMM with the natural plug-in estimator, we establish the superiority of the former hence provide concrete evidence/understanding to the effectiveness of KMM under covariate shift. version:1
arxiv-1206-4649 | Learning Efficient Structured Sparse Models | http://arxiv.org/abs/1206.4649 | id:1206.4649 author:Alex Bronstein, Pablo Sprechmann, Guillermo Sapiro category:cs.LG cs.CV stat.ML  published:2012-06-18 summary:We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. For this purpose, we develop a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, and show an efficient feed forward architecture derived from its iteration. This architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. We also show that by using different training objective functions, learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre-given dictionary, as in earlier formulations, but can be rather used as full-featured sparse encoders or even modelers. A simple implementation shows several orders of magnitude speedup compared to the state-of-the-art at minimal performance degradation, making the proposed framework suitable for real time and large-scale applications. version:1
arxiv-1206-4648 | Two-Manifold Problems with Applications to Nonlinear System Identification | http://arxiv.org/abs/1206.4648 | id:1206.4648 author:Byron Boots, Geoff Gordon category:cs.LG  published:2012-06-18 summary:Recently, there has been much interest in spectral approaches to learning manifolds---so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space, and discuss when two-manifold problems are useful. Finally, we demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data. version:1
arxiv-1206-4647 | Active Learning for Matching Problems | http://arxiv.org/abs/1206.4647 | id:1206.4647 author:Laurent Charlin, Rich Zemel, Craig Boutilier category:cs.LG cs.AI cs.IR  published:2012-06-18 summary:Effective learning of user preferences is critical to easing user burden in various types of matching problems. Equally important is active query selection to further reduce the amount of preference information users must provide. We address the problem of active learning of user preferences for matching problems, introducing a novel method for determining probabilistic matchings, and developing several new active learning strategies that are sensitive to the specific matching objective. Experiments with real-world data sets spanning diverse domains demonstrate that matching-sensitive active learning version:1
arxiv-1206-4646 | Partial-Hessian Strategies for Fast Learning of Nonlinear Embeddings | http://arxiv.org/abs/1206.4646 | id:1206.4646 author:Max Vladymyrov, Miguel Carreira-Perpinan category:cs.LG stat.ML  published:2012-06-18 summary:Stochastic neighbor embedding (SNE) and related nonlinear manifold learning algorithms achieve high-quality low-dimensional representations of similarity data, but are notoriously slow to train. We propose a generic formulation of embedding algorithms that includes SNE and other existing algorithms, and study their relation with spectral methods and graph Laplacians. This allows us to define several partial-Hessian optimization strategies, characterize their global and local convergence, and evaluate them empirically. We achieve up to two orders of magnitude speedup over existing training methods with a strategy (which we call the spectral direction) that adds nearly no overhead to the gradient and yet is simple, scalable and applicable to several existing and future embedding algorithms. version:1
arxiv-1206-4645 | Ensemble Methods for Convex Regression with Applications to Geometric Programming Based Circuit Design | http://arxiv.org/abs/1206.4645 | id:1206.4645 author:Lauren Hannah, David Dunson category:cs.LG cs.NA stat.ME stat.ML  published:2012-06-18 summary:Convex regression is a promising area for bridging statistical estimation and deterministic convex optimization. New piecewise linear convex regression methods are fast and scalable, but can have instability when used to approximate constraints or objective functions for optimization. Ensemble methods, like bagging, smearing and random partitioning, can alleviate this problem and maintain the theoretical properties of the underlying estimator. We empirically examine the performance of ensemble methods for prediction and optimization, and then apply them to device modeling and constraint approximation for geometric programming based circuit design. version:1
arxiv-1206-4644 | Groupwise Constrained Reconstruction for Subspace Clustering | http://arxiv.org/abs/1206.4644 | id:1206.4644 author:Ruijiang Li, Bin Li, Ke Zhang, Cheng Jin, Xiangyang Xue category:cs.LG stat.ML  published:2012-06-18 summary:Reconstruction based subspace clustering methods compute a self reconstruction matrix over the samples and use it for spectral clustering to obtain the final clustering result. Their success largely relies on the assumption that the underlying subspaces are independent, which, however, does not always hold in the applications with increasing number of subspaces. In this paper, we propose a novel reconstruction based subspace clustering model without making the subspace independence assumption. In our model, certain properties of the reconstruction matrix are explicitly characterized using the latent cluster indicators, and the affinity matrix used for spectral clustering can be directly built from the posterior of the latent cluster indicators instead of the reconstruction matrix. Experimental results on both synthetic and real-world datasets show that the proposed model can outperform the state-of-the-art methods. version:1
arxiv-1206-4643 | Lightning Does Not Strike Twice: Robust MDPs with Coupled Uncertainty | http://arxiv.org/abs/1206.4643 | id:1206.4643 author:Shie Mannor, Ofir Mebel, Huan Xu category:cs.LG cs.GT cs.SY  published:2012-06-18 summary:We consider Markov decision processes under parameter uncertainty. Previous studies all restrict to the case that uncertainties among different states are uncoupled, which leads to conservative solutions. In contrast, we introduce an intuitive concept, termed "Lightning Does not Strike Twice," to model coupled uncertain parameters. Specifically, we require that the system can deviate from its nominal parameters only a bounded number of times. We give probabilistic guarantees indicating that this model represents real life situations and devise tractable algorithms for computing optimal control policies using this concept. version:1
arxiv-1206-4642 | Fast Computation of Subpath Kernel for Trees | http://arxiv.org/abs/1206.4642 | id:1206.4642 author:Daisuke Kimura, Hisashi Kashima category:cs.DS cs.LG stat.ML  published:2012-06-18 summary:The kernel method is a potential approach to analyzing structured data such as sequences, trees, and graphs; however, unordered trees have not been investigated extensively. Kimura et al. (2011) proposed a kernel function for unordered trees on the basis of their subpaths, which are vertical substructures of trees responsible for hierarchical information in them. Their kernel exhibits practically good performance in terms of accuracy and speed; however, linear-time computation is not guaranteed theoretically, unlike the case of the other unordered tree kernel proposed by Vishwanathan and Smola (2003). In this paper, we propose a theoretically guaranteed linear-time kernel computation algorithm that is practically fast, and we present an efficient prediction algorithm whose running time depends only on the size of the input tree. Experimental results show that the proposed algorithms are quite efficient in practice. version:1
arxiv-1206-4641 | Total Variation and Euler's Elastica for Supervised Learning | http://arxiv.org/abs/1206.4641 | id:1206.4641 author:Tong Lin, Hanlin Xue, Ling Wang, Hongbin Zha category:cs.LG cs.CV stat.ML  published:2012-06-18 summary:In recent years, total variation (TV) and Euler's elastica (EE) have been successfully applied to image processing tasks such as denoising and inpainting. This paper investigates how to extend TV and EE to the supervised learning settings on high dimensional data. The supervised learning problem can be formulated as an energy functional minimization under Tikhonov regularization scheme, where the energy is composed of a squared loss and a total variation smoothing (or Euler's elastica smoothing). Its solution via variational principles leads to an Euler-Lagrange PDE. However, the PDE is always high-dimensional and cannot be directly solved by common methods. Instead, radial basis functions are utilized to approximate the target function, reducing the problem to finding the linear coefficients of basis functions. We apply the proposed methods to supervised learning tasks (including binary classification, multi-class classification, and regression) on benchmark data sets. Extensive experiments have demonstrated promising results of the proposed methods. version:1
arxiv-1206-4640 | Stability of matrix factorization for collaborative filtering | http://arxiv.org/abs/1206.4640 | id:1206.4640 author:Yu-Xiang Wang, Huan Xu category:cs.NA cs.LG stat.ML  published:2012-06-18 summary:We study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion. In particular, our results include: (I) we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error; (II) we treat the matrix factorization as a subspace fitting problem and analyze the difference between the solution subspace and the ground truth; (III) we analyze the prediction error of individual users based on the subspace stability. We apply these results to the problem of collaborative filtering under manipulator attack, which leads to useful insights and guidelines for collaborative filtering system design. version:1
arxiv-1206-4639 | Adaptive Regularization for Weight Matrices | http://arxiv.org/abs/1206.4639 | id:1206.4639 author:Koby Crammer, Gal Chechik category:cs.LG cs.AI  published:2012-06-18 summary:Algorithms for learning distributions over weight-vectors, such as AROW were recently shown empirically to achieve state-of-the-art performance at various problems, with strong theoretical guaranties. Extending these algorithms to matrix models pose challenges since the number of free parameters in the covariance of the distribution scales as $n^4$ with the dimension $n$ of the matrix, and $n$ tends to be large in real applications. We describe, analyze and experiment with two new algorithms for learning distribution of matrix models. Our first algorithm maintains a diagonal covariance over the parameters and can handle large covariance matrices. The second algorithm factors the covariance to capture inter-features correlation while keeping the number of parameters linear in the size of the original matrix. We analyze both algorithms in the mistake bound model and show a superior precision performance of our approach over other algorithms in two tasks: retrieving similar images, and ranking similar documents. The factored algorithm is shown to attain faster convergence rate. version:1
arxiv-1206-4638 | Efficient Euclidean Projections onto the Intersection of Norm Balls | http://arxiv.org/abs/1206.4638 | id:1206.4638 author:Adams Wei Yu, Hao Su, Li Fei-Fei category:cs.LG stat.ML  published:2012-06-18 summary:Using sparse-inducing norms to learn robust models has received increasing attention from many fields for its attractive properties. Projection-based methods have been widely applied to learning tasks constrained by such norms. As a key building block of these methods, an efficient operator for Euclidean projection onto the intersection of $\ell_1$ and $\ell_{1,q}$ norm balls $(q=2\text{or}\infty)$ is proposed in this paper. We prove that the projection can be reduced to finding the root of an auxiliary function which is piecewise smooth and monotonic. Hence, a bisection algorithm is sufficient to solve the problem. We show that the time complexity of our solution is $O(n+g\log g)$ for $q=2$ and $O(n\log n)$ for $q=\infty$, where $n$ is the dimensionality of the vector to be projected and $g$ is the number of disjoint groups; we confirm this complexity by experimentation. Empirical study reveals that our method achieves significantly better performance than classical methods in terms of running time and memory usage. We further show that embedded with our efficient projection operator, projection-based algorithms can solve regression problems with composite norm constraints more efficiently than other methods and give superior accuracy. version:1
arxiv-1206-4637 | Learning to Identify Regular Expressions that Describe Email Campaigns | http://arxiv.org/abs/1206.4637 | id:1206.4637 author:Paul Prasse, Christoph Sawade, Niels Landwehr, Tobias Scheffer category:cs.LG cs.CL stat.ML  published:2012-06-18 summary:This paper addresses the problem of inferring a regular expression from a given set of strings that resembles, as closely as possible, the regular expression that a human expert would have written to identify the language. This is motivated by our goal of automating the task of postmasters of an email service who use regular expressions to describe and blacklist email spam campaigns. Training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist. We model this task as a learning problem with structured output spaces and an appropriate loss function, derive a decoder and the resulting optimization problem, and a report on a case study conducted with an email service. version:1
arxiv-1206-4636 | Modeling Latent Variable Uncertainty for Loss-based Learning | http://arxiv.org/abs/1206.4636 | id:1206.4636 author:M. Pawan Kumar, Ben Packer, Daphne Koller category:cs.LG cs.AI cs.CV  published:2012-06-18 summary:We consider the problem of parameter estimation using weakly supervised datasets, where a training sample consists of the input and a partially specified annotation, which we refer to as the output. The missing information in the annotation is modeled using latent variables. Previous methods overburden a single distribution with two separate tasks: (i) modeling the uncertainty in the latent variables during training; and (ii) making accurate predictions for the output and the latent variables during testing. We propose a novel framework that separates the demands of the two tasks using two distributions: (i) a conditional distribution to model the uncertainty of the latent variables for a given input-output pair; and (ii) a delta distribution to predict the output and the latent variables for a given input. During learning, we encourage agreement between the two distributions by minimizing a loss-based dissimilarity coefficient. Our approach generalizes latent SVM in two important ways: (i) it models the uncertainty over latent variables instead of relying on a pointwise estimate; and (ii) it allows the use of loss functions that depend on latent variables, which greatly increases its applicability. We demonstrate the efficacy of our approach on two challenging problems---object detection and action detection---using publicly available datasets. version:1
arxiv-1206-4635 | Deep Mixtures of Factor Analysers | http://arxiv.org/abs/1206.4635 | id:1206.4635 author:Yichuan Tang, Ruslan Salakhutdinov, Geoffrey Hinton category:cs.LG stat.ML  published:2012-06-18 summary:An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper, we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets. version:1
arxiv-1206-4634 | Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting | http://arxiv.org/abs/1206.4634 | id:1206.4634 author:Ning Xie, Hirotaka Hachiya, Masashi Sugiyama category:cs.LG cs.GR stat.ML  published:2012-06-18 summary:Oriental ink painting, called Sumi-e, is one of the most appealing painting styles that has attracted artists around the world. Major challenges in computer-based Sumi-e simulation are to abstract complex scene information and draw smooth and natural brush strokes. To automatically find such strokes, we propose to model the brush as a reinforcement learning agent, and learn desired brush-trajectories by maximizing the sum of rewards in the policy search framework. We also provide elaborate design of actions, states, and rewards tailored for a Sumi-e agent. The effectiveness of our proposed approach is demonstrated through simulated Sumi-e experiments. version:1
arxiv-1206-4633 | Fast Bounded Online Gradient Descent Algorithms for Scalable Kernel-Based Online Learning | http://arxiv.org/abs/1206.4633 | id:1206.4633 author:Peilin Zhao, Jialei Wang, Pengcheng Wu, Rong Jin, Steven C. H. Hoi category:cs.LG stat.ML  published:2012-06-18 summary:Kernel-based online learning has often shown state-of-the-art performance for many online learning tasks. It, however, suffers from a major shortcoming, that is, the unbounded number of support vectors, making it non-scalable and unsuitable for applications with large-scale datasets. In this work, we study the problem of bounded kernel-based online learning that aims to constrain the number of support vectors by a predefined budget. Although several algorithms have been proposed in literature, they are neither computationally efficient due to their intensive budget maintenance strategy nor effective due to the use of simple Perceptron algorithm. To overcome these limitations, we propose a framework for bounded kernel-based online learning based on an online gradient descent approach. We propose two efficient algorithms of bounded online gradient descent (BOGD) for scalable kernel-based online learning: (i) BOGD by maintaining support vectors using uniform sampling, and (ii) BOGD++ by maintaining support vectors using non-uniform sampling. We present theoretical analysis of regret bound for both algorithms, and found promising empirical performance in terms of both efficacy and efficiency by comparing them to several well-known algorithms for bounded kernel-based online learning on large-scale datasets. version:1
arxiv-1206-4632 | A Complete Analysis of the l_1,p Group-Lasso | http://arxiv.org/abs/1206.4632 | id:1206.4632 author:Julia Vogt, Volker Roth category:cs.LG math.OC stat.ML  published:2012-06-18 summary:The Group-Lasso is a well-known tool for joint regularization in machine learning methods. While the l_{1,2} and the l_{1,\infty} version have been studied in detail and efficient algorithms exist, there are still open questions regarding other l_{1,p} variants. We characterize conditions for solutions of the l_{1,p} Group-Lasso for all p-norms with 1 <= p <= \infty, and we present a unified active set algorithm. For all p-norms, a highly efficient projected gradient algorithm is presented. This new algorithm enables us to compare the prediction performance of many variants of the Group-Lasso in a multi-task learning setting, where the aim is to solve many learning problems in parallel which are coupled via the Group-Lasso constraint. We conduct large-scale experiments on synthetic data and on two real-world data sets. In accordance with theoretical characterizations of the different norms we observe that the weak-coupling norms with p between 1.5 and 2 consistently outperform the strong-coupling norms with p >> 2. version:1
arxiv-1206-4560 | Residual Component Analysis: Generalising PCA for more flexible inference in linear-Gaussian models | http://arxiv.org/abs/1206.4560 | id:1206.4560 author:Alfredo Kalaitzis, Neil Lawrence category:cs.LG stat.ML  published:2012-06-18 summary:Probabilistic principal component analysis (PPCA) seeks a low dimensional representation of a data set in the presence of independent spherical Gaussian noise. The maximum likelihood solution for the model is an eigenvalue problem on the sample covariance matrix. In this paper we consider the situation where the data variance is already partially explained by other actors, for example sparse conditional dependencies between the covariates, or temporal correlations leaving some residual variance. We decompose the residual variance into its components through a generalised eigenvalue problem, which we call residual component analysis (RCA). We explore a range of new algorithms that arise from the framework, including one that factorises the covariance of a Gaussian density into a low-rank and a sparse-inverse component. We illustrate the ideas on the recovery of a protein-signaling network, a gene expression time-series data set and the recovery of the human skeleton from motion capture 3-D cloud data. version:1
arxiv-1206-4630 | Efficient Decomposed Learning for Structured Prediction | http://arxiv.org/abs/1206.4630 | id:1206.4630 author:Rajhans Samdani, Dan Roth category:cs.LG  published:2012-06-18 summary:Structured prediction is the cornerstone of several machine learning applications. Unfortunately, in structured prediction settings with expressive inter-variable interactions, exact inference-based learning algorithms, e.g. Structural SVM, are often intractable. We present a new way, Decomposed Learning (DecL), which performs efficient learning by restricting the inference step to a limited part of the structured spaces. We provide characterizations based on the structure, target parameters, and gold labels, under which DecL is equivalent to exact learning. We then show that in real world settings, where our theoretical assumptions may not completely hold, DecL-based algorithms are significantly more efficient and as accurate as exact learning. version:1
arxiv-1206-4629 | Multiple Kernel Learning from Noisy Labels by Stochastic Programming | http://arxiv.org/abs/1206.4629 | id:1206.4629 author:Tianbao Yang, Mehrdad Mahdavi, Rong Jin, Lijun Zhang, Yang Zhou category:cs.LG  published:2012-06-18 summary:We study the problem of multiple kernel learning from noisy labels. This is in contrast to most of the previous studies on multiple kernel learning that mainly focus on developing efficient algorithms and assume perfectly labeled training examples. Directly applying the existing multiple kernel learning algorithms to noisily labeled examples often leads to suboptimal performance due to the incorrect class assignments. We address this challenge by casting multiple kernel learning from noisy labels into a stochastic programming problem, and presenting a minimax formulation. We develop an efficient algorithm for solving the related convex-concave optimization problem with a fast convergence rate of $O(1/T)$ where $T$ is the number of iterations. Empirical studies on UCI data sets verify both the effectiveness of the proposed framework and the efficiency of the proposed optimization algorithm. version:1
arxiv-1206-4628 | Robust PCA in High-dimension: A Deterministic Approach | http://arxiv.org/abs/1206.4628 | id:1206.4628 author:Jiashi Feng, Huan Xu, Shuicheng Yan category:cs.LG stat.ML  published:2012-06-18 summary:We consider principal component analysis for contaminated data-set in the high dimensional regime, where the dimensionality of each observation is comparable or even more than the number of observations. We propose a deterministic high-dimensional robust PCA algorithm which inherits all theoretical properties of its randomized counterpart, i.e., it is tractable, robust to contaminated points, easily kernelizable, asymptotic consistent and achieves maximal robustness -- a breakdown point of 50%. More importantly, the proposed method exhibits significantly better computational efficiency, which makes it suitable for large-scale real applications. version:1
arxiv-1206-4627 | Convergence Rates of Biased Stochastic Optimization for Learning Sparse Ising Models | http://arxiv.org/abs/1206.4627 | id:1206.4627 author:Jean Honorio category:cs.LG stat.ML  published:2012-06-18 summary:We study the convergence rate of stochastic optimization of exact (NP-hard) objectives, for which only biased estimates of the gradient are available. We motivate this problem in the context of learning the structure and parameters of Ising models. We first provide a convergence-rate analysis of deterministic errors for forward-backward splitting (FBS). We then extend our analysis to biased stochastic errors, by first characterizing a family of samplers and providing a high probability bound that allows understanding not only FBS, but also proximal gradient (PG) methods. We derive some interesting conclusions: FBS requires only a logarithmically increasing number of random samples in order to converge (although at a very low rate); the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG; accelerated PG is not guaranteed to converge in the biased stochastic setting. version:1
arxiv-1206-4626 | On-Line Portfolio Selection with Moving Average Reversion | http://arxiv.org/abs/1206.4626 | id:1206.4626 author:Bin Li, Steven C. H. Hoi category:cs.CE cs.LG q-fin.PM  published:2012-06-18 summary:On-line portfolio selection has attracted increasing interests in machine learning and AI communities recently. Empirical evidences show that stock's high and low prices are temporary and stock price relatives are likely to follow the mean reversion phenomenon. While the existing mean reversion strategies are shown to achieve good empirical performance on many real datasets, they often make the single-period mean reversion assumption, which is not always satisfied in some real datasets, leading to poor performance when the assumption does not hold. To overcome the limitation, this article proposes a multiple-period mean reversion, or so-called Moving Average Reversion (MAR), and a new on-line portfolio selection strategy named "On-Line Moving Average Reversion" (OLMAR), which exploits MAR by applying powerful online learning techniques. From our empirical results, we found that OLMAR can overcome the drawback of existing mean reversion algorithms and achieve significantly better results, especially on the datasets where the existing mean reversion algorithms failed. In addition to superior trading performance, OLMAR also runs extremely fast, further supporting its practical applicability to a wide range of applications. version:1
arxiv-1206-4625 | Optimizing F-measure: A Tale of Two Approaches | http://arxiv.org/abs/1206.4625 | id:1206.4625 author:Ye Nan, Kian Ming Chai, Wee Sun Lee, Hai Leong Chieu category:cs.LG  published:2012-06-18 summary:F-measures are popular performance metrics, particularly for tasks with imbalanced data sets. Algorithms for learning to maximize F-measures follow two approaches: the empirical utility maximization (EUM) approach learns a classifier having optimal performance on training data, while the decision-theoretic approach learns a probabilistic model and then predicts labels with maximum expected F-measure. In this paper, we investigate the theoretical justifications and connections for these two approaches, and we study the conditions under which one approach is preferable to the other using synthetic and real datasets. Given accurate models, our results suggest that the two approaches are asymptotically equivalent given large training and test sets. Nevertheless, empirically, the EUM approach appears to be more robust against model misspecification, and given a good model, the decision-theoretic approach appears to be better for handling rare classes and a common domain adaptation scenario. version:1
arxiv-1206-4624 | Robust Multiple Manifolds Structure Learning | http://arxiv.org/abs/1206.4624 | id:1206.4624 author:Dian Gong, Xuemei Zhao, Gerard Medioni category:cs.LG stat.ML  published:2012-06-18 summary:We present a robust multiple manifolds structure learning (RMMSL) scheme to robustly estimate data structures under the multiple low intrinsic dimensional manifolds assumption. In the local learning stage, RMMSL efficiently estimates local tangent space by weighted low-rank matrix factorization. In the global learning stage, we propose a robust manifold clustering method based on local structure learning results. The proposed clustering method is designed to get the flattest manifolds clusters by introducing a novel curved-level similarity function. Our approach is evaluated and compared to state-of-the-art methods on synthetic data, handwritten digit images, human motion capture data and motorbike videos. We demonstrate the effectiveness of the proposed approach, which yields higher clustering accuracy, and produces promising results for challenging tasks of human motion segmentation and motion flow learning from videos. version:1
arxiv-1206-4623 | On the Size of the Online Kernel Sparsification Dictionary | http://arxiv.org/abs/1206.4623 | id:1206.4623 author:Yi Sun, Faustino Gomez, Juergen Schmidhuber category:cs.LG stat.ML  published:2012-06-18 summary:We analyze the size of the dictionary constructed from online kernel sparsification, using a novel formula that expresses the expected determinant of the kernel Gram matrix in terms of the eigenvalues of the covariance operator. Using this formula, we are able to connect the cardinality of the dictionary with the eigen-decay of the covariance operator. In particular, we show that under certain technical conditions, the size of the dictionary will always grow sub-linearly in the number of data points, and, as a consequence, the kernel linear regressor constructed from the resulting dictionary is consistent. version:1
arxiv-1206-4622 | A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training | http://arxiv.org/abs/1206.4622 | id:1206.4622 author:Aaron Defazio, Tiberio Caetano category:cs.LG cs.IR stat.ML  published:2012-06-18 summary:Item neighbourhood methods for collaborative filtering learn a weighted graph over the set of items, where each item is connected to those it is most similar to. The prediction of a user's rating on an item is then given by that rating of neighbouring items, weighted by their similarity. This paper presents a new neighbourhood approach which we call item fields, whereby an undirected graphical model is formed over the item graph. The resulting prediction rule is a simple generalization of the classical approaches, which takes into account non-local information in the graph, allowing its best results to be obtained when using drastically fewer edges than other neighbourhood approaches. A fast approximate maximum entropy training method based on the Bethe approximation is presented, which uses a simple gradient ascent procedure. When using precomputed sufficient statistics on the Movielens datasets, our method is faster than maximum likelihood approaches by two orders of magnitude. version:1
arxiv-1206-4621 | Path Integral Policy Improvement with Covariance Matrix Adaptation | http://arxiv.org/abs/1206.4621 | id:1206.4621 author:Freek Stulp, Olivier Sigaud category:cs.LG  published:2012-06-18 summary:There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI2 is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI2 as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI2 to other members of the same family - Cross-Entropy Methods and CMAES - at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for "Path Integral Policy Improvement with Covariance Matrix Adaptation". PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically. version:1
arxiv-1206-4620 | Improved Information Gain Estimates for Decision Tree Induction | http://arxiv.org/abs/1206.4620 | id:1206.4620 author:Sebastian Nowozin category:cs.LG stat.ML  published:2012-06-18 summary:Ensembles of classification and regression trees remain popular machine learning methods because they define flexible non-parametric models that predict well and are computationally efficient both during training and testing. During induction of decision trees one aims to find predicates that are maximally informative about the prediction target. To select good predicates most approaches estimate an information-theoretic scoring function, the information gain, both for classification and regression problems. We point out that the common estimation procedures are biased and show that by replacing them with improved estimators of the discrete and the differential entropy we can obtain better decision trees. In effect our modifications yield improved predictive performance and are simple to implement in any decision tree code. version:1
arxiv-1206-4619 | Inductive Kernel Low-rank Decomposition with Priors: A Generalized Nystrom Method | http://arxiv.org/abs/1206.4619 | id:1206.4619 author:Kai Zhang, Liang Lan, Jun Liu, andreas Rauber, Fabian Moerchen category:cs.LG  published:2012-06-18 summary:Low-rank matrix decomposition has gained great popularity recently in scaling up kernel methods to large amounts of data. However, some limitations could prevent them from working effectively in certain domains. For example, many existing approaches are intrinsically unsupervised, which does not incorporate side information (e.g., class labels) to produce task specific decompositions; also, they typically work "transductively", i.e., the factorization does not generalize to new samples, so the complete factorization needs to be recomputed when new samples become available. To solve these problems, in this paper we propose an"inductive"-flavored method for low-rank kernel decomposition with priors. We achieve this by generalizing the Nystr\"om method in a novel way. On the one hand, our approach employs a highly flexible, nonparametric structure that allows us to generalize the low-rank factors to arbitrarily new samples; on the other hand, it has linear time and space complexities, which can be orders of magnitudes faster than existing approaches and renders great efficiency in learning a low-rank kernel decomposition. Empirical results demonstrate the efficacy and efficiency of the proposed method. version:1
arxiv-1206-4618 | Compact Hyperplane Hashing with Bilinear Functions | http://arxiv.org/abs/1206.4618 | id:1206.4618 author:Wei Liu, Jun Wang, Yadong Mu, Sanjiv Kumar, Shih-Fu Chang category:cs.LG stat.ML  published:2012-06-18 summary:Hyperplane hashing aims at rapidly searching nearest points to a hyperplane, and has shown practical impact in scaling up active learning with SVMs. Unfortunately, the existing randomized methods need long hash codes to achieve reasonable search accuracy and thus suffer from reduced search speed and large memory overhead. To this end, this paper proposes a novel hyperplane hashing technique which yields compact hash codes. The key idea is the bilinear form of the proposed hash functions, which leads to higher collision probability than the existing hyperplane hash functions when using random projections. To further increase the performance, we propose a learning based framework in which the bilinear functions are directly learned from the data. This results in short yet discriminative codes, and also boosts the search performance over the random projection based solutions. Large-scale active learning experiments carried out on two datasets with up to one million samples demonstrate the overall superiority of the proposed approach. version:1
arxiv-1206-4617 | Continuous Inverse Optimal Control with Locally Optimal Examples | http://arxiv.org/abs/1206.4617 | id:1206.4617 author:Sergey Levine, Vladlen Koltun category:cs.LG cs.AI stat.ML  published:2012-06-18 summary:Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods. version:1
arxiv-1206-4616 | A Hierarchical Dirichlet Process Model with Multiple Levels of Clustering for Human EEG Seizure Modeling | http://arxiv.org/abs/1206.4616 | id:1206.4616 author:Drausin Wulsin, Shane Jensen, Brian Litt category:stat.AP cs.LG stat.ML  published:2012-06-18 summary:Driven by the multi-level structure of human intracranial electroencephalogram (iEEG) recordings of epileptic seizures, we introduce a new variant of a hierarchical Dirichlet Process---the multi-level clustering hierarchical Dirichlet Process (MLC-HDP)---that simultaneously clusters datasets on multiple levels. Our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient. The MLC-HDP model clusters over channels-types, seizure-types, and patient-types simultaneously. We describe this model and its implementation in detail. We also present the results of a simulation study comparing the MLC-HDP to a similar model, the Nested Dirichlet Process and finally demonstrate the MLC-HDP's use in modeling seizures across multiple patients. We find the MLC-HDP's clustering to be comparable to independent human physician clusterings. To our knowledge, the MLC-HDP model is the first in the epilepsy literature capable of clustering seizures within and between patients. version:1
arxiv-1206-4615 | Levy Measure Decompositions for the Beta and Gamma Processes | http://arxiv.org/abs/1206.4615 | id:1206.4615 author:Yingjian Wang, Lawrence Carin category:stat.ME cs.LG math.ST stat.TH  published:2012-06-18 summary:We develop new representations for the Levy measures of the beta and gamma processes. These representations are manifested in terms of an infinite sum of well-behaved (proper) beta and gamma distributions. Further, we demonstrate how these infinite sums may be truncated in practice, and explicitly characterize truncation errors. We also perform an analysis of the characteristics of posterior distributions, based on the proposed decompositions. The decompositions provide new insights into the beta and gamma processes (and their generalizations), and we demonstrate how the proposed representation unifies some properties of the two. This paper is meant to provide a rigorous foundation for and new perspectives on Levy processes, as these are of increasing importance in machine learning. version:1
arxiv-1206-4614 | Information-theoretic Semi-supervised Metric Learning via Entropy Regularization | http://arxiv.org/abs/1206.4614 | id:1206.4614 author:Gang Niu, Bo Dai, Makoto Yamada, Masashi Sugiyama category:cs.LG stat.ML  published:2012-06-18 summary:We propose a general information-theoretic approach called Seraph (SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric learning that does not rely upon the manifold assumption. Given the probability parameterized by a Mahalanobis distance, we maximize the entropy of that probability on labeled data and minimize it on unlabeled data following entropy regularization, which allows the supervised and unsupervised parts to be integrated in a natural and meaningful way. Furthermore, Seraph is regularized by encouraging a low-rank projection induced from the metric. The optimization of Seraph is solved efficiently and stably by an EM-like scheme with the analytical E-Step and convex M-Step. Experiments demonstrate that Seraph compares favorably with many well-known global and local metric learning methods. version:1
arxiv-1206-4613 | Near-Optimal BRL using Optimistic Local Transitions | http://arxiv.org/abs/1206.4613 | id:1206.4613 author:Mauricio Araya, Olivier Buffet, Vincent Thomas category:cs.AI cs.LG stat.ML  published:2012-06-18 summary:Model-based Bayesian Reinforcement Learning (BRL) allows a found formalization of the problem of acting optimally while facing an unknown environment, i.e., avoiding the exploration-exploitation dilemma. However, algorithms explicitly addressing BRL suffer from such a combinatorial explosion that a large body of work relies on heuristic algorithms. This paper introduces BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is optimistic about the transition function. We analyze BOLT's sample complexity, and show that under certain parameters, the algorithm is near-optimal in the Bayesian sense with high probability. Then, experimental results highlight the key differences of this method compared to previous work. version:1
arxiv-1206-4612 | Exact Soft Confidence-Weighted Learning | http://arxiv.org/abs/1206.4612 | id:1206.4612 author:Jialei Wang, Peilin Zhao, Steven C. H. Hoi category:cs.LG  published:2012-06-18 summary:In this paper, we propose a new Soft Confidence-Weighted (SCW) online learning scheme, which enables the conventional confidence-weighted learning method to handle non-separable cases. Unlike the previous confidence-weighted learning algorithms, the proposed soft confidence-weighted learning method enjoys all the four salient properties: (i) large margin training, (ii) confidence weighting, (iii) capability to handle non-separable data, and (iv) adaptive margin. Our experimental results show that the proposed SCW algorithms significantly outperform the original CW algorithm. When comparing with a variety of state-of-the-art algorithms (including AROW, NAROW and NHERD), we found that SCW generally achieves better or at least comparable predictive accuracy, but enjoys significant advantage of computational efficiency (i.e., smaller number of updates and lower time cost). version:1
arxiv-1206-4611 | A Convex Feature Learning Formulation for Latent Task Structure Discovery | http://arxiv.org/abs/1206.4611 | id:1206.4611 author:Pratik Jawanpuria, J. Saketha Nath category:cs.LG stat.ML  published:2012-06-18 summary:This paper considers the multi-task learning problem and in the setting where some relevant features could be shared across few related tasks. Most of the existing methods assume the extent to which the given tasks are related or share a common feature space to be known apriori. In real-world applications however, it is desirable to automatically discover the groups of related tasks that share a feature space. In this paper we aim at searching the exponentially large space of all possible groups of tasks that may share a feature space. The main contribution is a convex formulation that employs a graph-based regularizer and simultaneously discovers few groups of related tasks, having close-by task parameters, as well as the feature space shared within each group. The regularizer encodes an important structure among the groups of tasks leading to an efficient algorithm for solving it: if there is no feature space under which a group of tasks has close-by task parameters, then there does not exist such a feature space for any of its supersets. An efficient active set algorithm that exploits this simplification and performs a clever search in the exponentially large space is presented. The algorithm is guaranteed to solve the proposed formulation (within some precision) in a time polynomial in the number of groups of related tasks discovered. Empirical results on benchmark datasets show that the proposed formulation achieves good generalization and outperforms state-of-the-art multi-task learning algorithms in some cases. version:1
arxiv-1206-4610 | Manifold Relevance Determination | http://arxiv.org/abs/1206.4610 | id:1206.4610 author:Andreas Damianou, Carl Ek, Michalis Titsias, Neil Lawrence category:cs.LG cs.CV stat.ML  published:2012-06-18 summary:In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear(in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a "softly" shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data. version:1
arxiv-1206-4609 | On multi-view feature learning | http://arxiv.org/abs/1206.4609 | id:1206.4609 author:Roland Memisevic category:cs.CV cs.LG stat.ML  published:2012-06-18 summary:Sparse coding is a common approach to learning local features for object recognition. Recently, there has been an increasing interest in learning features from spatio-temporal, binocular, or other multi-observation data, where the goal is to encode the relationship between images rather than the content of a single image. We provide an analysis of multi-view feature learning, which shows that hidden variables encode transformations by detecting rotation angles in the eigenspaces shared among multiple image warps. Our analysis helps explain recent experimental results showing that transformation-specific features emerge when training complex cell models on videos. Our analysis also shows that transformation-invariant features can emerge as a by-product of learning representations of transformations. version:1
arxiv-1206-4608 | A Hybrid Algorithm for Convex Semidefinite Optimization | http://arxiv.org/abs/1206.4608 | id:1206.4608 author:Soeren Laue category:cs.LG cs.DS cs.NA stat.ML  published:2012-06-18 summary:We present a hybrid algorithm for optimizing a convex, smooth function over the cone of positive semidefinite matrices. Our algorithm converges to the global optimal solution and can be used to solve general large-scale semidefinite programs and hence can be readily applied to a variety of machine learning problems. We show experimental results on three machine learning problems (matrix completion, metric learning, and sparse PCA) . Our approach outperforms state-of-the-art algorithms. version:1
arxiv-1206-4607 | Distributed Tree Kernels | http://arxiv.org/abs/1206.4607 | id:1206.4607 author:Fabio Massimo Zanzotto, Lorenzo Dell'Arciprete category:cs.LG stat.ML  published:2012-06-18 summary:In this paper, we propose the distributed tree kernels (DTK) as a novel method to reduce time and space complexity of tree kernels. Using a linear complexity algorithm to compute vectors for trees, we embed feature spaces of tree fragments in low-dimensional spaces where the kernel computation is directly done with dot product. We show that DTKs are faster, correlate with tree kernels, and obtain a statistically similar performance in two natural language processing tasks. version:1
arxiv-1206-4606 | TrueLabel + Confusions: A Spectrum of Probabilistic Models in Analyzing Multiple Ratings | http://arxiv.org/abs/1206.4606 | id:1206.4606 author:Chao Liu, Yi-Min Wang category:cs.LG cs.AI stat.ML  published:2012-06-18 summary:This paper revisits the problem of analyzing multiple ratings given by different judges. Different from previous work that focuses on distilling the true labels from noisy crowdsourcing ratings, we emphasize gaining diagnostic insights into our in-house well-trained judges. We generalize the well-known DawidSkene model (Dawid & Skene, 1979) to a spectrum of probabilistic models under the same "TrueLabel + Confusion" paradigm, and show that our proposed hierarchical Bayesian model, called HybridConfusion, consistently outperforms DawidSkene on both synthetic and real-world data sets. version:1
arxiv-1206-4604 | Learning the Experts for Online Sequence Prediction | http://arxiv.org/abs/1206.4604 | id:1206.4604 author:Elad Eban, Aharon Birnbaum, Shai Shalev-Shwartz, Amir Globerson category:cs.LG cs.AI  published:2012-06-18 summary:Online sequence prediction is the problem of predicting the next element of a sequence given previous elements. This problem has been extensively studied in the context of individual sequence prediction, where no prior assumptions are made on the origin of the sequence. Individual sequence prediction algorithms work quite well for long sequences, where the algorithm has enough time to learn the temporal structure of the sequence. However, they might give poor predictions for short sequences. A possible remedy is to rely on the general model of prediction with expert advice, where the learner has access to a set of $r$ experts, each of which makes its own predictions on the sequence. It is well known that it is possible to predict almost as well as the best expert if the sequence length is order of $\log(r)$. But, without firm prior knowledge on the problem, it is not clear how to choose a small set of {\em good} experts. In this paper we describe and analyze a new algorithm that learns a good set of experts using a training set of previously observed sequences. We demonstrate the merits of our approach by applying it on the task of click prediction on the web. version:1
arxiv-1206-4602 | Quasi-Newton Methods: A New Direction | http://arxiv.org/abs/1206.4602 | id:1206.4602 author:Philipp Hennig, Martin Kiefel category:cs.NA cs.LG stat.ML  published:2012-06-18 summary:Four decades after their invention, quasi-Newton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that fit a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efficient use of available information at computational cost similar to its predecessors. version:1
arxiv-1206-4601 | Convex Multitask Learning with Flexible Task Clusters | http://arxiv.org/abs/1206.4601 | id:1206.4601 author:Wenliang Zhong, James Kwok category:cs.LG stat.ML  published:2012-06-18 summary:Traditionally, multitask learning (MTL) assumes that all the tasks are related. This can lead to negative transfer when tasks are indeed incoherent. Recently, a number of approaches have been proposed that alleviate this problem by discovering the underlying task clusters or relationships. However, they are limited to modeling these relationships at the task level, which may be restrictive in some applications. In this paper, we propose a novel MTL formulation that captures task relationships at the feature-level. Depending on the interactions among tasks and features, the proposed method construct different task clusters for different features, without even the need of pre-specifying the number of clusters. Computationally, the proposed formulation is strongly convex, and can be efficiently solved by accelerated proximal methods. Experiments are performed on a number of synthetic and real-world data sets. Under various degrees of task relationships, the accuracy of the proposed method is consistently among the best. Moreover, the feature-specific task clusters obtained agree with the known/plausible task structures of the data. version:1
arxiv-1206-4600 | Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes | http://arxiv.org/abs/1206.4600 | id:1206.4600 author:Murat Dundar, Ferit Akova, Alan Qi, Bartek Rajwa category:cs.LG stat.ML  published:2012-06-18 summary:We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference. Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak. version:1
arxiv-1206-4599 | A Unified Robust Classification Model | http://arxiv.org/abs/1206.4599 | id:1206.4599 author:Akiko Takeda, Hiroyuki Mitsugi, Takafumi Kanamori category:cs.LG stat.ML  published:2012-06-18 summary:A wide variety of machine learning algorithms such as support vector machine (SVM), minimax probability machine (MPM), and Fisher discriminant analysis (FDA), exist for binary classification. The purpose of this paper is to provide a unified classification model that includes the above models through a robust optimization approach. This unified model has several benefits. One is that the extensions and improvements intended for SVM become applicable to MPM and FDA, and vice versa. Another benefit is to provide theoretical results to above learning methods at once by dealing with the unified model. We give a statistical interpretation of the unified classification model and propose a non-convex optimization algorithm that can be applied to non-convex variants of existing learning methods. version:1
arxiv-1206-3881 | DANCo: Dimensionality from Angle and Norm Concentration | http://arxiv.org/abs/1206.3881 | id:1206.3881 author:Claudio Ceruti, Simone Bassis, Alessandro Rozza, Gabriele Lombardi, Elena Casiraghi, Paola Campadelli category:cs.LG stat.ML  published:2012-06-18 summary:In the last decades the estimation of the intrinsic dimensionality of a dataset has gained considerable importance. Despite the great deal of research work devoted to this task, most of the proposed solutions prove to be unreliable when the intrinsic dimensionality of the input dataset is high and the manifold where the points lie is nonlinearly embedded in a higher dimensional space. In this paper we propose a novel robust intrinsic dimensionality estimator that exploits the twofold complementary information conveyed both by the normalized nearest neighbor distances and by the angles computed on couples of neighboring points, providing also closed-forms for the Kullback-Leibler divergences of the respective distributions. Experiments performed on both synthetic and real datasets highlight the robustness and the effectiveness of the proposed algorithm when compared to state of the art methodologies. version:1
arxiv-1206-3777 | An Analysis of the Methods Employed for Breast Cancer Diagnosis | http://arxiv.org/abs/1206.3777 | id:1206.3777 author:Mahjabeen Mirza Beg, Monika Jain category:cs.NE q-bio.TO  published:2012-06-17 summary:Breast cancer research over the last decade has been tremendous. The ground breaking innovations and novel methods help in the early detection, in setting the stages of the therapy and in assessing the response of the patient to the treatment. The prediction of the recurrent cancer is also crucial for the survival of the patient. This paper studies various techniques used for the diagnosis of breast cancer. Different methods are explored for their merits and de-merits for the diagnosis of breast lesion. Some of the methods are yet unproven but the studies look very encouraging. It was found that the recent use of the combination of Artificial Neural Networks in most of the instances gives accurate results for the diagnosis of breast cancer and their use can also be extended to other diseases. version:1
arxiv-1206-4042 | The Stability of Convergence of Curve Evolutions in Vector Fields | http://arxiv.org/abs/1206.4042 | id:1206.4042 author:Junyan Wang, Kap Luk Chan category:cs.CV math.AP  published:2012-06-17 summary:Curve evolution is often used to solve computer vision problems. If the curve evolution fails to converge, we would not be able to solve the targeted problem in a lifetime. This paper studies the theoretical aspect of the convergence of a type of general curve evolutions. We establish a theory for analyzing and improving the stability of the convergence of the general curve evolutions. Based on this theory, we ascertain that the convergence of a known curve evolution is marginal stable. We propose a way of modifying the original curve evolution equation to improve the stability of the convergence according to our theory. Numerical experiments show that the modification improves the convergence of the curve evolution, which validates our theory. version:1
arxiv-1206-3721 | Constraint-free Graphical Model with Fast Learning Algorithm | http://arxiv.org/abs/1206.3721 | id:1206.3721 author:Kazuya Takabatake, Shotaro Akaho category:cs.LG stat.ML  published:2012-06-17 summary:In this paper, we propose a simple, versatile model for learning the structure and parameters of multivariate distributions from a data set. Learning a Markov network from a given data set is not a simple problem, because Markov networks rigorously represent Markov properties, and this rigor imposes complex constraints on the design of the networks. Our proposed model removes these constraints, acquiring important aspects from the information geometry. The proposed parameter- and structure-learning algorithms are simple to execute as they are based solely on local computation at each node. Experiments demonstrate that our algorithms work appropriately. version:1
arxiv-1206-3714 | How important are Deformable Parts in the Deformable Parts Model? | http://arxiv.org/abs/1206.3714 | id:1206.3714 author:Santosh K. Divvala, Alexei A. Efros, Martial Hebert category:cs.CV cs.AI cs.LG  published:2012-06-16 summary:The main stated contribution of the Deformable Parts Model (DPM) detector of Felzenszwalb et al. (over the Histogram-of-Oriented-Gradients approach of Dalal and Triggs) is the use of deformable parts. A secondary contribution is the latent discriminative learning. Tertiary is the use of multiple components. A common belief in the vision community (including ours, before this study) is that their ordering of contributions reflects the performance of detector in practice. However, what we have experimentally found is that the ordering of importance might actually be the reverse. First, we show that by increasing the number of components, and switching the initialization step from their aspect-ratio, left-right flipping heuristics to appearance-based clustering, considerable improvement in performance is obtained. But more intriguingly, we show that with these new components, the part deformations can now be completely switched off, yet obtaining results that are almost on par with the original DPM detector. Finally, we also show initial results for using multiple components on a different problem -- scene classification, suggesting that this idea might have wider applications in addition to object detection. version:1
arxiv-1206-3666 | Unsupervised adaptation of brain machine interface decoders | http://arxiv.org/abs/1206.3666 | id:1206.3666 author:Tayfun Gürel, Carsten Mehring category:cs.LG q-bio.NC I.2.6; I.2.8; G.1.6  published:2012-06-16 summary:The performance of neural decoders can degrade over time due to nonstationarities in the relationship between neuronal activity and behavior. In this case, brain-machine interfaces (BMI) require adaptation of their decoders to maintain high performance across time. One way to achieve this is by use of periodical calibration phases, during which the BMI system (or an external human demonstrator) instructs the user to perform certain movements or behaviors. This approach has two disadvantages: (i) calibration phases interrupt the autonomous operation of the BMI and (ii) between two calibration phases the BMI performance might not be stable but continuously decrease. A better alternative would be that the BMI decoder is able to continuously adapt in an unsupervised manner during autonomous BMI operation, i.e. without knowing the movement intentions of the user. In the present article, we present an efficient method for such unsupervised training of BMI systems for continuous movement control. The proposed method utilizes a cost function derived from neuronal recordings, which guides a learning algorithm to evaluate the decoding parameters. We verify the performance of our adaptive method by simulating a BMI user with an optimal feedback control model and its interaction with our adaptive BMI decoder. The simulation results show that the cost function and the algorithm yield fast and precise trajectories towards targets at random orientations on a 2-dimensional computer screen. For initially unknown and non-stationary tuning parameters, our unsupervised method is still able to generate precise trajectories and to keep its performance stable in the long term. The algorithm can optionally work also with neuronal error signals instead or in conjunction with the proposed unsupervised adaptation. version:1
arxiv-1206-3633 | Feature Based Fuzzy Rule Base Design for Image Extraction | http://arxiv.org/abs/1206.3633 | id:1206.3633 author:Koushik Mondal, Paramartha Dutta, Siddhartha Bhattacharyya category:cs.CV cs.AI  published:2012-06-16 summary:In the recent advancement of multimedia technologies, it becomes a major concern of detecting visual attention regions in the field of image processing. The popularity of the terminal devices in a heterogeneous environment of the multimedia technology gives us enough scope for the betterment of image visualization. Although there exist numerous methods, feature based image extraction becomes a popular one in the field of image processing. The objective of image segmentation is the domain-independent partition of the image into a set of regions, which are visually distinct and uniform with respect to some property, such as grey level, texture or colour. Segmentation and subsequent extraction can be considered the first step and key issue in object recognition, scene understanding and image analysis. Its application area encompasses mobile devices, industrial quality control, medical appliances, robot navigation, geophysical exploration, military applications, etc. In all these areas, the quality of the final results depends largely on the quality of the preprocessing work. Most of the times, acquiring spurious-free preprocessing data requires a lot of application cum mathematical intensive background works. We propose a feature based fuzzy rule guided novel technique that is functionally devoid of any external intervention during execution. Experimental results suggest that this approach is an efficient one in comparison to different other techniques extensively addressed in literature. In order to justify the supremacy of performance of our proposed technique in respect of its competitors, we take recourse to effective metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE) and Peak Signal to Noise Ratio (PSNR). version:1
arxiv-1206-3594 | Blind PSF estimation and methods of deconvolution optimization | http://arxiv.org/abs/1206.3594 | id:1206.3594 author:Yu. A. Bunyak, O. Yu. Sofina, R. N. Kvetnyy category:cs.CV  published:2012-06-15 summary:We have shown that the left side null space of the autoregression (AR) matrix operator is the lexicographical presentation of the point spread function (PSF) on condition the AR parameters are common for original and blurred images. The method of inverse PSF evaluation with regularization functional as the function of surface area is offered. The inverse PSF was used for primary image estimation. Two methods of original image estimate optimization were designed basing on maximum entropy generalization of sought and blurred images conditional probability density and regularization. The first method uses balanced variations of convolution and deconvolution transforms to obtaining iterative schema of image optimization. The variations balance was defined by dynamic regularization basing on condition of iteration process convergence. The regularization has dynamic character because depends on current and previous image estimate variations. The second method implements the regularization of deconvolution optimization in curved space with metric defined on image estimate surface. It is basing on target functional invariance to fluctuations of optimal argument value. The given iterative schemas have faster convergence in comparison with known ones, so they can be used for reconstruction of high resolution images series in real time. version:1
arxiv-1206-3204 | Improved Spectral-Norm Bounds for Clustering | http://arxiv.org/abs/1206.3204 | id:1206.3204 author:Pranjal Awasthi, Or Sheffet category:cs.LG cs.DS  published:2012-06-14 summary:Aiming to unify known results about clustering mixtures of distributions under separation conditions, Kumar and Kannan[2010] introduced a deterministic condition for clustering datasets. They showed that this single deterministic condition encompasses many previously studied clustering assumptions. More specifically, their proximity condition requires that in the target $k$-clustering, the projection of a point $x$ onto the line joining its cluster center $\mu$ and some other center $\mu'$, is a large additive factor closer to $\mu$ than to $\mu'$. This additive factor can be roughly described as $k$ times the spectral norm of the matrix representing the differences between the given (known) dataset and the means of the (unknown) target clustering. Clearly, the proximity condition implies center separation -- the distance between any two centers must be as large as the above mentioned bound. In this paper we improve upon the work of Kumar and Kannan along several axes. First, we weaken the center separation bound by a factor of $\sqrt{k}$, and secondly we weaken the proximity condition by a factor of $k$. Using these weaker bounds we still achieve the same guarantees when all points satisfy the proximity condition. We also achieve better guarantees when only $(1-\epsilon)$-fraction of the points satisfy the weaker proximity condition. The bulk of our analysis relies only on center separation under which one can produce a clustering which (i) has low error, (ii) has low $k$-means cost, and (iii) has centers very close to the target centers. Our improved separation condition allows us to match the results of the Planted Partition Model of McSherry[2001], improve upon the results of Ostrovsky et al[2006], and improve separation results for mixture of Gaussian models in a particular setting. version:2
arxiv-1206-3522 | General Upper Bounds on the Running Time of Parallel Evolutionary Algorithms | http://arxiv.org/abs/1206.3522 | id:1206.3522 author:Jörg Lässig, Dirk Sudholt category:cs.NE F.2.2  published:2012-06-15 summary:We present a new method for analyzing the running time of parallel evolutionary algorithms with spatially structured populations. Based on the fitness-level method, it yields upper bounds on the expected parallel running time. This allows to rigorously estimate the speedup gained by parallelization. Tailored results are given for common migration topologies: ring graphs, torus graphs, hypercubes, and the complete graph. Example applications for pseudo-Boolean optimization show that our method is easy to apply and that it gives powerful results. In our examples the possible speedup increases with the density of the topology. Surprisingly, even sparse topologies like ring graphs lead to a significant speedup for many functions while not increasing the total number of function evaluations by more than a constant factor. We also identify which number of processors yield asymptotically optimal speedups, thus giving hints on how to parametrize parallel evolutionary algorithms. version:1
arxiv-1206-3509 | A Novel Approach for Protein Structure Prediction | http://arxiv.org/abs/1206.3509 | id:1206.3509 author:Saurabh Sarkar, Prateek Malhotra, Virender Guman category:cs.LG q-bio.BM  published:2012-06-15 summary:The idea of this project is to study the protein structure and sequence relationship using the hidden markov model and artificial neural network. In this context we have assumed two hidden markov models. In first model we have taken protein secondary structures as hidden and protein sequences as observed. In second model we have taken protein sequences as hidden and protein structures as observed. The efficiencies for both the hidden markov models have been calculated. The results show that the efficiencies of first model is greater that the second one .These efficiencies are cross validated using artificial neural network. This signifies the importance of protein secondary structures as the main hidden controlling factors due to which we observe a particular amino acid sequence. This also signifies that protein secondary structure is more conserved in comparison to amino acid sequence. version:1
arxiv-1206-3564 | Functional Currents : a new mathematical tool to model and analyse functional shapes | http://arxiv.org/abs/1206.3564 | id:1206.3564 author:Nicolas Charon, Alain Trouvé category:cs.CG cs.CV math.DG  published:2012-06-15 summary:This paper introduces the concept of functional current as a mathematical framework to represent and treat functional shapes, i.e. sub-manifold supported signals. It is motivated by the growing occurrence, in medical imaging and computational anatomy, of what can be described as geometrico-functional data, that is a data structure that involves a deformable shape (roughly a finite dimensional sub manifold) together with a function defined on this shape taking value in another manifold. Indeed, if mathematical currents have already proved to be very efficient theoretically and numerically to model and process shapes as curves or surfaces, they are limited to the manipulation of purely geometrical objects. We show that the introduction of the concept of functional currents offers a genuine solution to the simultaneous processing of the geometric and signal information of any functional shape. We explain how functional currents can be equipped with a Hilbertian norm mixing geometrical and functional content of functional shapes nicely behaving under geometrical and functional perturbations and paving the way to various processing algorithms. We illustrate this potential on two problems: the redundancy reduction of functional shapes representations through matching pursuit schemes on functional currents and the simultaneous geometric and functional registration of functional shapes under diffeomorphic transport. version:1
arxiv-1206-3381 | On the Cover-Hart Inequality: What's a Sample of Size One Worth? | http://arxiv.org/abs/1206.3381 | id:1206.3381 author:Tilmann Gneiting category:math.ST cs.IT math.IT stat.ML stat.TH  published:2012-06-15 summary:Bob predicts a future observation based on a sample of size one. Alice can draw a sample of any size before issuing her prediction. How much better can she do than Bob? Perhaps surprisingly, under a large class of loss functions, which we refer to as the Cover-Hart family, the best Alice can do is to halve Bob's risk. In this sense, half the information in an infinite sample is contained in a sample of size one. The Cover-Hart family is a convex cone that includes metrics and negative definite functions, subject to slight regularity conditions. These results may help explain the small relative differences in empirical performance measures in applied classification and forecasting problems, as well as the success of reasoning and learning by analogy in general, and nearest neighbor techniques in particular. version:1
arxiv-1203-1596 | Multiple Operator-valued Kernel Learning | http://arxiv.org/abs/1203.1596 | id:1203.1596 author:Hachem Kadri, Alain Rakotomamonjy, Francis Bach, Philippe Preux category:stat.ML cs.LG  published:2012-03-07 summary:Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinatedescent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces. version:2
arxiv-1206-3137 | Identifiability and Unmixing of Latent Parse Trees | http://arxiv.org/abs/1206.3137 | id:1206.3137 author:Daniel Hsu, Sham M. Kakade, Percy Liang category:stat.ML cs.LG  published:2012-06-14 summary:This paper explores unsupervised learning of parsing models along two directions. First, which models are identifiable from infinite data? We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models. Second, for identifiable models, how do we estimate the parameters efficiently? EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences. We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models. version:1
arxiv-1111-0352 | Revisiting k-means: New Algorithms via Bayesian Nonparametrics | http://arxiv.org/abs/1111.0352 | id:1111.0352 author:Brian Kulis, Michael I. Jordan category:cs.LG stat.ML  published:2011-11-02 summary:Bayesian models offer great flexibility for clustering applications---Bayesian nonparametrics can be used for modeling infinite mixtures, and hierarchical Bayesian models can be utilized for sharing clusters across multiple data sets. For the most part, such flexibility is lacking in classical clustering methods such as k-means. In this paper, we revisit the k-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspired by the asymptotic connection between k-means and mixtures of Gaussians, we show that a Gibbs sampling algorithm for the Dirichlet process mixture approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like clustering objective that includes a penalty for the number of clusters. We generalize this analysis to the case of clustering multiple data sets through a similar asymptotic argument with the hierarchical Dirichlet process. We also discuss further extensions that highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that does not fix the number of clusters in the graph. version:2
arxiv-1206-3072 | Statistical Consistency of Finite-dimensional Unregularized Linear Classification | http://arxiv.org/abs/1206.3072 | id:1206.3072 author:Matus Telgarsky category:cs.LG stat.ML  published:2012-06-14 summary:This manuscript studies statistical properties of linear classifiers obtained through minimization of an unregularized convex risk over a finite sample. Although the results are explicitly finite-dimensional, inputs may be passed through feature maps; in this way, in addition to treating the consistency of logistic regression, this analysis also handles boosting over a finite weak learning class with, for instance, the exponential, logistic, and hinge losses. In this finite-dimensional setting, it is still possible to fit arbitrary decision boundaries: scaling the complexity of the weak learning class with the sample size leads to the optimal classification risk almost surely. version:1
arxiv-1206-3582 | Decentralized Learning for Multi-player Multi-armed Bandits | http://arxiv.org/abs/1206.3582 | id:1206.3582 author:Dileep Kalathil, Naumaan Nayyar, Rahul Jain category:math.OC cs.LG cs.SY  published:2012-06-14 summary:We consider the problem of distributed online learning with multiple players in multi-armed bandits (MAB) models. Each player can pick among multiple arms. When a player picks an arm, it gets a reward. We consider both i.i.d. reward model and Markovian reward model. In the i.i.d. model each arm is modelled as an i.i.d. process with an unknown distribution with an unknown mean. In the Markovian model, each arm is modelled as a finite, irreducible, aperiodic and reversible Markov chain with an unknown probability transition matrix and stationary distribution. The arms give different rewards to different players. If two players pick the same arm, there is a "collision", and neither of them get any reward. There is no dedicated control channel for coordination or communication among the players. Any other communication between the users is costly and will add to the regret. We propose an online index-based distributed learning policy called ${\tt dUCB_4}$ algorithm that trades off \textit{exploration v. exploitation} in the right way, and achieves expected regret that grows at most as near-$O(\log^2 T)$. The motivation comes from opportunistic spectrum access by multiple secondary users in cognitive radio networks wherein they must pick among various wireless channels that look different to different users. This is the first distributed learning algorithm for multi-player MABs to the best of our knowledge. version:1
arxiv-1111-6337 | Regret Bound by Variation for Online Convex Optimization | http://arxiv.org/abs/1111.6337 | id:1111.6337 author:Tianbao Yang, Mehrdad Mahdavi, Rong Jin, Shenghuo Zhu category:cs.LG  published:2011-11-28 summary:In citep{Hazan-2008-extract}, the authors showed that the regret of online linear optimization can be bounded by the total variation of the cost vectors. In this paper, we extend this result to general online convex optimization. We first analyze the limitations of the algorithm in \citep{Hazan-2008-extract} when applied it to online convex optimization. We then present two algorithms for online convex optimization whose regrets are bounded by the variation of cost functions. We finally consider the bandit setting, and present a randomized algorithm for online bandit convex optimization with a variation-based regret bound. We show that the regret bound for online bandit convex optimization is optimal when the variation of cost functions is independent of the number of trials. version:4
arxiv-1206-3297 | Hybrid Variational/Gibbs Collapsed Inference in Topic Models | http://arxiv.org/abs/1206.3297 | id:1206.3297 author:Max Welling, Yee Whye Teh, Hilbert Kappen category:cs.LG stat.ML  published:2012-06-13 summary:Variational Bayesian inference and (collapsed) Gibbs sampling are the two important classes of inference algorithms for Bayesian networks. Both have their advantages and disadvantages: collapsed Gibbs sampling is unbiased but is also inefficient for large count values and requires averaging over many samples to reduce variance. On the other hand, variational Bayesian inference is efficient and accurate for large count values but suffers from bias for small counts. We propose a hybrid algorithm that combines the best of both worlds: it samples very small counts and applies variational updates to large counts. This hybridization is shown to significantly improve testset perplexity relative to variational inference at no computational cost. version:1
arxiv-1206-3294 | Flexible Priors for Exemplar-based Clustering | http://arxiv.org/abs/1206.3294 | id:1206.3294 author:Daniel Tarlow, Richard S. Zemel, Brendan J. Frey category:cs.LG stat.ML  published:2012-06-13 summary:Exemplar-based clustering methods have been shown to produce state-of-the-art results on a number of synthetic and real-world clustering problems. They are appealing because they offer computational benefits over latent-mean models and can handle arbitrary pairwise similarity measures between data points. However, when trying to recover underlying structure in clustering problems, tailored similarity measures are often not enough; we also desire control over the distribution of cluster sizes. Priors such as Dirichlet process priors allow the number of clusters to be unspecified while expressing priors over data partitions. To our knowledge, they have not been applied to exemplar-based models. We show how to incorporate priors, including Dirichlet process priors, into the recently introduced affinity propagation algorithm. We develop an efficient maxproduct belief propagation algorithm for our new model and demonstrate experimentally how the expanded range of clustering priors allows us to better recover true clusterings in situations where we have some information about the generating process. version:1
arxiv-1206-3293 | Propagation using Chain Event Graphs | http://arxiv.org/abs/1206.3293 | id:1206.3293 author:Peter Thwaites, Jim Q. Smith, Robert G. Cowell category:cs.AI cs.CL  published:2012-06-13 summary:A Chain Event Graph (CEG) is a graphial model which designed to embody conditional independencies in problems whose state spaces are highly asymmetric and do not admit a natural product structure. In this paer we present a probability propagation algorithm which uses the topology of the CEG to build a transporter CEG. Intriungly,the transporter CEG is directly analogous to the triangulated Bayesian Network (BN) in the more conventional junction tree propagation algorithms used with BNs. The propagation method uses factorization formulae also analogous to (but different from) the ones using potentials on cliques and separators of the BN. It appears that the methods will be typically more efficient than the BN algorithms when applied to contexts where there is significant asymmetry present. version:1
arxiv-1206-3290 | Modelling local and global phenomena with sparse Gaussian processes | http://arxiv.org/abs/1206.3290 | id:1206.3290 author:Jarno Vanhatalo, Aki Vehtari category:cs.LG stat.ML  published:2012-06-13 summary:Much recent work has concerned sparse approximations to speed up the Gaussian process regression from the unfavorable O(n3) scaling in computational time to O(nm2). Thus far, work has concentrated on models with one covariance function. However, in many practical situations additive models with multiple covariance functions may perform better, since the data may contain both long and short length-scale phenomena. The long length-scales can be captured with global sparse approximations, such as fully independent conditional (FIC), and the short length-scales can be modeled naturally by covariance functions with compact support (CS). CS covariance functions lead to naturally sparse covariance matrices, which are computationally cheaper to handle than full covariance matrices. In this paper, we propose a new sparse Gaussian process model with two additive components: FIC for the long length-scales and CS covariance function for the short length-scales. We give theoretical and experimental results and show that under certain conditions the proposed model has the same computational complexity as FIC. We also compare the model performance of the proposed model to additive models approximated by fully and partially independent conditional (PIC). We use real data sets and show that our model outperforms FIC and PIC approximations for data sets with two additive phenomena. version:1
arxiv-1206-3287 | Learning the Bayesian Network Structure: Dirichlet Prior versus Data | http://arxiv.org/abs/1206.3287 | id:1206.3287 author:Harald Steck category:cs.LG stat.ME stat.ML  published:2012-06-13 summary:In the Bayesian approach to structure learning of graphical models, the equivalent sample size (ESS) in the Dirichlet prior over the model parameters was recently shown to have an important effect on the maximum-a-posteriori estimate of the Bayesian network structure. In our first contribution, we theoretically analyze the case of large ESS-values, which complements previous work: among other results, we find that the presence of an edge in a Bayesian network is favoured over its absence even if both the Dirichlet prior and the data imply independence, as long as the conditional empirical distribution is notably different from uniform. In our second contribution, we focus on realistic ESS-values, and provide an analytical approximation to the "optimal" ESS-value in a predictive sense (its accuracy is also validated experimentally): this approximation provides an understanding as to which properties of the data have the main effect determining the "optimal" ESS-value. version:1
arxiv-1206-3285 | Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping | http://arxiv.org/abs/1206.3285 | id:1206.3285 author:Richard S. Sutton, Csaba Szepesvari, Alborz Geramifard, Michael P. Bowling category:cs.AI cs.LG cs.SY  published:2012-06-13 summary:We consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world. This paper develops an explicitly model-based approach extending the Dyna architecture to linear function approximation. Dynastyle planning proceeds by generating imaginary experience from the world model and then applying model-free reinforcement learning algorithms to the imagined state transitions. Our main results are to prove that linear Dyna-style planning converges to a unique solution independent of the generating distribution, under natural conditions. In the policy evaluation setting, we prove that the limit point is the least-squares (LSTD) solution. An implication of our results is that prioritized-sweeping can be soundly extended to the linear approximation case, backing up to preceding features rather than to preceding states. We introduce two versions of prioritized sweeping with linear Dyna and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems. version:1
arxiv-1206-3279 | The Phylogenetic Indian Buffet Process: A Non-Exchangeable Nonparametric Prior for Latent Features | http://arxiv.org/abs/1206.3279 | id:1206.3279 author:Kurt T. Miller, Thomas Griffiths, Michael I. Jordan category:cs.LG stat.ML  published:2012-06-13 summary:Nonparametric Bayesian models are often based on the assumption that the objects being modeled are exchangeable. While appropriate in some applications (e.g., bag-of-words models for documents), exchangeability is sometimes assumed simply for computational reasons; non-exchangeable models might be a better choice for applications based on subject matter. Drawing on ideas from graphical models and phylogenetics, we describe a non-exchangeable prior for a class of nonparametric latent feature models that is nearly as efficient computationally as its exchangeable counterpart. Our model is applicable to the general setting in which the dependencies between objects can be expressed using a tree, where edge lengths indicate the strength of relationships. We demonstrate an application to modeling probabilistic choice. version:1
arxiv-1206-3275 | Learning Hidden Markov Models for Regression using Path Aggregation | http://arxiv.org/abs/1206.3275 | id:1206.3275 author:Keith Noto, Mark Craven category:cs.LG cs.CE q-bio.QM  published:2012-06-13 summary:We consider the task of learning mappings from sequential data to real-valued responses. We present and evaluate an approach to learning a type of hidden Markov model (HMM) for regression. The learning process involves inferring the structure and parameters of a conventional HMM, while simultaneously learning a regression model that maps features that characterize paths through the model to continuous responses. Our results, in both synthetic and biological domains, demonstrate the value of jointly learning the two components of our approach. version:1
arxiv-1206-3274 | Small Sample Inference for Generalization Error in Classification Using the CUD Bound | http://arxiv.org/abs/1206.3274 | id:1206.3274 author:Eric B. Laber, Susan A. Murphy category:cs.LG stat.ML  published:2012-06-13 summary:Confidence measures for the generalization error are crucial when small training samples are used to construct classifiers. A common approach is to estimate the generalization error by resampling and then assume the resampled estimator follows a known distribution to form a confidence set [Kohavi 1995, Martin 1996,Yang 2006]. Alternatively, one might bootstrap the resampled estimator of the generalization error to form a confidence set. Unfortunately, these methods do not reliably provide sets of the desired confidence. The poor performance appears to be due to the lack of smoothness of the generalization error as a function of the learned classifier. This results in a non-normal distribution of the estimated generalization error. We construct a confidence set for the generalization error by use of a smooth upper bound on the deviation between the resampled estimate and generalization error. The confidence set is formed by bootstrapping this upper bound. In cases in which the approximation class for the classifier can be represented as a parametric additive model, we provide a computationally efficient algorithm. This method exhibits superior performance across a series of test and simulated data sets. version:1
arxiv-1206-3270 | Estimation and Clustering with Infinite Rankings | http://arxiv.org/abs/1206.3270 | id:1206.3270 author:Marina Meila, Le Bao category:cs.LG stat.ML  published:2012-06-13 summary:This paper presents a natural extension of stagewise ranking to the the case of infinitely many items. We introduce the infinite generalized Mallows model (IGM), describe its properties and give procedures to estimate it from data. For estimation of multimodal distributions we introduce the Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The experiments highlight the properties of the new model and demonstrate that infinite models can be simple, elegant and practical. version:1
arxiv-1206-3269 | Bayesian Out-Trees | http://arxiv.org/abs/1206.3269 | id:1206.3269 author:Tony S. Jebara category:cs.LG stat.ML  published:2012-06-13 summary:A Bayesian treatment of latent directed graph structure for non-iid data is provided where each child datum is sampled with a directed conditional dependence on a single unknown parent datum. The latent graph structure is assumed to lie in the family of directed out-tree graphs which leads to efficient Bayesian inference. The latent likelihood of the data and its gradients are computable in closed form via Tutte's directed matrix tree theorem using determinants and inverses of the out-Laplacian. This novel likelihood subsumes iid likelihood, is exchangeable and yields efficient unsupervised and semi-supervised learning algorithms. In addition to handling taxonomy and phylogenetic datasets the out-tree assumption performs surprisingly well as a semi-parametric density estimator on standard iid datasets. Experiments with unsupervised and semisupervised learning are shown on various UCI and taxonomy datasets. version:1
arxiv-1206-3262 | Convergent Message-Passing Algorithms for Inference over General Graphs with Convex Free Energies | http://arxiv.org/abs/1206.3262 | id:1206.3262 author:Tamir Hazan, Amnon Shashua category:cs.LG stat.ML  published:2012-06-13 summary:Inference problems in graphical models can be represented as a constrained optimization of a free energy function. It is known that when the Bethe free energy is used, the fixedpoints of the belief propagation (BP) algorithm correspond to the local minima of the free energy. However BP fails to converge in many cases of interest. Moreover, the Bethe free energy is non-convex for graphical models with cycles thus introducing great difficulty in deriving efficient algorithms for finding local minima of the free energy for general graphs. In this paper we introduce two efficient BP-like algorithms, one sequential and the other parallel, that are guaranteed to converge to the global minimum, for any graph, over the class of energies known as "convex free energies". In addition, we propose an efficient heuristic for setting the parameters of the convex free energy based on the structure of the graph. version:1
arxiv-1206-3260 | Causal discovery of linear acyclic models with arbitrary distributions | http://arxiv.org/abs/1206.3260 | id:1206.3260 author:Patrik O. Hoyer, Aapo Hyvarinen, Richard Scheines, Peter L. Spirtes, Joseph Ramsey, Gustavo Lacerda, Shohei Shimizu category:stat.ML cs.AI cs.LG  published:2012-06-13 summary:An important task in data analysis is the discovery of causal relationships between observed variables. For continuous-valued data, linear acyclic causal models are commonly used to model the data-generating process, and the inference of such models is a well-studied problem. However, existing methods have significant limitations. Methods based on conditional independencies (Spirtes et al. 1993; Pearl 2000) cannot distinguish between independence-equivalent models, whereas approaches purely based on Independent Component Analysis (Shimizu et al. 2006) are inapplicable to data which is partially Gaussian. In this paper, we generalize and combine the two approaches, to yield a method able to learn the model structure in many cases for which the previous methods provide answers that are either incorrect or are not as informative as possible. We give exact graphical conditions for when two distinct models represent the same family of distributions, and empirically demonstrate the power of our method through thorough simulations. version:1
arxiv-1206-3259 | Cumulative distribution networks and the derivative-sum-product algorithm | http://arxiv.org/abs/1206.3259 | id:1206.3259 author:Jim Huang, Brendan J. Frey category:cs.LG stat.ML  published:2012-06-13 summary:We introduce a new type of graphical model called a "cumulative distribution network" (CDN), which expresses a joint cumulative distribution as a product of local functions. Each local function can be viewed as providing evidence about possible orderings, or rankings, of variables. Interestingly, we find that the conditional independence properties of CDNs are quite different from other graphical models. We also describe a messagepassing algorithm that efficiently computes conditional cumulative distributions. Due to the unique independence properties of the CDN, these messages do not in general have a one-to-one correspondence with messages exchanged in standard algorithms, such as belief propagation. We demonstrate the application of CDNs for structured ranking learning using a previously-studied multi-player gaming dataset. version:1
arxiv-1206-3257 | Constrained Approximate Maximum Entropy Learning of Markov Random Fields | http://arxiv.org/abs/1206.3257 | id:1206.3257 author:Varun Ganapathi, David Vickrey, John Duchi, Daphne Koller category:cs.LG stat.ML  published:2012-06-13 summary:Parameter estimation in Markov random fields (MRFs) is a difficult task, in which inference over the network is run in the inner loop of a gradient descent procedure. Replacing exact inference with approximate methods such as loopy belief propagation (LBP) can suffer from poor convergence. In this paper, we provide a different approach for combining MRF learning and Bethe approximation. We consider the dual of maximum likelihood Markov network learning - maximizing entropy with moment matching constraints - and then approximate both the objective and the constraints in the resulting optimization problem. Unlike previous work along these lines (Teh & Welling, 2003), our formulation allows parameter sharing between features in a general log-linear model, parameter regularization and conditional training. We show that piecewise training (Sutton & McCallum, 2005) is a very restricted special case of this formulation. We study two optimization strategies: one based on a single convex approximation and one that uses repeated convex approximations. We show results on several real-world networks that demonstrate that these algorithms can significantly outperform learning with loopy and piecewise. Our results also provide a framework for analyzing the trade-offs of different relaxations of the entropy objective and of the constraints. version:1
arxiv-1206-3256 | Multi-View Learning over Structured and Non-Identical Outputs | http://arxiv.org/abs/1206.3256 | id:1206.3256 author:Kuzman Ganchev, Joao Graca, John Blitzer, Ben Taskar category:cs.LG stat.ML  published:2012-06-13 summary:In many machine learning problems, labeled training data is limited but unlabeled data is ample. Some of these problems have instances that can be factored into multiple views, each of which is nearly sufficent in determining the correct labels. In this paper we present a new algorithm for probabilistic multi-view learning which uses the idea of stochastic agreement between views as regularization. Our algorithm works on structured and unstructured problems and easily generalizes to partial agreement scenarios. For the full agreement case, our algorithm minimizes the Bhattacharyya distance between the models of each view, and performs better than CoBoosting and two-view Perceptron on several flat and structured classification problems. version:1
arxiv-1206-3254 | Latent Topic Models for Hypertext | http://arxiv.org/abs/1206.3254 | id:1206.3254 author:Amit Gruber, Michal Rosen-Zvi, Yair Weiss category:cs.IR cs.CL cs.LG stat.ML  published:2012-06-13 summary:Latent topic models have been successfully applied as an unsupervised topic discovery technique in large document collections. With the proliferation of hypertext document collection such as the Internet, there has also been great interest in extending these approaches to hypertext [6, 9]. These approaches typically model links in an analogous fashion to how they model words - the document-link co-occurrence matrix is modeled in the same way that the document-word co-occurrence matrix is modeled in standard topic models. In this paper we present a probabilistic generative model for hypertext document collections that explicitly models the generation of links. Specifically, links from a word w to a document d depend directly on how frequent the topic of w is in d, in addition to the in-degree of d. We show how to perform EM learning on this model efficiently. By not modeling links as analogous to words, we end up using far fewer free parameters and obtain better link prediction results. version:1
arxiv-1206-3252 | Convex Point Estimation using Undirected Bayesian Transfer Hierarchies | http://arxiv.org/abs/1206.3252 | id:1206.3252 author:Gal Elidan, Ben Packer, Geremy Heitz, Daphne Koller category:cs.LG stat.ML  published:2012-06-13 summary:When related learning tasks are naturally arranged in a hierarchy, an appealing approach for coping with scarcity of instances is that of transfer learning using a hierarchical Bayes framework. As fully Bayesian computations can be difficult and computationally demanding, it is often desirable to use posterior point estimates that facilitate (relatively) efficient prediction. However, the hierarchical Bayes framework does not always lend itself naturally to this maximum aposteriori goal. In this work we propose an undirected reformulation of hierarchical Bayes that relies on priors in the form of similarity measures. We introduce the notion of "degree of transfer" weights on components of these similarity measures, and show how they can be automatically learned within a joint probabilistic framework. Importantly, our reformulation results in a convex objective for many learning problems, thus facilitating optimal posterior point estimation using standard optimization techniques. In addition, we no longer require proper priors, allowing for flexible and straightforward specification of joint distributions over transfer hierarchies. We show that our framework is effective for learning models that are part of transfer hierarchies for two real-life tasks: object shape modeling using Gaussian density estimation and document classification. version:1
arxiv-1206-3249 | Projected Subgradient Methods for Learning Sparse Gaussians | http://arxiv.org/abs/1206.3249 | id:1206.3249 author:John Duchi, Stephen Gould, Daphne Koller category:cs.LG stat.ML  published:2012-06-13 summary:Gaussian Markov random fields (GMRFs) are useful in a broad range of applications. In this paper we tackle the problem of learning a sparse GMRF in a high-dimensional space. Our approach uses the l1-norm as a regularization on the inverse covariance matrix. We utilize a novel projected gradient method, which is faster than previous methods in practice and equal to the best performing of these in asymptotic complexity. We also extend the l1-regularized objective to the problem of sparsifying entire blocks within the inverse covariance matrix. Our methods generalize fairly easily to this case, while other methods do not. We demonstrate that our extensions give better generalization performance on two real domains--biological network analysis and a 2D-shape modeling image task. version:1
arxiv-1206-3247 | Learning Convex Inference of Marginals | http://arxiv.org/abs/1206.3247 | id:1206.3247 author:Justin Domke category:cs.LG stat.ML  published:2012-06-13 summary:Graphical models trained using maximum likelihood are a common tool for probabilistic inference of marginal distributions. However, this approach suffers difficulties when either the inference process or the model is approximate. In this paper, the inference process is first defined to be the minimization of a convex function, inspired by free energy approximations. Learning is then done directly in terms of the performance of the inference process at univariate marginal prediction. The main novelty is that this is a direct minimization of emperical risk, where the risk measures the accuracy of predicted marginals. version:1
arxiv-1206-3243 | Bounds on the Bethe Free Energy for Gaussian Networks | http://arxiv.org/abs/1206.3243 | id:1206.3243 author:Botond Cseke, Tom Heskes category:cs.LG stat.ML  published:2012-06-13 summary:We address the problem of computing approximate marginals in Gaussian probabilistic models by using mean field and fractional Bethe approximations. As an extension of Welling and Teh (2001), we define the Gaussian fractional Bethe free energy in terms of the moment parameters of the approximate marginals and derive an upper and lower bound for it. We give necessary conditions for the Gaussian fractional Bethe free energies to be bounded from below. It turns out that the bounding condition is the same as the pairwise normalizability condition derived by Malioutov et al. (2006) as a sufficient condition for the convergence of the message passing algorithm. By giving a counterexample, we disprove the conjecture in Welling and Teh (2001): even when the Bethe free energy is not bounded from below, it can possess a local minimum to which the minimization algorithms can converge. version:1
arxiv-1206-3242 | Multi-View Learning in the Presence of View Disagreement | http://arxiv.org/abs/1206.3242 | id:1206.3242 author:C. Christoudias, Raquel Urtasun, Trevor Darrell category:cs.LG stat.ML  published:2012-06-13 summary:Traditional multi-view learning approaches suffer in the presence of view disagreement,i.e., when samples in each view do not belong to the same class due to view corruption, occlusion or other noise processes. In this paper we present a multi-view learning approach that uses a conditional entropy criterion to detect view disagreement. Once detected, samples with view disagreement are filtered and standard multi-view learning methods can be successfully applied to the remaining samples. Experimental evaluation on synthetic and audio-visual databases demonstrates that the detection and filtering of view disagreement considerably increases the performance of traditional multi-view learning approaches. version:1
arxiv-1206-3241 | Approximating the Partition Function by Deleting and then Correcting for Model Edges | http://arxiv.org/abs/1206.3241 | id:1206.3241 author:Arthur Choi, Adnan Darwiche category:cs.LG stat.ML  published:2012-06-13 summary:We propose an approach for approximating the partition function which is based on two steps: (1) computing the partition function of a simplified model which is obtained by deleting model edges, and (2) rectifying the result by applying an edge-by-edge correction. The approach leads to an intuitive framework in which one can trade-off the quality of an approximation with the complexity of computing it. It also includes the Bethe free energy approximation as a degenerate case. We develop the approach theoretically in this paper and provide a number of empirical results that reveal its practical utility. version:1
arxiv-1206-3238 | Greedy Block Coordinate Descent for Large Scale Gaussian Process Regression | http://arxiv.org/abs/1206.3238 | id:1206.3238 author:Liefeng Bo, Cristian Sminchisescu category:cs.LG stat.ML  published:2012-06-13 summary:We propose a variable decomposition algorithm -greedy block coordinate descent (GBCD)- in order to make dense Gaussian process regression practical for large scale problems. GBCD breaks a large scale optimization into a series of small sub-problems. The challenge in variable decomposition algorithms is the identification of a subproblem (the active set of variables) that yields the largest improvement. We analyze the limitations of existing methods and cast the active set selection into a zero-norm constrained optimization problem that we solve using greedy methods. By directly estimating the decrease in the objective function, we obtain not only efficient approximate solutions for GBCD, but we are also able to demonstrate that the method is globally convergent. Empirical comparisons against competing dense methods like Conjugate Gradient or SMO show that GBCD is an order of magnitude faster. Comparisons against sparse GP methods show that GBCD is both accurate and capable of handling datasets of 100,000 samples or more. version:1
arxiv-1206-3237 | Clique Matrices for Statistical Graph Decomposition and Parameterising Restricted Positive Definite Matrices | http://arxiv.org/abs/1206.3237 | id:1206.3237 author:David Barber category:cs.DM cs.LG stat.ML  published:2012-06-13 summary:We introduce Clique Matrices as an alternative representation of undirected graphs, being a generalisation of the incidence matrix representation. Here we use clique matrices to decompose a graph into a set of possibly overlapping clusters, de ned as well-connected subsets of vertices. The decomposition is based on a statistical description which encourages clusters to be well connected and few in number. Inference is carried out using a variational approximation. Clique matrices also play a natural role in parameterising positive de nite matrices under zero constraints on elements of the matrix. We show that clique matrices can parameterise all positive de nite matrices restricted according to a decomposable graph and form a structured Factor Analysis approximation in the non-decomposable case. version:1
arxiv-1206-3236 | Learning Inclusion-Optimal Chordal Graphs | http://arxiv.org/abs/1206.3236 | id:1206.3236 author:Vincent Auvray, Louis Wehenkel category:cs.LG cs.DS stat.ML  published:2012-06-13 summary:Chordal graphs can be used to encode dependency models that are representable by both directed acyclic and undirected graphs. This paper discusses a very simple and efficient algorithm to learn the chordal structure of a probabilistic model from data. The algorithm is a greedy hill-climbing search algorithm that uses the inclusion boundary neighborhood over chordal graphs. In the limit of a large sample size and under appropriate hypotheses on the scoring criterion, we prove that the algorithm will find a structure that is inclusion-optimal when the dependency model of the data-generating distribution can be represented exactly by an undirected graph. The algorithm is evaluated on simulated datasets. version:1
arxiv-1206-2807 | An efficient hierarchical graph based image segmentation | http://arxiv.org/abs/1206.2807 | id:1206.2807 author:Silvio Jamil F. Guimarães, Jean Cousty, Yukiko Kenmochi, Laurent Najman category:cs.CV  published:2012-06-13 summary:Hierarchical image segmentation provides region-oriented scalespace, i.e., a set of image segmentations at different detail levels in which the segmentations at finer levels are nested with respect to those at coarser levels. Most image segmentation algorithms, such as region merging algorithms, rely on a criterion for merging that does not lead to a hierarchy, and for which the tuning of the parameters can be difficult. In this work, we propose a hierarchical graph based image segmentation relying on a criterion popularized by Felzenzwalb and Huttenlocher. We illustrate with both real and synthetic images, showing efficiency, ease of use, and robustness of our method. version:1
arxiv-1206-3231 | CORL: A Continuous-state Offset-dynamics Reinforcement Learner | http://arxiv.org/abs/1206.3231 | id:1206.3231 author:Emma Brunskill, Bethany Leffler, Lihong Li, Michael L. Littman, Nicholas Roy category:cs.LG stat.ML  published:2012-06-13 summary:Continuous state spaces and stochastic, switching dynamics characterize a number of rich, realworld domains, such as robot navigation across varying terrain. We describe a reinforcementlearning algorithm for learning in these domains and prove for certain environments the algorithm is probably approximately correct with a sample complexity that scales polynomially with the state-space dimension. Unfortunately, no optimal planning techniques exist in general for such problems; instead we use fitted value iteration to solve the learned MDP, and include the error due to approximate planning in our bounds. Finally, we report an experiment using a robotic car driving over varying terrain to demonstrate that these dynamics representations adequately capture real-world dynamics and that our algorithm can be used to efficiently solve such problems. version:1
arxiv-1206-2691 | IDS: An Incremental Learning Algorithm for Finite Automata | http://arxiv.org/abs/1206.2691 | id:1206.2691 author:Muddassar A. Sindhu, Karl Meinke category:cs.LG cs.DS cs.FL  published:2012-06-13 summary:We present a new algorithm IDS for incremental learning of deterministic finite automata (DFA). This algorithm is based on the concept of distinguishing sequences introduced in (Angluin81). We give a rigorous proof that two versions of this learning algorithm correctly learn in the limit. Finally we present an empirical performance analysis that compares these two algorithms, focussing on learning times and different types of learning queries. We conclude that IDS is an efficient algorithm for software engineering applications of automata learning, such as testing and model inference. version:1
arxiv-1204-5043 | Sparse Prediction with the $k$-Support Norm | http://arxiv.org/abs/1204.5043 | id:1204.5043 author:Andreas Argyriou, Rina Foygel, Nathan Srebro category:stat.ML cs.LG  published:2012-04-23 summary:We derive a novel norm that corresponds to the tightest convex relaxation of sparsity combined with an $\ell_2$ penalty. We show that this new {\em $k$-support norm} provides a tighter relaxation than the elastic net and is thus a good replacement for the Lasso or the elastic net in sparse prediction problems. Through the study of the $k$-support norm, we also bound the looseness of the elastic net, thus shedding new light on it and providing justification for its use. version:2
arxiv-1206-2437 | A Novel Windowing Technique for Efficient Computation of MFCC for Speaker Recognition | http://arxiv.org/abs/1206.2437 | id:1206.2437 author:Md. Sahidullah, Goutam Saha category:cs.CV  published:2012-06-12 summary:In this paper, we propose a novel family of windowing technique to compute Mel Frequency Cepstral Coefficient (MFCC) for automatic speaker recognition from speech. The proposed method is based on fundamental property of discrete time Fourier transform (DTFT) related to differentiation in frequency domain. Classical windowing scheme such as Hamming window is modified to obtain derivatives of discrete time Fourier transform coefficients. It has been mathematically shown that the slope and phase of power spectrum are inherently incorporated in newly computed cepstrum. Speaker recognition systems based on our proposed family of window functions are shown to attain substantial and consistent performance improvement over baseline single tapered Hamming window as well as recently proposed multitaper windowing technique. version:1
arxiv-1206-2197 | Complex Orthogonal Matching Pursuit and Its Exact Recovery Conditions | http://arxiv.org/abs/1206.2197 | id:1206.2197 author:Rong Fan, Qun Wan, Yipeng Liu, Hui Chen, Xiao Zhang category:cs.IT math.IT math.NA stat.ML  published:2012-06-11 summary:In this paper, we present new results on using orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries for complex cases (i.e., complex measurement vector, complex dictionary and complex additive white Gaussian noise (CAWGN)). A sufficient condition that OMP can recover the optimal representation of an exactly sparse signal in the complex cases is proposed both in noiseless and bound Gaussian noise settings. Similar to exact recovery condition (ERC) results in real cases, we extend them to complex case and derivate the corresponding ERC in the paper. It leverages this theory to show that OMP succeed for k-sparse signal from a class of complex dictionary. Besides, an application with geometrical theory of diffraction (GTD) model is presented for complex cases. Finally, simulation experiments illustrate the validity of the theoretical analysis. version:1
arxiv-1206-2190 | Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation | http://arxiv.org/abs/1206.2190 | id:1206.2190 author:Jian-feng Yan, Zhi-Qiang Liu, Yang Gao, Jia Zeng category:cs.LG  published:2012-06-11 summary:This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA). Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture. Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP. Extensive experiments on different data sets demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces more than 80% communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm. version:1
arxiv-1206-2061 | Comments on "On Approximating Euclidean Metrics by Weighted t-Cost Distances in Arbitrary Dimension" | http://arxiv.org/abs/1206.2061 | id:1206.2061 author:M. Emre Celebi, Hassan A. Kingravi, Fatih Celiker category:cs.NA cs.CV G.1.2; I.5  published:2012-06-10 summary:Mukherjee (Pattern Recognition Letters, vol. 32, pp. 824-831, 2011) recently introduced a class of distance functions called weighted t-cost distances that generalize m-neighbor, octagonal, and t-cost distances. He proved that weighted t-cost distances form a family of metrics and derived an approximation for the Euclidean norm in $\mathbb{Z}^n$. In this note we compare this approximation to two previously proposed Euclidean norm approximations and demonstrate that the empirical average errors given by Mukherjee are significantly optimistic in $\mathbb{R}^n$. We also propose a simple normalization scheme that improves the accuracy of his approximation substantially with respect to both average and maximum relative errors. version:1
arxiv-1206-2058 | Dimension Reduction by Mutual Information Discriminant Analysis | http://arxiv.org/abs/1206.2058 | id:1206.2058 author:Ali Shadvar category:cs.CV cs.IT cs.LG math.IT  published:2012-06-10 summary:In the past few decades, researchers have proposed many discriminant analysis (DA) algorithms for the study of high-dimensional data in a variety of problems. Most DA algorithms for feature extraction are based on transformations that simultaneously maximize the between-class scatter and minimize the withinclass scatter matrices. This paper presents a novel DA algorithm for feature extraction using mutual information (MI). However, it is not always easy to obtain an accurate estimation for high-dimensional MI. In this paper, we propose an efficient method for feature extraction that is based on one-dimensional MI estimations. We will refer to this algorithm as mutual information discriminant analysis (MIDA). The performance of this proposed method was evaluated using UCI databases. The results indicate that MIDA provides robust performance over different data sets with different characteristics and that MIDA always performs better than, or at least comparable to, the best performing algorithms. version:1
arxiv-1206-2010 | Temporal expression normalisation in natural language texts | http://arxiv.org/abs/1206.2010 | id:1206.2010 author:Michele Filannino category:cs.CL cs.IR 68U02 D.3.2  published:2012-06-10 summary:Automatic annotation of temporal expressions is a research challenge of great interest in the field of information extraction. In this report, I describe a novel rule-based architecture, built on top of a pre-existing system, which is able to normalise temporal expressions detected in English texts. Gold standard temporally-annotated resources are limited in size and this makes research difficult. The proposed system outperforms the state-of-the-art systems with respect to TempEval-2 Shared Task (value attribute) and achieves substantially better results with respect to the pre-existing system on top of which it has been developed. I will also introduce a new free corpus consisting of 2822 unique annotated temporal expressions. Both the corpus and the system are freely available on-line. version:1
arxiv-1206-2009 | Developing a model for a text database indexed pedagogically for teaching the Arabic language | http://arxiv.org/abs/1206.2009 | id:1206.2009 author:Asma Boudhief, Mohsen Maraoui, Mounir Zrigui category:cs.CL  published:2012-06-10 summary:In this memory we made the design of an indexing model for Arabic language and adapting standards for describing learning resources used (the LOM and their application profiles) with learning conditions such as levels education of students, their levels of understanding...the pedagogical context with taking into account the repre-sentative elements of the text, text's length,...in particular, we highlight the specificity of the Arabic language which is a complex language, characterized by its flexion, its voyellation and its agglutination. version:1
arxiv-1206-1953 | Improvement of Loadability in Distribution System Using Genetic Algorithm | http://arxiv.org/abs/1206.1953 | id:1206.1953 author:Mojtaba Nouri, Mahdi Bayat Mokhtari, Sohrab Mirsaeidi, Mohammad Reza Miveh category:cs.SY cs.NE  published:2012-06-09 summary:Generally during recent decades due to development of power systems, the methods for delivering electrical energy to consumers, and because of voltage variations is a very important problem, the power plants follow this criteria. The good solution for improving transfer and distribution of electrical power the majority of consumers prefer to use energy near the loads .So small units that are connected to distribution system named "Decentralized Generation" or "Dispersed Generation". Deregulated in power industry and development of renewable energies are the most important factors in developing this type of electricity generation. Today DG has a key role in electrical distribution systems. For example we can refer to improving reliability indices, improvement of stability and reduction of losses in power system. One of the key problems in using DG's, is allocation of these sources in distribution networks. Load ability in distribution systems and its improvement has an effective role in the operation of power systems. However, placement of distributed generation sources in order to improve the distribution system load ability index was not considered, we show DG placement and allocation with genetic algorithm optimization method maximize load ability of power systems .This method implemented on the IEEE Standard bench marks. The results show the effectiveness of the proposed algorithm .Another benefits of DG in selected positions are also studied and compared. version:1
arxiv-1204-6725 | OCT Segmentation Survey and Summary Reviews and a Novel 3D Segmentation Algorithm and a Proof of Concept Implementation | http://arxiv.org/abs/1204.6725 | id:1204.6725 author:Serguei A. Mokhov, Yankui Sun category:cs.CV physics.optics  published:2012-04-30 summary:We overview the existing OCT work, especially the practical aspects of it. We create a novel algorithm for 3D OCT segmentation with the goals of speed and/or accuracy while remaining flexible in the design and implementation for future extensions and improvements. The document at this point is a running draft being iteratively "developed" as a progress report as the work and survey advance. It contains the review and summarization of select OCT works, the design and implementation of the OCTMARF experimentation application and some results. version:2
arxiv-1203-2557 | On the Necessity of Irrelevant Variables | http://arxiv.org/abs/1203.2557 | id:1203.2557 author:David P. Helmbold, Philip M. Long category:cs.LG  published:2012-03-12 summary:This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant. version:3
arxiv-1112-1133 | Multi-timescale Nexting in a Reinforcement Learning Robot | http://arxiv.org/abs/1112.1133 | id:1112.1133 author:Joseph Modayil, Adam White, Richard S. Sutton category:cs.LG cs.RO  published:2011-12-06 summary:The term "nexting" has been used by psychologists to refer to the propensity of people and many other animals to continually predict what will happen next in an immediate, local, and personal sense. The ability to "next" constitutes a basic kind of awareness and knowledge of one's environment. In this paper we present results with a robot that learns to next in real time, predicting thousands of features of the world's state, including all sensory inputs, at timescales from 0.1 to 8 seconds. This was achieved by treating each state feature as a reward-like target and applying temporal-difference methods to learn a corresponding value function with a discount rate corresponding to the timescale. We show that two thousand predictions, each dependent on six thousand state features, can be learned and updated online at better than 10Hz on a laptop computer, using the standard TD(lambda) algorithm with linear function approximation. We show that this approach is efficient enough to be practical, with most of the learning complete within 30 minutes. We also show that a single tile-coded feature representation suffices to accurately predict many different signals at a significant range of timescales. Finally, we show that the accuracy of our learned predictions compares favorably with the optimal off-line solution. version:3
arxiv-1202-1595 | Signal Recovery on Incoherent Manifolds | http://arxiv.org/abs/1202.1595 | id:1202.1595 author:Chinmay Hegde, Richard G. Baraniuk category:cs.IT math.IT stat.ML  published:2012-02-08 summary:Suppose that we observe noisy linear measurements of an unknown signal that can be modeled as the sum of two component signals, each of which arises from a nonlinear sub-manifold of a high dimensional ambient space. We introduce SPIN, a first order projected gradient method to recover the signal components. Despite the nonconvex nature of the recovery problem and the possibility of underdetermined measurements, SPIN provably recovers the signal components, provided that the signal manifolds are incoherent and that the measurement operator satisfies a certain restricted isometry property. SPIN significantly extends the scope of current recovery models and algorithms for low dimensional linear inverse problems and matches (or exceeds) the current state of the art in terms of performance. version:2
arxiv-0704-1409 | Preconditioned Temporal Difference Learning | http://arxiv.org/abs/0704.1409 | id:0704.1409 author:Yao HengShuai category:cs.LG cs.AI  published:2007-04-11 summary:This paper has been withdrawn by the author. This draft is withdrawn for its poor quality in english, unfortunately produced by the author when he was just starting his science route. Look at the ICML version instead: http://icml2008.cs.helsinki.fi/papers/111.pdf version:3
arxiv-1206-1147 | Memory-Efficient Topic Modeling | http://arxiv.org/abs/1206.1147 | id:1206.1147 author:Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao category:cs.LG cs.IR  published:2012-06-06 summary:As one of the simplest probabilistic topic modeling techniques, latent Dirichlet allocation (LDA) has found many important applications in text mining, computer vision and computational biology. Recent training algorithms for LDA can be interpreted within a unified message passing framework. However, message passing requires storing previous messages with a large amount of memory space, increasing linearly with the number of documents or the number of topics. Therefore, the high memory usage is often a major problem for topic modeling of massive corpora containing a large number of topics. To reduce the space complexity, we propose a novel algorithm without storing previous messages for training LDA: tiny belief propagation (TBP). The basic idea of TBP relates the message passing algorithms with the non-negative matrix factorization (NMF) algorithms, which absorb the message updating into the message passing process, and thus avoid storing previous messages. Experimental results on four large data sets confirm that TBP performs comparably well or even better than current state-of-the-art training algorithms for LDA but with a much less memory consumption. TBP can do topic modeling when massive corpora cannot fit in the computer memory, for example, extracting thematic topics from 7 GB PUBMED corpora on a common desktop computer with 2GB memory. version:2
arxiv-1103-0897 | Multiple Kernel Learning: A Unifying Probabilistic Viewpoint | http://arxiv.org/abs/1103.0897 | id:1103.0897 author:Hannes Nickisch, Matthias Seeger category:stat.ML  published:2011-03-04 summary:We present a probabilistic viewpoint to multiple kernel learning unifying well-known regularised risk approaches and recent advances in approximate Bayesian inference relaxations. The framework proposes a general objective function suitable for regression, robust regression and classification that is lower bound of the marginal likelihood and contains many regularised risk approaches as special cases. Furthermore, we derive an efficient and provably convergent optimisation algorithm. version:3
arxiv-1206-1557 | Soil Data Analysis Using Classification Techniques and Soil Attribute Prediction | http://arxiv.org/abs/1206.1557 | id:1206.1557 author:Jay Gholap, Anurag Ingole, Jayesh Gohil, Shailesh Gargade, Vahida Attar category:cs.AI stat.AP stat.ML  published:2012-06-07 summary:Agricultural research has been profited by technical advances such as automation, data mining. Today, data mining is used in a vast areas and many off-the-shelf data mining system products and domain specific data mining application soft wares are available, but data mining in agricultural soil datasets is a relatively a young research field. The large amounts of data that are nowadays virtually harvested along with the crops have to be analyzed and should be used to their full extent. This research aims at analysis of soil dataset using data mining techniques. It focuses on classification of soil using various algorithms available. Another important purpose is to predict untested attributes using regression technique, and implementation of automated soil sample classification. version:1
arxiv-1206-1552 | Performance Analysis of Unsymmetrical trimmed median as detector on image noises and its Fpga implementation | http://arxiv.org/abs/1206.1552 | id:1206.1552 author:K. Vasanth, V. Jawahar Senthil Kumar category:cs.CV  published:2012-06-07 summary:This Paper Analyze the performance of Unsymmetrical trimmed median, which is used as detector for the detection of impulse noise, Gaussian noise and mixed noise is proposed. The proposed algorithm uses a fixed 3x3 window for the increasing noise densities. The pixels in the current window are arranged in sorting order using a improved snake like sorting algorithm with reduced comparator. The processed pixel is checked for the occurrence of outliers, if the absolute difference between processed pixels is greater than fixed threshold. Under high noise densities the processed pixel is also noisy hence the median is checked using the above procedure. if found true then the pixel is considered as noisy hence the corrupted pixel is replaced by the median of the current processing window. If median is also noisy then replace the corrupted pixel with unsymmetrical trimmed median else if the pixel is termed uncorrupted and left unaltered. The proposed algorithm (PA) is tested on varying detail images for various noises. The proposed algorithm effectively removes the high density fixed value impulse noise, low density random valued impulse noise, low density Gaussian noise and lower proportion of mixed noise. The proposed algorithm is targeted on Xc3e5000-5fg900 FPGA using Xilinx 7.1 compiler version which requires less number of slices, optimum speed and low power when compared to the other median finding architectures. version:1
arxiv-1206-1518 | Off-Line Arabic Handwriting Character Recognition Using Word Segmentation | http://arxiv.org/abs/1206.1518 | id:1206.1518 author:Manal A. Abdullah, Lulwah M. Al-Harigy, Hanadi H. Al-Fraidi category:cs.CV  published:2012-06-07 summary:The ultimate aim of handwriting recognition is to make computers able to read and/or authenticate human written texts, with a performance comparable to or even better than that of humans. Reading means that the computer is given a piece of handwriting and it provides the electronic transcription of that (e.g. in ASCII format). Two types of handwriting: on-line and offline. The most important purpose of off-line handwriting recognition is in protection systems and authentication. Arabic Handwriting scripts are much more complicated in comparison to Latin scripts. This paper introduces a simple and novel methodology to authenticate Arabic handwriting characters. Reaching our aim, we built our own character database. The research methodology depends on two stages: The first is character extraction where preprocessing the word and then apply segmentation process to obtain the character. The second is the character recognition by matching the characters comprising the word with the letters in the database. Our results ensure character recognition with 81%. We eliminate FAR by using similarity percent between 45-55%. Our research is coded using MATLAB. version:1
arxiv-1206-1515 | Optimizing Face Recognition Using PCA | http://arxiv.org/abs/1206.1515 | id:1206.1515 author:Manal Abdullah, Majda Wazzan, Sahar Bo-saeed category:cs.CV  published:2012-06-07 summary:Principle Component Analysis PCA is a classical feature extraction and data representation technique widely used in pattern recognition. It is one of the most successful techniques in face recognition. But it has drawback of high computational especially for big size database. This paper conducts a study to optimize the time complexity of PCA (eigenfaces) that does not affects the recognition performance. The authors minimize the participated eigenvectors which consequently decreases the computational time. A comparison is done to compare the differences between the recognition time in the original algorithm and in the enhanced algorithm. The performance of the original and the enhanced proposed algorithm is tested on face94 face database. Experimental results show that the recognition time is reduced by 35% by applying our proposed enhanced algorithm. DET Curves are used to illustrate the experimental results. version:1
arxiv-1206-1443 | On applying Neuro - Computing in E-com Domain | http://arxiv.org/abs/1206.1443 | id:1206.1443 author:Asif Perwej category:cs.NE  published:2012-06-07 summary:Prior studies have generally suggested that Artificial Neural Networks (ANNs) are superior to conventional statistical models in predicting consumer buying behavior. There are, however, contradicting findings which raise question over usefulness of ANNs. This paper discusses development of three neural networks for modeling consumer e-commerce behavior and compares the findings to equivalent logistic regression models. The results showed that ANNs predict e-commerce adoption slightly more accurately than logistic models but this is hardly justifiable given the added complexity. Further, ANNs seem to be highly adaptive, particularly when a small sample is coupled with a large number of nodes in hidden layers which, in turn, limits the neural networks' generalisability. version:1
arxiv-1206-1402 | A New Greedy Algorithm for Multiple Sparse Regression | http://arxiv.org/abs/1206.1402 | id:1206.1402 author:Ali Jalali, Sujay Sanghavi category:stat.ML cs.LG  published:2012-06-07 summary:This paper proposes a new algorithm for multiple sparse regression in high dimensions, where the task is to estimate the support and values of several (typically related) sparse vectors from a few noisy linear measurements. Our algorithm is a "forward-backward" greedy procedure that -- uniquely -- operates on two distinct classes of objects. In particular, we organize our target sparse vectors as a matrix; our algorithm involves iterative addition and removal of both (a) individual elements, and (b) entire rows (corresponding to shared features), of the matrix. Analytically, we establish that our algorithm manages to recover the supports (exactly) and values (approximately) of the sparse vectors, under assumptions similar to existing approaches based on convex optimization. However, our algorithm has a much smaller computational complexity. Perhaps most interestingly, it is seen empirically to require visibly fewer samples. Ours represents the first attempt to extend greedy algorithms to the class of models that can only/best be represented by a combination of component structural assumptions (sparse and group-sparse, in our case). version:1
arxiv-1110-2529 | The Generalization Ability of Online Algorithms for Dependent Data | http://arxiv.org/abs/1110.2529 | id:1110.2529 author:Alekh Agarwal, John C. Duchi category:stat.ML cs.LG math.OC  published:2011-10-11 summary:We study the generalization performance of online learning algorithms trained on samples coming from a dependent source of data. We show that the generalization error of any stable online algorithm concentrates around its regret--an easily computable statistic of the online performance of the algorithm--when the underlying ergodic process is $\beta$- or $\phi$-mixing. We show high probability error bounds assuming the loss function is convex, and we also establish sharp convergence rates and deviation bounds for strongly convex losses and several linear prediction problems such as linear and logistic regression, least-squares SVM, and boosting on dependent data. In addition, our results have straightforward applications to stochastic optimization with dependent data, and our analysis requires only martingale convergence arguments; we need not rely on more powerful statistical tools such as empirical process theory. version:2
arxiv-1206-1309 | Evidence-Based Robust Design of Deflection Actions for Near Earth Objects | http://arxiv.org/abs/1206.1309 | id:1206.1309 author:Federico Zuiani, Massimiliano Vasile, Alison Gibbings category:cs.CE cs.NE math.OC stat.AP  published:2012-06-06 summary:This paper presents a novel approach to the robust design of deflection actions for Near Earth Objects (NEO). In particular, the case of deflection by means of Solar-pumped Laser ablation is studied here in detail. The basic idea behind Laser ablation is that of inducing a sublimation of the NEO surface, which produces a low thrust thereby slowly deviating the asteroid from its initial Earth threatening trajectory. This work investigates the integrated design of the Space-based Laser system and the deflection action generated by laser ablation under uncertainty. The integrated design is formulated as a multi-objective optimisation problem in which the deviation is maximised and the total system mass is minimised. Both the model for the estimation of the thrust produced by surface laser ablation and the spacecraft system model are assumed to be affected by epistemic uncertainties (partial or complete lack of knowledge). Evidence Theory is used to quantify these uncertainties and introduce them in the optimisation process. The propagation of the trajectory of the NEO under the laser-ablation action is performed with a novel approach based on an approximated analytical solution of Gauss' Variational Equations. An example of design of the deflection of asteroid Apophis with a swarm of spacecraft is presented. version:1
arxiv-1206-1305 | MACS: An Agent-Based Memetic Multiobjective Optimization Algorithm Applied to Space Trajectory Design | http://arxiv.org/abs/1206.1305 | id:1206.1305 author:Massimiliano Vasile, Federico Zuiani category:cs.CE cs.NE math.OC  published:2012-06-06 summary:This paper presents an algorithm for multiobjective optimization that blends together a number of heuristics. A population of agents combines heuristics that aim at exploring the search space both globally and in a neighborhood of each agent. These heuristics are complemented with a combination of a local and global archive. The novel agent- based algorithm is tested at first on a set of standard problems and then on three specific problems in space trajectory design. Its performance is compared against a number of state-of-the-art multiobjective optimisation algorithms that use the Pareto dominance as selection criterion: NSGA-II, PAES, MOPSO, MTS. The results demonstrate that the agent-based search can identify parts of the Pareto set that the other algorithms were not able to capture. Furthermore, convergence is statistically better although the variance of the results is in some cases higher. version:1
arxiv-0607062 | Get out the vote: Determining support or opposition from Congressional floor-debate transcripts | http://arxiv.org/abs/cs/0607062 | id:0607062 author:Matt Thomas, Bo Pang, Lillian Lee category:cs.CL cs.SI physics.soc-ph I.2.7  published:2006-07-12 summary:We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation. To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another. We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation. version:3
arxiv-1109-1396 | Gossip Learning with Linear Models on Fully Distributed Data | http://arxiv.org/abs/1109.1396 | id:1109.1396 author:Róbert Ormándi, István Hegedüs, Márk Jelasity category:cs.LG cs.DC  published:2011-09-07 summary:Machine learning over fully distributed data poses an important problem in peer-to-peer (P2P) applications. In this model we have one data record at each network node, but without the possibility to move raw data due to privacy considerations. For example, user profiles, ratings, history, or sensor readings can represent this case. This problem is difficult, because there is no possibility to learn local models, the system model offers almost no guarantees for reliability, yet the communication cost needs to be kept low. Here we propose gossip learning, a generic approach that is based on multiple models taking random walks over the network in parallel, while applying an online learning algorithm to improve themselves, and getting combined via ensemble learning methods. We present an instantiation of this approach for the case of classification with linear models. Our main contribution is an ensemble learning method which---through the continuous combination of the models in the network---implements a virtual weighted voting mechanism over an exponential number of models at practically no extra cost as compared to independent random walks. We prove the convergence of the method theoretically, and perform extensive experiments on benchmark datasets. Our experimental analysis demonstrates the performance and robustness of the proposed approach. version:3
arxiv-1012-2609 | Inverse-Category-Frequency based supervised term weighting scheme for text categorization | http://arxiv.org/abs/1012.2609 | id:1012.2609 author:Deqing Wang, Hui Zhang category:cs.LG cs.AI  published:2010-12-13 summary:Term weighting schemes often dominate the performance of many classifiers, such as kNN, centroid-based classifier and SVMs. The widely used term weighting scheme in text categorization, i.e., tf.idf, is originated from information retrieval (IR) field. The intuition behind idf for text categorization seems less reasonable than IR. In this paper, we introduce inverse category frequency (icf) into term weighting scheme and propose two novel approaches, i.e., tf.icf and icf-based supervised term weighting schemes. The tf.icf adopts icf to substitute idf factor and favors terms occurring in fewer categories, rather than fewer documents. And the icf-based approach combines icf and relevance frequency (rf) to weight terms in a supervised way. Our cross-classifier and cross-corpus experiments have shown that our proposed approaches are superior or comparable to six supervised term weighting schemes and three traditional schemes in terms of macro-F1 and micro-F1. version:4
arxiv-1206-1074 | Memetic Artificial Bee Colony Algorithm for Large-Scale Global Optimization | http://arxiv.org/abs/1206.1074 | id:1206.1074 author:Iztok Fister, Iztok Fister Jr., Janez Brest, Viljem Žumer category:cs.NE cs.AI  published:2012-06-05 summary:Memetic computation (MC) has emerged recently as a new paradigm of efficient algorithms for solving the hardest optimization problems. On the other hand, artificial bees colony (ABC) algorithms demonstrate good performances when solving continuous and combinatorial optimization problems. This study tries to use these technologies under the same roof. As a result, a memetic ABC (MABC) algorithm has been developed that is hybridized with two local search heuristics: the Nelder-Mead algorithm (NMA) and the random walk with direction exploitation (RWDE). The former is attended more towards exploration, while the latter more towards exploitation of the search space. The stochastic adaptation rule was employed in order to control the balancing between exploration and exploitation. This MABC algorithm was applied to a Special suite on Large Scale Continuous Global Optimization at the 2012 IEEE Congress on Evolutionary Computation. The obtained results the MABC are comparable with the results of DECC-G, DECC-G*, and MLCC. version:1
arxiv-1206-1066 | Hedge detection as a lens on framing in the GMO debates: A position paper | http://arxiv.org/abs/1206.1066 | id:1206.1066 author:Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian Danescu-Niculescu-Mizil, Jennifer Spindel category:cs.CL  published:2012-06-05 summary:Understanding the ways in which participants in public discussions frame their arguments is important in understanding how public opinion is formed. In this paper, we adopt the position that it is time for more computationally-oriented research on problems involving framing. In the interests of furthering that goal, we propose the following specific, interesting and, we believe, relatively accessible question: In the controversy regarding the use of genetically-modified organisms (GMOs) in agriculture, do pro- and anti-GMO articles differ in whether they choose to adopt a "scientific" tone? Prior work on the rhetoric and sociology of science suggests that hedging may distinguish popular-science text from text written by professional scientists for their colleagues. We propose a detailed approach to studying whether hedge detection can be used to understanding scientific framing in the GMO debates, and provide corpora to facilitate this study. Some of our preliminary analyses suggest that hedges occur less frequently in scientific discourse than in popular text, a finding that contradicts prior assertions in the literature. We hope that our initial work and data will encourage others to pursue this promising line of inquiry. version:1
arxiv-1206-0985 | Nearly optimal solutions for the Chow Parameters Problem and low-weight approximation of halfspaces | http://arxiv.org/abs/1206.0985 | id:1206.0985 author:Anindya De, Ilias Diakonikolas, Vitaly Feldman, Rocco A. Servedio category:cs.CC cs.DS cs.LG  published:2012-06-05 summary:The \emph{Chow parameters} of a Boolean function $f: \{-1,1\}^n \to \{-1,1\}$ are its $n+1$ degree-0 and degree-1 Fourier coefficients. It has been known since 1961 (Chow, Tannenbaum) that the (exact values of the) Chow parameters of any linear threshold function $f$ uniquely specify $f$ within the space of all Boolean functions, but until recently (O'Donnell and Servedio) nothing was known about efficient algorithms for \emph{reconstructing} $f$ (exactly or approximately) from exact or approximate values of its Chow parameters. We refer to this reconstruction problem as the \emph{Chow Parameters Problem.} Our main result is a new algorithm for the Chow Parameters Problem which, given (sufficiently accurate approximations to) the Chow parameters of any linear threshold function $f$, runs in time $\tilde{O}(n^2)\cdot (1/\eps)^{O(\log^2(1/\eps))}$ and with high probability outputs a representation of an LTF $f'$ that is $\eps$-close to $f$. The only previous algorithm (O'Donnell and Servedio) had running time $\poly(n) \cdot 2^{2^{\tilde{O}(1/\eps^2)}}.$ As a byproduct of our approach, we show that for any linear threshold function $f$ over $\{-1,1\}^n$, there is a linear threshold function $f'$ which is $\eps$-close to $f$ and has all weights that are integers at most $\sqrt{n} \cdot (1/\eps)^{O(\log^2(1/\eps))}$. This significantly improves the best previous result of Diakonikolas and Servedio which gave a $\poly(n) \cdot 2^{\tilde{O}(1/\eps^{2/3})}$ weight bound, and is close to the known lower bound of $\max\{\sqrt{n},$ $(1/\eps)^{\Omega(\log \log (1/\eps))}\}$ (Goldberg, Servedio). Our techniques also yield improved algorithms for related problems in learning theory. version:1
arxiv-1109-4540 | Manifold estimation and singular deconvolution under Hausdorff loss | http://arxiv.org/abs/1109.4540 | id:1109.4540 author:Christopher R. Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH  published:2011-09-21 summary:We find lower and upper bounds for the risk of estimating a manifold in Hausdorff distance under several models. We also show that there are close connections between manifold estimation and the problem of deconvolving a singular measure. version:2
arxiv-1206-0855 | A Mixed Observability Markov Decision Process Model for Musical Pitch | http://arxiv.org/abs/1206.0855 | id:1206.0855 author:Pouyan Rafiei Fard, Keyvan Yahya category:cs.AI cs.LG  published:2012-06-05 summary:Partially observable Markov decision processes have been widely used to provide models for real-world decision making problems. In this paper, we will provide a method in which a slightly different version of them called Mixed observability Markov decision process, MOMDP, is going to join with our problem. Basically, we aim at offering a behavioural model for interaction of intelligent agents with musical pitch environment and we will show that how MOMDP can shed some light on building up a decision making model for musical pitch conveniently. version:1
arxiv-0911-2919 | Kullback-Leibler aggregation and misspecified generalized linear models | http://arxiv.org/abs/0911.2919 | id:0911.2919 author:Philippe Rigollet category:stat.ML math.ST stat.TH  published:2009-11-16 summary:In a regression setup with deterministic design, we study the pure aggregation problem and introduce a natural extension from the Gaussian distribution to distributions in the exponential family. While this extension bears strong connections with generalized linear models, it does not require identifiability of the parameter or even that the model on the systematic component is true. It is shown that this problem can be solved by constrained and/or penalized likelihood maximization and we derive sharp oracle inequalities that hold both in expectation and with high probability. Finally all the bounds are proved to be optimal in a minimax sense. version:5
arxiv-1206-0773 | Changepoint Detection over Graphs with the Spectral Scan Statistic | http://arxiv.org/abs/1206.0773 | id:1206.0773 author:James Sharpnack, Alessandro Rinaldo, Aarti Singh category:math.ST cs.IT math.IT stat.ML stat.TH  published:2012-06-04 summary:We consider the change-point detection problem of deciding, based on noisy measurements, whether an unknown signal over a given graph is constant or is instead piecewise constant over two connected induced subgraphs of relatively low cut size. We analyze the corresponding generalized likelihood ratio (GLR) statistics and relate it to the problem of finding a sparsest cut in a graph. We develop a tractable relaxation of the GLR statistic based on the combinatorial Laplacian of the graph, which we call the spectral scan statistic, and analyze its properties. We show how its performance as a testing procedure depends directly on the spectrum of the graph, and use this result to explicitly derive its asymptotic properties on few significant graph topologies. Finally, we demonstrate both theoretically and by simulations that the spectral scan statistic can outperform naive testing procedures based on edge thresholding and $\chi^2$ testing. version:1
arxiv-1206-0771 | Topological graph clustering with thin position | http://arxiv.org/abs/1206.0771 | id:1206.0771 author:Jesse Johnson category:math.GT cs.LG stat.ML  published:2012-06-04 summary:A clustering algorithm partitions a set of data points into smaller sets (clusters) such that each subset is more tightly packed than the whole. Many approaches to clustering translate the vector data into a graph with edges reflecting a distance or similarity metric on the points, then look for highly connected subgraphs. We introduce such an algorithm based on ideas borrowed from the topological notion of thin position for knots and 3-dimensional manifolds. version:1
arxiv-1206-0730 | Theoretical foundation for CMA-ES from information geometric perspective | http://arxiv.org/abs/1206.0730 | id:1206.0730 author:Youhei Akimoto, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi category:cs.NE  published:2012-06-04 summary:This paper explores the theoretical basis of the covariance matrix adaptation evolution strategy (CMA-ES) from the information geometry viewpoint. To establish a theoretical foundation for the CMA-ES, we focus on a geometric structure of a Riemannian manifold of probability distributions equipped with the Fisher metric. We define a function on the manifold which is the expectation of fitness over the sampling distribution, and regard the goal of update of the parameters of sampling distribution in the CMA-ES as maximization of the expected fitness. We investigate the steepest ascent learning for the expected fitness maximization, where the steepest ascent direction is given by the natural gradient, which is the product of the inverse of the Fisher information matrix and the conventional gradient of the function. Our first result is that we can obtain under some types of parameterization of multivariate normal distribution the natural gradient of the expected fitness without the need for inversion of the Fisher information matrix. We find that the update of the distribution parameters in the CMA-ES is the same as natural gradient learning for expected fitness maximization. Our second result is that we derive the range of learning rates such that a step in the direction of the exact natural gradient improves the parameters in the expected fitness. We see from the close relation between the CMA-ES and natural gradient learning that the default setting of learning rates in the CMA-ES seems suitable in terms of monotone improvement in expected fitness. Then, we discuss the relation to the expectation-maximization framework and provide an information geometric interpretation of the CMA-ES. version:1
arxiv-1206-0663 | Multi-Sparse Signal Recovery for Compressive Sensing | http://arxiv.org/abs/1206.0663 | id:1206.0663 author:Yipeng Liu, Ivan Gligorijevic, Vladimir Matic, Maarten De Vos, Sabine Van Huffel category:cs.IT cs.SY math.IT math.OC stat.ML  published:2012-06-04 summary:Signal recovery is one of the key techniques of Compressive sensing (CS). It reconstructs the original signal from the linear sub-Nyquist measurements. Classical methods exploit the sparsity in one domain to formulate the L0 norm optimization. Recent investigation shows that some signals are sparse in multiple domains. To further improve the signal reconstruction performance, we can exploit this multi-sparsity to generate a new convex programming model. The latter is formulated with multiple sparsity constraints in multiple domains and the linear measurement fitting constraint. It improves signal recovery performance by additional a priori information. Since some EMG signals exhibit sparsity both in time and frequency domains, we take them as example in numerical experiments. Results show that the newly proposed method achieves better performance for multi-sparse signals. version:1
arxiv-1110-4784 | Web search queries can predict stock market volumes | http://arxiv.org/abs/1110.4784 | id:1110.4784 author:Ilaria Bordino, Stefano Battiston, Guido Caldarelli, Matthieu Cristelli, Antti Ukkonen, Ingmar Weber category:q-fin.ST cs.LG physics.soc-ph  published:2011-10-21 summary:We live in a computerized and networked society where many of our actions leave a digital trace and affect other people's actions. This has lead to the emergence of a new data-driven research field: mathematical methods of computer science, statistical physics and sociometry provide insights on a wide range of disciplines ranging from social science to human mobility. A recent important discovery is that query volumes (i.e., the number of requests submitted by users to search engines on the www) can be used to track and, in some cases, to anticipate the dynamics of social phenomena. Successful exemples include unemployment levels, car and home sales, and epidemics spreading. Few recent works applied this approach to stock prices and market sentiment. However, it remains unclear if trends in financial markets can be anticipated by the collective wisdom of on-line users on the web. Here we show that trading volumes of stocks traded in NASDAQ-100 are correlated with the volumes of queries related to the same stocks. In particular, query volumes anticipate in many cases peaks of trading by one day or more. Our analysis is carried out on a unique dataset of queries, submitted to an important web search engine, which enable us to investigate also the user behavior. We show that the query volume dynamics emerges from the collective but seemingly uncoordinated activity of many users. These findings contribute to the debate on the identification of early warnings of financial systemic risk, based on the activity of users of the www. version:3
arxiv-1204-6325 | CELL: Connecting Everyday Life in an archipeLago | http://arxiv.org/abs/1204.6325 | id:1204.6325 author:Konstantinos Chorianopoulos, Vassiliki Tsaknaki category:cs.HC cs.LG  published:2012-04-27 summary:We explore the design of a seamless broadcast communication system that brings together the distributed community of remote secondary education schools. In contrast to higher education, primary and secondary education establishments should remain distributed, in order to maintain a balance of urban and rural life in the developing and the developed world. We plan to deploy an ambient and social interactive TV platform (physical installation, authoring tools, interactive content) that supports social communication in a positive way. In particular, we present the physical design and the conceptual model of the system. version:2
arxiv-1206-0393 | Greedy expansions in convex optimization | http://arxiv.org/abs/1206.0393 | id:1206.0393 author:V. N. Temlyakov category:stat.ML math.OC 41A65  published:2012-06-02 summary:This paper is a follow up to the previous author's paper on convex optimization. In that paper we began the process of adjusting greedy-type algorithms from nonlinear approximation for finding sparse solutions of convex optimization problems. We modified there three the most popular in nonlinear approximation in Banach spaces greedy algorithms -- Weak Chebyshev Greedy Algorithm, Weak Greedy Algorithm with Free Relaxation and Weak Relaxed Greedy Algorithm -- for solving convex optimization problems. We continue to study sparse approximate solutions to convex optimization problems. It is known that in many engineering applications researchers are interested in an approximate solution of an optimization problem as a linear combination of elements from a given system of elements. There is an increasing interest in building such sparse approximate solutions using different greedy-type algorithms. In this paper we concentrate on greedy algorithms that provide expansions, which means that the approximant at the $m$th iteration is equal to the sum of the approximant from the previous iteration ($(m-1)$th iteration) and one element from the dictionary with an appropriate coefficient. The problem of greedy expansions of elements of a Banach space is well studied in nonlinear approximation theory. At a first glance the setting of a problem of expansion of a given element and the setting of the problem of expansion in an optimization problem are very different. However, it turns out that the same technique can be used for solving both problems. We show how the technique developed in nonlinear approximation theory, in particular, the greedy expansions technique can be adjusted for finding a sparse solution of an optimization problem given by an expansion with respect to a given dictionary. version:1
arxiv-1206-0392 | Greedy approximation in convex optimization | http://arxiv.org/abs/1206.0392 | id:1206.0392 author:V. N. Temlyakov category:stat.ML math.OC 41A65  published:2012-06-02 summary:We study sparse approximate solutions to convex optimization problems. It is known that in many engineering applications researchers are interested in an approximate solution of an optimization problem as a linear combination of elements from a given system of elements. There is an increasing interest in building such sparse approximate solutions using different greedy-type algorithms. The problem of approximation of a given element of a Banach space by linear combinations of elements from a given system (dictionary) is well studied in nonlinear approximation theory. At a first glance the settings of approximation and optimization problems are very different. In the approximation problem an element is given and our task is to find a sparse approximation of it. In optimization theory an energy function is given and we should find an approximate sparse solution to the minimization problem. It turns out that the same technique can be used for solving both problems. We show how the technique developed in nonlinear approximation theory, in particular, the greedy approximation technique can be adjusted for finding a sparse solution of an optimization problem. version:1
arxiv-1206-0381 | UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser Approach | http://arxiv.org/abs/1206.0381 | id:1206.0381 author:Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear category:cs.CL  published:2012-06-02 summary:Universal Networking Language (UNL) is a declarative formal language that is used to represent semantic data extracted from natural language texts. This paper presents a novel approach to converting Bangla natural language text into UNL using a method known as Predicate Preserving Parser (PPP) technique. PPP performs morphological, syntactic and semantic, and lexical analysis of text synchronously. This analysis produces a semantic-net like structure represented using UNL. We demonstrate how Bangla texts are analyzed following the PPP technique to produce UNL documents which can then be translated into any other suitable natural language facilitating the opportunity to develop a universal language translation method via UNL. version:1
arxiv-1206-0377 | Automated Word Puzzle Generation via Topic Dictionaries | http://arxiv.org/abs/1206.0377 | id:1206.0377 author:Balazs Pinter, Gyula Voros, Zoltan Szabo, Andras Lorincz category:cs.CL math.CO 68T50  15A23 I.2.7; G.2.3; G.1.2  published:2012-06-02 summary:We propose a general method for automated word puzzle generation. Contrary to previous approaches in this novel field, the presented method does not rely on highly structured datasets obtained with serious human annotation effort: it only needs an unstructured and unannotated corpus (i.e., document collection) as input. The method builds upon two additional pillars: (i) a topic model, which induces a topic dictionary from the input corpus (examples include e.g., latent semantic analysis, group-structured dictionaries or latent Dirichlet allocation), and (ii) a semantic similarity measure of word pairs. Our method can (i) generate automatically a large number of proper word puzzles of different types, including the odd one out, choose the related word and separate the topics puzzle. (ii) It can easily create domain-specific puzzles by replacing the corpus component. (iii) It is also capable of automatically generating puzzles with parameterizable levels of difficulty suitable for, e.g., beginners or intermediate learners. version:1
arxiv-1206-0335 | A Route Confidence Evaluation Method for Reliable Hierarchical Text Categorization | http://arxiv.org/abs/1206.0335 | id:1206.0335 author:Nima Hatami, Camelia Chira, Giuliano Armano category:cs.IR cs.LG  published:2012-06-02 summary:Hierarchical Text Categorization (HTC) is becoming increasingly important with the rapidly growing amount of text data available in the World Wide Web. Among the different strategies proposed to cope with HTC, the Local Classifier per Node (LCN) approach attains good performance by mirroring the underlying class hierarchy while enforcing a top-down strategy in the testing step. However, the problem of embedding hierarchical information (parent-child relationship) to improve the performance of HTC systems still remains open. A confidence evaluation method for a selected route in the hierarchy is proposed to evaluate the reliability of the final candidate labels in an HTC system. In order to take into account the information embedded in the hierarchy, weight factors are used to take into account the importance of each level. An acceptance/rejection strategy in the top-down decision making process is proposed, which improves the overall categorization accuracy by rejecting a few percentage of samples, i.e., those with low reliability score. Experimental results on the Reuters benchmark dataset (RCV1- v2) confirm the effectiveness of the proposed method, compared to other state-of-the art HTC methods. version:1
arxiv-1206-0333 | Sparse Trace Norm Regularization | http://arxiv.org/abs/1206.0333 | id:1206.0333 author:Jianhui Chen, Jieping Ye category:cs.LG stat.ML  published:2012-06-02 summary:We study the problem of estimating multiple predictive functions from a dictionary of basis functions in the nonparametric regression setting. Our estimation scheme assumes that each predictive function can be estimated in the form of a linear combination of the basis functions. By assuming that the coefficient matrix admits a sparse low-rank structure, we formulate the function estimation problem as a convex program regularized by the trace norm and the $\ell_1$-norm simultaneously. We propose to solve the convex program using the accelerated gradient (AG) method and the alternating direction method of multipliers (ADMM) respectively; we also develop efficient algorithms to solve the key components in both AG and ADMM. In addition, we conduct theoretical analysis on the proposed function estimation scheme: we derive a key property of the optimal solution to the convex program; based on an assumption on the basis functions, we establish a performance bound of the proposed function estimation scheme (via the composite regularization). Simulation studies demonstrate the effectiveness and efficiency of the proposed algorithms. version:1
arxiv-1206-0238 | Rapid Feature Extraction for Optical Character Recognition | http://arxiv.org/abs/1206.0238 | id:1206.0238 author:M. Zahid Hossain, M. Ashraful Amin, Hong Yan category:cs.CV I.5.2; I.7.5  published:2012-06-01 summary:Feature extraction is one of the fundamental problems of character recognition. The performance of character recognition system is depends on proper feature extraction and correct classifier selection. In this article, a rapid feature extraction method is proposed and named as Celled Projection (CP) that compute the projection of each section formed through partitioning an image. The recognition performance of the proposed method is compared with other widely used feature extraction methods that are intensively studied for many different scripts in literature. The experiments have been conducted using Bangla handwritten numerals along with three different well known classifiers which demonstrate comparable results including 94.12% recognition accuracy using celled projection. version:1
arxiv-1205-6432 | Multiclass Learning Approaches: A Theoretical Comparison with Implications | http://arxiv.org/abs/1205.6432 | id:1205.6432 author:Amit Daniely, Sivan Sabato, Shai Shalev Shwartz category:cs.LG  published:2012-05-29 summary:We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error. version:2
arxiv-1206-0111 | OpenGM: A C++ Library for Discrete Graphical Models | http://arxiv.org/abs/1206.0111 | id:1206.0111 author:Bjoern Andres, Thorsten Beier, Joerg H. Kappes category:cs.AI cs.MS stat.ML  published:2012-06-01 summary:OpenGM is a C++ template library for defining discrete graphical models and performing inference on these models, using a wide range of state-of-the-art algorithms. No restrictions are imposed on the factor graph to allow for higher-order factors and arbitrary neighborhood structures. Large models with repetitive structure are handled efficiently because (i) functions that occur repeatedly need to be stored only once, and (ii) distinct functions can be implemented differently, using different encodings alongside each other in the same model. Several parametric functions (e.g. metrics), sparse and dense value tables are provided and so is an interface for custom C++ code. Algorithms are separated by design from the representation of graphical models and are easily exchangeable. OpenGM, its algorithms, HDF5 file format and command line tools are modular and extendible. version:1
arxiv-1206-0042 | Language Acquisition in Computers | http://arxiv.org/abs/1206.0042 | id:1206.0042 author:Megan Belzner, Sean Colin-Ellerin, Jorge H. Roman category:cs.CL I.2.6; I.2.7  published:2012-05-31 summary:This project explores the nature of language acquisition in computers, guided by techniques similar to those used in children. While existing natural language processing methods are limited in scope and understanding, our system aims to gain an understanding of language from first principles and hence minimal initial input. The first portion of our system was implemented in Java and is focused on understanding the morphology of language using bigrams. We use frequency distributions and differences between them to define and distinguish languages. English and French texts were analyzed to determine a difference threshold of 55 before the texts are considered to be in different languages, and this threshold was verified using Spanish texts. The second portion of our system focuses on gaining an understanding of the syntax of a language using a recursive method. The program uses one of two possible methods to analyze given sentences based on either sentence patterns or surrounding words. Both methods have been implemented in C++. The program is able to understand the structure of simple sentences and learn new words. In addition, we have provided some suggestions regarding future work and potential extensions of the existing program. version:1
arxiv-1205-7009 | Oriented and Degree-generated Block Models: Generating and Inferring Communities with Inhomogeneous Degree Distributions | http://arxiv.org/abs/1205.7009 | id:1205.7009 author:Yaojia Zhu, Xiaoran Yan, Cristopher Moore category:cs.SI cond-mat.stat-mech physics.soc-ph stat.ML  published:2012-05-31 summary:The stochastic block model is a powerful tool for inferring community structure from network topology. However, it predicts a Poisson degree distribution within each community, while most real-world networks have a heavy-tailed degree distribution. The degree-corrected block model can accommodate arbitrary degree distributions within communities. But since it takes the vertex degrees as parameters rather than generating them, it cannot use them to help it classify the vertices, and its natural generalization to directed graphs cannot even use the orientations of the edges. In this paper, we present variants of the block model with the best of both worlds: they can use vertex degrees and edge orientations in the classification process, while tolerating heavy-tailed degree distributions within communities. We show that for some networks, including synthetic networks and networks of word adjacencies in English text, these new block models achieve a higher accuracy than either standard or degree-corrected block models. version:1
arxiv-1205-6849 | Beyond $\ell_1$-norm minimization for sparse signal recovery | http://arxiv.org/abs/1205.6849 | id:1205.6849 author:Hassan Mansour category:cs.IT cs.LG math.IT  published:2012-05-30 summary:Sparse signal recovery has been dominated by the basis pursuit denoise (BPDN) problem formulation for over a decade. In this paper, we propose an algorithm that outperforms BPDN in finding sparse solutions to underdetermined linear systems of equations at no additional computational cost. Our algorithm, called WSPGL1, is a modification of the spectral projected gradient for $\ell_1$ minimization (SPGL1) algorithm in which the sequence of LASSO subproblems are replaced by a sequence of weighted LASSO subproblems with constant weights applied to a support estimate. The support estimate is derived from the data and is updated at every iteration. The algorithm also modifies the Pareto curve at every iteration to reflect the new weighted $\ell_1$ minimization problem that is being solved. We demonstrate through extensive simulations that the sparse recovery performance of our algorithm is superior to that of $\ell_1$ minimization and approaches the recovery performance of iterative re-weighted $\ell_1$ (IRWL1) minimization of Cand{\`e}s, Wakin, and Boyd, although it does not match it in general. Moreover, our algorithm has the computational cost of a single BPDN problem. version:1
arxiv-1205-6745 | Fingerprint Gender Classification using Wavelet Transform and Singular Value Decomposition | http://arxiv.org/abs/1205.6745 | id:1205.6745 author:P Gnanasivam, Dr. S Muttan category:cs.CV  published:2012-05-30 summary:A novel method of gender Classification from fingerprint is proposed based on discrete wavelet transform (DWT) and singular value decomposition (SVD). The classification is achieved by extracting the energy computed from all the sub-bands of DWT combined with the spatial features of non-zero singular values obtained from the SVD of fingerprint images. K nearest neighbor (KNN) used as a classifier. This method is experimented with the internal database of 3570 fingerprints finger prints in which 1980 were male fingerprints and 1590 were female fingerprints. Finger-wise gender classification is achieved which is 94.32% for the left hand little fingers of female persons and 95.46% for the left hand index finger of male persons. Gender classification for any finger of male persons tested is attained as 91.67% and 84.69% for female persons respectively. Overall classification rate is 88.28% has been achieved. version:1
arxiv-1205-6605 | Template-Cut: A Pattern-Based Segmentation Paradigm | http://arxiv.org/abs/1205.6605 | id:1205.6605 author:Jan Egger, Bernd Freisleben, Christopher Nimsky, Tina Kapur category:cs.CV  published:2012-05-30 summary:We present a scale-invariant, template-based segmentation paradigm that sets up a graph and performs a graph cut to separate an object from the background. Typically graph-based schemes distribute the nodes of the graph uniformly and equidistantly on the image, and use a regularizer to bias the cut towards a particular shape. The strategy of uniform and equidistant nodes does not allow the cut to prefer more complex structures, especially when areas of the object are indistinguishable from the background. We propose a solution by introducing the concept of a "template shape" of the target object in which the nodes are sampled non-uniformly and non-equidistantly on the image. We evaluate it on 2D-images where the object's textures and backgrounds are similar, and large areas of the object have the same gray level appearance as the background. We also evaluate it in 3D on 60 brain tumor datasets for neurosurgical planning purposes. version:1
arxiv-1205-6572 | An Unsupervised Dynamic Image Segmentation using Fuzzy Hopfield Neural Network based Genetic Algorithm | http://arxiv.org/abs/1205.6572 | id:1205.6572 author:Amiya Halder, Soumajit Pramanik category:cs.CV  published:2012-05-30 summary:This paper proposes a Genetic Algorithm based segmentation method that can automatically segment gray-scale images. The proposed method mainly consists of spatial unsupervised grayscale image segmentation that divides an image into regions. The aim of this algorithm is to produce precise segmentation of images using intensity information along with neighborhood relationships. In this paper, Fuzzy Hopfield Neural Network (FHNN) clustering helps in generating the population of Genetic algorithm which there by automatically segments the image. This technique is a powerful method for image segmentation and works for both single and multiple-feature data with spatial information. Validity index has been utilized for introducing a robust technique for finding the optimum number of components in an image. Experimental results shown that the algorithm generates good quality segmented image. version:1
arxiv-1205-6544 | A Brief Summary of Dictionary Learning Based Approach for Classification (revised) | http://arxiv.org/abs/1205.6544 | id:1205.6544 author:Shu Kong, Donghui Wang category:cs.CV cs.LG  published:2012-05-30 summary:This note presents some representative methods which are based on dictionary learning (DL) for classification. We do not review the sophisticated methods or frameworks that involve DL for classification, such as online DL and spatial pyramid matching (SPM), but rather, we concentrate on the direct DL-based classification methods. Here, the "so-called direct DL-based method" is the approach directly deals with DL framework by adding some meaningful penalty terms. By listing some representative methods, we can roughly divide them into two categories, i.e. (1) directly making the dictionary discriminative and (2) forcing the sparse coefficients discriminative to push the discrimination power of the dictionary. From this taxonomy, we can expect some extensions of them as future researches. version:1
arxiv-1205-6391 | A Brief Summary of Dictionary Learning Based Approach for Classification | http://arxiv.org/abs/1205.6391 | id:1205.6391 author:Kong Shu, Wang Donghui category:cs.CV  published:2012-05-29 summary:This note presents some representative methods which are based on dictionary learning (DL) for classification. We do not review the sophisticated methods or frameworks that involve DL for classification, such as online DL and spatial pyramid matching (SPM), but rather, we concentrate on the direct DL-based classification methods. Here, the "so-called direct DL-based method" is the approach directly deals with DL framework by adding some meaningful penalty terms. By listing some representative methods, we can roughly divide them into two categories, i.e. (1) directly making the dictionary discriminative and (2) forcing the sparse coefficients discriminative to push the discrimination power of the dictionary. From this taxonomy, we can expect some extensions of them as future researches. version:2
arxiv-1205-6523 | Finding Important Genes from High-Dimensional Data: An Appraisal of Statistical Tests and Machine-Learning Approaches | http://arxiv.org/abs/1205.6523 | id:1205.6523 author:Chamont Wang, Jana Gevertz, Chaur-Chin Chen, Leonardo Auslender category:stat.ML cs.LG q-bio.QM  published:2012-05-30 summary:Over the past decades, statisticians and machine-learning researchers have developed literally thousands of new tools for the reduction of high-dimensional data in order to identify the variables most responsible for a particular trait. These tools have applications in a plethora of settings, including data analysis in the fields of business, education, forensics, and biology (such as microarray, proteomics, brain imaging), to name a few. In the present work, we focus our investigation on the limitations and potential misuses of certain tools in the analysis of the benchmark colon cancer data (2,000 variables; Alon et al., 1999) and the prostate cancer data (6,033 variables; Efron, 2010, 2008). Our analysis demonstrates that models that produce 100% accuracy measures often select different sets of genes and cannot stand the scrutiny of parameter estimates and model stability. Furthermore, we created a host of simulation datasets and "artificial diseases" to evaluate the reliability of commonly used statistical and data mining tools. We found that certain widely used models can classify the data with 100% accuracy without using any of the variables responsible for the disease. With moderate sample size and suitable pre-screening, stochastic gradient boosting will be shown to be a superior model for gene selection and variable screening from high-dimensional datasets. version:1
arxiv-1205-2334 | Sparse Approximation via Penalty Decomposition Methods | http://arxiv.org/abs/1205.2334 | id:1205.2334 author:Zhaosong Lu, Yong Zhang category:cs.LG math.OC stat.CO stat.ML  published:2012-05-10 summary:In this paper we consider sparse approximation problems, that is, general $l_0$ minimization problems with the $l_0$-"norm" of a vector being a part of constraints or objective function. In particular, we first study the first-order optimality conditions for these problems. We then propose penalty decomposition (PD) methods for solving them in which a sequence of penalty subproblems are solved by a block coordinate descent (BCD) method. Under some suitable assumptions, we establish that any accumulation point of the sequence generated by the PD methods satisfies the first-order optimality conditions of the problems. Furthermore, for the problems in which the $l_0$ part is the only nonconvex part, we show that such an accumulation point is a local minimizer of the problems. In addition, we show that any accumulation point of the sequence generated by the BCD method is a saddle point of the penalty subproblem. Moreover, for the problems in which the $l_0$ part is the only nonconvex part, we establish that such an accumulation point is a local minimizer of the penalty subproblem. Finally, we test the performance of our PD methods by applying them to sparse logistic regression, sparse inverse covariance selection, and compressed sensing problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality and/or speed. version:2
arxiv-1008-5373 | Penalty Decomposition Methods for Rank Minimization | http://arxiv.org/abs/1008.5373 | id:1008.5373 author:Zhaosong Lu, Yong Zhang category:math.OC cs.LG cs.NA cs.SY q-fin.CP q-fin.ST  published:2010-08-31 summary:In this paper we consider general rank minimization problems with rank appearing in either objective function or constraint. We first establish that a class of special rank minimization problems has closed-form solutions. Using this result, we then propose penalty decomposition methods for general rank minimization problems in which each subproblem is solved by a block coordinate descend method. Under some suitable assumptions, we show that any accumulation point of the sequence generated by the penalty decomposition methods satisfies the first-order optimality conditions of a nonlinear reformulation of the problems. Finally, we test the performance of our methods by applying them to the matrix completion and nearest low-rank correlation matrix problems. The computational results demonstrate that our methods are generally comparable or superior to the existing methods in terms of solution quality. version:4
arxiv-1205-6396 | Effective Listings of Function Stop words for Twitter | http://arxiv.org/abs/1205.6396 | id:1205.6396 author:Murphy Choy category:cs.IR cs.CL  published:2012-05-29 summary:Many words in documents recur very frequently but are essentially meaningless as they are used to join words together in a sentence. It is commonly understood that stop words do not contribute to the context or content of textual documents. Due to their high frequency of occurrence, their presence in text mining presents an obstacle to the understanding of the content in the documents. To eliminate the bias effects, most text mining software or approaches make use of stop words list to identify and remove those words. However, the development of such top words list is difficult and inconsistent between textual sources. This problem is further aggravated by sources such as Twitter which are highly repetitive or similar in nature. In this paper, we will be examining the original work using term frequency, inverse document frequency and term adjacency for developing a stop words list for the Twitter data source. We propose a new technique using combinatorial values as an alternative measure to effectively list out stop words. version:1
arxiv-1104-5617 | Learning high-dimensional directed acyclic graphs with latent and selection variables | http://arxiv.org/abs/1104.5617 | id:1104.5617 author:Diego Colombo, Marloes H. Maathuis, Markus Kalisch, Thomas S. Richardson category:stat.ME cs.LG math.ST stat.TH  published:2011-04-29 summary:We consider the problem of learning causal information between random variables in directed acyclic graphs (DAGs) when allowing arbitrarily many latent and selection variables. The FCI (Fast Causal Inference) algorithm has been explicitly designed to infer conditional independence and causal information in such settings. However, FCI is computationally infeasible for large graphs. We therefore propose the new RFCI algorithm, which is much faster than FCI. In some situations the output of RFCI is slightly less informative, in particular with respect to conditional independence information. However, we prove that any causal information in the output of RFCI is correct in the asymptotic limit. We also define a class of graphs on which the outputs of FCI and RFCI are identical. We prove consistency of FCI and RFCI in sparse high-dimensional settings, and demonstrate in simulations that the estimation performances of the algorithms are very similar. All software is implemented in the R-package pcalg. version:3
arxiv-1203-0631 | Checking Tests for Read-Once Functions over Arbitrary Bases | http://arxiv.org/abs/1203.0631 | id:1203.0631 author:Dmitry V. Chistikov category:cs.DM cs.CC cs.LG  published:2012-03-03 summary:A Boolean function is called read-once over a basis B if it can be expressed by a formula over B where no variable appears more than once. A checking test for a read-once function f over B depending on all its variables is a set of input vectors distinguishing f from all other read-once functions of the same variables. We show that every read-once function f over B has a checking test containing O(n^l) vectors, where n is the number of relevant variables of f and l is the largest arity of functions in B. For some functions, this bound cannot be improved by more than a constant factor. The employed technique involves reconstructing f from its l-variable projections and provides a stronger form of Kuznetsov's classic theorem on read-once representations. version:3
arxiv-1205-6154 | Potentials and Limits of Super-Resolution Algorithms and Signal Reconstruction from Sparse Data | http://arxiv.org/abs/1205.6154 | id:1205.6154 author:Gil Shabat category:physics.optics cs.CV math-ph math.MP  published:2012-05-28 summary:A common distortion in videos is image instability in the form of chaotic (global and local displacements). Those instabilities can be used to enhance image resolution by using subpixel elastic registration. In this work, we investigate the performance of such methods over the ability to improve the resolution by accumulating several frames. The second part of this work deals with reconstruction of discrete signals from a subset of samples under different basis functions such as DFT, Haar, Walsh, Daubechies wavelets and CT (Radon) projections. version:1
arxiv-1102-4399 | Semi-supervised logistic discrimination for functional data | http://arxiv.org/abs/1102.4399 | id:1102.4399 author:Shuichi Kawano, Sadanori Konishi category:stat.ME stat.ML 62H30  62G05  68T10  published:2011-02-22 summary:Multi-class classification methods based on both labeled and unlabeled functional data sets are discussed. We present a semi-supervised logistic model for classification in the context of functional data analysis. Unknown parameters in our proposed model are estimated by regularization with the help of EM algorithm. A crucial point in the modeling procedure is the choice of a regularization parameter involved in the semi-supervised functional logistic model. In order to select the adjusted parameter, we introduce model selection criteria from information-theoretic and Bayesian viewpoints. Monte Carlo simulations and a real data analysis are given to examine the effectiveness of our proposed modeling strategy. version:3
arxiv-1110-4123 | Positive words carry less information than negative words | http://arxiv.org/abs/1110.4123 | id:1110.4123 author:David Garcia, Antonios Garas, Frank Schweitzer category:cs.CL cs.IR physics.soc-ph  published:2011-10-18 summary:We show that the frequency of word use is not only determined by the word length \cite{Zipf1935} and the average information content \cite{Piantadosi2011}, but also by its emotional content. We have analyzed three established lexica of affective word usage in English, German, and Spanish, to verify that these lexica have a neutral, unbiased, emotional content. Taking into account the frequency of word usage, we find that words with a positive emotional content are more frequently used. This lends support to Pollyanna hypothesis \cite{Boucher1969} that there should be a positive bias in human expression. We also find that negative words contain more information than positive words, as the informativeness of a word increases uniformly with its valence decrease. Our findings support earlier conjectures about (i) the relation between word frequency and information content, and (ii) the impact of positive emotions on communication and social links. version:4
arxiv-1204-3514 | Distributed Learning, Communication Complexity and Privacy | http://arxiv.org/abs/1204.3514 | id:1204.3514 author:Maria-Florina Balcan, Avrim Blum, Shai Fine, Yishay Mansour category:cs.LG cs.DS F.2.2; I.2.6  published:2012-04-16 summary:We consider the problem of PAC-learning from distributed data and analyze fundamental communication complexity questions involved. We provide general upper and lower bounds on the amount of communication needed to learn well, showing that in addition to VC-dimension and covering number, quantities such as the teaching-dimension and mistake-bound of a class play an important role. We also present tight results for a number of common concept classes including conjunctions, parity functions, and decision lists. For linear separators, we show that for non-concentrated distributions, we can use a version of the Perceptron algorithm to learn with much less communication than the number of updates given by the usual margin bound. We also show how boosting can be performed in a generic manner in the distributed setting to achieve communication with only logarithmic dependence on 1/epsilon for any concept class, and demonstrate how recent work on agnostic learning from class-conditional queries can be used to achieve low communication in agnostic settings as well. We additionally present an analysis of privacy, considering both differential privacy and a notion of distributional privacy that is especially appealing in this context. version:3
arxiv-1205-4234 | Visualization of features of a series of measurements with one-dimensional cellular structure | http://arxiv.org/abs/1205.4234 | id:1205.4234 author:D. V. Lande category:cs.LG 68R  published:2012-05-19 summary:This paper describes the method of visualization of periodic constituents and instability areas in series of measurements, being based on the algorithm of smoothing out and concept of one-dimensional cellular automata. A method can be used at the analysis of temporal series, related to the volumes of thematic publications in web-space. version:2
arxiv-1205-4159 | Theory of Dependent Hierarchical Normalized Random Measures | http://arxiv.org/abs/1205.4159 | id:1205.4159 author:Changyou Chen, Wray Buntine, Nan Ding category:cs.LG math.ST stat.ML stat.TH  published:2012-05-18 summary:This paper presents theory for Normalized Random Measures (NRMs), Normalized Generalized Gammas (NGGs), a particular kind of NRM, and Dependent Hierarchical NRMs which allow networks of dependent NRMs to be analysed. These have been used, for instance, for time-dependent topic modelling. In this paper, we first introduce some mathematical background of completely random measures (CRMs) and their construction from Poisson processes, and then introduce NRMs and NGGs. Slice sampling is also introduced for posterior inference. The dependency operators in Poisson processes and for the corresponding CRMs and NRMs is then introduced and Posterior inference for the NGG presented. Finally, we give dependency and composition results when applying these operators to NRMs so they can be used in a network with hierarchical and dependent relations. version:2
arxiv-1111-1020 | Stochastic Belief Propagation: A Low-Complexity Alternative to the Sum-Product Algorithm | http://arxiv.org/abs/1111.1020 | id:1111.1020 author:Nima Noorshams, Martin J. Wainwright category:cs.IT math.IT stat.ML  published:2011-11-04 summary:The sum-product or belief propagation (BP) algorithm is a widely-used message-passing algorithm for computing marginal distributions in graphical models with discrete variables. At the core of the BP message updates, when applied to a graphical model with pairwise interactions, lies a matrix-vector product with complexity that is quadratic in the state dimension $d$, and requires transmission of a $(d-1)$-dimensional vector of real numbers (messages) to its neighbors. Since various applications involve very large state dimensions, such computation and communication complexities can be prohibitively complex. In this paper, we propose a low-complexity variant of BP, referred to as stochastic belief propagation (SBP). As suggested by the name, it is an adaptively randomized version of the BP message updates in which each node passes randomly chosen information to each of its neighbors. The SBP message updates reduce the computational complexity (per iteration) from quadratic to linear in $d$, without assuming any particular structure of the potentials, and also reduce the communication complexity significantly, requiring only $\log{d}$ bits transmission per edge. Moreover, we establish a number of theoretical guarantees for the performance of SBP, showing that it converges almost surely to the BP fixed point for any tree-structured graph, and for graphs with cycles satisfying a contractivity condition. In addition, for these graphical models, we provide non-asymptotic upper bounds on the convergence rate, showing that the $\ell_{\infty}$ norm of the error vector decays no slower than $O(1/\sqrt{t})$ with the number of iterations $t$ on trees and the mean square error decays as $O(1/t)$ for general graphs. These analysis show that SBP can provably yield reductions in computational and communication complexities for various classes of graphical models. version:2
arxiv-1202-6221 | Confusion Matrix Stability Bounds for Multiclass Classification | http://arxiv.org/abs/1202.6221 | id:1202.6221 author:Pierre Machart, Liva Ralaivola category:cs.LG  published:2012-02-28 summary:In this paper, we provide new theoretical results on the generalization properties of learning algorithms for multiclass classification problems. The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution is in the line of work which attempts to set up and study the statistical properties of new evaluation measures such as, e.g. ROC curves. In the confusion-based learning framework we propose, we claim that a targetted objective is to minimize the size of the confusion matrix C, measured through its operator norm C . We derive generalization bounds on the (size of the) confusion matrix in an extended framework of uniform stability, adapted to the case of matrix valued loss. Pivotal to our study is a very recent matrix concentration inequality that generalizes McDiarmid's inequality. As an illustration of the relevance of our theoretical results, we show how two SVM learning procedures can be proved to be confusion-friendly. To the best of our knowledge, the present paper is the first that focuses on the confusion matrix from a theoretical point of view. version:2
arxiv-1205-5425 | Locally Orderless Registration | http://arxiv.org/abs/1205.5425 | id:1205.5425 author:Sune Darkner, Jon Sporring category:cs.CV  published:2012-05-24 summary:Image registration is an important tool for medical image analysis and is used to bring images into the same reference frame by warping the coordinate field of one image, such that some similarity measure is minimized. We study similarity in image registration in the context of Locally Orderless Images (LOI), which is the natural way to study density estimates and reveals the 3 fundamental scales: the measurement scale, the intensity scale, and the integration scale. This paper has three main contributions: Firstly, we rephrase a large set of popular similarity measures into a common framework, which we refer to as Locally Orderless Registration, and which makes full use of the features of local histograms. Secondly, we extend the theoretical understanding of the local histograms. Thirdly, we use our framework to compare two state-of-the-art intensity density estimators for image registration: The Parzen Window (PW) and the Generalized Partial Volume (GPV), and we demonstrate their differences on a popular similarity measure, Normalized Mutual Information (NMI). We conclude, that complicated similarity measures such as NMI may be evaluated almost as fast as simple measures such as Sum of Squared Distances (SSD) regardless of the choice of PW and GPV. Also, GPV is an asymmetric measure, and PW is our preferred choice. version:1
arxiv-1205-5367 | Language-Constraint Reachability Learning in Probabilistic Graphs | http://arxiv.org/abs/1205.5367 | id:1205.5367 author:Claudio Taranto, Nicola Di Mauro, Floriana Esposito category:cs.AI cs.LG  published:2012-05-24 summary:The probabilistic graphs framework models the uncertainty inherent in real-world domains by means of probabilistic edges whose value quantifies the likelihood of the edge existence or the strength of the link it represents. The goal of this paper is to provide a learning method to compute the most likely relationship between two nodes in a framework based on probabilistic graphs. In particular, given a probabilistic graph we adopted the language-constraint reachability method to compute the probability of possible interconnections that may exists between two nodes. Each of these connections may be viewed as feature, or a factor, between the two nodes and the corresponding probability as its weight. Each observed link is considered as a positive instance for its corresponding link label. Given the training set of observed links a L2-regularized Logistic Regression has been adopted to learn a model able to predict unobserved link labels. The experiments on a real world collaborative filtering problem proved that the proposed approach achieves better results than that obtained adopting classical methods. version:1
arxiv-1205-5353 | A hybrid clustering algorithm for data mining | http://arxiv.org/abs/1205.5353 | id:1205.5353 author:Ravindra Jain category:cs.DB cs.LG  published:2012-05-24 summary:Data clustering is a process of arranging similar data into groups. A clustering algorithm partitions a data set into several groups such that the similarity within a group is better than among groups. In this paper a hybrid clustering algorithm based on K-mean and K-harmonic mean (KHM) is described. The proposed algorithm is tested on five different datasets. The research is focused on fast and accurate clustering. Its performance is compared with the traditional K-means & KHM algorithm. The result obtained from proposed hybrid algorithm is much better than the traditional K-mean & KHM algorithm. version:1
arxiv-1205-4336 | Fuzzy - Rough Feature Selection With Π- Membership Function For Mammogram Classification | http://arxiv.org/abs/1205.4336 | id:1205.4336 author:K. Thangavel, R. Roselin category:cs.CV  published:2012-05-19 summary:Breast cancer is the second leading cause for death among women and it is diagnosed with the help of mammograms. Oncologists are miserably failed in identifying the micro calcification at the early stage with the help of the mammogram visually. In order to improve the performance of the breast cancer screening, most of the researchers have proposed Computer Aided Diagnosis using image processing. In this study mammograms are preprocessed and features are extracted, then the abnormality is identified through the classification. If all the extracted features are used, most of the cases are misidentified. Hence feature selection procedure is sought. In this paper, Fuzzy-Rough feature selection with {\pi} membership function is proposed. The selected features are used to classify the abnormalities with help of Ant-Miner and Weka tools. The experimental analysis shows that the proposed method improves the mammograms classification accuracy. version:2
arxiv-1205-5097 | Neural Network Approach for Eye Detection | http://arxiv.org/abs/1205.5097 | id:1205.5097 author:Vijayalaxmi, P. Sudhakara Rao, S. Sreehari category:cs.CV  published:2012-05-23 summary:Driving support systems, such as car navigation systems are becoming common and they support driver in several aspects. Non-intrusive method of detecting Fatigue and drowsiness based on eye-blink count and eye directed instruction controlhelps the driver to prevent from collision caused by drowsy driving. Eye detection and tracking under various conditions such as illumination, background, face alignment and facial expression makes the problem complex.Neural Network based algorithm is proposed in this paper to detect the eyes efficiently. In the proposed algorithm, first the neural Network is trained to reject the non-eye regionbased on images with features of eyes and the images with features of non-eye using Gabor filter and Support Vector Machines to reduce the dimension and classify efficiently. In the algorithm, first the face is segmented using L*a*btransform color space, then eyes are detected using HSV and Neural Network approach. The algorithm is tested on nearly 100 images of different persons under different conditions and the results are satisfactory with success rate of 98%.The Neural Network is trained with 50 non-eye images and 50 eye images with different angles using Gabor filter. This paper is a part of research work on "Development of Non-Intrusive system for real-time Monitoring and Prediction of Driver Fatigue and drowsiness" project sponsored by Department of Science & Technology, Govt. of India, New Delhi at Vignan Institute of Technology and Sciences, Vignan Hills, Hyderabad. version:1
arxiv-1205-4893 | On the practically interesting instances of MAXCUT | http://arxiv.org/abs/1205.4893 | id:1205.4893 author:Yonatan Bilu, Amit Daniely, Nati Linial, Michael Saks category:cs.CC cs.LG  published:2012-05-22 summary:The complexity of a computational problem is traditionally quantified based on the hardness of its worst case. This approach has many advantages and has led to a deep and beautiful theory. However, from the practical perspective, this leaves much to be desired. In application areas, practically interesting instances very often occupy just a tiny part of an algorithm's space of instances, and the vast majority of instances are simply irrelevant. Addressing these issues is a major challenge for theoretical computer science which may make theory more relevant to the practice of computer science. Following Bilu and Linial, we apply this perspective to MAXCUT, viewed as a clustering problem. Using a variety of techniques, we investigate practically interesting instances of this problem. Specifically, we show how to solve in polynomial time distinguished, metric, expanding and dense instances of MAXCUT under mild stability assumptions. In particular, $(1+\epsilon)$-stability (which is optimal) suffices for metric and dense MAXCUT. We also show how to solve in polynomial time $\Omega(\sqrt{n})$-stable instances of MAXCUT, substantially improving the best previously known result. version:1
arxiv-1205-4891 | Clustering is difficult only when it does not matter | http://arxiv.org/abs/1205.4891 | id:1205.4891 author:Amit Daniely, Nati Linial, Michael Saks category:cs.LG cs.DS  published:2012-05-22 summary:Numerous papers ask how difficult it is to cluster data. We suggest that the more relevant and interesting question is how difficult it is to cluster data sets {\em that can be clustered well}. More generally, despite the ubiquity and the great importance of clustering, we still do not have a satisfactory mathematical theory of clustering. In order to properly understand clustering, it is clearly necessary to develop a solid theoretical basis for the area. For example, from the perspective of computational complexity theory the clustering problem seems very hard. Numerous papers introduce various criteria and numerical measures to quantify the quality of a given clustering. The resulting conclusions are pessimistic, since it is computationally difficult to find an optimal clustering of a given data set, if we go by any of these popular criteria. In contrast, the practitioners' perspective is much more optimistic. Our explanation for this disparity of opinions is that complexity theory concentrates on the worst case, whereas in reality we only care for data sets that can be clustered well. We introduce a theoretical framework of clustering in metric spaces that revolves around a notion of "good clustering". We show that if a good clustering exists, then in many cases it can be efficiently found. Our conclusion is that contrary to popular belief, clustering should not be considered a hard task. version:1
arxiv-1205-4831 | Gray Level Co-Occurrence Matrices: Generalisation and Some New Features | http://arxiv.org/abs/1205.4831 | id:1205.4831 author:Bino Sebastian V, A. Unnikrishnan, Kannan Balakrishnan category:cs.CV  published:2012-05-22 summary:Gray Level Co-occurrence Matrices (GLCM) are one of the earliest techniques used for image texture analysis. In this paper we defined a new feature called trace extracted from the GLCM and its implications in texture analysis are discussed in the context of Content Based Image Retrieval (CBIR). The theoretical extension of GLCM to n-dimensional gray scale images are also discussed. The results indicate that trace features outperform Haralick features when applied to CBIR. version:1
arxiv-1205-4776 | Visual and semantic interpretability of projections of high dimensional data for classification tasks | http://arxiv.org/abs/1205.4776 | id:1205.4776 author:Ilknur Icke, Andrew Rosenberg category:cs.HC cs.LG  published:2012-05-22 summary:A number of visual quality measures have been introduced in visual analytics literature in order to automatically select the best views of high dimensional data from a large number of candidate data projections. These methods generally concentrate on the interpretability of the visualization and pay little attention to the interpretability of the projection axes. In this paper, we argue that interpretability of the visualizations and the feature transformation functions are both crucial for visual exploration of high dimensional labeled data. We present a two-part user study to examine these two related but orthogonal aspects of interpretability. We first study how humans judge the quality of 2D scatterplots of various datasets with varying number of classes and provide comparisons with ten automated measures, including a number of visual quality measures and related measures from various machine learning fields. We then investigate how the user perception on interpretability of mathematical expressions relate to various automated measures of complexity that can be used to characterize data projection functions. We conclude with a discussion of how automated measures of visual and semantic interpretability of data projections can be used together for exploratory analysis in classification tasks. version:1
arxiv-1205-4770 | Variance function estimation in high-dimensions | http://arxiv.org/abs/1205.4770 | id:1205.4770 author:Mladen Kolar, James Sharpnack category:stat.ML stat.ME  published:2012-05-21 summary:We consider the high-dimensional heteroscedastic regression model, where the mean and the log variance are modeled as a linear combination of input variables. Existing literature on high-dimensional linear regres- sion models has largely ignored non-constant error variances, even though they commonly occur in a variety of applications ranging from biostatis- tics to finance. In this paper we study a class of non-convex penalized pseudolikelihood estimators for both the mean and variance parameters. We show that the Heteroscedastic Iterative Penalized Pseudolikelihood Optimizer (HIPPO) achieves the oracle property, that is, we prove that the rates of convergence are the same as if the true model was known. We demonstrate numerical properties of the procedure on a simulation study and real world data. version:1
arxiv-1205-0411 | Hypothesis testing using pairwise distances and associated kernels (with Appendix) | http://arxiv.org/abs/1205.0411 | id:1205.0411 author:Dino Sejdinovic, Arthur Gretton, Bharath Sriperumbudur, Kenji Fukumizu category:cs.LG stat.ME stat.ML  published:2012-05-02 summary:We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. The equivalence holds when energy distances are computed with semimetrics of negative type, in which case a kernel may be defined such that the RKHS distance between distributions corresponds exactly to the energy distance. We determine the class of probability distributions for which kernels induced by semimetrics are characteristic (that is, for which embeddings of the distributions to an RKHS are injective). Finally, we investigate the performance of this family of kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests. version:2
arxiv-1205-1782 | Approximate Dynamic Programming By Minimizing Distributionally Robust Bounds | http://arxiv.org/abs/1205.1782 | id:1205.1782 author:Marek Petrik category:stat.ML cs.LG  published:2012-05-08 summary:Approximate dynamic programming is a popular method for solving large Markov decision processes. This paper describes a new class of approximate dynamic programming (ADP) methods- distributionally robust ADP-that address the curse of dimensionality by minimizing a pessimistic bound on the policy loss. This approach turns ADP into an optimization problem, for which we derive new mathematical program formulations and analyze its properties. DRADP improves on the theoretical guarantees of existing ADP methods-it guarantees convergence and L1 norm based error bounds. The empirical evaluation of DRADP shows that the theoretical guarantees translate well into good performance on benchmark problems. version:2
arxiv-1205-4546 | Latent Multi-group Membership Graph Model | http://arxiv.org/abs/1205.4546 | id:1205.4546 author:Myunghwan Kim, Jure Leskovec category:cs.SI physics.soc-ph stat.ML  published:2012-05-21 summary:We develop the Latent Multi-group Membership Graph (LMMG) model, a model of networks with rich node feature structure. In the LMMG model, each node belongs to multiple groups and each latent group models the occurrence of links as well as the node feature structure. The LMMG can be used to summarize the network structure, to predict links between the nodes, and to predict missing features of a node. We derive efficient inference and learning algorithms and evaluate the predictive performance of the LMMG on several social and document network datasets. version:1
arxiv-1205-4477 | Streaming Algorithms for Pattern Discovery over Dynamically Changing Event Sequences | http://arxiv.org/abs/1205.4477 | id:1205.4477 author:Debprakash Patnaik, Naren Ramakrishnan, Srivatsan Laxman, Badrish Chandramouli category:cs.LG cs.DB  published:2012-05-21 summary:Discovering frequent episodes over event sequences is an important data mining task. In many applications, events constituting the data sequence arrive as a stream, at furious rates, and recent trends (or frequent episodes) can change and drift due to the dynamical nature of the underlying event generation process. The ability to detect and track such the changing sets of frequent episodes can be valuable in many application scenarios. Current methods for frequent episode discovery are typically multipass algorithms, making them unsuitable in the streaming context. In this paper, we propose a new streaming algorithm for discovering frequent episodes over a window of recent events in the stream. Our algorithm processes events as they arrive, one batch at a time, while discovering the top frequent episodes over a window consisting of several batches in the immediate past. We derive approximation guarantees for our algorithm under the condition that frequent episodes are approximately well-separated from infrequent ones in every batch of the window. We present extensive experimental evaluations of our algorithm on both real and synthetic data. We also present comparisons with baselines and adaptations of streaming algorithms from itemset mining literature. version:1
arxiv-1205-4471 | Sparse Signal Recovery in the Presence of Intra-Vector and Inter-Vector Correlation | http://arxiv.org/abs/1205.4471 | id:1205.4471 author:Bhaskar D. Rao, Zhilin Zhang, Yuzhe Jin category:cs.IT cs.LG math.IT stat.ME stat.ML  published:2012-05-20 summary:This work discusses the problem of sparse signal recovery when there is correlation among the values of non-zero entries. We examine intra-vector correlation in the context of the block sparse model and inter-vector correlation in the context of the multiple measurement vector model, as well as their combination. Algorithms based on the sparse Bayesian learning are presented and the benefits of incorporating correlation at the algorithm level are discussed. The impact of correlation on the limits of support recovery is also discussed highlighting the different impact intra-vector and inter-vector correlations have on such limits. version:1
arxiv-1206-4880 | Dynamic Domain Classification for Fractal Image Compression | http://arxiv.org/abs/1206.4880 | id:1206.4880 author:K. Revathy, M. Jayamohan category:cs.CV cs.GR  published:2012-05-20 summary:Fractal image compression is attractive except for its high encoding time requirements. The image is encoded as a set of contractive affine transformations. The image is partitioned into non-overlapping range blocks, and a best matching domain block larger than the range block is identified. There are many attempts on improving the encoding time by reducing the size of search pool for range-domain matching. But these methods are attempting to prepare a static domain pool that remains unchanged throughout the encoding process. This paper proposes dynamic preparation of separate domain pool for each range block. This will result in significant reduction in the encoding time. The domain pool for a particular range block can be selected based upon a parametric value. Here we use classification based on local fractal dimension. version:1
arxiv-1205-4387 | Precision-biased Parsing and High-Quality Parse Selection | http://arxiv.org/abs/1205.4387 | id:1205.4387 author:Yoav Goldberg, Michael Elhadad category:cs.CL  published:2012-05-20 summary:We introduce precision-biased parsing: a parsing task which favors precision over recall by allowing the parser to abstain from decisions deemed uncertain. We focus on dependency-parsing and present an ensemble method which is capable of assigning parents to 84% of the text tokens while being over 96% accurate on these tokens. We use the precision-biased parsing task to solve the related high-quality parse-selection task: finding a subset of high-quality (accurate) trees in a large collection of parsed text. We present a method for choosing over a third of the input trees while keeping unlabeled dependency parsing accuracy of 97% on these trees. We also present a method which is not based on an ensemble but rather on directly predicting the risk associated with individual parser decisions. In addition to its efficiency, this method demonstrates that a parsing system can provide reasonable estimates of confidence in its predictions without relying on ensembles or aggregate corpus counts. version:1
arxiv-1205-0079 | Complexity Analysis of the Lasso Regularization Path | http://arxiv.org/abs/1205.0079 | id:1205.0079 author:Julien Mairal, Bin Yu category:stat.ML cs.LG math.OC  published:2012-05-01 summary:The regularization path of the Lasso can be shown to be piecewise linear, making it possible to "follow" and explicitly compute the entire path. We analyze in this paper this popular strategy, and prove that its worst case complexity is exponential in the number of variables. We then oppose this pessimistic result to an (optimistic) approximate analysis: We show that an approximate path with at most O(1/sqrt(epsilon)) linear segments can always be obtained, where every point on the path is guaranteed to be optimal up to a relative epsilon-duality gap. We complete our theoretical analysis with a practical algorithm to compute these approximate paths. version:2
arxiv-1204-2847 | Segmentation Similarity and Agreement | http://arxiv.org/abs/1204.2847 | id:1204.2847 author:Chris Fournier, Diana Inkpen category:cs.CL  published:2012-04-12 summary:We propose a new segmentation evaluation metric, called segmentation similarity (S), that quantifies the similarity between two segmentations as the proportion of boundaries that are not transformed when comparing them using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size. We propose several adapted inter-annotator agreement coefficients which use S that are suitable for segmentation. We show that S is configurable enough to suit a wide variety of segmentation evaluations, and is an improvement upon the state of the art. We also propose using inter-annotator agreement coefficients to evaluate automatic segmenters in terms of human performance. version:2
arxiv-1205-4349 | From Exact Learning to Computing Boolean Functions and Back Again | http://arxiv.org/abs/1205.4349 | id:1205.4349 author:Sergiu Goschin category:cs.LG cs.DM  published:2012-05-19 summary:The goal of the paper is to relate complexity measures associated with the evaluation of Boolean functions (certificate complexity, decision tree complexity) and learning dimensions used to characterize exact learning (teaching dimension, extended teaching dimension). The high level motivation is to discover non-trivial relations between exact learning of an unknown concept and testing whether an unknown concept is part of a concept class or not. Concretely, the goal is to provide lower and upper bounds of complexity measures for one problem type in terms of the other. version:1
arxiv-1205-0088 | ProPPA: A Fast Algorithm for $\ell_1$ Minimization and Low-Rank Matrix Completion | http://arxiv.org/abs/1205.0088 | id:1205.0088 author:Ranch Y. Q. Lai, Pong C. Yuen category:cs.LG math.OC  published:2012-05-01 summary:We propose a Projected Proximal Point Algorithm (ProPPA) for solving a class of optimization problems. The algorithm iteratively computes the proximal point of the last estimated solution projected into an affine space which itself is parallel and approaching to the feasible set. We provide convergence analysis theoretically supporting the general algorithm, and then apply it for solving $\ell_1$-minimization problems and the matrix completion problem. These problems arise in many applications including machine learning, image and signal processing. We compare our algorithm with the existing state-of-the-art algorithms. Experimental results on solving these problems show that our algorithm is very efficient and competitive. version:2
arxiv-1205-4298 | Task-specific Word-Clustering for Part-of-Speech Tagging | http://arxiv.org/abs/1205.4298 | id:1205.4298 author:Yoav Goldberg category:cs.CL  published:2012-05-19 summary:While the use of cluster features became ubiquitous in core NLP tasks, most cluster features in NLP are based on distributional similarity. We propose a new type of clustering criteria, specific to the task of part-of-speech tagging. Instead of distributional similarity, these clusters are based on the beha vior of a baseline tagger when applied to a large corpus. These cluster features provide similar gains in accuracy to those achieved by distributional-similarity derived clusters. Using both types of cluster features together further improve tagging accuracies. We show that the method is effective for both the in-domain and out-of-domain scenarios for English, and for French, German and Italian. The effect is larger for out-of-domain text. version:1
arxiv-1205-4295 | Efficient Methods for Unsupervised Learning of Probabilistic Models | http://arxiv.org/abs/1205.4295 | id:1205.4295 author:Jascha Sohl-Dickstein category:cs.LG cs.AI cs.IT cs.NE math.IT physics.data-an  published:2012-05-19 summary:In this thesis I develop a variety of techniques to train, evaluate, and sample from intractable and high dimensional probabilistic models. Abstract exceeds arXiv space limitations -- see PDF. version:1
arxiv-1205-4120 | Two New Algorithms for Solving Covariance Graphical Lasso Based on Coordinate Descent and ECM | http://arxiv.org/abs/1205.4120 | id:1205.4120 author:Hao Wang category:stat.CO stat.ML  published:2012-05-18 summary:Covariance graphical lasso applies a lasso penalty on the elements of the covariance matrix. This method is useful because it not only produces sparse estimation of covariance matrix but also discovers marginal independence structures by generating zeros in the covariance matrix. We propose and explore two new algorithms for solving the covariance graphical lasso problem. Our new algorithms are based on coordinate descent and ECM. We show that these two algorithms are more attractive than the only existing competing algorithm of Bien and Tibshirani (2011) in terms of simplicity, speed and stability. We also discuss convergence properties of our algorithms. version:1
arxiv-1203-3391 | Adaptive experimental design for one-qubit state estimation with finite data based on a statistical update criterion | http://arxiv.org/abs/1203.3391 | id:1203.3391 author:Takanori Sugiyama, Peter S. Turner, Mio Murao category:quant-ph math.ST stat.ML stat.TH  published:2012-03-15 summary:We consider 1-qubit mixed quantum state estimation by adaptively updating measurements according to previously obtained outcomes and measurement settings. Updates are determined by the average-variance-optimality (A-optimality) criterion, known in the classical theory of experimental design and applied here to quantum state estimation. In general, A-optimization is a nonlinear minimization problem; however, we find an analytic solution for 1-qubit state estimation using projective measurements, reducing computational effort. We compare numerically two adaptive and two nonadaptive schemes for finite data sets and show that the A-optimality criterion gives more precise estimates than standard quantum tomography. version:2
arxiv-1205-3999 | Optimal Weights Mixed Filter for Removing Mixture of Gaussian and Impulse Noises | http://arxiv.org/abs/1205.3999 | id:1205.3999 author:Qiyu Jin, Ion Grama, Quansheng Liu category:cs.CV  published:2012-05-17 summary:According to the character of Gaussian, we modify the Rank-Ordered Absolute Differences (ROAD) to Rank-Ordered Absolute Differences of mixture of Gaussian and impulse noises (ROADG). It will be more effective to detect impulse noise when the impulse is mixed with Gaussian noise. Combining rightly the ROADG with Optimal Weights Filter (OWF), we obtain a new method to deal with the mixed noise, called Optimal Weights Mixed Filter (OWMF). The simulation results show that the method is effective to remove the mixed noise. version:1
arxiv-1205-3997 | Free Energy and the Generalized Optimality Equations for Sequential Decision Making | http://arxiv.org/abs/1205.3997 | id:1205.3997 author:Pedro A. Ortega, Daniel A. Braun category:stat.ML cs.AI cs.GT cs.SY  published:2012-05-17 summary:The free energy functional has recently been proposed as a variational principle for bounded rational decision-making, since it instantiates a natural trade-off between utility gains and information processing costs that can be axiomatically derived. Here we apply the free energy principle to general decision trees that include both adversarial and stochastic environments. We derive generalized sequential optimality equations that not only include the Bellman optimality equations as a limit case, but also lead to well-known decision-rules such as Expectimax, Minimax and Expectiminimax. We show how these decision-rules can be derived from a single free energy principle that assigns a resource parameter to each node in the decision tree. These resource parameters express a concrete computational cost that can be measured as the amount of samples that are needed from the distribution that belongs to each node. The free energy principle therefore provides the normative basis for generalized optimality equations that account for both adversarial and stochastic environments. version:1
arxiv-1205-3966 | Neural Networks for Handwritten English Alphabet Recognition | http://arxiv.org/abs/1205.3966 | id:1205.3966 author:Yusuf Perwej, Ashish Chaturvedi category:cs.AI cs.CV  published:2012-05-17 summary:This paper demonstrates the use of neural networks for developing a system that can recognize hand-written English alphabets. In this system, each English alphabet is represented by binary values that are used as input to a simple feature extraction system, whose output is fed to our neural network system. version:1
arxiv-1204-6326 | Background subtraction based on Local Shape | http://arxiv.org/abs/1204.6326 | id:1204.6326 author:Jean-Philippe Jodoin, Guillaume-Alexandre Bilodeau, Nicolas Saunier category:cs.CV  published:2012-04-27 summary:We present a novel approach to background subtraction that is based on the local shape of small image regions. In our approach, an image region centered on a pixel is mod-eled using the local self-similarity descriptor. We aim at obtaining a reliable change detection based on local shape change in an image when foreground objects are moving. The method first builds a background model and compares the local self-similarities between the background model and the subsequent frames to distinguish background and foreground objects. Post-processing is then used to refine the boundaries of moving objects. Results show that this approach is promising as the foregrounds obtained are com-plete, although they often include shadows. version:2
arxiv-1205-3549 | Normalized Maximum Likelihood Coding for Exponential Family with Its Applications to Optimal Clustering | http://arxiv.org/abs/1205.3549 | id:1205.3549 author:So Hirai, Kenji Yamanishi category:cs.LG  published:2012-05-16 summary:We are concerned with the issue of how to calculate the normalized maximum likelihood (NML) code-length. There is a problem that the normalization term of the NML code-length may diverge when it is continuous and unbounded and a straightforward computation of it is highly expensive when the data domain is finite . In previous works it has been investigated how to calculate the NML code-length for specific types of distributions. We first propose a general method for computing the NML code-length for the exponential family. Then we specifically focus on Gaussian mixture model (GMM), and propose a new efficient method for computing the NML to them. We develop it by generalizing Rissanen's re-normalizing technique. Then we apply this method to the clustering issue, in which a clustering structure is modeled using a GMM, and the main task is to estimate the optimal number of clusters on the basis of the NML code-length. We demonstrate using artificial data sets the superiority of the NML-based clustering over other criteria such as AIC, BIC in terms of the data size required for high accuracy rate to be achieved. version:2
arxiv-1204-4779 | Paraiso : An Automated Tuning Framework for Explicit Solvers of Partial Differential Equations | http://arxiv.org/abs/1204.4779 | id:1204.4779 author:Takayuki Muranushi category:astro-ph.IM cs.DC cs.NE  published:2012-04-21 summary:We propose Paraiso, a domain specific language embedded in functional programming language Haskell, for automated tuning of explicit solvers of partial differential equations (PDEs) on GPUs as well as multicore CPUs. In Paraiso, one can describe PDE solving algorithms succinctly using tensor equations notation. Hydrodynamic properties, interpolation methods and other building blocks are described in abstract, modular, re-usable and combinable forms, which lets us generate versatile solvers from little set of Paraiso source codes. We demonstrate Paraiso by implementing a compressive hydrodynamics solver. A single source code less than 500 lines can be used to generate solvers of arbitrary dimensions, for both multicore CPUs and GPUs. We demonstrate both manual annotation based tuning and evolutionary computing based automated tuning of the program. version:2
arxiv-1205-3776 | The ideal of the trifocal variety | http://arxiv.org/abs/1205.3776 | id:1205.3776 author:Chris Aholt, Luke Oeding category:math.AG cs.CV  published:2012-05-16 summary:Techniques from representation theory, symbolic computational algebra, and numerical algebraic geometry are used to find the minimal generators of the ideal of the trifocal variety. An effective test for determining whether a given tensor is a trifocal tensor is also given. version:1
arxiv-1205-3766 | Efficient Topology-Controlled Sampling of Implicit Shapes | http://arxiv.org/abs/1205.3766 | id:1205.3766 author:Jason Chang, John W. Fisher III category:cs.CV  published:2012-05-16 summary:Sampling from distributions of implicitly defined shapes enables analysis of various energy functionals used for image segmentation. Recent work describes a computationally efficient Metropolis-Hastings method for accomplishing this task. Here, we extend that framework so that samples are accepted at every iteration of the sampler, achieving an order of magnitude speed up in convergence. Additionally, we show how to incorporate topological constraints. version:1
arxiv-1205-3336 | Distribution of the search of evolutionary product unit neural networks for classification | http://arxiv.org/abs/1205.3336 | id:1205.3336 author:A. J. Tallón-Ballesteros, P. A. Gutiérrez-Peña, C. Hervás-Martínez category:cs.NE cs.AI cs.CV  published:2012-05-15 summary:This paper deals with the distributed processing in the search for an optimum classification model using evolutionary product unit neural networks. For this distributed search we used a cluster of computers. Our objective is to obtain a more efficient design than those net architectures which do not use a distributed process and which thus result in simpler designs. In order to get the best classification models we use evolutionary algorithms to train and design neural networks, which require a very time consuming computation. The reasons behind the need for this distribution are various. It is complicated to train this type of nets because of the difficulty entailed in determining their architecture due to the complex error surface. On the other hand, the use of evolutionary algorithms involves running a great number of tests with different seeds and parameters, thus resulting in a high computational cost version:1
arxiv-1205-3316 | Arabic Language Learning Assisted by Computer, based on Automatic Speech Recognition | http://arxiv.org/abs/1205.3316 | id:1205.3316 author:Naim Terbeh, Mounir Zrigui category:cs.CL  published:2012-05-15 summary:This work consists of creating a system of the Computer Assisted Language Learning (CALL) based on a system of Automatic Speech Recognition (ASR) for the Arabic language using the tool CMU Sphinx3 [1], based on the approach of HMM. To this work, we have constructed a corpus of six hours of speech recordings with a number of nine speakers. we find in the robustness to noise a grounds for the choice of the HMM approach [2]. the results achieved are encouraging since our corpus is made by only nine speakers, but they are always reasons that open the door for other improvement works. version:1
arxiv-1205-3193 | A Comparative Study of Collaborative Filtering Algorithms | http://arxiv.org/abs/1205.3193 | id:1205.3193 author:Joonseok Lee, Mingxuan Sun, Guy Lebanon category:cs.IR stat.ML I.2.6; H.2.8  published:2012-05-14 summary:Collaborative filtering is a rapidly advancing research area. Every year several new techniques are proposed and yet it is not clear which of the techniques work best and under what conditions. In this paper we conduct a study comparing several collaborative filtering techniques -- both classic and recent state-of-the-art -- in a variety of experimental contexts. Specifically, we report conclusions controlling for number of items, number of users, sparsity level, performance criteria, and computational complexity. Our conclusions identify what algorithms work well and in what conditions, and contribute to both industrial deployment collaborative filtering algorithms and to the research community. version:1
arxiv-1205-3183 | A Model-Driven Probabilistic Parser Generator | http://arxiv.org/abs/1205.3183 | id:1205.3183 author:Luis Quesada, Fernando Berzal, Francisco J. Cortijo category:cs.CL  published:2012-05-14 summary:Existing probabilistic scanners and parsers impose hard constraints on the way lexical and syntactic ambiguities can be resolved. Furthermore, traditional grammar-based parsing tools are limited in the mechanisms they allow for taking context into account. In this paper, we propose a model-driven tool that allows for statistical language models with arbitrary probability estimators. Our work on model-driven probabilistic parsing is built on top of ModelCC, a model-based parser generator, and enables the probabilistic interpretation and resolution of anaphoric, cataphoric, and recursive references in the disambiguation of abstract syntax graphs. In order to prove the expression power of ModelCC, we describe the design of a general-purpose natural language parser. version:1
arxiv-1205-3181 | Multiple Identifications in Multi-Armed Bandits | http://arxiv.org/abs/1205.3181 | id:1205.3181 author:Sébastien Bubeck, Tengyao Wang, Nitin Viswanathan category:cs.LG stat.ML  published:2012-05-14 summary:We study the problem of identifying the top $m$ arms in a multi-armed bandit game. Our proposed solution relies on a new algorithm based on successive rejects of the seemingly bad arms, and successive accepts of the good ones. This algorithmic contribution allows to tackle other multiple identifications settings that were previously out of reach. In particular we show that this idea of successive accepts and rejects applies to the multi-bandit best arm identification problem. version:1
arxiv-1205-2958 | b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning and Using GPUs for Fast Preprocessing with Simple Hash Functions | http://arxiv.org/abs/1205.2958 | id:1205.2958 author:Ping Li, Anshumali Shrivastava, Arnd Christian Konig category:cs.IR cs.DB cs.LG  published:2012-05-14 summary:In this paper, we study several critical issues which must be tackled before one can apply b-bit minwise hashing to the volumes of data often used industrial applications, especially in the context of search. 1. (b-bit) Minwise hashing requires an expensive preprocessing step that computes k (e.g., 500) minimal values after applying the corresponding permutations for each data vector. We developed a parallelization scheme using GPUs and observed that the preprocessing time can be reduced by a factor of 20-80 and becomes substantially smaller than the data loading time. 2. One major advantage of b-bit minwise hashing is that it can substantially reduce the amount of memory required for batch learning. However, as online algorithms become increasingly popular for large-scale learning in the context of search, it is not clear if b-bit minwise yields significant improvements for them. This paper demonstrates that $b$-bit minwise hashing provides an effective data size/dimension reduction scheme and hence it can dramatically reduce the data loading time for each epoch of the online training process. This is significant because online learning often requires many (e.g., 10 to 100) epochs to reach a sufficient accuracy. 3. Another critical issue is that for very large data sets it becomes impossible to store a (fully) random permutation matrix, due to its space requirements. Our paper is the first study to demonstrate that $b$-bit minwise hashing implemented using simple hash functions, e.g., the 2-universal (2U) and 4-universal (4U) hash families, can produce very similar learning results as using fully random permutations. Experiments on datasets of up to 200GB are presented. version:1
arxiv-1205-2930 | Density Sensitive Hashing | http://arxiv.org/abs/1205.2930 | id:1205.2930 author:Yue Lin, Deng Cai, Cheng Li category:cs.IR cs.LG  published:2012-05-14 summary:Nearest neighbors search is a fundamental problem in various research fields like machine learning, data mining and pattern recognition. Recently, hashing-based approaches, e.g., Locality Sensitive Hashing (LSH), are proved to be effective for scalable high dimensional nearest neighbors search. Many hashing algorithms found their theoretic root in random projection. Since these algorithms generate the hash tables (projections) randomly, a large number of hash tables (i.e., long codewords) are required in order to achieve both high precision and recall. To address this limitation, we propose a novel hashing algorithm called {\em Density Sensitive Hashing} (DSH) in this paper. DSH can be regarded as an extension of LSH. By exploring the geometric structure of the data, DSH avoids the purely random projections selection and uses those projective functions which best agree with the distribution of the data. Extensive experimental results on real-world data sets have shown that the proposed method achieves better performance compared to the state-of-the-art hashing approaches. version:1
arxiv-1205-2821 | Texture Analysis And Characterization Using Probability Fractal Descriptors | http://arxiv.org/abs/1205.2821 | id:1205.2821 author:J. B. Florindo, O. M. Bruno category:physics.data-an cs.CV  published:2012-05-13 summary:A gray-level image texture descriptors based on fractal dimension estimation is proposed in this work. The proposed method estimates the fractal dimension using probability (Voss) method. The descriptors are computed applying a multiscale transform to the fractal dimension curves of the texture image. The proposed texture descriptor method is evaluated in a classification task of well known benchmark texture datasets. The results show the great performance of the proposed method as a tool for texture images analysis and characterization. version:1
arxiv-1111-0034 | Diffusion Adaptation Strategies for Distributed Optimization and Learning over Networks | http://arxiv.org/abs/1111.0034 | id:1111.0034 author:Jianshu Chen, Ali H. Sayed category:math.OC cs.IT cs.LG cs.SI math.IT physics.soc-ph  published:2011-10-31 summary:We propose an adaptive diffusion mechanism to optimize a global cost function in a distributed manner over a network of nodes. The cost function is assumed to consist of a collection of individual components. Diffusion adaptation allows the nodes to cooperate and diffuse information in real-time; it also helps alleviate the effects of stochastic gradient noise and measurement noise through a continuous learning process. We analyze the mean-square-error performance of the algorithm in some detail, including its transient and steady-state behavior. We also apply the diffusion algorithm to two problems: distributed estimation with sparse parameters and distributed localization. Compared to well-studied incremental methods, diffusion methods do not require the use of a cyclic path over the nodes and are robust to node and link failure. Diffusion methods also endow networks with adaptation abilities that enable the individual nodes to continue learning even when the cost function changes with time. Examples involving such dynamic cost functions with moving targets are common in the context of biological networks. version:3
arxiv-1205-2797 | Forecasting of Indian Rupee (INR) / US Dollar (USD) Currency Exchange Rate Using Artificial Neural Network | http://arxiv.org/abs/1205.2797 | id:1205.2797 author:Yusuf Perwej, Asif Perwej category:cs.NE  published:2012-05-12 summary:A large part of the workforce, and growing every day, is originally from India. India one of the second largest populations in the world, they have a lot to offer in terms of jobs. The sheer number of IT workers makes them a formidable travelling force as well, easily picking up employment in English speaking countries. The beginning of the economic crises since 2008 September, many Indians have return homeland, and this has had a substantial impression on the Indian Rupee (INR) as liken to the US Dollar (USD). We are using numerational knowledge based techniques for forecasting has been proved highly successful in present time. The purpose of this paper is to examine the effects of several important neural network factors on model fitting and forecasting the behaviours. In this paper, Artificial Neural Network has successfully been used for exchange rate forecasting. This paper examines the effects of the number of inputs and hidden nodes and the size of the training sample on the in-sample and out-of-sample performance. The Indian Rupee (INR) / US Dollar (USD) is used for detailed examinations. The number of input nodes has a greater impact on performance than the number of hidden nodes, while a large number of observations do reduce forecast errors. version:1
arxiv-1204-5431 | Robust Head Pose Estimation Using Contourlet Transform | http://arxiv.org/abs/1204.5431 | id:1204.5431 author:Mohammad Tofighi, Hashem Kalbkhani, Mahrokh G. Shayesteh, Mehdi Ghasemzadeh category:cs.CV  published:2012-04-24 summary:Estimating pose of the head is an important preprocessing step in many pattern recognition and computer vision systems such as face recognition. Since the performance of the face recognition systems is greatly affected by the poses of the face, how to estimate the accurate pose of the face in human face image is still a challenging problem. In this paper, we represent a novel method for head pose estimation. To enhance the efficiency of the estimation we use contourlet transform for feature extraction. Contourlet transform is multi-resolution, multi-direction transform. In order to reduce the feature space dimension and obtain appropriate features we use LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) to remove ineffcient features. Then, we apply different classifiers such as k-nearest neighborhood (knn) and minimum distance. We use the public available FERET database to evaluate the performance of proposed method. Simulation results indicate the superior robustness of the proposed method. version:2
arxiv-1205-2663 | Are visual dictionaries generalizable? | http://arxiv.org/abs/1205.2663 | id:1205.2663 author:Otavio A. B. Penatti, Eduardo Valle, Ricardo da S. Torres category:cs.CV  published:2012-05-11 summary:Mid-level features based on visual dictionaries are today a cornerstone of systems for classification and retrieval of images. Those state-of-the-art representations depend crucially on the choice of a codebook (visual dictionary), which is usually derived from the dataset. In general-purpose, dynamic image collections (e.g., the Web), one cannot have the entire collection in order to extract a representative dictionary. However, based on the hypothesis that the dictionary reflects only the diversity of low-level appearances and does not capture semantics, we argue that a dictionary based on a small subset of the data, or even on an entirely different dataset, is able to produce a good representation, provided that the chosen images span a diverse enough portion of the low-level feature space. Our experiments confirm that hypothesis, opening the opportunity to greatly alleviate the burden in generating the codebook, and confirming the feasibility of employing visual dictionaries in large-scale dynamic environments. version:1
arxiv-1008-5372 | Penalty Decomposition Methods for $L0$-Norm Minimization | http://arxiv.org/abs/1008.5372 | id:1008.5372 author:Zhaosong Lu, Yong Zhang category:math.OC cs.CV cs.IT cs.LG cs.NA math.IT stat.ME  published:2010-08-31 summary:In this paper we consider general l0-norm minimization problems, that is, the problems with l0-norm appearing in either objective function or constraint. In particular, we first reformulate the l0-norm constrained problem as an equivalent rank minimization problem and then apply the penalty decomposition (PD) method proposed in [33] to solve the latter problem. By utilizing the special structures, we then transform all matrix operations of this method to vector operations and obtain a PD method that only involves vector operations. Under some suitable assumptions, we establish that any accumulation point of the sequence generated by the PD method satisfies a first-order optimality condition that is generally stronger than one natural optimality condition. We further extend the PD method to solve the problem with the l0-norm appearing in objective function. Finally, we test the performance of our PD methods by applying them to compressed sensing, sparse logistic regression and sparse inverse covariance selection. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality and/or speed. version:2
arxiv-1104-3571 | Visualization techniques for data mining of Latur district satellite imagery | http://arxiv.org/abs/1104.3571 | id:1104.3571 author:B. G. Kodge, P. S. Hiremath category:cs.CE cs.CV D.1.7; H.2.8; I.4.6  published:2011-03-28 summary:This study presents a new visualization tool for classification of satellite imagery. Visualization of feature space allows exploration of patterns in the image data and insight into the classification process and related uncertainty. Visual Data Mining provides added value to image classifications as the user can be involved in the classification process providing increased confidence in and understanding of the results. In this study, we present a prototype visualization tool for visual data mining (VDM) of satellite imagery. The visualization tool is showcased in a classification study of highresolution imageries of Latur district in Maharashtra state of India. version:2
arxiv-1205-2345 | Hajj and Umrah Event Recognition Datasets | http://arxiv.org/abs/1205.2345 | id:1205.2345 author:Hossam Zawbaa, Salah A. Aly category:cs.CV cs.CY  published:2012-05-10 summary:In this note, new Hajj and Umrah Event Recognition datasets (HUER) are presented. The demonstrated datasets are based on videos and images taken during 2011-2012 Hajj and Umrah seasons. HUER is the first collection of datasets covering the six types of Hajj and Umrah ritual events (rotating in Tawaf around Kabaa, performing Sa'y between Safa and Marwa, standing on the mount of Arafat, staying overnight in Muzdalifah, staying two or three days in Mina, and throwing Jamarat). The HUER datasets also contain video and image databases for nine types of human actions during Hajj and Umrah (walking, drinking from Zamzam water, sleeping, smiling, eating, praying, sitting, shaving hairs and ablutions, reading the holy Quran and making duaa). The spatial resolutions are 1280 x 720 pixels for images and 640 x 480 pixels for videos and have lengths of 20 seconds in average with 30 frame per second rates. version:1
arxiv-1205-2282 | A Discussion on Parallelization Schemes for Stochastic Vector Quantization Algorithms | http://arxiv.org/abs/1205.2282 | id:1205.2282 author:Matthieu Durut, Benoît Patra, Fabrice Rossi category:stat.ML cs.DC cs.LG  published:2012-05-10 summary:This paper studies parallelization schemes for stochastic Vector Quantization algorithms in order to obtain time speed-ups using distributed resources. We show that the most intuitive parallelization scheme does not lead to better performances than the sequential algorithm. Another distributed scheme is therefore introduced which obtains the expected speed-ups. Then, it is improved to fit implementation on distributed architectures where communications are slow and inter-machines synchronization too costly. The schemes are tested with simulated distributed architectures and, for the last one, with Microsoft Windows Azure platform obtaining speed-ups up to 32 Virtual Machines. version:1
arxiv-1205-2164 | Discrimination of English to other Indian languages (Kannada and Hindi) for OCR system | http://arxiv.org/abs/1205.2164 | id:1205.2164 author:Ankit Kumar, Tushar Patnaik, Vivek Kr Verma category:cs.CV  published:2012-05-10 summary:India is a multilingual multi-script country. In every state of India there are two languages one is state local language and the other is English. For example in Andhra Pradesh, a state in India, the document may contain text words in English and Telugu script. For Optical Character Recognition (OCR) of such a bilingual document, it is necessary to identify the script before feeding the text words to the OCRs of individual scripts. In this paper, we are introducing a simple and efficient technique of script identification for Kannada, English and Hindi text words of a printed document. The proposed approach is based on the horizontal and vertical projection profile for the discrimination of the three scripts. The feature extraction is done based on the horizontal projection profile of each text words. We analysed 700 different words of Kannada, English and Hindi in order to extract the discrimination features and for the development of knowledge base. We use the horizontal projection profile of each text word and based on the horizontal projection profile we extract the appropriate features. The proposed system is tested on 100 different document images containing more than 1000 text words of each script and a classification rate of 98.25%, 99.25% and 98.87% is achieved for Kannada, English and Hindi respectively. version:1
arxiv-1205-2151 | A Converged Algorithm for Tikhonov Regularized Nonnegative Matrix Factorization with Automatic Regularization Parameters Determination | http://arxiv.org/abs/1205.2151 | id:1205.2151 author:Andri Mirzal category:cs.LG  published:2012-05-10 summary:We present a converged algorithm for Tikhonov regularized nonnegative matrix factorization (NMF). We specially choose this regularization because it is known that Tikhonov regularized least square (LS) is the more preferable form in solving linear inverse problems than the conventional LS. Because an NMF problem can be decomposed into LS subproblems, it can be expected that Tikhonov regularized NMF will be the more appropriate approach in solving NMF problems. The algorithm is derived using additive update rules which have been shown to have convergence guarantee. We equip the algorithm with a mechanism to automatically determine the regularization parameters based on the L-curve, a well-known concept in the inverse problems community, but is rather unknown in the NMF research. The introduction of this algorithm thus solves two inherent problems in Tikhonov regularized NMF algorithm research, i.e., convergence guarantee and regularization parameters determination. version:1
arxiv-1007-3098 | Reduced Rank Vector Generalized Linear Models for Feature Extraction | http://arxiv.org/abs/1007.3098 | id:1007.3098 author:Yiyuan She category:stat.ML  published:2010-07-19 summary:Supervised linear feature extraction can be achieved by fitting a reduced rank multivariate model. This paper studies rank penalized and rank constrained vector generalized linear models. From the perspective of thresholding rules, we build a framework for fitting singular value penalized models and use it for feature extraction. Through solving the rank constraint form of the problem, we propose progressive feature space reduction for fast computation in high dimensions with little performance loss. A novel projective cross-validation is proposed for parameter tuning in such nonconvex setups. Real data applications are given to show the power of the methodology in supervised dimension reduction and feature extraction. version:3
arxiv-1205-2106 | Spatial Multiresolution Cluster Detection Method | http://arxiv.org/abs/1205.2106 | id:1205.2106 author:Lingsong Zhang, Zhengyuan Zhu category:stat.ME stat.CO stat.ML 62H11  published:2012-05-09 summary:A novel multi-resolution cluster detection (MCD) method is proposed to identify irregularly shaped clusters in space. Multi-scale test statistic on a single cell is derived based on likelihood ratio statistic for Bernoulli sequence, Poisson sequence and Normal sequence. A neighborhood variability measure is defined to select the optimal test threshold. The MCD method is compared with single scale testing methods controlling for false discovery rate and the spatial scan statistics using simulation and f-MRI data. The MCD method is shown to be more effective for discovering irregularly shaped clusters, and the implementation of this method does not require heavy computation, making it suitable for cluster detection for large spatial data. version:1
arxiv-1205-2599 | On the Identifiability of the Post-Nonlinear Causal Model | http://arxiv.org/abs/1205.2599 | id:1205.2599 author:Kun Zhang, Aapo Hyvarinen category:stat.ML cs.LG  published:2012-05-09 summary:By taking into account the nonlinear effect of the cause, the inner noise effect, and the measurement distortion effect in the observed variables, the post-nonlinear (PNL) causal model has demonstrated its excellent performance in distinguishing the cause from effect. However, its identifiability has not been properly addressed, and how to apply it in the case of more than two variables is also a problem. In this paper, we conduct a systematic investigation on its identifiability in the two-variable case. We show that this model is identifiable in most cases; by enumerating all possible situations in which the model is not identifiable, we provide sufficient conditions for its identifiability. Simulations are given to support the theoretical results. Moreover, in the case of more than two variables, we show that the whole causal structure can be found by applying the PNL causal model to each structure in the Markov equivalent class and testing if the disturbance is independent of the direct causes for each variable. In this way the exhaustive search over all possible causal structures is avoided. version:1
arxiv-1205-2600 | A Uniqueness Theorem for Clustering | http://arxiv.org/abs/1205.2600 | id:1205.2600 author:Reza Bosagh Zadeh, Shai Ben-David category:cs.LG  published:2012-05-09 summary:Despite the widespread use of Clustering, there is distressingly little general theory of clustering available. Questions like "What distinguishes a clustering of data from other data partitioning?", "Are there any principles governing all clustering paradigms?", "How should a user choose an appropriate clustering algorithm for a particular task?", etc. are almost completely unanswered by the existing body of clustering literature. We consider an axiomatic approach to the theory of Clustering. We adopt the framework of Kleinberg, [Kle03]. By relaxing one of Kleinberg's clustering axioms, we sidestep his impossibility result and arrive at a consistent set of axioms. We suggest to extend these axioms, aiming to provide an axiomatic taxonomy of clustering paradigms. Such a taxonomy should provide users some guidance concerning the choice of the appropriate clustering paradigm for a given task. The main result of this paper is a set of abstract properties that characterize the Single-Linkage clustering function. This characterization result provides new insight into the properties of desired data groupings that make Single-Linkage the appropriate choice. We conclude by considering a taxonomy of clustering functions based on abstract properties that each satisfies. version:1
arxiv-1205-2602 | The Entire Quantile Path of a Risk-Agnostic SVM Classifier | http://arxiv.org/abs/1205.2602 | id:1205.2602 author:Jin Yu, S. V. N. Vishwanatan, Jian Zhang category:cs.LG  published:2012-05-09 summary:A quantile binary classifier uses the rule: Classify x as +1 if P(Y = 1 X = x) >= t, and as -1 otherwise, for a fixed quantile parameter t {[0, 1]. It has been shown that Support Vector Machines (SVMs) in the limit are quantile classifiers with t = 1/2 . In this paper, we show that by using asymmetric cost of misclassification SVMs can be appropriately extended to recover, in the limit, the quantile binary classifier for any t. We then present a principled algorithm to solve the extended SVM classifier for all values of t simultaneously. This has two implications: First, one can recover the entire conditional distribution P(Y = 1 X = x) = t for t {[0, 1]. Second, we can build a risk-agnostic SVM classifier where the cost of misclassification need not be known apriori. Preliminary numerical experiments show the effectiveness of the proposed algorithm. version:1
arxiv-1205-2604 | The Infinite Latent Events Model | http://arxiv.org/abs/1205.2604 | id:1205.2604 author:David Wingate, Noah Goodman, Daniel Roy, Joshua Tenenbaum category:stat.ML cs.LG  published:2012-05-09 summary:We present the Infinite Latent Events Model, a nonparametric hierarchical Bayesian distribution over infinite dimensional Dynamic Bayesian Networks with binary state representations and noisy-OR-like transitions. The distribution can be used to learn structure in discrete timeseries data by simultaneously inferring a set of latent events, which events fired at each timestep, and how those events are causally linked. We illustrate the model on a sound factorization task, a network topology identification task, and a video game task. version:1
arxiv-1205-2605 | Herding Dynamic Weights for Partially Observed Random Field Models | http://arxiv.org/abs/1205.2605 | id:1205.2605 author:Max Welling category:cs.LG stat.ML  published:2012-05-09 summary:Learning the parameters of a (potentially partially observable) random field model is intractable in general. Instead of focussing on a single optimal parameter value we propose to treat parameters as dynamical quantities. We introduce an algorithm to generate complex dynamics for parameters and (both visible and hidden) state vectors. We show that under certain conditions averages computed over trajectories of the proposed dynamical system converge to averages computed over the data. Our "herding dynamics" does not require expensive operations such as exponentiation and is fully deterministic. version:1
arxiv-1205-2606 | Exploring compact reinforcement-learning representations with linear regression | http://arxiv.org/abs/1205.2606 | id:1205.2606 author:Thomas J. Walsh, Istvan Szita, Carlos Diuk, Michael L. Littman category:cs.LG cs.AI  published:2012-05-09 summary:This paper presents a new algorithm for online linear regression whose efficiency guarantees satisfy the requirements of the KWIK (Knows What It Knows) framework. The algorithm improves on the complexity bounds of the current state-of-the-art procedure in this setting. We explore several applications of this algorithm for learning compact reinforcement-learning representations. We show that KWIK linear regression can be used to learn the reward function of a factored MDP and the probabilities of action outcomes in Stochastic STRIPS and Object Oriented MDPs, none of which have been proven to be efficiently learnable in the RL setting before. We also combine KWIK linear regression with other KWIK learners to learn larger portions of these models, including experiments on learning factored MDP transition and reward functions together. version:1
arxiv-1205-2608 | Temporal-Difference Networks for Dynamical Systems with Continuous Observations and Actions | http://arxiv.org/abs/1205.2608 | id:1205.2608 author:Christopher M. Vigorito category:cs.LG stat.ML  published:2012-05-09 summary:Temporal-difference (TD) networks are a class of predictive state representations that use well-established TD methods to learn models of partially observable dynamical systems. Previous research with TD networks has dealt only with dynamical systems with finite sets of observations and actions. We present an algorithm for learning TD network representations of dynamical systems with continuous observations and actions. Our results show that the algorithm is capable of learning accurate and robust models of several noisy continuous dynamical systems. The algorithm presented here is the first fully incremental method for learning a predictive representation of a continuous dynamical system. version:1
arxiv-1205-2609 | Which Spatial Partition Trees are Adaptive to Intrinsic Dimension? | http://arxiv.org/abs/1205.2609 | id:1205.2609 author:Nakul Verma, Samory Kpotufe, Sanjoy Dasgupta category:stat.ML cs.LG  published:2012-05-09 summary:Recent theory work has found that a special type of spatial partition tree - called a random projection tree - is adaptive to the intrinsic dimension of the data from which it is built. Here we examine this same question, with a combination of theory and experiments, for a broader class of trees that includes k-d trees, dyadic trees, and PCA trees. Our motivation is to get a feel for (i) the kind of intrinsic low dimensional structure that can be empirically verified, (ii) the extent to which a spatial partition can exploit such structure, and (iii) the implications for standard statistical tasks such as regression, vector quantization, and nearest neighbor search. version:1
arxiv-1205-2610 | Probabilistic Structured Predictors | http://arxiv.org/abs/1205.2610 | id:1205.2610 author:Shankar Vembu, Thomas Gartner, Mario Boley category:cs.LG  published:2012-05-09 summary:We consider MAP estimators for structured prediction with exponential family models. In particular, we concentrate on the case that efficient algorithms for uniform sampling from the output space exist. We show that under this assumption (i) exact computation of the partition function remains a hard problem, and (ii) the partition function and the gradient of the log partition function can be approximated efficiently. Our main result is an approximation scheme for the partition function based on Markov Chain Monte Carlo theory. We also show that the efficient uniform sampling assumption holds in several application settings that are of importance in machine learning. version:1
arxiv-1205-2611 | Ordinal Boltzmann Machines for Collaborative Filtering | http://arxiv.org/abs/1205.2611 | id:1205.2611 author:Tran The Truyen, Dinh Q. Phung, Svetha Venkatesh category:cs.IR cs.LG  published:2012-05-09 summary:Collaborative filtering is an effective recommendation technique wherein the preference of an individual can potentially be predicted based on preferences of other members. Early algorithms often relied on the strong locality in the preference data, that is, it is enough to predict preference of a user on a particular item based on a small subset of other users with similar tastes or of other items with similar properties. More recently, dimensionality reduction techniques have proved to be equally competitive, and these are based on the co-occurrence patterns rather than locality. This paper explores and extends a probabilistic model known as Boltzmann Machine for collaborative filtering tasks. It seamlessly integrates both the similarity and co-occurrence in a principled manner. In particular, we study parameterisation options to deal with the ordinal nature of the preferences, and propose a joint modelling of both the user-based and item-based processes. Experiments on moderate and large-scale movie recommendation show that our framework rivals existing well-known methods. version:1
arxiv-1205-2612 | Computing Posterior Probabilities of Structural Features in Bayesian Networks | http://arxiv.org/abs/1205.2612 | id:1205.2612 author:Jin Tian, Ru He category:cs.LG stat.ML  published:2012-05-09 summary:We study the problem of learning Bayesian network structures from data. Koivisto and Sood (2004) and Koivisto (2006) presented algorithms that can compute the exact marginal posterior probability of a subnetwork, e.g., a single edge, in O(n2n) time and the posterior probabilities for all n(n-1) potential edges in O(n2n) total time, assuming that the number of parents per node or the indegree is bounded by a constant. One main drawback of their algorithms is the requirement of a special structure prior that is non uniform and does not respect Markov equivalence. In this paper, we develop an algorithm that can compute the exact posterior probability of a subnetwork in O(3n) time and the posterior probabilities for all n(n-1) potential edges in O(n3n) total time. Our algorithm also assumes a bounded indegree but allows general structure priors. We demonstrate the applicability of the algorithm on several data sets with up to 20 variables. version:1
arxiv-1205-2614 | Products of Hidden Markov Models: It Takes N>1 to Tango | http://arxiv.org/abs/1205.2614 | id:1205.2614 author:Graham W Taylor, Geoffrey E. Hinton category:cs.LG stat.ML  published:2012-05-09 summary:Products of Hidden Markov Models(PoHMMs) are an interesting class of generative models which have received little attention since their introduction. This maybe in part due to their more computationally expensive gradient-based learning algorithm,and the intractability of computing the log likelihood of sequences under the model. In this paper, we demonstrate how the partition function can be estimated reliably via Annealed Importance Sampling. We perform experiments using contrastive divergence learning on rainfall data and data captured from pairs of people dancing. Our results suggest that advances in learning and evaluation for undirected graphical models and recent increases in available computing power make PoHMMs worth considering for complex time-series modeling tasks. version:1
arxiv-1205-2617 | Modeling Discrete Interventional Data using Directed Cyclic Graphical Models | http://arxiv.org/abs/1205.2617 | id:1205.2617 author:Mark Schmidt, Kevin Murphy category:stat.ML cs.LG stat.ME  published:2012-05-09 summary:We outline a representation for discrete multivariate distributions in terms of interventional potential functions that are globally normalized. This representation can be used to model the effects of interventions, and the independence properties encoded in this model can be represented as a directed graph that allows cycles. In addition to discussing inference and sampling with this representation, we give an exponential family parametrization that allows parameter estimation to be stated as a convex optimization problem; we also give a convex relaxation of the task of simultaneous parameter and structure learning using group l1-regularization. The model is evaluated on simulated data and intracellular flow cytometry data. version:1
arxiv-1205-2618 | BPR: Bayesian Personalized Ranking from Implicit Feedback | http://arxiv.org/abs/1205.2618 | id:1205.2618 author:Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme category:cs.IR cs.LG stat.ML  published:2012-05-09 summary:Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion. version:1
arxiv-1205-2056 | Dynamic Behavioral Mixed-Membership Model for Large Evolving Networks | http://arxiv.org/abs/1205.2056 | id:1205.2056 author:Ryan Rossi, Brian Gallagher, Jennifer Neville, Keith Henderson category:cs.SI cs.LG physics.soc-ph stat.ML  published:2012-05-09 summary:The majority of real-world networks are dynamic and extremely large (e.g., Internet Traffic, Twitter, Facebook, ...). To understand the structural behavior of nodes in these large dynamic networks, it may be necessary to model the dynamics of behavioral roles representing the main connectivity patterns over time. In this paper, we propose a dynamic behavioral mixed-membership model (DBMM) that captures the roles of nodes in the graph and how they evolve over time. Unlike other node-centric models, our model is scalable for analyzing large dynamic networks. In addition, DBMM is flexible, parameter-free, has no functional form or parameterization, and is interpretable (identifies explainable patterns). The performance results indicate our approach can be applied to very large networks while the experimental results show that our model uncovers interesting patterns underlying the dynamics of these networks. version:1
arxiv-1205-2622 | Using the Gene Ontology Hierarchy when Predicting Gene Function | http://arxiv.org/abs/1205.2622 | id:1205.2622 author:Sara Mostafavi, Quaid Morris category:cs.LG cs.CE stat.ML  published:2012-05-09 summary:The problem of multilabel classification when the labels are related through a hierarchical categorization scheme occurs in many application domains such as computational biology. For example, this problem arises naturally when trying to automatically assign gene function using a controlled vocabularies like Gene Ontology. However, most existing approaches for predicting gene functions solve independent classification problems to predict genes that are involved in a given function category, independently of the rest. Here, we propose two simple methods for incorporating information about the hierarchical nature of the categorization scheme. In the first method, we use information about a gene's previous annotation to set an initial prior on its label. In a second approach, we extend a graph-based semi-supervised learning algorithm for predicting gene function in a hierarchy. We show that we can efficiently solve this problem by solving a linear system of equations. We compare these approaches with a previous label reconciliation-based approach. Results show that using the hierarchy information directly, compared to using reconciliation methods, improves gene function prediction. version:1
arxiv-1205-2623 | Virtual Vector Machine for Bayesian Online Classification | http://arxiv.org/abs/1205.2623 | id:1205.2623 author:Thomas P. Minka, Rongjing Xiang, Yuan, Qi category:cs.LG stat.ML  published:2012-05-09 summary:In a typical online learning scenario, a learner is required to process a large data stream using a small memory buffer. Such a requirement is usually in conflict with a learner's primary pursuit of prediction accuracy. To address this dilemma, we introduce a novel Bayesian online classi cation algorithm, called the Virtual Vector Machine. The virtual vector machine allows you to smoothly trade-off prediction accuracy with memory size. The virtual vector machine summarizes the information contained in the preceding data stream by a Gaussian distribution over the classi cation weights plus a constant number of virtual data points. The virtual data points are designed to add extra non-Gaussian information about the classi cation weights. To maintain the constant number of virtual points, the virtual vector machine adds the current real data point into the virtual point set, merges two most similar virtual points into a new virtual point or deletes a virtual point that is far from the decision boundary. The information lost in this process is absorbed into the Gaussian distribution. The extra information provided by the virtual points leads to improved predictive accuracy over previous online classification algorithms. version:1
arxiv-1205-2624 | Convexifying the Bethe Free Energy | http://arxiv.org/abs/1205.2624 | id:1205.2624 author:Ofer Meshi, Ariel Jaimovich, Amir Globerson, Nir Friedman category:cs.AI cs.LG  published:2012-05-09 summary:The introduction of loopy belief propagation (LBP) revitalized the application of graphical models in many domains. Many recent works present improvements on the basic LBP algorithm in an attempt to overcome convergence and local optima problems. Notable among these are convexified free energy approximations that lead to inference procedures with provable convergence and quality properties. However, empirically LBP still outperforms most of its convex variants in a variety of settings, as we also demonstrate here. Motivated by this fact we seek convexified free energies that directly approximate the Bethe free energy. We show that the proposed approximations compare favorably with state-of-the art convex free energy approximations. version:1
