arxiv-1706-00322 | TransFlow: Unsupervised Motion Flow by Joint Geometric and Pixel-level Estimation | http://arxiv.org/abs/1706.00322 | id:1706.00322 author:Stefano Alletto, Davide Abati, Simone Calderara, Rita Cucchiara, Luca Rigazio category:cs.CV  published:2017-06-01 summary:We address unsupervised optical flow estimation for ego-centric motion. We argue that optical flow can be cast as a geometrical warping between two successive video frames and devise a deep architecture to estimate such transformation in two stages. First, a dense pixel-level flow is computed with a geometric prior imposing strong spatial constraints. Such prior is typical of driving scenes, where the point of view is coherent with the vehicle motion. We show how such global transformation can be approximated with an homography and how spatial transformer layers can be employed to compute the flow field implied by such transformation. The second stage then refines the prediction feeding a second deeper network. A final reconstruction loss compares the warping of frame X(t) with the subsequent frame X(t+1) and guides both estimates. The model, which we named TransFlow, performs favorably compared to other unsupervised algorithms, and shows better generalization compared to supervised methods with a 3x reduction in error on unseen data. version:2
arxiv-1705-05950 | Kernel clustering: density biases and solutions | http://arxiv.org/abs/1705.05950 | id:1705.05950 author:Dmitrii Marin, Meng Tang, Ismail Ben Ayed, Yuri Boykov category:stat.ML  published:2017-05-16 summary:Clustering is widely used in data analysis where kernel methods are particularly popular due to their generality and discriminating power. However, kernel clustering has a practically significant bias to small dense clusters, e.g. empirically observed in (Shi and Malik, TPAMI'00). Its causes have never been analyzed and understood theoretically, even though many attempts were made to improve the results. We provide conditions and formally prove this bias in kernel clustering. Previously, (Breiman, ML'96) proved a bias to histogram mode isolation in discrete Gini criterion for decision tree learning. We found that kernel clustering reduces to a continuous generalization of Gini criterion for a common class of kernels where we prove a bias to density mode isolation and call it Breiman's bias. These theoretical findings suggest that a principal solution for the bias should directly address data density inhomogeneity. In particular, we show that density equalization can be implicitly achieved using either locally adaptive weights or a general class of Riemannian (geodesic) kernels. Our density equalization principle unifies many popular kernel clustering criteria including normalized cut, which we show has a bias to sparse subsets inversely related to Breiman's bias. Our synthetic and real data experiments illustrate these density biases and proposed solutions. We anticipate that theoretical understanding of kernel clustering limitations and their principled solutions will be important for a broad spectrum of data analysis applications across the disciplines. version:3
arxiv-1706-02613 | Decoupling "when to update" from "how to update" | http://arxiv.org/abs/1706.02613 | id:1706.02613 author:Eran Malach, Shai Shalev-Shwartz category:cs.LG  published:2017-06-08 summary:Deep learning requires data. A useful approach to obtain data is to be creative and mine data from various sources, that were created for different purposes. Unfortunately, this approach often leads to noisy labels. In this paper, we propose a meta algorithm for tackling the noisy labels problem. The key idea is to decouple "when to update" from "how to update". We demonstrate the effectiveness of our algorithm by mining data for gender classification by combining the Labeled Faces in the Wild (LFW) face recognition dataset with a textual genderizing service, which leads to a noisy dataset. While our approach is very simple to implement, it leads to state-of-the-art results. We analyze some convergence properties of the proposed algorithm. version:1
arxiv-1706-02609 | Spatio-Temporal Backpropagation for Training High-performance Spiking Neural Networks | http://arxiv.org/abs/1706.02609 | id:1706.02609 author:Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, Luping Shi category:cs.NE q-bio.NC stat.ML  published:2017-06-08 summary:Compared with artificial neural networks (ANNs), spiking neural networks (SNNs) are promising to explore the brain-like behaviors since the spikes could encode more spatio-temporal information. Although pre-training from ANN or direct training based on backpropagation (BP) makes the supervised training of SNNs possible, these methods only exploit the networks' spatial domain information which leads to the performance bottleneck and requires many complicated training skills. One fundamental issue is that the spike activity is naturally non-differentiable which causes great difficulties in training SNNs. To this end, we build an iterative LIF model that is more friendly for gradient descent training. By simultaneously considering the layer-by-layer spatial domain (SD) and the timing-dependent temporal domain (TD) in the training phase, as well as an approximated derivative for the spike activity, we propose a spatio-temporal backpropagation (STBP) training framework without using any complicated technology. We achieve the best performance of multi-layered perceptron (MLP) compared with existing state-of-the-art algorithms over the static MNIST and the dynamic N-MNIST dataset as well as a custom object detection dataset. This work provides a new perspective to explore the high-performance SNNs for future brain-like computing paradigm with rich spatio-temporal dynamics. version:1
arxiv-1706-02596 | Reading Twice for Natural Language Understanding | http://arxiv.org/abs/1706.02596 | id:1706.02596 author:Dirk Weissenborn category:cs.CL cs.AI cs.NE  published:2017-06-08 summary:Despite the recent success of neural networks in tasks involving natural language understanding (NLU) there has only been limited progress in some of the fundamental challenges of NLU, such as the disambiguation of the meaning and function of words in context. This work approaches this problem by incorporating contextual information into word representations prior to processing the task at hand. To this end we propose a general-purpose reading architecture that is employed prior to a task-specific NLU model. It is responsible for refining context-agnostic word representations with contextual information and lends itself to the introduction of additional, context-relevant information from external knowledge sources. We demonstrate that previously non-competitive models benefit dramatically from employing contextual representations, closing the gap between general-purpose reading architectures and the state-of-the-art performance obtained with fine-tuned, task-specific architectures. Apart from our empirical results we present a comprehensive analysis of the computed representations which gives insights into the kind of information added during the refinement process. version:1
arxiv-1706-02586 | DSOS and SDSOS Optimization: More Tractable Alternatives to Sum of Squares and Semidefinite Optimization | http://arxiv.org/abs/1706.02586 | id:1706.02586 author:Amir Ali Ahmadi, Anirudha Majumdar category:math.OC cs.DS cs.SY stat.ML  published:2017-06-08 summary:In recent years, optimization theory has been greatly impacted by the advent of sum of squares (SOS) optimization. The reliance of this technique on large-scale semidefinite programs however, has limited the scale of problems to which it can be applied. In this paper, we introduce DSOS and SDSOS optimization as more tractable alternatives to sum of squares optimization that rely instead on linear and second order cone programs respectively. These are optimization problems over certain subsets of sum of squares polynomials (or equivalently subsets of positive semidefinite matrices), which can be of interest in general applications of semidefinite programming where scalability is a limitation. We show that some basic theorems from SOS optimization which rely on results from real algebraic geometry are still valid for DSOS and SDSOS optimization. Furthermore, we show with numerical experiments from diverse application areas---polynomial optimization, statistics and machine learning, derivative pricing, and control theory---that with reasonable tradeoffs in accuracy, we can handle problems at scales that are currently far beyond the reach of sum of squares approaches. Finally, we provide a review of recent techniques that bridge the gap between our DSOS/SDSOS approach and the SOS approach at the expense of additional running time. The appendix of the paper introduces an accompanying MATLAB package for DSOS and SDSOS optimization. version:1
arxiv-1706-02582 | Clustering with t-SNE, provably | http://arxiv.org/abs/1706.02582 | id:1706.02582 author:George C. Linderman, Stefan Steinerberger category:cs.LG stat.ML  published:2017-06-08 summary:t-distributed Stochastic Neighborhood Embedding (t-SNE), a clustering and visualization method proposed by van der Maaten & Hinton in 2008, has rapidly become a standard tool in a number of natural sciences. Despite its overwhelming success, there is a distinct lack of mathematical foundations and the inner workings of the algorithm are not well understood. The purpose of this paper is to prove that t-SNE is able to recover well-separated clusters; more precisely, we prove that t-SNE in the `early exaggeration' phase, an optimization technique proposed by van der Maaten & Hinton (2008) and van der Maaten (2014), can be rigorously analyzed. As a byproduct, the proof suggests novel ways for setting the exaggeration parameter $\alpha$ and step size $h$. Numerical examples illustrate the effectiveness of these rules: in particular, the quality of embedding of topological structures (e.g. the swiss roll) improves. We also discuss a connection to spectral clustering methods. version:1
arxiv-1706-02577 | ToxTrac: a fast and robust software for tracking organisms | http://arxiv.org/abs/1706.02577 | id:1706.02577 author:Alvaro Rodriquez, Hanqing Zhang, Jonatan Klaminder, Tomas Brodin, Patrik L. Andersson, Magnus Andersson category:cs.CV  published:2017-06-08 summary:1. Behavioral analysis based on video recording is becoming increasingly popular within research fields such as; ecology, medicine, ecotoxicology, and toxicology. However, the programs available to analyze the data, which are; free of cost, user-friendly, versatile, robust, fast and provide reliable statistics for different organisms (invertebrates, vertebrates and mammals) are significantly limited. 2. We present an automated open-source executable software (ToxTrac) for image-based tracking that can simultaneously handle several organisms monitored in a laboratory environment. We compare the performance of ToxTrac with current accessible programs on the web. 3. The main advantages of ToxTrac are: i) no specific knowledge of the geometry of the tracked bodies is needed; ii) processing speed, ToxTrac can operate at a rate >25 frames per second in HD videos using modern desktop computers; iii) simultaneous tracking of multiple organisms in multiple arenas; iv) integrated distortion correction and camera calibration; v) robust against false positives; vi) preservation of individual identification if crossing occurs; vii) useful statistics and heat maps in real scale are exported in: image, text and excel formats. 4. ToxTrac can be used for high speed tracking of insects, fish, rodents or other species, and provides useful locomotor information. We suggest using ToxTrac for future studies of animal behavior independent of research area. Download ToxTrac here: https://toxtrac.sourceforge.io version:1
arxiv-1706-02562 | Pain-Free Random Differential Privacy with Sensitivity Sampling | http://arxiv.org/abs/1706.02562 | id:1706.02562 author:Benjamin I. P. Rubinstein, Francesco Aldà category:cs.LG cs.CR cs.DB stat.ML  published:2017-06-08 summary:Popular approaches to differential privacy, such as the Laplace and exponential mechanisms, calibrate randomised smoothing through global sensitivity of the target non-private function. Bounding such sensitivity is often a prohibitively complex analytic calculation. As an alternative, we propose a straightforward sampler for estimating sensitivity of non-private mechanisms. Since our sensitivity estimates hold with high probability, any mechanism that would be $(\epsilon,\delta)$-differentially private under bounded global sensitivity automatically achieves $(\epsilon,\delta,\gamma)$-random differential privacy (Hall et al., 2012), without any target-specific calculations required. We demonstrate on worked example learners how our usable approach adopts a naturally-relaxed privacy guarantee, while achieving more accurate releases even for non-private functions that are black-box computer programs. version:1
arxiv-1706-02556 | Surprise Search for Evolutionary Divergence | http://arxiv.org/abs/1706.02556 | id:1706.02556 author:Daniele Gravina, Antonios Liapis, Georgios N. Yannakakis category:cs.NE  published:2017-06-08 summary:Inspired by the notion of surprise for unconventional discovery we introduce a general search algorithm we name surprise search as a new method of evolutionary divergent search. Surprise search is grounded in the divergent search paradigm and is fabricated within the principles of evolutionary search. The algorithm mimics the self-surprise cognitive process and equips evolutionary search with the ability to seek for solutions that deviate from the algorithm's expected behaviour. The predictive model of expected solutions is based on historical trails of where the search has been and local information about the search space. Surprise search is tested extensively in a robot maze navigation task: experiments are held in four authored deceptive mazes and in 60 generated mazes and compared against objective-based evolutionary search and novelty search. The key findings of this study reveal that surprise search is advantageous compared to the other two search processes. In particular, it outperforms objective search and it is as efficient as novelty search in all tasks examined. Most importantly, surprise search is faster, on average, and more robust in solving the navigation problem compared to any other algorithm examined. Finally, our analysis reveals that surprise search explores the behavioural space more extensively and yields higher population diversity compared to novelty search. What distinguishes surprise search from other forms of divergent search, such as the search for novelty, is its ability to diverge not from earlier and seen solutions but rather from predicted and unseen points in the domain considered. version:1
arxiv-1706-02551 | The Algorithmic Inflection of Russian and Generation of Grammatically Correct Text | http://arxiv.org/abs/1706.02551 | id:1706.02551 author:T. M. Sadykov, T. A. Zhukov category:cs.CL 68T35  published:2017-06-08 summary:We present a deterministic algorithm for Russian inflection. This algorithm is implemented in a publicly available web-service www.passare.ru which provides functions for inflection of single words, word matching and synthesis of grammatically correct Russian text. The inflectional functions have been tested against the annotated corpus of Russian language OpenCorpora. version:1
arxiv-1705-05690 | A Long Short-Term Memory Recurrent Neural Network Framework for Network Traffic Matrix Prediction | http://arxiv.org/abs/1705.05690 | id:1705.05690 author:Abdelhadi Azzouni, Guy Pujolle category:cs.NI cs.LG  published:2017-05-16 summary:Network Traffic Matrix (TM) prediction is defined as the problem of estimating future network traffic from the previous and achieved network traffic data. It is widely used in network planning, resource management and network security. Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that is well-suited to learn from experience to classify, process and predict time series with time lags of unknown size. LSTMs have been shown to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we propose a LSTM RNN framework for predicting short and long term Traffic Matrix (TM) in large networks. By validating our framework on real-world data from GEANT network, we show that our LSTM models converge quickly and give state of the art TM prediction performance for relatively small sized models. version:3
arxiv-1706-02524 | Scaling up the Automatic Statistician: Scalable Structure Discovery using Gaussian Processes | http://arxiv.org/abs/1706.02524 | id:1706.02524 author:Hyunjik Kim, Yee Whye Teh category:stat.ML cs.LG  published:2017-06-08 summary:Automating statistical modelling is a challenging problem that has far-reaching implications for artificial intelligence. The Automatic Statistician employs a kernel search algorithm to provide a first step in this direction for regression problems. However this does not scale due to its $O(N^3)$ running time for the model selection. This is undesirable not only because the average size of data sets is growing fast, but also because there is potentially more information in bigger data, implying a greater need for more expressive models that can discover finer structure. We propose Scalable Kernel Composition (SKC), a scalable kernel search algorithm, to encompass big data within the boundaries of automated statistical modelling. version:1
arxiv-1706-02515 | Self-Normalizing Neural Networks | http://arxiv.org/abs/1706.02515 | id:1706.02515 author:Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter category:cs.LG stat.ML  published:2017-06-08 summary:Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs. version:1
arxiv-1706-02501 | Unlocking the Potential of Simulators: Design with RL in Mind | http://arxiv.org/abs/1706.02501 | id:1706.02501 author:Rika Antonova, Silvia Cruciani category:cs.LG cs.RO  published:2017-06-08 summary:Using Reinforcement Learning (RL) in simulation to construct policies useful in real life is challenging. This is often attributed to the sequential decision making aspect: inaccuracies in simulation accumulate over multiple steps, hence the simulated trajectories diverge from what would happen in reality. In our work we show the need to consider another important aspect: the mismatch in simulating control. We bring attention to the need for modeling control as well as dynamics, since oversimplifying assumptions about applying actions of RL policies could make the policies fail on real-world systems. We design a simulator for solving a pivoting task (of interest in Robotics) and demonstrate that even a simple simulator designed with RL in mind outperforms high-fidelity simulators when it comes to learning a policy that is to be deployed on a real robotic system. We show that a phenomenon that is hard to model - friction - could be exploited successfully, even when RL is performed using a simulator with a simple dynamics and noise model. Hence, we demonstrate that as long as the main sources of uncertainty are identified, it could be possible to learn policies applicable to real systems even using a simple simulator. RL-compatible simulators could open the possibilities for applying a wide range of RL algorithms in various fields. This is important, since currently data sparsity in fields like healthcare and education frequently forces researchers and engineers to only consider sample-efficient RL approaches. Successful simulator-aided RL could increase flexibility of experimenting with RL algorithms and help applying RL policies to real-world settings in fields where data is scarce. We believe that lessons learned in Robotics could help other fields design RL-compatible simulators, so we summarize our experience and conclude with suggestions. version:1
arxiv-1706-02949 | K+ Means : An Enhancement Over K-Means Clustering Algorithm | http://arxiv.org/abs/1706.02949 | id:1706.02949 author:Srikanta Kolay, Kumar Sankar Ray category:cs.LG  published:2017-06-08 summary:K-means (MacQueen, 1967) [1] is one of the simplest unsupervised learning algorithms that solve the well-known clustering problem. The procedure follows a simple and easy way to classify a given data set to a predefined, say K number of clusters. Determination of K is a difficult job and it is not known that which value of K can partition the objects as per our intuition. To overcome this problem we proposed K+ Means algorithm. This algorithm is an enhancement over K-Means algorithm. version:1
arxiv-1706-02496 | Context encoders as a simple but powerful extension of word2vec | http://arxiv.org/abs/1706.02496 | id:1706.02496 author:Franziska Horn category:stat.ML cs.CL cs.LG  published:2017-06-08 summary:With a simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today. However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings. Additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word's average context vector, out-of-vocabulary (OOV) embeddings and representations for a word with multiple meanings can be created based on the word's local contexts. The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task. version:1
arxiv-1706-02495 | The Generalized Cross Validation Filter | http://arxiv.org/abs/1706.02495 | id:1706.02495 author:Giulio Bottegal, Gianluigi Pillonetto category:stat.ML cs.SY  published:2017-06-08 summary:Generalized cross validation (GCV) is one of the most important approaches used to estimate parameters in the context of inverse problems and regularization techniques. A notable example is the determination of the smoothness parameter in splines. When the data are generated by a state space model, like in the spline case, efficient algorithms are available to evaluate the GCV score with complexity that scales linearly in the data set size. However, these methods are not amenable to on-line applications since they rely on forward and backward recursions. Hence, if the objective has been evaluated at time $t-1$ and new data arrive at time t, then O(t) operations are needed to update the GCV score. In this paper we instead show that the update cost is $O(1)$, thus paving the way to the on-line use of GCV. This result is obtained by deriving the novel GCV filter which extends the classical Kalman filter equations to efficiently propagate the GCV score over time. We also illustrate applications of the new filter in the context of state estimation and on-line regularized linear system identification. version:1
arxiv-1706-02492 | Consistency Results for Stationary Autoregressive Processes with Constrained Coefficients | http://arxiv.org/abs/1706.02492 | id:1706.02492 author:Alessio Sancetta category:stat.ML  published:2017-06-08 summary:We consider stationary autoregressive processes with coefficients restricted to an ellipsoid, which includes autoregressive processes with absolutely summable coefficients. We provide consistency results under different norms for the estimation of such processes using constrained and penalized estimators. As an application we show some weak form of universal consistency. Simulations show that directly including the constraint in the estimation can lead to more robust results. version:1
arxiv-1706-02490 | Where is my forearm? Clustering of body parts from simultaneous tactile and linguistic input using sequential mapping | http://arxiv.org/abs/1706.02490 | id:1706.02490 author:Karla Stepanova, Matej Hoffmann, Zdenek Straka, Frederico B. Klein, Angelo Cangelosi, Michal Vavrecka category:cs.NE cs.AI cs.CL cs.LG cs.RO  published:2017-06-08 summary:Humans and animals are constantly exposed to a continuous stream of sensory information from different modalities. At the same time, they form more compressed representations like concepts or symbols. In species that use language, this process is further structured by this interaction, where a mapping between the sensorimotor concepts and linguistic elements needs to be established. There is evidence that children might be learning language by simply disambiguating potential meanings based on multiple exposures to utterances in different contexts (cross-situational learning). In existing models, the mapping between modalities is usually found in a single step by directly using frequencies of referent and meaning co-occurrences. In this paper, we present an extension of this one-step mapping and introduce a newly proposed sequential mapping algorithm together with a publicly available Matlab implementation. For demonstration, we have chosen a less typical scenario: instead of learning to associate objects with their names, we focus on body representations. A humanoid robot is receiving tactile stimulations on its body, while at the same time listening to utterances of the body part names (e.g., hand, forearm and torso). With the goal at arriving at the correct "body categories", we demonstrate how a sequential mapping algorithm outperforms one-step mapping. In addition, the effect of data set size and noise in the linguistic input are studied. version:1
arxiv-1706-02179 | Learning to Represent Mechanics via Long-term Extrapolation and Interpolation | http://arxiv.org/abs/1706.02179 | id:1706.02179 author:Sébastien Ehrhardt, Aron Monszpart, Andrea Vedaldi, Niloy Mitra category:cs.CV cs.AI  published:2017-06-06 summary:While the basic laws of Newtonian mechanics are well understood, explaining a physical scenario still requires manually modeling the problem with suitable equations and associated parameters. In order to adopt such models for artificial intelligence, researchers have handcrafted the relevant states, and then used neural networks to learn the state transitions using simulation runs as training data. Unfortunately, such approaches can be unsuitable for modeling complex real-world scenarios, where manually authoring relevant state spaces tend to be challenging. In this work, we investigate if neural networks can implicitly learn physical states of real-world mechanical processes only based on visual data, and thus enable long-term physical extrapolation. We develop a recurrent neural network architecture for this task and also characterize resultant uncertainties in the form of evolving variance estimates. We evaluate our setup to extrapolate motion of a rolling ball on bowl of varying shape and orientation using only images as input, and report competitive results with approaches that assume access to internal physics models and parameters. version:2
arxiv-1706-02480 | Forward Thinking: Building and Training Neural Networks One Layer at a Time | http://arxiv.org/abs/1706.02480 | id:1706.02480 author:Chris Hettinger, Tanner Christensen, Ben Ehlert, Jeffrey Humpherys, Tyler Jarvis, Sean Wade category:stat.ML cs.LG  published:2017-06-08 summary:We present a general framework for training deep neural networks without backpropagation. This substantially decreases training time and also allows for construction of deep networks with many sorts of learners, including networks whose layers are defined by functions that are not easily differentiated, like decision trees. The main idea is that layers can be trained one at a time, and once they are trained, the input data are mapped forward through the layer to create a new learning problem. The process is repeated, transforming the data through multiple layers, one at a time, rendering a new data set, which is expected to be better behaved, and on which a final output layer can achieve good performance. We call this forward thinking and demonstrate a proof of concept by achieving state-of-the-art accuracy on the MNIST dataset for convolutional neural networks. We also provide a general mathematical formulation of forward thinking that allows for other types of deep learning problems to be considered. version:1
arxiv-1706-02471 | Distribution-Free One-Pass Learning | http://arxiv.org/abs/1706.02471 | id:1706.02471 author:Peng Zhao, Zhi-Hua Zhou category:cs.LG stat.ML  published:2017-06-08 summary:In many large-scale machine learning applications, data are accumulated with time, and thus, an appropriate model should be able to update in an online paradigm. Moreover, as the whole data volume is unknown when constructing the model, it is desired to scan each data item only once with a storage independent with the data volume. It is also noteworthy that the distribution underlying may change during the data accumulation procedure. To handle such tasks, in this paper we propose DFOP, a distribution-free one-pass learning approach. This approach works well when distribution change occurs during data accumulation, without requiring prior knowledge about the change. Every data item can be discarded once it has been scanned. Besides, theoretical guarantee shows that the estimate error, under a mild assumption, decreases until convergence with high probability. The performance of DFOP for both regression and classification are validated in experiments. version:1
arxiv-1706-02459 | Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization | http://arxiv.org/abs/1706.02459 | id:1706.02459 author:Shuming Ma, Xu Sun, Jingjing Xu, Houfeng Wang, Wenjie Li, Qi Su category:cs.CL  published:2017-06-08 summary:Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus. version:1
arxiv-1706-02447 | Luck is Hard to Beat: The Difficulty of Sports Prediction | http://arxiv.org/abs/1706.02447 | id:1706.02447 author:Raquel YS Aoki, Renato M Assuncao, Pedro OS Vaz de Melo category:cs.LG stat.ML  published:2017-06-08 summary:Predicting the outcome of sports events is a hard task. We quantify this difficulty with a coefficient that measures the distance between the observed final results of sports leagues and idealized perfectly balanced competitions in terms of skill. This indicates the relative presence of luck and skill. We collected and analyzed all games from 198 sports leagues comprising 1503 seasons from 84 countries of 4 different sports: basketball, soccer, volleyball and handball. We measured the competitiveness by countries and sports. We also identify in each season which teams, if removed from its league, result in a completely random tournament. Surprisingly, not many of them are needed. As another contribution of this paper, we propose a probabilistic graphical model to learn about the teams' skills and to decompose the relative weights of luck and skill in each game. We break down the skill component into factors associated with the teams' characteristics. The model also allows to estimate as 0.36 the probability that an underdog team wins in the NBA league, with a home advantage adding 0.09 to this probability. As shown in the first part of the paper, luck is substantially present even in the most competitive championships, which partially explains why sophisticated and complex feature-based models hardly beat simple models in the task of forecasting sports' outcomes. version:1
arxiv-1706-02444 | Predictive Coding-based Deep Dynamic Neural Network for Visuomotor Learning | http://arxiv.org/abs/1706.02444 | id:1706.02444 author:Jungsik Hwang, Jinhyung Kim, Ahmadreza Ahmadi, Minkyu Choi, Jun Tani category:cs.AI cs.LG cs.RO q-bio.NC  published:2017-06-08 summary:This study presents a dynamic neural network model based on the predictive coding framework for perceiving and predicting the dynamic visuo-proprioceptive patterns. In our previous study [1], we have shown that the deep dynamic neural network model was able to coordinate visual perception and action generation in a seamless manner. In the current study, we extended the previous model under the predictive coding framework to endow the model with a capability of perceiving and predicting dynamic visuo-proprioceptive patterns as well as a capability of inferring intention behind the perceived visuomotor information through minimizing prediction error. A set of synthetic experiments were conducted in which a robot learned to imitate the gestures of another robot in a simulation environment. The experimental results showed that with given intention states, the model was able to mentally simulate the possible incoming dynamic visuo-proprioceptive patterns in a top-down process without the inputs from the external environment. Moreover, the results highlighted the role of minimizing prediction error in inferring underlying intention of the perceived visuo-proprioceptive patterns, supporting the predictive coding account of the mirror neuron systems. The results also revealed that minimizing prediction error in one modality induced the recall of the corresponding representation of another modality acquired during the consolidative learning of raw-level visuo-proprioceptive patterns. version:1
arxiv-1706-02909 | Deriving a Representative Vector for Ontology Classes with Instance Word Vector Embeddings | http://arxiv.org/abs/1706.02909 | id:1706.02909 author:Vindula Jayawardana, Dimuthu Lakmal, Nisansa de Silva, Amal Shehan Perera, Keet Sugathadasa, Buddhi Ayesha category:cs.CL  published:2017-06-08 summary:Selecting a representative vector for a set of vectors is a very common requirement in many algorithmic tasks. Traditionally, the mean or median vector is selected. Ontology classes are sets of homogeneous instance objects that can be converted to a vector space by word vector embeddings. This study proposes a methodology to derive a representative vector for ontology classes whose instances were converted to the vector space. We start by deriving five candidate vectors which are then used to train a machine learning model that would calculate a representative vector for the class. We show that our methodology out-performs the traditional mean and median vector representations. version:1
arxiv-1706-02434 | Automatic tracking of vessel-like structures from a single starting point | http://arxiv.org/abs/1706.02434 | id:1706.02434 author:Dario Augusto Borges Oliveira, Laura Leal-Taixe, Raul Queiroz Feitosa, Bodo Rosenhahn category:cs.CV  published:2017-06-08 summary:The identification of vascular networks is an important topic in the medical image analysis community. While most methods focus on single vessel tracking, the few solutions that exist for tracking complete vascular networks are usually computationally intensive and require a lot of user interaction. In this paper we present a method to track full vascular networks iteratively using a single starting point. Our approach is based on a cloud of sampling points distributed over concentric spherical layers. We also proposed a vessel model and a metric of how well a sample point fits this model. Then, we implement the network tracking as a min-cost flow problem, and propose a novel optimization scheme to iteratively track the vessel structure by inherently handling bifurcations and paths. The method was tested using both synthetic and real images. On the 9 different data-sets of synthetic blood vessels, we achieved maximum accuracies of more than 98\%. We further use the synthetic data-set to analyse the sensibility of our method to parameter setting, showing the robustness of the proposed algorithm. For real images, we used coronary, carotid and pulmonary data to segment vascular structures and present the visual results. Still for real images, we present numerical and visual results for networks of nerve fibers in the olfactory system. Further visual results also show the potential of our approach for identifying vascular networks topologies. The presented method delivers good results for the several different datasets tested and have potential for segmenting vessel-like structures. Also, the topology information, inherently extracted, can be used for further analysis to computed aided diagnosis and surgical planning. Finally, the method's modular aspect holds potential for problem-oriented adjustments and improvements. version:1
arxiv-1706-00550 | On Unifying Deep Generative Models | http://arxiv.org/abs/1706.00550 | id:1706.00550 author:Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Eric P. Xing category:cs.LG stat.ML  published:2017-06-02 summary:Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent study respectively. This paper establishes formal connections between deep generative modeling approaches through a new formulation of GANs and VAEs. We show that GANs and VAEs are essentially minimizing KL divergences with opposite directions and reversed latent/visible treatments, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to exchange ideas across research lines in a principled way. For example, we transfer the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism. Quantitative experiments show generality and effectiveness of the imported extensions. version:2
arxiv-1706-02430 | Image Captioning with Object Detection and Localization | http://arxiv.org/abs/1706.02430 | id:1706.02430 author:Zhongliang Yang, Yu-Jin Zhang, Sadaqat ur Rehman, Yongfeng Huang category:cs.CV  published:2017-06-08 summary:Automatically generating a natural language description of an image is a task close to the heart of image understanding. In this paper, we present a multi-model neural network method closely related to the human visual system that automatically learns to describe the content of images. Our model consists of two sub-models: an object detection and localization model, which extract the information of objects and their spatial relationship in images respectively; Besides, a deep recurrent neural network (RNN) based on long short-term memory (LSTM) units with attention mechanism for sentences generation. Each word of the description will be automatically aligned to different objects of the input image when it is generated. This is similar to the attention mechanism of the human visual system. Experimental results on the COCO dataset showcase the merit of the proposed method, which outperforms previous benchmark models. version:1
arxiv-1706-02427 | Content-Based Table Retrieval for Web Queries | http://arxiv.org/abs/1706.02427 | id:1706.02427 author:Zhao Yan, Duyu Tang, Nan Duan, Junwei Bao, Yuanhua Lv, Ming Zhou, Zhoujun Li category:cs.CL  published:2017-06-08 summary:Understanding the connections between unstructured text and semi-structured table is an important yet neglected problem in natural language processing. In this work, we focus on content-based table retrieval. Given a query, the task is to find the most relevant table from a collection of tables. Further progress towards improving this area requires powerful models of semantic matching and richer training and evaluation resources. To remedy this, we present a ranking based approach, and implement both carefully designed features and neural network architectures to measure the relevance between a query and the content of a table. Furthermore, we release an open-domain dataset that includes 21,113 web queries for 273,816 tables. We conduct comprehensive experiments on both real world and synthetic datasets. Results verify the effectiveness of our approach and present the challenges for this task. version:1
arxiv-1706-02425 | C-arm Tomographic Imaging Technique for Nephrolithiasis and Detection of Kidney Stones | http://arxiv.org/abs/1706.02425 | id:1706.02425 author:Nuhad A. Malalla, Ying Chen category:cs.CV  published:2017-06-08 summary:In this paper, we investigated a C-arm tomographic technique as a new three dimensional (3D) kidney imaging method for nephrolithiasis and kidney stone detection over view angle less than 180o. Our C-arm tomographic technique provides a series of two dimensional (2D) images with a single scan over 40o view angle. Experimental studies were performed with a kidney phantom that was formed from a pig kidney with two embedded kidney stones. Different reconstruction methods were developed for C-arm tomographic technique to generate 3D kidney information including: point by point back projection (BP), filtered back projection (FBP), simultaneous algebraic reconstruction technique (SART) and maximum likelihood expectation maximization (MLEM). Computer simulation study was also done with simulated 3D spherical object to evaluate the reconstruction results. Preliminary results demonstrated the capability of our C-arm tomographic technique to generate 3D kidney information for kidney stone detection with low exposure of radiation. The kidney stones are visible on reconstructed planes with identifiable shapes and sizes. version:1
arxiv-1706-02423 | Seamless Integration and Coordination of Cognitive Skills in Humanoid Robots: A Deep Learning Approach | http://arxiv.org/abs/1706.02423 | id:1706.02423 author:Jungsik Hwang, Jun Tani category:cs.AI cs.LG cs.RO  published:2017-06-08 summary:This study investigates how adequate coordination among the different cognitive processes of a humanoid robot can be developed through end-to-end learning of direct perception of visuomotor stream. We propose a deep dynamic neural network model built on a dynamic vision network, a motor generation network, and a higher-level network. The proposed model was designed to process and to integrate direct perception of dynamic visuomotor patterns in a hierarchical model characterized by different spatial and temporal constraints imposed on each level. We conducted synthetic robotic experiments in which a robot learned to read human's intention through observing the gestures and then to generate the corresponding goal-directed actions. Results verify that the proposed model is able to learn the tutored skills and to generalize them to novel situations. The model showed synergic coordination of perception, action and decision making, and it integrated and coordinated a set of cognitive skills including visual perception, intention reading, attention switching, working memory, action preparation and execution in a seamless manner. Analysis reveals that coherent internal representations emerged at each level of the hierarchy. Higher-level representation reflecting actional intention developed by means of continuous integration of the lower-level visuo-proprioceptive stream. version:1
arxiv-1706-02419 | Estimating Mixture Entropy with Pairwise Distances | http://arxiv.org/abs/1706.02419 | id:1706.02419 author:Artemy Kolchinsky, Brendan D. Tracey category:cs.IT math.IT stat.ME stat.ML  published:2017-06-08 summary:Mixture distributions arise in many parametric and non-parametric settings, for example in Gaussian mixture models and in non-parametric estimation. It is often necessary to compute the entropy of a mixture, but in most cases this quantity has no closed-form expression, making some form of approximation necessary. We propose a family of estimators based on a pairwise distance function between mixture components, and show that this estimator class has many attractive properties. For many distributions of interest, the proposed estimators are efficient to compute, differentiable in the mixture parameters, and become exact when the mixture components are clustered. We prove this family includes lower and upper bounds on the mixture entropy. The Chernoff $\alpha$-divergence gives a lower bound when chosen as the distance function, with the Bhattacharyaa distance providing the tightest lower bound for components that are symmetric and members of a location family. The Kullback-Leibler divergence gives an upper bound when used as the distance function. We provide closed-form expressions of these bounds for mixtures of Gaussians, and discuss their applications to the estimation of mutual information. We then demonstrate that our bounds are significantly tighter than well-known existing bounds using numeric simulations. This pairwise estimator class is very useful in optimization problems involving maximization/minimization of entropy and mutual information, such as MaxEnt and rate distortion problems. version:1
arxiv-1705-08881 | Dense Transformer Networks | http://arxiv.org/abs/1705.08881 | id:1705.08881 author:Jun Li, Yongjun Chen, Lei Cai, Ian Davidson, Shuiwang Ji category:cs.CV cs.LG cs.NE stat.ML  published:2017-05-24 summary:The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on natural and biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods. version:2
arxiv-1706-02417 | Leveraging deep neural networks to capture psychological representations | http://arxiv.org/abs/1706.02417 | id:1706.02417 author:Joshua C. Peterson, Joshua T. Abbott, Thomas L. Griffiths category:cs.CV  published:2017-06-08 summary:Artificial neural networks have seen a recent surge in popularity for their ability to solve complex problems as well as or better than humans. In computer vision, deep convolutional neural networks have become the standard for object classification and image understanding due to their ability to learn efficient representations of high-dimensional data. However, the relationship between these representations and human psychological representations has remained unclear. Here we evaluate the quantitative and qualitative nature of this correspondence. We find that state-of-the-art object classification networks provide a reasonable first approximation to human similarity judgments, but fail to capture some of the structure of psychological representations. We show that a simple transformation that corrects these discrepancies can be obtained through convex optimization. Such representations provide a tool that can be used to study human performance on complex tasks with naturalistic stimuli, such as predicting the difficulty of learning novel categories. Our results extend the scope of psychological experiments and computational modeling of cognition by enabling tractable use of large natural stimulus sets. version:1
arxiv-1706-02416 | Generalized Value Iteration Networks: Life Beyond Lattices | http://arxiv.org/abs/1706.02416 | id:1706.02416 author:Sufeng Niu, Siheng Chen, Hanyu Guo, Colin Targonski, Melissa C. Smith, Jelena Kovačević category:cs.LG cs.AI  published:2017-06-08 summary:In this paper, we introduce a generalized value iteration network (GVIN), which is an end-to-end neural network planning module. GVIN emulates the value iteration algorithm by using a novel graph convolution operator, which enables GVIN to learn and plan on irregular spatial graphs. We propose three novel differentiable kernels as graph convolution operators and show that the embedding based kernel achieves the best performance. We further propose episodic Q-learning, an improvement upon traditional n-step Q-learning that stabilizes training for networks that contain a planning module. Lastly, we evaluate GVIN on planning problems in 2D mazes, irregular graphs, and real-world street networks, showing that GVIN generalizes well for both arbitrary graphs and unseen graphs of larger scale and outperforms a naive generalization of VIN (discretizing a spatial graph into a 2D image). version:1
arxiv-1706-02413 | PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space | http://arxiv.org/abs/1706.02413 | id:1706.02413 author:Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas category:cs.CV  published:2017-06-07 summary:Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds. version:1
arxiv-1706-02412 | Outlier Detection Using Distributionally Robust Optimization under the Wasserstein Metric | http://arxiv.org/abs/1706.02412 | id:1706.02412 author:Ruidi Chen, Ioannis Ch. Paschalidis category:stat.ML G.1.6; G.3; I.2.6  published:2017-06-07 summary:We present a Distributionally Robust Optimization (DRO) approach to outlier detection in a linear regression setting, where the closeness of probability distributions is measured using the Wasserstein metric. Training samples contaminated with outliers skew the regression plane computed by least squares and thus impede outlier detection. Classical approaches, such as robust regression, remedy this problem by downweighting the contribution of atypical data points. In contrast, our Wasserstein DRO approach hedges against a family of distributions that are close to the empirical distribution. We show that the resulting formulation encompasses a class of models, which include the regularized Least Absolute Deviation (LAD) as a special case. We provide new insights into the regularization term and give guidance on the selection of the regularization coefficient from the standpoint of a confidence region. We establish two types of performance guarantees for the solution to our formulation under mild conditions. One is related to its out-of-sample behavior, and the other concerns the discrepancy between the estimated and true regression planes. Extensive numerical results demonstrate the superiority of our approach to both robust regression and the regularized LAD in terms of estimation accuracy and outlier detection rates. version:1
arxiv-1706-02409 | A Convex Framework for Fair Regression | http://arxiv.org/abs/1706.02409 | id:1706.02409 author:Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, Aaron Roth category:cs.LG stat.ML  published:2017-06-07 summary:We introduce a flexible family of fairness regularizers for (linear and logistic) regression problems. These regularizers all enjoy convexity, permitting fast optimization, and they span the rang from notions of group fairness to strong individual fairness. By varying the weight on the fairness regularizer, we can compute the efficient frontier of the accuracy-fairness trade-off on any given dataset, and we measure the severity of this trade-off via a numerical quantity we call the Price of Fairness (PoF). The centerpiece of our results is an extensive comparative study of the PoF across six different datasets in which fairness is a primary consideration. version:1
arxiv-1705-08045 | Learning multiple visual domains with residual adapters | http://arxiv.org/abs/1705.08045 | id:1705.08045 author:Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi category:cs.CV stat.ML  published:2017-05-22 summary:There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly. version:2
arxiv-1705-06908 | Unbiased estimates for linear regression via volume sampling | http://arxiv.org/abs/1705.06908 | id:1705.06908 author:Michal Derezinski, Manfred K. Warmuth category:cs.LG  published:2017-05-19 summary:Given a full rank matrix $X$ with more columns than rows consider the task of estimating the pseudo inverse $X^+$ based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times $X^+X^{+\top}$. Pseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of $X$. We assume labels are expensive and we are only given the labels for the small subset of columns we sample from $X$. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all column labels. We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns. version:2
arxiv-1706-00834 | Online Dynamic Programming | http://arxiv.org/abs/1706.00834 | id:1706.00834 author:Holakou Rahmanian, S. V. N. Vishwanathan, Manfred K. Warmuth category:cs.LG  published:2017-06-02 summary:We consider the problem of repeatedly solving a variant of the same dynamic programming problem in successive trials. An instance of the type of problems we consider is to find the optimal binary search tree. At the beginning of each trial, the learner probabilistically chooses a tree with the n keys at the internal nodes and the n + 1 gaps between keys at the leaves. It is then told the frequencies of the keys and gaps and is charged by the average search cost for the chosen tree. The problem is online because the frequencies can change between trials. The goal is to develop algorithms with the property that their total average search cost (loss) in all trials is close to the total loss of the best tree chosen in hind sight for all trials. The challenge, of course, is that the algorithm has to deal with exponential number of trees. We develop a methodology for tackling such problems for a wide class of dynamic programming algorithms. Our framework allows us to extend online learning algorithms like Hedge and Component Hedge to a significantly wider class of combinatorial objects than was possible before. version:2
arxiv-1706-02393 | ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks | http://arxiv.org/abs/1706.02393 | id:1706.02393 author:Denis A. Gudovskiy, Luca Rigazio category:cs.CV cs.NE  published:2017-06-07 summary:In this paper we introduce ShiftCNN, a generalized low-precision architecture for inference of multiplierless convolutional neural networks (CNNs). ShiftCNN is based on a power-of-two weight representation and, as a result, performs only shift and addition operations. Furthermore, ShiftCNN substantially reduces computational cost of convolutional layers by precomputing convolution terms. Such an optimization can be applied to any CNN architecture with a relatively small codebook of weights and allows to decrease the number of product operations by at least two orders of magnitude. The proposed architecture targets custom inference accelerators and can be realized on FPGAs or ASICs. Extensive evaluation on ImageNet shows that the state-of-the-art CNNs can be converted without retraining into ShiftCNN with less than 1% drop in accuracy when the proposed quantization algorithm is employed. RTL simulations, targeting modern FPGAs, show that power consumption of convolutional layers is reduced by a factor of 4 compared to conventional 8-bit fixed-point architectures. version:1
arxiv-1706-02390 | Creating Virtual Universes Using Generative Adversarial Networks | http://arxiv.org/abs/1706.02390 | id:1706.02390 author:Mustafa Mustafa, Deborah Bard, Wahid Bhimji, Rami Al-Rfou, Zarija Lukić category:astro-ph.IM cs.LG  published:2017-06-07 summary:Inferring model parameters from experimental data is a grand challenge in many sciences, including cosmology. This often relies critically on high fidelity numerical simulations, which are prohibitively computationally expensive. The application of deep learning techniques to generative modeling is renewing interest in using high dimensional density estimators as computationally inexpensive emulators of fully-fledged simulations. These generative models have the potential to make a dramatic shift in the field of scientific simulations, but for that shift to happen we need to study the performance of such generators in the precision regime needed for science applications. To this end, in this letter we apply Generative Adversarial Networks to the problem of generating cosmological weak lensing convergence maps. We show that our generator network produces maps that are described by, with high statistical confidence, the same summary statistics as the fully simulated maps. version:1
arxiv-1706-02386 | On learning the structure of Bayesian Networks and submodular function maximization | http://arxiv.org/abs/1706.02386 | id:1706.02386 author:Giulio Caravagna, Daniele Ramazzotti, Guido Sanguinetti category:cs.LG stat.ML  published:2017-06-07 summary:Learning the structure of dependencies among multiple random variables is a problem of considerable theoretical and practical interest. In practice, score optimisation with multiple restarts provides a practical and surprisingly successful solution, yet the conditions under which this may be a well founded strategy are poorly understood. In this paper, we prove that the problem of identifying the structure of a Bayesian Network via regularised score optimisation can be recast, in expectation, as a submodular optimisation problem, thus guaranteeing optimality with high probability. This result both explains the practical success of optimisation heuristics, and suggests a way to improve on such algorithms by artificially simulating multiple data sets via a bootstrap procedure. We show on several synthetic data sets that the resulting algorithm yields better recovery performance than the state of the art, and illustrate in a real cancer genomic study how such an approach can lead to valuable practical insights. version:1
arxiv-1706-02379 | Training Quantized Nets: A Deeper Understanding | http://arxiv.org/abs/1706.02379 | id:1706.02379 author:Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, Tom Goldstein category:cs.LG cs.CV stat.ML  published:2017-06-07 summary:Currently, deep neural networks are deployed on low-power embedded devices by first training a full-precision model using powerful computing hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized network, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of algorithms for non-convex problems, and we show that training algorithms that exploit high-precision representations have an important annealing property that purely quantized training methods lack, which explains many of the observed empirical differences between these types of algorithms. version:1
arxiv-1706-02375 | Fast Black-box Variational Inference through Stochastic Trust-Region Optimization | http://arxiv.org/abs/1706.02375 | id:1706.02375 author:Jeffrey Regier, Michael I. Jordan, Jon McAuliffe category:cs.LG stat.ML 62F15 G.3  published:2017-06-07 summary:We introduce TrustVI, a fast second-order algorithm for black-box variational inference based on trust-region optimization and the reparameterization trick. At each iteration, TrustVI proposes and assesses a step based on minibatches of draws from the variational distribution. The algorithm provably converges to a stationary point. We implement TrustVI in the Stan framework and compare it to ADVI. TrustVI typically converges in tens of iterations to a solution at least as good as the one that ADVI reaches in thousands of iterations. TrustVI iterations can be more computationally expensive, but total computation is typically an order of magnitude less in our experiments. version:1
arxiv-1706-02361 | On the Robustness of Deep Convolutional Neural Networks for Music Classification | http://arxiv.org/abs/1706.02361 | id:1706.02361 author:Keunwoo Choi, George Fazekas, Kyunghyun Cho, Mark Sandler category:cs.IR cs.LG cs.MM cs.SD  published:2017-06-07 summary:Deep neural networks (DNN) have been successfully applied for music classification including music tagging. However, there are several open questions regarding generalisation and best practices in the choice of network architectures, hyper-parameters and input representations. In this article, we investigate specific aspects of neural networks to deepen our understanding of their properties. We analyse and (re-)validate a large music tagging dataset to investigate the reliability of training and evaluation. We perform comprehensive experiments involving audio preprocessing using different time-frequency representations, logarithmic magnitude compression, frequency weighting and scaling. Using a trained network, we compute label vector similarities which is compared to groundtruth similarity. The results highlight several import aspects of music tagging and neural networks. We show that networks can be effective despite of relatively large error rates in groundtruth datasets. We subsequently show that many commonly used input preprocessing techniques are redundant except magnitude compression. Lastly, the analysis of our trained network provides valuable insight into the relationships between music tags. These results highlight the benefit of using data-driven methods to address automatic music tagging. version:1
arxiv-1706-02342 | Active Learning for Structured Prediction from Partially Labeled Data | http://arxiv.org/abs/1706.02342 | id:1706.02342 author:Mehran Khodabandeh, Zhiwei Deng, Mostafa S. Ibrahim, Shinichi Satoh, Greg Mori category:cs.CV  published:2017-06-07 summary:We propose a general purpose active learning algorithm for structured prediction, gathering labeled data for training a model that outputs a set of related labels for an image or video. Active learning starts with a limited initial training set, then iterates querying a user for labels on unlabeled data and retraining the model. We propose a novel algorithm for selecting data for labeling, choosing examples to maximize expected information gain based on belief propagation inference. This is a general purpose method and can be applied to a variety of tasks or models. As a specific example we demonstrate this framework for learning to recognize human actions and group activities in video sequences. Experiments show that our proposed algorithm outperforms previous active learning methods and can achieve accuracy comparable to fully supervised methods while utilizing significantly less labeled data. version:1
arxiv-1706-02337 | Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Network | http://arxiv.org/abs/1706.02337 | id:1706.02337 author:Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer, C. Lee Giles category:cs.CV cs.LG  published:2017-06-07 summary:We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance. version:1
arxiv-1705-03821 | Context Attentive Bandits: Contextual Bandit with Restricted Context | http://arxiv.org/abs/1705.03821 | id:1705.03821 author:Djallel Bouneffouf, Irina Rish, Guillermo A. Cecchi, Raphael Feraud category:cs.AI cs.LG stat.ML  published:2017-05-10 summary:We consider a novel formulation of the multi-armed bandit model, which we call the contextual bandit with restricted context, where only a limited number of features can be accessed by the learner at every iteration. This novel formulation is motivated by different online problems arising in clinical trials, recommender systems and attention modeling. Herein, we adapt the standard multi-armed bandit algorithm known as Thompson Sampling to take advantage of our restricted context setting, and propose two novel algorithms, called the Thompson Sampling with Restricted Context(TSRC) and the Windows Thompson Sampling with Restricted Context(WTSRC), for handling stationary and nonstationary environments, respectively. Our empirical results demonstrate advantages of the proposed approaches on several real-life datasets version:2
arxiv-1706-02332 | Low-shot learning with large-scale diffusion | http://arxiv.org/abs/1706.02332 | id:1706.02332 author:Matthijs Douze, Arthur Szlam, Bharath Hariharan, Hervé Jégou category:cs.CV cs.LG stat.ML  published:2017-06-07 summary:This paper considers the problem of inferring image labels for which only a few labelled examples are available at training time. This setup is often referred to as low-shot learning in the literature, where a standard approach is to re-train the last few layers of a convolutional neural network learned on separate classes. We consider a semi-supervised setting in which we exploit a large collection of images to support label propagation. This is made possible by leveraging the recent advances on large-scale similarity graph construction. We show that despite its conceptual simplicity, scaling up label propagation to up hundred millions of images leads to state of the art accuracy in the low-shot learning regime. version:1
arxiv-1706-02331 | CoMaL Tracking: Tracking Points at the Object Boundaries | http://arxiv.org/abs/1706.02331 | id:1706.02331 author:Santhosh K. Ramakrishnan, Swarna Kamlam Ravindran, Anurag Mittal category:cs.CV  published:2017-06-07 summary:Traditional point tracking algorithms such as the KLT use local 2D information aggregation for feature detection and tracking, due to which their performance degrades at the object boundaries that separate multiple objects. Recently, CoMaL Features have been proposed that handle such a case. However, they proposed a simple tracking framework where the points are re-detected in each frame and matched. This is inefficient and may also lose many points that are not re-detected in the next frame. We propose a novel tracking algorithm to accurately and efficiently track CoMaL points. For this, the level line segment associated with the CoMaL points is matched to MSER segments in the next frame using shape-based matching and the matches are further filtered using texture-based matching. Experiments show improvements over a simple re-detect-and-match framework as well as KLT in terms of speed/accuracy on different real-world applications, especially at the object boundaries. version:1
arxiv-1706-02326 | Improving Variational Auto-Encoders using convex combination linear Inverse Autoregressive Flow | http://arxiv.org/abs/1706.02326 | id:1706.02326 author:Jakub M. Tomczak, Max Welling category:stat.ML  published:2017-06-07 summary:In this paper, we propose a new volume-preserving flow and show that it performs similarly to the linear general normalizing flow. The idea is to enrich a linear Inverse Autoregressive Flow by introducing multiple lower-triangular matrices with ones on the diagonal and combining them using a convex combination. In the experimental studies on MNIST and Histopathology data we show that the proposed approach outperforms other volume-preserving flows and is competitive with current state-of-the-art linear normalizing flow. version:1
arxiv-1706-02275 | Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments | http://arxiv.org/abs/1706.02275 | id:1706.02275 author:Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch category:cs.LG cs.AI cs.NE  published:2017-06-07 summary:We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies. version:1
arxiv-1705-06820 | Pixel Deconvolutional Networks | http://arxiv.org/abs/1705.06820 | id:1705.06820 author:Hongyang Gao, Hao Yuan, Zhengyang Wang, Shuiwang Ji category:cs.LG cs.CV cs.NE stat.ML  published:2017-05-18 summary:Deconvolutional layers have been widely used in a variety of deep models for up-sampling, including encoder-decoder networks for semantic segmentation and deep generative models for unsupervised learning. One of the key limitations of deconvolutional operations is that they result in the so-called checkerboard problem. This is caused by the fact that no direct relationship exists among adjacent pixels on the output feature map. To address this problem, we propose the pixel deconvolutional layer (PixelDCL) to establish direct relationships among adjacent pixels on the up-sampled feature map. Our method is based on a fresh interpretation of the regular deconvolution operation. The resulting PixelDCL can be used to replace any deconvolutional layer in a plug-and-play manner without compromising the fully trainable capabilities of original models. The proposed PixelDCL may result in slight decrease in efficiency, but this can be overcome by an implementation trick. Experimental results on semantic segmentation demonstrate that PixelDCL can consider spatial features such as edges and shapes and yields more accurate segmentation outputs than deconvolutional layers. When used in image generation tasks, our PixelDCL can largely overcome the checkerboard problem suffered by regular deconvolution operations. version:2
arxiv-1706-02263 | Graph Convolutional Matrix Completion | http://arxiv.org/abs/1706.02263 | id:1706.02263 author:Rianne van den Berg, Thomas N. Kipf, Max Welling category:stat.ML cs.DB cs.IR cs.LG  published:2017-06-07 summary:In this paper we revisit matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges representing observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. This framework can be viewed as an important first step towards end-to-end learning in settings where the interaction data is integrated into larger graphs such as social networks or knowledge graphs, circumventing the need for multistage frameworks. Our model achieves competitive performance on standard collaborative filtering benchmarks, significantly outperforming related methods in a recommendation task with side information. version:1
arxiv-1706-02262 | InfoVAE: Information Maximizing Variational Autoencoders | http://arxiv.org/abs/1706.02262 | id:1706.02262 author:Shengjia Zhao, Jiaming Song, Stefano Ermon category:cs.LG cs.AI stat.ML  published:2017-06-07 summary:It has been previously observed that variational autoencoders tend to ignore the latent code when combined with a decoding distribution that is too flexible. This undermines the purpose of unsupervised representation learning. We identify the reason for this short-coming in the regularization term used in the ELBO criterion to match the variational posterior to the latent prior distribution. We show that removing this regularization term leads to a model that can still discover meaningful latent features. Even though ancestral sampling is no longer tractable, sampling is possible using a Markov chain. Furthermore, we propose a class of training criteria that use alternative divergences for the regularization term, generalizing the standard ELBO which employs KL divergence. These models can discover meaningful latent features and allow for tractable ancestral sampling. In particular, we propose an alternative based on Maximum Mean Discrepancy (MMD) that is simple to implement, robust, and has similar or better performance in every quantitative and qualitative metric we experimented on. version:1
arxiv-1706-02257 | Driver Action Prediction Using Deep (Bidirectional) Recurrent Neural Network | http://arxiv.org/abs/1706.02257 | id:1706.02257 author:Oluwatobi Olabiyi, Eric Martinson, Vijay Chintalapudi, Rui Guo category:stat.ML cs.CV  published:2017-06-07 summary:Advanced driver assistance systems (ADAS) can be significantly improved with effective driver action prediction (DAP). Predicting driver actions early and accurately can help mitigate the effects of potentially unsafe driving behaviors and avoid possible accidents. In this paper, we formulate driver action prediction as a timeseries anomaly prediction problem. While the anomaly (driver actions of interest) detection might be trivial in this context, finding patterns that consistently precede an anomaly requires searching for or extracting features across multi-modal sensory inputs. We present such a driver action prediction system, including a real-time data acquisition, processing and learning framework for predicting future or impending driver action. The proposed system incorporates camera-based knowledge of the driving environment and the driver themselves, in addition to traditional vehicle dynamics. It then uses a deep bidirectional recurrent neural network (DBRNN) to learn the correlation between sensory inputs and impending driver behavior achieving accurate and high horizon action prediction. The proposed system performs better than other existing systems on driver action prediction tasks and can accurately predict key driver actions including acceleration, braking, lane change and turning at durations of 5sec before the action is executed by the driver. version:1
arxiv-1706-02256 | A Mention-Ranking Model for Abstract Anaphora Resolution | http://arxiv.org/abs/1706.02256 | id:1706.02256 author:Ana Marasović, Leo Born, Juri Opitz, Anette Frank category:cs.CL stat.ML  published:2017-06-07 summary:Resolving abstract anaphora is an important, but difficult task for text understanding. With recent advances in representation learning this task becomes a tangible aim. A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent. We propose an LSTM-based mention-ranking model that learns how abstract anaphors relate to their antecedents with a Siamese Net. We overcome the lack of training data by generating artificial anaphoric sentence-antecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus. This corpus presents a greater challenge due to a greater range of confounders. Our model is able to select syntactically plausible candidates and - if disregarding syntax - discriminates candidates using deeper features. Deeper inspection shows that the model is able to learn a relation between the anaphor in the anaphoric sentence and its antecedent. version:1
arxiv-1706-00764 | Hyperparameter Optimization: A Spectral Approach | http://arxiv.org/abs/1706.00764 | id:1706.00764 author:Elad Hazan, Adam Klivans, Yang Yuan category:cs.LG cs.AI math.OC stat.ML  published:2017-06-02 summary:We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions. We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm - an iterative application of compressed sensing techniques for orthogonal polynomials - requires only uniform sampling of the hyperparameters and is thus easily parallelizable. Experiments for training deep nets on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases matching what is attainable by hand-tuning. In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and even more so compared to Bayesian Optimization. We also outperform Random Search 5X. Additionally, our method comes with provable guarantees and yields the first quasi-polynomial time algorithm for learning decision trees under the uniform distribution with polynomial sample complexity, the first improvement in over two decades. version:2
arxiv-1706-02248 | Comparative Analysis of Open Source Frameworks for Machine Learning with Use Case in Single-Threaded and Multi-Threaded Modes | http://arxiv.org/abs/1706.02248 | id:1706.02248 author:Yuriy Kochura, Sergii Stirenko, Anis Rojbi, Oleg Alienin, Michail Novotarskiy, Yuri Gordienko category:cs.LG cs.CV cs.DC  published:2017-06-07 summary:The basic features of some of the most versatile and popular open source frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are considered and compared. Their comparative analysis was performed and conclusions were made as to the advantages and disadvantages of these platforms. The performance tests for the de facto standard MNIST data set were carried out on H2O framework for deep learning algorithms designed for CPU and GPU platforms for single-threaded and multithreaded modes of operation. version:1
arxiv-1706-02241 | Insights into Analogy Completion from the Biomedical Domain | http://arxiv.org/abs/1706.02241 | id:1706.02241 author:Denis Newman-Griffis, Albert M Lai, Eric Fosler-Lussier category:cs.CL  published:2017-06-07 summary:Analogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about analogies that do not always hold, either in recent benchmark datasets or when expanding into other domains. Through an analysis of analogies in the biomedical domain, we identify three assumptions: that of a Single Answer for any given analogy, that the pairs involved describe the Same Relationship, and that each pair is Informative with respect to the other. We propose modifying the standard methodology to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to accuracy, and using multiple example pairs. We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods. version:1
arxiv-1706-02240 | Recurrent computations for visual pattern completion | http://arxiv.org/abs/1706.02240 | id:1706.02240 author:Hanlin Tang, Bill Lotter, Martin Schrimpf, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, Gabriel Kreiman category:q-bio.NC cs.AI cs.CV cs.LG  published:2017-06-07 summary:Making inferences from partial information constitutes a critical aspect of cognition. During visual perception, pattern completion enables recognition of poorly visible or occluded objects. We combined psychophysics, physiology and computational models to test the hypothesis that pattern completion is implemented by recurrent computations and present three pieces of evidence that are consistent with this hypothesis. First, subjects robustly recognized objects even when rendered <10% visible, but recognition was largely impaired when processing was interrupted by backward masking. Second, invasive physiological responses along the human ventral cortex exhibited visually selective responses to partially visible objects that were delayed compared to whole objects, suggesting the need for additional computations. These physiological delays were correlated with the effects of backward masking. Third, state-of-the-art feed-forward computational architectures were not robust to partial visibility. However, recognition performance was recovered when the model was augmented with attractor-based recurrent connectivity. These results provide a strong argument of plausibility for the role of recurrent computations in making visual inferences from partial information. version:1
arxiv-1706-02237 | Efficient Reinforcement Learning via Initial Pure Exploration | http://arxiv.org/abs/1706.02237 | id:1706.02237 author:Sudeep Raja Putta, Theja Tulabandhula category:cs.LG stat.ML  published:2017-06-07 summary:In several realistic situations, an interactive learning agent can practice and refine its strategy before going on to be evaluated. For instance, consider a student preparing for a series of tests. She would typically take a few practice tests to know which areas she needs to improve upon. Based of the scores she obtains in these practice tests, she would formulate a strategy for maximizing her scores in the actual tests. We treat this scenario in the context of an agent exploring a fixed-horizon episodic Markov Decision Process (MDP), where the agent can practice on the MDP for some number of episodes (not necessarily known in advance) before starting to incur regret for its actions. During practice, the agent's goal must be to maximize the probability of following an optimal policy. This is akin to the problem of Pure Exploration (PE). We extend the PE problem of Multi Armed Bandits (MAB) to MDPs and propose a Bayesian algorithm called Posterior Sampling for Pure Exploration (PSPE), which is similar to its bandit counterpart. We show that the Bayesian simple regret converges at an optimal exponential rate when using PSPE. When the agent starts being evaluated, its goal would be to minimize the cumulative regret incurred. This is akin to the problem of Reinforcement Learning (RL). The agent uses the Posterior Sampling for Reinforcement Learning algorithm (PSRL) initialized with the posteriors of the practice phase. We hypothesize that this PSPE + PSRL combination is an optimal strategy for minimizing regret in RL problems with an initial practice phase. We show empirical results which prove that having a lower simple regret at the end of the practice phase results in having lower cumulative regret during evaluation. version:1
arxiv-1706-02901 | Characterizing Types of Convolution in Deep Convolutional Recurrent Neural Networks for Robust Speech Emotion Recognition | http://arxiv.org/abs/1706.02901 | id:1706.02901 author:Che-Wei Huang, Shrikanth. S. Narayanan category:cs.LG cs.CL cs.MM cs.SD  published:2017-06-07 summary:Deep convolutional neural networks are being actively investigated in a wide range of speech and audio processing applications including speech recognition, audio event detection and computational paralinguistics, owing to their ability to reduce factors of variations, such as speaker and environment information in signals, for speech recognition. However, studies have suggested to favor a certain type of convolutional operations when building a deep convolutional neural network for speech applications although there has been promising results using different types of convolutional operations. In this work, we study four types of convolutional operations on different input features for speech emotion recognition in order to derive a comprehensive understanding. Since affective behavioral information has been shown to reflect temporally varying of mental state and convolutional operation are applied locally in time, all deep neural networks share a deep recurrent sub-network architecture for further temporal modeling. We present detailed quantitative module-wise performance analysis to gain insights into information flows within the proposed architectures. In particular, we demonstrate the interplay of affective information and the other irrelevant information during the progression from one module to another. Finally we show that all of our deep neural networks provide state-of-the-art performance on the eNTERFACE'05 corpus. version:1
arxiv-1706-02222 | Gated Recurrent Neural Tensor Network | http://arxiv.org/abs/1706.02222 | id:1706.02222 author:Andros Tjandra, Sakriani Sakti, Ruli Manurung, Mirna Adriani, Satoshi Nakamura category:cs.LG cs.CL stat.ML  published:2017-06-07 summary:Recurrent Neural Networks (RNNs), which are a powerful scheme for modeling temporal and sequential data need to capture long-term dependencies on datasets and represent them in hidden layers with a powerful model to capture more information from inputs. For modeling long-term dependencies in a dataset, the gating mechanism concept can help RNNs remember and forget previous information. Representing the hidden layers of an RNN with more expressive operations (i.e., tensor products) helps it learn a more complex relationship between the current input and the previous hidden layer information. These ideas can generally improve RNN performances. In this paper, we proposed a novel RNN architecture that combine the concepts of gating mechanism and the tensor product into a single model. By combining these two concepts into a single RNN, our proposed models learn long-term dependencies by modeling with gating units and obtain more expressive and direct interaction between input and hidden layers using a tensor product on 3-dimensional array (tensor) weight parameters. We use Long Short Term Memory (LSTM) RNN and Gated Recurrent Unit (GRU) RNN and combine them with a tensor product inside their formulations. Our proposed RNNs, which are called a Long-Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN) and Gated Recurrent Unit Recurrent Neural Tensor Network (GRURNTN), are made by combining the LSTM and GRU RNN models with the tensor product. We conducted experiments with our proposed models on word-level and character-level language modeling tasks and revealed that our proposed models significantly improved their performance compared to our baseline models. version:1
arxiv-1706-02216 | Inductive Representation Learning on Large Graphs | http://arxiv.org/abs/1706.02216 | id:1706.02216 author:William L. Hamilton, Rex Ying, Jure Leskovec category:cs.SI cs.LG stat.ML  published:2017-06-07 summary:Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions. version:1
arxiv-1706-02185 | Synthesizing Filamentary Structured Images with GANs | http://arxiv.org/abs/1706.02185 | id:1706.02185 author:He Zhao, Huiqi Li, Li Cheng category:cs.CV  published:2017-06-07 summary:This paper aims at synthesizing filamentary structured images such as retinal fundus images and neuronal images, as follows: Given a ground-truth, to generate multiple realistic looking phantoms. A ground-truth could be a binary segmentation map containing the filamentary structured morphology, while the synthesized output image is of the same size as the ground-truth and has similar visual appearance to what have been presented in the training set. Our approach is inspired by the recent progresses in generative adversarial nets (GANs) as well as image style transfer. In particular, it is dedicated to our problem context with the following properties: Rather than large-scale dataset, it works well in the presence of as few as 10 training examples, which is common in medical image analysis; It is capable of synthesizing diverse images from the same ground-truth; Last and importantly, the synthetic images produced by our approach are demonstrated to be useful in boosting image analysis performance. Empirical examination over various benchmarks of fundus and neuronal images demonstrate the advantages of the proposed approach. version:1
arxiv-1706-02141 | How Important is Syntactic Parsing Accuracy? An Empirical Evaluation on Sentiment Analysis | http://arxiv.org/abs/1706.02141 | id:1706.02141 author:Carlos Gómez-Rodríguez, Iago Alonso-Alonso, David Vilares category:cs.CL cs.AI 68T50  97R40 I.2.7  published:2017-06-07 summary:Syntactic parsing, the process of obtaining the internal structure of sentences in natural languages, is a crucial task for artificial intelligence applications that need to extract meaning from natural language text or speech. Sentiment analysis is one example of application for which parsing has recently proven useful. In recent years, there have been significant advances in the accuracy of parsing algorithms. In this article, we perform an empirical, task-oriented evaluation to determine how parsing accuracy influences the performance of a state-of-the-art sentiment analysis system that determines the polarity of sentences from their parse trees. In particular, we evaluate the system using four well-known dependency parsers, including both current models with state-of-the-art accuracy and more innacurate models which, however, require less computational resources. The experiments show that all of the parsers produce similarly good results in the sentiment analysis task, without their accuracy having any relevant influence on the results. Since parsing is currently a task with a relatively high computational cost that varies strongly between algorithms, this suggests that sentiment analysis researchers and users should prioritize speed over accuracy when choosing a parser; and parsing researchers should investigate models that improve speed further, even at some cost to accuracy. version:1
arxiv-1706-02135 | BiSeg: Simultaneous Instance Segmentation and Semantic Segmentation with Fully Convolutional Networks | http://arxiv.org/abs/1706.02135 | id:1706.02135 author:Viet-Quoc Pham, Satoshi Ito, Tatsuo Kozakaya category:cs.CV  published:2017-06-07 summary:We present a simple and effective framework for simultaneous semantic segmentation and instance segmentation with Fully Convolutional Networks (FCNs). The method, called BiSeg, predicts instance segmentation as a posterior in Bayesian inference, where semantic segmentation is used as a prior. We extend the idea of position-sensitive score maps used in recent methods to a fusion of multiple score maps at different scales and partition modes, and adopt it as a robust likelihood for instance segmentation inference. As both Bayesian inference and map fusion are performed per pixel, BiSeg is a fully convolutional end-to-end solution that inherits all the advantages of FCNs. We demonstrate state-of-the-art instance segmentation accuracy on PASCAL VOC. version:1
arxiv-1705-08106 | Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition | http://arxiv.org/abs/1705.08106 | id:1705.08106 author:Hong Liu, Juanhui Tu, Mengyuan Liu category:cs.CV  published:2017-05-23 summary:It remains a challenge to efficiently extract spatialtemporal information from skeleton sequences for 3D human action recognition. Although most recent action recognition methods are based on Recurrent Neural Networks which present outstanding performance, one of the shortcomings of these methods is the tendency to overemphasize the temporal information. Since 3D convolutional neural network(3D CNN) is a powerful tool to simultaneously learn features from both spatial and temporal dimensions through capturing the correlations between three dimensional signals, this paper proposes a novel two-stream model using 3D CNN. To our best knowledge, this is the first application of 3D CNN in skeleton-based action recognition. Our method consists of three stages. First, skeleton joints are mapped into a 3D coordinate space and then encoding the spatial and temporal information, respectively. Second, 3D CNN models are seperately adopted to extract deep features from two streams. Third, to enhance the ability of deep features to capture global relationships, we extend every stream into multitemporal version. Extensive experiments on the SmartHome dataset and the large-scale NTU RGB-D dataset demonstrate that our method outperforms most of RNN-based methods, which verify the complementary property between spatial and temporal information and the robustness to noise. version:2
arxiv-1706-02124 | Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks | http://arxiv.org/abs/1706.02124 | id:1706.02124 author:Marian Tietz, Tayfun Alpay, Johannes Twiefel, Stefan Wermter category:cs.CL cs.LG cs.NE  published:2017-06-07 summary:Ladder networks are a notable new concept in the field of semi-supervised learning by showing state-of-the-art results in image recognition tasks while being compatible with many existing neural architectures. We present the recurrent ladder network, a novel modification of the ladder network, for semi-supervised learning of recurrent neural networks which we evaluate with a phoneme recognition task on the TIMIT corpus. Our results show that the model is able to consistently outperform the baseline and achieve fully-supervised baseline performance with only 75% of all labels which demonstrates that the model is capable of using unsupervised data as an effective regulariser. version:1
arxiv-1706-02295 | Generative-Discriminative Variational Model for Visual Recognition | http://arxiv.org/abs/1706.02295 | id:1706.02295 author:Chih-Kuan Yeh, Yao-Hung Hubert Tsai, Yu-Chiang Frank Wang category:cs.LG  published:2017-06-07 summary:The paradigm shift from shallow classifiers with hand-crafted features to end-to-end trainable deep learning models has shown significant improvements on supervised learning tasks. Despite the promising power of deep neural networks (DNN), how to alleviate overfitting during training has been a research topic of interest. In this paper, we present a Generative-Discriminative Variational Model (GDVM) for visual classification, in which we introduce a latent variable inferred from inputs for exhibiting generative abilities towards prediction. In other words, our GDVM casts the supervised learning task as a generative learning process, with data discrimination to be jointly exploited for improved classification. In our experiments, we consider the tasks of multi-class classification, multi-label classification, and zero-shot learning. We show that our GDVM performs favorably against the baselines or recent generative DNN models. version:1
arxiv-1705-08940 | Visual Servoing from Deep Neural Networks | http://arxiv.org/abs/1705.08940 | id:1705.08940 author:Quentin Bateux, Eric Marchand, Jürgen Leitner, Francois Chaumette, Peter Corke category:cs.RO cs.CV  published:2017-05-24 summary:We present a deep neural network-based method to perform high-precision, robust and real-time 6 DOF visual servoing. The paper describes how to create a dataset simulating various perturbations (occlusions and lighting conditions) from a single real-world image of the scene. A convolutional neural network is fine-tuned using this dataset to estimate the relative pose between two images of the same scene. The output of the network is then employed in a visual servoing control scheme. The method converges robustly even in difficult real-world settings with strong lighting variations and occlusions.A positioning error of less than one millimeter is obtained in experiments with a 6 DOF robot. version:2
arxiv-1706-02095 | Macquarie University at BioASQ 5b -- Query-based Summarisation Techniques for Selecting the Ideal Answers | http://arxiv.org/abs/1706.02095 | id:1706.02095 author:Diego Molla-Aliod category:cs.CL  published:2017-06-07 summary:Macquarie University's contribution to the BioASQ challenge (Task 5b Phase B) focused on the use of query-based extractive summarisation techniques for the generation of the ideal answers. Four runs were submitted, with approaches ranging from a trivial system that selected the first $n$ snippets, to the use of deep learning approaches under a regression framework. Our experiments and the ROUGE results of the five test batches of BioASQ indicate surprisingly good results for the trivial approach. Overall, most of our runs on the first three test batches achieved the best ROUGE-SU4 results in the challenge. version:1
arxiv-1706-02093 | Cascade Ranking for Operational E-commerce Search | http://arxiv.org/abs/1706.02093 | id:1706.02093 author:Shichen Liu, Fei Xiao, Wenwu Ou, Luo Si category:stat.ML cs.IR  published:2017-06-07 summary:In the 'Big Data' era, many real-world applications like search involve the ranking problem for a large number of items. It is important to obtain effective ranking results and at the same time obtain the results efficiently in a timely manner for providing good user experience and saving computational costs. Valuable prior research has been conducted for learning to efficiently rank like the cascade ranking (learning) model, which uses a sequence of ranking functions to progressively filter some items and rank the remaining items. However, most existing research of learning to efficiently rank in search is studied in a relatively small computing environments with simulated user queries. This paper presents novel research and thorough study of designing and deploying a Cascade model in a Large-scale Operational E-commerce Search application (CLOES), which deals with hundreds of millions of user queries per day with hundreds of servers. The challenge of the real-world application provides new insights for research: 1). Real-world search applications often involve multiple factors of preferences or constraints with respect to user experience and computational costs such as search accuracy, search latency, size of search results and total CPU cost, while most existing search solutions only address one or two factors; 2). Effectiveness of e-commerce search involves multiple types of user behaviors such as click and purchase, while most existing cascade ranking in search only models the click behavior. Based on these observations, a novel cascade ranking model is designed and deployed in an operational e-commerce search application. An extensive set of experiments demonstrate the advantage of the proposed work to address multiple factors of effectiveness, efficiency and user experience in the real-world application. version:1
arxiv-1706-02071 | DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data | http://arxiv.org/abs/1706.02071 | id:1706.02071 author:Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, Venkatesh Babu Radhakrishnan category:cs.CV  published:2017-06-07 summary:A class of recent approaches for generating images, called Generative Adversarial Networks (GAN), have been used to generate impressively realistic images of objects, bedrooms, handwritten digits and a variety of other image modalities. However, typical GAN-based approaches require large amounts of training data to capture the diversity across the image modality. In this paper, we propose DeLiGAN -- a novel GAN-based architecture for diverse and limited training data scenarios. In our approach, we reparameterize the latent generative space as a mixture model and learn the mixture model's parameters along with those of GAN. This seemingly simple modification to the GAN framework is surprisingly effective and results in models which enable diversity in generated samples although trained with limited data. In our work, we show that DeLiGAN can generate images of handwritten digits, objects and hand-drawn sketches, all using limited amounts of data. To quantitatively characterize intra-class diversity of generated samples, we also introduce a modified version of "inception-score", a measure which has been found to correlate well with human assessment of generated samples. version:1
arxiv-1706-02293 | Sound Event Detection in Multichannel Audio Using Spatial and Harmonic Features | http://arxiv.org/abs/1706.02293 | id:1706.02293 author:Sharath Adavanne, Giambattista Parascandolo, Pasi Pertilä, Toni Heittola, Tuomas Virtanen category:cs.SD cs.LG  published:2017-06-07 summary:In this paper, we propose the use of spatial and harmonic features in combination with long short term memory (LSTM) recurrent neural network (RNN) for automatic sound event detection (SED) task. Real life sound recordings typically have many overlapping sound events, making it hard to recognize with just mono channel audio. Human listeners have been successfully recognizing the mixture of overlapping sound events using pitch cues and exploiting the stereo (multichannel) audio signal available at their ears to spatially localize these events. Traditionally SED systems have only been using mono channel audio, motivated by the human listener we propose to extend them to use multichannel audio. The proposed SED system is compared against the state of the art mono channel method on the development subset of TUT sound events detection 2016 database. The usage of spatial and harmonic features are shown to improve the performance of SED. version:1
arxiv-1706-02055 | Early Experiences with Crowdsourcing Airway Annotations in Chest CT | http://arxiv.org/abs/1706.02055 | id:1706.02055 author:Veronika Cheplygina, Adria Perez-Rovira, Wieying Kuo, Harm A. W. M. Tiddens, Marleen de Bruijne category:cs.CV  published:2017-06-07 summary:Measuring airways in chest computed tomography (CT) images is important for characterizing diseases such as cystic fibrosis, yet very time-consuming to perform manually. Machine learning algorithms offer an alternative, but need large sets of annotated data to perform well. We investigate whether crowdsourcing can be used to gather airway annotations which can serve directly for measuring the airways, or as training data for the algorithms. We generate image slices at known locations of airways and request untrained crowd workers to outline the airway lumen and airway wall. Our results show that the workers are able to interpret the images, but that the instructions are too complex, leading to many unusable annotations. After excluding unusable annotations, quantitative results show medium to high correlations with expert measurements of the airways. Based on this positive experience, we describe a number of further research directions and provide insight into the challenges of crowdsourcing in medical images from the perspective of first-time users. version:1
arxiv-1706-02292 | Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition | http://arxiv.org/abs/1706.02292 | id:1706.02292 author:Miroslav Malik, Sharath Adavanne, Konstantinos Drossos, Tuomas Virtanen, Dasa Ticha, Roman Jarina category:cs.SD cs.LG  published:2017-06-07 summary:This paper studies the emotion recognition from musical tracks in the 2-dimensional valence-arousal (V-A) emotional space. We propose a method based on convolutional (CNN) and recurrent neural networks (RNN), having significantly fewer parameters compared with the state-of-the-art method for the same task. We utilize one CNN layer followed by two branches of RNNs trained separately for arousal and valence. The method was evaluated using the 'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for arousal and 0.268 for valence, which is the best result reported on this dataset. version:1
arxiv-1706-02291 | Sound Event Detection Using Spatial Features and Convolutional Recurrent Neural Network | http://arxiv.org/abs/1706.02291 | id:1706.02291 author:Sharath Adavanne, Pasi Pertilä, Tuomas Virtanen category:cs.SD cs.LG  published:2017-06-07 summary:This paper proposes to use low-level spatial features extracted from multichannel audio for sound event detection. We extend the convolutional recurrent neural network to handle more than one type of these multichannel features by learning from each of them separately in the initial stages. We show that instead of concatenating the features of each channel into a single feature vector the network learns sound events in multichannel audio better when they are presented as separate layers of a volume. Using the proposed spatial features over monaural features on the same network gives an absolute F-score improvement of 6.1% on the publicly available TUT-SED 2016 dataset and 2.7% on the TUT-SED 2009 dataset that is fifteen times larger. version:1
arxiv-1706-02054 | Unsupervised Place Discovery for Place-Specific Change Classifier | http://arxiv.org/abs/1706.02054 | id:1706.02054 author:Fei Xiaoxiao, Tanaka Kanji category:cs.CV  published:2017-06-07 summary:In this study, we address the problem of supervised change detection for robotic map learning applications, in which the aim is to train a place-specific change classifier (e.g., support vector machine (SVM)) to predict changes from a robot's view image. An open question is the manner in which to partition a robot's workspace into places (e.g., SVMs) to maximize the overall performance of change classifiers. This is a chicken-or-egg problem: if we have a well-trained change classifier, partitioning the robot's workspace into places is rather easy. However, training a change classifier requires a set of place-specific training data. In this study, we address this novel problem, which we term unsupervised place discovery. In addition, we present a solution powered by convolutional-feature-based visual place recognition, and validate our approach by applying it to two place-specific change classifiers, namely, nuisance and anomaly predictors. version:1
arxiv-1706-02052 | Are Saddles Good Enough for Deep Learning? | http://arxiv.org/abs/1706.02052 | id:1706.02052 author:Adepu Ravi Sankar, Vineeth N Balasubramanian category:stat.ML cs.LG cs.NE  published:2017-06-07 summary:Recent years have seen a growing interest in understanding deep neural networks from an optimization perspective. It is understood now that converging to low-cost local minima is sufficient for such models to become effective in practice. However, in this work, we propose a new hypothesis based on recent theoretical findings and empirical studies that deep neural network models actually converge to saddle points with high degeneracy. Our findings from this work are new, and can have a significant impact on the development of gradient descent based methods for training deep networks. We validated our hypotheses using an extensive experimental evaluation on standard datasets such as MNIST and CIFAR-10, and also showed that recent efforts that attempt to escape saddles finally converge to saddles with high degeneracy, which we define as `good saddles'. We also verified the famous Wigner's Semicircle Law in our experimental results. version:1
arxiv-1706-02051 | Automatic Emphysema Detection using Weakly Labeled HRCT Lung Images | http://arxiv.org/abs/1706.02051 | id:1706.02051 author:Isabel Pino Peña, Veronika Cheplygina, Sofia Paschaloudi, Morten Vuust, Jesper Carl, Ulla Møller Weinreich, Lasse Riis Østergaard, Marleen de Bruijne category:cs.CV  published:2017-06-07 summary:A method for automatically quantifying emphysema regions using High-Resolution Computed Tomography (HRCT) scans of patients with chronic obstructive pulmonary disease (COPD) that does not require manually annotated scans for training is presented. HRCT scans of controls and of COPD patients with diverse disease severity are acquired at two different centers. Textural features from co-occurrence matrices and Gaussian filter banks are used to characterize the lung parenchyma in the scans. Two robust versions of multiple instance learning (MIL) classifiers, miSVM and MILES, are investigated. The classifiers are trained with the weak labels extracted from the forced expiratory volume in one minute (FEV$_1$) and diffusing capacity of the lungs for carbon monoxide (DLCO). At test time, the classifiers output a patient label indicating overall COPD diagnosis and local labels indicating the presence of emphysema. The classifier performance is compared with manual annotations by two radiologists, a classical density based method, and pulmonary function tests (PFTs). The miSVM classifier performed better than MILES on both patient and emphysema classification. The classifier has a stronger correlation with PFT than the density based method, the percentage of emphysema in the intersection of annotations from both radiologists, and the percentage of emphysema annotated by one of the radiologists. The correlation between the classifier and the PFT is only outperformed by the second radiologist. The method is therefore promising for facilitating assessment of emphysema and reducing inter-observer variability. version:1
arxiv-1706-02047 | Stacked Convolutional and Recurrent Neural Networks for Bird Audio Detection | http://arxiv.org/abs/1706.02047 | id:1706.02047 author:Sharath Adavanne, Konstantinos Drossos, Emre Çakır, Tuomas Virtanen category:cs.SD cs.LG  published:2017-06-07 summary:This paper studies the detection of bird calls in audio segments using stacked convolutional and recurrent neural networks. Data augmentation by blocks mixing and domain adaptation using a novel method of test mixing are proposed and evaluated in regard to making the method robust to unseen data. The contributions of two kinds of acoustic features (dominant frequency and log mel-band energy) and their combinations are studied in the context of bird audio detection. Our best achieved AUC measure on five cross-validations of the development data is 95.5% and 88.1% on the unseen evaluation data. version:1
arxiv-1706-00396 | Line Profile Based Segmentation Algorithm for Touching Corn Kernels | http://arxiv.org/abs/1706.00396 | id:1706.00396 author:Ali Mahdi, Jun Qin category:cs.CV  published:2017-06-01 summary:Image segmentation of touching objects plays a key role in providing accurate classification for computer vision technologies. A new line profile based imaging segmentation algorithm has been developed to provide a robust and accurate segmentation of a group of touching corns. The performance of the line profile based algorithm has been compared to a watershed based imaging segmentation algorithm. Both algorithms are tested on three different patterns of images, which are isolated corns, single-lines, and random distributed formations. The experimental results show that the algorithm can segment a large number of touching corn kernels efficiently and accurately. version:2
arxiv-1706-02042 | DeepSketch2Face: A Deep Learning Based Sketching System for 3D Face and Caricature Modeling | http://arxiv.org/abs/1706.02042 | id:1706.02042 author:Xiaoguang Han, Chang Gao, Yizhou Yu category:cs.GR cs.CV  published:2017-06-07 summary:Face modeling has been paid much attention in the field of visual computing. There exist many scenarios, including cartoon characters, avatars for social media, 3D face caricatures as well as face-related art and design, where low-cost interactive face modeling is a popular approach especially among amateur users. In this paper, we propose a deep learning based sketching system for 3D face and caricature modeling. This system has a labor-efficient sketching interface, that allows the user to draw freehand imprecise yet expressive 2D lines representing the contours of facial features. A novel CNN based deep regression network is designed for inferring 3D face models from 2D sketches. Our network fuses both CNN and shape based features of the input sketch, and has two independent branches of fully connected layers generating independent subsets of coefficients for a bilinear face representation. Our system also supports gesture based interactions for users to further manipulate initial face models. Both user studies and numerical results indicate that our sketching system can help users create face models quickly and effectively. A significantly expanded face database with diverse identities, expressions and levels of exaggeration is constructed to promote further research and evaluation of face modeling techniques. version:1
arxiv-1706-02027 | Question Answering and Question Generation as Dual Tasks | http://arxiv.org/abs/1706.02027 | id:1706.02027 author:Duyu Tang, Nan Duan, Tao Qin, Ming Zhou category:cs.CL  published:2017-06-07 summary:We study the problem of joint question answering (QA) and question generation (QG) in this paper. Our intuition is that QA and QG have intrinsic connections and these two tasks could improve each other. On one side, the QA model judges whether the generated question of a QG model is relevant to the answer. On the other side, the QG model provides the probability of generating a question given the answer, which is a useful evidence that in turn facilitates QA. In this paper we regard QA and QG as dual tasks. We propose a novel training framework that trains the models of QA and QG simultaneously, and explicitly leverages their probabilistic correlation to guide the training process. We implement a QG model based on sequence-to-sequence learning, and a QA model based on recurrent neural network. All the parameters involved in these two tasks are jointly learned with back propagation. Experimental results on two datasets show that our approach improves both QA and QG tasks. version:1
arxiv-1706-02025 | Imposing Hard Constraints on Deep Networks: Promises and Limitations | http://arxiv.org/abs/1706.02025 | id:1706.02025 author:Pablo Márquez-Neila, Mathieu Salzmann, Pascal Fua category:cs.CV  published:2017-06-07 summary:Imposing constraints on the output of a Deep Neural Net is one way to improve the quality of its predictions while loosening the requirements for labeled training data. Such constraints are usually imposed as soft constraints by adding new terms to the loss function that is minimized during training. An alternative is to impose them as hard constraints, which has a number of theoretical benefits but has not been explored so far due to the perceived intractability of the problem. In this paper, we show that imposing hard constraints can in fact be done in a computationally feasible way and delivers reasonable results. However, the theoretical benefits do not materialize and the resulting technique is no better than existing ones relying on soft constraints. We analyze the reasons for this and hope to spur other researchers into proposing better solutions. version:1
arxiv-1706-02021 | Network Sketching: Exploiting Binary Structure in Deep CNNs | http://arxiv.org/abs/1706.02021 | id:1706.02021 author:Yiwen Guo, Anbang Yao, Hao Zhao, Yurong Chen category:cs.NE cs.CV  published:2017-06-07 summary:Convolutional neural networks (CNNs) with deep architectures have substantially advanced the state-of-the-art in computer vision tasks. However, deep networks are typically resource-intensive and thus difficult to be deployed on mobile devices. Recently, CNNs with binary weights have shown compelling efficiency to the community, whereas the accuracy of such models is usually unsatisfactory in practice. In this paper, we introduce network sketching as a novel technique of pursuing binary-weight CNNs, targeting at more faithful inference and better trade-off for practical applications. Our basic idea is to exploit binary structure directly in pre-trained filter banks and produce binary-weight models via tensor expansion. The whole process can be treated as a coarse-to-fine model approximation, akin to the pencil drawing steps of outlining and shading. To further speedup the generated models, namely the sketches, we also propose an associative implementation of binary tensor convolutions. Experimental results demonstrate that a proper sketch of AlexNet (or ResNet) outperforms the existing binary-weight models by large margins on the ImageNet large scale classification task, while the committed memory for network parameters only exceeds a little. version:1
arxiv-1706-01556 | Deep learning for extracting protein-protein interactions from biomedical literature | http://arxiv.org/abs/1706.01556 | id:1706.01556 author:Yifan Peng, Zhiyong Lu category:cs.CL cs.LG q-bio.QM  published:2017-06-05 summary:State-of-the-art methods for protein-protein interaction (PPI) extraction are primarily feature-based or kernel-based by leveraging lexical and syntactic information. But how to incorporate such knowledge in the recent deep learning methods remains an open question. In this paper, we propose a multichannel dependency-based convolutional neural network model (McDepCNN). It applies one channel to the embedding vector of each word in the sentence, and another channel to the embedding vector of the head of the corresponding word. Therefore, the model can use richer information obtained from different channels. Experiments on two public benchmarking datasets, AIMed and BioInfer, demonstrate that McDepCNN compares favorably to the state-of-the-art rich-feature and single-kernel based methods. In addition, McDepCNN achieves 24.4% relative improvement in F1-score over the state-of-the-art methods on cross-corpus evaluation and 12% improvement in F1-score over kernel-based methods on "difficult" instances. These results suggest that McDepCNN generalizes more easily over different corpora, and is capable of capturing long distance features in the sentences. version:2
arxiv-1705-04402 | Negative Results in Computer Vision: A Perspective | http://arxiv.org/abs/1705.04402 | id:1705.04402 author:Ali Borji category:cs.CV  published:2017-05-11 summary:A negative result is when the outcome of an experiment or a model is not what is expected or when a hypothesis does not hold. Despite being often overlooked in the scientific community, negative results are results and they carry value. While this topic has been extensively discussed in other fields such as social sciences and biosciences, less attention has been paid to it in the computer vision community. The unique characteristics of computer vision, particularly its experimental aspect, call for a special treatment of this matter. In this paper, I will address what makes negative results important, how they should be disseminated and incentivized, and what lessons can be learned from cognitive vision research in this regard. Further, I will discuss issues such as computer vision and human vision interaction, experimental design and statistical hypothesis testing, explanatory versus predictive modeling, performance evaluation, model comparison, as well as computer vision research culture. version:3
arxiv-1706-02003 | Deep Convolutional Decision Jungle for Image Classification | http://arxiv.org/abs/1706.02003 | id:1706.02003 author:Seungryul Baek, Kwang In Kim, Tae-Kyun Kim category:cs.CV  published:2017-06-06 summary:We propose a novel method called deep convolutional decision jungle (CDJ) and its learning algorithm for image classification. The CDJ maintains the structure of standard convolutional neural networks (CNNs), i.e. multiple layers of multiple response maps fully connected. Each response map-or node-in both the convolutional and fully-connected layers selectively respond to class labels s.t. each data sample travels via a specific soft route of those activated nodes. The proposed method CDJ automatically learns features, whereas decision forests and jungles require pre-defined feature sets. Compared to CNNs, the method embeds the benefits of using data-dependent discriminative functions, which better handles multi-modal/heterogeneous data; further,the method offers more diverse sparse network responses, which in turn can be used for cost-effective learning/classification. The network is learnt by combining conventional softmax and proposed entropy losses in each layer. The entropy loss,as used in decision tree growing, measures the purity of data activation according to the class label distribution. The back-propagation rule for the proposed loss function is derived from stochastic gradient descent (SGD) optimization of CNNs. We show that our proposed method outperforms state-of-the-art methods on three public image classification benchmarks and one face verification dataset. We also demonstrate the use of auxiliary data labels, when available, which helps our method to learn more discriminative routing and representations and leads to improved classification. version:1
arxiv-1706-02289 | Meta-Learning for Construction of Resampling Recommendation Systems | http://arxiv.org/abs/1706.02289 | id:1706.02289 author:Evgeny Burnaev, Pavel Erofeev, Artem Papanov category:cs.LG stat.AP stat.CO stat.ME  published:2017-06-06 summary:One possible approach to tackle class imbalance in classification tasks is to resample training dataset, i.e., to drop some of its elements or to synthesize new ones. There exist several widely-used resampling methods. Recent research showed that selection of resampling method essentially affects quality of classification, which raises resampling selection problem. Exhaustive search for optimal resampling is time-consuming and hence it is of limited use. In this paper, we describe an alternative approach to resampling selection. We follow meta-learning concept to build resampling recommendation systems, i.e., algorithms recommending resampling for datasets on the basis of their properties. version:1
arxiv-1706-01875 | Measuring Offensive Speech in Online Political Discourse | http://arxiv.org/abs/1706.01875 | id:1706.01875 author:Rishab Nithyanand, Brian Schaffner, Phillipa Gill category:cs.CL cs.CY cs.SI  published:2017-06-06 summary:The Internet and online forums such as Reddit have become an increasingly popular medium for citizens to engage in political conversations. However, the online disinhibition effect resulting from the ability to use pseudonymous identities may manifest in the form of offensive speech, consequently making political discussions more aggressive and polarizing than they already are. Such environments may result in harassment and self-censorship from its targets. In this paper, we present preliminary results from a large-scale temporal measurement aimed at quantifying offensiveness in online political discussions. To enable our measurements, we develop and evaluate an offensive speech classifier. We then use this classifier to quantify and compare offensiveness in the political and general contexts. We perform our study using a database of over 168M Reddit comments made by over 7M pseudonyms between January 2015 and January 2017 -- a period covering several divisive political events including the 2016 US presidential elections. version:1
arxiv-1706-01983 | Deep Learning: Generalization Requires Deep Compositional Feature Space Design | http://arxiv.org/abs/1706.01983 | id:1706.01983 author:Mrinal Haloi category:cs.LG stat.ML 68T45  published:2017-06-06 summary:Generalization error defines the discriminability and the representation power of a deep model. In this work, we claim that feature space design using deep compositional function plays a significant role in generalization along with explicit and implicit regularizations. Our claims are being established with several image classification experiments. We show that the information loss due to convolution and max pooling can be marginalized with the compositional design, improving generalization performance. Also, we will show that learning rate decay acts as an implicit regularizer in deep model training. version:1
arxiv-1706-01912 | Full Quantification of Left Ventricle via Deep Multitask Learning Network Respecting Intra- and Inter-Task Relatedness | http://arxiv.org/abs/1706.01912 | id:1706.01912 author:Wufeng Xue, Andrea Lum, Ashley Mercado, Mark Landis, James Warringto, Shuo Li category:cs.CV  published:2017-06-06 summary:Cardiac left ventricle (LV) quantification is among the most clinically important tasks for identification and diagnosis of cardiac diseases, yet still a challenge due to the high variability of cardiac structure and the complexity of temporal dynamics. Full quantification, i.e., to simultaneously quantify all LV indices including two areas (cavity and myocardium), six regional wall thicknesses (RWT), three LV dimensions, and one cardiac phase, is even more challenging since the uncertain relatedness intra and inter each type of indices may hinder the learning procedure from better convergence and generalization. In this paper, we propose a newly-designed multitask learning network (FullLVNet), which is constituted by a deep convolution neural network (CNN) for expressive feature embedding of cardiac structure; two followed parallel recurrent neural network (RNN) modules for temporal dynamic modeling; and four linear models for the final estimation. During the final estimation, both intra- and inter-task relatedness are modeled to enforce improvement of generalization: 1) respecting intra-task relatedness, group lasso is applied to each of the regression tasks for sparse and common feature selection and consistent prediction; 2) respecting inter-task relatedness, three phase-guided constraints are proposed to penalize violation of the temporal behavior of the obtained LV indices. Experiments on MR sequences of 145 subjects show that FullLVNet achieves high accurate prediction with our intra- and inter-task relatedness, leading to MAE of 190mm$^2$, 1.41mm, 2.68mm for average areas, RWT, dimensions and error rate of 10.4\% for the phase classification. This endows our method a great potential in comprehensive clinical assessment of global, regional and dynamic cardiac function. version:1
arxiv-1706-01905 | Parameter Space Noise for Exploration | http://arxiv.org/abs/1706.01905 | id:1706.01905 author:Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, Marcin Andrychowicz category:cs.LG cs.AI cs.NE cs.RO stat.ML  published:2017-06-06 summary:Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually. version:1
arxiv-1706-01869 | StreetStyle: Exploring world-wide clothing styles from millions of photos | http://arxiv.org/abs/1706.01869 | id:1706.01869 author:Kevin Matzen, Kavita Bala, Noah Snavely category:cs.CV  published:2017-06-06 summary:Each day billions of photographs are uploaded to photo-sharing services and social media platforms. These images are packed with information about how people live around the world. In this paper we exploit this rich trove of data to understand fashion and style trends worldwide. We present a framework for visual discovery at scale, analyzing clothing and fashion across millions of images of people around the world and spanning several years. We introduce a large-scale dataset of photos of people annotated with clothing attributes, and use this dataset to train attribute classifiers via deep learning. We also present a method for discovering visually consistent style clusters that capture useful visual correlations in this massive dataset. Using these tools, we analyze millions of photos to derive visual insight, producing a first-of-its-kind analysis of global and per-city fashion choices and spatio-temporal trends. version:1
arxiv-1706-01865 | Shape Parameter Estimation | http://arxiv.org/abs/1706.01865 | id:1706.01865 author:Peng Zheng, Aleksandr Y. Aravkin, Karthikeyan Natesan Ramamurthy category:stat.ML  published:2017-06-06 summary:Performance of machine learning approaches depends strongly on the choice of misfit penalty, and correct choice of penalty parameters, such as the threshold of the Huber function. These parameters are typically chosen using expert knowledge, cross-validation, or black-box optimization, which are time consuming for large-scale applications. We present a principled, data-driven approach to simultaneously learn the model pa- rameters and the misfit penalty parameters. We discuss theoretical properties of these joint inference problems, and develop algorithms for their solution. We show synthetic examples of automatic parameter tuning for piecewise linear-quadratic (PLQ) penalties, and use the approach to develop a self-tuning robust PCA formulation for background separation. version:1
arxiv-1706-01863 | Marmara Turkish Coreference Corpus and Coreference Resolution Baseline | http://arxiv.org/abs/1706.01863 | id:1706.01863 author:Peter Schüller, Kübra Cıngıllı, Ferit Tunçer, Barış Gün Sürmeli, Ayşegül Pekel, Ayşe Hande Karatay, Hacer Ezgi Karakaş category:cs.CL cs.AI  published:2017-06-06 summary:We describe the Marmara Turkish Coreference Corpus, which is an annotation of the whole METU-Sabanci Turkish Treebank with mentions and coreference chains. Collecting nine or more independent annotations for each document allowed for fully automatic adjudication. We provide a baseline system for Turkish mention detection and coreference resolution and evaluate it on the corpus. version:1
arxiv-1706-01862 | Director Field Analysis (DFA): Exploring Local White Matter Geometric Structure in diffusion MRI | http://arxiv.org/abs/1706.01862 | id:1706.01862 author:Jian Cheng, Peter J. Basser category:cs.CV physics.med-ph  published:2017-06-06 summary:In diffusion MRI, a tensor field or a spherical function field (e.g., an Orientation Distribution Function (ODF) field), can be estimated from measured diffusion weighted images. In this paper, inspired by the microscopic theoretical treatment of phases in liquid crystals, we introduce a novel mathematical framework, called Director Field Analysis (DFA), to study local geometric structural information of white matter based on the reconstructed tensor field or spherical function field: 1) We propose a set of mathematical tools to process general director data which consists of dyadic tensors that have orientations but no direction. 2) We propose Orientational Order (OO) and Orientational Dispersion (OD) indices to describe the degree of alignment and dispersion of a spherical function in a single voxel or in a region, respectively; 3) We also show how to estimate a local orthogonal coordinate frame in each voxel exhibiting anisotropic diffusion; 4) Finally, we define three indices to describe three types of orientational distortion (splay, bend, and twist) in a local spatial neighborhood, and a total distortion index to describe distortions of all three types. To our knowledge, this is the first work to quantitatively describe orientational distortion (splay, bend, and twist) in diffusion MRI data. The proposed DFA and its related mathematical tools can be used to process not only diffusion MRI data but also general director field data, and the proposed scalar indices are useful to detect local geometric changes of white matter for voxel-based or tract-based analysis in both DTI and HARDI acquisitions. version:1
arxiv-1706-01860 | Attributed Network Embedding for Learning in a Dynamic Environment | http://arxiv.org/abs/1706.01860 | id:1706.01860 author:Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, Huan Liu category:cs.SI cs.LG stat.ML  published:2017-06-06 summary:Network embedding leverages the node proximity manifested to learn a low-dimensional node vector representation. The learned embeddings could advance various learning tasks such as node classification, network clustering, and link prediction. Most, if not all, of the existing work, is overwhelmingly performed in the context of plain and static networks. Nonetheless, in reality, network structure often evolves over time with addition/deletion of links and nodes. Also, a vast majority of real-world networks are associated with a rich set of node attributes, and their attribute values are also naturally changing, with the emerging of new content and the fading of old content. These changing characteristics motivate us to seek an effective embedding representation to capture network and attribute evolving patterns, which is of fundamental importance for learning in a dynamic environment. To our best knowledge, we are the first to tackle this problem with the following two challenges: (1) the inherently correlated network and node attributes could be noisy and incomplete, it necessitates a robust consensus representation to capture their individual properties and correlations; (2) the embedding learning needs to be performed in an online fashion to adapt to the changes accordingly. In this paper, we tackle this problem by proposing a novel dynamic attributed network embedding framework - DANE. In particular, DANE provides an offline method for a consensus embedding first and then leverages matrix perturbation theory to maintain the freshness of the end embedding results in an online manner. We perform extensive experiments on both synthetic and real attributed networks to corroborate the effectiveness and efficiency of the proposed framework. version:1
arxiv-1706-01855 | Added value of morphological features to breast lesion diagnosis in ultrasound | http://arxiv.org/abs/1706.01855 | id:1706.01855 author:Michał Byra, Katarzyna Dobruch-Sobczak, Hanna Piotrzkowska-Wróblewska, Andrzej Nowicki category:cs.CV  published:2017-06-06 summary:Ultrasound imaging plays an important role in breast lesion differentiation. However, diagnostic accuracy depends on ultrasonographer experience. Various computer aided diagnosis systems has been developed to improve breast cancer detection and reduce the number of unnecessary biopsies. In this study, our aim was to improve breast lesion classification based on the BI-RADS (Breast Imaging - Reporting and Data System). This was accomplished by combining the BI-RADS with morphological features which assess lesion boundary. A dataset of 214 lesion images was used for analysis. 30 morphological features were extracted and feature selection scheme was applied to find features which improve the BI-RADS classification performance. Additionally, the best performing morphological feature subset was indicated. We obtained a better classification by combining the BI-RADS with six morphological features. These features were the extent, overlap ratio, NRL entropy, circularity, elliptic-normalized circumference and the normalized residual value. The area under the receiver operating curve calculated with the use of the combined classifier was 0.986. The best performing morphological feature subset contained six features: the DWR, NRL entropy, normalized residual value, overlap ratio, extent and the morphological closing ratio. For this set, the area under the curve was 0.901. The combination of the radiologist's experience related to the BI-RADS and the morphological features leads to a more effective breast lesion classification. version:1
arxiv-1705-06264 | Deep Diagnostics: Applying Convolutional Neural Networks for Vessels Defects Detection | http://arxiv.org/abs/1705.06264 | id:1705.06264 author:Stanislav Filippov, Arsenii Moiseev, Andronenko Andrey category:cs.CV  published:2017-05-17 summary:Coronary angiography is considered to be a safe tool for the evaluation of coronary artery disease and perform in approximately 12 million patients each year worldwide. [1] In most cases, angiograms are manually analyzed by a cardiologist. Actually, there are no clinical practice algorithms which could improve and automate this work. Neural networks show high efficiency in tasks of image analysis and they can be used for the analysis of angiograms and facilitate diagnostics. We have developed an algorithm based on Convolutional Neural Network and Neural Network U-Net [2] for vessels segmentation and defects detection such as stenosis. For our research we used anonymized angiography data obtained from one of the city's hospitals and augmented them to improve learning efficiency. U-Net usage provided high quality segmentation and the combination of our algorithm with an ensemble of classifiers shows a good accuracy in the task of ischemia evaluation on test data. Subsequently, this approach can be served as a basis for the creation of an analytical system that could speed up the diagnosis of cardiovascular diseases and greatly facilitate the work of a specialist. version:2
arxiv-1706-01847 | Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext | http://arxiv.org/abs/1706.01847 | id:1706.01847 author:John Wieting, Jonathan Mallinson, Kevin Gimpel category:cs.CL  published:2017-06-06 summary:We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use neural machine translation to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embeddings. We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains. We experiment with several language pairs and data sources, and develop a variety of data filtering techniques. In the process, we explore how neural machine translation output differs from human-written sentences, finding clear differences in length, the amount of repetition, and the use of rare words. version:1
arxiv-1706-01839 | Assessing the Linguistic Productivity of Unsupervised Deep Neural Networks | http://arxiv.org/abs/1706.01839 | id:1706.01839 author:Lawrence Phillips, Nathan Hodas category:cs.CL  published:2017-06-06 summary:Increasingly, cognitive scientists have demonstrated interest in applying tools from deep learning. One use for deep learning is in language acquisition where it is useful to know if a linguistic phenomenon can be learned through domain-general means. To assess whether unsupervised deep learning is appropriate, we first pose a smaller question: Can unsupervised neural networks apply linguistic rules productively, using them in novel situations? We draw from the literature on determiner/noun productivity by training an unsupervised, autoencoder network measuring its ability to combine nouns with determiners. Our simple autoencoder creates combinations it has not previously encountered and produces a degree of overlap matching adults. While this preliminary work does not provide conclusive evidence for productivity, it warrants further investigation with more complex models. Further, this work helps lay the foundations for future collaboration between the deep learning and cognitive science communities. version:1
arxiv-1706-01833 | Online Adaptive Machine Learning Based Algorithm for Implied Volatility Surface Modeling | http://arxiv.org/abs/1706.01833 | id:1706.01833 author:Yaxiong Zeng, Diego Klabjan category:stat.ML cs.LG q-fin.CP  published:2017-06-06 summary:In this work, we design a machine learning based method, online adaptive primal support vector regression (SVR), to model the implied volatility surface. The algorithm proposed is the first derivation and implementation of an online primal kernel SVR. It features enhancements that allow online adaptive learning by embedding the idea of local fitness and budget maintenance. To accelerate our algorithm, we implement its most computationally intensive parts in a Field Programmable Gate Arrays hardware. Using intraday tick data from the E-mini S&P 500 options market, we show that our algorithm outperforms two competing methods and the Gaussian kernel is a better choice than the linear kernel. Sensitivity analysis is also presented to demonstrate how hyper parameters affect the error rates and the number of support vectors in our models. version:1
arxiv-1706-01831 | Information Bottleneck in Control Tasks with Recurrent Spiking Neural Networks | http://arxiv.org/abs/1706.01831 | id:1706.01831 author:Madhavun Candadai Vasu, Eduardo Izquierdo category:cs.NE cs.IT math.IT q-bio.NC  published:2017-06-06 summary:The nervous system encodes continuous information from the environment in the form of discrete spikes, and then decodes these to produce smooth motor actions. Understanding how spikes integrate, represent, and process information to produce behavior is one of the greatest challenges in neuroscience. Information theory has the potential to help us address this challenge. Informational analyses of deep and feed-forward artificial neural networks solving static input-output tasks, have led to the proposal of the \emph{Information Bottleneck} principle, which states that deeper layers encode more relevant yet minimal information about the inputs. Such an analyses on networks that are recurrent, spiking, and perform control tasks is relatively unexplored. Here, we present results from a Mutual Information analysis of a recurrent spiking neural network that was evolved to perform the classic pole-balancing task. Our results show that these networks deviate from the \emph{Information Bottleneck} principle prescribed for feed-forward networks. version:1
arxiv-1706-01826 | Efficient Antihydrogen Detection in Antimatter Physics by Deep Learning | http://arxiv.org/abs/1706.01826 | id:1706.01826 author:Peter Sadowski, Balint Radics, Ananya, Yasunori Yamazaki, Pierre Baldi category:physics.ins-det cs.LG hep-ex  published:2017-06-06 summary:Antihydrogen is at the forefront of antimatter research at the CERN Antiproton Decelerator. Experiments aiming to test the fundamental CPT symmetry and antigravity effects require the efficient detection of antihydrogen annihilation events, which is performed using highly granular tracking detectors installed around an antimatter trap. Improving the efficiency of the antihydrogen annihilation detection plays a central role in the final sensitivity of the experiments. We propose deep learning as a novel technique to analyze antihydrogen annihilation data, and compare its performance with a traditional track and vertex reconstruction method. We report that the deep learning approach yields significant improvement, tripling event coverage while simultaneously improving performance by over 5% in terms of Area Under Curve (AUC). version:1
arxiv-1706-01825 | Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space | http://arxiv.org/abs/1706.01825 | id:1706.01825 author:José Miguel Hernández-Lobato, James Requeima, Edward O. Pyzer-Knapp, Alán Aspuru-Guzik category:stat.ML  published:2017-06-06 summary:Chemical space is so large that brute force searches for new interesting molecules are infeasible. High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or thousands of parallel measurements. Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next. However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS). We show that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic. Additionally, in settings where parallel EI does not scale, PDTS outperforms other scalable baselines such as a greedy search, $\epsilon$-greedy approaches and a random search method. These results show that PDTS is a successful solution for large-scale parallel BO. version:1
arxiv-1706-01824 | Robust Online Multi-Task Learning with Correlative and Personalized Structures | http://arxiv.org/abs/1706.01824 | id:1706.01824 author:Peng Yang, Peilin Zhao, Xin Gao category:cs.LG stat.ML  published:2017-06-06 summary:Multi-Task Learning (MTL) can enhance a classifier's generalization performance by learning multiple related tasks simultaneously. Conventional MTL works under the offline or batch setting, and suffers from expensive training cost and poor scalability. To address such inefficiency issues, online learning techniques have been applied to solve MTL problems. However, most existing algorithms of online MTL constrain task relatedness into a presumed structure via a single weight matrix, which is a strict restriction that does not always hold in practice. In this paper, we propose a robust online MTL framework that overcomes this restriction by decomposing the weight matrix into two components: the first one captures the low-rank common structure among tasks via a nuclear norm and the second one identifies the personalized patterns of outlier tasks via a group lasso. Theoretical analysis shows the proposed algorithm can achieve a sub-linear regret with respect to the best linear model in hindsight. Even though the above framework achieves good performance, the nuclear norm that simply adds all nonzero singular values together may not be a good low-rank approximation. To improve the results, we use a log-determinant function as a non-convex rank approximation. The gradient scheme is applied to optimize log-determinant function and can obtain a closed-form solution for this refined problem. Experimental results on a number of real-world applications verify the efficacy of our method. version:1
arxiv-1706-01820 | Face Alignment Using K-Cluster Regression Forests With Weighted Splitting | http://arxiv.org/abs/1706.01820 | id:1706.01820 author:Marek Kowalski, Jacek Naruniec category:cs.CV  published:2017-06-06 summary:In this work we present a face alignment pipeline based on two novel methods: weighted splitting for K-cluster Regression Forests and 3D Affine Pose Regression for face shape initialization. Our face alignment method is based on the Local Binary Feature framework, where instead of standard regression forests and pixel difference features used in the original method, we use our K-cluster Regression Forests with Weighted Splitting (KRFWS) and Pyramid HOG features. We also use KRFWS to perform Affine Pose Regression (APR) and 3D-Affine Pose Regression (3D-APR), which intend to improve the face shape initialization. APR applies a rigid 2D transform to the initial face shape that compensates for inaccuracy in the initial face location, size and in-plane rotation. 3D-APR estimates the parameters of a 3D transform that additionally compensates for out-of-plane rotation. The resulting pipeline, consisting of APR and 3D-APR followed by face alignment, shows an improvement of 20% over standard LBF on the challenging IBUG dataset, and state-of-theart accuracy on the entire 300-W dataset. version:1
arxiv-1705-01088 | Visual Attribute Transfer through Deep Image Analogy | http://arxiv.org/abs/1705.01088 | id:1705.01088 author:Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, Sing Bing Kang category:cs.CV  published:2017-05-02 summary:We propose a new technique for visual attribute transfer across images that may have very different appearance but have perceptually similar semantic structure. By visual attribute transfer, we mean transfer of visual information (such as color, tone, texture, and style) from one image to another. For example, one image could be that of a painting or a sketch while the other is a photo of a real scene, and both depict the same type of scene. Our technique finds semantically-meaningful dense correspondences between two input images. To accomplish this, it adapts the notion of "image analogy" with features extracted from a Deep Convolutional Neutral Network for matching; we call our technique Deep Image Analogy. A coarse-to-fine strategy is used to compute the nearest-neighbor field for generating the results. We validate the effectiveness of our proposed method in a variety of cases, including style/texture transfer, color/style swap, sketch/painting to photo, and time lapse. version:2
arxiv-1706-01807 | GAN and VAE from an Optimal Transport Point of View | http://arxiv.org/abs/1706.01807 | id:1706.01807 author:Aude Genevay, Gabriel Peyré, Marco Cuturi category:stat.ML  published:2017-06-06 summary:This short article revisits some of the ideas introduced in arXiv:1701.07875 and arXiv:1705.07642 in a simple setup. This sheds some lights on the connexions between Variational Autoencoders (VAE), Generative Adversarial Networks (GAN) and Minimum Kantorovitch Estimators (MKE). version:1
arxiv-1706-01805 | SegAN: Adversarial Network with Multi-scale $L_1$ Loss for Medical Image Segmentation | http://arxiv.org/abs/1706.01805 | id:1706.01805 author:Yuan Xue, Tao Xu, Han Zhang, Rodney Long, Xiaolei Huang category:cs.CV 68T45  published:2017-06-06 summary:Inspired by classic generative adversarial networks (GAN), we propose a novel end-to-end adversarial neural network, called SegAN, for the task of medical image segmentation. Since image segmentation requires dense, pixel-level labeling, the single scalar real/fake output of a classic GAN's discriminator may be ineffective in producing stable and sufficient gradient feedback to the networks. Instead, we use a fully convolutional neural network as the segmentor to generate segmentation label maps, and propose a novel adversarial critic network with a multi-scale $L_1$ loss function to force the critic and segmentor to learn both global and local features that capture long- and short-range spatial relationships between pixels. In our SegAN framework, the segmentor and critic networks are trained in an alternating fashion in a min-max game: The critic takes as input a pair of images, (original_image $*$ predicted_label_map, original_image $*$ ground_truth_label_map), and then is trained by maximizing a multi-scale loss function; The segmentor is trained with only gradients passed along by the critic, with the aim to minimize the multi-scale loss function. We show that such a SegAN framework is more effective and stable for the segmentation task, and it leads to better performance than the state-of-the-art U-net segmentation method. We tested our SegAN method using datasets from the MICCAI BRATS brain tumor segmentation challenge. Extensive experimental results demonstrate the effectiveness of the proposed SegAN with multi-scale loss: on BRATS 2013 SegAN gives performance comparable to the state-of-the-art for whole tumor and tumor core segmentation while achieves better precision and sensitivity for Gd-enhance tumor core segmentation; on BRATS 2015 SegAN achieves better performance than the state-of-the-art in both dice score and precision. version:1
arxiv-1706-01797 | Understanding and Eliminating the Large-kernel Effect in Blind Deconvolution | http://arxiv.org/abs/1706.01797 | id:1706.01797 author:Li Si-Yao, Qian Yin, Ping Guo category:cs.CV  published:2017-06-06 summary:Blind deconvolution consists of recovering a clear version of an observed blurry image without specific knowledge of the degradation kernel. The kernel size, however, is a required hyper-parameter that defines the range of the support domain. In this study, we experimentally and theoretically show how large kernel sizes yield inferior results by introducing noises to expected zeros in the kernel. We explain this effect by demonstrating that sizeable kernels lower the square cost in optimization. We also prove that this effect persists with a probability of one for noisy images. To eliminate this effect, we propose a low-rank based penalty that reflects structural information of the kernel. Compared to the generic $\ell_\alpha$, our penalty can respond to even a small amount of random noise in the kernel. Our regularization process reduces the noise and efficiently enhances the success rate of large kernel sizes. We also compare our method to state-of-art approaches and test it using real-world images. version:1
arxiv-1706-01789 | Deep Alignment Network: A convolutional neural network for robust face alignment | http://arxiv.org/abs/1706.01789 | id:1706.01789 author:Marek Kowalski, Jacek Naruniec, Tomasz Trzcinski category:cs.CV  published:2017-06-06 summary:In this paper, we propose Deep Alignment Network (DAN), a robust face alignment method based on a deep neural network architecture. DAN consists of multiple stages, where each stage improves the locations of the facial landmarks estimated by the previous stage. Our method uses entire face images at all stages, contrary to the recently proposed face alignment methods that rely on local patches. This is possible thanks to the use of landmark heatmaps which provide visual information about landmark locations estimated at the previous stages of the algorithm. The use of entire face images rather than patches allows DAN to handle face images with large variation in head pose and difficult initializations. An extensive evaluation on two publicly available datasets shows that DAN reduces the state-of-the-art failure rate by up to 70%. Our method has also been submitted for evaluation as part of the Menpo challenge. version:1
arxiv-1706-01763 | Adversarial-Playground: A Visualization Suite for Adversarial Sample Generation | http://arxiv.org/abs/1706.01763 | id:1706.01763 author:Andrew Norton, Yanjun Qi category:cs.CR cs.AI cs.LG  published:2017-06-06 summary:With growing interest in adversarial machine learning, it is important for machine learning practitioners and users to understand how their models may be attacked. We propose a web-based visualization tool, \textit{Adversarial-Playground}, to demonstrate the efficacy of common adversarial methods against a deep neural network (DNN) model, built on top of the TensorFlow library. Adversarial-Playground provides users an efficient and effective experience in exploring techniques generating adversarial examples, which are inputs crafted by an adversary to fool a machine learning system. To enable Adversarial-Playground to generate quick and accurate responses for users, we use two primary tactics: (1) We propose a faster variant of the state-of-the-art Jacobian saliency map approach that maintains a comparable evasion rate. (2) Our visualization does not transmit the generated adversarial images to the client, but rather only the matrix describing the sample and the vector representing classification likelihoods \footnote{The source code along with the data from all of our experiments are available at \url{https://github.com/QData/AdversarialDNN-Playground}. version:1
arxiv-1706-01750 | Multi-View Kernels for Low-Dimensional Modeling of Seismic Events | http://arxiv.org/abs/1706.01750 | id:1706.01750 author:Ofir Lindenbaum, Yuri Bregman, Neta Rabin, Amir Averbuch category:cs.LG  published:2017-06-06 summary:The problem of learning from seismic recordings has been studied for years. There is a growing interest in developing automatic mechanisms for identifying the properties of a seismic event. One main motivation is the ability have a reliable identification of man-made explosions. The availability of multiple high-dimensional observations has increased the use of machine learning techniques in a variety of fields. In this work, we propose to use a kernel-fusion based dimensionality reduction framework for generating meaningful seismic representations from raw data. The proposed method is tested on 2023 events that were recorded in Israel and in Jordan. The method achieves promising results in classification of event type as well as in estimating the location of the event. The proposed fusion and dimensionality reduction tools may be applied to other types of geophysical data. version:1
arxiv-1706-01740 | Label-Dependencies Aware Recurrent Neural Networks | http://arxiv.org/abs/1706.01740 | id:1706.01740 author:Yoann Dupont, Marco Dinarelli, Isabelle Tellier category:cs.CL  published:2017-06-06 summary:In the last few years, Recurrent Neural Networks (RNNs) have proved effective on several NLP tasks. Despite such great success, their ability to model \emph{sequence labeling} is still limited. This lead research toward solutions where RNNs are combined with models which already proved effective in this domain, such as CRFs. In this work we propose a solution far simpler but very effective: an evolution of the simple Jordan RNN, where labels are re-injected as input into the network, and converted into embeddings, in the same way as words. We compare this RNN variant to all the other RNN models, Elman and Jordan RNN, LSTM and GRU, on two well-known tasks of Spoken Language Understanding (SLU). Thanks to label embeddings and their combination at the hidden layer, the proposed variant, which uses more parameters than Elman and Jordan RNNs, but far fewer than LSTM and GRU, is more effective than other RNNs, but also outperforms sophisticated CRF models. version:1
arxiv-1706-01724 | Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC | http://arxiv.org/abs/1706.01724 | id:1706.01724 author:Yulai Cong, Bo Chen, Hongwei Liu, Mingyuan Zhou category:stat.ML cs.LG stat.CO  published:2017-06-06 summary:It is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models (LVMs), due to the difficulties in not only computing the gradients, but also adapting the step sizes to different latent factors and hidden layers. For the Poisson gamma belief network (PGBN), a recently proposed deep discrete LVM, we derive an alternative representation that is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data augmentation and marginalization techniques, we derive a block-diagonal Fisher information matrix and its inverse for the simplex-constrained global model parameters of DLDA. Exploiting that Fisher information matrix with stochastic gradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. State-of-the-art results are demonstrated on big data sets. version:1
arxiv-1706-01723 | A General-Purpose Tagger with Convolutional Neural Networks | http://arxiv.org/abs/1706.01723 | id:1706.01723 author:Xiang Yu, Agnieszka Faleńska, Ngoc Thang Vu category:cs.CL  published:2017-06-06 summary:We present a general-purpose tagger based on convolutional neural networks (CNN), used for both composing word vectors and encoding context information. The CNN tagger is robust across different tagging tasks: without task-specific tuning of hyper-parameters, it achieves state-of-the-art results in part-of-speech tagging, morphological tagging and supertagging. The CNN tagger is also robust against the out-of-vocabulary problem, it performs well on artificially unnormalized texts. version:1
arxiv-1706-01690 | A Frame Tracking Model for Memory-Enhanced Dialogue Systems | http://arxiv.org/abs/1706.01690 | id:1706.01690 author:Hannes Schulz, Jeremie Zumer, Layla El Asri, Shikhar Sharma category:cs.CL  published:2017-06-06 summary:Recently, resources and tasks were proposed to go beyond state tracking in dialogue systems. An example is the frame tracking task, which requires recording multiple frames, one for each user goal set during the dialogue. This allows a user, for instance, to compare items corresponding to different goals. This paper proposes a model which takes as input the list of frames created so far during the dialogue, the current user utterance as well as the dialogue acts, slot types, and slot values associated with this utterance. The model then outputs the frame being referenced by each triple of dialogue act, slot type, and slot value. We show that on the recently published Frames dataset, this model significantly outperforms a previously proposed rule-based baseline. In addition, we propose an extensive analysis of the frame tracking task by dividing it into sub-tasks and assessing their difficulty with respect to our model. version:1
arxiv-1706-01252 | Empirical Bayes Matrix Completion | http://arxiv.org/abs/1706.01252 | id:1706.01252 author:Takeru Matsuda, Fumiyasu Komaki category:stat.ML math.ST stat.ME stat.TH  published:2017-06-05 summary:We develop an empirical Bayes (EB) algorithm for the matrix completion problems. The EB algorithm is motivated from the singular value shrinkage estimator for matrix means by Efron and Morris (1972). Since the EB algorithm is essentially the EM algorithm applied to a simple model, it does not require heuristic parameter tuning other than tolerance. Numerical results demonstrated that the EB algorithm achieves a good trade-off between accuracy and efficiency compared to existing algorithms and that it works particularly well when the difference between the number of rows and columns is large. Application to real data also shows the practical utility of the EB algorithm. version:2
arxiv-1706-01686 | Limitations on Variance-Reduction and Acceleration Schemes for Finite Sum Optimization | http://arxiv.org/abs/1706.01686 | id:1706.01686 author:Yossi Arjevani category:math.OC cs.LG stat.ML  published:2017-06-06 summary:We study the conditions under which one is able to efficiently apply variance-reduction and acceleration schemes on finite sum optimization problems. First, we show that, perhaps surprisingly, the finite sum structure by itself, is not sufficient for obtaining a complexity bound of $\tilde{\cO}((n+L/\mu)\ln(1/\epsilon))$ for $L$-smooth and $\mu$-strongly convex individual functions - one must also know which individual function is being referred to by the oracle at each iteration. Next, we show that for a broad class of first-order and coordinate-descent finite sum algorithms (including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated' complexity bound of $\tilde{\cO}((n+\sqrt{n L/\mu})\ln(1/\epsilon))$, unless the strong convexity parameter is given explicitly. Lastly, we show that when this class of algorithms is used for minimizing $L$-smooth and convex finite sums, the optimal complexity bound is $\tilde{\cO}(n+L/\epsilon)$, assuming that (on average) the same update rule is used in every iteration, and $\tilde{\cO}(n+\sqrt{nL/\epsilon})$, otherwise. version:1
arxiv-1704-06498 | Time Series Prediction for Graphs in Kernel and Dissimilarity Spaces | http://arxiv.org/abs/1704.06498 | id:1704.06498 author:Benjamin Paaßen, Christina Göpfert, Barbara Hammer category:cs.AI cs.LG  published:2017-04-21 summary:Graph models are relevant in many fields, such as distributed computing, intelligent tutoring systems or social network analysis. In many cases, such models need to take changes in the graph structure into account, i.e. a varying number of nodes or edges. Predicting such changes within graphs can be expected to yield important insight with respect to the underlying dynamics, e.g. with respect to user behaviour. However, predictive techniques in the past have almost exclusively focused on single edges or nodes. In this contribution, we attempt to predict the future state of a graph as a whole. We propose to phrase time series prediction as a regression problem and apply dissimilarity- or kernel-based regression techniques, such as 1-nearest neighbor, kernel regression and Gaussian process regression, which can be applied to graphs via graph kernels. The output of the regression is a point embedded in a pseudo-Euclidean space, which can be analyzed using subsequent dissimilarity- or kernel-based processing methods. We discuss strategies to speed up Gaussian Processes regression from cubic to linear time and evaluate our approach on two well-established theoretical models of graph evolution as well as two real data sets from the domain of intelligent tutoring systems. We find that simple regression methods, such as kernel regression, are sufficient to capture the dynamics in the theoretical models, but that Gaussian process regression significantly improves the prediction error for real-world data. version:2
arxiv-1706-01678 | Text Summarization using Abstract Meaning Representation | http://arxiv.org/abs/1706.01678 | id:1706.01678 author:Shibhansh Dohare, Harish Karnick category:cs.CL  published:2017-06-06 summary:Summarization of large texts is still an open problem in language processing. In this work we develop a full fledged pipeline to generate summaries of news articles using the Abstract Meaning Representation(AMR). We first generate the AMR graphs of stories then extract summary graphs from the story graphs and finally generate sentences from the summary graph. For extracting summary AMRs from the story AMRs we use a two step process. First, we find important sentences from the text and then extract the summary AMRs from those selected sentences. We outperform the previous methods using AMR for summarization by more that 3 ROGUE-1 points. On the CNN-Dailymail corpus we achieve results competitive with the strong lead-3 baseline till summary graph extraction step. version:1
arxiv-1706-01671 | Compression Fractures Detection on CT | http://arxiv.org/abs/1706.01671 | id:1706.01671 author:Amir Bar, Lior Wolf, Orna Bergman Amitai, Eyal Toledano, Eldad Elnekave category:cs.CV  published:2017-06-06 summary:The presence of a vertebral compression fracture is highly indicative of osteoporosis and represents the single most robust predictor for development of a second osteoporotic fracture in the spine or elsewhere. Less than one third of vertebral compression fractures are diagnosed clinically. We present an automated method for detecting spine compression fractures in Computed Tomography (CT) scans. The algorithm is composed of three processes. First, the spinal column is segmented and sagittal patches are extracted. The patches are then binary classified using a Convolutional Neural Network (CNN). Finally a Recurrent Neural Network (RNN) is utilized to predict whether a vertebral fracture is present in the series of patches. version:1
arxiv-1706-01663 | Learning Pairwise Disjoint Simple Languages from Positive Examples | http://arxiv.org/abs/1706.01663 | id:1706.01663 author:Alexis Linard, Rick Smetsers, Frits Vaandrager, Umar Waqas, Joost van Pinxten, Sicco Verwer category:cs.LG cs.FL  published:2017-06-06 summary:A classical problem in grammatical inference is to identify a deterministic finite automaton (DFA) from a set of positive and negative examples. In this paper, we address the related - yet seemingly novel - problem of identifying a set of DFAs from examples that belong to different unknown simple regular languages. We propose two methods based on compression for clustering the observed positive examples. We apply our methods to a set of print jobs submitted to large industrial printers. version:1
arxiv-1705-07006 | Bayesian Nonparametric Poisson Process Allocation | http://arxiv.org/abs/1705.07006 | id:1705.07006 author:Hongyi Ding, Issei Sato, Masashi Sugiyama category:stat.ML  published:2017-05-19 summary:Analyzing the structure of multiple point process observations provides insight into the understanding of complex networks and human activities. In this work, we present Bayesian nonparametric Poisson process allocation (BNPPA), a generative model to automatically infer the number of latent groups in temporal data based on the previous point estimation model, latent Poisson process allocation (LPPA). We derive a variational inference algorithm when incorporating a Dirichlet process prior and adding an integral constraint. Finally, we demonstrate the usefulness of this Bayesian nonparametric model through experiments on both synthetic and real-world data sets. version:2
arxiv-1706-01876 | Predicting drug-target interactions via sparse learning | http://arxiv.org/abs/1706.01876 | id:1706.01876 author:Ratha Pech, Dong Hao, Maryna Po, Tao Zhou category:cs.LG cs.CE physics.bio-ph  published:2017-06-06 summary:Drug-target interaction (DTI) prediction plays a very important role in drug development. Biochemical experiments or in vitro methods to identify such interactions are very expensive, laborious and time-consuming. Therefore, in silico approaches including docking simulation and machine learning have been proposed to solve this problem. In particular, machine learning approaches have attracted increasing attentions recently. However, in addition to the known drug-target interactions, most of the machine learning methods require extra information such as chemical structures, genome sequences, binding types and so on. Whenever such information is not available, they may perform poor. Very recently, the similarity-based link prediction methods were extended to bipartite networks, which can be applied to solve the DTI prediction problem by using topological information only. In this work, we propose a sparse learning method to solve the DTI prediction problem, which does not require extra information and performs much better than similarity-based methods. We compare the proposed method with similarity-based methods including common neighbor index, Katz index and Jaccard index on the DTI prediction problem over the four renowned and benchmark datasets. The proposed method performs remarkably better. The results suggest that although the proposed method utilizes only the known drug-target interactions, it performs very satisfactorily. The method is very suitable to predict the potential uses of the existing drugs, especially, when extra information about the drugs and targets is not available. version:1
arxiv-1706-01649 | A Minimal Solution for Two-view Focal-length Estimation using Two Affine Correspondences | http://arxiv.org/abs/1706.01649 | id:1706.01649 author:Daniel Barath, Tekla Toth, Levente Hajder category:cs.CV  published:2017-06-06 summary:A minimal solution using two affine correspondences is presented to estimate the common focal length and the fundamental matrix between two semi-calibrated cameras - known intrinsic parameters except a common focal length. To the best of our knowledge, this problem is unsolved. The proposed approach extends point correspondence-based techniques with linear constraints derived from local affine transformations. The obtained multivariate polynomial system is efficiently solved by the hidden-variable technique. Observing the geometry of local affinities, we introduce novel conditions eliminating invalid roots. To select the best one out of the remaining candidates, a root selection technique is proposed outperforming the recent ones especially in case of high-level noise. The proposed 2-point algorithm is validated on both synthetic data and 104 publicly available real image pairs. A Matlab implementation of the proposed solution is included in the paper. version:1
arxiv-1706-01644 | Volume Calculation of CT lung Lesions based on Halton Low-discrepancy Sequences | http://arxiv.org/abs/1706.01644 | id:1706.01644 author:Liansheng Wang, Shusheng Li, Shuo Li category:cs.CV  published:2017-06-06 summary:Volume calculation from the Computed Tomography (CT) lung lesions data is a significant parameter for clinical diagnosis. The volume is widely used to assess the severity of the lung nodules and track its progression, however, the accuracy and efficiency of previous studies are not well achieved for clinical uses. It remains to be a challenging task due to its tight attachment to the lung wall, inhomogeneous background noises and large variations in sizes and shape. In this paper, we employ Halton low-discrepancy sequences to calculate the volume of the lung lesions. The proposed method directly compute the volume without the procedure of three-dimension (3D) model reconstruction and surface triangulation, which significantly improves the efficiency and reduces the complexity. The main steps of the proposed method are: (1) generate a certain number of random points in each slice using Halton low-discrepancy sequences and calculate the lesion area of each slice through the proportion; (2) obtain the volume by integrating the areas in the sagittal direction. In order to evaluate our proposed method, the experiments were conducted on the sufficient data sets with different size of lung lesions. With the uniform distribution of random points, our proposed method achieves more accurate results compared with other methods, which demonstrates the robustness and accuracy for the volume calculation of CT lung lesions. In addition, our proposed method is easy to follow and can be extensively applied to other applications, e.g., volume calculation of liver tumor, atrial wall aneurysm, etc. version:1
arxiv-1706-01643 | Retrosynthetic reaction prediction using neural sequence-to-sequence models | http://arxiv.org/abs/1706.01643 | id:1706.01643 author:Bowen Liu, Bharath Ramsundar, Prasad Kawthekar, Jade Shi, Joseph Gomes, Quang Luu Nguyen, Stephen Ho, Jack Sloane, Paul Wender, Vijay Pande category:cs.LG q-bio.QM stat.ML  published:2017-06-06 summary:We describe a fully data driven model that learns to perform a retrosynthetic reaction prediction task, which is treated as a sequence-to-sequence mapping problem. The end-to-end trained model has an encoder-decoder architecture that consists of two recurrent neural networks, which has previously shown great success in solving other sequence-to-sequence prediction tasks such as machine translation. The model is trained on 50,000 experimental reaction examples from the United States patent literature, which span 10 broad reaction types that are commonly used by medicinal chemists. We find that our model performs comparably with a rule-based expert system baseline model, and also overcomes certain limitations associated with rule-based expert systems and with any machine learning approach that contains a rule-based expert system component. Our model provides an important first step towards solving the challenging problem of computational retrosynthetic analysis. version:1
arxiv-1706-00842 | Neural Network-Based Automatic Liver Tumor Segmentation With Random Forest-Based Candidate Filtering | http://arxiv.org/abs/1706.00842 | id:1706.00842 author:Grzegorz Chlebus, Hans Meine, Jan Hendrik Moltz, Andrea Schenk category:cs.CV  published:2017-06-02 summary:We present a fully automatic method employing convolutional neural networks based on the 2D U-net architecture and random forest classifier to solve the automatic liver lesion segmentation problem of the ISBI 2017 Liver Tumor Segmentation Challenge (LiTS). In order to constrain the ROI in which the tumors could be located, a liver segmentation is performed first. For the organ segmentation, an ensemble of convolutional networks is trained to segment a liver using a set of 179 liver CT datasets from liver surgery planning. Inside of the liver ROI a neural network, trained using 127 challenge training datasets, identifies tumor candidates, which are subsequently filtered with a random forest classifier yielding the final tumor segmentation. The evaluation on the 70 challenge test cases resulted in a mean Dice coefficient of 0.65, ranking our method in the second place. version:2
arxiv-1706-01606 | DeepKey: An EEG and Gait Based Dual-Authentication System | http://arxiv.org/abs/1706.01606 | id:1706.01606 author:Xiang Zhang, Lina Yao, Kaixuan Chen, Xianzhi Wang, Quanz. Sheng, Tao Gu category:cs.LG  published:2017-06-06 summary:Biometric authentication involves various technologies to identify individuals by exploiting their unique, measurable physiological and behavioral characteristics. However, biometric authentication systems (e.g., face recognition, iris, retina, voice, and fingerprint) are increasingly facing the risk of being tricked by biometric tools such as anti-surveillance masks, contact lenses, vocoders, or fingerprint films. In this regard, we design a multimodal biometric authentication system named DeepKey which uses both gait and Electroencephalography (EEG) signals to provide better protection against such risks. DeepKey consists of three key components: an Invalid ID Filter Model to block invalid subjects, a Gait Identification Model to recognize Gait IDs and an EEG Identification Model to recognize EEG IDs. In particular, the first two models employ a one-class SVM algorithm and a Recurrent Neural Network based deep learning model, respectively. The third model combines autoregressive coefficients, an RNN structure, and an SVM classifier. DeepKey is trained with a gait dataset of 160,000 samples and an EEG dataset of 108,000 samples. Experimental results show DeepKey outperforms a series of comparison methods and achieves an overall accuracy of 0.983 along with an overall false acceptance rate (FAR) of 0.0 and a false rejection rate (FRR) of 0.019. version:1
arxiv-1706-01108 | Stochastic Reformulations of Linear Systems: Algorithms and Convergence Theory | http://arxiv.org/abs/1706.01108 | id:1706.01108 author:Peter Richtárik, Martin Takáč category:math.NA cs.LG stat.ML  published:2017-06-04 summary:We develop a family of reformulations of an arbitrary consistent linear system into a stochastic problem. The reformulations are governed by two user-defined parameters: a positive definite matrix defining a norm, and an arbitrary discrete or continuous distribution over random matrices. Our reformulation has several equivalent interpretations, allowing for researchers from various communities to leverage their domain specific insights. In particular, our reformulation can be equivalently seen as a stochastic optimization problem, stochastic linear system, stochastic fixed point problem and a probabilistic intersection problem. We prove sufficient, and necessary and sufficient conditions for the reformulation to be exact. Further, we propose and analyze three stochastic algorithms for solving the reformulated problem---basic, parallel and accelerated methods---with global linear convergence rates. The rates can be interpreted as condition numbers of a matrix which depends on the system matrix and on the reformulation parameters. This gives rise to a new phenomenon which we call stochastic preconditioning, and which refers to the problem of finding parameters (matrix and distribution) leading to a sufficiently small condition number. Our basic method can be equivalently interpreted as stochastic gradient descent, stochastic Newton method, stochastic proximal point method, stochastic fixed point method, and stochastic projection method, with fixed stepsize (relaxation parameter), applied to the reformulations. version:2
arxiv-1706-01604 | Hyperplane Clustering Via Dual Principal Component Pursuit | http://arxiv.org/abs/1706.01604 | id:1706.01604 author:Manolis C. Tsakiris, Rene Vidal category:cs.CV cs.LG stat.ML  published:2017-06-06 summary:We extend the theoretical analysis of a recently proposed single subspace learning algorithm, called Dual Principal Component Pursuit (DPCP), to the case where the data are drawn from of a union of hyperplanes. To gain insight into the properties of the $\ell_1$ non-convex problem associated with DPCP, we develop a geometric analysis of a closely related continuous optimization problem. Then transferring this analysis to the discrete problem, our results state that as long as the hyperplanes are sufficiently separated, the dominant hyperplane is sufficiently dominant and the points are uniformly distributed inside the associated hyperplanes, then the non-convex DPCP problem has a unique global solution, equal to the normal vector of the dominant hyperplane. This suggests the correctness of a sequential hyperplane learning algorithm based on DPCP. A thorough experimental evaluation reveals that hyperplane learning schemes based on DPCP dramatically improve over the state-of-the-art methods for the case of synthetic data, while are competitive to the state-of-the-art in the case of 3D plane clustering for Kinect data. version:1
arxiv-1706-01596 | Sample-Efficient Learning of Mixtures | http://arxiv.org/abs/1706.01596 | id:1706.01596 author:Hassan Ashtiani, Shai Ben-David, Abbas Mehrabian category:cs.LG  published:2017-06-06 summary:We consider PAC learning of probability distributions (a.k.a. density estimation), where we are given an i.i.d. sample generated from an unknown target distribution, and want to output a distribution that is close to the target in total variation distance. Let $\mathcal F$ be an arbitrary class of probability distributions, and let $\mathcal{F}^k$ denote the class of $k$-mixtures of elements of $\mathcal F$. Assuming the existence of a method for learning $\mathcal F$ with sample complexity $m_{\mathcal{F}}(\varepsilon)$ in the realizable setting, we provide a method for learning $\mathcal F^k$ with sample complexity $O({k\log k \cdot m_{\mathcal F}(\varepsilon) }/{\varepsilon^{2}})$ in the agnostic setting. Our mixture learning algorithm has the property that, if the $\mathcal F$-learner is proper, then the $\mathcal F^k$-learner is proper as well. We provide two applications of our main result. First, we show that the class of mixtures of $k$ axis-aligned Gaussians in $\mathbb{R}^d$ is PAC-learnable in the agnostic setting with sample complexity $\widetilde{O}({kd}/{\epsilon ^ 4})$, which is tight in $k$ and $d$. Second, we show that the class of mixtures of $k$ Gaussians in $\mathbb{R}^d$ is PAC-learnable in the agnostic setting with sample complexity $\widetilde{O}({kd^2}/{\epsilon ^ 4})$, which improves the previous known bounds of $\widetilde{O}({k^3d^2}/{\varepsilon ^ 4})$ and $\widetilde{O}(k^4d^4/\varepsilon ^ 2)$ in its dependence on $k$ and $d$. version:1
arxiv-1706-02189 | Incorporating Network Built-in Priors in Weakly-supervised Semantic Segmentation | http://arxiv.org/abs/1706.02189 | id:1706.02189 author:Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian, Mathieu Salzmann, Lars Petersson, Jose M. Alvarez, Stephen Gould category:cs.CV  published:2017-06-06 summary:Pixel-level annotations are expensive and time consuming to obtain. Hence, weak supervision using only image tags could have a significant impact in semantic segmentation. Recently, CNN-based methods have proposed to fine-tune pre-trained networks using image tags. Without additional information, this leads to poor localization accuracy. This problem, however, was alleviated by making use of objectness priors to generate foreground/background masks. Unfortunately these priors either require pixel-level annotations/bounding boxes, or still yield inaccurate object boundaries. Here, we propose a novel method to extract accurate masks from networks pre-trained for the task of object recognition, thus forgoing external objectness modules. We first show how foreground/background masks can be obtained from the activations of higher-level convolutional layers of a network. We then show how to obtain multi-class masks by the fusion of foreground/background ones with information extracted from a weakly-supervised localization network. Our experiments evidence that exploiting these masks in conjunction with a weakly-supervised training loss yields state-of-the-art tag-based weakly-supervised semantic segmentation results. version:1
arxiv-1706-01583 | Classifying Documents within Multiple Hierarchical Datasets using Multi-Task Learning | http://arxiv.org/abs/1706.01583 | id:1706.01583 author:Azad Naik, Anveshi Charuvaka, Huzefa Rangwala category:cs.LG stat.ML  published:2017-06-06 summary:Multi-task learning (MTL) is a supervised learning paradigm in which the prediction models for several related tasks are learned jointly to achieve better generalization performance. When there are only a few training examples per task, MTL considerably outperforms the traditional Single task learning (STL) in terms of prediction accuracy. In this work we develop an MTL based approach for classifying documents that are archived within dual concept hierarchies, namely, DMOZ and Wikipedia. We solve the multi-class classification problem by defining one-versus-rest binary classification tasks for each of the different classes across the two hierarchical datasets. Instead of learning a linear discriminant for each of the different tasks independently, we use a MTL approach with relationships between the different tasks across the datasets established using the non-parametric, lazy, nearest neighbor approach. We also develop and evaluate a transfer learning (TL) approach and compare the MTL (and TL) methods against the standard single task learning and semi-supervised learning approaches. Our empirical results demonstrate the strength of our developed methods that show an improvement especially when there are fewer number of training examples per classification task. version:1
arxiv-1706-01581 | Embedding Feature Selection for Large-scale Hierarchical Classification | http://arxiv.org/abs/1706.01581 | id:1706.01581 author:Azad Naik, Huzefa Rangwala category:cs.LG stat.ML  published:2017-06-06 summary:Large-scale Hierarchical Classification (HC) involves datasets consisting of thousands of classes and millions of training instances with high-dimensional features posing several big data challenges. Feature selection that aims to select the subset of discriminant features is an effective strategy to deal with large-scale HC problem. It speeds up the training process, reduces the prediction time and minimizes the memory requirements by compressing the total size of learned model weight vectors. Majority of the studies have also shown feature selection to be competent and successful in improving the classification accuracy by removing irrelevant features. In this work, we investigate various filter-based feature selection methods for dimensionality reduction to solve the large-scale HC problem. Our experimental evaluation on text and image datasets with varying distribution of features, classes and instances shows upto 3x order of speed-up on massive datasets and upto 45% less memory requirements for storing the weight vectors of learned model without any significant loss (improvement for some datasets) in the classification accuracy. Source Code: https://cs.gmu.edu/~mlbio/featureselection. version:1
arxiv-1706-01580 | Global-Local Airborne Mapping (GLAM): Reconstructing a City from Aerial Videos | http://arxiv.org/abs/1706.01580 | id:1706.01580 author:Hasnain Vohra, Maxim Bazik, Matthew Antone, Joseph Mundy, William Stephenson category:cs.CV  published:2017-06-06 summary:Monocular visual SLAM has become an attractive practical approach for robot localization and 3D environment mapping, since cameras are small, lightweight, inexpensive, and produce high-rate, high-resolution data streams. Although numerous robust tools have been developed, most existing systems are designed to operate in terrestrial environments and at relatively small scale (a few thousand frames) due to constraints on computation and storage. In this paper, we present a feature-based visual SLAM system for aerial video whose simple design permits near real-time operation, and whose scalability permits large-area mapping using tens of thousands of frames, all on a single conventional computer. Our approach consists of two parallel threads: the first incrementally creates small locally consistent submaps and estimates camera poses at video rate; the second aligns these submaps with one another to produce a single globally consistent map via factor graph optimization over both poses and landmarks. Scale drift is minimized through the use of 7-degree-of-freedom similarity transformations during submap alignment. We quantify our system's performance on both simulated and real data sets, and demonstrate city-scale map reconstruction accurate to within 2 meters using nearly $90,000$ aerial video frames - to our knowledge, the largest and fastest such reconstruction to date. version:1
arxiv-1706-01570 | Acquisition of Translation Lexicons for Historically Unwritten Languages via Bridging Loanwords | http://arxiv.org/abs/1706.01570 | id:1706.01570 author:Michael Bloodgood, Benjamin Strauss category:cs.CL I.2.7  published:2017-06-06 summary:With the advent of informal electronic communications such as social media, colloquial languages that were historically unwritten are being written for the first time in heavily code-switched environments. We present a method for inducing portions of translation lexicons through the use of expert knowledge in these settings where there are approximately zero resources available other than a language informant, potentially not even large amounts of monolingual data. We investigate inducing a Moroccan Darija-English translation lexicon via French loanwords bridging into English and find that a useful lexicon is induced for human-assisted translation and statistical machine translation. version:1
arxiv-1706-01566 | Random Search for Hyperparameters using Determinantal Point Processes | http://arxiv.org/abs/1706.01566 | id:1706.01566 author:Jesse Dodge, Catrìona Anderson, Noah A. Smith category:stat.ML cs.LG  published:2017-06-06 summary:We propose the use of k-determinantal point processes in hyperparameter optimization via random search. Compared to conventional approaches where hyperparameter settings are sampled independently, a k-DPP promotes diversity. We describe an approach that transforms hyperparameter search spaces for efficient use with a k-DPP. Our experiments show significant benefits over uniform random search in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel. version:1
arxiv-1706-01554 | Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model | http://arxiv.org/abs/1706.01554 | id:1706.01554 author:Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra category:cs.CV cs.AI cs.CL  published:2017-06-05 summary:We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce 'safe' and generic responses ("I don't know", "I can't tell"). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it can not be deployed to have real conversations with users. Our work aims to achieve the best of both worlds -- the practical usefulness of G and the strong performance of D -- via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution -- specifically, a RNN augmented with a sequence of GS samplers, coupled with the straight-through gradient estimator to enable end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10). version:1
arxiv-1706-01553 | Geometric Multi-Model Fitting with a Convex Relaxation Algorithm | http://arxiv.org/abs/1706.01553 | id:1706.01553 author:Paul Amayo, Pedro Pinies, Lina M. Paz, Paul Newman category:cs.CV  published:2017-06-05 summary:We propose a novel method to fit and segment multi-structural data via convex relaxation. Unlike greedy methods --which maximise the number of inliers-- this approach efficiently searches for a soft assignment of points to models by minimising the energy of the overall classification. Our approach is similar to state-of-the-art energy minimisation techniques which use a global energy. However, we deal with the scaling factor (as the number of models increases) of the original combinatorial problem by relaxing the solution. This relaxation brings two advantages: first, by operating in the continuous domain we can parallelize the calculations. Second, it allows for the use of different metrics which results in a more general formulation. We demonstrate the versatility of our technique on two different problems of estimating structure from images: plane extraction from RGB-D data and homography estimation from pairs of images. In both cases, we report accurate results on publicly available datasets, in most of the cases outperforming the state-of-the-art. version:1
arxiv-1706-01531 | Progressive Boosting for Class Imbalance | http://arxiv.org/abs/1706.01531 | id:1706.01531 author:Roghayeh Soleymani, Eric Granger, Giorgio Fumera category:cs.LG cs.CV  published:2017-06-05 summary:Pattern recognition applications often suffer from skewed data distributions between classes, which may vary during operations w.r.t. the design data. Two-class classification systems designed using skewed data tend to recognize the majority class better than the minority class of interest. Several data-level techniques have been proposed to alleviate this issue by up-sampling minority samples or under-sampling majority samples. However, some informative samples may be neglected by random under-sampling and adding synthetic positive samples through up-sampling adds to training complexity. In this paper, a new ensemble learning algorithm called Progressive Boosting (PBoost) is proposed that progressively inserts uncorrelated groups of samples into a Boosting procedure to avoid loss of information while generating a diverse pool of classifiers. Base classifiers in this ensemble are generated from one iteration to the next, using subsets from a validation set that grows gradually in size and imbalance. Consequently, PBoost is more robust to unknown and variable levels of skew in operational data, and has lower computation complexity than Boosting ensembles in literature. In PBoost, a new loss factor is proposed to avoid bias of performance towards the negative class. Using this loss factor, the weight update of samples and classifier contribution in final predictions are set based on the ability to recognize both classes. Using the proposed loss factor instead of standard accuracy can avoid biasing performance in any Boosting ensemble. The proposed approach was validated and compared using synthetic data, videos from the FIA dataset that emulates face re-identification applications, and KEEL collection of datasets. Results show that PBoost can outperform state of the art techniques in terms of both accuracy and complexity over different levels of imbalance and overlap between classes. version:1
arxiv-1706-00286 | Learning to Compute Word Embeddings On the Fly | http://arxiv.org/abs/1706.00286 | id:1706.00286 author:Dzmitry Bahdanau, Tom Bosc, Stanisław Jastrzębski, Edward Grefenstette, Pascal Vincent, Yoshua Bengio category:cs.LG cs.CL  published:2017-06-01 summary:Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the "long tail" of this distribution requires enormous amounts of data. Representations of rare words trained directly on end-tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained against the end task. We show that this improves results against baselines where embeddings are trained on the end task in a reading comprehension task, a recognizing textual entailment task, and in language modelling. version:2
arxiv-1706-01509 | Facial Emotion Detection Using Convolutional Neural Networks and Representational Autoencoder Units | http://arxiv.org/abs/1706.01509 | id:1706.01509 author:Prudhvi Raj Dachapally category:cs.CV stat.ML  published:2017-06-05 summary:Emotion being a subjective thing, leveraging knowledge and science behind labeled data and extracting the components that constitute it, has been a challenging problem in the industry for many years. With the evolution of deep learning in computer vision, emotion recognition has become a widely-tackled research problem. In this work, we propose two independent methods for this very task. The first method uses autoencoders to construct a unique representation of each emotion, while the second method is an 8-layer convolutional neural network (CNN). These methods were trained on the posed-emotion dataset (JAFFE), and to test their robustness, both the models were also tested on 100 random images from the Labeled Faces in the Wild (LFW) dataset, which consists of images that are candid than posed. The results show that with more fine-tuning and depth, our CNN model can outperform the state-of-the-art methods for emotion recognition. We also propose some exciting ideas for expanding the concept of representational autoencoders to improve their performance. version:1
arxiv-1706-01502 | UCB and InfoGain Exploration via $\boldsymbol{Q}$-Ensembles | http://arxiv.org/abs/1706.01502 | id:1706.01502 author:Richard Y. Chen, John Schulman, Pieter Abbeel, Szymon Sidor category:cs.LG stat.ML  published:2017-06-05 summary:We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. First we propose an exploration strategy based on upper-confidence bounds (UCB). Next, we define an ''InfoGain'' exploration bonus, which depends on the disagreement of the $Q$-ensemble. Our experiments show significant gains on the Atari benchmark. version:1
arxiv-1706-01498 | Stochastic Gradient Monomial Gamma Sampler | http://arxiv.org/abs/1706.01498 | id:1706.01498 author:Yizhe Zhang, Changyou Chen, Zhe Gan, Ricardo Henao, Lawrence Carin category:stat.ML  published:2017-06-05 summary:Recent advances in stochastic gradient techniques have made it possible to estimate posterior distributions from large datasets via Markov Chain Monte Carlo (MCMC). However, when the target posterior is multimodal, mixing performance is often poor. This results in inadequate exploration of the posterior distribution. A framework is proposed to improve the sampling efficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A generalized kinetic function is leveraged, delivering superior stationary mixing, especially for multimodal distributions. Techniques are also discussed to overcome the practical issues introduced by this generalization. It is shown that the proposed approach is better at exploring complex multimodal posterior distributions, as demonstrated on multiple applications and in comparison with other stochastic gradient MCMC methods. version:1
arxiv-1705-07208 | PixColor: Pixel Recursive Colorization | http://arxiv.org/abs/1705.07208 | id:1705.07208 author:Sergio Guadarrama, Ryan Dahl, David Bieber, Mohammad Norouzi, Jonathon Shlens, Kevin Murphy category:cs.CV cs.LG  published:2017-05-19 summary:We propose a novel approach to automatically produce multiple colorized versions of a grayscale image. Our method results from the observation that the task of automated colorization is relatively easy given a low-resolution version of the color image. We first train a conditional PixelCNN to generate a low resolution color for a given grayscale image. Then, given the generated low-resolution color image and the original grayscale image as inputs, we train a second CNN to generate a high-resolution colorization of an image. We demonstrate that our approach produces more diverse and plausible colorizations than existing methods, as judged by human raters in a "Visual Turing Test". version:2
arxiv-1706-01487 | Visual attention models for scene text recognition | http://arxiv.org/abs/1706.01487 | id:1706.01487 author:Suman K. Ghosh, Ernest Valveny, Andrew D. Bagdanov category:cs.CV  published:2017-06-05 summary:In this paper we propose an approach to lexicon-free recognition of text in scene images. Our approach relies on a LSTM-based soft visual attention model learned from convolutional features. A set of feature vectors are derived from an intermediate convolutional layer corresponding to different areas of the image. This permits encoding of spatial information into the image representation. In this way, the framework is able to learn how to selectively focus on different parts of the image. At every time step the recognizer emits one character using a weighted combination of the convolutional feature vectors according to the learned attention model. Training can be done end-to-end using only word level annotations. In addition, we show that modifying the beam search algorithm by integrating an explicit language model leads to significantly better recognition results. We validate the performance of our approach on standard SVT and ICDAR'03 scene text datasets, showing state-of-the-art performance in unconstrained text recognition. version:1
arxiv-1706-01450 | A Joint Model for Question Answering and Question Generation | http://arxiv.org/abs/1706.01450 | id:1706.01450 author:Tong Wang, Xingdi Yuan, Adam Trischler category:cs.CL cs.AI cs.LG cs.NE  published:2017-06-05 summary:We propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. The proposed model uses a sequence-to-sequence framework that encodes the document and generates a question (answer) given an answer (question). Significant improvement in model performance is observed empirically on the SQuAD corpus, confirming our hypothesis that the model benefits from jointly learning to perform both tasks. We believe the joint model's novelty offers a new perspective on machine comprehension beyond architectural engineering, and serves as a first step towards autonomous information seeking. version:1
arxiv-1706-01445 | Ensemble Bayesian Optimization | http://arxiv.org/abs/1706.01445 | id:1706.01445 author:Zi Wang, Clement Gehring, Pushmeet Kohli, Stefanie Jegelka category:stat.ML  published:2017-06-05 summary:Bayesian Optimization (BO) has been shown to be a very effective paradigm for tackling hard black-box and non-convex optimization problems encountered in Machine Learning. Despite these successes, the computational complexity of the underlying function approximation has restricted the use of BO to problems that can be handled with less than a few thousand function evaluations. Harder problems like those involving functions operating in very high dimensional spaces may require hundreds of thousands or millions of evaluations or more and become computationally intractable to handle using standard Bayesian Optimization methods. In this paper, we propose Ensemble Bayesian Optimization (EBO) to overcome this problem. Unlike conventional BO methods that operate on a single posterior GP model, EBO works with an ensemble of posterior GP models. Further, we represent each GP model using tile coding random features and an additive function structure. Our approach generates speedups by parallelizing the time consuming hyper-parameter posterior inference and functional evaluations on hundreds of cores and aggregating the models in every iteration of BO. Our extensive experimental evaluation shows that EBO can speed up the posterior inference between 2-3 orders of magnitude (400 times in one experiment) compared to the state-of-the-art by putting data into Mondrian bins without sacrificing the sample quality. We demonstrate the ability of EBO to handle sample-intensive hard optimization problems by applying it to a rover navigation problem with tens of thousands of observations. version:1
arxiv-1706-01433 | Visual Interaction Networks | http://arxiv.org/abs/1706.01433 | id:1706.01433 author:Nicholas Watters, Andrea Tacchetti, Theophane Weber, Razvan Pascanu, Peter Battaglia, Daniel Zoran category:cs.CV  published:2017-06-05 summary:From just a glance, humans can make rich predictions about the future state of a wide range of physical systems. On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains and require direct measurements of the underlying states. We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions and dynamics, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. Our results demonstrate that the perceptual module and the object-based dynamics predictor module can induce factored latent representations that support accurate dynamical predictions. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments. version:1
arxiv-1706-01427 | A simple neural network module for relational reasoning | http://arxiv.org/abs/1706.01427 | id:1706.01427 author:Adam Santoro, David Raposo, David G. T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap category:cs.CL cs.LG  published:2017-06-05 summary:Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations. version:1
arxiv-1706-01418 | Learning Whenever Learning is Possible: Universal Learning under General Stochastic Processes | http://arxiv.org/abs/1706.01418 | id:1706.01418 author:Steve Hanneke category:stat.ML cs.LG math.PR math.ST stat.TH  published:2017-06-05 summary:This work initiates a general study of learning and generalization without the i.i.d. assumption, starting from first principles. While the standard approach to statistical learning theory is based on assumptions chosen largely for their convenience (e.g., i.i.d. or stationary ergodic), in this work we are interested in developing a theory of learning based only on the most fundamental and natural assumptions implicit in the requirements of the learning problem itself. We specifically study universally consistent function learning, where the objective is to obtain low long-run average loss for any target function, when the data follow a given stochastic process. We are then interested in the question of whether there exist learning rules guaranteed to be universally consistent given only the assumption that universally consistent learning is possible for the given data process. The reasoning that motivates this criterion emanates from a kind of optimist's decision theory, and so we refer to such learning rules as being optimistically universal. We study this question in three natural learning settings: inductive, self-adaptive, and online. Remarkably, as our strongest positive result, we find that optimistically universal learning rules do indeed exist in the self-adaptive learning setting. Establishing this fact requires us to develop new approaches to the design of learning algorithms. Along the way, we also identify concise characterizations of the family of processes under which universally consistent learning is possible in the inductive and self-adaptive settings. We additionally pose a number of enticing open problems, particularly for the online learning setting. version:1
arxiv-1706-00130 | Teaching Machines to Describe Images via Natural Language Feedback | http://arxiv.org/abs/1706.00130 | id:1706.00130 author:Huan Ling, Sanja Fidler category:cs.CL cs.AI cs.CV cs.HC  published:2017-06-01 summary:Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions. version:2
arxiv-1706-01406 | NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps | http://arxiv.org/abs/1706.01406 | id:1706.01406 author:Alessandro Aimar, Hesham Mostafa, Enrico Calabrese, Antonio Rios-Navarro, Ricardo Tapiador-Morales, Iulia-Alexandra Lungu, Moritz B. Milde, Federico Corradi, Alejandro Linares-Barranco, Shih-Chii Liu, Tobi Delbruck category:cs.CV cs.NE  published:2017-06-05 summary:Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving many state-of-the-art (SOA) visual processing tasks. Even though Graphical Processing Units (GPUs) are most often used in training and deploying CNNs, their power consumption becomes a problem for real time mobile applications. We propose a flexible and efficient CNN accelerator architecture which can support the implementation of SOA CNNs in low-power and low-latency application scenarios. This architecture exploits the sparsity of neuron activations in CNNs to accelerate the computation and reduce memory requirements. The flexible architecture allows high utilization of available computing resources across a wide range of convolutional network kernel sizes; and numbers of input and output feature maps. We implemented the proposed architecture on an FPGA platform and present results showing how our implementation reduces external memory transfers and compute time in five different CNNs ranging from small ones up to the widely known large VGG16 and VGG19 CNNs. We show how in RTL simulations in a 28nm process with a clock frequency of 500MHz, the NullHop core is able to reach over 450 GOp/s and efficiency of 368%, maintaining over 98% utilization of the MAC units and achieving a power efficiency of over 3TOp/s/W in a core area of 5.8mm2 version:1
arxiv-1706-01399 | Language Generation with Recurrent Generative Adversarial Networks without Pre-training | http://arxiv.org/abs/1706.01399 | id:1706.01399 author:Ofir Press, Amir Bar, Ben Bogin, Jonathan Berant, Lior Wolf category:cs.CL  published:2017-06-05 summary:Generative Adversarial Networks (GANs) have shown great promise recently in image generation. Training GANs for text generation has proven to be more difficult, because of the non-differentiable nature of generating text with recurrent neural networks. Consequently, past work has either resorted to pre-training with maximum-likelihood or used convolutional networks for generation. In this work, we show that recurrent neural networks can be trained to generate text with GANs from scratch by employing curriculum learning, slowly increasing the length of the generated text, and by training the RNN simultaneously to generate sequences of different lengths. We show that this approach vastly improves the quality of generated sequences compared to the convolutional baseline. version:1
arxiv-1706-01396 | ToPs: Ensemble Learning with Trees of Predictors | http://arxiv.org/abs/1706.01396 | id:1706.01396 author:Jinsung Yoon, William R. Zame, Mihaela van der Schaar category:cs.CV  published:2017-06-05 summary:We present a new approach to ensemble learning. Our approach constructs a tree of subsets of the feature space and associates a predictor (predictive model) - determined by training one of a given family of base learners on an endogenously determined training set - to each node of the tree; we call the resulting object a tree of predictors. The (locally) optimal tree of predictors is derived recursively; each step involves jointly optimizing the split of the terminal nodes of the previous tree and the choice of learner and training set (hence predictor) for each set in the split. The feature vector of a new instance determines a unique path through the optimal tree of predictors; the final prediction aggregates the predictions of the predictors along this path. We derive loss bounds for the final predictor in terms of the Rademacher complexity of the base learners. We report the results of a number of experiments on a variety of datasets, showing that our approach provides statistically significant improvements over state-of-the-art machine learning algorithms, including various ensemble learning methods. Our approach works because it allows us to endogenously create more complex learners - when needed - and endogenously match both the learner and the training set to the characteristics of the dataset while still avoiding over-fitting. version:1
arxiv-1706-01394 | Multi-Observation Elicitation | http://arxiv.org/abs/1706.01394 | id:1706.01394 author:Sebastian Casalaina-Martin, Rafael Frongillo, Tom Morgan, Bo Waggoner category:cs.LG  published:2017-06-05 summary:We study loss functions that measure the accuracy of a prediction based on multiple data points simultaneously. To our knowledge, such loss functions have not been studied before in the area of property elicitation or in machine learning more broadly. As compared to traditional loss functions that take only a single data point, these multi-observation loss functions can in some cases drastically reduce the dimensionality of the hypothesis required. In elicitation, this corresponds to requiring many fewer reports; in empirical risk minimization, it corresponds to algorithms on a hypothesis space of much smaller dimension. We explore some examples of the tradeoff between dimensionality and number of observations, give some geometric characterizations and intuition for relating loss functions and the properties that they elicit, and discuss some implications for both elicitation and machine-learning contexts. version:1
arxiv-1706-01383 | Sparse Stochastic Bandits | http://arxiv.org/abs/1706.01383 | id:1706.01383 author:Joon Kwon, Vianney Perchet, Claire Vernade category:cs.LG  published:2017-06-05 summary:In the classical multi-armed bandit problem, d arms are available to the decision maker who pulls them sequentially in order to maximize his cumulative reward. Guarantees can be obtained on a relative quantity called regret, which scales linearly with d (or with sqrt(d) in the minimax sense). We here consider the sparse case of this classical problem in the sense that only a small number of arms, namely s < d, have a positive expected reward. We are able to leverage this additional assumption to provide an algorithm whose regret scales with s instead of d. Moreover, we prove that this algorithm is optimal by providing a matching lower bound - at least for a wide and pertinent range of parameters that we determine - and by evaluating its performance on simulated data. version:1
arxiv-1706-01382 | Neuro-RAM Unit with Applications to Similarity Testing and Compression in Spiking Neural Networks | http://arxiv.org/abs/1706.01382 | id:1706.01382 author:Nancy Lynch, Cameron Musco, Merav Parter category:cs.NE cs.DC cs.DS q-bio.NC  published:2017-06-05 summary:We study distributed algorithms implemented in a simplified but biologically plausible model for stochastic spiking neural networks. We focus on tradeoffs between computation time and network complexity, along with the role of randomness in efficient neural computation. It is widely accepted that neural computation is inherently stochastic. In recent work, we explored how this stochasticity could be leveraged to solve the `winner-take-all' leader election task. Here, we focus on using randomness in neural algorithms for similarity testing and compression. In the most basic setting, given two $n$-length patterns of firing neurons, we wish to distinguish if the patterns are equal or $\epsilon$-far from equal. Randomization allows us to solve this task with a very compact network, using $O \left (\frac{\sqrt{n}\log n}{\epsilon}\right)$ auxiliary neurons, which is sublinear in the input size. At the heart of our solution is the design of a $t$-round neural random access memory, or indexing network, which we call a neuro-RAM. This module can be implemented with $O(n/t)$ auxiliary neurons and is useful in many applications beyond similarity testing. Using a combination of Yao's minimax principle and a VC dimension-based argument, we show that the tradeoff between runtime and network size in our neuro-RAM is nearly optimal. Our result has several implications -- since our neuro-RAM construction can be implemented with deterministic threshold gates, it demonstrates that, in contrast to similarity testing, randomness does not provide significant computational advantages for this problem. It also establishes a separation between our networks, which spike with a sigmoidal probability function, and well-studied but less biologically plausible deterministic sigmoidal networks, whose gates output real number values, and which can implement a neuro-RAM much more efficiently. version:1
arxiv-1705-07348 | Calibrating Black Box Classification Models through the Thresholding Method | http://arxiv.org/abs/1705.07348 | id:1705.07348 author:Arun Srinivasan category:stat.ML  published:2017-05-20 summary:In high-dimensional classification settings, we wish to seek a balance between high power and ensuring control over a desired loss function. In many settings, the points most likely to be misclassified are those who lie near the decision boundary of the given classification method. Often, these uninformative points should not be classified as they are noisy and do not exhibit strong signals. In this paper, we introduce the Thresholding Method to parameterize the problem of determining which points exhibit strong signals and should be classified. We demonstrate the empirical performance of this novel calibration method in providing loss function control at a desired level, as well as explore how the method assuages the effect of overfitting. We explore the benefits of error control through the Thresholding Method in difficult, high-dimensional, simulated settings. Finally, we show the flexibility of the Thresholding Method through applying the method in a variety of real data settings. version:2
arxiv-1706-01362 | The Geometry of Nodal Sets and Outlier Detection | http://arxiv.org/abs/1706.01362 | id:1706.01362 author:Xiuyuan Cheng, Gal Mishne, Stefan Steinerberger category:math.SP math-ph math.AP math.FA math.MP stat.ML  published:2017-06-05 summary:Let $(M,g)$ be a compact manifold and let $-\Delta \phi_k = \lambda_k \phi_k$ be the sequence of Laplacian eigenfunctions. We present a curious new phenomenon which, so far, we only managed to understand in a few highly specialized cases: the family of functions $f_N:M \rightarrow \mathbb{R}_{\geq 0}$ $$ f_N(x) = \sum_{k \leq N}{ \frac{1}{\sqrt{\lambda_k}} \frac{ \phi_k(x) }{\ \phi_k\ _{L^{\infty}(M)}}}$$ seems strangely suited for the detection of anomalous points on the manifold. It may be heuristically interpreted as the sum over distances to the nearest nodal line and potentially hints at a new phenomenon in spectral geometry. We give rigorous statements on the unit square $[0,1]^2$ (where minima localize in $\mathbb{Q}^2$) and on Paley graphs (where $f_N$ recovers the geometry of quadratic residues of the underlying finite field $\mathbb{F}_p$). Numerical examples show that the phenomenon seems to arise on fairly generic manifolds. version:1
arxiv-1706-01777 | Deep Factorization for Speech Signal | http://arxiv.org/abs/1706.01777 | id:1706.01777 author:Dong Wang, Lantian Li, Ying Shi, Yixiang Chen, Zhiyuan Tang category:cs.SD cs.LG  published:2017-06-05 summary:Speech signals are complex intermingling of various informative factors, and this information blending makes decoding any of the individual factors extremely difficult. A natural idea is to factorize each speech frame into independent factors, though it turns out to be even more difficult than decoding each individual factor. A major encumbrance is that the speaker trait, a major factor in speech signals, has been suspected to be a long-term distributional pattern and so not identifiable at the frame level. In this paper, we demonstrated that the speaker factor is also a short-time spectral pattern and can be largely identified with just a few frames using a simple deep neural network (DNN). This discovery motivated a cascade deep factorization (CDF) framework that infers speech factors in a sequential way, and factors previously inferred are used as conditional variables when inferring other factors. Our experiment on an automatic emotion recognition (AER) task demonstrated that this approach can effectively factorize speech signals, and using these factors, the original speech spectrum can be recovered with high accuracy. This factorization and reconstruction approach provides a novel tool for many speech processing tasks. version:1
arxiv-1706-01350 | On the Emergence of Invariance and Disentangling in Deep Representations | http://arxiv.org/abs/1706.01350 | id:1706.01350 author:Alessandro Achille, Stefano Soatto category:cs.LG cs.AI stat.ML  published:2017-06-05 summary:Using classical notions of statistical decision and information theory, we show that invariance in a deep neural network is equivalent to minimality of the representation it computes, and can be achieved by stacking layers and injecting noise in the computation, under realistic and empirically validated assumptions. We use an Information Decomposition of the empirical loss to show that overfitting can be reduced by limiting the information content stored in the weights. We then present a sharp inequality that relates the information content in the weights -- which are a representation of the training set and inferred by generic optimization agnostic of invariance and disentanglement -- and the minimality and total correlation of the activation functions, which are a representation of the test datum. This allows us to tackle recent puzzles concerning the generalization properties of deep networks and their relation to the geometry of the optimization residual. version:1
arxiv-1706-01331 | Event Representations for Automated Story Generation with Deep Neural Nets | http://arxiv.org/abs/1706.01331 | id:1706.01331 author:Lara J. Martin, Prithviraj Ammanabrolu, William Hancock, Shruti Singh, Brent Harrison, Mark O. Riedl category:cs.CL cs.AI cs.LG cs.NE  published:2017-06-05 summary:Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language. version:1
arxiv-1706-01330 | Neuroevolution on the Edge of Chaos | http://arxiv.org/abs/1706.01330 | id:1706.01330 author:Filip Matzner category:cs.NE  published:2017-06-05 summary:Echo state networks represent a special type of recurrent neural networks. Recent papers stated that the echo state networks maximize their computational performance on the transition between order and chaos, the so-called edge of chaos. This work confirms this statement in a comprehensive set of experiments. Furthermore, the echo state networks are compared to networks evolved via neuroevolution. The evolved networks outperform the echo state networks, however, the evolution consumes significant computational resources. It is demonstrated that echo state networks with local connections combine the best of both worlds, the simplicity of random echo state networks and the performance of evolved networks. Finally, it is shown that evolution tends to stay close to the ordered side of the edge of chaos. version:1
arxiv-1705-09137 | Neural Decomposition of Time-Series Data for Effective Generalization | http://arxiv.org/abs/1705.09137 | id:1705.09137 author:Luke B. Godfrey, Michael S. Gashler category:cs.NE  published:2017-05-25 summary:We present a neural network technique for the analysis and extrapolation of time-series data called Neural Decomposition (ND). Units with a sinusoidal activation function are used to perform a Fourier-like decomposition of training samples into a sum of sinusoids, augmented by units with nonperiodic activation functions to capture linear trends and other nonperiodic components. We show how careful weight initialization can be combined with regularization to form a simple model that generalizes well. Our method generalizes effectively on the Mackey-Glass series, a dataset of unemployment rates as reported by the U.S. Department of Labor Statistics, a time-series of monthly international airline passengers, the monthly ozone concentration in downtown Los Angeles, and an unevenly sampled time-series of oxygen isotope measurements from a cave in north India. We find that ND outperforms popular time-series forecasting techniques including LSTM, echo state networks, ARIMA, SARIMA, SVR with a radial basis function, and Gashler and Ashmore's model. version:2
arxiv-1706-01322 | Deep learning evaluation using deep linguistic processing | http://arxiv.org/abs/1706.01322 | id:1706.01322 author:Alexander Kuhnle, Ann Copestake category:cs.CL cs.AI cs.CV cs.LG  published:2017-06-05 summary:We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing 'deep' linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail. version:1
arxiv-1706-01307 | Submanifold Sparse Convolutional Networks | http://arxiv.org/abs/1706.01307 | id:1706.01307 author:Benjamin Graham, Laurens van der Maaten category:cs.NE cs.CV  published:2017-06-05 summary:Convolutional network are the de-facto standard for analysing spatio-temporal data such as images, videos, 3D shapes, etc. Whilst some of this data is naturally dense (for instance, photos), many other data sources are inherently sparse. Examples include pen-strokes forming on a piece of paper, or (colored) 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard "dense" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce a sparse convolutional operation tailored to processing sparse data that differs from prior work on sparse convolutional networks in that it operates strictly on submanifolds, rather than "dilating" the observation with every layer in the network. Our empirical analysis of the resulting submanifold sparse convolutional networks shows that they perform on par with state-of-the-art methods whilst requiring substantially less computation. version:1
arxiv-1706-01284 | Learning Neural Programs To Parse Programs | http://arxiv.org/abs/1706.01284 | id:1706.01284 author:Xinyun Chen, Chang Liu, Dawn Song category:cs.LG cs.AI cs.PL  published:2017-06-05 summary:In this work, we study an important problem: learning programs from input-output examples. We propose a novel method to learn a neural program operating a domain-specific non-differentiable machine, and demonstrate that this method can be applied to learn programs that are significantly more complex than the ones synthesized before: programming language parsers from input-output pairs without knowing the underlying grammar. The main challenge is to train the neural program without supervision on execution traces. To tackle it, we propose: (1) LL machines and neural programs operating them to effectively regularize the space of the learned programs; and (2) a two-phase reinforcement learning-based search technique to train the model. Our evaluation demonstrates that our approach can successfully learn to parse programs in both an imperative language and a functional language, and achieve 100% test accuracy, while existing approaches' accuracies are almost 0%. This is the first successful demonstration of applying reinforcement learning to train a neural program operating a non-differentiable machine that can fully generalize to test sets on a non-trivial task. version:1
arxiv-1706-01242 | Bayesian LSTMs in medicine | http://arxiv.org/abs/1706.01242 | id:1706.01242 author:Jos van der Westhuizen, Joan Lasenby category:stat.ML cs.LG stat.AP  published:2017-06-05 summary:The medical field stands to see significant benefits from the recent advances in deep learning. Knowing the uncertainty in the decision made by any machine learning algorithm is of utmost importance for medical practitioners. This study demonstrates the utility of using Bayesian LSTMs for classification of medical time series. Four medical time series datasets are used to show the accuracy improvement Bayesian LSTMs provide over standard LSTMs. Moreover, we show cherry-picked examples of confident and uncertain classifications of the medical time series. With simple modifications of the common practice for deep learning, significant improvements can be made for the medical practitioner and patient. version:1
arxiv-1706-01237 | Learning Structured Semantic Embeddings for Visual Recognition | http://arxiv.org/abs/1706.01237 | id:1706.01237 author:Dong Li, Hsin-Ying Lee, Jia-Bin Huang, Shengjin Wang, Ming-Hsuan Yang category:cs.CV  published:2017-06-05 summary:Numerous embedding models have been recently explored to incorporate semantic knowledge into visual recognition. Existing methods typically focus on minimizing the distance between the corresponding images and texts in the embedding space but do not explicitly optimize the underlying structure. Our key observation is that modeling the pairwise image-image relationship improves the discrimination ability of the embedding model. In this paper, we propose the structured discriminative and difference constraints to learn visual-semantic embeddings. First, we exploit the discriminative constraints to capture the intra- and inter-class relationships of image embeddings. The discriminative constraints encourage separability for image instances of different classes. Second, we align the difference vector between a pair of image embeddings with that of the corresponding word embeddings. The difference constraints help regularize image embeddings to preserve the semantic relationships among word embeddings. Extensive evaluations demonstrate the effectiveness of the proposed structured embeddings for single-label classification, multi-label classification, and zero-shot recognition. version:1
arxiv-1706-01231 | Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning | http://arxiv.org/abs/1706.01231 | id:1706.01231 author:Jingkuan Song, Zhao Guo, Lianli Gao, Wu Liu, Dongxiang Zhang, Heng Tao Shen category:cs.CV  published:2017-06-05 summary:Recent progress has been made in using attention based encoder-decoder framework for video captioning. However, most existing decoders apply the attention mechanism to every generated word including both visual words (e.g., "gun" and "shooting") and non-visual words (e.g. "the", "a"). However, these non-visual words can be easily predicted using natural language model without considering visual signals or attention. Imposing attention mechanism on non-visual words could mislead and decrease the overall performance of video captioning. To address this issue, we propose a hierarchical LSTM with adjusted temporal attention (hLSTMat) approach for video captioning. Specifically, the proposed framework utilizes the temporal attention for selecting specific frames to predict the related words, while the adjusted temporal attention is for deciding whether to depend on the visual information or the language context information. Also, a hierarchical LSTMs is designed to simultaneously consider both low-level visual information and high-level language context information to support the video caption generation. To demonstrate the effectiveness of our proposed framework, we test our method on two prevalent datasets: MSVD and MSR-VTT, and experimental results show that our approach outperforms the state-of-the-art methods on both two datasets. version:1
arxiv-1706-01215 | Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework | http://arxiv.org/abs/1706.01215 | id:1706.01215 author:Shuochao Yao, Yiran Zhao, Aston Zhang, Lu Su, Tarek Abdelzaher category:cs.LG cs.NE cs.NI  published:2017-06-05 summary:Recent advances in deep learning motivate the use of deep neutral networks in sensing applications, but their excessive resource needs on constrained embedded devices remain an important impediment. A recently explored solution space lies in compressing (approximating or simplifying) deep neural networks in some manner before use on the device. We propose a new compression solution, called DeepIoT, that makes two key contributions in that space. First, unlike current solutions geared for compressing specific types of neural networks, DeepIoT presents a unified approach that compresses all commonly used deep learning structures for sensing applications, including fully-connected, convolutional, and recurrent neural networks, as well as their combinations. Second, unlike solutions that either sparsify weight matrices or assume linear structure within weight matrices, DeepIoT compresses neural network structures into smaller dense matrices by finding the minimum number of non-redundant hidden elements, such as filters and dimensions required by each layer, while keeping the performance of sensing applications the same. Importantly, it does so using an approach that obtains a global view of parameter redundancies, which is shown to produce superior compression. We conduct experiments with five different sensing-related tasks on Intel Edison devices. DeepIoT outperforms all compared baseline algorithms with respect to execution time and energy consumption by a significant margin. It reduces the size of deep neural networks by 90% to 98.9%. It is thus able to shorten execution time by 71.4% to 94.5%, and decrease energy consumption by 72.2% to 95.7%. These improvements are achieved without loss of accuracy. The results underscore the potential of DeepIoT for advancing the exploitation of deep neural networks on resource-constrained embedded devices. version:1
arxiv-1706-01214 | Inconsistent Node Flattening for Improving Top-down Hierarchical Classification | http://arxiv.org/abs/1706.01214 | id:1706.01214 author:Azad Naik, Huzefa Rangwala category:cs.LG stat.ML  published:2017-06-05 summary:Large-scale classification of data where classes are structurally organized in a hierarchy is an important area of research. Top-down approaches that exploit the hierarchy during the learning and prediction phase are efficient for large scale hierarchical classification. However, accuracy of top-down approaches is poor due to error propagation i.e., prediction errors made at higher levels in the hierarchy cannot be corrected at lower levels. One of the main reason behind errors at the higher levels is the presence of inconsistent nodes that are introduced due to the arbitrary process of creating these hierarchies by domain experts. In this paper, we propose two different data-driven approaches (local and global) for hierarchical structure modification that identifies and flattens inconsistent nodes present within the hierarchy. Our extensive empirical evaluation of the proposed approaches on several image and text datasets with varying distribution of features, classes and training instances per class shows improved classification performance over competing hierarchical modification approaches. Specifically, we see an improvement upto 7% in Macro-F1 score with our approach over best TD baseline. SOURCE CODE: http://www.cs.gmu.edu/~mlbio/InconsistentNodeFlattening version:1
arxiv-1706-01209 | A Kind of Affine Weighted Moment Invariants | http://arxiv.org/abs/1706.01209 | id:1706.01209 author:Hanlin Mo, You Hao, Shirui Li, Hua Li category:cs.CV  published:2017-06-05 summary:A new kind of geometric invariants is proposed in this paper, which is called affine weighted moment invariant (AWMI). By combination of local affine differential invariants and a framework of global integral, they can more effectively extract features of images and help to increase the number of low-order invariants and to decrease the calculating cost. The experimental results show that AWMIs have good stability and distinguishability and achieve better results in image retrieval than traditional affine geometric moment invariants(AGMIs). An extension to 3D is straightforward. version:1
arxiv-1706-01206 | One-step and Two-step Classification for Abusive Language Detection on Twitter | http://arxiv.org/abs/1706.01206 | id:1706.01206 author:Ji Ho Park, Pascale Fung category:cs.CL  published:2017-06-05 summary:Automatic abusive language detection is a difficult but important task for online social media. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 F-measure by using HybridCNN in one-step and 0.824 F-measure by using logistic regression in two-steps. version:1
arxiv-1706-01191 | The Likelihood Ratio Test in High-Dimensional Logistic Regression Is Asymptotically a Rescaled Chi-Square | http://arxiv.org/abs/1706.01191 | id:1706.01191 author:Pragya Sur, Yuxin Chen, Emmanuel J. Candès category:math.ST cs.IT math.IT math.PR stat.ML stat.TH  published:2017-06-05 summary:Logistic regression is used thousands of times a day to fit data, predict future outcomes, and assess the statistical significance of explanatory variables. When used for the purpose of statistical inference, logistic models produce p-values for the regression coefficients by using an approximation to the distribution of the likelihood-ratio test. Indeed, Wilks' theorem asserts that whenever we have a fixed number $p$ of variables, twice the log-likelihood ratio (LLR) $2\Lambda$ is distributed as a $\chi^2_k$ variable in the limit of large sample sizes $n$; here, $k$ is the number of variables being tested. In this paper, we prove that when $p$ is not negligible compared to $n$, Wilks' theorem does not hold and that the chi-square approximation is grossly incorrect; in fact, this approximation produces p-values that are far too small (under the null hypothesis). Assume that $n$ and $p$ grow large in such a way that $p/n\rightarrow\kappa$ for some constant $\kappa < 1/2$. We prove that for a class of logistic models, the LLR converges to a rescaled chi-square, namely, $2\Lambda~\stackrel{\mathrm{d}}{\rightarrow}~\alpha(\kappa)\chi_k^2$, where the scaling factor $\alpha(\kappa)$ is greater than one as soon as the dimensionality ratio $\kappa$ is positive. Hence, the LLR is larger than classically assumed. For instance, when $\kappa=0.3$, $\alpha(\kappa)\approx1.5$. In general, we show how to compute the scaling factor by solving a nonlinear system of two equations with two unknowns. Our mathematical arguments are involved and use techniques from approximate message passing theory, non-asymptotic random matrix theory and convex geometry. We also complement our mathematical study by showing that the new limiting distribution is accurate for finite sample sizes. Finally, all the results from this paper extend to some other regression models such as the probit regression model. version:1
arxiv-1706-01177 | PReP: Path-Based Relevance from a Probabilistic Perspective in Heterogeneous Information Networks | http://arxiv.org/abs/1706.01177 | id:1706.01177 author:Yu Shi, Po-Wei Chan, Honglei Zhuang, Huan Gui, Jiawei Han category:cs.SI cs.LG  published:2017-06-05 summary:As a powerful representation paradigm for networked and multi-typed data, the heterogeneous information network (HIN) is ubiquitous. Meanwhile, defining proper relevance measures has always been a fundamental problem and of great pragmatic importance for network mining tasks. Inspired by our probabilistic interpretation of existing path-based relevance measures, we propose to study HIN relevance from a probabilistic perspective. We also identify, from real-world data, and propose to model cross-meta-path synergy, which is a characteristic important for defining path-based HIN relevance and has not been modeled by existing methods. A generative model is established to derive a novel path-based relevance measure, which is data-driven and tailored for each HIN. We develop an inference algorithm to find the maximum a posteriori (MAP) estimate of the model parameters, which entails non-trivial tricks. Experiments on two real-world datasets demonstrate the effectiveness of the proposed model and relevance measure. version:1
arxiv-1706-00553 | Rank Persistence: Assessing the Temporal Performance of Real-World Person Re-Identification | http://arxiv.org/abs/1706.00553 | id:1706.00553 author:Srikrishna Karanam, Eric Lam, Richard J. Radke category:cs.CV  published:2017-06-02 summary:Designing useful person re-identification systems for real-world applications requires attention to operational aspects not typically considered in academic research. Here, we focus on the temporal aspect of re-identification; that is, instead of finding a match to a probe person of interest in a fixed candidate gallery, we consider the more realistic scenario in which the gallery is continuously populated by new candidates over a long time period. A key question of interest for an operator of such a system is: how long is a correct match to a probe likely to remain in a rank-k shortlist of possible candidates? We propose to distill this information into a Rank Persistence Curve (RPC), which allows different algorithms' temporal performance characteristics to be directly compared. We present examples to illustrate the RPC using a new long-term dataset with multiple candidate reappearances, and discuss considerations for future re-identification research that explicitly involves temporal aspects. version:2
arxiv-1706-01171 | Binary Patterns Encoded Convolutional Neural Networks for Texture Recognition and Remote Sensing Scene Classification | http://arxiv.org/abs/1706.01171 | id:1706.01171 author:Rao Muhammad Anwer, Fahad Shahbaz Khan, Joost van de Weijer, Matthieu Molinier, Jorma Laaksonen category:cs.CV  published:2017-06-05 summary:Designing discriminative powerful texture features robust to realistic imaging conditions is a challenging computer vision problem with many applications, including material recognition and analysis of satellite or aerial imagery. In the past, most texture description approaches were based on dense orderless statistical distribution of local features. However, most recent approaches to texture recognition and remote sensing scene classification are based on Convolutional Neural Networks (CNNs). The d facto practice when learning these CNN models is to use RGB patches as input with training performed on large amounts of labeled data (ImageNet). In this paper, we show that Binary Patterns encoded CNN models, codenamed TEX-Nets, trained using mapped coded images with explicit texture information provide complementary information to the standard RGB deep models. Additionally, two deep architectures, namely early and late fusion, are investigated to combine the texture and color information. To the best of our knowledge, we are the first to investigate Binary Patterns encoded CNNs and different deep network fusion architectures for texture recognition and remote sensing scene classification. We perform comprehensive experiments on four texture recognition datasets and four remote sensing scene classification benchmarks: UC-Merced with 21 scene categories, WHU-RS19 with 19 scene classes, RSSCN7 with 7 categories and the recently introduced large scale aerial image dataset (AID) with 30 aerial scene types. We demonstrate that TEX-Nets provide complementary information to standard RGB deep model of the same network architecture. Our late fusion TEX-Net architecture always improves the overall performance compared to the standard RGB network on both recognition problems. Our final combination outperforms the state-of-the-art without employing fine-tuning or ensemble of RGB network architectures. version:1
arxiv-1705-08922 | Exploring the Regularity of Sparse Structure in Convolutional Neural Networks | http://arxiv.org/abs/1705.08922 | id:1705.08922 author:Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu Wang, William J. Dally category:cs.LG stat.ML  published:2017-05-24 summary:Sparsity helps reduce the computational complexity of deep neural networks by skipping zeros. Taking advantage of sparsity is listed as a high priority in next generation DNN accelerators such as TPU. The structure of sparsity, i.e., the granularity of pruning, affects the efficiency of hardware accelerator design as well as the prediction accuracy. Coarse-grained pruning creates regular sparsity patterns, making it more amenable for hardware acceleration but more challenging to maintain the same accuracy. In this paper we quantitatively measure the trade-off between sparsity regularity and prediction accuracy, providing insights in how to maintain accuracy while having more a more structured sparsity pattern. Our experimental results show that coarse-grained pruning can achieve a sparsity ratio similar to unstructured pruning without loss of accuracy. Moreover, due to the index saving effect, coarse-grained pruning is able to obtain a better compression ratio than fine-grained sparsity at the same accuracy threshold. Based on the recent sparse convolutional neural network accelerator (SCNN), our experiments further demonstrate that coarse-grained sparsity saves about 2x the memory references compared to fine-grained sparsity. Since memory reference is more than two orders of magnitude more expensive than arithmetic operations, the regularity of sparse structure leads to more efficient hardware design. version:3
arxiv-1705-06640 | DeepXplore: Automated Whitebox Testing of Deep Learning Systems | http://arxiv.org/abs/1705.06640 | id:1705.06640 author:Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana category:cs.LG cs.CR cs.SE  published:2017-05-18 summary:Deep learning (DL) systems are increasingly deployed in security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner-case inputs are of great importance. However, systematic testing of large-scale DL systems with thousands of neurons and millions of parameters for all possible corner-cases is a hard problem. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose different erroneous behaviors for rare inputs. We present DeepXplore, the first whitebox framework for systematically testing real-world DL systems. We address two problems: (1) generating inputs that trigger different parts of a DL system's logic and (2) identifying incorrect behaviors of DL systems without manual effort. First, we introduce neuron coverage for estimating the parts of DL system exercised by a set of test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles and thus avoid manual checking for erroneous behaviors. We demonstrate how finding inputs triggering differential behaviors while achieving high neuron coverage for DL algorithms can be represented as a joint optimization problem and solved efficiently using gradient-based optimization techniques. DeepXplore finds thousands of incorrect corner-case behaviors in state-of-the-art DL models trained on five popular datasets. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running on a commodity laptop. The inputs generated by DeepXplore achieved 33.2% higher neuron coverage on average than existing testing methods. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve classification accuracy or identify polluted training data. version:2
arxiv-1705-07220 | Data-adaptive Active Sampling for Efficient Graph-Cognizant Classification | http://arxiv.org/abs/1705.07220 | id:1705.07220 author:Dimitris Berberidis, Georgios B. Giannakis category:stat.ML  published:2017-05-19 summary:The present work deals with active sampling of graph nodes representing training data for binary classification. The graph may be given or constructed using similarity measures among nodal features. Leveraging the graph for classification builds on the premise that labels across neighboring nodes are correlated according to a categorical Markov random field (MRF). This model is further relaxed to a Gaussian (G)MRF with labels taking continuous values - an approximation that not only mitigates the combinatorial complexity of the categorical model, but also offers optimal unbiased soft predictors of the unlabeled nodes. The proposed sampling strategy is based on querying the node whose label disclosure is expected to inflict the largest change on the GMRF, and in this sense it is the most informative on average. Such a strategy subsumes several measures of expected model change, including uncertainty sampling, variance minimization, and sampling based on the $\Sigma-$optimality criterion. A simple yet effective heuristic is also introduced for increasing the exploration capabilities of the sampler, and reducing bias of the resultant classifier, by taking into account the confidence on the model label predictions. The novel sampling strategies are based on quantities that are readily available without the need for model retraining, rendering them computationally efficient and scalable to large graphs. Numerical tests using synthetic and real data demonstrate that the proposed methods achieve accuracy that is comparable or superior to the state-of-the-art even at reduced runtime. version:2
arxiv-1706-01159 | Deep Frame Interpolation | http://arxiv.org/abs/1706.01159 | id:1706.01159 author:Vladislav Samsonov category:cs.CV  published:2017-06-04 summary:This work presents a supervised learning based approach to the computer vision problem of frame interpolation. The presented technique could also be used in the cartoon animations since drawing each individual frame consumes a noticeable amount of time. The most existing solutions to this problem use unsupervised methods and focus only on real life videos with already high frame rate. However, the experiments show that such methods do not work as well when the frame rate becomes low and object displacements between frames becomes large. This is due to the fact that interpolation of the large displacement motion requires knowledge of the motion structure thus the simple techniques such as frame averaging start to fail. In this work the deep convolutional neural network is used to solve the frame interpolation problem. In addition, it is shown that incorporating the prior information such as optical flow improves the interpolation quality significantly. version:1
arxiv-1706-01158 | Graphical Nonconvex Optimization for Optimal Estimation in Gaussian Graphical Models | http://arxiv.org/abs/1706.01158 | id:1706.01158 author:Qiang Sun, Kean Ming Tan, Han Liu, Tong Zhang category:stat.ML math.ST stat.TH  published:2017-06-04 summary:We consider the problem of learning high-dimensional Gaussian graphical models. The graphical lasso is one of the most popular methods for estimating Gaussian graphical models. However, it does not achieve the oracle rate of convergence. In this paper, we propose the graphical nonconvex optimization for optimal estimation in Gaussian graphical models, which is then approximated by a sequence of convex programs. Our proposal is computationally tractable and produces an estimator that achieves the oracle rate of convergence. The statistical error introduced by the sequential approximation using the convex programs are clearly demonstrated via a contraction property. The rate of convergence can be further improved using the notion of sparsity pattern. The proposed methodology is then extended to semiparametric graphical models. We show through numerical studies that the proposed estimator outperforms other popular methods for estimating Gaussian graphical models. version:1
arxiv-1705-08360 | Efficient and principled score estimation | http://arxiv.org/abs/1705.08360 | id:1705.08360 author:Dougal J. Sutherland, Heiko Strathmann, Michael Arbel, Arthur Gretton category:stat.ML cs.LG stat.ME  published:2017-05-23 summary:We propose a fast method with statistical guarantees for learning an exponential family density model where the natural parameter is in a reproducing kernel Hilbert space, and may be infinite dimensional. The model is learned by fitting the derivative of the log density, the score, thus avoiding the need to compute a normalization constant. We improved the computational efficiency of an earlier solution with a low-rank, Nystr\"om-like solution. The new solution remains consistent, and is shown to converge in Fisher distance at the same rate as a full-rank solution, with guarantees on the degree of cost and storage reduction. We compare to a popular score learning approach using a denoising autoencoder, in experiments on density estimation and in the construction of an adaptive Hamiltonian Monte Carlo sampler. Apart from the lack of statistical guarantees for the autoencoder, our estimator is more data-efficient when estimating the score, runs faster, and has fewer parameters (which can be tuned in a principled and interpretable way). version:2
arxiv-1706-01151 | Deep MIMO Detection | http://arxiv.org/abs/1706.01151 | id:1706.01151 author:Neev Samuel, Tzvi Diskin, Ami Wiesel category:stat.ML cs.IT cs.LG math.IT  published:2017-06-04 summary:In this paper, we consider the use of deep neural networks in the context of Multiple-Input-Multiple-Output (MIMO) detection. We give a brief introduction to deep learning and propose a modern neural network architecture suitable for this detection task. First, we consider the case in which the MIMO channel is constant, and we learn a detector for a specific system. Next, we consider the harder case in which the parameters are known yet changing and a single detector must be learned for all multiple varying channels. We demonstrate the performance of our deep MIMO detector using numerical simulations in comparison to competing methods including approximate message passing and semidefinite relaxation. The results show that deep networks can achieve state of the art accuracy with significantly lower complexity while providing robustness against ill conditioned channels and mis-specified noise variance. version:1
arxiv-1706-01148 | Segmentation of Intracranial Arterial Calcification with Deeply Supervised Residual Dropout Networks | http://arxiv.org/abs/1706.01148 | id:1706.01148 author:Gerda Bortsova, Gijs van Tulder, Florian Dubost, Tingying Peng, Nassir Navab, Aad van der Lugt, Daniel Bos, Marleen de Bruijne category:cs.CV  published:2017-06-04 summary:Intracranial carotid artery calcification (ICAC) is a major risk factor for stroke, and might contribute to dementia and cognitive decline. Reliance on time-consuming manual annotation of ICAC hampers much demanded further research into the relationship between ICAC and neurological diseases. Automation of ICAC segmentation is therefore highly desirable, but difficult due to the proximity of the lesions to bony structures with a similar attenuation coefficient. In this paper, we propose a method for automatic segmentation of ICAC; the first to our knowledge. Our method is based on a 3D fully convolutional neural network that we extend with two regularization techniques. Firstly, we use deep supervision (hidden layers supervision) to encourage discriminative features in the hidden layers. Secondly, we augment the network with skip connections, as in the recently developed ResNet, and dropout layers, inserted in a way that skip connections circumvent them. We investigate the effect of skip connections and dropout. In addition, we propose a simple problem-specific modification of the network objective function that restricts the focus to the most important image regions and simplifies the optimization. We train and validate our model using 882 CT scans and test on 1,000. Our regularization techniques and objective improve the average Dice score by 7.1%, yielding an average Dice of 76.2% and 97.7% correlation between predicted ICAC volumes and manual annotations. version:1
arxiv-1705-00956 | Experimental Design for Non-Parametric Correction of Misspecified Dynamical Models | http://arxiv.org/abs/1705.00956 | id:1705.00956 author:Gal Shulkind, Lior Horesh, Haim Avron category:stat.ML  published:2017-05-02 summary:We consider a class of misspecified dynamical models where the governing term is only approximately known. Under the assumption that observations of the system's evolution are accessible for various initial conditions, our goal is to infer a non-parametric correction to the misspecified driving term such as to faithfully represent the system dynamics and devise system evolution predictions for unobserved initial conditions. We model the unknown correction term as a Gaussian Process and analyze the problem of efficient experimental design to find an optimal correction term under constraints such as a limited experimental budget. We suggest a novel formulation for experimental design for this Gaussian Process and show that approximately optimal (up to a constant factor) designs may be efficiently derived by utilizing results from the literature on submodular optimization. Our numerical experiments exemplify the effectiveness of these techniques. version:3
arxiv-1706-01120 | Evolving imputation strategies for missing data in classification problems with TPOT | http://arxiv.org/abs/1706.01120 | id:1706.01120 author:Unai Garciarena, Roberto Santana, Alexander Mendiburu category:cs.LG stat.ML 65C99 D.2.2  published:2017-06-04 summary:Missing data has a ubiquitous presence in real-life applications of machine learning techniques. Imputation methods are algorithms conceived for restoring missing values in the data, based on other entries in the database. The choice of the imputation method has an influence on the performance of the machine learning technique, e.g., it influences the accuracy of the classification algorithm applied to the data. Therefore, selecting and applying the right imputation method is important and usually requires a substantial amount of human intervention. In this paper we propose the use of genetic programming techniques to search for the right combination of imputation and classification algorithms. We build our work on the recently introduced Python-based TPOT library, and incorporate a heterogeneous set of imputation algorithms as part of the machine learning pipeline search. We show that genetic programming can automatically find increasingly better pipelines that include the most effective combinations of imputation methods, feature pre-processing, and classifiers for a variety of classification problems with missing data. version:1
arxiv-1706-01115 | A Random-Fern based Feature Approach for Image Matching | http://arxiv.org/abs/1706.01115 | id:1706.01115 author:Yong Khoo, Seo-hyeon Keun category:cs.CV  published:2017-06-04 summary:Image or object recognition is an important task in computer vision. With the hight-speed processing power on modern platforms and the availability of mobile phones everywhere, millions of photos are uploaded to the internet per minute, it is critical to establish a generic framework for fast and accurate image processing for automatic recognition and information retrieval. In this paper, we proposed an efficient image recognition and matching method that is originally derived from Naive Bayesian classification method to construct a probabilistic model. Our method support real-time performance and have very high ability to distinguish similar images with high details. Experiments are conducted together with intensive comparison with state-of-the-arts on image matching, such as Ferns recognition and SIFT recognition. The results demonstrate satisfactory performance. version:1
arxiv-1706-01109 | InfiniteBoost: building infinite ensembles with gradient descent | http://arxiv.org/abs/1706.01109 | id:1706.01109 author:Alex Rogozhnikov, Tatiana Likhomanenko category:stat.ML cs.LG  published:2017-06-04 summary:In machine learning ensemble methods have demonstrated high accuracy for the variety of problems in different areas. The most known algorithms intensively used in practice are random forests and gradient boosting. In this paper we present InfiniteBoost - a novel algorithm, which combines the best properties of these two approaches. The algorithm constructs the ensemble of trees for which two properties hold: trees of the ensemble incorporate the mistakes done by others; at the same time the ensemble could contain the infinite number of trees without the over-fitting effect. The proposed algorithm is evaluated on the regression, classification, and ranking tasks using large scale, publicly available datasets. version:1
arxiv-1705-02033 | KATE: K-Competitive Autoencoder for Text | http://arxiv.org/abs/1705.02033 | id:1705.02033 author:Yu Chen, Mohammed J. Zaki category:stat.ML cs.LG  published:2017-05-04 summary:Autoencoders have been successful in learning meaningful representations from image datasets. However, their performance on text datasets has not been widely studied. Traditional autoencoders tend to learn possibly trivial representations of text documents due to their confounding properties such as high-dimensionality, sparsity and power-law word distributions. In this paper, we propose a novel k-competitive autoencoder, called KATE, for text documents. Due to the competition between the neurons in the hidden layer, each neuron becomes specialized in recognizing specific data patterns, and overall the model can learn meaningful representations of textual data. A comprehensive set of experiments show that KATE can learn better representations than traditional autoencoders including denoising, contractive, variational, and k-sparse autoencoders. Our model also outperforms deep generative models, probabilistic topic models, and even word representation models (e.g., Word2Vec) in terms of several downstream tasks such as document classification, regression, and retrieval. version:2
arxiv-1706-01084 | Joint Text Embedding for Personalized Content-based Recommendation | http://arxiv.org/abs/1706.01084 | id:1706.01084 author:Ting Chen, Liangjie Hong, Yue Shi, Yizhou Sun category:cs.IR cs.CL cs.LG  published:2017-06-04 summary:Learning a good representation of text is key to many recommendation applications. Examples include news recommendation where texts to be recommended are constantly published everyday. However, most existing recommendation techniques, such as matrix factorization based methods, mainly rely on interaction histories to learn representations of items. While latent factors of items can be learned effectively from user interaction data, in many cases, such data is not available, especially for newly emerged items. In this work, we aim to address the problem of personalized recommendation for completely new items with text information available. We cast the problem as a personalized text ranking problem and propose a general framework that combines text embedding with personalized recommendation. Users and textual content are embedded into latent feature space. The text embedding function can be learned end-to-end by predicting user interactions with items. To alleviate sparsity in interaction data, and leverage large amount of text data with little or no user interactions, we further propose a joint text embedding model that incorporates unsupervised text embedding with a combination module. Experimental results show that our model can significantly improve the effectiveness of recommendation systems on real-world datasets. version:1
arxiv-1706-01081 | Nearly Optimal Sampling Algorithms for Combinatorial Pure Exploration | http://arxiv.org/abs/1706.01081 | id:1706.01081 author:Lijie Chen, Anupam Gupta, Jian Li, Mingda Qiao, Ruosong Wang category:cs.LG cs.DS stat.ML  published:2017-06-04 summary:We study the combinatorial pure exploration problem Best-Set in stochastic multi-armed bandits. In a Best-Set instance, we are given $n$ arms with unknown reward distributions, as well as a family $\mathcal{F}$ of feasible subsets over the arms. Our goal is to identify the feasible subset in $\mathcal{F}$ with the maximum total mean using as few samples as possible. The problem generalizes the classical best arm identification problem and the top-$k$ arm identification problem, both of which have attracted significant attention in recent years. We provide a novel instance-wise lower bound for the sample complexity of the problem, as well as a nontrivial sampling algorithm, matching the lower bound up to a factor of $\ln \mathcal{F} $. For an important class of combinatorial families, we also provide polynomial time implementation of the sampling algorithm, using the equivalence of separation and optimization for convex program, and approximate Pareto curves in multi-objective optimization. We also show that the $\ln \mathcal{F} $ factor is inevitable in general through a nontrivial lower bound construction. Our results significantly improve several previous results for several important combinatorial constraints, and provide a tighter understanding of the general Best-Set problem. We further introduce an even more general problem, formulated in geometric terms. We are given $n$ Gaussian arms with unknown means and unit variance. Consider the $n$-dimensional Euclidean space $\mathbb{R}^n$, and a collection $\mathcal{O}$ of disjoint subsets. Our goal is to determine the subset in $\mathcal{O}$ that contains the $n$-dimensional vector of the means. The problem generalizes most pure exploration bandit problems studied in the literature. We provide the first nearly optimal sample complexity upper and lower bounds for the problem. version:1
arxiv-1706-01069 | CRNN: A Joint Neural Network for Redundancy Detection | http://arxiv.org/abs/1706.01069 | id:1706.01069 author:Xinyu Fu, Eugene Ch'ng, Uwe Aickelin, Simon See category:cs.CL  published:2017-06-04 summary:This paper proposes a novel framework for detecting redundancy in supervised sentence categorisation. Unlike traditional singleton neural network, our model incorporates character-aware convolutional neural network (Char-CNN) with character-aware recurrent neural network (Char-RNN) to form a convolutional recurrent neural network (CRNN). Our model benefits from Char-CNN in that only salient features are selected and fed into the integrated Char-RNN. Char-RNN effectively learns long sequence semantics via sophisticated update mechanism. We compare our framework against the state-of-the-art text classification algorithms on four popular benchmarking corpus. For instance, our model achieves competing precision rate, recall ratio, and F1 score on the Google-news data-set. For twenty-news-groups data stream, our algorithm obtains the optimum on precision rate, recall ratio, and F1 score. For Brown Corpus, our framework obtains the best F1 score and almost equivalent precision rate and recall ratio over the top competitor. For the question classification collection, CRNN produces the optimal recall rate and F1 score and comparable precision rate. We also analyse three different RNN hidden recurrent cells' impact on performance and their runtime efficiency. We observe that MGU achieves the optimal runtime and comparable performance against GRU and LSTM. For TFIDF based algorithms, we experiment with word2vec, GloVe, and sent2vec embeddings and report their performance differences. version:1
arxiv-1706-01061 | Face R-CNN | http://arxiv.org/abs/1706.01061 | id:1706.01061 author:Hao Wang, Zhifeng Li, Xing Ji, Yitong Wang category:cs.CV  published:2017-06-04 summary:Faster R-CNN is one of the most representative and successful methods for object detection, and has been becoming increasingly popular in various objection detection applications. In this report, we propose a robust deep face detection approach based on Faster R-CNN. In our approach, we exploit several new techniques including new multi-task loss function design, online hard example mining, and multi-scale training strategy to improve Faster R-CNN in multiple aspects. The proposed approach is well suited for face detection, so we call it Face R-CNN. Extensive experiments are conducted on two most popular and challenging face detection benchmarks, FDDB and WIDER FACE, to demonstrate the superiority of the proposed approach over state-of-the-arts. version:1
arxiv-1706-01040 | Brain Intelligence: Go Beyond Artificial Intelligence | http://arxiv.org/abs/1706.01040 | id:1706.01040 author:Huimin Lu, Yujie Li, Min Chen, Hyoungseop Kim, Seiichi Serikawa category:cs.CV  published:2017-06-04 summary:Artificial intelligence (AI) is an important technology that supports daily social life and economic activities. It contributes greatly to the sustainable growth of Japan's economy and solves various social problems. In recent years, AI has attracted attention as a key for growth in developed countries such as Europe and the United States and developing countries such as China and India. The attention has been focused mainly on developing new artificial intelligence information communication technology (ICT) and robot technology (RT). Although recently developed AI technology certainly excels in extracting certain patterns, there are many limitations. Most ICT models are overly dependent on big data, lack a self-idea function, and are complicated. In this paper, rather than merely developing next-generation artificial intelligence technology, we aim to develop a new concept of general-purpose intelligence cognition technology called Beyond AI. Specifically, we plan to develop an intelligent learning model called Brain Intelligence (BI) that generates new ideas about events without having experienced them by using artificial life with an imagine function. We will also conduct demonstrations of the developed BI intelligence learning model on automatic driving, precision medical care, and industrial robots. version:1
arxiv-1706-01039 | Personalized Age Progression with Bi-level Aging Dictionary Learning | http://arxiv.org/abs/1706.01039 | id:1706.01039 author:Xiangbo Shu, Jinhui Tang, Zechao Li, Hanjiang Lai, Liyan Zhang, Shuicheng Yan category:cs.CV  published:2017-06-04 summary:Age progression is defined as aesthetically re-rendering the aging face at any future age for an individual face. In this work, we aim to automatically render aging faces in a personalized way. Basically, for each age group, we learn an aging dictionary to reveal its aging characteristics (e.g., wrinkles), where the dictionary bases corresponding to the same index yet from two neighboring aging dictionaries form a particular aging pattern cross these two age groups, and a linear combination of all these patterns expresses a particular personalized aging process. Moreover, two factors are taken into consideration in the dictionary learning process. First, beyond the aging dictionaries, each person may have extra personalized facial characteristics, e.g. mole, which are invariant in the aging process. Second, it is challenging or even impossible to collect faces of all age groups for a particular person, yet much easier and more practical to get face pairs from neighboring age groups. To this end, we propose a novel Bi-level Dictionary Learning based Personalized Age Progression (BDL-PAP) method. Here, bi-level dictionary learning is formulated to learn the aging dictionaries based on face pairs from neighboring age groups. Extensive experiments well demonstrate the advantages of the proposed BDL-PAP over other state-of-the-arts in term of personalized age progression, as well as the performance gain for cross-age face verification by synthesizing aging faces. version:1
arxiv-1706-01038 | Improving Legal Information Retrieval by Distributional Composition with Term Order Probabilities | http://arxiv.org/abs/1706.01038 | id:1706.01038 author:Danilo S. Carvalho, Duc-Vu Tran, Van-Khanh Tran, Le-Nguyen Minh category:cs.IR cs.CL  published:2017-06-04 summary:Legal professionals worldwide are currently trying to get up-to-pace with the explosive growth in legal document availability through digital means. This drives a need for high efficiency Legal Information Retrieval (IR) and Question Answering (QA) methods. The IR task in particular has a set of unique challenges that invite the use of semantic motivated NLP techniques. In this work, a two-stage method for Legal Information Retrieval is proposed, combining lexical statistics and distributional sentence representations in the context of Competition on Legal Information Extraction/Entailment (COLIEE). The combination is done with the use of disambiguation rules, applied over the rankings obtained through n-gram statistics. After the ranking is done, its results are evaluated for ambiguity, and disambiguation is done if a result is decided to be unreliable for a given query. Competition and experimental results indicate small gains in overall retrieval performance using the proposed approach. Additionally, an analysis of error and improvement cases is presented for a better understanding of the contributions. version:1
arxiv-1706-01026 | Adaptive Multiple-Arm Identification | http://arxiv.org/abs/1706.01026 | id:1706.01026 author:Jiecao Chen, Xi Chen, Qin Zhang, Yuan Zhou category:cs.LG  published:2017-06-04 summary:We study the problem of selecting $K$ arms with the highest expected rewards in a stochastic $n$-armed bandit game. This problem has a wide range of applications, e.g., A/B testing, crowdsourcing, simulation optimization. Our goal is to develop a PAC algorithm, which, with probability at least $1-\delta$, identifies a set of $K$ arms with the aggregate regret at most $\epsilon$. The notion of aggregate regret for multiple-arm identification was first introduced in \cite{Zhou:14} , which is defined as the difference of the averaged expected rewards between the selected set of arms and the best $K$ arms. In contrast to \cite{Zhou:14} that only provides instance-independent sample complexity, we introduce a new hardness parameter for characterizing the difficulty of any given instance. We further develop two algorithms and establish the corresponding sample complexity in terms of this hardness parameter. The derived sample complexity can be significantly smaller than state-of-the-art results for a large class of instances and matches the instance-independent lower bound upto a $\log(\epsilon^{-1})$ factor in the worst case. We also prove a lower bound result showing that the extra $\log(\epsilon^{-1})$ is necessary for instance-dependent algorithms using the introduced hardness parameter. version:1
arxiv-1706-01021 | Where and Who? Automatic Semantic-Aware Person Composition | http://arxiv.org/abs/1706.01021 | id:1706.01021 author:Fuwen Tan, Crispin Bernier, Benjamin Cohen, Vicente Ordonez, Connelly Barnes category:cs.GR cs.CV  published:2017-06-04 summary:Image compositing is a popular and successful method used to generate realistic yet fake imagery. Much previous work in compositing has focused on improving the appearance compatibility between a given object segment and a background image. However, most previous work does not investigate the topic of automatically selecting semantically compatible segments and predicting their locations and sizes given a background image. In this work, we attempt to fill this gap by developing a fully automatic compositing system that learns this information. To simplify the task, we restrict our problem by focusing on human instance composition, because human segments exhibit strong correlations with the background scene and are easy to collect. The first problem we investigate is determining where should a person segment be placed given a background image, and what should be its size in the background image. We tackle this by developing a novel Convolutional Neural Network (CNN) model that jointly predicts the potential location and size of the person segment. The second problem we investigate is, given the background image, which person segments (who) can be composited with the previously predicted locations and sizes, while retaining compatibility with both the local context and the global scene semantics? To achieve this, we propose an efficient context-based segment retrieval method that incorporates pre-trained deep feature representations. To demonstrate the effectiveness of the proposed compositing system, we conduct quantitative and qualitative experiments including a user study. Experimental results show our system can generate composite images that look semantically and visually convincing. We also develop a proof-of-concept user interface to demonstrate the potential application of our method. version:1
arxiv-1706-01014 | Non-convex Penalties with Analytical Solutions for One-bit Compressive Sensing | http://arxiv.org/abs/1706.01014 | id:1706.01014 author:Xiaolin Huang, Ming Yan category:cs.LG stat.ML  published:2017-06-04 summary:One-bit measurements widely exist in the real world, and they can be used to recover sparse signals. This task is known as the problem of learning halfspaces in learning theory and one-bit compressive sensing (1bit-CS) in signal processing. In this paper, we propose novel algorithms based on both convex and nonconvex sparsity-inducing penalties for robust 1bit-CS. We provide a sufficient condition to verify whether a solution is globally optimal or not. Then we show that the globally optimal solution for positive homogeneous penalties can be obtained in two steps: a proximal operator and a normalization step. For several nonconvex penalties, including minimax concave penalty (MCP), $\ell_0$ norm, and sorted $\ell_1$ penalty, we provide fast algorithms for finding the analytical solutions by solving the dual problem. Specifically, our algorithm is more than $200$ times faster than the existing algorithm for MCP. Its efficiency is comparable to the algorithm for the $\ell_1$ penalty in time, while its performance is much better. Among these penalties, the sorted $\ell_1$ penalty is most robust to noise in different settings. version:1
arxiv-1706-01010 | DeepSF: deep convolutional neural network for mapping protein sequences to folds | http://arxiv.org/abs/1706.01010 | id:1706.01010 author:Jie Hou, Badri Adhikari, Jianlin Cheng category:cs.LG q-bio.BM  published:2017-06-04 summary:Motivation Protein fold recognition is an important problem in structural bioinformatics. Almost all traditional fold recognition methods use sequence (homology) comparison to indirectly predict the fold of a tar get protein based on the fold of a template protein with known structure, which cannot explain the relationship between sequence and fold. Only a few methods had been developed to classify protein sequences into a small number of folds due to methodological limitations, which are not generally useful in practice. Results We develop a deep 1D-convolution neural network (DeepSF) to directly classify any protein se quence into one of 1195 known folds, which is useful for both fold recognition and the study of se quence-structure relationship. Different from traditional sequence alignment (comparison) based methods, our method automatically extracts fold-related features from a protein sequence of any length and map it to the fold space. We train and test our method on the datasets curated from SCOP1.75, yielding a classification accuracy of 80.4%. On the independent testing dataset curated from SCOP2.06, the classification accuracy is 77.0%. We compare our method with a top profile profile alignment method - HHSearch on hard template-based and template-free modeling targets of CASP9-12 in terms of fold recognition accuracy. The accuracy of our method is 14.5%-29.1% higher than HHSearch on template-free modeling targets and 4.5%-16.7% higher on hard template-based modeling targets for top 1, 5, and 10 predicted folds. The hidden features extracted from sequence by our method is robust against sequence mutation, insertion, deletion and truncation, and can be used for other protein pattern recognition problems such as protein clustering, comparison and ranking. version:1
arxiv-1705-03820 | Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks | http://arxiv.org/abs/1705.03820 | id:1705.03820 author:Hao Dong, Guang Yang, Fangde Liu, Yuanhan Mo, Yike Guo category:cs.CV  published:2017-05-10 summary:A major challenge in brain tumor treatment planning and quantitative evaluation is determination of the tumor extent. The noninvasive magnetic resonance imaging (MRI) technique has emerged as a front-line diagnostic tool for brain tumors without ionizing radiation. Manual segmentation of brain tumor extent from 3D MRI volumes is a very time-consuming task and the performance is highly relied on operator's experience. In this context, a reliable fully automatic segmentation method for the brain tumor segmentation is necessary for an efficient measurement of the tumor extent. In this study, we propose a fully automatic method for brain tumor segmentation, which is developed using U-Net based deep convolutional networks. Our method was evaluated on Multimodal Brain Tumor Image Segmentation (BRATS 2015) datasets, which contain 220 high-grade brain tumor and 54 low-grade tumor cases. Cross-validation has shown that our method can obtain promising segmentation efficiently. version:3
arxiv-1706-01000 | Image Compression Based on Compressive Sensing: End-to-End Comparison with JPEG | http://arxiv.org/abs/1706.01000 | id:1706.01000 author:Xin Yuan, Raziel Haimi-Cohen category:cs.CV  published:2017-06-03 summary:We present an end-to-end image compression system based on compressive sensing. The presented system integrates the conventional scheme of compressive sampling and reconstruction with quantization and entropy coding. The compression performance, in terms of decoded image quality versus data rate, is shown to be comparable with JPEG and significantly better at the low rate range. We study the parameters that influence the system performance, including (i) the choice of sensing matrix, (ii) the trade-off between quantization and compression ratio, and (iii) the reconstruction algorithms. We propose an effective method to jointly control the quantization step and compression ratio in order to achieve near optimal quality at any given bit rate. Furthermore, our proposed image compression system can be directly used in the compressive sensing camera, e.g. the single pixel camera, to construct a hardware compressive sampling system. version:1
arxiv-1706-00999 | Order embeddings and character-level convolutions for multimodal alignment | http://arxiv.org/abs/1706.00999 | id:1706.00999 author:Jônatas Wehrmann, Anderson Mattjie, Rodrigo C. Barros category:cs.CV  published:2017-06-03 summary:With the novel and fast advances in the area of deep neural networks, several challenging image-based tasks have been recently approached by researchers in pattern recognition and computer vision. In this paper, we address one of these tasks, which is to match image content with natural language descriptions, sometimes referred as multimodal content retrieval. Such a task is particularly challenging considering that we must find a semantic correspondence between captions and the respective image, a challenge for both computer vision and natural language processing areas. For such, we propose a novel multimodal approach based solely on convolutional neural networks for aligning images with their captions by directly convolving raw characters. Our proposed character-based textual embeddings allow the replacement of both word-embeddings and recurrent neural networks for text understanding, saving processing time and requiring fewer learnable parameters. Our method is based on the idea of projecting both visual and textual information into a common embedding space. For training such embeddings we optimize a contrastive loss function that is computed to minimize order-violations between images and their respective descriptions. We achieve state-of-the-art performance in the largest and most well-known image-text alignment dataset, namely Microsoft COCO, with a method that is conceptually much simpler and that possesses considerably fewer parameters than current approaches. version:1
arxiv-1706-00998 | Swarm Intelligence in Semi-supervised Classification | http://arxiv.org/abs/1706.00998 | id:1706.00998 author:Shahira Shaaban Azab, Hesham Ahmed Hefny category:cs.LG  published:2017-06-03 summary:This Paper represents a literature review of Swarm intelligence algorithm in the area of semi-supervised classification. There are many research papers for applying swarm intelligence algorithms in the area of machine learning. Some algorithms of SI are applied in the area of ML either solely or hybrid with other ML algorithms. SI algorithms are also used for tuning parameters of ML algorithm, or as a backbone for ML algorithms. This paper introduces a brief literature review for applying swarm intelligence algorithms in the field of semi-supervised learning version:1
arxiv-1706-00997 | Center of Gravity PSO for Partitioning Clustering | http://arxiv.org/abs/1706.00997 | id:1706.00997 author:Shahira Shaaban Azab, Hesham Ahmed Hefny category:cs.LG  published:2017-06-03 summary:This paper presents the local best model of PSO for partition-based clustering. The proposed model gets rid off the drawbacks of gbest PSO for clustering. The model uses a pre-specified number of clusters K. The LPOSC has K neighborhoods. Each neighborhood represents one of the clusters. The goal of the particles in each neighborhood is optimizing the position of the centroid of the cluster. The performance of the proposed algorithms is measured using adjusted rand index. The results is compared with k-means and global best model of PSO. version:1
arxiv-1706-00996 | Semi-supervised Classification: Cluster and Label Approach using Particle Swarm Optimization | http://arxiv.org/abs/1706.00996 | id:1706.00996 author:Shahira Shaaban Azab, Mohamed Farouk Abdel Hady, Hesham Ahmed Hefny category:cs.LG  published:2017-06-03 summary:Classification predicts classes of objects using the knowledge learned during the training phase. This process requires learning from labeled samples. However, the labeled samples usually limited. Annotation process is annoying, tedious, expensive, and requires human experts. Meanwhile, unlabeled data is available and almost free. Semi-supervised learning approaches make use of both labeled and unlabeled data. This paper introduces cluster and label approach using PSO for semi-supervised classification. PSO is competitive to traditional clustering algorithms. A new local best PSO is presented to cluster the unlabeled data. The available labeled data guides the learning process. The experiments are conducted using four state-of-the-art datasets from different domains. The results compared with Label Propagation a popular semi-supervised classifier and two state-of-the-art supervised classification models, namely k-nearest neighbors and decision trees. The experiments show the efficiency of the proposed model. version:1
arxiv-1705-09275 | Who Will Share My Image? Predicting the Content Diffusion Path in Online Social Networks | http://arxiv.org/abs/1705.09275 | id:1705.09275 author:Wenjian Hu, Krishna Kumar Singh, Fanyi Xiao, Jinyoung Han, Chen-Nee Chuah, Yong Jae Lee category:cs.CV cs.SI  published:2017-05-25 summary:Content popularity prediction has been extensively studied due to its importance and interest for both users and hosts of social media sites like Facebook, Instagram, Twitter, and Pinterest. However, existing work mainly focuses on modeling popularity using a single metric such as the total number of likes or shares. In this work, we propose Diffusion-LSTM, a memory-based deep recurrent network that learns to recursively predict the entire diffusion path of an image through a social network. By combining user social features and image features, and encoding the diffusion path taken thus far with an explicit memory cell, our model predicts the diffusion path of an image more accurately compared to alternate baselines that either encode only image or social features, or lack memory. By mapping individual users to user prototypes, our model can generalize to new users not seen during training. Finally, we demonstrate our model's capability of generating diffusion trees, and show that the generated trees closely resemble ground-truth trees. version:2
arxiv-1705-10312 | Classification of Major Depressive Disorder via Multi-Site Weighted LASSO Model | http://arxiv.org/abs/1705.10312 | id:1705.10312 author:Dajiang Zhu, Brandalyn C. Riedel, Neda Jahanshad, Nynke A. Groenewold, Dan J. Stein, Ian H. Gotlib, Matthew D. Sacchet, Danai Dima, James H. Cole, Cynthia H. Y. Fu, Henrik Walter, Ilya M. Veer, Thomas Frodl, Lianne Schmaal, Dick J. Veltman, Paul M. Thompson category:cs.LG cs.CE stat.AP  published:2017-05-26 summary:Large-scale collaborative analysis of brain imaging data, in psychiatry and neu-rology, offers a new source of statistical power to discover features that boost ac-curacy in disease classification, differential diagnosis, and outcome prediction. However, due to data privacy regulations or limited accessibility to large datasets across the world, it is challenging to efficiently integrate distributed information. Here we propose a novel classification framework through multi-site weighted LASSO: each site performs an iterative weighted LASSO for feature selection separately. Within each iteration, the classification result and the selected features are collected to update the weighting parameters for each feature. This new weight is used to guide the LASSO process at the next iteration. Only the fea-tures that help to improve the classification accuracy are preserved. In tests on da-ta from five sites (299 patients with major depressive disorder (MDD) and 258 normal controls), our method boosted classification accuracy for MDD by 4.9% on average. This result shows the potential of the proposed new strategy as an ef-fective and practical collaborative platform for machine learning on large scale distributed imaging and biobank data. version:2
arxiv-1706-00984 | Graph-Cut RANSAC | http://arxiv.org/abs/1706.00984 | id:1706.00984 author:Daniel Barath, Jiri Matas category:cs.CV  published:2017-06-03 summary:A novel method, called Graph Cut RANSAC, GC-RANSAC in short, is presented. To separate inliers and outliers, it runs the graph cut algorithm in the local optimization (LO) step which is applied after a \textit{so-far-the-best} model is found. The proposed LO step is conceptually simple, easy to implement, globally optimal and efficient. Experiments show that GC-RANSAC outperforms LO-RANSAC and its state-of-the-art variants in terms of both accuracy and the required number of iterations for line, homography and fundamental matrix estimation on standard public datasets. GC-RANSAC is very efficient, its processing time for hundreds of input points is approximately \ $1$ -- $10$ milliseconds, depending on the inlier-outlier ratio. version:1
arxiv-1706-00977 | Thompson Sampling for the MNL-Bandit | http://arxiv.org/abs/1706.00977 | id:1706.00977 author:Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, Assaf Zeevi category:cs.LG  published:2017-06-03 summary:We consider a sequential subset selection problem under parameter uncertainty, where at each time step, the decision maker selects a subset of cardinality $K$ from $N$ possible items (arms), and observes a (bandit) feedback in the form of the index of one of the items in said subset, or none. Each item in the index set is ascribed a certain value (reward), and the feedback is governed by a Multinomial Logit (MNL) choice model whose parameters are a priori unknown. The objective of the decision maker is to maximize the expected cumulative rewards over a finite horizon $T$, or alternatively, minimize the regret relative to an oracle that knows the MNL parameters. We refer to this as the MNL-Bandit problem. This problem is representative of a larger family of exploration-exploitation problems that involve a combinatorial objective, and arise in several important application domains. We present an approach to adapt Thompson Sampling to this problem and show that it achieves near-optimal regret as well as attractive numerical performance. version:1
arxiv-1706-00948 | Financial Series Prediction: Comparison Between Precision of Time Series Models and Machine Learning Methods | http://arxiv.org/abs/1706.00948 | id:1706.00948 author:Xin-Yao Qian, Shan Gao category:cs.LG q-fin.ST  published:2017-06-03 summary:Precise financial series predicting has long been a difficult problem because of unstableness and many noises within the series. Although Traditional time series models like ARIMA and GARCH have been researched and proved to be effective in predicting, their performances are still far from satisfying. Machine Learning, as an emerging research field in recent years, has brought about many incredible improvements in tasks such as regressing and classifying, and it's also promising to exploit the methodology in financial time series predicting. In this paper, the predicting precision of financial time series between traditional time series models and mainstream machine learning models including some state-of-the-art ones of deep learning are compared through experiment using real stock index data from history. The result shows that machine learning as a modern method far surpasses traditional models in precision. version:1
arxiv-1706-00947 | Context-aware, Adaptive and Scalable Android Malware Detection through Online Learning (extended version) | http://arxiv.org/abs/1706.00947 | id:1706.00947 author:Annamalai Narayanan, Mahinthan Chandramohan, Lihui Chen, Yang Liu category:cs.CR cs.LG cs.SE  published:2017-06-03 summary:It is well-known that Android malware constantly evolves so as to evade detection. This causes the entire malware population to be non-stationary. Contrary to this fact, most of the prior works on Machine Learning based Android malware detection have assumed that the distribution of the observed malware characteristics (i.e., features) does not change over time. In this work, we address the problem of malware population drift and propose a novel online learning based framework to detect malware, named CASANDRA (Contextaware, Adaptive and Scalable ANDRoid mAlware detector). In order to perform accurate detection, a novel graph kernel that facilitates capturing apps' security-sensitive behaviors along with their context information from dependency graphs is proposed. Besides being accurate and scalable, CASANDRA has specific advantages: i) being adaptive to the evolution in malware features over time ii) explaining the significant features that led to an app's classification as being malicious or benign. In a large-scale comparative analysis, CASANDRA outperforms two state-of-the-art techniques on a benchmark dataset achieving 99.23% F-measure. When evaluated with more than 87,000 apps collected in-the-wild, CASANDRA achieves 89.92% accuracy, outperforming existing techniques by more than 25% in their typical batch learning setting and more than 7% when they are continuously retained, while maintaining comparable efficiency. Besides this, CASANDRA demonstrates excellent ability to adapt to the drift in real-world malware over time and great potential as a reliable framework to perform market-scale analysis. version:1
arxiv-1705-09837 | On the relation between dependency distance, crossing dependencies, and parsing. Comment on "Dependency distance: a new perspective on syntactic patterns in natural languages" by Haitao Liu et al | http://arxiv.org/abs/1705.09837 | id:1705.09837 author:Carlos Gómez-Rodríguez category:cs.CL 68T50  91F20  97C30 I.2.7; J.5; F.4.2  published:2017-05-27 summary:Liu et al. (2017) provide a comprehensive account of research on dependency distance in human languages. While the article is a very rich and useful report on this complex subject, here I will expand on a few specific issues where research in computational linguistics (specifically natural language processing) can inform DDM research, and vice versa. These aspects have not been explored much in the article by Liu et al. or elsewhere, probably due to the little overlap between both research communities, but they may provide interesting insights for improving our understanding of the evolution of human languages, the mechanisms by which the brain processes and understands language, and the construction of effective computer systems to achieve this goal. version:2
arxiv-1706-00932 | See, Hear, and Read: Deep Aligned Representations | http://arxiv.org/abs/1706.00932 | id:1706.00932 author:Yusuf Aytar, Carl Vondrick, Antonio Torralba category:cs.CV  published:2017-06-03 summary:We capitalize on large amounts of readily-available, synchronous data to learn a deep discriminative representations shared across three major natural modalities: vision, sound and language. By leveraging over a year of sound from video and millions of sentences paired with images, we jointly train a deep convolutional network for aligned representation learning. Our experiments suggest that this representation is useful for several tasks, such as cross-modal retrieval or transferring classifiers between modalities. Moreover, although our network is only trained with image+text and image+sound pairs, it can transfer between text and sound as well, a transfer the network never observed during training. Visualizations of our representation reveal many hidden units which automatically emerge to detect concepts, independent of the modality. version:1
arxiv-1706-00931 | Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition | http://arxiv.org/abs/1706.00931 | id:1706.00931 author:Xiangbo Shu, Jinhui Tang, Guo-Jun Qi, Yan Song, Zechao Li, Liyan Zhang category:cs.CV  published:2017-06-03 summary:Recently, Long Short-Term Memory (LSTM) has become a popular choice to model individual dynamics for single-person action recognition due to its ability of modeling the temporal information in various ranges of dynamic contexts. However, existing RNN models only focus on capturing the temporal dynamics of the person-person interactions by naively combining the activity dynamics of individuals or modeling them as a whole. This neglects the inter-related dynamics of how person-person interactions change over time. To this end, we propose a novel Concurrence-Aware Long Short-Term Sub-Memories (Co-LSTSM) to model the long-term inter-related dynamics between two interacting people on the bounding boxes covering people. Specifically, for each frame, two sub-memory units store individual motion information, while a concurrent LSTM unit selectively integrates and stores inter-related motion information between interacting people from these two sub-memory units via a new co-memory cell. Experimental results on the BIT and UT datasets show the superiority of Co-LSTSM compared with the state-of-the-art methods. version:1
arxiv-1706-00927 | Concept Transfer Learning for Adaptive Language Understanding | http://arxiv.org/abs/1706.00927 | id:1706.00927 author:Su Zhu, Kai Yu category:cs.CL  published:2017-06-03 summary:Semantic transfer is an important problem of the language understanding (LU), which is about how the recognition pattern of a semantic concept benefits other associated concepts. In this paper, we propose a new semantic representation based on combinatory concepts. Semantic slot is represented as a composition of different atomic concepts in different semantic dimensions. Specifically, we propose the concept transfer learning methods for extending combinatory concepts in LU. The concept transfer learning makes use of the common ground of combinatory concepts shown in the literal description. Our methods are applied to two adaptive LU problems: semantic slot refinement and domain adaptation, and respectively evaluated on two benchmark LU datasets: ATIS and DSTC 2&3. The experiment results show that the concept transfer learning is very efficient for semantic slot refinement and domain adaptation in the LU. version:1
arxiv-1706-00917 | Deep-Learning Convolutional Neural Networks for scattered shrub detection with Google Earth Imagery | http://arxiv.org/abs/1706.00917 | id:1706.00917 author:Emilio Guirado, Siham Tabik, Domingo Alcaraz-Segura, Javier Cabello, Francisco Herrera category:cs.CV  published:2017-06-03 summary:There is a growing demand for accurate high-resolution land cover maps in many fields, e.g., in land-use planning and biodiversity conservation. Developing such maps has been performed using Object-Based Image Analysis (OBIA) methods, which usually reach good accuracies, but require a high human supervision and the best configuration for one image can hardly be extrapolated to a different image. Recently, the deep learning Convolutional Neural Networks (CNNs) have shown outstanding results in object recognition in the field of computer vision. However, they have not been fully explored yet in land cover mapping for detecting species of high biodiversity conservation interest. This paper analyzes the potential of CNNs-based methods for plant species detection using free high-resolution Google Earth T M images and provides an objective comparison with the state-of-the-art OBIA-methods. We consider as case study the detection of Ziziphus lotus shrubs, which are protected as a priority habitat under the European Union Habitats Directive. According to our results, compared to OBIA-based methods, the proposed CNN-based detection model, in combination with data-augmentation, transfer learning and pre-processing, achieves higher performance with less human intervention and the knowledge it acquires in the first image can be transferred to other images, which makes the detection process very fast. The provided methodology can be systematically reproduced for other species detection. version:1
arxiv-1706-00909 | Learning by Association - A versatile semi-supervised training method for neural networks | http://arxiv.org/abs/1706.00909 | id:1706.00909 author:Philip Häusser, Alexander Mordvintsev, Daniel Cremers category:cs.CV cs.LG  published:2017-06-03 summary:In many real-world scenarios, labeled data for a specific machine learning task is costly to obtain. Semi-supervised training methods make use of abundantly available unlabeled data and a smaller number of labeled examples. We propose a new framework for semi-supervised training of deep neural networks inspired by learning in humans. "Associations" are made from embeddings of labeled samples to those of unlabeled ones and back. The optimization schedule encourages correct association cycles that end up at the same class from which the association was started and penalizes wrong associations ending at a different class. The implementation is easy to use and can be added to any existing end-to-end training setup. We demonstrate the capabilities of learning by association on several data sets and show that it can improve performance on classification tasks tremendously by making use of additionally available unlabeled data. In particular, for cases with few labeled data, our training scheme outperforms the current state of the art on SVHN. version:1
arxiv-1706-00906 | Heterogeneous Face Attribute Estimation: A Deep Multi-Task Learning Approach | http://arxiv.org/abs/1706.00906 | id:1706.00906 author:Hu Han, Anil K. Jain, Shiguang Shan, Xilin Chen category:cs.CV  published:2017-06-03 summary:Face attribute estimation has many potential applications in video surveillance, face retrieval, and social media. While a number of methods have been proposed for face attribute estimation, most of them did not explicitly consider the attribute correlation and heterogeneity (e.g., ordinal vs. nominal attributes) during feature representation learning. In this paper, we present a Deep Multi-Task Learning (DMTL) approach to jointly estimate multiple heterogeneous attributes from a single face image. In DMTL, we tackle attribute correlation and heterogeneity with convolutional neural networks (CNNs) consisting of shared feature learning for all the attributes, and category-specific feature learning for heterogeneous attributes. We also introduce an unconstrained face database (LFW+), an extension of public-domain LFW, with heterogeneous demographic attributes (age, gender, and race) obtained via crowdsourcing. Experimental results on benchmarks with multiple heterogeneous attributes (LFW+ and MORPH II) show that the proposed approach has superior performance compared to state of the art. Finally, evaluations on public-domain face databases with multiple binary attributes (CelebA and LFWA) and a single attribute (LAP) show that the proposed approach has excellent generalization ability. version:1
arxiv-1706-00893 | Learning Person Trajectory Representations for Team Activity Analysis | http://arxiv.org/abs/1706.00893 | id:1706.00893 author:Nazanin Mehrasa, Yatao Zhong, Frederick Tung, Luke Bornn, Greg Mori category:cs.CV  published:2017-06-03 summary:Activity analysis in which multiple people interact across a large space is challenging due to the interplay of individual actions and collective group dynamics. We propose an end-to-end approach for learning person trajectory representations for group activity analysis. The learned representations encode rich spatio-temporal dependencies and capture useful motion patterns for recognizing individual events, as well as characteristic group dynamics that can be used to identify groups from their trajectories alone. We develop our deep learning approach in the context of team sports, which provide well-defined sets of events (e.g. pass, shot) and groups of people (teams). Analysis of events and team formations using NHL hockey and NBA basketball datasets demonstrate the generality of our approach. version:1
arxiv-1706-00891 | Spectrum-based deep neural networks for fraud detection | http://arxiv.org/abs/1706.00891 | id:1706.00891 author:Shuhan Yuan, Xintao Wu, Jun Li, Aidong Lu category:cs.CR cs.LG cs.SI  published:2017-06-03 summary:In this paper, we focus on fraud detection on a signed graph with only a small set of labeled training data. We propose a novel framework that combines deep neural networks and spectral graph analysis. In particular, we use the node projection (called as spectral coordinate) in the low dimensional spectral space of the graph's adjacency matrix as input of deep neural networks. Spectral coordinates in the spectral space capture the most useful topology information of the network. Due to the small dimension of spectral coordinates (compared with the dimension of the adjacency matrix derived from a graph), training deep neural networks becomes feasible. We develop and evaluate two neural networks, deep autoencoder and convolutional neural network, in our fraud detection framework. Experimental results on a real signed graph show that our spectrum based deep neural networks are effective in fraud detection. version:1
arxiv-1706-00887 | Wikipedia Vandal Early Detection: from User Behavior to User Embedding | http://arxiv.org/abs/1706.00887 | id:1706.00887 author:Shuhan Yuan, Panpan Zheng, Xintao Wu, Yang Xiang category:cs.CR cs.CL cs.CY  published:2017-06-03 summary:Wikipedia is the largest online encyclopedia that allows anyone to edit articles. In this paper, we propose the use of deep learning to detect vandals based on their edit history. In particular, we develop a multi-source long-short term memory network (M-LSTM) to model user behaviors by using a variety of user edit aspects as inputs, including the history of edit reversion information, edit page titles and categories. With M-LSTM, we can encode each user into a low dimensional real vector, called user embedding. Meanwhile, as a sequential model, M-LSTM updates the user embedding each time after the user commits a new edit. Thus, we can predict whether a user is benign or vandal dynamically based on the up-to-date user embedding. Furthermore, those user embeddings are crucial to discover collaborative vandals. version:1
arxiv-1706-00885 | IDK Cascades: Fast Deep Learning by Learning not to Overthink | http://arxiv.org/abs/1706.00885 | id:1706.00885 author:Xin Wang, Yujia Luo, Daniel Crankshaw, Alexey Tumanov, Joseph E. Gonzalez category:cs.CV cs.LG  published:2017-06-03 summary:Advances in deep learning have led to substantial increases in prediction accuracy as well as the cost of rendering predictions. We conjecture that for a majority of real-world inputs, the recent advances in deep learning have created models that effectively "over-think" on simple inputs. In this paper we revisit the classic idea of prediction cascades to reduce prediction costs. We introduce the "I Don't Know" (IDK) prediction cascades framework, a general framework for constructing prediction cascades for arbitrary multi-class prediction tasks. We propose two baseline methods for constructing cascades as well as a new objective within this framework and evaluate these techniques on a range of benchmark and real-world datasets to demonstrate the prediction cascades can achieve 1.7-10.5x speedups in image classification tasks while maintaining comparable accuracy to state-of-the-art models. When combined with human experts, prediction cascades can achieve nearly perfect accuracy(within 5%) while requiring human intervention on less than 30% of the queries. version:1
arxiv-1706-00884 | Task-specific Word Identification from Short Texts Using a Convolutional Neural Network | http://arxiv.org/abs/1706.00884 | id:1706.00884 author:Shuhan Yuan, Xintao Wu, Yang Xiang category:cs.CL cs.IR cs.LG  published:2017-06-03 summary:Task-specific word identification aims to choose the task-related words that best describe a short text. Existing approaches require well-defined seed words or lexical dictionaries (e.g., WordNet), which are often unavailable for many applications such as social discrimination detection and fake review detection. However, we often have a set of labeled short texts where each short text has a task-related class label, e.g., discriminatory or non-discriminatory, specified by users or learned by classification algorithms. In this paper, we focus on identifying task-specific words and phrases from short texts by exploiting their class labels rather than using seed words or lexical dictionaries. We consider the task-specific word and phrase identification as feature learning. We train a convolutional neural network over a set of labeled texts and use score vectors to localize the task-specific words and phrases. Experimental results on sentiment word identification show that our approach significantly outperforms existing methods. We further conduct two case studies to show the effectiveness of our approach. One case study on a crawled tweets dataset demonstrates that our approach can successfully capture the discrimination-related words/phrases. The other case study on fake review detection shows that our approach can identify the fake-review words/phrases. version:1
arxiv-1706-00878 | MobiRNN: Efficient Recurrent Neural Network Execution on Mobile GPU | http://arxiv.org/abs/1706.00878 | id:1706.00878 author:Qingqing Cao, Niranjan Balasubramanian, Aruna Balasubramanian category:cs.DC cs.LG cs.NE  published:2017-06-03 summary:In this paper, we explore optimizations to run Recurrent Neural Network (RNN) models locally on mobile devices. RNN models are widely used for Natural Language Processing, Machine Translation, and other tasks. However, existing mobile applications that use RNN models do so on the cloud. To address privacy and efficiency concerns, we show how RNN models can be run locally on mobile devices. Existing work on porting deep learning models to mobile devices focus on Convolution Neural Networks (CNNs) and cannot be applied directly to RNN models. In response, we present MobiRNN, a mobile-specific optimization framework that implements GPU offloading specifically for mobile GPUs. Evaluations using an RNN model for activity recognition shows that MobiRNN does significantly decrease the latency of running RNN models on phones. version:1
arxiv-1706-00134 | Semantic Refinement GRU-based Neural Language Generation for Spoken Dialogue Systems | http://arxiv.org/abs/1706.00134 | id:1706.00134 author:Van-Khanh Tran, Le-Minh Nguyen category:cs.CL  published:2017-06-01 summary:Natural language generation (NLG) plays a critical role in spoken dialogue systems. This paper presents a new approach to NLG by using recurrent neural networks (RNN), in which a gating mechanism is applied before RNN computation. This allows the proposed model to generate appropriate sentences. The RNN-based generator can be learned from unaligned data by jointly training sentence planning and surface realization to produce natural language responses. The model was extensively evaluated on four different NLG domains. The results show that the proposed generator achieved better performance on all the NLG domains compared to previous generators. version:2
arxiv-1706-00868 | Active learning machine learns to create new quantum experiments | http://arxiv.org/abs/1706.00868 | id:1706.00868 author:Alexey A. Melnikov, Hendrik Poulsen Nautrup, Mario Krenn, Vedran Dunjko, Markus Tiersch, Anton Zeilinger, Hans J. Briegel category:quant-ph cs.AI stat.ML  published:2017-06-02 summary:How useful can machine learning be in a quantum laboratory? Here we raise the question of the potential of intelligent machines in the context of scientific research. We investigate this question by using the projective simulation model, a physics-oriented approach to artificial intelligence. In our approach, the projective simulation system is challenged to design complex photonic quantum experiments that produce high-dimensional entangled multiphoton states, which are of high interest in modern quantum experiments. The artificial intelligence system learns to create a variety of entangled states, in number surpassing the best previously studied automated approaches, and improves the efficiency of their realization. In the process, the system autonomously (re)discovers experimental techniques which are only becoming standard in modern quantum optical experiments - a trait which was not explicitly demanded from the system but emerged through the process of learning. Such features highlight the possibility that machines could have a significantly more creative role in future research. version:1
arxiv-1704-06735 | Asynchronous Distributed Variational Gaussian Processes for Regression | http://arxiv.org/abs/1704.06735 | id:1704.06735 author:Hao Peng, Shandian Zhe, Xiao Zhang, Yuan Qi category:stat.ML  published:2017-04-22 summary:Gaussian processes (GPs) are powerful non-parametric function estimators. However, their applications are largely limited by the expensive computational cost of the inference procedures. Existing stochastic or distributed synchronous variational inferences, although have alleviated this issue by scaling up GPs to millions of samples, are still far from satisfactory for real-world large applications, where the data sizes are often orders of magnitudes larger, say, billions. To solve this problem, we propose ADVGP, the first Asynchronous Distributed Variational Gaussian Process inference for regression, on the recent large-scale machine learning platform, PARAMETERSERVER. ADVGP uses a novel, flexible variational framework based on a weight space augmentation, and implements the highly efficient, asynchronous proximal gradient optimization. While maintaining comparable or better predictive performance, ADVGP greatly improves upon the efficiency of the existing variational methods. With ADVGP, we effortlessly scale up GP regression to a real-world application with billions of samples and demonstrate an excellent, superior prediction accuracy to the popular linear models. version:2
arxiv-1706-00856 | Multiple Kernel Learning and Automatic Subspace Relevance Determination for High-dimensional Neuroimaging Data | http://arxiv.org/abs/1706.00856 | id:1706.00856 author:Murat Seckin Ayhan, Vijay Raghavan, Alzheimer's disease  category:cs.LG q-bio.NC stat.ML  published:2017-06-02 summary:Alzheimer's disease is a major cause of dementia. Its diagnosis requires accurate biomarkers that are sensitive to disease stages. In this respect, we regard probabilistic classification as a method of designing a probabilistic biomarker for disease staging. Probabilistic biomarkers naturally support the interpretation of decisions and evaluation of uncertainty associated with them. In this paper, we obtain probabilistic biomarkers via Gaussian Processes. Gaussian Processes enable probabilistic kernel machines that offer flexible means to accomplish Multiple Kernel Learning. Exploiting this flexibility, we propose a new variation of Automatic Relevance Determination and tackle the challenges of high dimensionality through multiple kernels. Our research results demonstrate that the Gaussian Process models are competitive with or better than the well-known Support Vector Machine in terms of classification performance even in the cases of single kernel learning. Extending the basic scheme towards the Multiple Kernel Learning, we improve the efficacy of the Gaussian Process models and their interpretability in terms of the known anatomical correlates of the disease. For instance, the disease pathology starts in and around the hippocampus and entorhinal cortex. Through the use of Gaussian Processes and Multiple Kernel Learning, we have automatically and efficiently determined those portions of neuroimaging data. In addition to their interpretability, our Gaussian Process models are competitive with recent deep learning solutions under similar settings. version:1
arxiv-1705-10865 | Sparse canonical correlation analysis | http://arxiv.org/abs/1705.10865 | id:1705.10865 author:Xiaotong Suo, Victor Minden, Bradley Nelson, Robert Tibshirani, Michael Saunders category:stat.ML stat.AP  published:2017-05-30 summary:Canonical correlation analysis was proposed by Hotelling [6] and it measures linear relationship between two multidimensional variables. In high dimensional setting, the classical canonical correlation analysis breaks down. We propose a sparse canonical correlation analysis by adding l1 constraints on the canonical vectors and show how to solve it efficiently using linearized alternating direction method of multipliers (ADMM) and using TFOCS as a black box. We illustrate this idea on simulated data. version:2
arxiv-1706-00827 | Multi-Class Model Fitting by Energy Minimization and Mode-Seeking | http://arxiv.org/abs/1706.00827 | id:1706.00827 author:Daniel Barath, Jiri Matas category:cs.CV  published:2017-06-02 summary:We propose a novel method, called Multi-X, for general multi-class multi-instance model fitting - the problem of interpreting input data as a mixture of noisy observations originating from multiple instances of multiple types. The proposed approach combines global energy optimization and mode-seeking in the parameter domain. Robustness is achieved by using an outlier class. Key optimization parameters like the outlier threshold are set automatically within the algorithm. Considering that a group of outliers may form spatially coherent structures in the data, we propose a cross-validation-based technique removing statistically insignificant instances. Multi-X outperforms significantly the state-of-the-art on the standard AdelaideRMF (multiple plane segmentation, multiple rigid motion detection) and Hopkins datasets (motion segmentation) and in experiments on 3D LIDAR data (simultaneous plane and cylinder fitting) and on 2D edge interpretation (circle and line fitting). Multi-X runs in time approximately linear in the number of data points at around 0.1 second per 100 points, an order of magnitude faster than available implementations of commonly used methods. version:1
arxiv-1706-00826 | One-Sided Unsupervised Domain Mapping | http://arxiv.org/abs/1706.00826 | id:1706.00826 author:Sagie Benaim, Lior Wolf category:cs.CV  published:2017-06-02 summary:In unsupervised domain mapping, the learner is given two unmatched datasets $A$ and $B$. The goal is to learn a mapping $G_{AB}$ that translates a sample in $A$ to the analog sample in $B$. Recent approaches have shown that when learning simultaneously both $G_{AB}$ and the inverse mapping $G_{BA}$, convincing mappings are obtained. In this work, we present a method of learning $G_{AB}$ without learning $G_{BA}$. This is done by learning a mapping that maintains the distance between a pair of samples. Moreover, good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. We present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint. Our entire code is made publicly available at https://github.com/sagiebenaim/DistanceGAN . version:1
arxiv-1706-00820 | Information, Privacy and Stability in Adaptive Data Analysis | http://arxiv.org/abs/1706.00820 | id:1706.00820 author:Adam Smith category:cs.LG stat.ML  published:2017-06-02 summary:Traditional statistical theory assumes that the analysis to be performed on a given data set is selected independently of the data themselves. This assumption breaks downs when data are re-used across analyses and the analysis to be performed at a given stage depends on the results of earlier stages. Such dependency can arise when the same data are used by several scientific studies, or when a single analysis consists of multiple stages. How can we draw statistically valid conclusions when data are re-used? This is the focus of a recent and active line of work. At a high level, these results show that limiting the information revealed by earlier stages of analysis controls the bias introduced in later stages by adaptivity. Here we review some known results in this area and highlight the role of information-theoretic concepts, notably several one-shot notions of mutual information. version:1
arxiv-1706-00815 | A watershed-based algorithm to segment and classify cells in fluorescence microscopy images | http://arxiv.org/abs/1706.00815 | id:1706.00815 author:Lena R. Bartell, Lawrence J. Bonassar, Itai Cohen category:cs.CV  published:2017-06-02 summary:Imaging assays of cellular function, especially those using fluorescent stains, are ubiquitous in the biological and medical sciences. Despite advances in computer vision, such images are often analyzed using only manual or rudimentary automated processes. Watershed-based segmentation is an effective technique for identifying objects in images; it outperforms commonly used image analysis methods, but requires familiarity with computer-vision techniques to be applied successfully. In this report, we present and implement a watershed-based image analysis and classification algorithm in a GUI, enabling a broad set of users to easily understand the algorithm and adjust the parameters to their specific needs. As an example, we implement this algorithm to find and classify cells in a complex imaging assay for mitochondrial function. In a second example, we demonstrate a workflow using manual comparisons and receiver operator characteristics to optimize the algorithm parameters for finding live and dead cells in a standard viability assay. Overall, this watershed-based algorithm is more advanced than traditional thresholding and can produce optimized, automated results. By incorporating associated pre-processing steps in the GUI, the algorithm is also easily adjusted, rendering it user-friendly. version:1
arxiv-1706-01340 | Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor | http://arxiv.org/abs/1706.01340 | id:1706.01340 author:Robin Ruede, Markus Müller, Sebastian Stüker, Alex Waibel category:cs.CL cs.CV cs.HC cs.LG cs.SD  published:2017-06-02 summary:Using supporting backchannel (BC) cues can make human-computer interaction more social. BCs provide a feedback from the listener to the speaker indicating to the speaker that he is still listened to. BCs can be expressed in different ways, depending on the modality of the interaction, for example as gestures or acoustic cues. In this work, we only considered acoustic cues. We are proposing an approach towards detecting BC opportunities based on acoustic input features like power and pitch. While other works in the field rely on the use of a hand-written rule set or specialized features, we made use of artificial neural networks. They are capable of deriving higher order features from input features themselves. In our setup, we first used a fully connected feed-forward network to establish an updated baseline in comparison to our previously proposed setup. We also extended this setup by the use of Long Short-Term Memory (LSTM) networks which have shown to outperform feed-forward based setups on various tasks. Our best system achieved an F1-Score of 0.37 using power and pitch features. Adding linguistic information using word2vec, the score increased to 0.39. version:1
arxiv-1706-00754 | Learning Bayes networks using interventional path queries in polynomial time and sample complexity | http://arxiv.org/abs/1706.00754 | id:1706.00754 author:Kevin Bello, Jean Honorio category:cs.LG stat.ML  published:2017-06-02 summary:Causal discovery from empirical data is a fundamental problem in many scientific domains. Observational data allows for identifiability only up to Markov equivalence class. In this paper, we propose a polynomial time algorithm for learning the exact structure of Bayesian networks with high probability, by using interventional path queries. Each path query takes as input an origin node and a target node, and answers whether there is a directed path from the origin to the target. This is done by intervening the origin node and observing samples from the target node. We theoretically show the logarithmic sample complexity for the size of interventional data per path query. Finally, we experimentally validate the correctness of our algorithm in synthetic and real-world networks. version:1
arxiv-1706-00741 | Prosodic Event Recognition using Convolutional Neural Networks with Context Information | http://arxiv.org/abs/1706.00741 | id:1706.00741 author:Sabrina Stehwien, Ngoc Thang Vu category:cs.CL  published:2017-06-02 summary:This paper demonstrates the potential of convolutional neural networks (CNN) for detecting and classifying prosodic events on words, specifically pitch accents and phrase boundary tones, from frame-based acoustic features. Typical approaches use not only feature representations of the word in question but also its surrounding context. We show that adding position features indicating the current word benefits the CNN. In addition, this paper discusses the generalization from a speaker-dependent modelling approach to a speaker-independent setup. The proposed method is simple and efficient and yields strong results not only in speaker-dependent but also speaker-independent cases. version:1
arxiv-1706-00729 | Parameter identification in Markov chain choice models | http://arxiv.org/abs/1706.00729 | id:1706.00729 author:Arushi Gupta, Daniel Hsu category:math.ST cs.LG stat.ML stat.TH  published:2017-06-02 summary:This work studies the parameter identification problem for the Markov chain choice model of Blanchet, Gallego, and Goyal used in assortment planning. In this model, the product selected by a customer is determined by a Markov chain over the products, where the products in the offered assortment are absorbing states. The underlying parameters of the model were previously shown to be identifiable from the choice probabilities for the all-products assortment, together with choice probabilities for assortments of all-but-one products. Obtaining and estimating choice probabilities for such large assortments is not desirable in many settings. The main result of this work is that the parameters may be identified from assortments of sizes two and three, regardless of the total number of products. The result is obtained via a simple and efficient parameter recovery algorithm. version:1
arxiv-1706-00719 | Automating Carotid Intima-Media Thickness Video Interpretation with Convolutional Neural Networks | http://arxiv.org/abs/1706.00719 | id:1706.00719 author:Jae Y. Shin, Nima Tajbakhsh, R. Todd Hurst, Christopher B. Kendall, Jianming Liang category:cs.CV cs.LG  published:2017-06-02 summary:Cardiovascular disease (CVD) is the leading cause of mortality yet largely preventable, but the key to prevention is to identify at-risk individuals before adverse events. For predicting individual CVD risk, carotid intima-media thickness (CIMT), a noninvasive ultrasound method, has proven to be valuable, offering several advantages over CT coronary artery calcium score. However, each CIMT examination includes several ultrasound videos, and interpreting each of these CIMT videos involves three operations: (1) select three end-diastolic ultrasound frames (EUF) in the video, (2) localize a region of interest (ROI) in each selected frame, and (3) trace the lumen-intima interface and the media-adventitia interface in each ROI to measure CIMT. These operations are tedious, laborious, and time consuming, a serious limitation that hinders the widespread utilization of CIMT in clinical practice. To overcome this limitation, this paper presents a new system to automate CIMT video interpretation. Our extensive experiments demonstrate that the suggested system significantly outperforms the state-of-the-art methods. The superior performance is attributable to our unified framework based on convolutional neural networks (CNNs) coupled with our informative image representation and effective post-processing of the CNN outputs, which are uniquely designed for each of the above three operations. version:1
arxiv-1706-00712 | Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning? | http://arxiv.org/abs/1706.00712 | id:1706.00712 author:Nima Tajbakhsh, Jae Y. Shin, Suryakanth R. Gurudu, R. Todd Hurst, Christopher B. Kendall, Michael B. Gotway, Jianming Liang category:cs.CV cs.LG  published:2017-06-02 summary:Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: \emph{Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch?} To address this question, we considered 4 distinct medical imaging applications in 3 specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from 3 different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that (1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; (2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; (3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and (4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data. version:1
arxiv-1705-07250 | Speedup from a different parametrization within the Neural Network algorithm | http://arxiv.org/abs/1705.07250 | id:1705.07250 author:Michael F. Zimmer category:cs.LG K.3.2  published:2017-05-20 summary:A different parametrization of the hyperplanes is used in the neural network algorithm. As demonstrated on several autoencoder examples it significantly outperforms the usual parametrization, reaching lower training error values with only a fraction of the number of epochs. It's argued that it makes it easier to understand and initialize the parameters. version:3
arxiv-1706-00705 | Streaming Bayesian inference: theoretical limits and mini-batch approximate message-passing | http://arxiv.org/abs/1706.00705 | id:1706.00705 author:Andre Manoel, Florent Krzakala, Eric W. Tramel, Lenka Zdeborová category:stat.ML cond-mat.stat-mech cs.IT math.IT  published:2017-06-02 summary:In statistical learning for real-world large-scale data problems, one must often resort to "streaming" algorithms which operate sequentially on small batches of data. In this work, we present an analysis of the information-theoretic limits of mini-batch inference in the context of generalized linear models and low-rank matrix factorization. In a controlled Bayes-optimal setting, we characterize the optimal performance and phase transitions as a function of mini-batch size. We base part of our results on a detailed analysis of a mini-batch version of the approximate message-passing algorithm (Mini-AMP), which we introduce. Additionally, we show that this theoretical optimality carries over into real-data problems by illustrating that Mini-AMP is competitive with standard streaming algorithms for clustering. version:1
arxiv-1706-00699 | Temporal Action Labeling using Action Sets | http://arxiv.org/abs/1706.00699 | id:1706.00699 author:Alexander Richard, Hilde Kuehne, Juergen Gall category:cs.CV  published:2017-06-02 summary:Action detection and temporal segmentation of actions in videos are topics of increasing interest. While fully supervised systems have gained much attention lately, full annotation of each action within the video is costly and impractical for large amounts of video data. Thus, weakly supervised action detection and temporal segmentation methods are of great importance. While most works in this area assume an ordered sequence of occurring actions to be given, our approach only uses a set of actions. Such action sets provide much less supervision since neither action ordering nor the number of action occurrences are known. In exchange, they can be easily obtained, for instance, from meta-tags, while ordered sequences still require human annotation. We introduce a system that automatically learns to temporally segment and label actions in a video, where the only supervision that is used are action sets. We evaluate our method on three datasets and show that it performs close to or on par with recent weakly supervised methods that require ordering constraints. version:1
arxiv-1706-00687 | Weight Sharing is Crucial to Succesful Optimization | http://arxiv.org/abs/1706.00687 | id:1706.00687 author:Shai Shalev-Shwartz, Ohad Shamir, Shaked Shammah category:cs.LG  published:2017-06-02 summary:Exploiting the great expressive power of Deep Neural Network architectures, relies on the ability to train them. While current theoretical work provides, mostly, results showing the hardness of this task, empirical evidence usually differs from this line, with success stories in abundance. A strong position among empirically successful architectures is captured by networks where extensive weight sharing is used, either by Convolutional or Recurrent layers. Additionally, characterizing specific aspects of different tasks, making them "harder" or "easier", is an interesting direction explored both theoretically and empirically. We consider a family of ConvNet architectures, and prove that weight sharing can be crucial, from an optimization point of view. We explore different notions of the frequency, of the target function, proving necessity of the target function having some low frequency components. This necessity is not sufficient - only with weight sharing can it be exploited, thus theoretically separating architectures using it, from others which do not. Our theoretical results are aligned with empirical experiments in an even more general setting, suggesting viability of examination of the role played by interleaving those aspects in broader families of tasks. version:1
arxiv-1706-00636 | WiFi based trajectory alignment, calibration and easy site survey using smart phones and foot-mounted IMUs | http://arxiv.org/abs/1706.00636 | id:1706.00636 author:Yang Gu, Caifa Zhou, Andreas Wieser, Zhimin Zhou category:stat.ML stat.AP  published:2017-06-02 summary:Foot-mounted inertial positioning (FMIP) can face problems of inertial drifts and unknown initial states in real applications, which renders the estimated trajectories inaccurate and not obtained in a well defined coordinate system for matching trajectories of different users. In this paper, an approach adopting received signal strength (RSS) measurements for Wifi access points (APs) are proposed to align and calibrate the trajectories estimated from foot mounted inertial measurement units (IMUs). A crowd-sourced radio map (RM) can be built subsequently and can be used for fingerprinting based Wifi indoor positioning (FWIP). The foundation of the proposed approach is graph based simultaneously localization and mapping (SLAM). The nodes in the graph denote users poses and the edges denote the pairwise constrains between the nodes. The constrains are derived from: (1) inertial estimated trajectories; (2) vicinity in the RSS space. With these constrains, an error functions is defined. By minimizing the error function, the graph is optimized and the aligned/calibrated trajectories along with the RM are acquired. The experimental results have corroborated the effectiveness of the approach for trajectory alignment, calibration as well as RM construction. version:1
arxiv-1706-00633 | Robust Deep Learning via Reverse Cross-Entropy Training and Thresholding Test | http://arxiv.org/abs/1706.00633 | id:1706.00633 author:Tianyu Pang, Chao Du, Jun Zhu category:cs.LG  published:2017-06-02 summary:Though the recent progress is substantial, deep learning methods can be vulnerable to the elaborately crafted adversarial samples. In this paper, we attempt to improve the robustness by presenting a new training procedure and a thresholding test strategy. In training, we propose to minimize the reverse cross-entropy, which encourages a deep network to learn latent representations that better distinguish adversarial samples from normal ones. In testing, we propose to use a thresholding strategy based on a new metric to filter out adversarial samples for reliable predictions. Our method is simple to implement using standard algorithms, with little extra training cost compared to the common cross-entropy minimization. We apply our method to various state-of-the-art networks (e.g., residual networks) and we achieve significant improvements on robust predictions in the adversarial setting. version:1
arxiv-1706-00631 | Dual-reference Face Retrieval: What Does He/She Look Like at Age `X'? | http://arxiv.org/abs/1706.00631 | id:1706.00631 author:BingZhang Hu, Feng Zheng, Ling Shao category:cs.CV  published:2017-06-02 summary:Face retrieval has received much attention over the past few decades, and many efforts have been made in retrieving face images against pose, illumination, and expression variations. However, the conventional works fail to meet the requirements of a potential and novel task --- retrieving a person's face image at a given age, \ie `what does a person look like at age $X$?' The reason that previous works struggle is that text-based approaches generally suffer from insufficient age labels and content-based methods typically take a single input image as query, which can only indicate either the identity or the age. To tackle this problem, we propose a dual reference face retrieval framework in this paper, where the identity and the age are reflected by two reference images respectively. In our framework, the raw images are first projected on a joint manifold, which preserves both the age and identity locality. Then two similarity metrics of age and identity are exploited and optimized by utilizing our proposed quartet-based model. The quartet-based model is novel as it simultaneously describes the similarity in two aspects: identity and age. The experiment shows a promising result, outperforming hierarchical methods. It is also shown that the learned joint manifold is a powerful representation of the human face. version:1
arxiv-1706-00613 | Facies classification from well logs using an inception convolutional network | http://arxiv.org/abs/1706.00613 | id:1706.00613 author:Valentin Tschannen, Matthias Delescluse, Mathieu Rodriguez, Janis Keuper category:cs.CV 86-04  published:2017-06-02 summary:The idea to use automated algorithms to determine geological facies from well logs is not new (see e.g Busch et al. (1987); Rabaute (1998)) but the recent and dramatic increase in research in the field of machine learning makes it a good time to revisit the topic. Following an exercise proposed by Dubois et al. (2007) and Hall (2016) we employ a modern type of deep convolutional network, called \textit{inception network} (Szegedy et al., 2015), to tackle the supervised classification task and we discuss the methodological limits of such problem as well as further research opportunities. version:1
arxiv-1706-00612 | Attentive Convolutional Neural Network based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech | http://arxiv.org/abs/1706.00612 | id:1706.00612 author:Michael Neumann, Ngoc Thang Vu category:cs.CL  published:2017-06-02 summary:Speech emotion recognition is an important and challenging task in the realm of human-computer interaction. Prior work proposed a variety of models and feature sets for training a system. In this work, we conduct extensive experiments using an attentive convolutional neural network with multi-view learning objective function. We compare system performance using different lengths of the input signal, different types of acoustic features and different types of emotion speech (improvised/scripted). Our experimental results on the Interactive Emotional Motion Capture (IEMOCAP) database reveal that the recognition performance strongly depends on the type of speech data independent of the choice of input features. Furthermore, we achieved state-of-the-art results on the improvised speech data of IEMOCAP. version:1
arxiv-1706-00598 | Dynamic Steerable Blocks in Deep Residual Networks | http://arxiv.org/abs/1706.00598 | id:1706.00598 author:Jörn-Henrik Jacobsen, Bert de Brabandere, Arnold W. M. Smeulders category:cs.CV stat.ML  published:2017-06-02 summary:Filters in convolutional networks are typically parameterized in a pixel basis, that does not take prior knowledge about the visual world into account. We investigate the generalized notion of frames, that can be designed with image properties in mind, as alternatives to this parametrization. We show that frame-based ResNets and Densenets can improve performance on Cifar-10+ consistently, while having additional pleasant properties like steerability. By exploiting these transformation properties explicitly, we arrive at dynamic steerable blocks. They are an extension of residual blocks, that are able to seamlessly transform filters under pre-defined transformations, conditioned on the input at training and inference time. Dynamic steerable blocks learn the degree of invariance from data and locally adapt filters, allowing them to apply a different geometrical variant of the same filter to each location of the feature map. When evaluated on the Berkeley Segmentation contour detection dataset, our approach outperforms all competing approaches that do not utilize pre-training, highlighting the benefits of image-based regularization to deep networks. version:1
arxiv-1706-00597 | Image Restoration from Patch-based Compressed Sensing Measurement | http://arxiv.org/abs/1706.00597 | id:1706.00597 author:Guangtao Nie, Ying Fu, Yinqiang Zheng, Hua Huang category:cs.CV  published:2017-06-02 summary:A series of methods have been proposed to reconstruct an image from compressively sensed random measurement, but most of them have high time complexity and are inappropriate for patch-based compressed sensing capture, because of their serious blocky artifacts in the restoration results. In this paper, we present a non-iterative image reconstruction method from patch-based compressively sensed random measurement. Our method features two cascaded networks based on residual convolution neural network to learn the end-to-end full image restoration, which is capable of reconstructing image patches and removing the blocky effect with low time cost. Experimental results on synthetic and real data show that our method outperforms state-of-the-art compressive sensing (CS) reconstruction methods with patch-based CS measurement. To demonstrate the effectiveness of our method in more general setting, we apply the de-block process in our method to JPEG compression artifacts removal and achieve outstanding performance as well. version:1
arxiv-1706-00593 | Joint Modeling of Topics, Citations, and Topical Authority in Academic Corpora | http://arxiv.org/abs/1706.00593 | id:1706.00593 author:Jooyeon Kim, Dongwoo Kim, Alice Oh category:cs.CL cs.DL cs.SI  published:2017-06-02 summary:Much of scientific progress stems from previously published findings, but searching through the vast sea of scientific publications is difficult. We often rely on metrics of scholarly authority to find the prominent authors but these authority indices do not differentiate authority based on research topics. We present Latent Topical-Authority Indexing (LTAI) for jointly modeling the topics, citations, and topical authority in a corpus of academic papers. Compared to previous models, LTAI differs in two main aspects. First, it explicitly models the generative process of the citations, rather than treating the citations as given. Second, it models each author's influence on citations of a paper based on the topics of the cited papers, as well as the citing papers. We fit LTAI to four academic corpora: CORA, Arxiv Physics, PNAS, and Citeseer. We compare the performance of LTAI against various baselines, starting with the latent Dirichlet allocation, to the more advanced models including author-link topic model and dynamic author citation topic model. The results show that LTAI achieves improved accuracy over other similar models when predicting words, citations and authors of publications. version:1
arxiv-1706-00587 | Learning-based Surgical Workflow Detection from Intra-Operative Signals | http://arxiv.org/abs/1706.00587 | id:1706.00587 author:Ralf Stauder, Ergün Kayis, Nassir Navab category:cs.LG  published:2017-06-02 summary:A modern operating room (OR) provides a plethora of advanced medical devices. In order to better facilitate the information offered by them, they need to automatically react to the intra-operative context. To this end, the progress of the surgical workflow must be detected and interpreted, so that the current status can be given in machine-readable form. In this work, Random Forests (RF) and Hidden Markov Models (HMM) are compared and combined to detect the surgical workflow phase of a laparoscopic cholecystectomy. Various combinations of data were tested, from using only raw sensor data to filtered and augmented datasets. Achieved accuracies ranged from 64% to 72% for the RF approach, and from 80% to 82% for the combination of RF and HMM. version:1
arxiv-1706-01338 | Understanding the Learned Iterative Soft Thresholding Algorithm with matrix factorization | http://arxiv.org/abs/1706.01338 | id:1706.01338 author:Thomas Moreau, Joan Bruna category:stat.ML  published:2017-06-02 summary:Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in Gregor and Le Cun (2010), which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately. In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails. version:1
arxiv-1705-04358 | Object-Level Context Modeling For Scene Classification with Context-CNN | http://arxiv.org/abs/1705.04358 | id:1705.04358 author:Syed Ashar Javed, Anil Kumar Nelakanti category:cs.CV  published:2017-05-11 summary:Convolutional Neural Networks (CNNs) have been used extensively for computer vision tasks and produce rich feature representation for objects or parts of an image. But reasoning about scenes requires integration between the low-level feature representations and the high-level semantic information. We propose a deep network architecture which models the semantic context of scenes by capturing object-level information. We use Long Short Term Memory(LSTM) units in conjunction with object proposals to incorporate object-object relationship and object-scene relationship in an end-to-end trainable manner. We evaluate our model on the LSUN dataset and achieve results comparable to the state-of-art. We further show visualization of the learned features and analyze the model with experiments to verify our model's ability to model context. version:2
arxiv-1706-00556 | Recursive Cross-Domain Face/Sketch Generation from Limited Facial Parts | http://arxiv.org/abs/1706.00556 | id:1706.00556 author:Yang Song, Zhifei Zhang, Hairong Qi category:cs.CV  published:2017-06-02 summary:We start by asking an interesting yet challenging question, "If an eyewitness can only recall the eye features of the suspect, such that the forensic artist can only produce a sketch of the eyes (e.g., the top-left sketch shown in Fig. 1), can advanced computer vision techniques help generate the whole face image?" A more generalized question is that if a large proportion (e.g., more than 50%) of the face/sketch is missing, can a realistic whole face sketch/image still be estimated. Existing face completion and generation methods either do not conduct domain transfer learning or can not handle large missing area. For example, the inpainting approach tends to blur the generated region when the missing area is large (i.e., more than 50%). In this paper, we exploit the potential of deep learning networks in filling large missing region (e.g., as high as 95% missing) and generating realistic faces with high-fidelity in cross domains. We propose the recursive generation by bidirectional transformation networks (r-BTN) that recursively generates a whole face/sketch from a small sketch/face patch. The large missing area and the cross domain challenge make it difficult to generate satisfactory results using a unidirectional cross-domain learning structure. On the other hand, a forward and backward bidirectional learning between the face and sketch domains would enable recursive estimation of the missing region in an incremental manner (Fig. 1) and yield appealing results. r-BTN also adopts an adversarial constraint to encourage the generation of realistic faces/sketches. Extensive experiments have been conducted to demonstrate the superior performance from r-BTN as compared to existing potential solutions. version:1
arxiv-1706-00552 | SAR Image Despeckling Using a Convolutional | http://arxiv.org/abs/1706.00552 | id:1706.00552 author:Puyang Wang, He Zhang, Vishal M. Patel category:cs.CV  published:2017-06-02 summary:Synthetic Aperture Radar (SAR) images are often contaminated by a multiplicative noise known as speckle. Speckle makes the processing and interpretation of SAR images difficult. We propose a deep learning-based approach called, Image Despeckling Convolutional Neural Network (ID-CNN), for automatically removing speckle from the input noisy images. In particular, ID-CNN uses a set of convolutional layers along with batch normalization and rectified linear unit (ReLU) activation function and a component-wise division residual layer to estimate speckle and it is trained in an end-to-end fashion using a combination of Euclidean loss and Total Variation (TV) loss. Extensive experiments on synthetic and real SAR images show that the proposed method achieves significant improvements over the state-of-the-art speckle reduction methods. version:1
arxiv-1706-00544 | Bias-Variance Tradeoff of Graph Laplacian Regularizer | http://arxiv.org/abs/1706.00544 | id:1706.00544 author:Pin-Yu Chen, Sijia Liu category:stat.ML cs.LG cs.SI  published:2017-06-02 summary:This paper presents a bias-variance tradeoff of graph Laplacian regularizer, which is widely used in graph signal processing and semi-supervised learning tasks. The scaling law of the optimal regularization parameter is specified in terms of the spectral graph properties and a novel signal-to-noise ratio parameter, which suggests selecting a mediocre regularization parameter is often suboptimal. The analysis is applied to three applications, including random, band-limited, and multiple-sampled graph signals. Experiments on synthetic and real-world graphs demonstrate near-optimal performance of the established analysis. version:1
arxiv-1705-09407 | An Efficient Algorithm for Bayesian Nearest Neighbours | http://arxiv.org/abs/1705.09407 | id:1705.09407 author:Giuseppe Nuti category:cs.LG stat.ML  published:2017-05-26 summary:K-Nearest Neighbours (k-NN) is a popular classification and regression algorithm, yet one of its main limitations is the difficulty in choosing the number of neighbours. We present a Bayesian algorithm to compute the posterior probability distribution for k given a target point within a data-set, efficiently and without the use of Markov Chain Monte Carlo (MCMC) methods or simulation - alongside an exact solution for distributions within the exponential family. The central idea is that data points around our target are generated by the same probability distribution, extending outwards over the appropriate, though unknown, number of neighbours. Once the data is projected onto a distance metric of choice, we can transform the choice of k into a change-point detection problem, for which there is an efficient solution: we recursively compute the probability of the last change-point as we move towards our target, and thus de facto compute the posterior probability distribution over k. Applying this approach to both a classification and a regression UCI data-sets, we compare favourably and, most importantly, by removing the need for simulation, we are able to compute the posterior probability of k exactly and rapidly. As an example, the computational time for the Ripley data-set is a few milliseconds compared to a few hours when using a MCMC approach. version:2
arxiv-1706-00531 | PixelGAN Autoencoders | http://arxiv.org/abs/1706.00531 | id:1706.00531 author:Alireza Makhzani, Brendan Frey category:cs.LG  published:2017-06-02 summary:In this paper, we describe the "PixelGAN autoencoder", a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels (PixelCNN) that is conditioned on a latent code, and the recognition path uses a generative adversarial network (GAN) to impose a prior distribution on the latent code. We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder. For example, by imposing a Gaussian distribution as the prior, we can achieve a global vs. local decomposition, or by imposing a categorical distribution as the prior, we can disentangle the style and content information of images in an unsupervised fashion. We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi-supervised settings and achieve competitive semi-supervised classification results on the MNIST, SVHN and NORB datasets. version:1
arxiv-1706-00530 | Integrated Deep and Shallow Networks for Salient Object Detection | http://arxiv.org/abs/1706.00530 | id:1706.00530 author:Jing Zhang, Bo Li, Yuchao Dai, Fatih Porikli, Mingyi He category:cs.CV  published:2017-06-02 summary:Deep convolutional neural network (CNN) based salient object detection methods have achieved state-of-the-art performance and outperform those unsupervised methods with a wide margin. In this paper, we propose to integrate deep and unsupervised saliency for salient object detection under a unified framework. Specifically, our method takes results of unsupervised saliency (Robust Background Detection, RBD) and normalized color images as inputs, and directly learns an end-to-end mapping between inputs and the corresponding saliency maps. The color images are fed into a Fully Convolutional Neural Networks (FCNN) adapted from semantic segmentation to exploit high-level semantic cues for salient object detection. Then the results from deep FCNN and RBD are concatenated to feed into a shallow network to map the concatenated feature maps to saliency maps. Finally, to obtain a spatially consistent saliency map with sharp object boundaries, we fuse superpixel level saliency map at multi-scale. Extensive experimental results on 8 benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art approaches with a margin. version:1
arxiv-1706-00527 | Data Augmentation of Wearable Sensor Data for Parkinson's Disease Monitoring using Convolutional Neural Networks | http://arxiv.org/abs/1706.00527 | id:1706.00527 author:Terry Taewoong Um, Franz Michael Josef Pfister, Daniel Pichler, Satoshi Endo, Muriel Lang, Sandra Hirche, Urban Fietzek, Dana Kulić category:cs.CV  published:2017-06-02 summary:While convolutional neural networks (CNNs) have been successfully applied to many challenging classification applications, they typically require large datasets for training. When the availability of labeled data is limited, data augmentation is a critical preprocessing step for CNNs. However, data augmentation for wearable sensor data has not been deeply investigated yet. In this paper, various data augmentation methods for wearable sensor data are proposed. The proposed methods and CNNs are applied to the problem of classifying the motor state of Parkinson's Disease (PD) patients, which is challenging due to small dataset size, noisy labels, and large within-class variability. Appropriate augmentation improves the classification performance from 76.7% to 92.0%. version:1
arxiv-1705-11175 | Long-term Correlation Tracking using Multi-layer Hybrid Features in Sparse and Dense Environments | http://arxiv.org/abs/1705.11175 | id:1705.11175 author:Nathanael L. Baisa, Deepayan Bhowmik, Andrew Wallace category:cs.CV  published:2017-05-31 summary:Tracking a target of interest in both sparse and crowded environments is a challenging problem, not yet successfully addressed in the literature. In this paper, we propose a new long-term visual tracking algorithm, learning discriminative correlation filters and using an online classifier, to track a target of interest in both sparse and crowded video sequences. First, we learn a translation correlation filter using a multi-layer hybrid of convolutional neural networks (CNN) and traditional hand-crafted features. We combine advantages of both the lower convolutional layer which retains more spatial details for precise localization and the higher convolutional layer which encodes semantic information for handling appearance variations, and then integrate these with histogram of oriented gradients (HOG) and color-naming traditional features. Second, we include a re-detection module for overcoming tracking failures due to long-term occlusions by training an incremental (online) SVM on the most confident frames using hand-engineered features. This re-detection module is activated only when the correlation response of the object is below some pre-defined threshold. This generates high score detection proposals which are temporally filtered using a Gaussian mixture probability hypothesis density (GM-PHD) filter to find the detection proposal with the maximum weight as the target state estimate by removing the other detection proposals as clutter. Finally, we learn a scale correlation filter for estimating the scale of a target by constructing a target pyramid around the estimated or re-detected position using the HOG features. We carry out extensive experiments on both sparse and dense data sets which show that our method significantly outperforms state-of-the-art methods. version:2
arxiv-1706-00514 | Selective Inference for Multi-Dimensional Multiple Change Point Detection | http://arxiv.org/abs/1706.00514 | id:1706.00514 author:Yuta Umezu, Ichiro Takeuchi category:stat.ML  published:2017-06-01 summary:We consider the problem of multiple change point (CP) detection from a multi-dimensional sequence. We are mainly interested in the situation where changes are observed only in a subset of multiple dimensions at each CP. In such a situation, we need to select not only the time points but also the dimensions where changes actually occur. In this paper we study a class of multi-dimensional multiple CP detection algorithms for this task. Our main contribution is to introduce a statistical framework for controlling the false detection probability of these class of CP detection algorithms. The key idea is to regard a CP detection problem as a {\it selective inference} problem, and derive the sampling distribution of the test statistic under the condition that those CPs are detected by applying the algorithm to the data. By using an analytical tool recently developed in the selective inference literature, we show that, for a wide class of multi-dimensional multiple CP detection algorithms, it is possible to exactly (non-asymptotically) control the false detection probability at the desired significance level. version:1
arxiv-1706-00510 | A Vision System for Multi-View Face Recognition | http://arxiv.org/abs/1706.00510 | id:1706.00510 author:M. Y. Shams, A. S. Tolba, S. H. Sarhan category:cs.CV  published:2017-06-01 summary:Multimodal biometric identification has been grown a great attention in the most interests in the security fields. In the real world there exist modern system devices that are able to detect, recognize, and classify the human identities with reliable and fast recognition rates. Unfortunately most of these systems rely on one modality, and the reliability for two or more modalities are further decreased. The variations of face images with respect to different poses are considered as one of the important challenges in face recognition systems. In this paper, we propose a multimodal biometric system that able to detect the human face images that are not only one view face image, but also multi-view face images. Each subject entered to the system adjusted their face at front of the three cameras, and then the features of the face images are extracted based on Speeded Up Robust Features (SURF) algorithm. We utilize Multi-Layer Perceptron (MLP) and combined classifiers based on both Learning Vector Quantization (LVQ), and Radial Basis Function (RBF) for classification purposes. The proposed system has been tested using SDUMLA-HMT, and CASIA datasets. Furthermore, we collected a database of multi-view face images by which we take the additive white Gaussian noise into considerations. The results indicated the reliability, robustness of the proposed system with different poses and variations including noise images. version:1
arxiv-1706-00506 | Morphological Embeddings for Named Entity Recognition in Morphologically Rich Languages | http://arxiv.org/abs/1706.00506 | id:1706.00506 author:Onur Gungor, Eray Yildiz, Suzan Uskudarli, Tunga Gungor category:cs.CL  published:2017-06-01 summary:In this work, we present new state-of-the-art results of 93.59,% and 79.59,% for Turkish and Czech named entity recognition based on the model of (Lample et al., 2016). We contribute by proposing several schemes for representing the morphological analysis of a word in the context of named entity recognition. We show that a concatenation of this representation with the word and character embeddings improves the performance. The effect of these representation schemes on the tagging performance is also investigated. version:1
arxiv-1706-00505 | Discriminative conditional restricted Boltzmann machine for discrete choice and latent variable modelling | http://arxiv.org/abs/1706.00505 | id:1706.00505 author:Melvin Wong, Bilal Farooq, Guillaume-Alexandre Bilodeau category:cs.LG  published:2017-06-01 summary:Conventional methods of estimating latent behaviour generally use attitudinal questions which are subjective and these survey questions may not always be available. We hypothesize that an alternative approach can be used for latent variable estimation through an undirected graphical models. For instance, non-parametric artificial neural networks. In this study, we explore the use of generative non-parametric modelling methods to estimate latent variables from prior choice distribution without the conventional use of measurement indicators. A restricted Boltzmann machine is used to represent latent behaviour factors by analyzing the relationship information between the observed choices and explanatory variables. The algorithm is adapted for latent behaviour analysis in discrete choice scenario and we use a graphical approach to evaluate and understand the semantic meaning from estimated parameter vector values. We illustrate our methodology on a financial instrument choice dataset and perform statistical analysis on parameter sensitivity and stability. Our findings show that through non-parametric statistical tests, we can extract useful latent information on the behaviour of latent constructs through machine learning methods and present strong and significant influence on the choice process. Furthermore, our modelling framework shows robustness in input variability through sampling and validation. version:1
arxiv-1706-00504 | Dynamic Stripes: Exploiting the Dynamic Precision Requirements of Activation Values in Neural Networks | http://arxiv.org/abs/1706.00504 | id:1706.00504 author:Alberto Delmas, Patrick Judd, Sayeh Sharify, Andreas Moshovos category:cs.NE cs.LG  published:2017-06-01 summary:Stripes is a Deep Neural Network (DNN) accelerator that uses bit-serial computation to offer performance that is proportional to the fixed-point precision of the activation values. The fixed-point precisions are determined a priori using profiling and are selected at a per layer granularity. This paper presents Dynamic Stripes, an extension to Stripes that detects precision variance at runtime and at a finer granularity. This extra level of precision reduction increases performance by 41% over Stripes. version:1
arxiv-1704-06625 | Learned D-AMP: Principled Neural-Network-based Compressive Image Recovery | http://arxiv.org/abs/1704.06625 | id:1704.06625 author:Christopher A. Metzler, Ali Mousavi, Richard G. Baraniuk category:stat.ML cs.LG  published:2017-04-21 summary:Compressive image recovery is a challenging problem that requires fast and accurate algorithms. Recently, neural networks have been applied to this problem with promising results. By exploiting massively parallel GPU processing architectures and oodles of training data, they are able to run orders of magnitude faster than existing methods. Unfortunately, these methods are difficult to train, often-times specific to a single measurement matrix, and largely unprincipled blackboxes. It was recently demonstrated that iterative sparse-signal-recovery algorithms can be unrolled to form interpretable deep neural networks. Taking inspiration from this work, we develop novel neural network architectures that mimic the behavior of the denoising-based approximate message passing (D-AMP) and denoising-based vector approximate message passing (D-VAMP) algorithms. We call these new networks {\em Learned} D-AMP (LDAMP) and {\em Learned} D-VAMP (LDVAMP). The LDAMP/LDVAMP networks are easy to train, can be applied to a variety of different measurement matrices, and come with a state-evolution heuristic that accurately predicts their performance. Most importantly, our networks outperform the state-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and runtime. At high resolutions, and when used with matrices which have fast matrix multiply implementations, LDAMP runs over $50\times$ faster than BM3D-AMP and hundreds of times faster than NLR-CS. version:3
arxiv-1704-04932 | Deep Relaxation: partial differential equations for optimizing deep neural networks | http://arxiv.org/abs/1704.04932 | id:1704.04932 author:Pratik Chaudhari, Adam Oberman, Stanley Osher, Stefano Soatto, Guillaume Carlier category:cs.LG math.AP math.OC  published:2017-04-17 summary:In this paper we establish a connection between non-convex optimization methods for training deep neural networks and nonlinear partial differential equations (PDEs). Relaxation techniques arising in statistical physics which have already been used successfully in this context are reinterpreted as solutions of a viscous Hamilton-Jacobi PDE. Using a stochastic control interpretation allows we prove that the modified algorithm performs better in expectation that stochastic gradient descent. Well-known PDE regularity results allow us to analyze the geometry of the relaxed energy landscape, confirming empirical evidence. The PDE is derived from a stochastic homogenization problem, which arises in the implementation of the algorithm. The algorithms scale well in practice and can effectively tackle the high dimensionality of modern neural networks. version:2
arxiv-1706-00493 | Personalized Pancreatic Tumor Growth Prediction via Group Learning | http://arxiv.org/abs/1706.00493 | id:1706.00493 author:Ling Zhang, Le Lu, Ronald M. Summers, Electron Kebebew, Jianhua Yao category:cs.CV cs.LG  published:2017-06-01 summary:Tumor growth prediction, a highly challenging task, has long been viewed as a mathematical modeling problem, where the tumor growth pattern is personalized based on imaging and clinical data of a target patient. Though mathematical models yield promising results, their prediction accuracy may be limited by the absence of population trend data and personalized clinical characteristics. In this paper, we propose a statistical group learning approach to predict the tumor growth pattern that incorporates both the population trend and personalized data, in order to discover high-level features from multimodal imaging data. A deep convolutional neural network approach is developed to model the voxel-wise spatio-temporal tumor progression. The deep features are combined with the time intervals and the clinical factors to feed a process of feature selection. Our predictive model is pretrained on a group data set and personalized on the target patient data to estimate the future spatio-temporal progression of the patient's tumor. Multimodal imaging data at multiple time points are used in the learning, personalization and inference stages. Our method achieves a Dice coefficient of 86.8% +- 3.6% and RVD of 7.9% +- 5.4% on a pancreatic tumor data set, outperforming the DSC of 84.4% +- 4.0% and RVD 13.9% +- 9.8% obtained by a previous state-of-the-art model-based method. version:1
arxiv-1706-01513 | Beyond Volume: The Impact of Complex Healthcare Data on the Machine Learning Pipeline | http://arxiv.org/abs/1706.01513 | id:1706.01513 author:Keith Feldman, Louis Faust, Xian Wu, Chao Huang, Nitesh V. Chawla category:cs.CY cs.LG stat.ML  published:2017-06-01 summary:From medical charts to national census, healthcare has traditionally operated under a paper-based paradigm. However, the past decade has marked a long and arduous transformation bringing healthcare into the digital age. Ranging from electronic health records, to digitized imaging and laboratory reports, to public health datasets, today, healthcare now generates an incredible amount of information. Such a wealth of data presents an exciting opportunity for integrated machine learning solutions to address problems across multiple facets of healthcare practice and administration. Unfortunately, the ability to derive accurate and informative clinical insights requires more than the ability to execute machine learning models. Rather, a deeper understanding of the data on which the models are run is imperative for their success. While a significant effort has been undertaken to develop models able to process the volume of data obtained during the analysis of millions of digitalized patient records, it is important to remember that volume represents only one aspect of the data. In fact, drawing on data from an increasingly diverse set of sources, healthcare data presents an incredibly complex set of attributes that must be accounted for throughout the machine learning pipeline. This chapter focused on highlighting such challenges, and is broken down into three distinct components, each representing a phase of the pipeline. We begin with attributes of the data accounted for during preprocessing, then move to considerations during model building, and end with challenges to the interpretation of model output. For each component, we present a discussion around data as it related to the healthcare domain and offer insight into the challenges each may impose on the efficiency of machine learning techniques. version:1
arxiv-1706-00476 | The Mixing method: coordinate descent for low-rank semidefinite programming | http://arxiv.org/abs/1706.00476 | id:1706.00476 author:Po-Wei Wang, Wei-Cheng Chang, J. Zico Kolter category:math.OC cs.LG stat.ML  published:2017-06-01 summary:In this paper, we propose a coordinate descent approach to low-rank structured semidefinite programming. The approach, which we call the Mixing method, is extremely simple to implement, has no free parameters, and typically attains an order of magnitude or better improvement in optimization performance over the current state of the art. We show that for certain problems, the method is strictly decreasing and guaranteed to converge to a critical point. We then apply the algorithm to three separate domains: solving the maximum cut semidefinite relaxation, solving a (novel) maximum satisfiability relaxation, and solving the GloVe word embedding optimization problem. In all settings, we demonstrate improvement over the existing state of the art along various dimensions. In total, this work substantially expands the scope and scale of problems that can be solved using semidefinite programming methods. version:1
arxiv-1706-00473 | Deep Learning: A Bayesian Perspective | http://arxiv.org/abs/1706.00473 | id:1706.00473 author:Nicholas Polson, Vadim Sokolov category:stat.ML cs.LG stat.ME  published:2017-06-01 summary:Deep learning is a form of machine learning for nonlinear high dimensional data reduction and prediction. A Bayesian probabilistic perspective provides a number of advantages. Specifically statistical interpretation and properties, more efficient algorithms for optimisation and hyper-parameter tuning, and an explanation of predictive performance. Traditional high-dimensional statistical techniques; principal component analysis (PCA), partial least squares (PLS), reduced rank regression (RRR), projection pursuit regression (PPR) are shown to be shallow learners. Their deep learning counterparts exploit multiple layers of of data reduction which leads to performance gains. Stochastic gradient descent (SGD) training and optimisation and Dropout (DO) provides model and variable selection. Bayesian regularization is central to finding networks and provides a framework for optimal bias-variance trade-off to achieve good out-of sample performance. Constructing good Bayesian predictors in high dimensions is discussed. To illustrate our methodology, we provide an analysis of first time international bookings on Airbnb. Finally, we conclude with directions for future research. version:1
arxiv-1706-00468 | Function Assistant: A Tool for NL Querying of APIs | http://arxiv.org/abs/1706.00468 | id:1706.00468 author:Kyle Richardson, Jonas Kuhn category:cs.CL  published:2017-06-01 summary:In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using natural language. The toolkit is designed to help end-users of a target API quickly find information about functions through high-level natural language queries and descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the API using the semantic parsing approach of Richardson and Kuhn (2017). Translations are automatically learned from example text-code pairs in example APIs. The toolkit includes features for building translation pipelines and query engines for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github. version:1
arxiv-1706-00465 | Machine Assisted Analysis of Vowel Length Contrasts in Wolof | http://arxiv.org/abs/1706.00465 | id:1706.00465 author:Elodie Gauthier, Laurent Besacier, Sylvie Voisin category:cs.CL  published:2017-06-01 summary:Growing digital archives and improving algorithms for automatic analysis of text and speech create new research opportunities for fundamental research in phonetics. Such empirical approaches allow statistical evaluation of a much larger set of hypothesis about phonetic variation and its conditioning factors (among them geographical / dialectal variants). This paper illustrates this vision and proposes to challenge automatic methods for the analysis of a not easily observable phenomenon: vowel length contrast. We focus on Wolof, an under-resourced language from Sub-Saharan Africa. In particular, we propose multiple features to make a fine evaluation of the degree of length contrast under different factors such as: read vs semi spontaneous speech ; standard vs dialectal Wolof. Our measures made fully automatically on more than 20k vowel tokens show that our proposed features can highlight different degrees of contrast for each vowel considered. We notably show that contrast is weaker in semi-spontaneous speech and in a non standard semi-spontaneous dialect. version:1
arxiv-1706-00457 | NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems | http://arxiv.org/abs/1706.00457 | id:1706.00457 author:Ozan Caglayan, Mercedes García-Martínez, Adrien Bardet, Walid Aransa, Fethi Bougares, Loïc Barrault category:cs.CL  published:2017-06-01 summary:In this paper, we present nmtpy, a flexible Python toolkit based on Theano for training Neural Machine Translation and other neural sequence-to-sequence architectures. nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written. nmtpy has been used for LIUM's top-ranked submissions to WMT Multimodal Machine Translation and News Translation tasks in 2016 and 2017. version:1
arxiv-1706-00447 | Provenance Filtering for Multimedia Phylogeny | http://arxiv.org/abs/1706.00447 | id:1706.00447 author:Allan Pinto, Daniel Moreira, Aparna Bharati, Joel Brogan, Kevin Bowyer, Patrick Flynn, Walter Scheirer, Anderson Rocha category:cs.IR cs.CV cs.MM  published:2017-06-01 summary:Departing from traditional digital forensics modeling, which seeks to analyze single objects in isolation, multimedia phylogeny analyzes the evolutionary processes that influence digital objects and collections over time. One of its integral pieces is provenance filtering, which consists of searching a potentially large pool of objects for the most related ones with respect to a given query, in terms of possible ancestors (donors or contributors) and descendants. In this paper, we propose a two-tiered provenance filtering approach to find all the potential images that might have contributed to the creation process of a given query $q$. In our solution, the first (coarse) tier aims to find the most likely "host" images --- the major donor or background --- contributing to a composite/doctored image. The search is then refined in the second tier, in which we search for more specific (potentially small) parts of the query that might have been extracted from other images and spliced into the query image. Experimental results with a dataset containing more than a million images show that the two-tiered solution underpinned by the context of the query is highly useful for solving this difficult task. version:1
arxiv-1706-00439 | Tensor Contraction Layers for Parsimonious Deep Nets | http://arxiv.org/abs/1706.00439 | id:1706.00439 author:Jean Kossaifi, Aran Khanna, Zachary C. Lipton, Tommaso Furlanello, Anima Anandkumar category:cs.LG  published:2017-06-01 summary:Tensors offer a natural representation for many kinds of data frequently encountered in machine learning. Images, for example, are naturally represented as third order tensors, where the modes correspond to height, width, and channels. Tensor methods are noted for their ability to discover multi-dimensional dependencies, and tensor decompositions in particular, have been used to produce compact low-rank approximations of data. In this paper, we explore the use of tensor contractions as neural network layers and investigate several ways to apply them to activation tensors. Specifically, we propose the Tensor Contraction Layer (TCL), the first attempt to incorporate tensor contractions as end-to-end trainable neural network layers. Applied to existing networks, TCLs reduce the dimensionality of the activation tensors and thus the number of model parameters. We evaluate the TCL on the task of image recognition, augmenting two popular networks (AlexNet, VGG). The resulting models are trainable end-to-end. Applying the TCL to the task of image recognition, using the CIFAR100 and ImageNet datasets, we evaluate the effect of parameter reduction via tensor contraction on performance. We demonstrate significant model compression without significant impact on the accuracy and, in some cases, improved performance. version:1
arxiv-1706-00409 | Fader Networks: Manipulating Images by Sliding Attributes | http://arxiv.org/abs/1706.00409 | id:1706.00409 author:Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic Denoyer, Marc'Aurelio Ranzato category:cs.CV  published:2017-06-01 summary:This paper introduces a new encoder-decoder architecture that is trained to reconstruct images by disentangling the salient information of the image and the values of attributes directly in the latent space. As a result, after training, our model can generate different realistic versions of an input image by varying the attribute values. By using continuous attribute values, we can choose how much a specific attribute is perceivable in the generated image. This property could allow for applications where users can modify an image using sliding knobs, like faders on a mixing console, to change the facial expression of a portrait, or to update the color of some objects. Compared to the state-of-the-art which mostly relies on training adversarial networks in pixel space by altering attribute values at train time, our approach results in much simpler training schemes and nicely scales to multiple attributes. We present evidence that our model can significantly change the perceived value of the attributes while preserving the naturalness of images. version:1
arxiv-1706-00400 | Learning Disentangled Representations with Semi-Supervised Deep Generative Models | http://arxiv.org/abs/1706.00400 | id:1706.00400 author:N. Siddharth, Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, Frank Wood, Noah D. Goodman, Pushmeet Kohli, Philip H. S. Torr category:stat.ML cs.AI cs.LG  published:2017-06-01 summary:Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalize from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets. version:1
arxiv-1706-00388 | DiracNets: Training Very Deep Neural Networks Without Skip-Connections | http://arxiv.org/abs/1706.00388 | id:1706.00388 author:Sergey Zagoruyko, Nikos Komodakis category:cs.CV  published:2017-06-01 summary:Deep neural networks with skip-connections, such as ResNet, show excellent performance in various image classification benchmarks. It is though observed that the initial motivation behind them - training deeper networks - does not actually hold true, and the benefits come from increased capacity, rather than from depth. Motivated by this, and inspired from ResNet, we propose a simple Dirac weight parameterization, which allows us to train very deep plain networks without skip-connections, and achieve nearly the same performance. This parameterization has a minor computational cost at training time and no cost at all at inference. We're able to achieve 95.5% accuracy on CIFAR-10 with 34-layer deep plain network, surpassing 1001-layer deep ResNet, and approaching Wide ResNet. Our parameterization also mostly eliminates the need of careful initialization in residual and non-residual networks. The code and models for our experiments are available at https://github.com/szagoruyko/diracnets version:1
arxiv-1706-00387 | Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning | http://arxiv.org/abs/1706.00387 | id:1706.00387 author:Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, Bernhard Schölkopf, Sergey Levine category:cs.LG cs.AI cs.RO  published:2017-06-01 summary:Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks. version:1
arxiv-1706-00384 | Deep Mutual Learning | http://arxiv.org/abs/1706.00384 | id:1706.00384 author:Ying Zhang, Tao Xiang, Timothy M. Hospedales, Huchuan Lu category:cs.CV  published:2017-06-01 summary:Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, that is better suited to low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy where, rather than one way transfer between a static pre-defined teacher and a student, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on CIFAR-100 recognition and Market-1501 person re-identification benchmarks. Surprisingly, it is revealed that no prior powerful teacher network is necessary -- mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher. version:1
arxiv-1706-00382 | Blind nonnegative source separation using biological neural networks | http://arxiv.org/abs/1706.00382 | id:1706.00382 author:Cengiz Pehlevan, Sreyas Mohan, Dmitri B. Chklovskii category:q-bio.NC cs.NE  published:2017-06-01 summary:Blind source separation, i.e. extraction of independent sources from a mixture, is an important problem for both artificial and natural signal processing. Here, we address a special case of this problem when sources (but not the mixing matrix) are known to be nonnegative, for example, due to the physical nature of the sources. We search for the solution to this problem that can be implemented using biologically plausible neural networks. Specifically, we consider the online setting where the dataset is streamed to a neural network. The novelty of our approach is that we formulate blind nonnegative source separation as a similarity matching problem and derive neural networks from the similarity matching objective. Importantly, synaptic weights in our networks are updated according to biologically plausible local learning rules. version:1
arxiv-1705-06920 | Hyperspectral Band Selection Using Unsupervised Non-Linear Deep Auto Encoder to Train External Classifiers | http://arxiv.org/abs/1705.06920 | id:1705.06920 author:Muhammad Ahmad, Stanislav Protasov, Adil Mehmood Khan category:cs.CV  published:2017-05-19 summary:In order to make hyperspectral image classification compu- tationally tractable, it is often necessary to select the most informative bands instead to process the whole data without losing the geometrical representation of original data. To cope with said issue, an improved un- supervised non-linear deep auto encoder (UDAE) based band selection method is proposed. The proposed UDAE is able to select the most infor- mative bands in such a way that preserve the key information but in the lower dimensions, where the hidden representation is a non-linear trans- formation that maps the original space to a space of lower dimensions. This work emphasizes to analyze what type of information is needed to preserve the hierarchical UDAE representation while selecting a sub- set from original space. Our experiments on publically available hyper- spectral dataset demonstrate the effectiveness of UDAE method, which equates favorably with other state-of-the-art methods. version:2
