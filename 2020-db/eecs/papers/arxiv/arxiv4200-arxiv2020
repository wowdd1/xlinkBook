arxiv-1312-1462 | Geometric Feature Based Face-Sketch Recognition | http://arxiv.org/abs/1312.1462 | id:1312.1462 author:Sourav Pramanik, Debotosh Bhattacharjee category:cs.CV  published:2013-12-05 summary:This paper presents a novel facial sketch image or face-sketch recognition approach based on facial feature extraction. To recognize a face-sketch, we have concentrated on a set of geometric face features like eyes, nose, eyebrows, lips, etc and their length and width ratio because it is difficult to match photos and sketches because they belong to two different modalities. In this system, first the facial features/components from training images are extracted, then ratios of length, width, and area etc. are calculated and those are stored as feature vectors for individual images. After that the mean feature vectors are computed and subtracted from each feature vector for centering of the feature vectors. In the next phase, feature vector for the incoming probe face-sketch is also computed in similar fashion. Here, K-NN classifier is used to recognize probe face-sketch. It is experimentally verified that the proposed method is robust against faces are in a frontal pose, with normal lighting and neutral expression and have no occlusions. The experiment has been conducted with 80 male and female face images from different face databases. It has useful applications for both law enforcement and digital entertainment. version:1
arxiv-1312-1461 | Multi-Sensor Image Fusion Based on Moment Calculation | http://arxiv.org/abs/1312.1461 | id:1312.1461 author:Sourav Pramanik, Debotosh Bhattacharjee category:cs.CV  published:2013-12-05 summary:An image fusion method based on salient features is proposed in this paper. In this work, we have concentrated on salient features of the image for fusion in order to preserve all relevant information contained in the input images and tried to enhance the contrast in fused image and also suppressed noise to a maximum extent. In our system, first we have applied a mask on two input images in order to conserve the high frequency information along with some low frequency information and stifle noise to a maximum extent. Thereafter, for identification of salience features from sources images, a local moment is computed in the neighborhood of a coefficient. Finally, a decision map is generated based on local moment in order to get the fused image. To verify our proposed algorithm, we have tested it on 120 sensor image pairs collected from Manchester University UK database. The experimental results show that the proposed method can provide superior fused image in terms of several quantitative fusion evaluation index. version:1
arxiv-1312-1681 | An Approach: Modality Reduction and Face-Sketch Recognition | http://arxiv.org/abs/1312.1681 | id:1312.1681 author:Sourav Pramanik, Dr. Debotosh Bhattacharjee category:cs.CV  published:2013-12-05 summary:To recognize face sketch through face photo database is a challenging task for todays researchers. Because face photo images in training set and face sketch images in testing set have different modality. Difference between two face photos of difference person is smaller than the difference between same person in a face photo and face sketched. In this paper, for reduction of the modality between face photo and face sketch we first bring face photo and face sketch images in a new dimension using 2D Discrete Haar wavelet transform with scale 3 followed by a negative approach. After that, extract features from transformed images using Principal Component Analysis (PCA). Thereafter, we use SVM classifier and K-NN classifier for better classification. Our proposed method is experimentally verified by its robustness against faces that are captured in a good lighting condition and in a frontal pose. The experiment has been conducted with 100 male and female face images as training set and 100 male and female face sketch images as testing set collected from CUHK training and testing cropped photos and CUHK training and testing cropped sketches. version:1
arxiv-1312-1423 | ABC-SG: A New Artificial Bee Colony Algorithm-Based Distance of Sequential Data Using Sigma Grams | http://arxiv.org/abs/1312.1423 | id:1312.1423 author:Muhammad Marwan Muhammad Fuad category:cs.NE cs.AI  published:2013-12-05 summary:The problem of similarity search is one of the main problems in computer science. This problem has many applications in text-retrieval, web search, computational biology, bioinformatics and others. Similarity between two data objects can be depicted using a similarity measure or a distance metric. There are numerous distance metrics in the literature, some are used for a particular data type, and others are more general. In this paper we present a new distance metric for sequential data which is based on the sum of n-grams. The novelty of our distance is that these n-grams are weighted using artificial bee colony; a recent optimization algorithm based on the collective intelligence of a swarm of bees on their search for nectar. This algorithm has been used in optimizing a large number of numerical problems. We validate the new distance experimentally. version:1
arxiv-1312-1244 | Chebushev Greedy Algorithm in convex optimization | http://arxiv.org/abs/1312.1244 | id:1312.1244 author:Vladimir Temlyakov category:stat.ML math.OC  published:2013-12-04 summary:Chebyshev Greedy Algorithm is a generalization of the well known Orthogonal Matching Pursuit defined in a Hilbert space to the case of Banach spaces. We apply this algorithm for constructing sparse approximate solutions (with respect to a given dictionary) to convex optimization problems. Rate of convergence results in a style of the Lebesgue-type inequalities are proved. version:1
arxiv-1312-1121 | Interpreting random forest classification models using a feature contribution method | http://arxiv.org/abs/1312.1121 | id:1312.1121 author:Anna Palczewska, Jan Palczewski, Richard Marchese Robinson, Daniel Neagu category:cs.LG I.5.2; I.2.1; J.3  published:2013-12-04 summary:Model interpretation is one of the key aspects of the model evaluation process. The explanation of the relationship between model variables and outputs is relatively easy for statistical models, such as linear regressions, thanks to the availability of model parameters and their statistical significance. For "black box" models, such as random forest, this information is hidden inside the model structure. This work presents an approach for computing feature contributions for random forest classification models. It allows for the determination of the influence of each variable on the model prediction for an individual instance. By analysing feature contributions for a training dataset, the most significant variables can be determined and their typical contribution towards predictions made for individual classes, i.e., class-specific feature contribution "patterns", are discovered. These patterns represent a standard behaviour of the model and allow for an additional assessment of the model reliability for a new data. Interpretation of feature contributions for two UCI benchmark datasets shows the potential of the proposed methodology. The robustness of results is demonstrated through an extensive analysis of feature contributions calculated for a large number of generated random forest models. version:1
arxiv-1312-1099 | Multiscale Dictionary Learning for Estimating Conditional Distributions | http://arxiv.org/abs/1312.1099 | id:1312.1099 author:Francesca Petralia, Joshua Vogelstein, David B. Dunson category:stat.ML cs.LG  published:2013-12-04 summary:Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner. The algorithm scales efficiently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features. version:1
arxiv-1211-1041 | Algorithms and Hardness for Robust Subspace Recovery | http://arxiv.org/abs/1211.1041 | id:1211.1041 author:Moritz Hardt, Ankur Moitra category:cs.CC cs.DS cs.IT cs.LG math.IT  published:2012-11-05 summary:We consider a fundamental problem in unsupervised learning called \emph{subspace recovery}: given a collection of $m$ points in $\mathbb{R}^n$, if many but not necessarily all of these points are contained in a $d$-dimensional subspace $T$ can we find it? The points contained in $T$ are called {\em inliers} and the remaining points are {\em outliers}. This problem has received considerable attention in computer science and in statistics. Yet efficient algorithms from computer science are not robust to {\em adversarial} outliers, and the estimators from robust statistics are hard to compute in high dimensions. Are there algorithms for subspace recovery that are both robust to outliers and efficient? We give an algorithm that finds $T$ when it contains more than a $\frac{d}{n}$ fraction of the points. Hence, for say $d = n/2$ this estimator is both easy to compute and well-behaved when there are a constant fraction of outliers. We prove that it is Small Set Expansion hard to find $T$ when the fraction of errors is any larger, thus giving evidence that our estimator is an {\em optimal} compromise between efficiency and robustness. As it turns out, this basic problem has a surprising number of connections to other areas including small set expansion, matroid theory and functional analysis that we make use of here. version:3
arxiv-1308-3925 | Distance Correlation Methods for Discovering Associations in Large Astrophysical Databases | http://arxiv.org/abs/1308.3925 | id:1308.3925 author:Elizabeth Martinez-Gomez, Mercedes T. Richards, Donald St. P. Richards category:astro-ph.CO math.ST stat.AP stat.ML stat.TH  published:2013-08-19 summary:High-dimensional, large-sample astrophysical databases of galaxy clusters, such as the Chandra Deep Field South COMBO-17 database, provide measurements on many variables for thousands of galaxies and a range of redshifts. Current understanding of galaxy formation and evolution rests sensitively on relationships between different astrophysical variables; hence an ability to detect and verify associations or correlations between variables is important in astrophysical research. In this paper, we apply a recently defined statistical measure called the distance correlation coefficient which can be used to identify new associations and correlations between astrophysical variables. The distance correlation coefficient applies to variables of any dimension; it can be used to determine smaller sets of variables that provide equivalent astrophysical information; it is zero only when variables are independent; and it is capable of detecting nonlinear associations that are undetectable by the classical Pearson correlation coefficient. Hence, the distance correlation coefficient provides more information than the Pearson coefficient. We analyze numerous pairs of variables in the COMBO-17 database with the distance correlation method and with the maximal information coefficient. We show that the Pearson coefficient can be estimated with higher accuracy from the corresponding distance correlation coefficient than from the maximal information coefficient. For given values of the Pearson coefficient, the distance correlation method has a greater ability than the maximal information coefficient to resolve astrophysical data into highly concentrated V-shapes, which enhances classification and pattern identification. These results are observed over a range of redshifts beyond the local universe and for galaxies from elliptical to spiral. version:2
arxiv-1311-7071 | Sparse Linear Dynamical System with Its Application in Multivariate Clinical Time Series | http://arxiv.org/abs/1311.7071 | id:1311.7071 author:Zitao Liu, Milos Hauskrecht category:cs.AI cs.LG stat.ML  published:2013-11-27 summary:Linear Dynamical System (LDS) is an elegant mathematical framework for modeling and learning multivariate time series. However, in general, it is difficult to set the dimension of its hidden state space. A small number of hidden states may not be able to model the complexities of a time series, while a large number of hidden states can lead to overfitting. In this paper, we study methods that impose an $\ell_1$ regularization on the transition matrix of an LDS model to alleviate the problem of choosing the optimal number of hidden states. We incorporate a generalized gradient descent method into the Maximum a Posteriori (MAP) framework and use Expectation Maximization (EM) to iteratively achieve sparsity on the transition matrix of an LDS model. We show that our Sparse Linear Dynamical System (SLDS) improves the predictive performance when compared to ordinary LDS on a multivariate clinical time series dataset. version:2
arxiv-1312-0852 | Feature Extraction of Human Lip Prints | http://arxiv.org/abs/1312.0852 | id:1312.0852 author:Samir Kumar Bandyopadhyay, S Arunkumar, Saptarshi Bhattacharjee category:cs.CV  published:2013-12-03 summary:Methods have been used for identification of human by recognizing lip prints. Human lips have a number of elevation and depressions features called lip prints and examination of lip prints is referred to as cheiloscopy. Lip prints of each human being are unique in nature like many others features of human. In this paper lip print is first smoothened using a Gaussian Filter. Next Sobel Edge Detector and Canny Edge Detector are used to detect the vertical and horizontal groove pattern in the lip. This method of identification will be useful both in criminal forensics and personal identification. It is our assumption that study of lip prints and their types are well connected to play a song in a better way that are well accepted to people who loves to hear songs. version:1
arxiv-1312-0940 | Medical Aid for Automatic Detection of Malaria | http://arxiv.org/abs/1312.0940 | id:1312.0940 author:Pramit Ghosh, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CY cs.CV  published:2013-12-03 summary:The analysis and counting of blood cells in a microscope image can provide useful information concerning to the health of a person. In particular, morphological analysis of red blood cells deformations can effectively detect important disease like malaria. Blood images, obtained by the microscope, which is coupled with a digital camera, are analyzed by the computer for diagnosis or can be transmitted easily to clinical centers than liquid blood samples. Automatic analysis system for the presence of Plasmodium in microscopic image of blood can greatly help pathologists and doctors that typically inspect blood films manually. Unfortunately, the analysis made by human experts is not rapid and not yet standardized due to the operators capabilities and tiredness. The paper shows how effectively and accurately it is possible to identify the Plasmodium in the blood film. In particular, the paper presents how to enhance the microscopic image and filter out the unnecessary segments followed by the threshold based segmentation and recognize the presence of Plasmodium. The proposed system can be deployed in the remote area as a supporting aid for telemedicine technology and only basic training is sufficient to operate it. This system achieved more than 98 percentage accuracy for the samples collected to test this system. version:1
arxiv-1312-0809 | Automatic White Blood Cell Measuring Aid for Medical Diagnosis | http://arxiv.org/abs/1312.0809 | id:1312.0809 author:Pramit Ghosh, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CY cs.CV  published:2013-12-03 summary:Blood related invasive pathological investigations play a major role in diagnosis of diseases. But in India and other third world countries there are no enough pathological infrastructures for medical diagnosis. Moreover, most of the remote places of those countries have neither pathologists nor physicians. Telemedicine partially solves the lack of physicians. But the pathological investigation infrastructure can not be integrated with the telemedicine technology. The objective of this work is to automate the blood related pathological investigation process. Detection of different white blood cells has been automated in this work. This system can be deployed in the remote area as a supporting aid for telemedicine technology and only high school education is sufficient to operate it. The proposed system achieved 97.33 percent accuracy for the samples collected to test this system. version:1
arxiv-1312-0760 | Template-Based Active Contours | http://arxiv.org/abs/1312.0760 | id:1312.0760 author:Jayanth Krishna Mogali, Adithya Kumar Pediredla, Chandra Sekhar Seelamantula category:cs.CV  published:2013-12-03 summary:We develop a generalized active contour formalism for image segmentation based on shape templates. The shape template is subjected to a restricted affine transformation (RAT) in order to segment the object of interest. RAT allows for translation, rotation, and scaling, which give a total of five degrees of freedom. The proposed active contour comprises an inner and outer contour pair, which are closed and concentric. The active contour energy is a contrast function defined based on the intensities of pixels that lie inside the inner contour and those that lie in the annulus between the inner and outer contours. We show that the contrast energy functional is optimal under certain conditions. The optimal RAT parameters are computed by maximizing the contrast function using a gradient descent optimizer. We show that the calculations are made efficient through use of Green's theorem. The proposed formalism is capable of handling a variety of shapes because for a chosen template, optimization is carried with respect to the RAT parameters only. The proposed formalism is validated on multiple images to show robustness to Gaussian and Poisson noise, to initialization, and to partial loss of structure in the object to be segmented. version:1
arxiv-1312-0631 | Phase Transitions in Community Detection: A Solvable Toy Model | http://arxiv.org/abs/1312.0631 | id:1312.0631 author:Greg Ver Steeg, Cristopher Moore, Aram Galstyan, Armen E. Allahverdyan category:cs.SI cond-mat.stat-mech physics.soc-ph stat.ML  published:2013-12-02 summary:Recently, it was shown that there is a phase transition in the community detection problem. This transition was first computed using the cavity method, and has been proved rigorously in the case of $q=2$ groups. However, analytic calculations using the cavity method are challenging since they require us to understand probability distributions of messages. We study analogous transitions in so-called "zero-temperature inference" model, where this distribution is supported only on the most-likely messages. Furthermore, whenever several messages are equally likely, we break the tie by choosing among them with equal probability. While the resulting analysis does not give the correct values of the thresholds, it does reproduce some of the qualitative features of the system. It predicts a first-order detectability transition whenever $q > 2$, while the finite-temperature cavity method shows that this is the case only when $q > 4$. It also has a regime analogous to the "hard but detectable" phase, where the community structure can be partially recovered, but only when the initial messages are sufficiently accurate. Finally, we study a semisupervised setting where we are given the correct labels for a fraction $\rho$ of the nodes. For $q > 2$, we find a regime where the accuracy jumps discontinuously at a critical value of $\rho$. version:1
arxiv-1312-0579 | SpeedMachines: Anytime Structured Prediction | http://arxiv.org/abs/1312.0579 | id:1312.0579 author:Alexander Grubb, Daniel Munoz, J. Andrew Bagnell, Martial Hebert category:cs.LG  published:2013-12-02 summary:Structured prediction plays a central role in machine learning applications from computational biology to computer vision. These models require significantly more computation than unstructured models, and, in many applications, algorithms may need to make predictions within a computational budget or in an anytime fashion. In this work we propose an anytime technique for learning structured prediction that, at training time, incorporates both structural elements and feature computation trade-offs that affect test-time inference. We apply our technique to the challenging problem of scene understanding in computer vision and demonstrate efficient and anytime predictions that gradually improve towards state-of-the-art classification performance as the allotted time increases. version:1
arxiv-1304-4910 | A Junction Tree Framework for Undirected Graphical Model Selection | http://arxiv.org/abs/1304.4910 | id:1304.4910 author:Divyanshu Vats, Robert Nowak category:stat.ML cs.AI cs.IT math.IT  published:2013-04-17 summary:An undirected graphical model is a joint probability distribution defined on an undirected graph G*, where the vertices in the graph index a collection of random variables and the edges encode conditional independence relationships among random variables. The undirected graphical model selection (UGMS) problem is to estimate the graph G* given observations drawn from the undirected graphical model. This paper proposes a framework for decomposing the UGMS problem into multiple subproblems over clusters and subsets of the separators in a junction tree. The junction tree is constructed using a graph that contains a superset of the edges in G*. We highlight three main properties of using junction trees for UGMS. First, different regularization parameters or different UGMS algorithms can be used to learn different parts of the graph. This is possible since the subproblems we identify can be solved independently of each other. Second, under certain conditions, a junction tree based UGMS algorithm can produce consistent results with fewer observations than the usual requirements of existing algorithms. Third, both our theoretical and experimental results show that the junction tree framework does a significantly better job at finding the weakest edges in a graph than existing methods. This property is a consequence of both the first and second properties. Finally, we note that our framework is independent of the choice of the UGMS algorithm and can be used as a wrapper around standard UGMS algorithms for more accurate graph estimation. version:2
arxiv-1312-0518 | Families of Parsimonious Finite Mixtures of Regression Models | http://arxiv.org/abs/1312.0518 | id:1312.0518 author:Utkarsh J. Dang, Paul D. McNicholas category:stat.ME stat.CO stat.ML  published:2013-12-02 summary:Finite mixtures of regression models offer a flexible framework for investigating heterogeneity in data with functional dependencies. These models can be conveniently used for unsupervised learning on data with clear regression relationships. We extend such models by imposing an eigen-decomposition on the multivariate error covariance matrix. By constraining parts of this decomposition, we obtain families of parsimonious mixtures of regressions and mixtures of regressions with concomitant variables. These families of models account for correlations between multiple responses. An expectation-maximization algorithm is presented for parameter estimation and performance is illustrated on simulated and real data. version:1
arxiv-1208-2043 | High-Dimensional Screening Using Multiple Grouping of Variables | http://arxiv.org/abs/1208.2043 | id:1208.2043 author:Divyanshu Vats category:stat.ML cs.IT math.IT  published:2012-08-09 summary:Screening is the problem of finding a superset of the set of non-zero entries in an unknown p-dimensional vector \beta* given n noisy observations. Naturally, we want this superset to be as small as possible. We propose a novel framework for screening, which we refer to as Multiple Grouping (MuG), that groups variables, performs variable selection over the groups, and repeats this process multiple number of times to estimate a sequence of sets that contains the non-zero entries in \beta*. Screening is done by taking an intersection of all these estimated sets. The MuG framework can be used in conjunction with any group based variable selection algorithm. In the high-dimensional setting, where p >> n, we show that when MuG is used with the group Lasso estimator, screening can be consistently performed without using any tuning parameter. Our numerical simulations clearly show the merits of using the MuG framework in practice. version:3
arxiv-1312-0493 | Bidirectional Recursive Neural Networks for Token-Level Labeling with Structure | http://arxiv.org/abs/1312.0493 | id:1312.0493 author:Ozan İrsoy, Claire Cardie category:cs.LG cs.CL stat.ML  published:2013-12-02 summary:Recently, deep architectures, such as recurrent and recursive neural networks have been successfully applied to various natural language processing tasks. Inspired by bidirectional recurrent neural networks which use representations that summarize the past and future around an instance, we propose a novel architecture that aims to capture the structural information around an input, and use it to label instances. We apply our method to the task of opinion expression extraction, where we employ the binary parse tree of a sentence as the structure, and word vector representations as the initial representation of a single token. We conduct preliminary experiments to investigate its performance and compare it to the sequential approach. version:1
arxiv-1312-0485 | Precise Semidefinite Programming Formulation of Atomic Norm Minimization for Recovering d-Dimensional ($d\geq 2$) Off-the-Grid Frequencies | http://arxiv.org/abs/1312.0485 | id:1312.0485 author:Weiyu Xu, Jian-Feng Cai, Kumar Vijay Mishra, Myung Cho, Anton Kruger category:cs.IT math.IT math.OC stat.ML  published:2013-12-02 summary:Recent research in off-the-grid compressed sensing (CS) has demonstrated that, under certain conditions, one can successfully recover a spectrally sparse signal from a few time-domain samples even though the dictionary is continuous. In particular, atomic norm minimization was proposed in \cite{tang2012csotg} to recover $1$-dimensional spectrally sparse signal. However, in spite of existing research efforts \cite{chi2013compressive}, it was still an open problem how to formulate an equivalent positive semidefinite program for atomic norm minimization in recovering signals with $d$-dimensional ($d\geq 2$) off-the-grid frequencies. In this paper, we settle this problem by proposing equivalent semidefinite programming formulations of atomic norm minimization to recover signals with $d$-dimensional ($d\geq 2$) off-the-grid frequencies. version:1
arxiv-1312-0412 | Practical Collapsed Stochastic Variational Inference for the HDP | http://arxiv.org/abs/1312.0412 | id:1312.0412 author:Arnim Bleier category:cs.LG  published:2013-12-02 summary:Recent advances have made it feasible to apply the stochastic variational paradigm to a collapsed representation of latent Dirichlet allocation (LDA). While the stochastic variational paradigm has successfully been applied to an uncollapsed representation of the hierarchical Dirichlet process (HDP), no attempts to apply this type of inference in a collapsed setting of non-parametric topic modeling have been put forward so far. In this paper we explore such a collapsed stochastic variational Bayes inference for the HDP. The proposed online algorithm is easy to implement and accounts for the inference of hyper-parameters. First experiments show a promising improvement in predictive performance. version:1
arxiv-1312-0335 | Inferring Regulatory Networks by Combining Perturbation Screens and Steady State Gene Expression Profiles | http://arxiv.org/abs/1312.0335 | id:1312.0335 author:Ali Shojaie, Alexandra Jauhiainen, Michael Kallitsis, George Michailidis category:stat.ML q-bio.MN  published:2013-12-02 summary:Reconstructing transcriptional regulatory networks is an important task in functional genomics. Data obtained from experiments that perturb genes by knockouts or RNA interference contain useful information for addressing this reconstruction problem. However, such data can be limited in size and/or are expensive to acquire. On the other hand, observational data of the organism in steady state (e.g. wild-type) are more readily available, but their informational content is inadequate for the task at hand. We develop a computational approach to appropriately utilize both data sources for estimating a regulatory network. The proposed approach is based on a three-step algorithm to estimate the underlying directed but cyclic network, that uses as input both perturbation screens and steady state gene expression data. In the first step, the algorithm determines causal orderings of the genes that are consistent with the perturbation data, by combining an exhaustive search method with a fast heuristic that in turn couples a Monte Carlo technique with a fast search algorithm. In the second step, for each obtained causal ordering, a regulatory network is estimated using a penalized likelihood based method, while in the third step a consensus network is constructed from the highest scored ones. Extensive computational experiments show that the algorithm performs well in reconstructing the underlying network and clearly outperforms competing approaches that rely only on a single data source. Further, it is established that the algorithm produces a consistent estimate of the regulatory network. version:1
arxiv-1307-6962 | Reduced egomotion estimation drift using omnidirectional views | http://arxiv.org/abs/1307.6962 | id:1307.6962 author:Yalin Bastanlar category:cs.CV cs.RO  published:2013-07-26 summary:Estimation of camera motion from a given image sequence becomes degraded as the length of the sequence increases. In this letter, this phenomenon is demonstrated and an approach to increase the estimation accuracy is proposed. The proposed method uses an omnidirectional camera in addition to the perspective one and takes advantage of its enlarged view by exploiting the correspondences between the omnidirectional and perspective images. Simulated and real image experiments show that the proposed approach improves the estimation accuracy. version:2
arxiv-1308-0371 | Sparse arrays of signatures for online character recognition | http://arxiv.org/abs/1308.0371 | id:1308.0371 author:Benjamin Graham category:cs.CV cs.NE  published:2013-08-01 summary:In mathematics the signature of a path is a collection of iterated integrals, commonly used for solving differential equations. We show that the path signature, used as a set of features for consumption by a convolutional neural network (CNN), improves the accuracy of online character recognition---that is the task of reading characters represented as a collection of paths. Using datasets of letters, numbers, Assamese and Chinese characters, we show that the first, second, and even the third iterated integrals contain useful information for consumption by a CNN. On the CASIA-OLHWDB1.1 3755 Chinese character dataset, our approach gave a test error of 3.58%, compared with 5.61% for a traditional CNN [Ciresan et al.]. A CNN trained on the CASIA-OLHWDB1.0-1.2 datasets won the ICDAR2013 Online Isolated Chinese Character recognition competition. Computationally, we have developed a sparse CNN implementation that make it practical to train CNNs with many layers of max-pooling. Extending the MNIST dataset by translations, our sparse CNN gets a test error of 0.31%. version:2
arxiv-1311-6041 | No Free Lunch Theorem and Bayesian probability theory: two sides of the same coin. Some implications for black-box optimization and metaheuristics | http://arxiv.org/abs/1311.6041 | id:1311.6041 author:Loris Serafino category:cs.LG  published:2013-11-23 summary:Challenging optimization problems, which elude acceptable solution via conventional calculus methods, arise commonly in different areas of industrial design and practice. Hard optimization problems are those who manifest the following behavior: a) high number of independent input variables; b) very complex or irregular multi-modal fitness; c) computational expensive fitness evaluation. This paper will focus on some theoretical issues that have strong implications for practice. I will stress how an interpretation of the No Free Lunch theorem leads naturally to a general Bayesian optimization framework. The choice of a prior over the space of functions is a critical and inevitable step in every black-box optimization. version:3
arxiv-1312-0162 | A Typology of Collaboration Platform Users | http://arxiv.org/abs/1312.0162 | id:1312.0162 author:Anastasia Bezzubtseva, Dmitry I. Ignatov category:cs.CY cs.HC cs.SI stat.ML 68U35  91D30 K.4.3  published:2013-11-30 summary:In this paper we present a review of the existing typologies of Internet service users. We zoom in on social networking services including blogs and crowdsourcing websites. Based on the results of the analysis of the considered typologies obtained by means of FCA we developed a new user typology of a certain class of Internet services, namely a collaboration innovation platform. Cluster analysis of data extracted from the collaboration platform Witology was used to divide more than 500 participants into six groups based on three activity indicators: idea generation, commenting, and evaluation (assigning marks) The obtained groups and their percentages appear to follow the "90 - 9 - 1" rule. version:1
arxiv-1308-5038 | Group-Sparse Signal Denoising: Non-Convex Regularization, Convex Optimization | http://arxiv.org/abs/1308.5038 | id:1308.5038 author:Po-Yu Chen, Ivan W. Selesnick category:cs.CV cs.LG stat.ML  published:2013-08-23 summary:Convex optimization with sparsity-promoting convex regularization is a standard approach for estimating sparse signals in noise. In order to promote sparsity more strongly than convex regularization, it is also standard practice to employ non-convex optimization. In this paper, we take a third approach. We utilize a non-convex regularization term chosen such that the total cost function (consisting of data consistency and regularization terms) is convex. Therefore, sparsity is more strongly promoted than in the standard convex formulation, but without sacrificing the attractive aspects of convex optimization (unique minimum, robust algorithms, etc.). We use this idea to improve the recently developed 'overlapping group shrinkage' (OGS) algorithm for the denoising of group-sparse signals. The algorithm is applied to the problem of speech enhancement with favorable results in terms of both SNR and perceptual quality. version:2
arxiv-1312-0072 | Improving Texture Categorization with Biologically Inspired Filtering | http://arxiv.org/abs/1312.0072 | id:1312.0072 author:Ngoc-Son Vu, Thanh Phuong Nguyen, Christophe Garcia category:cs.CV  published:2013-11-30 summary:Within the domain of texture classification, a lot of effort has been spent on local descriptors, leading to many powerful algorithms. However, preprocessing techniques have received much less attention despite their important potential for improving the overall classification performance. We address this question by proposing a novel, simple, yet very powerful biologically-inspired filtering (BF) which simulates the performance of human retina. In the proposed approach, given a texture image, after applying a DoG filter to detect the "edges", we first split the filtered image into two "maps" alongside the sides of its edges. The feature extraction step is then carried out on the two "maps" instead of the input image. Our algorithm has several advantages such as simplicity, robustness to illumination and noise, and discriminative power. Experimental results on three large texture databases show that with an extremely low computational cost, the proposed method improves significantly the performance of many texture classification systems, notably in noisy environments. The source codes of the proposed algorithm can be downloaded from https://sites.google.com/site/nsonvu/code. version:1
arxiv-1312-0049 | One-Class Classification: Taxonomy of Study and Review of Techniques | http://arxiv.org/abs/1312.0049 | id:1312.0049 author:Shehroz S. Khan, Michael G. Madden category:cs.LG cs.AI  published:2013-11-30 summary:One-class classification (OCC) algorithms aim to build classification models when the negative class is either absent, poorly sampled or not well defined. This unique situation constrains the learning of efficient classifiers by defining class boundary just with the knowledge of positive class. The OCC problem has been considered and applied under many research themes, such as outlier/novelty detection and concept learning. In this paper we present a unified view of the general problem of OCC by presenting a taxonomy of study for OCC problems, which is based on the availability of training data, algorithms used and the application domains applied. We further delve into each of the categories of the proposed taxonomy and present a comprehensive literature review of the OCC algorithms, techniques and methodologies with a focus on their significance, limitations and applications. We conclude our paper by discussing some open research problems in the field of OCC and present our vision for future research. version:1
arxiv-1312-0048 | Stochastic Optimization of Smooth Loss | http://arxiv.org/abs/1312.0048 | id:1312.0048 author:Rong Jin category:cs.LG  published:2013-11-30 summary:In this paper, we first prove a high probability bound rather than an expectation bound for stochastic optimization with smooth loss. Furthermore, the existing analysis requires the knowledge of optimal classifier for tuning the step size in order to achieve the desired bound. However, this information is usually not accessible in advanced. We also propose a strategy to address the limitation. version:1
arxiv-1311-7679 | Combination of Diverse Ranking Models for Personalized Expedia Hotel Searches | http://arxiv.org/abs/1311.7679 | id:1311.7679 author:Xudong Liu, Bing Xu, Yuyu Zhang, Qiang Yan, Liang Pang, Qiang Li, Hanxiao Sun, Bin Wang category:cs.LG  published:2013-11-29 summary:The ICDM Challenge 2013 is to apply machine learning to the problem of hotel ranking, aiming to maximize purchases according to given hotel characteristics, location attractiveness of hotels, user's aggregated purchase history and competitive online travel agency information for each potential hotel choice. This paper describes the solution of team "binghsu & MLRush & BrickMover". We conduct simple feature engineering work and train different models by each individual team member. Afterwards, we use listwise ensemble method to combine each model's output. Besides describing effective model and features, we will discuss about the lessons we learned while using deep learning in this competition. version:1
arxiv-1311-7662 | The Power of Asymmetry in Binary Hashing | http://arxiv.org/abs/1311.7662 | id:1311.7662 author:Behnam Neyshabur, Payman Yadollahpour, Yury Makarychev, Ruslan Salakhutdinov, Nathan Srebro category:cs.LG cs.CV cs.IR  published:2013-11-29 summary:When approximating binary similarity using the hamming distance between short binary hashes, we show that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between $x$ and $x'$ as the hamming distance between $f(x)$ and $g(x')$, for two distinct binary codes $f,g$, rather than as the hamming distance between $f(x)$ and $f(x')$. version:1
arxiv-1311-7656 | Statistical estimation for optimization problems on graphs | http://arxiv.org/abs/1311.7656 | id:1311.7656 author:Mikhail Langovoy, Suvrit Sra category:stat.ML cs.DM math.OC stat.CO stat.ME  published:2013-11-29 summary:Large graphs abound in machine learning, data mining, and several related areas. A useful step towards analyzing such graphs is that of obtaining certain summary statistics - e.g., or the expected length of a shortest path between two nodes, or the expected weight of a minimum spanning tree of the graph, etc. These statistics provide insight into the structure of a graph, and they can help predict global properties of a graph. Motivated thus, we propose to study statistical properties of structured subgraphs (of a given graph), in particular, to estimate the expected objective function value of a combinatorial optimization problem over these subgraphs. The general task is very difficult, if not unsolvable; so for concreteness we describe a more specific statistical estimation problem based on spanning trees. We hope that our position paper encourages others to also study other types of graphical structures for which one can prove nontrivial statistical estimates. version:1
arxiv-1311-7650 | Adaptive nonparametric detection in cryo-electron microscopy | http://arxiv.org/abs/1311.7650 | id:1311.7650 author:Mikhail Langovoy, Michael Habeck, Bernhard Schoelkopf category:stat.AP stat.ME stat.ML  published:2013-11-29 summary:Cryo-electron microscopy (cryo-EM) is an emerging experimental method to characterize the structure of large biomolecular assemblies. Single particle cryo-EM records 2D images (so-called micrographs) of projections of the three-dimensional particle, which need to be processed to obtain the three-dimensional reconstruction. A crucial step in the reconstruction process is particle picking which involves detection of particles in noisy 2D micrographs with low signal-to-noise ratios of typically 1:10 or even lower. Typically, each picture contains a large number of particles, and particles have unknown irregular and nonconvex shapes. version:1
arxiv-1311-2901 | Visualizing and Understanding Convolutional Networks | http://arxiv.org/abs/1311.2901 | id:1311.2901 author:Matthew D Zeiler, Rob Fergus category:cs.CV  published:2013-11-12 summary:Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets. version:3
arxiv-1311-7401 | Shape from Texture using Locally Scaled Point Processes | http://arxiv.org/abs/1311.7401 | id:1311.7401 author:Eva-Maria Didden, Thordis Linda Thorarinsdottir, Alex Lenkoski, Christoph Schnörr category:stat.AP cs.CV  published:2013-11-28 summary:Shape from texture refers to the extraction of 3D information from 2D images with irregular texture. This paper introduces a statistical framework to learn shape from texture where convex texture elements in a 2D image are represented through a point process. In a first step, the 2D image is preprocessed to generate a probability map corresponding to an estimate of the unnormalized intensity of the latent point process underlying the texture elements. The latent point process is subsequently inferred from the probability map in a non-parametric, model free manner. Finally, the 3D information is extracted from the point pattern by applying a locally scaled point process model where the local scaling function represents the deformation caused by the projection of a 3D surface onto a 2D image. version:1
arxiv-1111-6832 | Gaussian Probabilities and Expectation Propagation | http://arxiv.org/abs/1111.6832 | id:1111.6832 author:John P. Cunningham, Philipp Hennig, Simon Lacoste-Julien category:stat.ML  published:2011-11-29 summary:While Gaussian probability densities are omnipresent in applied mathematics, Gaussian cumulative probabilities are hard to calculate in any but the univariate case. We study the utility of Expectation Propagation (EP) as an approximate integration method for this problem. For rectangular integration regions, the approximation is highly accurate. We also extend the derivations to the more general case of polyhedral integration regions. However, we find that in this polyhedral case, EP's answer, though often accurate, can be almost arbitrarily wrong. We consider these unexpected results empirically and theoretically, both for the problem of Gaussian probabilities and for EP more generally. These results elucidate an interesting and non-obvious feature of EP not yet studied in detail. version:2
arxiv-1311-7327 | Unobtrusive Low Cost Pupil Size Measurements using Web cameras | http://arxiv.org/abs/1311.7327 | id:1311.7327 author:Sergios Petridis, Theodoros Giannakopoulos, Costantine D. Spyropoulos category:cs.CV J.3; I.4.8  published:2013-11-28 summary:Unobtrusive every day health monitoring can be of important use for the elderly population. In particular, pupil size may be a valuable source of information, since, apart from pathological cases, it can reveal the emotional state, the fatigue and the ageing. To allow for unobtrusive monitoring to gain acceptance, one should seek for efficient methods of monitoring using com- mon low-cost hardware. This paper describes a method for monitoring pupil sizes using a common web camera in real time. Our method works by first detecting the face and the eyes area. Subsequently, optimal iris and sclera location and radius, modelled as ellipses, are found using efficient filtering. Finally, the pupil center and radius is estimated by optimal filtering within the area of the iris. Experimental result show both the efficiency and the effectiveness of our approach. version:1
arxiv-1311-7295 | Glasgow's Stereo Image Database of Garments | http://arxiv.org/abs/1311.7295 | id:1311.7295 author:Gerardo Aragon-Camarasa, Susanne B. Oehler, Yuan Liu, Sun Li, Paul Cockshott, J. Paul Siebert category:cs.RO cs.CV  published:2013-11-28 summary:To provide insight into cloth perception and manipulation with an active binocular robotic vision system, we compiled a database of 80 stereo-pair colour images with corresponding horizontal and vertical disparity maps and mask annotations, for 3D garment point cloud rendering has been created and released. The stereo-image garment database is part of research conducted under the EU-FP7 Clothes Perception and Manipulation (CloPeMa) project and belongs to a wider database collection released through CloPeMa (www.clopema.eu). This database is based on 16 different off-the-shelve garments. Each garment has been imaged in five different pose configurations on the project's binocular robot head. A full copy of the database is made available for scientific research only at https://sites.google.com/site/ugstereodatabase/. version:1
arxiv-1311-7251 | Spatially-Adaptive Reconstruction in Computed Tomography using Neural Networks | http://arxiv.org/abs/1311.7251 | id:1311.7251 author:Joseph Shtok, Michael Zibulevsky, Michael Elad category:cs.CV cs.LG cs.NE  published:2013-11-28 summary:We propose a supervised machine learning approach for boosting existing signal and image recovery methods and demonstrate its efficacy on example of image reconstruction in computed tomography. Our technique is based on a local nonlinear fusion of several image estimates, all obtained by applying a chosen reconstruction algorithm with different values of its control parameters. Usually such output images have different bias/variance trade-off. The fusion of the images is performed by feed-forward neural network trained on a set of known examples. Numerical experiments show an improvement in reconstruction quality relatively to existing direct and iterative reconstruction methods. version:1
arxiv-1311-7213 | Finding a Maximum Clique using Ant Colony Optimization and Particle Swarm Optimization in Social Networks | http://arxiv.org/abs/1311.7213 | id:1311.7213 author:Mohammad Soleimani-Pouri, Alireza Rezvanian, Mohammad Reza Meybodi category:cs.SI cs.NE  published:2013-11-28 summary:Interaction between users in online social networks plays a key role in social network analysis. One on important types of social group is full connected relation between some users, which known as clique structure. Therefore finding a maximum clique is essential for some analysis. In this paper, we proposed a new method using ant colony optimization algorithm and particle swarm optimization algorithm. In the proposed method, in order to attain better results, it is improved process of pheromone update by particle swarm optimization. Simulation results on popular standard social network benchmarks in comparison standard ant colony optimization algorithm are shown a relative enhancement of proposed algorithm. version:1
arxiv-1401-2902 | An Alternate Approach for Designing a Domain Specific Image Search Prototype Using Histogram | http://arxiv.org/abs/1401.2902 | id:1401.2902 author:Sukanta Sinha, Rana Dattagupta, Debajyoti Mukhopadhyay category:cs.CV cs.IR  published:2013-11-28 summary:Everyone knows that thousand of words are represented by a single image. As a result image search has become a very popular mechanism for the Web searchers. Image search means, the search results are produced by the search engine should be a set of images along with their Web page Unified Resource Locator. Now Web searcher can perform two types of image search, they are Text to Image and Image to Image search. In Text to Image search, search query should be a text. Based on the input text data system will generate a set of images along with their Web page URL as an output. On the other hand, in Image to Image search, search query should be an image and based on this image system will generate a set of images along with their Web page URL as an output. According to the current scenarios, Text to Image search mechanism always not returns perfect result. It matches the text data and then displays the corresponding images as an output, which is not always perfect. To resolve this problem, Web researchers have introduced the Image to Image search mechanism. In this paper, we have also proposed an alternate approach of Image to Image search mechanism using Histogram. version:1
arxiv-1312-0482 | Learning Semantic Representations for the Phrase Translation Model | http://arxiv.org/abs/1312.0482 | id:1312.0482 author:Jianfeng Gao, Xiaodong He, Wen-tau Yih, Li Deng category:cs.CL  published:2013-11-28 summary:This paper presents a novel semantic-based phrase translation model. A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent semantic space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a multi-layer neural network whose weights are learned on parallel training data. The learning is aimed to directly optimize the quality of end-to-end machine translation results. Experimental evaluation has been performed on two Europarl translation tasks, English-French and German-English. The results show that the new semantic-based phrase translation model significantly improves the performance of a state-of-the-art phrase-based statistical machine translation sys-tem, leading to a gain of 0.7-1.0 BLEU points. version:1
arxiv-1311-7198 | ADMM Algorithm for Graphical Lasso with an $\ell_{\infty}$ Element-wise Norm Constraint | http://arxiv.org/abs/1311.7198 | id:1311.7198 author:Karthik Mohan category:cs.LG math.OC stat.ML  published:2013-11-28 summary:We consider the problem of Graphical lasso with an additional $\ell_{\infty}$ element-wise norm constraint on the precision matrix. This problem has applications in high-dimensional covariance decomposition such as in \citep{Janzamin-12}. We propose an ADMM algorithm to solve this problem. We also use a continuation strategy on the penalty parameter to have a fast implemenation of the algorithm. version:1
arxiv-1311-7194 | Real-time High Resolution Fusion of Depth Maps on GPU | http://arxiv.org/abs/1311.7194 | id:1311.7194 author:Dmitry Trifonov category:cs.GR cs.CV  published:2013-11-28 summary:A system for live high quality surface reconstruction using a single moving depth camera on a commodity hardware is presented. High accuracy and real-time frame rate is achieved by utilizing graphics hardware computing capabilities via OpenCL and by using sparse data structure for volumetric surface representation. Depth sensor pose is estimated by combining serial texture registration algorithm with iterative closest points algorithm (ICP) aligning obtained depth map to the estimated scene model. Aligned surface is then fused into the scene. Kalman filter is used to improve fusion quality. Truncated signed distance function (TSDF) stored as block-based sparse buffer is used to represent surface. Use of sparse data structure greatly increases accuracy of scanned surfaces and maximum scanning area. Traditional GPU implementation of volumetric rendering and fusion algorithms were modified to exploit sparsity to achieve desired performance. Incorporation of texture registration for sensor pose estimation and Kalman filter for measurement integration improved accuracy and robustness of scanning process. version:1
arxiv-1311-7186 | A Novel Illumination-Invariant Loss for Monocular 3D Pose Estimation | http://arxiv.org/abs/1311.7186 | id:1311.7186 author:Srimal Jayawardena, Marcus Hutter, Nathan Brewer category:cs.CV  published:2013-11-28 summary:The problem of identifying the 3D pose of a known object from a given 2D image has important applications in Computer Vision. Our proposed method of registering a 3D model of a known object on a given 2D photo of the object has numerous advantages over existing methods. It does not require prior training, knowledge of the camera parameters, explicit point correspondences or matching features between the image and model. Unlike techniques that estimate a partial 3D pose (as in an overhead view of traffic or machine parts on a conveyor belt), our method estimates the complete 3D pose of the object. It works on a single static image from a given view under varying and unknown lighting conditions. For this purpose we derive a novel illumination-invariant distance measure between the 2D photo and projected 3D model, which is then minimised to find the best pose parameters. Results for vehicle pose detection in real photographs are presented. version:1
arxiv-1311-7184 | Using Multiple Samples to Learn Mixture Models | http://arxiv.org/abs/1311.7184 | id:1311.7184 author:Jason D Lee, Ran Gilad-Bachrach, Rich Caruana category:stat.ML cs.LG  published:2013-11-28 summary:In the mixture models problem it is assumed that there are $K$ distributions $\theta_{1},\ldots,\theta_{K}$ and one gets to observe a sample from a mixture of these distributions with unknown coefficients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same $K$ underlying distributions, but with different mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption. Instead of pooling the data into one sample, we prove that it is possible to use the differences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. The methods, when applied to topic modeling, allow generalization to words not present in the training data. version:1
arxiv-1311-7080 | Cross-Domain Sparse Coding | http://arxiv.org/abs/1311.7080 | id:1311.7080 author:Jim Jing-Yan Wang category:cs.CV stat.ML  published:2013-11-27 summary:Sparse coding has shown its power as an effective data representation method. However, up to now, all the sparse coding approaches are limited within the single domain learning problem. In this paper, we extend the sparse coding to cross domain learning problem, which tries to learn from a source domain to a target domain with significant different distribution. We impose the Maximum Mean Discrepancy (MMD) criterion to reduce the cross-domain distribution difference of sparse codes, and also regularize the sparse codes by the class labels of the samples from both domains to increase the discriminative ability. The encouraging experiment results of the proposed cross-domain sparse coding algorithm on two challenging tasks --- image classification of photograph and oil painting domains, and multiple user spam detection --- show the advantage of the proposed method over other cross-domain data representation methods. version:1
arxiv-1311-6934 | Image forgery detection based on the fusion of machine learning and block-matching methods | http://arxiv.org/abs/1311.6934 | id:1311.6934 author:Davide Cozzolino, Diego Gragnaniello, Luisa Verdoliva category:cs.CV  published:2013-11-27 summary:Dense local descriptors and machine learning have been used with success in several applications, like classification of textures, steganalysis, and forgery detection. We develop a new image forgery detector building upon some descriptors recently proposed in the steganalysis field suitably merging some of such descriptors, and optimizing a SVM classifier on the available training set. Despite the very good performance, very small forgeries are hardly ever detected because they contribute very little to the descriptors. Therefore we also develop a simple, but extremely specific, copy-move detector based on region matching and fuse decisions so as to reduce the missing detection rate. Overall results appear to be extremely encouraging. version:1
arxiv-1311-6932 | A novel framework for image forgery localization | http://arxiv.org/abs/1311.6932 | id:1311.6932 author:Davide Cozzolino, Diego Gragnaniello, Luisa Verdoliva category:cs.CV  published:2013-11-27 summary:Image forgery localization is a very active and open research field for the difficulty to handle the large variety of manipulations a malicious user can perform by means of more and more sophisticated image editing tools. Here, we propose a localization framework based on the fusion of three very different tools, based, respectively, on sensor noise, patch-matching, and machine learning. The binary masks provided by these tools are finally fused based on some suitable reliability indexes. According to preliminary experiments on the training set, the proposed framework provides often a very good localization accuracy and sometimes valuable clues for visual scrutiny. version:1
arxiv-1311-6371 | On Approximate Inference for Generalized Gaussian Process Models | http://arxiv.org/abs/1311.6371 | id:1311.6371 author:Lifeng Shang, Antoni B. Chan category:stat.ML cs.CV cs.LG  published:2013-11-25 summary:A generalized Gaussian process model (GGPM) is a unifying framework that encompasses many existing Gaussian process (GP) models, such as GP regression, classification, and counting. In the GGPM framework, the observation likelihood of the GP model is itself parameterized using the exponential family distribution (EFD). In this paper, we consider efficient algorithms for approximate inference on GGPMs using the general form of the EFD. A particular GP model and its associated inference algorithms can then be formed by changing the parameters of the EFD, thus greatly simplifying its creation for task-specific output domains. We demonstrate the efficacy of this framework by creating several new GP models for regressing to non-negative reals and to real intervals. We also consider a closed-form Taylor approximation for efficient inference on GGPMs, and elaborate on its connections with other model-specific heuristic closed-form approximations. Finally, we present a comprehensive set of experiments to compare approximate inference algorithms on a wide variety of GGPMs. version:3
arxiv-1311-6881 | Color and Shape Content Based Image Classification using RBF Network and PSO Technique: A Survey | http://arxiv.org/abs/1311.6881 | id:1311.6881 author:Abhishek Pandey, Anjna Jayant Deen, Rajeev Pandey category:cs.CV cs.LG cs.NE  published:2013-11-27 summary:The improvement of the accuracy of image query retrieval used image classification technique. Image classification is well known technique of supervised learning. The improved method of image classification increases the working efficiency of image query retrieval. For the improvements of classification technique we used RBF neural network function for better prediction of feature used in image retrieval.Colour content is represented by pixel values in image classification using radial base function(RBF) technique. This approach provides better result compare to SVM technique in image representation.Image is represented by matrix though RBF using pixel values of colour intensity of image. Firstly we using RGB colour model. In this colour model we use red, green and blue colour intensity values in matrix.SVM with partical swarm optimization for image classification is implemented in content of images which provide better Results based on the proposed approach are found encouraging in terms of color image classification accuracy. version:1
arxiv-1107-0845 | Automatic Road Lighting System (ARLS) Model Based on Image Processing of Moving Object | http://arxiv.org/abs/1107.0845 | id:1107.0845 author:Suprijadi, Thomas Muliawan, Sparisoma Viridi category:cs.CV  published:2011-07-05 summary:Using a vehicle toy (in next future called vehicle) as a moving object an automatic road lighting system (ARLS) model is constructed. A digital video camera with 25 fps is used to capture the vehicle motion as it moves in the test segment of the road. Captured images are then processed to calculate vehicle speed. This information of the speed together with position of vehicle is then used to control the lighting system along the path that passes by the vehicle. Length of the road test segment is 1 m, the video camera is positioned about 1.1 m above the test segment, and the vehicle toy dimension is 13 cm \times 9.3 cm. In this model, the maximum speed that ARLS can handle is about 1.32 m/s, and the highest performance is obtained about 91% at speed 0.93 m/s. version:4
arxiv-1311-6838 | Learning Prices for Repeated Auctions with Strategic Buyers | http://arxiv.org/abs/1311.6838 | id:1311.6838 author:Kareem Amin, Afshin Rostamizadeh, Umar Syed category:cs.LG cs.GT  published:2013-11-26 summary:Inspired by real-time ad exchanges for online display advertising, we consider the problem of inferring a buyer's value distribution for a good when the buyer is repeatedly interacting with a seller through a posted-price mechanism. We model the buyer as a strategic agent, whose goal is to maximize her long-term surplus, and we are interested in mechanisms that maximize the seller's long-term revenue. We define the natural notion of strategic regret --- the lost revenue as measured against a truthful (non-strategic) buyer. We present seller algorithms that are no-(strategic)-regret when the buyer discounts her future surplus --- i.e. the buyer prefers showing advertisements to users sooner rather than later. We also give a lower bound on strategic regret that increases as the buyer's discounting weakens and shows, in particular, that any seller algorithm will suffer linear strategic regret if there is no discounting. version:1
arxiv-1108-4142 | Dynamic Pricing with Limited Supply | http://arxiv.org/abs/1108.4142 | id:1108.4142 author:Moshe Babaioff, Shaddin Dughmi, Robert Kleinberg, Aleksandrs Slivkins category:cs.GT cs.DS cs.LG  published:2011-08-20 summary:We consider the problem of dynamic pricing with limited supply. A seller has $k$ identical items for sale and is facing $n$ potential buyers ("agents") that are arriving sequentially. Each agent is interested in buying one item. Each agent's value for an item is an IID sample from some fixed distribution with support $[0,1]$. The seller offers a take-it-or-leave-it price to each arriving agent (possibly different for different agents), and aims to maximize his expected revenue. We focus on "prior-independent" mechanisms -- ones that do not use any information about the distribution. They are desirable because knowing the distribution is unrealistic in many practical scenarios. We study how the revenue of such mechanisms compares to the revenue of the optimal offline mechanism that knows the distribution ("offline benchmark"). We present a prior-independent dynamic pricing mechanism whose revenue is at most $O((k \log n)^{2/3})$ less than the offline benchmark, for every distribution that is regular. In fact, this guarantee holds without *any* assumptions if the benchmark is relaxed to fixed-price mechanisms. Further, we prove a matching lower bound. The performance guarantee for the same mechanism can be improved to $O(\sqrt{k} \log n)$, with a distribution-dependent constant, if $k/n$ is sufficiently small. We show that, in the worst case over all demand distributions, this is essentially the best rate that can be obtained with a distribution-specific constant. On a technical level, we exploit the connection to multi-armed bandits (MAB). While dynamic pricing with unlimited supply can easily be seen as an MAB problem, the intuition behind MAB approaches breaks when applied to the setting with limited supply. Our high-level conceptual contribution is that even the limited supply setting can be fruitfully treated as a bandit problem. version:3
arxiv-1207-3071 | Supervised Texture Classification Using a Novel Compression-Based Similarity Measure | http://arxiv.org/abs/1207.3071 | id:1207.3071 author:Mehrdad J. Gangeh, Ali Ghodsi, Mohamed S. Kamel category:cs.CV cs.LG  published:2012-07-12 summary:Supervised pixel-based texture classification is usually performed in the feature space. We propose to perform this task in (dis)similarity space by introducing a new compression-based (dis)similarity measure. The proposed measure utilizes two dimensional MPEG-1 encoder, which takes into consideration the spatial locality and connectivity of pixels in the images. The proposed formulation has been carefully designed based on MPEG encoder functionality. To this end, by design, it solely uses P-frame coding to find the (dis)similarity among patches/images. We show that the proposed measure works properly on both small and large patch sizes. Experimental results show that the proposed approach significantly improves the performance of supervised pixel-based texture classification on Brodatz and outdoor images compared to other compression-based dissimilarity measures as well as approaches performed in feature space. It also improves the computation speed by about 40% compared to its rivals. version:2
arxiv-1207-2488 | Kernelized Supervised Dictionary Learning | http://arxiv.org/abs/1207.2488 | id:1207.2488 author:Mehrdad J. Gangeh, Ali Ghodsi, Mohamed S. Kamel category:cs.CV cs.LG  published:2012-07-10 summary:In this paper, we propose supervised dictionary learning (SDL) by incorporating information on class labels into the learning of the dictionary. To this end, we propose to learn the dictionary in a space where the dependency between the signals and their corresponding labels is maximized. To maximize this dependency, the recently introduced Hilbert Schmidt independence criterion (HSIC) is used. One of the main advantages of this novel approach for SDL is that it can be easily kernelized by incorporating a kernel, particularly a data-derived kernel such as normalized compression distance, into the formulation. The learned dictionary is compact and the proposed approach is fast. We show that it outperforms other unsupervised and supervised dictionary learning approaches in the literature, using real-world data. version:4
arxiv-1006-5831 | Statistical Inference in Dynamic Treatment Regimes | http://arxiv.org/abs/1006.5831 | id:1006.5831 author:Eric B. Laber, Min Qian, Dan J. Lizotte, William E. Pelham, Susan A. Murphy category:stat.ME stat.ML stat.OT 47N30  published:2010-06-30 summary:Dynamic treatment regimes are of growing interest across the clinical sciences as these regimes provide one way to operationalize and thus inform sequential personalized clinical decision making. A dynamic treatment regime is a sequence of decision rules, with a decision rule per stage of clinical intervention; each decision rule maps up-to-date patient information to a recommended treatment. We briefly review a variety of approaches for using data to construct the decision rules. We then review an interesting challenge, that of nonregularity that often arises in this area. By nonregularity, we mean the parameters indexing the optimal dynamic treatment regime are nonsmooth functionals of the underlying generative distribution. A consequence is that no regular or asymptotically unbiased estimator of these parameters exists. Nonregularity arises in inference for parameters in the optimal dynamic treatment regime; we illustrate the effect of nonregularity on asymptotic bias and via sensitivity of asymptotic, limiting, distributions to local perturbations. We propose and evaluate a locally consistent Adaptive Confidence Interval (ACI) for the parameters of the optimal dynamic treatment regime. We use data from the Adaptive Interventions for Children with ADHD study as an illustrative example. We conclude by highlighting and discussing emerging theoretical problems in this area. version:3
arxiv-1212-6788 | Local and global asymptotic inference in smoothing spline models | http://arxiv.org/abs/1212.6788 | id:1212.6788 author:Zuofeng Shang, Guang Cheng category:math.ST stat.ML stat.TH  published:2012-12-30 summary:This article studies local and global inference for smoothing spline estimation in a unified asymptotic framework. We first introduce a new technical tool called functional Bahadur representation, which significantly generalizes the traditional Bahadur representation in parametric models, that is, Bahadur [Ann. Inst. Statist. Math. 37 (1966) 577-580]. Equipped with this tool, we develop four interconnected procedures for inference: (i) pointwise confidence interval; (ii) local likelihood ratio testing; (iii) simultaneous confidence band; (iv) global likelihood ratio testing. In particular, our confidence intervals are proved to be asymptotically valid at any point in the support, and they are shorter on average than the Bayesian confidence intervals proposed by Wahba [J. R. Stat. Soc. Ser. B Stat. Methodol. 45 (1983) 133-150] and Nychka [J. Amer. Statist. Assoc. 83 (1988) 1134-1143]. We also discuss a version of the Wilks phenomenon arising from local/global likelihood ratio testing. It is also worth noting that our simultaneous confidence bands are the first ones applicable to general quasi-likelihood models. Furthermore, issues relating to optimality and efficiency are carefully addressed. As a by-product, we discover a surprising relationship between periodic and nonperiodic smoothing splines in terms of inference. version:3
arxiv-1311-6809 | A Novel Family of Adaptive Filtering Algorithms Based on The Logarithmic Cost | http://arxiv.org/abs/1311.6809 | id:1311.6809 author:Muhammed O. Sayin, N. Denizcan Vanli, Suleyman S. Kozat category:cs.LG  published:2013-11-26 summary:We introduce a novel family of adaptive filtering algorithms based on a relative logarithmic cost. The new family intrinsically combines the higher and lower order measures of the error into a single continuous update based on the error amount. We introduce important members of this family of algorithms such as the least mean logarithmic square (LMLS) and least logarithmic absolute difference (LLAD) algorithms that improve the convergence performance of the conventional algorithms. However, our approach and analysis are generic such that they cover other well-known cost functions as described in the paper. The LMLS algorithm achieves comparable convergence performance with the least mean fourth (LMF) algorithm and extends the stability bound on the step size. The LLAD and least mean square (LMS) algorithms demonstrate similar convergence performance in impulse-free noise environments while the LLAD algorithm is robust against impulsive interferences and outperforms the sign algorithm (SA). We analyze the transient, steady state and tracking performance of the introduced algorithms and demonstrate the match of the theoretical analyzes and simulation results. We show the extended stability bound of the LMLS algorithm and analyze the robustness of the LLAD algorithm against impulsive interferences. Finally, we demonstrate the performance of our algorithms in different scenarios through numerical examples. version:1
arxiv-1204-1688 | The asymptotics of ranking algorithms | http://arxiv.org/abs/1204.1688 | id:1204.1688 author:John C. Duchi, Lester Mackey, Michael I. Jordan category:math.ST cs.LG stat.ML stat.TH  published:2012-04-07 summary:We consider the predictive problem of supervised ranking, where the task is to rank sets of candidate items returned in response to queries. Although there exist statistical procedures that come with guarantees of consistency in this setting, these procedures require that individuals provide a complete ranking of all items, which is rarely feasible in practice. Instead, individuals routinely provide partial preference information, such as pairwise comparisons of items, and more practical approaches to ranking have aimed at modeling this partial preference data directly. As we show, however, such an approach raises serious theoretical challenges. Indeed, we demonstrate that many commonly used surrogate losses for pairwise comparison data do not yield consistency; surprisingly, we show inconsistency even in low-noise settings. With these negative results as motivation, we present a new approach to supervised ranking based on aggregation of partial preferences, and we develop $U$-statistic-based empirical risk minimization procedures. We present an asymptotic analysis of these new procedures, showing that they yield consistency results that parallel those available for classification. We complement our theoretical results with an experiment studying the new procedures in a large-scale web-ranking task. version:3
arxiv-1308-3052 | Gradient Magnitude Similarity Deviation: A Highly Efficient Perceptual Image Quality Index | http://arxiv.org/abs/1308.3052 | id:1308.3052 author:Wufeng Xue, Lei Zhang, Xuanqin Mou, Alan C. Bovik category:cs.CV  published:2013-08-14 summary:It is an important task to faithfully evaluate the perceptual quality of output images in many applications such as image compression, image restoration and multimedia streaming. A good image quality assessment (IQA) model should not only deliver high quality prediction accuracy but also be computationally efficient. The efficiency of IQA metrics is becoming particularly important due to the increasing proliferation of high-volume visual data in high-speed networks. We present a new effective and efficient IQA model, called gradient magnitude similarity deviation (GMSD). The image gradients are sensitive to image distortions, while different local structures in a distorted image suffer different degrees of degradations. This motivates us to explore the use of global variation of gradient based local quality map for overall image quality prediction. We find that the pixel-wise gradient magnitude similarity (GMS) between the reference and distorted images combined with a novel pooling strategy the standard deviation of the GMS map can predict accurately perceptual image quality. The resulting GMSD algorithm is much faster than most state-of-the-art IQA methods, and delivers highly competitive prediction accuracy. version:2
arxiv-1311-4486 | Discriminative Density-ratio Estimation | http://arxiv.org/abs/1311.4486 | id:1311.4486 author:Yun-Qian Miao, Ahmed K. Farahat, Mohamed S. Kamel category:cs.LG  published:2013-11-18 summary:The covariate shift is a challenging problem in supervised learning that results from the discrepancy between the training and test distributions. An effective approach which recently drew a considerable attention in the research community is to reweight the training samples to minimize that discrepancy. In specific, many methods are based on developing Density-ratio (DR) estimation techniques that apply to both regression and classification problems. Although these methods work well for regression problems, their performance on classification problems is not satisfactory. This is due to a key observation that these methods focus on matching the sample marginal distributions without paying attention to preserving the separation between classes in the reweighted space. In this paper, we propose a novel method for Discriminative Density-ratio (DDR) estimation that addresses the aforementioned problem and aims at estimating the density-ratio of joint distributions in a class-wise manner. The proposed algorithm is an iterative procedure that alternates between estimating the class information for the test data and estimating new density ratio for each class. To incorporate the estimated class information of the test data, a soft matching technique is proposed. In addition, we employ an effective criterion which adopts mutual information as an indicator to stop the iterative procedure while resulting in a decision boundary that lies in a sparse region. Experiments on synthetic and benchmark datasets demonstrate the superiority of the proposed method in terms of both accuracy and robustness. version:2
arxiv-1311-6536 | Universal Codes from Switching Strategies | http://arxiv.org/abs/1311.6536 | id:1311.6536 author:Wouter M. Koolen, Steven de Rooij category:cs.IT cs.LG math.IT  published:2013-11-26 summary:We discuss algorithms for combining sequential prediction strategies, a task which can be viewed as a natural generalisation of the concept of universal coding. We describe a graphical language based on Hidden Markov Models for defining prediction strategies, and we provide both existing and new models as examples. The models include efficient, parameterless models for switching between the input strategies over time, including a model for the case where switches tend to occur in clusters, and finally a new model for the scenario where the prediction strategies have a known relationship, and where jumps are typically between strongly related ones. This last model is relevant for coding time series data where parameter drift is expected. As theoretical ontributions we introduce an interpolation construction that is useful in the development and analysis of new algorithms, and we establish a new sophisticated lemma for analysing the individual sequence regret of parameterised models. version:1
arxiv-1311-6529 | A Blockwise Descent Algorithm for Group-penalized Multiresponse and Multinomial Regression | http://arxiv.org/abs/1311.6529 | id:1311.6529 author:Noah Simon, Jerome Friedman, Trevor Hastie category:stat.CO stat.ML  published:2013-11-26 summary:In this paper we purpose a blockwise descent algorithm for group-penalized multiresponse regression. Using a quasi-newton framework we extend this to group-penalized multinomial regression. We give a publicly available implementation for these in R, and compare the speed of this algorithm to a competing algorithm --- we show that our implementation is an order of magnitude faster than its competitor, and can solve gene-expression-sized problems in real time. version:1
arxiv-1311-6510 | Are all training examples equally valuable? | http://arxiv.org/abs/1311.6510 | id:1311.6510 author:Agata Lapedriza, Hamed Pirsiavash, Zoya Bylinskii, Antonio Torralba category:cs.CV cs.LG stat.ML  published:2013-11-25 summary:When learning a new concept, not all training examples may prove equally useful for training: some may have higher or lower training value than others. The goal of this paper is to bring to the attention of the vision community the following considerations: (1) some examples are better than others for training detectors or classifiers, and (2) in the presence of better examples, some examples may negatively impact performance and removing them may be beneficial. In this paper, we propose an approach for measuring the training value of an example, and use it for ranking and greedily sorting examples. We test our methods on different vision tasks, models, datasets and classifiers. Our experiments show that the performance of current state-of-the-art detectors and classifiers can be improved when training on a subset, rather than the whole training set. version:1
arxiv-1212-0873 | Parallel Coordinate Descent Methods for Big Data Optimization | http://arxiv.org/abs/1212.0873 | id:1212.0873 author:Peter Richtárik, Martin Takáč category:math.OC cs.AI stat.ML  published:2012-12-04 summary:In this work we show that randomized (block) coordinate descent methods can be accelerated by parallelization when applied to the problem of minimizing the sum of a partially separable smooth convex function and a simple separable convex function. The theoretical speedup, as compared to the serial method, and referring to the number of iterations needed to approximately solve the problem with high probability, is a simple expression depending on the number of parallel processors and a natural and easily computable measure of separability of the smooth component of the objective function. In the worst case, when no degree of separability is present, there may be no speedup; in the best case, when the problem is separable, the speedup is equal to the number of processors. Our analysis also works in the mode when the number of blocks being updated at each iteration is random, which allows for modeling situations with busy or unreliable processors. We show that our algorithm is able to solve a LASSO problem involving a matrix with 20 billion nonzeros in 2 hours on a large memory node with 24 cores. version:2
arxiv-1005-5141 | On Recursive Edit Distance Kernels with Application to Time Series Classification | http://arxiv.org/abs/1005.5141 | id:1005.5141 author:Pierre-François Marteau, Sylvie Gibet category:cs.LG cs.IR  published:2010-05-27 summary:This paper proposes some extensions to the work on kernels dedicated to string or time series global alignment based on the aggregation of scores obtained by local alignments. The extensions we propose allow to construct, from classical recursive definition of elastic distances, recursive edit distance (or time-warp) kernels that are positive definite if some sufficient conditions are satisfied. The sufficient conditions we end-up with are original and weaker than those proposed in earlier works, although a recursive regularizing term is required to get the proof of the positive definiteness as a direct consequence of the Haussler's convolution theorem. The classification experiment we conducted on three classical time warp distances (two of which being metrics), using Support Vector Machine classifier, leads to conclude that, when the pairwise distance matrix obtained from the training data is \textit{far} from definiteness, the positive definite recursive elastic kernels outperform in general the distance substituting kernels for the classical elastic distances we have tested. version:12
arxiv-1311-6425 | Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching | http://arxiv.org/abs/1311.6425 | id:1311.6425 author:Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo Musé, Guillermo Sapiro category:math.OC cs.LG stat.ML  published:2013-11-25 summary:Graph matching is a challenging problem with very important applications in a wide range of fields, from image and video analysis to biological and biomedical problems. We propose a robust graph matching algorithm inspired in sparsity-related techniques. We cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efficiently solved using augmented Lagrangian techniques. The method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data. The proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence. The algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs. We also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fMRI) data. The code is publicly available. version:1
arxiv-1311-6421 | Synchronous Context-Free Grammars and Optimal Linear Parsing Strategies | http://arxiv.org/abs/1311.6421 | id:1311.6421 author:Pierluigi Crescenzi, Daniel Gildea, Andrea Marino, Gianluca Rossi, Giorgio Satta category:cs.FL cs.CL  published:2013-11-25 summary:Synchronous Context-Free Grammars (SCFGs), also known as syntax-directed translation schemata, are unlike context-free grammars in that they do not have a binary normal form. In general, parsing with SCFGs takes space and time polynomial in the length of the input strings, but with the degree of the polynomial depending on the permutations of the SCFG rules. We consider linear parsing strategies, which add one nonterminal at a time. We show that for a given input permutation, the problems of finding the linear parsing strategy with the minimum space and time complexity are both NP-hard. version:1
arxiv-1307-5101 | Large-scale Multi-label Learning with Missing Labels | http://arxiv.org/abs/1307.5101 | id:1307.5101 author:Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, Inderjit S. Dhillon category:cs.LG  published:2013-07-18 summary:The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges: (a) the ability to tackle problems with a large number (say millions) of labels, and (b) the ability to handle data with missing labels. In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework. Our framework, despite being simple, is surprisingly able to encompass several recent label-compression based methods which can be derived as special cases of our method. To optimize the ERM problem, we develop techniques that exploit the structure of specific loss functions - such as the squared loss function - to offer efficient algorithms. We further show that our learning framework admits formal excess risk bounds even in the presence of missing labels. Our risk bounds are tight and demonstrate better generalization performance for low-rank promoting trace-norm regularization when compared to (rank insensitive) Frobenius norm regularization. Finally, we present extensive empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods and can scale up to very large datasets such as the Wikipedia dataset. version:3
arxiv-1311-6334 | Learning Reputation in an Authorship Network | http://arxiv.org/abs/1311.6334 | id:1311.6334 author:Charanpal Dhanjal, Stéphan Clémençon category:cs.SI cs.IR cs.LG stat.ML  published:2013-11-25 summary:The problem of searching for experts in a given academic field is hugely important in both industry and academia. We study exactly this issue with respect to a database of authors and their publications. The idea is to use Latent Semantic Indexing (LSI) and Latent Dirichlet Allocation (LDA) to perform topic modelling in order to find authors who have worked in a query field. We then construct a coauthorship graph and motivate the use of influence maximisation and a variety of graph centrality measures to obtain a ranked list of experts. The ranked lists are further improved using a Markov Chain-based rank aggregation approach. The complete method is readily scalable to large datasets. To demonstrate the efficacy of the approach we report on an extensive set of computational simulations using the Arnetminer dataset. An improvement in mean average precision is demonstrated over the baseline case of simply using the order of authors found by the topic models. version:1
arxiv-1310-0509 | Summary Statistics for Partitionings and Feature Allocations | http://arxiv.org/abs/1310.0509 | id:1310.0509 author:Işık Barış Fidaner, Ali Taylan Cemgil category:cs.LG stat.ML  published:2013-10-01 summary:Infinite mixture models are commonly used for clustering. One can sample from the posterior of mixture assignments by Monte Carlo methods or find its maximum a posteriori solution by optimization. However, in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings. In this paper, we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations. We develop an element-based definition of entropy to quantify segmentation among their elements. Then we propose a simple algorithm called entropy agglomeration (EA) to summarize and visualize this information. Experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice. version:4
arxiv-1311-6211 | Novelty Detection Under Multi-Instance Multi-Label Framework | http://arxiv.org/abs/1311.6211 | id:1311.6211 author:Qi Lou, Raviv Raich, Forrest Briggs, Xiaoli Z. Fern category:cs.LG  published:2013-11-25 summary:Novelty detection plays an important role in machine learning and signal processing. This paper studies novelty detection in a new setting where the data object is represented as a bag of instances and associated with multiple class labels, referred to as multi-instance multi-label (MIML) learning. Contrary to the common assumption in MIML that each instance in a bag belongs to one of the known classes, in novelty detection, we focus on the scenario where bags may contain novel-class instances. The goal is to determine, for any given instance in a new bag, whether it belongs to a known class or a novel class. Detecting novelty in the MIML setting captures many real-world phenomena and has many potential applications. For example, in a collection of tagged images, the tag may only cover a subset of objects existing in the images. Discovering an object whose class has not been previously tagged can be useful for the purpose of soliciting a label for the new object class. To address this novel problem, we present a discriminative framework for detecting new class instances. Experiments demonstrate the effectiveness of our proposed method, and reveal that the presence of unlabeled novel instances in training bags is helpful to the detection of such instances in testing stage. version:1
arxiv-1002-3183 | A Complete Characterization of Statistical Query Learning with Applications to Evolvability | http://arxiv.org/abs/1002.3183 | id:1002.3183 author:Vitaly Feldman category:cs.CC cs.LG  published:2010-02-16 summary:Statistical query (SQ) learning model of Kearns (1993) is a natural restriction of the PAC learning model in which a learning algorithm is allowed to obtain estimates of statistical properties of the examples but cannot see the examples themselves. We describe a new and simple characterization of the query complexity of learning in the SQ learning model. Unlike the previously known bounds on SQ learning our characterization preserves the accuracy and the efficiency of learning. The preservation of accuracy implies that that our characterization gives the first characterization of SQ learning in the agnostic learning framework. The preservation of efficiency is achieved using a new boosting technique and allows us to derive a new approach to the design of evolutionary algorithms in Valiant's (2006) model of evolvability. We use this approach to demonstrate the existence of a large class of monotone evolutionary learning algorithms based on square loss performance estimation. These results differ significantly from the few known evolutionary algorithms and give evidence that evolvability in Valiant's model is a more versatile phenomenon than there had been previous reason to suspect. version:3
arxiv-1311-5750 | Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization | http://arxiv.org/abs/1311.5750 | id:1311.5750 author:Xiao-Tong Yuan, Ping Li, Tong Zhang category:cs.LG cs.NA stat.ML  published:2013-11-22 summary:Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantee and impressive numerical performance. In this paper, we generalize HTP from compressive sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard thresholding step with or without debiasing. We prove that our method enjoys the strong guarantees analogous to HTP in terms of rate of convergence and parameter estimation accuracy. Numerical evidences show that our method is superior to the state-of-the-art greedy selection methods in sparse logistic regression and sparse precision matrix estimation tasks. version:2
arxiv-1311-6186 | Sparse CCA via Precision Adjusted Iterative Thresholding | http://arxiv.org/abs/1311.6186 | id:1311.6186 author:Mengjie Chen, Chao Gao, Zhao Ren, Harrison H. Zhou category:math.ST stat.ME stat.ML stat.TH  published:2013-11-24 summary:Sparse Canonical Correlation Analysis (CCA) has received considerable attention in high-dimensional data analysis to study the relationship between two sets of random variables. However, there has been remarkably little theoretical statistical foundation on sparse CCA in high-dimensional settings despite active methodological and applied research activities. In this paper, we introduce an elementary sufficient and necessary characterization such that the solution of CCA is indeed sparse, propose a computationally efficient procedure, called CAPIT, to estimate the canonical directions, and show that the procedure is rate-optimal under various assumptions on nuisance parameters. The procedure is applied to a breast cancer dataset from The Cancer Genome Atlas project. We identify methylation probes that are associated with genes, which have been previously characterized as prognosis signatures of the metastasis of breast cancer. version:1
arxiv-1311-6182 | Robust Low-rank Tensor Recovery: Models and Algorithms | http://arxiv.org/abs/1311.6182 | id:1311.6182 author:Donald Goldfarb, Zhiwei Qin category:stat.ML  published:2013-11-24 summary:Robust tensor recovery plays an instrumental role in robustifying tensor decompositions for multilinear data analysis against outliers, gross corruptions and missing values and has a diverse array of applications. In this paper, we study the problem of robust low-rank tensor recovery in a convex optimization framework, drawing upon recent advances in robust Principal Component Analysis and tensor completion. We propose tailored optimization algorithms with global convergence guarantees for solving both the constrained and the Lagrangian formulations of the problem. These algorithms are based on the highly efficient alternating direction augmented Lagrangian and accelerated proximal gradient methods. We also propose a nonconvex model that can often improve the recovery results from the convex models. We investigate the empirical recoverability properties of the convex and nonconvex formulations and compare the computational performance of the algorithms on simulated data. We demonstrate through a number of real applications the practical effectiveness of this convex optimization framework for robust low-rank tensor recovery. version:1
arxiv-1303-3257 | Ranking and combining multiple predictors without labeled data | http://arxiv.org/abs/1303.3257 | id:1303.3257 author:Fabio Parisi, Francesco Strino, Boaz Nadler, Yuval Kluger category:stat.ML cs.LG  published:2013-03-13 summary:In a broad range of classification and decision making problems, one is given the advice or predictions of several classifiers, of unknown reliability, over multiple questions or queries. This scenario is different from the standard supervised setting, where each classifier accuracy can be assessed using available labeled data, and raises two questions: given only the predictions of several classifiers over a large set of unlabeled test data, is it possible to a) reliably rank them; and b) construct a meta-classifier more accurate than most classifiers in the ensemble? Here we present a novel spectral approach to address these questions. First, assuming conditional independence between classifiers, we show that the off-diagonal entries of their covariance matrix correspond to a rank-one matrix. Moreover, the classifiers can be ranked using the leading eigenvector of this covariance matrix, as its entries are proportional to their balanced accuracies. Second, via a linear approximation to the maximum likelihood estimator, we derive the Spectral Meta-Learner (SML), a novel ensemble classifier whose weights are equal to this eigenvector entries. On both simulated and real data, SML typically achieves a higher accuracy than most classifiers in the ensemble and can provide a better starting point than majority voting, for estimating the maximum likelihood solution. Furthermore, SML is robust to the presence of small malicious groups of classifiers designed to veer the ensemble prediction away from the (unknown) ground truth. version:3
arxiv-1311-6758 | Detection of Partially Visible Objects | http://arxiv.org/abs/1311.6758 | id:1311.6758 author:Patrick Ott, Mark Everingham, Jiri Matas category:cs.CV  published:2013-11-24 summary:An "elephant in the room" for most current object detection and localization methods is the lack of explicit modelling of partial visibility due to occlusion by other objects or truncation by the image boundary. Based on a sliding window approach, we propose a detection method which explicitly models partial visibility by treating it as a latent variable. A novel non-maximum suppression scheme is proposed which takes into account the inferred partial visibility of objects while providing a globally optimal solution. The method gives more detailed scene interpretations than conventional detectors in that we are able to identify the visible parts of an object. We report improved average precision on the PASCAL VOC 2010 dataset compared to a baseline detector. version:1
arxiv-1311-6049 | Skin Texture Recognition Using Neural Networks | http://arxiv.org/abs/1311.6049 | id:1311.6049 author:Nidhal K. El Abbadi, Nazar Dahir, Zaid Abd Alkareem category:cs.CV  published:2013-11-23 summary:Skin recognition is used in many applications ranging from algorithms for face detection, hand gesture analysis, and to objectionable image filtering. In this work a skin recognition system was developed and tested. While many skin segmentation algorithms relay on skin color, our work relies on both skin color and texture features (features derives from the GLCM) to give a better and more efficient recognition accuracy of skin textures. We used feed forward neural networks to classify input textures images to be skin or non skin textures. The system gave very encouraging results during the neural network generalization face. version:1
arxiv-1311-6048 | On the Design and Analysis of Multiple View Descriptors | http://arxiv.org/abs/1311.6048 | id:1311.6048 author:Jingming Dong, Jonathan Balzer, Damek Davis, Joshua Hernandez, Stefano Soatto category:cs.CV  published:2013-11-23 summary:We propose an extension of popular descriptors based on gradient orientation histograms (HOG, computed in a single image) to multiple views. It hinges on interpreting HOG as a conditional density in the space of sampled images, where the effects of nuisance factors such as viewpoint and illumination are marginalized. However, such marginalization is performed with respect to a very coarse approximation of the underlying distribution. Our extension leverages on the fact that multiple views of the same scene allow separating intrinsic from nuisance variability, and thus afford better marginalization of the latter. The result is a descriptor that has the same complexity of single-view HOG, and can be compared in the same manner, but exploits multiple views to better trade off insensitivity to nuisance variability with specificity to intrinsic variability. We also introduce a novel multi-view wide-baseline matching dataset, consisting of a mixture of real and synthetic objects with ground truthed camera motion and dense three-dimensional geometry. version:1
arxiv-1311-6045 | Build Electronic Arabic Lexicon | http://arxiv.org/abs/1311.6045 | id:1311.6045 author:Nidhal El-Abbadi, Ahmed Nidhal Khdhair, Adel Al-Nasrawi category:cs.CL  published:2013-11-23 summary:There are many known Arabic lexicons organized on different ways, each of them has a different number of Arabic words according to its organization way. This paper has used mathematical relations to count a number of Arabic words, which proofs the number of Arabic words presented by Al Farahidy. The paper also presents new way to build an electronic Arabic lexicon by using a hash function that converts each word (as input) to correspond a unique integer number (as output), these integer numbers will be used as an index to a lexicon entry. version:1
arxiv-1311-6007 | Dynamic Model of Facial Expression Recognition based on Eigen-face Approach | http://arxiv.org/abs/1311.6007 | id:1311.6007 author:Nikunj Bajaj, Aurobinda Routray, S L Happy category:cs.CV  published:2013-11-23 summary:Emotions are best way of communicating information; and sometimes it carry more information than words. Recently, there has been a huge interest in automatic recognition of human emotion because of its wide spread application in security, surveillance, marketing, advertisement, and human-computer interaction. To communicate with a computer in a natural way, it will be desirable to use more natural modes of human communication based on voice, gestures and facial expressions. In this paper, a holistic approach for facial expression recognition is proposed which captures the variation in facial features in temporal domain and classifies the sequence of images in different emotions. The proposed method uses Haar-like features to detect face in an image. The dimensionality of the eigenspace is reduced using Principal Component Analysis (PCA). By projecting the subsequent face images into principal eigen directions, the variation pattern of the obtained weight vector is modeled to classify it into different emotions. Owing to the variations of expressions for different people and its intensity, a person specific method for emotion recognition is followed. Using the gray scale images of the frontal face, the system is able to classify four basic emotions such as happiness, sadness, surprise, and anger. version:1
arxiv-1401-6127 | Brain Tumor Detection Based On Symmetry Information | http://arxiv.org/abs/1401.6127 | id:1401.6127 author:Narkhede Sachin G, Vaishali Khairnar category:cs.CV  published:2013-11-23 summary:Advances in computing technology have allowed researchers across many fields of endeavor to collect and maintain vast amounts of observational statistical data such as clinical data, biological patient data, data regarding access of web sites, financial data, and the like. This paper addresses some of the challenging issues on brain magnetic resonance (MR) image tumor segmentation caused by the weak correlation between magnetic resonance imaging (MRI) intensity and anatomical meaning. With the objective of utilizing more meaningful information to improve brain tumor segmentation, an approach which employs bilateral symmetry information as an additional feature for segmentation is proposed. This is motivated by potential performance improvement in the general automatic brain tumor segmentation systems which are important for many medical and scientific applications version:1
arxiv-1311-5947 | Fast Training of Effective Multi-class Boosting Using Coordinate Descent Optimization | http://arxiv.org/abs/1311.5947 | id:1311.5947 author:Guosheng Lin, Chunhua Shen, Anton van den Hengel, David Suter category:cs.CV cs.LG stat.CO  published:2013-11-23 summary:Wepresentanovelcolumngenerationbasedboostingmethod for multi-class classification. Our multi-class boosting is formulated in a single optimization problem as in Shen and Hao (2011). Different from most existing multi-class boosting methods, which use the same set of weak learners for all the classes, we train class specified weak learners (i.e., each class has a different set of weak learners). We show that using separate weak learner sets for each class leads to fast convergence, without introducing additional computational overhead in the training procedure. To further make the training more efficient and scalable, we also propose a fast co- ordinate descent method for solving the optimization problem at each boosting iteration. The proposed coordinate descent method is conceptually simple and easy to implement in that it is a closed-form solution for each coordinate update. Experimental results on a variety of datasets show that, compared to a range of existing multi-class boosting meth- ods, the proposed method has much faster convergence rate and better generalization performance in most cases. We also empirically show that the proposed fast coordinate descent algorithm needs less training time than the MultiBoost algorithm in Shen and Hao (2011). version:1
arxiv-1311-5836 | Automatic Ranking of MT Outputs using Approximations | http://arxiv.org/abs/1311.5836 | id:1311.5836 author:Pooja Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-11-22 summary:Since long, research on machine translation has been ongoing. Still, we do not get good translations from MT engines so developed. Manual ranking of these outputs tends to be very time consuming and expensive. Identifying which one is better or worse than the others is a very taxing task. In this paper, we show an approach which can provide automatic ranks to MT outputs (translations) taken from different MT Engines and which is based on N-gram approximations. We provide a solution where no human intervention is required for ranking systems. Further we also show the evaluations of our results which show equivalent results as that of human ranking. version:1
arxiv-1311-5830 | Dictionary-Learning-Based Reconstruction Method for Electron Tomography | http://arxiv.org/abs/1311.5830 | id:1311.5830 author:Baodong Liu, Hengyong Yu, Scott S. Verbridge, Lizhi Sun, Ge Wang category:cs.CV physics.med-ph  published:2013-11-22 summary:Electron tomography usually suffers from so called missing wedge artifacts caused by limited tilt angle range. An equally sloped tomography (EST) acquisition scheme (which should be called the linogram sampling scheme) was recently applied to achieve 2.4-angstrom resolution. On the other hand, a compressive sensing-inspired reconstruction algorithm, known as adaptive dictionary based statistical iterative reconstruction (ADSIR), has been reported for x-ray computed tomography. In this paper, we evaluate the EST, ADSIR and an ordered-subset simultaneous algebraic reconstruction technique (OS-SART), and compare the ES and equally angled (EA) data acquisition modes. Our results show that OS-SART is comparable to EST, and the ADSIR outperforms EST and OS-SART. Furthermore, the equally sloped projection data acquisition mode has no advantage over the conventional equally angled mode in the context. version:1
arxiv-1311-5763 | Automated and Weighted Self-Organizing Time Maps | http://arxiv.org/abs/1311.5763 | id:1311.5763 author:Peter Sarlin category:cs.NE cs.HC  published:2013-11-22 summary:This paper proposes schemes for automated and weighted Self-Organizing Time Maps (SOTMs). The SOTM provides means for a visual approach to evolutionary clustering, which aims at producing a sequence of clustering solutions. This task we denote as visual dynamic clustering. The implication of an automated SOTM is not only a data-driven parametrization of the SOTM, but also the feature of adjusting the training to the characteristics of the data at each time step. The aim of the weighted SOTM is to improve learning from more trustworthy or important data with an instance-varying weight. The schemes for automated and weighted SOTMs are illustrated on two real-world datasets: (i) country-level risk indicators to measure the evolution of global imbalances, and (ii) credit applicant data to measure the evolution of firm-level credit risks. version:1
arxiv-1311-5479 | Learning Pairwise Graphical Models with Nonlinear Sufficient Statistics | http://arxiv.org/abs/1311.5479 | id:1311.5479 author:Xiao-Tong Yuan, Ping Li, Tong Zhang category:stat.ML  published:2013-11-21 summary:We investigate a generic problem of learning pairwise exponential family graphical models with pairwise sufficient statistics defined by a global mapping function, e.g., Mercer kernels. This subclass of pairwise graphical models allow us to flexibly capture complex interactions among variables beyond pairwise product. We propose two $\ell_1$-norm penalized maximum likelihood estimators to learn the model parameters from i.i.d. samples. The first one is a joint estimator which estimates all the parameters simultaneously. The second one is a node-wise conditional estimator which estimates the parameters individually for each node. For both estimators, we show that under proper conditions the extra flexibility gained in our model comes at almost no cost of statistical and computational efficiency. We demonstrate the advantages of our model over state-of-the-art methods on synthetic and real datasets. version:2
arxiv-1311-5427 | Complexity measurement of natural and artificial languages | http://arxiv.org/abs/1311.5427 | id:1311.5427 author:Gerardo Febres, Klaus Jaffe, Carlos Gershenson category:cs.CL cs.IT math.IT nlin.AO physics.soc-ph  published:2013-11-20 summary:We compared entropy for texts written in natural languages (English, Spanish) and artificial languages (computer software) based on a simple expression for the entropy as a function of message length and specific word diversity. Code text written in artificial languages showed higher entropy than text of similar length expressed in natural languages. Spanish texts exhibit more symbolic diversity than English ones. Results showed that algorithms based on complexity measures differentiate artificial from natural languages, and that text analysis based on complexity measures allows the unveiling of important aspects of their nature. We propose specific expressions to examine entropy related aspects of tests and estimate the values of entropy, emergence, self-organization and complexity based on specific diversity and message length. version:2
arxiv-1311-5422 | Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis | http://arxiv.org/abs/1311.5422 | id:1311.5422 author:Nikhil Rao, Christopher Cox, Robert Nowak, Timothy Rogers category:cs.LG stat.ML  published:2013-11-20 summary:Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multi- subject fMRI studies in which functional activity is classified using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. version:2
arxiv-1202-2026 | A quantum genetic algorithm with quantum crossover and mutation operations | http://arxiv.org/abs/1202.2026 | id:1202.2026 author:Akira SaiToh, Robabeh Rahimi, Mikio Nakahara category:cs.NE quant-ph 68Q12  published:2012-02-09 summary:In the context of evolutionary quantum computing in the literal meaning, a quantum crossover operation has not been introduced so far. Here, we introduce a novel quantum genetic algorithm which has a quantum crossover procedure performing crossovers among all chromosomes in parallel for each generation. A complexity analysis shows that a quadratic speedup is achieved over its classical counterpart in the dominant factor of the run time to handle each generation. version:5
arxiv-1311-5636 | Learning Non-Linear Feature Maps | http://arxiv.org/abs/1311.5636 | id:1311.5636 author:Dimitrios Athanasakis, John Shawe-Taylor, Delmiro Fernandez-Reyes category:cs.LG  published:2013-11-22 summary:Feature selection plays a pivotal role in learning, particularly in areas were parsimonious features can provide insight into the underlying process, such as biology. Recent approaches for non-linear feature selection employing greedy optimisation of Centred Kernel Target Alignment(KTA), while exhibiting strong results in terms of generalisation accuracy and sparsity, can become computationally prohibitive for high-dimensional datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for the correct identification of relevant features. Experimental results on real and artificial data, show that the method successfully identifies effective features, performing better than a number of competitive approaches. version:1
arxiv-1311-5599 | Compressive Measurement Designs for Estimating Structured Signals in Structured Clutter: A Bayesian Experimental Design Approach | http://arxiv.org/abs/1311.5599 | id:1311.5599 author:Swayambhoo Jain, Akshay Soni, Jarvis Haupt category:stat.ML cs.LG  published:2013-11-21 summary:This work considers an estimation task in compressive sensing, where the goal is to estimate an unknown signal from compressive measurements that are corrupted by additive pre-measurement noise (interference, or clutter) as well as post-measurement noise, in the specific setting where some (perhaps limited) prior knowledge on the signal, interference, and noise is available. The specific aim here is to devise a strategy for incorporating this prior information into the design of an appropriate compressive measurement strategy. Here, the prior information is interpreted as statistics of a prior distribution on the relevant quantities, and an approach based on Bayesian Experimental Design is proposed. Experimental results on synthetic data demonstrate that the proposed approach outperforms traditional random compressive measurement designs, which are agnostic to the prior information, as well as several other knowledge-enhanced sensing matrix designs based on more heuristic notions. version:1
arxiv-1311-5590 | Adaptive Learning of Region-based pLSA Model for Total Scene Annotation | http://arxiv.org/abs/1311.5590 | id:1311.5590 author:Yuzhu Zhou, Le Li, Honggang Zhang category:cs.CV  published:2013-11-21 summary:In this paper, we present a region-based pLSA model to accomplish the task of total scene annotation. To be more specific, we not only properly generate a list of tags for each image, but also localizing each region with its corresponding tag. We integrate advantages of different existing region-based works: employ efficient and powerful JSEG algorithm for segmentation so that each region can easily express meaningful object information; the introduction of pLSA model can help better capturing semantic information behind the low-level features. Moreover, we also propose an adaptive padding mechanism to automatically choose the optimal padding strategy for each region, which directly increases the overall system performance. Finally we conduct 3 experiments to verify our ideas on Corel database and demonstrate the effectiveness and accuracy of our system. version:1
arxiv-1302-2686 | Covariance Estimation in High Dimensions via Kronecker Product Expansions | http://arxiv.org/abs/1302.2686 | id:1302.2686 author:Theodoros Tsiligkaridis, Alfred O. Hero III category:stat.ME stat.ML  published:2013-02-12 summary:This paper presents a new method for estimating high dimensional covariance matrices. The method, permuted rank-penalized least-squares (PRLS), is based on a Kronecker product series expansion of the true covariance matrix. Assuming an i.i.d. Gaussian random sample, we establish high dimensional rates of convergence to the true covariance as both the number of samples and the number of variables go to infinity. For covariance matrices of low separation rank, our results establish that PRLS has significantly faster convergence than the standard sample covariance matrix (SCM) estimator. The convergence rate captures a fundamental tradeoff between estimation error and approximation error, thus providing a scalable covariance estimation framework in terms of separation rank, similar to low rank approximation of covariance matrices. The MSE convergence rates generalize the high dimensional rates recently obtained for the ML Flip-flop algorithm for Kronecker product covariance estimation. We show that a class of block Toeplitz covariance matrices is approximatable by low separation rank and give bounds on the minimal separation rank $r$ that ensures a given level of bias. Simulations are presented to validate the theoretical bounds. As a real world application, we illustrate the utility of the proposed Kronecker covariance estimator for spatio-temporal linear least squares prediction of multivariate wind speed measurements. version:10
arxiv-1311-5406 | A Unified SVM Framework for Signal Estimation | http://arxiv.org/abs/1311.5406 | id:1311.5406 author:José Luis Rojo-Álvarez, Manel Martínez-Ramón, Jordi Muñoz-Marí, Gustavo Camps-Valls category:stat.ML stat.AP  published:2013-11-21 summary:This paper presents a unified framework to tackle estimation problems in Digital Signal Processing (DSP) using Support Vector Machines (SVMs). The use of SVMs in estimation problems has been traditionally limited to its mere use as a black-box model. Noting such limitations in the literature, we take advantage of several properties of Mercer's kernels and functional analysis to develop a family of SVM methods for estimation in DSP. Three types of signal model equations are analyzed. First, when a specific time-signal structure is assumed to model the underlying system that generated the data, the linear signal model (so called Primal Signal Model formulation) is first stated and analyzed. Then, non-linear versions of the signal structure can be readily developed by following two different approaches. On the one hand, the signal model equation is written in reproducing kernel Hilbert spaces (RKHS) using the well-known RKHS Signal Model formulation, and Mercer's kernels are readily used in SVM non-linear algorithms. On the other hand, in the alternative and not so common Dual Signal Model formulation, a signal expansion is made by using an auxiliary signal model equation given by a non-linear regression of each time instant in the observed time series. These building blocks can be used to generate different novel SVM-based methods for problems of signal estimation, and we deal with several of the most important ones in DSP. We illustrate the usefulness of this methodology by defining SVM algorithms for linear and non-linear system identification, spectral analysis, nonuniform interpolation, sparse deconvolution, and array processing. The performance of the developed SVM methods is compared to standard approaches in all these settings. The experimental results illustrate the generality, simplicity, and capabilities of the proposed SVM framework for DSP. version:1
arxiv-1311-1033 | Nonparametric Bayesian models of hierarchical structure in complex networks | http://arxiv.org/abs/1311.1033 | id:1311.1033 author:Mikkel N. Schmidt, Tue Herlau, Morten Mørup category:stat.ML  published:2013-11-05 summary:Analyzing and understanding the structure of complex relational data is important in many applications including analysis of the connectivity in the human brain. Such networks can have prominent patterns on different scales, calling for a hierarchically structured model. We propose two non-parametric Bayesian hierarchical network models based on Gibbs fragmentation tree priors, and demonstrate their ability to capture nested patterns in simulated networks. On real networks we demonstrate detection of hierarchical structure and show predictive performance on par with the state of the art. We envision that our methods can be employed in exploratory analysis of large scale complex networks for example to model human brain connectivity. version:2
arxiv-1311-4963 | Comparative Study Of Image Edge Detection Algorithms | http://arxiv.org/abs/1311.4963 | id:1311.4963 author:Shubham Saini, Bhavesh Kasliwal, Shraey Bhatia category:cs.CV  published:2013-11-20 summary:Since edge detection is in the forefront of image processing for object detection, it is crucial to have a good understanding of edge detection algorithms. The reason for this is that edges form the outline of an object. An edge is the boundary between an object and the background, and indicates the boundary between overlapping objects. This means that if the edges in an image can be identified accurately, all of the objects can be located and basic properties such as area, perimeter, and shape can be measured. Since computer vision involves the identification and classification of objects in an image, edge detection is an essential tool. We tested two edge detectors that use different methods for detecting edges and compared their results under a variety of situations to determine which detector was preferable under different sets of conditions. version:2
arxiv-1311-5290 | Texture descriptor combining fractal dimension and artificial crawlers | http://arxiv.org/abs/1311.5290 | id:1311.5290 author:Wesley Nunes Gonçalves, Bruno Brandoli Machado, Odemir Martinez Bruno category:physics.data-an cs.CV  published:2013-11-21 summary:Texture is an important visual attribute used to describe images. There are many methods available for texture analysis. However, they do not capture the details richness of the image surface. In this paper, we propose a new method to describe textures using the artificial crawler model. This model assumes that each agent can interact with the environment and each other. Since this swarm system alone does not achieve a good discrimination, we developed a new method to increase the discriminatory power of artificial crawlers, together with the fractal dimension theory. Here, we estimated the fractal dimension by the Bouligand-Minkowski method due to its precision in quantifying structural properties of images. We validate our method on two texture datasets and the experimental results reveal that our method leads to highly discriminative textural features. The results indicate that our method can be used in different texture applications. version:1
arxiv-1311-0053 | Robust Compressed Sensing and Sparse Coding with the Difference Map | http://arxiv.org/abs/1311.0053 | id:1311.0053 author:Will Landecker, Rick Chartrand, Simon DeDeo category:cs.CV physics.data-an stat.ML  published:2013-10-31 summary:In compressed sensing, we wish to reconstruct a sparse signal $x$ from observed data $y$. In sparse coding, on the other hand, we wish to find a representation of an observed signal $y$ as a sparse linear combination, with coefficients $x$, of elements from an overcomplete dictionary. While many algorithms are competitive at both problems when $x$ is very sparse, it can be challenging to recover $x$ when it is less sparse. We present the Difference Map, which excels at sparse recovery when sparseness is lower and noise is higher. The Difference Map out-performs the state of the art with reconstruction from random measurements and natural image reconstruction via sparse coding. version:2
arxiv-1307-6769 | Streaming Variational Bayes | http://arxiv.org/abs/1307.6769 | id:1307.6769 author:Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson, Michael I. Jordan category:stat.ML cs.LG  published:2013-07-25 summary:We present SDA-Bayes, a framework for (S)treaming, (D)istributed, (A)synchronous computation of a Bayesian posterior. The framework makes streaming updates to the estimated posterior according to a user-specified approximation batch primitive. We demonstrate the usefulness of our framework, with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet allocation model to two large-scale document collections. We demonstrate the advantages of our algorithm over stochastic variational inference (SVI) by comparing the two after a single pass through a known amount of data---a case where SVI may be applied---and in the streaming setting, where SVI does not apply. version:2
arxiv-1108-2283 | A survey on independence-based Markov networks learning | http://arxiv.org/abs/1108.2283 | id:1108.2283 author:Federico Schlüter category:cs.AI cs.LG  published:2011-08-10 summary:This work reports the most relevant technical aspects in the problem of learning the \emph{Markov network structure} from data. Such problem has become increasingly important in machine learning, and many other application fields of machine learning. Markov networks, together with Bayesian networks, are probabilistic graphical models, a widely used formalism for handling probability distributions in intelligent systems. Learning graphical models from data have been extensively applied for the case of Bayesian networks, but for Markov networks learning it is not tractable in practice. However, this situation is changing with time, given the exponential growth of computers capacity, the plethora of available digital data, and the researching on new learning technologies. This work stresses on a technology called independence-based learning, which allows the learning of the independence structure of those networks from data in an efficient and sound manner, whenever the dataset is sufficiently large, and data is a representative sampling of the target distribution. In the analysis of such technology, this work surveys the current state-of-the-art algorithms for learning Markov networks structure, discussing its current limitations, and proposing a series of open problems where future works may produce some advances in the area in terms of quality and efficiency. The paper concludes by opening a discussion about how to develop a general formalism for improving the quality of the structures learned, when data is scarce. version:2
arxiv-1011-3023 | Classification with Scattering Operators | http://arxiv.org/abs/1011.3023 | id:1011.3023 author:Joan Bruna, Stéphane Mallat category:cs.CV  published:2010-11-12 summary:A scattering vector is a local descriptor including multiscale and multi-direction co-occurrence information. It is computed with a cascade of wavelet decompositions and complex modulus. This scattering representation is locally translation invariant and linearizes deformations. A supervised classification algorithm is computed with a PCA model selection on scattering vectors. State of the art results are obtained for handwritten digit recognition and texture classification. version:4
arxiv-1311-5068 | Gromov-Hausdorff stability of linkage-based hierarchical clustering methods | http://arxiv.org/abs/1311.5068 | id:1311.5068 author:A. Martínez-Pérez category:cs.LG  published:2013-11-20 summary:A hierarchical clustering method is stable if small perturbations on the data set produce small perturbations in the result. These perturbations are measured using the Gromov-Hausdorff metric. We study the problem of stability on linkage-based hierarchical clustering methods. We obtain that, under some basic conditions, standard linkage-based methods are semi-stable. This means that they are stable if the input data is close enough to an ultrametric space. We prove that, apart from exotic examples, introducing any unchaining condition in the algorithm always produces unstable methods. version:1
arxiv-1311-3859 | Mapping cognitive ontologies to and from the brain | http://arxiv.org/abs/1311.3859 | id:1311.3859 author:Yannick Schwartz, Bertrand Thirion, Gaël Varoquaux category:stat.ML cs.LG q-bio.NC  published:2013-11-15 summary:Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to find commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the first demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies. version:2
arxiv-1212-5332 | Optimal classification in sparse Gaussian graphic model | http://arxiv.org/abs/1212.5332 | id:1212.5332 author:Yingying Fan, Jiashun Jin, Zhigang Yao category:stat.ML math.ST stat.TH  published:2012-12-21 summary:Consider a two-class classification problem where the number of features is much larger than the sample size. The features are masked by Gaussian noise with mean zero and covariance matrix $\Sigma$, where the precision matrix $\Omega=\Sigma^{-1}$ is unknown but is presumably sparse. The useful features, also unknown, are sparse and each contributes weakly (i.e., rare and weak) to the classification decision. By obtaining a reasonably good estimate of $\Omega$, we formulate the setting as a linear regression model. We propose a two-stage classification method where we first select features by the method of Innovated Thresholding (IT), and then use the retained features and Fisher's LDA for classification. In this approach, a crucial problem is how to set the threshold of IT. We approach this problem by adapting the recent innovation of Higher Criticism Thresholding (HCT). We find that when useful features are rare and weak, the limiting behavior of HCT is essentially just as good as the limiting behavior of ideal threshold, the threshold one would choose if the underlying distribution of the signals is known (if only). Somewhat surprisingly, when $\Omega$ is sufficiently sparse, its off-diagonal coordinates usually do not have a major influence over the classification decision. Compared to recent work in the case where $\Omega$ is the identity matrix [Proc. Natl. Acad. Sci. USA 105 (2008) 14790-14795; Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci. 367 (2009) 4449-4470], the current setting is much more general, which needs a new approach and much more sophisticated analysis. One key component of the analysis is the intimate relationship between HCT and Fisher's separation. Another key component is the tight large-deviation bounds for empirical processes for data with unconventional correlation structures, where graph theory on vertex coloring plays an important role. version:2
arxiv-1311-4987 | Analyzing Evolutionary Optimization in Noisy Environments | http://arxiv.org/abs/1311.4987 | id:1311.4987 author:Chao Qian, Yang Yu, Zhi-Hua Zhou category:cs.AI cs.NE  published:2013-11-20 summary:Many optimization tasks have to be handled in noisy environments, where we cannot obtain the exact evaluation of a solution but only a noisy one. For noisy optimization tasks, evolutionary algorithms (EAs), a kind of stochastic metaheuristic search algorithm, have been widely and successfully applied. Previous work mainly focuses on empirical studying and designing EAs for noisy optimization, while, the theoretical counterpart has been little investigated. In this paper, we investigate a largely ignored question, i.e., whether an optimization problem will always become harder for EAs in a noisy environment. We prove that the answer is negative, with respect to the measurement of the expected running time. The result implies that, for optimization tasks that have already been quite hard to solve, the noise may not have a negative effect, and the easier a task the more negatively affected by the noise. On a representative problem where the noise has a strong negative effect, we examine two commonly employed mechanisms in EAs dealing with noise, the re-evaluation and the threshold selection strategies. The analysis discloses that the two strategies, however, both are not effective, i.e., they do not make the EA more noise tolerant. We then find that a small modification of the threshold selection allows it to be proven as an effective strategy for dealing with the noise in the problem. version:1
arxiv-1311-5829 | Neural Network Application on Foliage Plant Identification | http://arxiv.org/abs/1311.5829 | id:1311.5829 author:Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa category:cs.CV cs.NE  published:2013-11-20 summary:Several researches in leaf identification did not include color information as features. The main reason is caused by a fact that they used green colored leaves as samples. However, for foliage plants, plants with colorful leaves, fancy patterns in their leaves, and interesting plants with unique shape, color and also texture could not be neglected. For example, Epipremnum pinnatum 'Aureum' and Epipremnum pinnatum 'Marble Queen' have similar patterns, same shape, but different colors. Combination of shape, color, texture features, and other attribute contained on the leaf is very useful in leaf identification. In this research, Polar Fourier Transform and three kinds of geometric features were used to represent shape features, color moments that consist of mean, standard deviation, skewness were used to represent color features, texture features are extracted from GLCMs, and vein features were added to improve performance of the identification system. The identification system uses Probabilistic Neural Network (PNN) as a classifier. The result shows that the system gives average accuracy of 93.0833% for 60 kinds of foliage plants. version:1
arxiv-1401-4447 | Leaf Classification Using Shape, Color, and Texture Features | http://arxiv.org/abs/1401.4447 | id:1401.4447 author:Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa category:cs.CV cs.CY  published:2013-11-20 summary:Several methods to identify plants have been proposed by several researchers. Commonly, the methods did not capture color information, because color was not recognized as an important aspect to the identification. In this research, shape and vein, color, and texture features were incorporated to classify a leaf. In this case, a neural network called Probabilistic Neural network (PNN) was used as a classifier. The experimental result shows that the method for classification gives average accuracy of 93.75% when it was tested on Flavia dataset, that contains 32 kinds of plant leaves. It means that the method gives better performance compared to the original work. version:1
arxiv-1401-3584 | Experiments of Distance Measurements in a Foliage Plant Retrieval System | http://arxiv.org/abs/1401.3584 | id:1401.3584 author:Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa category:cs.CV  published:2013-11-20 summary:One of important components in an image retrieval system is selecting a distance measure to compute rank between two objects. In this paper, several distance measures were researched to implement a foliage plant retrieval system. Sixty kinds of foliage plants with various leaf color and shape were used to test the performance of 7 different kinds of distance measures: city block distance, Euclidean distance, Canberra distance, Bray-Curtis distance, x2 statistics, Jensen Shannon divergence and Kullback Leibler divergence. The results show that city block and Euclidean distance measures gave the best performance among the others. version:1
arxiv-1311-2694 | Hypothesis Testing for Automated Community Detection in Networks | http://arxiv.org/abs/1311.2694 | id:1311.2694 author:Peter J. Bickel, Purnamrita Sarkar category:stat.ML cs.LG cs.SI physics.soc-ph  published:2013-11-12 summary:Community detection in networks is a key exploratory tool with applications in a diverse set of areas, ranging from finding communities in social and biological networks to identifying link farms in the World Wide Web. The problem of finding communities or clusters in a network has received much attention from statistics, physics and computer science. However, most clustering algorithms assume knowledge of the number of clusters k. In this paper we propose to automatically determine k in a graph generated from a Stochastic Blockmodel. Our main contribution is twofold; first, we theoretically establish the limiting distribution of the principal eigenvalue of the suitably centered and scaled adjacency matrix, and use that distribution for our hypothesis test. Secondly, we use this test to design a recursive bipartitioning algorithm. Using quantifiable classification tasks on real world networks with ground truth, we show that our algorithm outperforms existing probabilistic models for learning overlapping clusters, and on unlabeled networks, we show that we uncover nested community structure. version:2
arxiv-1311-4833 | Domain Adaptation of Majority Votes via Perturbed Variation-based Label Transfer | http://arxiv.org/abs/1311.4833 | id:1311.4833 author:Emilie Morvant category:stat.ML cs.LG  published:2013-11-19 summary:We tackle the PAC-Bayesian Domain Adaptation (DA) problem. This arrives when one desires to learn, from a source distribution, a good weighted majority vote (over a set of classifiers) on a different target distribution. In this context, the disagreement between classifiers is known crucial to control. In non-DA supervised setting, a theoretical bound - the C-bound - involves this disagreement and leads to a majority vote learning algorithm: MinCq. In this work, we extend MinCq to DA by taking advantage of an elegant divergence between distribution called the Perturbed Varation (PV). Firstly, justified by a new formulation of the C-bound, we provide to MinCq a target sample labeled thanks to a PV-based self-labeling focused on regions where the source and target marginal distributions are closer. Secondly, we propose an original process for tuning the hyperparameters. Our framework shows very promising results on a toy problem. version:1
arxiv-1308-6498 | Universal Approximation Using Shuffled Linear Models | http://arxiv.org/abs/1308.6498 | id:1308.6498 author:Laurens Bliek category:math.DS cs.NE  published:2013-08-29 summary:This paper proposes a specific type of Local Linear Model, the Shuffled Linear Model (SLM), that can be used as a universal approximator. Local operating points are chosen randomly and linear models are used to approximate a function or system around these points. The model can also be interpreted as an extension to Extreme Learning Machines with Radial Basis Function nodes, or as a specific way of using Takagi-Sugeno fuzzy models. Using the available theory of Extreme Learning Machines, universal approximation of the SLM and an upper bound on the number of models are proved mathematically, and an efficient algorithm is proposed. version:2
arxiv-1401-6129 | Image enhancement using fusion by wavelet transform and laplacian pyramid | http://arxiv.org/abs/1401.6129 | id:1401.6129 author:S. M. Mukane, Y. S. Ghodake, P. S. Khandagle category:cs.CV  published:2013-11-19 summary:The idea of combining multiple image modalities to provide a single, enhanced image is well established different fusion methods have been proposed in literature. This paper is based on image fusion using laplacian pyramid and wavelet transform method. Images of same size are used for experimentation. Images used for the experimentation are standard images and averaging filter is used of equal weights in original images to burl. Performance of image fusion technique is measured by mean square error, normalized absolute error and peak signal to noise ratio. From the performance analysis it has been observed that MSE is decreased in case of both the methods where as PSNR increased, NAE decreased in case of laplacian pyramid where as constant for wavelet transform method. version:1
arxiv-1111-5280 | Stochastic gradient descent on Riemannian manifolds | http://arxiv.org/abs/1111.5280 | id:1111.5280 author:Silvere Bonnabel category:math.OC cs.LG stat.ML  published:2011-11-22 summary:Stochastic gradient descent is a simple approach to find the local minima of a cost function whose evaluations are corrupted by noise. In this paper, we develop a procedure extending stochastic gradient descent algorithms to the case where the function is defined on a Riemannian manifold. We prove that, as in the Euclidian case, the gradient descent algorithm converges to a critical point of the cost function. The algorithm has numerous potential applications, and is illustrated here by four examples. In particular a novel gossip algorithm on the set of covariance matrices is derived and tested numerically. version:4
arxiv-1401-6108 | Face Verification Using Kernel Principle Component Analysis | http://arxiv.org/abs/1401.6108 | id:1401.6108 author:V. Karthikeyan, Manjupriya, C. K. Chithra, M. Divya category:cs.CV  published:2013-11-19 summary:In the beginning stage, face verification is done using easy method of geometric algorithm models, but the verification route has now developed into a scientific progress of complicated geometric representation and matching process. In modern time the skill have enhanced face detection system into the vigorous focal point. Researchers currently undergoing strong research on finding face recognition system for wider area information taken under hysterical elucidation dissimilarity. The proposed face recognition system consists of a narrative exposition indiscreet preprocessing method, a hybrid Fourier-based facial feature extraction and a score fusion scheme. We take in conventional the face detection in unlike cheer up circumstances and at unusual setting. Image processing, Image detection, Feature removal and Face detection are the methods used for Face Verification System . This paper focuses mainly on the issue of toughness to lighting variations. The proposed system has obtained an average of verification rate on Two-Dimensional images under different lightening conditions. version:1
arxiv-1401-6112 | Face Verification System based on Integral Normalized Gradient Image(INGI) | http://arxiv.org/abs/1401.6112 | id:1401.6112 author:V. Karthikeyan, M. Divya, C. K. Chithra, K. Manju Priya category:cs.CV  published:2013-11-19 summary:Character identification plays a vital role in the contemporary world of Image processing. It can solve many composite problems and makes humans work easier. An instance is Handwritten Character detection. Handwritten recognition is not a novel expertise, but it has not gained community notice until Now. The eventual aim of designing Handwritten Character recognition structure with an accurateness rate of 100% is pretty illusionary. Tamil Handwritten Character recognition system uses the Neural Networks to distinguish them. Neural Network and structural characteristics are used to instruct and recognize written characters. After training and testing the exactness rate reached 99%. This correctness rate is extremely high. In this paper we are exploring image processing through the Hilditch algorithm foundation and structural characteristics of a character in the image. And we recognized some character of the Tamil language, and we are trying to identify all the character of Tamil In our future works. version:1
arxiv-1311-4669 | Nonparametric Bayes dynamic modeling of relational data | http://arxiv.org/abs/1311.4669 | id:1311.4669 author:Daniele Durante, David B. Dunson category:stat.ML  published:2013-11-19 summary:Symmetric binary matrices representing relations among entities are commonly collected in many areas. Our focus is on dynamically evolving binary relational matrices, with interest being in inference on the relationship structure and prediction. We propose a nonparametric Bayesian dynamic model, which reduces dimensionality in characterizing the binary matrix through a lower-dimensional latent space representation, with the latent coordinates evolving in continuous time via Gaussian processes. By using a logistic mapping function from the probability matrix space to the latent relational space, we obtain a flexible and computational tractable formulation. Employing P\`olya-Gamma data augmentation, an efficient Gibbs sampler is developed for posterior computation, with the dimension of the latent space automatically inferred. We provide some theoretical results on flexibility of the model, and illustrate performance via simulation experiments. We also consider an application to co-movements in world financial markets. version:1
arxiv-1311-4665 | Analysis of Farthest Point Sampling for Approximating Geodesics in a Graph | http://arxiv.org/abs/1311.4665 | id:1311.4665 author:Pegah Kamousi, Sylvain Lazard, Anil Maheshwari, Stefanie Wuhrer category:cs.CG cs.CV cs.GR  published:2013-11-19 summary:A standard way to approximate the distance between any two vertices $p$ and $q$ on a mesh is to compute, in the associated graph, a shortest path from $p$ to $q$ that goes through one of $k$ sources, which are well-chosen vertices. Precomputing the distance between each of the $k$ sources to all vertices of the graph yields an efficient computation of approximate distances between any two vertices. One standard method for choosing $k$ sources, which has been used extensively and successfully for isometry-invariant surface processing, is the so-called Farthest Point Sampling (FPS), which starts with a random vertex as the first source, and iteratively selects the farthest vertex from the already selected sources. In this paper, we analyze the stretch factor $\mathcal{F}_{FPS}$ of approximate geodesics computed using FPS, which is the maximum, over all pairs of distinct vertices, of their approximated distance over their geodesic distance in the graph. We show that $\mathcal{F}_{FPS}$ can be bounded in terms of the minimal value $\mathcal{F}^*$ of the stretch factor obtained using an optimal placement of $k$ sources as $\mathcal{F}_{FPS}\leq 2 r_e^2 \mathcal{F}^*+ 2 r_e^2 + 8 r_e + 1$, where $r_e$ is the ratio of the lengths of the longest and the shortest edges of the graph. This provides some evidence explaining why farthest point sampling has been used successfully for isometry-invariant shape processing. Furthermore, we show that it is NP-complete to find $k$ sources that minimize the stretch factor. version:1
arxiv-1311-6740 | Hilditchs Algorithm Based Tamil Character Recognition | http://arxiv.org/abs/1311.6740 | id:1311.6740 author:V. Karthikeyan category:cs.CV  published:2013-11-19 summary:Character identification plays a vital role in the contemporary world of Image processing. It can solve many composite problems and makes humans work easier. An instance is Handwritten Character detection. Handwritten recognition is not a novel expertise, but it has not gained community notice until Now. The eventual aim of designing Handwritten Character recognition structure with an accurateness rate of 100% is pretty illusionary. Tamil Handwritten Character recognition system uses the Neural Networks to distinguish them. Neural Network and structural characteristics are used to instruct and recognize written characters. After training and testing the exactness rate reached 99%. This correctness rate is extremely high. In this paper we are exploring image processing through the Hilditch algorithm foundation and structural characteristics of a character in the image. And we recognized some character of the Tamil language, and we are trying to identify all the character of Tamil In our future works. version:1
arxiv-1311-4643 | Near-Optimal Entrywise Sampling for Data Matrices | http://arxiv.org/abs/1311.4643 | id:1311.4643 author:Dimitris Achlioptas, Zohar Karnin, Edo Liberty category:cs.LG cs.IT cs.NA math.IT stat.ML  published:2013-11-19 summary:We consider the problem of selecting non-zero entries of a matrix $A$ in order to produce a sparse sketch of it, $B$, that minimizes $\ A-B\ _2$. For large $m \times n$ matrices, such that $n \gg m$ (for example, representing $n$ observations over $m$ attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding $A$. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with $O(1)$ computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution. Note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model. version:1
arxiv-1311-4639 | Post-Proceedings of the First International Workshop on Learning and Nonmonotonic Reasoning | http://arxiv.org/abs/1311.4639 | id:1311.4639 author:Katsumi Inoue, Chiaki Sakama category:cs.AI cs.LG cs.LO  published:2013-11-19 summary:Knowledge Representation and Reasoning and Machine Learning are two important fields in AI. Nonmonotonic logic programming (NMLP) and Answer Set Programming (ASP) provide formal languages for representing and reasoning with commonsense knowledge and realize declarative problem solving in AI. On the other side, Inductive Logic Programming (ILP) realizes Machine Learning in logic programming, which provides a formal background to inductive learning and the techniques have been applied to the fields of relational learning and data mining. Generally speaking, NMLP and ASP realize nonmonotonic reasoning while lack the ability of learning. By contrast, ILP realizes inductive learning while most techniques have been developed under the classical monotonic logic. With this background, some researchers attempt to combine techniques in the context of nonmonotonic ILP. Such combination will introduce a learning mechanism to programs and would exploit new applications on the NMLP side, while on the ILP side it will extend the representation language and enable us to use existing solvers. Cross-fertilization between learning and nonmonotonic reasoning can also occur in such as the use of answer set solvers for ILP, speed-up learning while running answer set solvers, learning action theories, learning transition rules in dynamical systems, abductive learning, learning biological networks with inhibition, and applications involving default and negation. This workshop is the first attempt to provide an open forum for the identification of problems and discussion of possible collaborations among researchers with complementary expertise. The workshop was held on September 15th of 2013 in Corunna, Spain. This post-proceedings contains five technical papers (out of six accepted papers) and the abstract of the invited talk by Luc De Raedt. version:1
arxiv-1310-2997 | Bandits with Switching Costs: T^{2/3} Regret | http://arxiv.org/abs/1310.2997 | id:1310.2997 author:Ofer Dekel, Jian Ding, Tomer Koren, Yuval Peres category:cs.LG math.PR  published:2013-10-11 summary:We study the adversarial multi-armed bandit problem in a setting where the player incurs a unit cost each time he switches actions. We prove that the player's $T$-round minimax regret in this setting is $\widetilde{\Theta}(T^{2/3})$, thereby closing a fundamental gap in our understanding of learning with bandit feedback. In the corresponding full-information version of the problem, the minimax regret is known to grow at a much slower rate of $\Theta(\sqrt{T})$. The difference between these two rates provides the \emph{first} indication that learning with bandit feedback can be significantly harder than learning with full-information feedback (previous results only showed a different dependence on the number of actions, but not on $T$.) In addition to characterizing the inherent difficulty of the multi-armed bandit problem with switching costs, our results also resolve several other open problems in online learning. One direct implication is that learning with bandit feedback against bounded-memory adaptive adversaries has a minimax regret of $\widetilde{\Theta}(T^{2/3})$. Another implication is that the minimax regret of online learning in adversarial Markov decision processes (MDPs) is $\widetilde{\Theta}(T^{2/3})$. The key to all of our results is a new randomized construction of a multi-scale random walk, which is of independent interest and likely to prove useful in additional settings. version:2
arxiv-1202-3663 | Guaranteed clustering and biclustering via semidefinite programming | http://arxiv.org/abs/1202.3663 | id:1202.3663 author:Brendan P. W. Ames category:math.OC cs.LG  published:2012-02-16 summary:Identifying clusters of similar objects in data plays a significant role in a wide range of applications. As a model problem for clustering, we consider the densest k-disjoint-clique problem, whose goal is to identify the collection of k disjoint cliques of a given weighted complete graph maximizing the sum of the densities of the complete subgraphs induced by these cliques. In this paper, we establish conditions ensuring exact recovery of the densest k cliques of a given graph from the optimal solution of a particular semidefinite program. In particular, the semidefinite relaxation is exact for input graphs corresponding to data consisting of k large, distinct clusters and a smaller number of outliers. This approach also yields a semidefinite relaxation for the biclustering problem with similar recovery guarantees. Given a set of objects and a set of features exhibited by these objects, biclustering seeks to simultaneously group the objects and features according to their expression levels. This problem may be posed as partitioning the nodes of a weighted bipartite complete graph such that the sum of the densities of the resulting bipartite complete subgraphs is maximized. As in our analysis of the densest k-disjoint-clique problem, we show that the correct partition of the objects and features can be recovered from the optimal solution of a semidefinite program in the case that the given data consists of several disjoint sets of objects exhibiting similar features. Empirical evidence from numerical experiments supporting these theoretical guarantees is also provided. version:6
arxiv-1311-5595 | On Nonrigid Shape Similarity and Correspondence | http://arxiv.org/abs/1311.5595 | id:1311.5595 author:Alon Shtern, Ron Kimmel category:cs.CV cs.GR  published:2013-11-18 summary:An important operation in geometry processing is finding the correspondences between pairs of shapes. The Gromov-Hausdorff distance, a measure of dissimilarity between metric spaces, has been found to be highly useful for nonrigid shape comparison. Here, we explore the applicability of related shape similarity measures to the problem of shape correspondence, adopting spectral type distances. We propose to evaluate the spectral kernel distance, the spectral embedding distance and the novel spectral quasi-conformal distance, comparing the manifolds from different viewpoints. By matching the shapes in the spectral domain, important attributes of surface structure are being aligned. For the purpose of testing our ideas, we introduce a fully automatic framework for finding intrinsic correspondence between two shapes. The proposed method achieves state-of-the-art results on the Princeton isometric shape matching protocol applied, as usual, to the TOSCA and SCAPE benchmarks. version:1
arxiv-1309-7340 | Early Stage Influenza Detection from Twitter | http://arxiv.org/abs/1309.7340 | id:1309.7340 author:Jiwei Li, Claire Cardie category:cs.SI cs.CL  published:2013-09-27 summary:Influenza is an acute respiratory illness that occurs virtually every year and results in substantial disease, death and expense. Detection of Influenza in its earliest stage would facilitate timely action that could reduce the spread of the illness. Existing systems such as CDC and EISS which try to collect diagnosis data, are almost entirely manual, resulting in about two-week delays for clinical data acquisition. Twitter, a popular microblogging service, provides us with a perfect source for early-stage flu detection due to its real- time nature. For example, when a flu breaks out, people that get the flu may post related tweets which enables the detection of the flu breakout promptly. In this paper, we investigate the real-time flu detection problem on Twitter data by proposing Flu Markov Network (Flu-MN): a spatio-temporal unsupervised Bayesian algorithm based on a 4 phase Markov Network, trying to identify the flu breakout at the earliest stage. We test our model on real Twitter datasets from the United States along with baselines in multiple applications, such as real-time flu breakout detection, future epidemic phase prediction, or Influenza-like illness (ILI) physician visits. Experimental results show the robustness and effectiveness of our approach. We build up a real time flu reporting system based on the proposed approach, and we are hopeful that it would help government or health organizations in identifying flu outbreaks and facilitating timely actions to decrease unnecessary mortality. version:3
arxiv-1312-1909 | From Maxout to Channel-Out: Encoding Information on Sparse Pathways | http://arxiv.org/abs/1312.1909 | id:1312.1909 author:Qi Wang, Joseph JaJa category:cs.NE cs.CV cs.LG stat.ML  published:2013-11-18 summary:Motivated by an important insight from neural science, we propose a new framework for understanding the success of the recently proposed "maxout" networks. The framework is based on encoding information on sparse pathways and recognizing the correct pathway at inference time. Elaborating further on this insight, we propose a novel deep network architecture, called "channel-out" network, which takes a much better advantage of sparse pathway encoding. In channel-out networks, pathways are not only formed a posteriori, but they are also actively selected according to the inference outputs from the lower layers. From a mathematical perspective, channel-out networks can represent a wider class of piece-wise continuous functions, thereby endowing the network with more expressive power than that of maxout networks. We test our channel-out networks on several well-known image classification benchmarks, setting new state-of-the-art performance on CIFAR-100 and STL-10, which represent some of the "harder" image classification benchmarks. version:1
arxiv-1311-4319 | Ranking Algorithms by Performance | http://arxiv.org/abs/1311.4319 | id:1311.4319 author:Lars Kotthoff category:cs.AI cs.LG  published:2013-11-18 summary:A common way of doing algorithm selection is to train a machine learning model and predict the best algorithm from a portfolio to solve a particular problem. While this method has been highly successful, choosing only a single algorithm has inherent limitations -- if the choice was bad, no remedial action can be taken and parallelism cannot be exploited, to name but a few problems. In this paper, we investigate how to predict the ranking of the portfolio algorithms on a particular problem. This information can be used to choose the single best algorithm, but also to allocate resources to the algorithms according to their rank. We evaluate a range of approaches to predict the ranking of a set of algorithms on a problem. We furthermore introduce a framework for categorizing ranking predictions that allows to judge the expressiveness of the predictive output. Our experimental evaluation demonstrates on a range of data sets from the literature that it is beneficial to consider the relationship between algorithms when predicting rankings. We furthermore show that relatively naive approaches deliver rankings of good quality already. version:1
arxiv-1306-0237 | Guided Random Forest in the RRF Package | http://arxiv.org/abs/1306.0237 | id:1306.0237 author:Houtao Deng category:cs.LG  published:2013-06-02 summary:Random Forest (RF) is a powerful supervised learner and has been popularly used in many applications such as bioinformatics. In this work we propose the guided random forest (GRF) for feature selection. Similar to a feature selection method called guided regularized random forest (GRRF), GRF is built using the importance scores from an ordinary RF. However, the trees in GRRF are built sequentially, are highly correlated and do not allow for parallel computing, while the trees in GRF are built independently and can be implemented in parallel. Experiments on 10 high-dimensional gene data sets show that, with a fixed parameter value (without tuning the parameter), RF applied to features selected by GRF outperforms RF applied to all features on 9 data sets and 7 of them have significant differences at the 0.05 level. Therefore, both accuracy and interpretability are significantly improved. GRF selects more features than GRRF, however, leads to better classification accuracy. Note in this work the guided random forest is guided by the importance scores from an ordinary random forest, however, it can also be guided by other methods such as human insights (by specifying $\lambda_i$). GRF can be used in "RRF" v1.4 (and later versions), a package that also includes the regularized random forest methods. version:3
arxiv-1311-4296 | Reflection methods for user-friendly submodular optimization | http://arxiv.org/abs/1311.4296 | id:1311.4296 author:Stefanie Jegelka, Francis Bach, Suvrit Sra category:cs.LG cs.NA cs.RO math.OC  published:2013-11-18 summary:Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efficient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reflections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the benefits of our method on two image segmentation tasks. version:1
arxiv-1311-4252 | Contour polygonal approximation using shortest path in networks | http://arxiv.org/abs/1311.4252 | id:1311.4252 author:André Ricardo Backes, Dalcimar Casanova, Odemir Martinez Bruno category:physics.comp-ph cs.CV  published:2013-11-18 summary:Contour polygonal approximation is a simplified representation of a contour by line segments, so that the main characteristics of the contour remain in a small number of line segments. This paper presents a novel method for polygonal approximation based on the Complex Networks theory. We convert each point of the contour into a vertex, so that we model a regular network. Then we transform this network into a Small-World Complex Network by applying some transformations over its edges. By analyzing of network properties, especially the geodesic path, we compute the polygonal approximation. The paper presents the main characteristics of the method, as well as its functionality. We evaluate the proposed method using benchmark contours, and compare its results with other polygonal approximation methods. version:1
arxiv-1311-4235 | On the definition of a general learning system with user-defined operators | http://arxiv.org/abs/1311.4235 | id:1311.4235 author:Fernando Martínez-Plumed, Cèsar Ferri, José Hernández-Orallo, María-José Ramírez-Quintana category:cs.LG  published:2013-11-18 summary:In this paper, we push forward the idea of machine learning systems whose operators can be modified and fine-tuned for each problem. This allows us to propose a learning paradigm where users can write (or adapt) their operators, according to the problem, data representation and the way the information should be navigated. To achieve this goal, data instances, background knowledge, rules, programs and operators are all written in the same functional language, Erlang. Since changing operators affect how the search space needs to be explored, heuristics are learnt as a result of a decision process based on reinforcement learning where each action is defined as a choice of operator and rule. As a result, the architecture can be seen as a 'system for writing machine learning systems' or to explore new operators where the policy reuse (as a kind of transfer learning) is allowed. States and actions are represented in a Q matrix which is actually a table, from which a supervised model is learnt. This makes it possible to have a more flexible mapping between old and new problems, since we work with an abstraction of rules and actions. We include some examples sharing reuse and the application of the system gErl to IQ problems. In order to evaluate gErl, we will test it against some structured problems: a selection of IQ test tasks and some experiments on some structured prediction problems (list patterns). version:1
arxiv-1311-4150 | Towards Big Topic Modeling | http://arxiv.org/abs/1311.4150 | id:1311.4150 author:Jian-Feng Yan, Jia Zeng, Zhi-Qiang Liu, Yang Gao category:cs.LG cs.DC cs.IR stat.ML  published:2013-11-17 summary:To solve the big topic modeling problem, we need to reduce both time and space complexities of batch latent Dirichlet allocation (LDA) algorithms. Although parallel LDA algorithms on the multi-processor architecture have low time and space complexities, their communication costs among processors often scale linearly with the vocabulary size and the number of topics, leading to a serious scalability problem. To reduce the communication complexity among processors for a better scalability, we propose a novel communication-efficient parallel topic modeling architecture based on power law, which consumes orders of magnitude less communication time when the number of topics is large. We combine the proposed communication-efficient parallel architecture with the online belief propagation (OBP) algorithm referred to as POBP for big topic modeling tasks. Extensive empirical results confirm that POBP has the following advantages to solve the big topic modeling problem: 1) high accuracy, 2) communication-efficient, 3) fast speed, and 4) constant memory usage when compared with recent state-of-the-art parallel LDA algorithms on the multi-processor architecture. version:1
arxiv-1310-8428 | Multilabel Classification through Random Graph Ensembles | http://arxiv.org/abs/1310.8428 | id:1310.8428 author:Hongyu Su, Juho Rousu category:cs.LG  published:2013-10-31 summary:We present new methods for multilabel classification, relying on ensemble learning on a collection of random output graphs imposed on the multilabel and a kernel-based structured output learner as the base classifier. For ensemble learning, differences among the output graphs provide the required base classifier diversity and lead to improved performance in the increasing size of the ensemble. We study different methods of forming the ensemble prediction, including majority voting and two methods that perform inferences over the graph structures before or after combining the base models into the ensemble. We compare the methods against the state-of-the-art machine learning approaches on a set of heterogeneous multilabel benchmark problems, including multilabel AdaBoost, convex multitask feature learning, as well as single target learning approaches represented by Bagging and SVM. In our experiments, the random graph ensembles are very competitive and robust, ranking first or second on most of the datasets. Overall, our results show that random graph ensembles are viable alternatives to flat multilabel and multitask learners. version:2
arxiv-1311-4088 | The Optimization of Running Queries in Relational Databases Using ANT-Colony Algorithm | http://arxiv.org/abs/1311.4088 | id:1311.4088 author:Adel Alinezhad Kolaei, Marzieh Ahmadzadeh category:cs.DB cs.NE  published:2013-11-16 summary:The issue of optimizing queries is a cost-sensitive process and with respect to the number of associated tables in a query, its number of permutations grows exponentially. On one hand, in comparison with other operators in relational database, join operator is the most difficult and complicated one in terms of optimization for reducing its runtime. Accordingly, various algorithms have so far been proposed to solve this problem. On the other hand, the success of any database management system (DBMS) means exploiting the query model. In the current paper, the heuristic ant algorithm has been proposed to solve this problem and improve the runtime of join operation. Experiments and observed results reveal the efficiency of this algorithm compared to its similar algorithms. version:1
arxiv-1311-4086 | A hybrid decision support system : application on healthcare | http://arxiv.org/abs/1311.4086 | id:1311.4086 author:Abdelhak Mansoul, Baghdad Atmani, Sofia Benbelkacem category:cs.AI cs.LG  published:2013-11-16 summary:Many systems based on knowledge, especially expert systems for medical decision support have been developed. Only systems are based on production rules, and cannot learn and evolve only by updating them. In addition, taking into account several criteria induces an exorbitant number of rules to be injected into the system. It becomes difficult to translate medical knowledge or a support decision as a simple rule. Moreover, reasoning based on generic cases became classic and can even reduce the range of possible solutions. To remedy that, we propose an approach based on using a multi-criteria decision guided by a case-based reasoning (CBR) approach. version:1
arxiv-1302-2606 | A new bio-inspired method for remote sensing imagery classification | http://arxiv.org/abs/1302.2606 | id:1302.2606 author:Amghar Yasmina Teldja, Fizazi Hadria category:cs.NE cs.CV  published:2013-02-11 summary:The problem of supervised classification of the satellite image is considered to be the task of grouping pixels into a number of homogeneous regions in space intensity. This paper proposes a novel approach that combines a radial basic function clustering network with a growing neural gas include utility factor classifier to yield improved solutions, obtained with previous networks. The double objective technique is first used to the development of a method to perform the satellite images classification, and finally, the implementation to address the issue of the number of nodes in the hidden layer of the classic Radial Basis functions network. Results demonstrating the effectiveness of the proposed technique are provided for numeric remote sensing imagery. Moreover, the remotely sensed image of Oran city in Algeria has been classified using the proposed technique to establish its utility. version:2
arxiv-1311-4033 | A Comparative Study of Histogram Equalization Based Image Enhancement Techniques for Brightness Preservation and Contrast Enhancement | http://arxiv.org/abs/1311.4033 | id:1311.4033 author:Omprakash Patel, Yogendra P. S. Maravi, Sanjeev Sharma category:cs.CV  published:2013-11-16 summary:Histogram Equalization is a contrast enhancement technique in the image processing which uses the histogram of image. However histogram equalization is not the best method for contrast enhancement because the mean brightness of the output image is significantly different from the input image. There are several extensions of histogram equalization has been proposed to overcome the brightness preservation challenge. Contrast enhancement using brightness preserving bi-histogram equalization (BBHE) and Dualistic sub image histogram equalization (DSIHE) which divides the image histogram into two parts based on the input mean and median respectively then equalizes each sub histogram independently. This paper provides review of different popular histogram equalization techniques and experimental study based on the absolute mean brightness error (AMBE), peak signal to noise ratio (PSNR), Structure similarity index (SSI) and Entropy. version:1
arxiv-1109-1077 | Nonparametric Link Prediction in Large Scale Dynamic Networks | http://arxiv.org/abs/1109.1077 | id:1109.1077 author:Purnamrita Sarkar, Deepayan Chakrabarti, Michael Jordan category:stat.ML cs.SI physics.soc-ph  published:2011-09-06 summary:We propose a nonparametric approach to link prediction in large-scale dynamic networks. Our model uses graph-based features of pairs of nodes as well as those of their local neighborhoods to predict whether those nodes will be linked at each time step. The model allows for different types of evolution in different parts of the graph (e.g, growing or shrinking communities). We focus on large-scale graphs and present an implementation of our model that makes use of locality-sensitive hashing to allow it to be scaled to large problems. Experiments with simulated data as well as five real-world dynamic graphs show that we outperform the state of the art, especially when sharp fluctuations or nonlinearities are present. We also establish theoretical properties of our estimator, in particular consistency and weak convergence, the latter making use of an elaboration of Stein's method for dependency graphs. version:3
arxiv-1305-3932 | Inferring the Origin Locations of Tweets with Quantitative Confidence | http://arxiv.org/abs/1305.3932 | id:1305.3932 author:Reid Priedhorsky, Aron Culotta, Sara Y. Del Valle category:cs.SI cs.HC cs.LG  published:2013-05-16 summary:Social Internet content plays an increasingly critical role in many domains, including public health, disaster management, and politics. However, its utility is limited by missing geographic information; for example, fewer than 1.6% of Twitter messages (tweets) contain a geotag. We propose a scalable, content-based approach to estimate the location of tweets using a novel yet simple variant of gaussian mixture models. Further, because real-world applications depend on quantified uncertainty for such estimates, we propose novel metrics of accuracy, precision, and calibration, and we evaluate our approach accordingly. Experiments on 13 million global, comprehensively multi-lingual tweets show that our approach yields reliable, well-calibrated results competitive with previous computationally intensive methods. We also show that a relatively small number of training data are required for good estimates (roughly 30,000 tweets) and models are quite time-invariant (effective on tweets many weeks newer than the training set). Finally, we show that toponyms and languages with small geographic footprint provide the most useful location signals. version:3
arxiv-1311-3405 | The STONE Transform: Multi-Resolution Image Enhancement and Real-Time Compressive Video | http://arxiv.org/abs/1311.3405 | id:1311.3405 author:Tom Goldstein, Lina Xu, Kevin F. Kelly, Richard Baraniuk category:cs.CV  published:2013-11-14 summary:Compressed sensing enables the reconstruction of high-resolution signals from under-sampled data. While compressive methods simplify data acquisition, they require the solution of difficult recovery problems to make use of the resulting measurements. This article presents a new sensing framework that combines the advantages of both conventional and compressive sensing. Using the proposed \stone transform, measurements can be reconstructed instantly at Nyquist rates at any power-of-two resolution. The same data can then be "enhanced" to higher resolutions using compressive methods that leverage sparsity to "beat" the Nyquist limit. The availability of a fast direct reconstruction enables compressive measurements to be processed on small embedded devices. We demonstrate this by constructing a real-time compressive video camera. version:2
arxiv-1311-3961 | HEVAL: Yet Another Human Evaluation Metric | http://arxiv.org/abs/1311.3961 | id:1311.3961 author:Nisheeth Joshi, Iti Mathur, Hemant Darbari, Ajai Kumar category:cs.CL  published:2013-11-15 summary:Machine translation evaluation is a very important activity in machine translation development. Automatic evaluation metrics proposed in literature are inadequate as they require one or more human reference translations to compare them with output produced by machine translation. This does not always give accurate results as a text can have several different translations. Human evaluation metrics, on the other hand, lacks inter-annotator agreement and repeatability. In this paper we have proposed a new human evaluation metric which addresses these issues. Moreover this metric also provides solid grounds for making sound assumptions on the quality of the text produced by a machine translation. version:1
arxiv-1311-3618 | Describing Textures in the Wild | http://arxiv.org/abs/1311.3618 | id:1311.3618 author:Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, Andrea Vedaldi category:cs.CV  published:2013-11-14 summary:Patterns and textures are defining characteristics of many natural objects: a shirt can be striped, the wings of a butterfly can be veined, and the skin of an animal can be scaly. Aiming at supporting this analytical dimension in image understanding, we address the challenging problem of describing textures with semantic attributes. We identify a rich vocabulary of forty-seven texture terms and use them to describe a large dataset of patterns collected in the wild.The resulting Describable Textures Dataset (DTD) is the basis to seek for the best texture representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition the Improved Fisher Vector (IFV) and show that, surprisingly, it outperforms specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also show that the describable attributes are excellent texture descriptors, transferring between datasets and tasks; in particular, combined with IFV, they significantly outperform the state-of-the-art by more than 8 percent on both FMD and KTHTIPS-2b benchmarks. We also demonstrate that they produce intuitive descriptions of materials and Internet images. version:2
arxiv-1311-3840 | Mixing Energy Models in Genetic Algorithms for On-Lattice Protein Structure Prediction | http://arxiv.org/abs/1311.3840 | id:1311.3840 author:Mahmood A. Rashid, M. A. Hakim Newton, Md. Tamjidul Hoque, Abdul Sattar category:cs.CE cs.NE  published:2013-11-15 summary:Protein structure prediction (PSP) is computationally a very challenging problem. The challenge largely comes from the fact that the energy function that needs to be minimised in order to obtain the native structure of a given protein is not clearly known. A high resolution 20x20 energy model could better capture the behaviour of the actual energy function than a low resolution energy model such as hydrophobic polar. However, the fine grained details of the high resolution interaction energy matrix are often not very informative for guiding the search. In contrast, a low resolution energy model could effectively bias the search towards certain promising directions. In this paper, we develop a genetic algorithm that mainly uses a high resolution energy model for protein structure evaluation but uses a low resolution HP energy model in focussing the search towards exploring structures that have hydrophobic cores. We experimentally show that this mixing of energy models leads to significant lower energy structures compared to the state-of-the-art results. version:1
arxiv-1307-4990 | Video Text Localization using Wavelet and Shearlet Transforms | http://arxiv.org/abs/1307.4990 | id:1307.4990 author:Purnendu Banerjee, B. B. Chaudhuri category:cs.CV  published:2013-07-18 summary:Text in video is useful and important in indexing and retrieving the video documents efficiently and accurately. In this paper, we present a new method of text detection using a combined dictionary consisting of wavelets and a recently introduced transform called shearlets. Wavelets provide optimally sparse expansion for point-like structures and shearlets provide optimally sparse expansions for curve-like structures. By combining these two features we have computed a high frequency sub-band to brighten the text part. Then K-means clustering is used for obtaining text pixels from the Standard Deviation (SD) of combined coefficient of wavelets and shearlets as well as the union of wavelets and shearlets features. Text parts are obtained by grouping neighboring regions based on geometric properties of the classified output frame of unsupervised K-means classification. The proposed method tested on a standard as well as newly collected database shows to be superior to some existing methods. version:2
arxiv-1311-3808 | Periodicity Extraction using Superposition of Distance Matching Function and One-dimensional Haar Wavelet Transform | http://arxiv.org/abs/1311.3808 | id:1311.3808 author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV I.0; I.2.7  published:2013-11-15 summary:Periodicity of a texture is one of the important visual characteristics and is often used as a measure for textural discrimination at the structural level. Knowledge about periodicity of a texture is very essential in the field of texture synthesis and texture compression and also in the design of frieze and wall papers. In this paper, we propose a method of periodicity extraction from noisy images based on superposition of distance matching function (DMF) and wavelet decomposition without de-noising the test images. Overall DMFs are subjected to single-level Haar wavelet decomposition to obtain approximate and detailed coefficients. Extracted coefficients help in determination of periodicities in row and column directions. We illustrate the usefulness and the effectiveness of the proposed method in a texture synthesis application. version:1
arxiv-1311-3735 | Ensemble Relational Learning based on Selective Propositionalization | http://arxiv.org/abs/1311.3735 | id:1311.3735 author:Nicola Di Mauro, Floriana Esposito category:cs.LG cs.AI  published:2013-11-15 summary:Dealing with structured data needs the use of expressive representation formalisms that, however, puts the problem to deal with the computational complexity of the machine learning process. Furthermore, real world domains require tools able to manage their typical uncertainty. Many statistical relational learning approaches try to deal with these problems by combining the construction of relevant relational features with a probabilistic tool. When the combination is static (static propositionalization), the constructed features are considered as boolean features and used offline as input to a statistical learner; while, when the combination is dynamic (dynamic propositionalization), the feature construction and probabilistic tool are combined into a single process. In this paper we propose a selective propositionalization method that search the optimal set of relational features to be used by a probabilistic learner in order to minimize a loss function. The new propositionalization approach has been combined with the random subspace ensemble method. Experiments on real-world datasets shows the validity of the proposed method. version:1
arxiv-1311-3709 | On Estimating Many Means, Selection Bias, and the Bootstrap | http://arxiv.org/abs/1311.3709 | id:1311.3709 author:Noah Simon, Richard Simon category:stat.ML stat.ME  published:2013-11-15 summary:With recent advances in high throughput technology, researchers often find themselves running a large number of hypothesis tests (thousands+) and esti- mating a large number of effect-sizes. Generally there is particular interest in those effects estimated to be most extreme. Unfortunately naive estimates of these effect-sizes (even after potentially accounting for multiplicity in a testing procedure) can be severely biased. In this manuscript we explore this bias from a frequentist perspective: we give a formal definition, and show that an oracle estimator using this bias dominates the naive maximum likelihood estimate. We give a resampling estimator to approximate this oracle, and show that it works well on simulated data. We also connect this to ideas in empirical Bayes. version:1
arxiv-1311-3674 | Evolutionary perspectives on collective decision making: Studying the implications of diversity and social network structure with agent-based simulations | http://arxiv.org/abs/1311.3674 | id:1311.3674 author:Hiroki Sayama, Shelley D. Dionne, Francis J. Yammarino category:cs.MA cs.NE cs.SI physics.soc-ph  published:2013-11-14 summary:Collective, especially group-based, managerial decision making is crucial in organizations. Using an evolutionary theory approach to collective decision making, agent-based simulations were conducted to investigate how collective decision making would be affected by the agents' diversity in problem understanding and/or behavior in discussion, as well as by their social network structure. Simulation results indicated that groups with consistent problem understanding tended to produce higher utility values of ideas and displayed better decision convergence, but only if there was no group-level bias in collective problem understanding. Simulation results also indicated the importance of balance between selection-oriented (i.e., exploitative) and variation-oriented (i.e., explorative) behaviors in discussion to achieve quality final decisions. Expanding the group size and introducing non-trivial social network structure generally improved the quality of ideas at the cost of decision convergence. Simulations with different social network topologies revealed that collective decision making on small-world networks with high local clustering tended to achieve highest decision quality more often than on random or scale-free networks. Implications of this evolutionary theory and simulation approach for future managerial research on collective, group, and multi-level decision making are discussed. version:1
arxiv-1311-3669 | Scalable Influence Estimation in Continuous-Time Diffusion Networks | http://arxiv.org/abs/1311.3669 | id:1311.3669 author:Nan Du, Le Song, Manuel Gomez Rodriguez, Hongyuan Zha category:cs.SI cs.LG H.2.8  published:2013-11-14 summary:If a piece of information is released from a media site, can it spread, in 1 month, to a million web pages? This influence estimation problem is very challenging since both the time-sensitive nature of the problem and the issue of scalability need to be addressed simultaneously. In this paper, we propose a randomized algorithm for influence estimation in continuous-time diffusion networks. Our algorithm can estimate the influence of every node in a network with V nodes and E edges to an accuracy of $\varepsilon$ using $n=O(1/\varepsilon^2)$ randomizations and up to logarithmic factors O(n E +n V ) computations. When used as a subroutine in a greedy influence maximization algorithm, our proposed method is guaranteed to find a set of nodes with an influence of at least (1-1/e)OPT-2$\varepsilon$, where OPT is the optimal value. Experiments on both synthetic and real-world data show that the proposed method can easily scale up to networks of millions of nodes while significantly improves over previous state-of-the-arts in terms of the accuracy of the estimated influence and the quality of the selected nodes in maximizing the influence. version:1
arxiv-1202-6590 | Uniform random generation of large acyclic digraphs | http://arxiv.org/abs/1202.6590 | id:1202.6590 author:Jack Kuipers, Giusi Moffa category:stat.CO math.ST stat.ML stat.TH  published:2012-02-29 summary:Directed acyclic graphs are the basic representation of the structure underlying Bayesian networks, which represent multivariate probability distributions. In many practical applications, such as the reverse engineering of gene regulatory networks, not only the estimation of model parameters but the reconstruction of the structure itself is of great interest. As well as for the assessment of different structure learning algorithms in simulation studies, a uniform sample from the space of directed acyclic graphs is required to evaluate the prevalence of certain structural features. Here we analyse how to sample acyclic digraphs uniformly at random through recursive enumeration, an approach previously thought too computationally involved. Based on complexity considerations, we discuss in particular how the enumeration directly provides an exact method, which avoids the convergence issues of the alternative Markov chain methods and is actually computationally much faster. The limiting behaviour of the distribution of acyclic digraphs then allows us to sample arbitrarily large graphs. Building on the ideas of recursive enumeration based sampling we also introduce a novel hybrid Markov chain with much faster convergence than current alternatives while still being easy to adapt to various restrictions. Finally we discuss how to include such restrictions in the combinatorial enumeration and the new hybrid Markov chain method for efficient uniform sampling of the corresponding graphs. version:4
arxiv-1306-2685 | Flexible sampling of discrete data correlations without the marginal distributions | http://arxiv.org/abs/1306.2685 | id:1306.2685 author:Alfredo Kalaitzis, Ricardo Silva category:stat.ML cs.LG stat.CO  published:2013-06-12 summary:Learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. More recently, the framework of copula modeling has gained popularity due to its modular parametrization of joint distributions. Among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. More radically, the extended rank likelihood approach of Hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. The main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. Inference is typically done in a Bayesian framework with Gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increases quadratically with the number of data points. The result is slow mixing when using off-the-shelf Gibbs sampling. We present an efficient algorithm based on recent advances on constrained Hamiltonian Markov chain Monte Carlo that is simple to implement and does not require paying for a quadratic cost in sample size. version:3
arxiv-1309-6415 | Stratified Graphical Models - Context-Specific Independence in Graphical Models | http://arxiv.org/abs/1309.6415 | id:1309.6415 author:Henrik Nyman, Johan Pensar, Timo Koski, Jukka Corander category:stat.ML  published:2013-09-25 summary:Theory of graphical models has matured over more than three decades to provide the backbone for several classes of models that are used in a myriad of applications such as genetic mapping of diseases, credit risk evaluation, reliability and computer security, etc. Despite of their generic applicability and wide adoptance, the constraints imposed by undirected graphical models and Bayesian networks have also been recognized to be unnecessarily stringent under certain circumstances. This observation has led to the proposal of several generalizations that aim at more relaxed constraints by which the models can impose local or context-specific dependence structures. Here we consider an additional class of such models, termed as stratified graphical models. We develop a method for Bayesian learning of these models by deriving an analytical expression for the marginal likelihood of data under a specific subclass of decomposable stratified models. A non-reversible Markov chain Monte Carlo approach is further used to identify models that are highly supported by the posterior distribution over the model space. Our method is illustrated and compared with ordinary graphical models through application to several real and synthetic datasets. version:2
arxiv-1311-3492 | High-dimensional learning of linear causal networks via inverse covariance estimation | http://arxiv.org/abs/1311.3492 | id:1311.3492 author:Po-Ling Loh, Peter Bühlmann category:stat.ML math.ST stat.TH 62F12  published:2013-11-14 summary:We establish a new framework for statistical estimation of directed acyclic graphs (DAGs) when data are generated from a linear, possibly non-Gaussian structural equation model. Our framework consists of two parts: (1) inferring the moralized graph from the support of the inverse covariance matrix; and (2) selecting the best-scoring graph amongst DAGs that are consistent with the moralized graph. We show that when the error variances are known or estimated to close enough precision, the true DAG is the unique minimizer of the score computed using the reweighted squared l_2-loss. Our population-level results have implications for the identifiability of linear SEMs when the error covariances are specified up to a constant multiple. On the statistical side, we establish rigorous conditions for high-dimensional consistency of our two-part algorithm, defined in terms of a "gap" between the true DAG and the next best candidate. Finally, we demonstrate that dynamic programming may be used to select the optimal DAG in linear time when the treewidth of the moralized graph is bounded. version:1
arxiv-1311-3987 | Big Data and Cross-Document Coreference Resolution: Current State and Future Opportunities | http://arxiv.org/abs/1311.3987 | id:1311.3987 author:Seyed-Mehdi-Reza Beheshti, Srikumar Venugopal, Seung Hwan Ryu, Boualem Benatallah, Wei Wang category:cs.CL cs.DC cs.IR  published:2013-11-14 summary:Information Extraction (IE) is the task of automatically extracting structured information from unstructured/semi-structured machine-readable documents. Among various IE tasks, extracting actionable intelligence from ever-increasing amount of data depends critically upon Cross-Document Coreference Resolution (CDCR) - the task of identifying entity mentions across multiple documents that refer to the same underlying entity. Recently, document datasets of the order of peta-/tera-bytes has raised many challenges for performing effective CDCR such as scaling to large numbers of mentions and limited representational power. The problem of analysing such datasets is called "big data". The aim of this paper is to provide readers with an understanding of the central concepts, subtasks, and the current state-of-the-art in CDCR process. We provide assessment of existing tools/techniques for CDCR subtasks and highlight big data challenges in each of them to help readers identify important and outstanding issues for further investigation. Finally, we provide concluding remarks and discuss possible directions for future work. version:1
arxiv-1311-3368 | Anytime Belief Propagation Using Sparse Domains | http://arxiv.org/abs/1311.3368 | id:1311.3368 author:Sameer Singh, Sebastian Riedel, Andrew McCallum category:stat.ML cs.AI cs.LG  published:2013-11-14 summary:Belief Propagation has been widely used for marginal inference, however it is slow on problems with large-domain variables and high-order factors. Previous work provides useful approximations to facilitate inference on such models, but lacks important anytime properties such as: 1) providing accurate and consistent marginals when stopped early, 2) improving the approximation when run longer, and 3) converging to the fixed point of BP. To this end, we propose a message passing algorithm that works on sparse (partially instantiated) domains, and converges to consistent marginals using dynamic message scheduling. The algorithm grows the sparse domains incrementally, selecting the next value to add using prioritization schemes based on the gradients of the marginal inference objective. Our experiments demonstrate local anytime consistency and fast convergence, providing significant speedups over BP to obtain low-error marginals: up to 25 times on grid models, and up to 6 times on a real-world natural language processing task. version:1
arxiv-1311-3318 | A Study of Actor and Action Semantic Retention in Video Supervoxel Segmentation | http://arxiv.org/abs/1311.3318 | id:1311.3318 author:Chenliang Xu, Richard F. Doell, Stephen José Hanson, Catherine Hanson, Jason J. Corso category:cs.CV  published:2013-11-13 summary:Existing methods in the semantic computer vision community seem unable to deal with the explosion and richness of modern, open-source and social video content. Although sophisticated methods such as object detection or bag-of-words models have been well studied, they typically operate on low level features and ultimately suffer from either scalability issues or a lack of semantic meaning. On the other hand, video supervoxel segmentation has recently been established and applied to large scale data processing, which potentially serves as an intermediate representation to high level video semantic extraction. The supervoxels are rich decompositions of the video content: they capture object shape and motion well. However, it is not yet known if the supervoxel segmentation retains the semantics of the underlying video content. In this paper, we conduct a systematic study of how well the actor and action semantics are retained in video supervoxel segmentation. Our study has human observers watching supervoxel segmentation videos and trying to discriminate both actor (human or animal) and action (one of eight everyday actions). We gather and analyze a large set of 640 human perceptions over 96 videos in 3 different supervoxel scales. Furthermore, we conduct machine recognition experiments on a feature defined on supervoxel segmentation, called supervoxel shape context, which is inspired by the higher order processes in human perception. Our ultimate findings suggest that a significant amount of semantics have been well retained in the video supervoxel segmentation and can be used for further video analysis. version:1
arxiv-1311-3269 | On a non-local spectrogram for denoising one-dimensional signals | http://arxiv.org/abs/1311.3269 | id:1311.3269 author:Gonzalo Galiano, Julián Velasco category:cs.CV  published:2013-11-13 summary:In previous works, we investigated the use of local filters based on partial differential equations (PDE) to denoise one-dimensional signals through the image processing of time-frequency representations, such as the spectrogram. In this image denoising algorithms, the particularity of the image was hardly taken into account. We turn, in this paper, to study the performance of non-local filters, like Neighborhood or Yaroslavsky filters, in the same problem. We show that, for certain iterative schemes involving the Neighborhood filter, the computational time is drastically reduced with respect to Yaroslavsky or nonlinear PDE based filters, while the outputs of the filtering processes are similar. This is heuristically justified by the connection between the (fast) Neighborhood filter applied to a spectrogram and the corresponding Nonlocal Means filter (accurate) applied to the Wigner-Ville distribution of the signal. This correspondence holds only for time-frequency representations of one-dimensional signals, not to usual images, and in this sense the particularity of the image is exploited. We compare though a series of experiments on synthetic and biomedical signals the performance of local and non-local filters. version:1
arxiv-1311-3211 | Stochastic inference with deterministic spiking neurons | http://arxiv.org/abs/1311.3211 | id:1311.3211 author:Mihai A. Petrovici, Johannes Bill, Ilja Bytschok, Johannes Schemmel, Karlheinz Meier category:q-bio.NC cond-mat.dis-nn cs.NE physics.bio-ph 92-08 C.1.3; I.5.1  published:2013-11-13 summary:The seemingly stochastic transient dynamics of neocortical circuits observed in vivo have been hypothesized to represent a signature of ongoing stochastic inference. In vitro neurons, on the other hand, exhibit a highly deterministic response to various types of stimulation. We show that an ensemble of deterministic leaky integrate-and-fire neurons embedded in a spiking noisy environment can attain the correct firing statistics in order to sample from a well-defined target distribution. We provide an analytical derivation of the activation function on the single cell level; for recurrent networks, we examine convergence towards stationarity in computer simulations and demonstrate sample-based Bayesian inference in a mixed graphical model. This establishes a rigorous link between deterministic neuron models and functional stochastic dynamics on the network level. version:1
arxiv-1305-6916 | Optimal Rates of Convergence for Latent Generalized Correlation Matrix Estimation in Transelliptical Distribution | http://arxiv.org/abs/1305.6916 | id:1305.6916 author:Fang Han, Han Liu category:stat.ML  published:2013-05-29 summary:Correlation matrix plays a key role in many multivariate methods (e.g., graphical model estimation and factor analysis). The current state-of-the-art in estimating large correlation matrices focuses on the use of Pearson's sample correlation matrix. Although Pearson's sample correlation matrix enjoys various good properties under Gaussian models, its not an effective estimator when facing heavy-tail distributions with possible outliers. As a robust alternative, \cite{han2012transelliptical} advocated the use of a transformed version of the Kendall's tau sample correlation matrix in estimating high dimensional latent generalized correlation matrix under the transelliptical distribution family (or elliptical copula). The transelliptical family assumes that after unspecified marginal monotone transformations, the data follow an elliptical distribution. In this paper, we study the theoretical properties of the Kendall's tau sample correlation matrix and its transformed version proposed in \cite{han2012transelliptical} for estimating the population Kendall's tau correlation matrix and the latent Pearson's correlation matrix under both spectral and restricted spectral norms. With regard to the spectral norm, we highlight the role of "effective rank" in quantifying the rate of convergence. With regard to the restricted spectral norm, we for the first time present a "sign subgaussian condition" which is sufficient to guarantee that the rank-based correlation matrix estimator attains the optimal rate of convergence. In both cases, we do not need any moment condition. version:3
arxiv-1311-3175 | Architecture of an Ontology-Based Domain-Specific Natural Language Question Answering System | http://arxiv.org/abs/1311.3175 | id:1311.3175 author:Athira P. M., Sreeja M., P. C. Reghu Raj category:cs.CL cs.IR  published:2013-11-13 summary:Question answering (QA) system aims at retrieving precise information from a large collection of documents against a query. This paper describes the architecture of a Natural Language Question Answering (NLQA) system for a specific domain based on the ontological information, a step towards semantic web question answering. The proposed architecture defines four basic modules suitable for enhancing current QA capabilities with the ability of processing complex questions. The first module was the question processing, which analyses and classifies the question and also reformulates the user query. The second module allows the process of retrieving the relevant documents. The next module processes the retrieved documents, and the last module performs the extraction and generation of a response. Natural language processing techniques are used for processing the question and documents and also for answer extraction. Ontology and domain knowledge are used for reformulating queries and identifying the relations. The aim of the system is to generate short and specific answer to the question that is asked in the natural language in a specific domain. We have achieved 94 % accuracy of natural language question answering in our implementation. version:1
arxiv-1401-6130 | smart application for AMS using Face Recognition | http://arxiv.org/abs/1401.6130 | id:1401.6130 author:MuthuKalyani. K, VeeraMuthu. A category:cs.CY cs.CV  published:2013-11-13 summary:Attendance Management System (AMS) can be made into smarter way by using face recognition technique, where we use a CCTV camera to be fixed at the entry point of a classroom, which automatically captures the image of the person and checks the observed image with the face database using android enhanced smart phone. It is typically used for two purposes. Firstly, marking attendance for student by comparing the face images produced recently and secondly, recognition of human who are strange to the environment i.e. an unauthorized person For verification of image, a newly emerging trend 3D Face Recognition is used which claims to provide more accuracy in matching the image databases and has an ability to recognize a subject at different view angles. version:1
arxiv-1311-3076 | An Efficient Method for Recognizing the Low Quality Fingerprint Verification by Means of Cross Correlation | http://arxiv.org/abs/1311.3076 | id:1311.3076 author:V. Karthikeyan, V. J. Vijayalakshmi category:cs.CV  published:2013-11-13 summary:In this paper, we propose an efficient method to provide personal identification using fingerprint to get better accuracy even in noisy condition. The fingerprint matching based on the number of corresponding minutia pairings, has been in use for a long time, which is not very efficient for recognizing the low quality fingerprints. To overcome this problem, correlation technique is used. The correlation-based fingerprint verification system is capable of dealing with low quality images from which no minutiae can be extracted reliably and with fingerprints that suffer from non-uniform shape distortions, also in case of damaged and partial images. Orientation Field Methodology (OFM) has been used as a preprocessing module, and it converts the images into a field pattern based on the direction of the ridges, loops and bifurcations in the image of a fingerprint. The input image is then Cross Correlated (CC) with all the images in the cluster and the highest correlated image is taken as the output. The result gives a good recognition rate, as the proposed scheme uses Cross Correlation of Field Orientation (CCFO = OFM + CC) for fingerprint identification. version:1
arxiv-1311-3011 | UW SPF: The University of Washington Semantic Parsing Framework | http://arxiv.org/abs/1311.3011 | id:1311.3011 author:Yoav Artzi, Luke Zettlemoyer category:cs.CL  published:2013-11-13 summary:The University of Washington Semantic Parsing Framework (SPF) is a learning and inference framework for mapping natural language to formal representation of its meaning. version:1
arxiv-1311-3001 | Informed Source Separation: A Bayesian Tutorial | http://arxiv.org/abs/1311.3001 | id:1311.3001 author:Kevin H. Knuth category:stat.ML cs.LG  published:2013-11-13 summary:Source separation problems are ubiquitous in the physical sciences; any situation where signals are superimposed calls for source separation to estimate the original signals. In this tutorial I will discuss the Bayesian approach to the source separation problem. This approach has a specific advantage in that it requires the designer to explicitly describe the signal model in addition to any other information or assumptions that go into the problem description. This leads naturally to the idea of informed source separation, where the algorithm design incorporates relevant information about the specific problem. This approach promises to enable researchers to design their own high-quality algorithms that are specifically tailored to the problem at hand. version:1
arxiv-1311-2987 | Learning Input and Recurrent Weight Matrices in Echo State Networks | http://arxiv.org/abs/1311.2987 | id:1311.2987 author:Hamid Palangi, Li Deng, Rabab K Ward category:cs.LG  published:2013-11-13 summary:Echo State Networks (ESNs) are a special type of the temporally deep network model, the Recurrent Neural Network (RNN), where the recurrent matrix is carefully designed and both the recurrent and input matrices are fixed. An ESN uses the linearity of the activation function of the output units to simplify the learning of the output matrix. In this paper, we devise a special technique that take advantage of this linearity in the output units of an ESN, to learn the input and recurrent matrices. This has not been done in earlier ESNs due to their well known difficulty in learning those matrices. Compared to the technique of BackPropagation Through Time (BPTT) in learning general RNNs, our proposed method exploits linearity of activation function in the output units to formulate the relationships amongst the various matrices in an RNN. These relationships results in the gradient of the cost function having an analytical form and being more accurate. This would enable us to compute the gradients instead of obtaining them by recursion as in BPTT. Experimental results on phone state classification show that learning one or both the input and recurrent matrices in an ESN yields superior results compared to traditional ESNs that do not learn these matrices, especially when longer time steps are used. version:1
arxiv-1311-2978 | Authorship Attribution Using Word Network Features | http://arxiv.org/abs/1311.2978 | id:1311.2978 author:Shibamouli Lahiri, Rada Mihalcea category:cs.CL  published:2013-11-12 summary:In this paper, we explore a set of novel features for authorship attribution of documents. These features are derived from a word network representation of natural language text. As has been noted in previous studies, natural language tends to show complex network structure at word level, with low degrees of separation and scale-free (power law) degree distribution. There has also been work on authorship attribution that incorporates ideas from complex networks. The goal of our paper is to explore properties of these complex networks that are suitable as features for machine-learning-based authorship attribution of documents. We performed experiments on three different datasets, and obtained promising results. version:1
arxiv-1311-2971 | Approximate Inference in Continuous Determinantal Point Processes | http://arxiv.org/abs/1311.2971 | id:1311.2971 author:Raja Hafiz Affandi, Emily B. Fox, Ben Taskar category:stat.ML cs.LG stat.ME  published:2013-11-12 summary:Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and finite base set. This discrete setting admits an efficient sampling algorithm based on the eigendecomposition of the defining kernel matrix. Recently, there has been growing interest in using DPPs defined on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. In this paper, we present two efficient DPP sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via Nystrom and random Fourier feature techniques and another based on Gibbs sampling. We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces. version:1
arxiv-1311-1422 | Structural Learning for Template-free Protein Folding | http://arxiv.org/abs/1311.1422 | id:1311.1422 author:Feng Zhao category:cs.LG cs.CE q-bio.QM  published:2013-11-06 summary:The thesis is aimed to solve the template-free protein folding problem by tackling two important components: efficient sampling in vast conformation space, and design of knowledge-based potentials with high accuracy. We have proposed the first-order and second-order CRF-Sampler to sample structures from the continuous local dihedral angles space by modeling the lower and higher order conditional dependency between neighboring dihedral angles given the primary sequence information. A framework combining the Conditional Random Fields and the energy function is introduced to guide the local conformation sampling using long range constraints with the energy function. The relationship between the sequence profile and the local dihedral angle distribution is nonlinear. Hence we proposed the CNF-Folder to model this complex relationship by applying a novel machine learning model Conditional Neural Fields which utilizes the structural graphical model with the neural network. CRF-Samplers and CNF-Folder perform very well in CASP8 and CASP9. Further, a novel pairwise distance statistical potential (EPAD) is designed to capture the dependency of the energy profile on the positions of the interacting amino acids as well as the types of those amino acids, opposing the common assumption that this energy profile depends only on the types of amino acids. EPAD has also been successfully applied in the CASP 10 Free Modeling experiment with CNF-Folder, especially outstanding on some uncommon structured targets. version:2
arxiv-1310-1519 | Moments and Root-Mean-Square Error of the Bayesian MMSE Estimator of Classification Error in the Gaussian Model | http://arxiv.org/abs/1310.1519 | id:1310.1519 author:Amin Zollanvari, Edward R. Dougherty category:stat.ML 62C10  62C10  46N30  published:2013-10-05 summary:The most important aspect of any classifier is its error rate, because this quantifies its predictive capacity. Thus, the accuracy of error estimation is critical. Error estimation is problematic in small-sample classifier design because the error must be estimated using the same data from which the classifier has been designed. Use of prior knowledge, in the form of a prior distribution on an uncertainty class of feature-label distributions to which the true, but unknown, feature-distribution belongs, can facilitate accurate error estimation (in the mean-square sense) in circumstances where accurate completely model-free error estimation is impossible. This paper provides analytic asymptotically exact finite-sample approximations for various performance metrics of the resulting Bayesian Minimum Mean-Square-Error (MMSE) error estimator in the case of linear discriminant analysis (LDA) in the multivariate Gaussian model. These performance metrics include the first, second, and cross moments of the Bayesian MMSE error estimator with the true error of LDA, and therefore, the Root-Mean-Square (RMS) error of the estimator. We lay down the theoretical groundwork for Kolmogorov double-asymptotics in a Bayesian setting, which enables us to derive asymptotic expressions of the desired performance metrics. From these we produce analytic finite-sample approximations and demonstrate their accuracy via numerical examples. Various examples illustrate the behavior of these approximations and their use in determining the necessary sample size to achieve a desired RMS. The Supplementary Material contains derivations for some equations and added figures. version:2
arxiv-1311-3157 | Multiple Closed-Form Local Metric Learning for K-Nearest Neighbor Classifier | http://arxiv.org/abs/1311.3157 | id:1311.3157 author:Jianbo Ye category:cs.LG  published:2013-11-12 summary:Many researches have been devoted to learn a Mahalanobis distance metric, which can effectively improve the performance of kNN classification. Most approaches are iterative and computational expensive and linear rigidity still critically limits metric learning algorithm to perform better. We proposed a computational economical framework to learn multiple metrics in closed-form. version:1
arxiv-1311-2799 | Aggregation of Affine Estimators | http://arxiv.org/abs/1311.2799 | id:1311.2799 author:Dong Dai, Philippe Rigollet, Lucy Xia, Tong Zhang category:math.ST cs.LG stat.TH 62G08  published:2013-11-12 summary:We consider the problem of aggregating a general collection of affine estimators for fixed design regression. Relevant examples include some commonly used statistical estimators such as least squares, ridge and robust least squares estimators. Dalalyan and Salmon (2012) have established that, for this problem, exponentially weighted (EW) model selection aggregation leads to sharp oracle inequalities in expectation, but similar bounds in deviation were not previously known. While results indicate that the same aggregation scheme may not satisfy sharp oracle inequalities with high probability, we prove that a weaker notion of oracle inequality for EW that holds with high probability. Moreover, using a generalization of the newly introduced $Q$-aggregation scheme we also prove sharp oracle inequalities that hold with high probability. Finally, we apply our results to universal aggregation and show that our proposed estimator leads simultaneously to all the best known bounds for aggregation, including $\ell_q$-aggregation, $q \in (0,1)$, with high probability. version:1
arxiv-1311-2791 | When Does More Regularization Imply Fewer Degrees of Freedom? Sufficient Conditions and Counter Examples from Lasso and Ridge Regression | http://arxiv.org/abs/1311.2791 | id:1311.2791 author:Shachar Kaufman, Saharon Rosset category:math.ST stat.ML stat.TH  published:2013-11-12 summary:Regularization aims to improve prediction performance of a given statistical modeling approach by moving to a second approach which achieves worse training error but is expected to have fewer degrees of freedom, i.e., better agreement between training and prediction error. We show here, however, that this expected behavior does not hold in general. In fact, counter examples are given that show regularization can increase the degrees of freedom in simple situations, including lasso and ridge regression, which are the most common regularization approaches in use. In such situations, the regularization increases both training error and degrees of freedom, and is thus inherently without merit. On the other hand, two important regularization scenarios are described where the expected reduction in degrees of freedom is indeed guaranteed: (a) all symmetric linear smoothers, and (b) linear regression versus convex constrained linear regression (as in the constrained variant of ridge regression and lasso). version:1
arxiv-1207-6076 | Equivalence of distance-based and RKHS-based statistics in hypothesis testing | http://arxiv.org/abs/1207.6076 | id:1207.6076 author:Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, Kenji Fukumizu category:stat.ME cs.LG math.ST stat.ML stat.TH  published:2012-07-25 summary:We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, maximum mean discrepancies (MMD), that is, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. In the case where the energy distance is computed with a semimetric of negative type, a positive definite kernel, termed distance kernel, may be defined such that the MMD corresponds exactly to the energy distance. Conversely, for any positive definite kernel, we can interpret the MMD as energy distance with respect to some negative-type semimetric. This equivalence readily extends to distance covariance using kernels on the product space. We determine the class of probability distributions for which the test statistics are consistent against all alternatives. Finally, we investigate the performance of the family of distance kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests. version:3
arxiv-1311-2746 | Deep neural networks for single channel source separation | http://arxiv.org/abs/1311.2746 | id:1311.2746 author:Emad M. Grais, Mehmet Umut Sen, Hakan Erdogan category:cs.NE cs.LG  published:2013-11-12 summary:In this paper, a novel approach for single channel source separation (SCSS) using a deep neural network (DNN) architecture is introduced. Unlike previous studies in which DNN and other classifiers were used for classifying time-frequency bins to obtain hard masks for each source, we use the DNN to classify estimated source spectra to check for their validity during separation. In the training stage, the training data for the source signals are used to train a DNN. In the separation stage, the trained DNN is utilized to aid in estimation of each source in the mixed signal. Single channel source separation problem is formulated as an energy minimization problem where each source spectra estimate is encouraged to fit the trained DNN model and the mixed signal spectrum is encouraged to be written as a weighted sum of the estimated source spectra. The proposed approach works regardless of the energy scale differences between the source signals in the training and separation stages. Nonnegative matrix factorization (NMF) is used to initialize the DNN estimate for each source. The experimental results show that using DNN initialized by NMF for source separation improves the quality of the separated signal compared with using NMF for source separation. version:1
arxiv-1206-1874 | Multivariate Bernoulli distribution | http://arxiv.org/abs/1206.1874 | id:1206.1874 author:Bin Dai, Shilin Ding, Grace Wahba category:stat.AP math.ST stat.ML stat.TH  published:2012-06-08 summary:In this paper, we consider the multivariate Bernoulli distribution as a model to estimate the structure of graphs with binary nodes. This distribution is discussed in the framework of the exponential family, and its statistical properties regarding independence of the nodes are demonstrated. Importantly the model can estimate not only the main effects and pairwise interactions among the nodes but also is capable of modeling higher order interactions, allowing for the existence of complex clique effects. We compare the multivariate Bernoulli model with existing graphical inference models - the Ising model and the multivariate Gaussian model, where only the pairwise interactions are considered. On the other hand, the multivariate Bernoulli distribution has an interesting property in that independence and uncorrelatedness of the component random variables are equivalent. Both the marginal and conditional distributions of a subset of variables in the multivariate Bernoulli distribution still follow the multivariate Bernoulli distribution. Furthermore, the multivariate Bernoulli logistic model is developed under generalized linear model theory by utilizing the canonical link function in order to include covariate information on the nodes, edges and cliques. We also consider variable selection techniques such as LASSO in the logistic model to impose sparsity structure on the graph. Finally, we discuss extending the smoothing spline ANOVA approach to the multivariate Bernoulli logistic model to enable estimation of non-linear effects of the predictor variables. version:2
arxiv-1311-2702 | Verifiable Source Code Documentation in Controlled Natural Language | http://arxiv.org/abs/1311.2702 | id:1311.2702 author:Tobias Kuhn, Alexandre Bergel category:cs.SE cs.AI cs.CL cs.HC cs.LO H.5.2; D.2.7  published:2013-11-12 summary:Writing documentation about software internals is rarely considered a rewarding activity. It is highly time-consuming and the resulting documentation is fragile when the software is continuously evolving in a multi-developer setting. Unfortunately, traditional programming environments poorly support the writing and maintenance of documentation. Consequences are severe as the lack of documentation on software structure negatively impacts the overall quality of the software product. We show that using a controlled natural language with a reasoner and a query engine is a viable technique for verifying the consistency and accuracy of documentation and source code. Using ACE, a state-of-the-art controlled natural language, we present positive results on the comprehensibility and the general feasibility of creating and verifying documentation. As a case study, we used automatic documentation verification to identify and fix severe flaws in the architecture of a non-trivial piece of software. Moreover, a user experiment shows that our language is faster and easier to learn and understand than other formal languages for software documentation. version:1
arxiv-1311-2677 | Sampling Based Approaches to Handle Imbalances in Network Traffic Dataset for Machine Learning Techniques | http://arxiv.org/abs/1311.2677 | id:1311.2677 author:Raman Singh, Harish Kumar, R. K. Singla category:cs.NI cs.CR cs.LG  published:2013-11-12 summary:Network traffic data is huge, varying and imbalanced because various classes are not equally distributed. Machine learning (ML) algorithms for traffic analysis uses the samples from this data to recommend the actions to be taken by the network administrators as well as training. Due to imbalances in dataset, it is difficult to train machine learning algorithms for traffic analysis and these may give biased or false results leading to serious degradation in performance of these algorithms. Various techniques can be applied during sampling to minimize the effect of imbalanced instances. In this paper various sampling techniques have been analysed in order to compare the decrease in variation in imbalances of network traffic datasets sampled for these algorithms. Various parameters like missing classes in samples, probability of sampling of the different instances have been considered for comparison. version:1
arxiv-1311-2561 | Performing edge detection by difference of Gaussians using q-Gaussian kernels | http://arxiv.org/abs/1311.2561 | id:1311.2561 author:Lucas Assirati, Núbia R. da Silva, Lilian Berton, Alneu de A. Lopes, Odemir M. Bruno category:cs.CV physics.comp-ph  published:2013-11-11 summary:In image processing, edge detection is a valuable tool to perform the extraction of features from an image. This detection reduces the amount of information to be processed, since the redundant information (considered less relevant) can be unconsidered. The technique of edge detection consists of determining the points of a digital image whose intensity changes sharply. This changes are due to the discontinuities of the orientation on a surface for example. A well known method of edge detection is the Difference of Gaussians (DoG). The method consists of subtracting two Gaussians, where a kernel has a standard deviation smaller than the previous one. The convolution between the subtraction of kernels and the input image results in the edge detection of this image. This paper introduces a method of extracting edges using DoG with kernels based on the q-Gaussian probability distribution, derived from the q-statistic proposed by Constantino Tsallis. To demonstrate the method's potential, we compare the introduced method with the traditional DoG using Gaussians kernels. The results showed that the proposed method can extract edges with more accurate details. version:2
arxiv-1308-5269 | A comparative analysis of methods for estimating axon diameter using DWI | http://arxiv.org/abs/1308.5269 | id:1308.5269 author:Hamed Yousefi Mesri category:cs.NE  published:2013-08-24 summary:The importance of studying the brain microstructure is described and the existing and state of the art non-invasive methods for the investigation of the brain microstructure using Diffusion Weighted Magnetic Resonance Imaging (DWI) is studied. In the next step, Cramer-Rao Lower Bound (CRLB) analysis is described and utilised for assessment of the minimum estimation error and uncertainty level of different Diffusion Weighted Magnetic Resonance (DWMR) signal decay models. The analyses are performed considering the best scenario through which, we assume that the models are the appropriate representation of the measured phenomena. This includes the study of the sensitivity of the estimations to the measurement and model parameters. It is demonstrated that none of the existing models can achieve a reasonable minimum uncertainty level under typical measurement setup. At the end, the practical obstacles for achieving higher performance in clinical and experimental environments are studied and their effects on feasibility of the methods are discussed. version:2
arxiv-1311-2642 | Volumetric Reconstruction Applied to Perceptual Studies of Size and Weight | http://arxiv.org/abs/1311.2642 | id:1311.2642 author:J. Balzer, M. Peters, S. Soatto category:cs.CV  published:2013-11-11 summary:We explore the application of volumetric reconstruction from structured-light sensors in cognitive neuroscience, specifically in the quantification of the size-weight illusion, whereby humans tend to systematically perceive smaller objects as heavier. We investigate the performance of two commercial structured-light scanning systems in comparison to one we developed specifically for this application. Our method has two main distinct features: First, it only samples a sparse series of viewpoints, unlike other systems such as the Kinect Fusion. Second, instead of building a distance field for the purpose of points-to-surface conversion directly, we pursue a first-order approach: the distance function is recovered from its gradient by a screened Poisson reconstruction, which is very resilient to noise and yet preserves high-frequency signal components. Our experiments show that the quality of metric reconstruction from structured light sensors is subject to systematic biases, and highlights the factors that influence it. Our main performance index rates estimates of volume (a proxy of size), for which we review a well-known formula applicable to incomplete meshes. Our code and data will be made publicly available upon completion of the anonymous review process. version:1
arxiv-1311-2621 | Determining Leishmania Infection Levels by Automatic Analysis of Microscopy Images | http://arxiv.org/abs/1311.2621 | id:1311.2621 author:P. A Nogueira category:cs.CV  published:2013-11-11 summary:Analysis of microscopy images is one important tool in many fields of biomedical research, as it allows the quantification of a multitude of parameters at the cellular level. However, manual counting of these images is both tiring and unreliable and ultimately very time-consuming for biomedical researchers. Not only does this slow down the overall research process, it also introduces counting errors due to a lack of objectivity and consistency inherent to the researchers own human nature. This thesis addresses this issue by automatically determining infection indexes of macrophages parasite by Leishmania in microscopy images using computer vision and pattern recognition methodologies. Initially images are submitted to a pre-processing stage that consists in a normalization of illumination conditions. Three algorithms are then applied in parallel to each image. Algorithm A intends to detect macrophage nuclei and consists of segmentation via adaptive multi-threshold, and classification of resulting regions using a set of collected features. Algorithm B intends to detect parasites and is similar to Algorithm A but the adaptive multi-threshold is parameterized with a different constraints vector. Algorithm C intends to detect the macrophages and parasites cytoplasm and consists of a cut-off version of the previous two algorithms, where the classification step is skipped. Regions with multiple nuclei or parasites are processed by a voting system that employs both a Support Vector Machine and a set of region features for determining the number of objects present in each region. The previous vote is then taken into account as the number of mixtures to be used in a Gaussian Mixture Model to decluster the said region. Finally each parasite is assigned to, at most, a single macrophage using minimum Euclidean distance to a cell nucleus, thus quantifying Leishmania infection levels. version:1
arxiv-1311-6500 | Stitched Panoramas from Toy Airborne Video Cameras | http://arxiv.org/abs/1311.6500 | id:1311.6500 author:Camille Goudeseune category:cs.CV I.3.3; I.4; J.5  published:2013-11-11 summary:Effective panoramic photographs are taken from vantage points that are high. High vantage points have recently become easier to reach as the cost of quadrotor helicopters has dropped to nearly disposable levels. Although cameras carried by such aircraft weigh only a few grams, their low-quality video can be converted into panoramas of high quality and high resolution. Also, the small size of these aircraft vastly reduces the risks inherent to flight. version:1
arxiv-1311-2531 | Motility at the origin of life: Its characterization and a model | http://arxiv.org/abs/1311.2531 | id:1311.2531 author:Tom Froese, Nathaniel Virgo, Takashi Ikegami category:cs.AI cs.NE nlin.AO nlin.PS q-bio.PE 35K57  published:2013-11-11 summary:Due to recent advances in synthetic biology and artificial life, the origin of life is currently a hot topic of research. We review the literature and argue that the two traditionally competing "replicator-first" and "metabolism-first" approaches are merging into one integrated theory of individuation and evolution. We contribute to the maturation of this more inclusive approach by highlighting some problematic assumptions that still lead to an impoverished conception of the phenomenon of life. In particular, we argue that the new consensus has so far failed to consider the relevance of intermediate timescales. We propose that an adequate theory of life must account for the fact that all living beings are situated in at least four distinct timescales, which are typically associated with metabolism, motility, development, and evolution. On this view, self-movement, adaptive behavior and morphological changes could have already been present at the origin of life. In order to illustrate this possibility we analyze a minimal model of life-like phenomena, namely of precarious, individuated, dissipative structures that can be found in simple reaction-diffusion systems. Based on our analysis we suggest that processes in intermediate timescales could have already been operative in prebiotic systems. They may have facilitated and constrained changes occurring in the faster- and slower-paced timescales of chemical self-individuation and evolution by natural selection, respectively. version:1
arxiv-1306-3855 | Two-View Matching with View Synthesis Revisited | http://arxiv.org/abs/1306.3855 | id:1306.3855 author:Dmytro Mishkin, Michal Perdoch, Jiri Matas category:cs.CV  published:2013-06-17 summary:Wide-baseline matching focussing on problems with extreme viewpoint change is considered. We introduce the use of view synthesis with affine-covariant detectors to solve such problems and show that matching with the Hessian-Affine or MSER detectors outperforms the state-of-the-art ASIFT. To minimise the loss of speed caused by view synthesis, we propose the Matching On Demand with view Synthesis algorithm (MODS) that uses progressively more synthesized images and more (time-consuming) detectors until reliable estimation of geometry is possible. We show experimentally that the MODS algorithm solves problems beyond the state-of-the-art and yet is comparable in speed to standard wide-baseline matchers on simpler problems. Minor contributions include an improved method for tentative correspondence selection, applicable both with and without view synthesis and a view synthesis setup greatly improving MSER robustness to blur and scale change that increase its running time by 10% only. version:2
arxiv-1311-2503 | Predictable Feature Analysis | http://arxiv.org/abs/1311.2503 | id:1311.2503 author:Stefan Richthofer, Laurenz Wiskott category:cs.LG stat.ML  published:2013-11-11 summary:Every organism in an environment, whether biological, robotic or virtual, must be able to predict certain aspects of its environment in order to survive or perform whatever task is intended. It needs a model that is capable of estimating the consequences of possible actions, so that planning, control, and decision-making become feasible. For scientific purposes, such models are usually created in a problem specific manner using differential equations and other techniques from control- and system-theory. In contrast to that, we aim for an unsupervised approach that builds up the desired model in a self-organized fashion. Inspired by Slow Feature Analysis (SFA), our approach is to extract sub-signals from the input, that behave as predictable as possible. These "predictable features" are highly relevant for modeling, because predictability is a desired property of the needed consequence-estimating model by definition. In our approach, we measure predictability with respect to a certain prediction model. We focus here on the solution of the arising optimization problem and present a tractable algorithm based on algebraic methods which we call Predictable Feature Analysis (PFA). We prove that the algorithm finds the globally optimal signal, if this signal can be predicted with low error. To deal with cases where the optimal signal has a significant prediction error, we provide a robust, heuristically motivated variant of the algorithm and verify it empirically. Additionally, we give formal criteria a prediction-model must meet to be suitable for measuring predictability in the PFA setting and also provide a suitable default-model along with a formal proof that it meets these criteria. version:1
arxiv-1311-2492 | Notes on Elementary Spectral Graph Theory. Applications to Graph Clustering Using Normalized Cuts | http://arxiv.org/abs/1311.2492 | id:1311.2492 author:Jean Gallier category:cs.CV  published:2013-11-11 summary:These are notes on the method of normalized graph cuts and its applications to graph clustering. I provide a fairly thorough treatment of this deeply original method due to Shi and Malik, including complete proofs. I include the necessary background on graphs and graph Laplacians. I then explain in detail how the eigenvectors of the graph Laplacian can be used to draw a graph. This is an attractive application of graph Laplacians. The main thrust of this paper is the method of normalized cuts. I give a detailed account for K = 2 clusters, and also for K > 2 clusters, based on the work of Yu and Shi. Three points that do not appear to have been clearly articulated before are elaborated: 1. The solutions of the main optimization problem should be viewed as tuples in the K-fold cartesian product of projective space RP^{N-1}. 2. When K > 2, the solutions of the relaxed problem should be viewed as elements of the Grassmannian G(K,N). 3. Two possible Riemannian distances are available to compare the closeness of solutions: (a) The distance on (RP^{N-1})^K. (b) The distance on the Grassmannian. I also clarify what should be the necessary and sufficient conditions for a matrix to represent a partition of the vertices of a graph to be clustered. version:1
arxiv-1311-2483 | Global Sensitivity Analysis with Dependence Measures | http://arxiv.org/abs/1311.2483 | id:1311.2483 author:Sébastien Da Veiga category:math.ST cs.LG stat.ML stat.TH  published:2013-11-11 summary:Global sensitivity analysis with variance-based measures suffers from several theoretical and practical limitations, since they focus only on the variance of the output and handle multivariate variables in a limited way. In this paper, we introduce a new class of sensitivity indices based on dependence measures which overcomes these insufficiencies. Our approach originates from the idea to compare the output distribution with its conditional counterpart when one of the input variables is fixed. We establish that this comparison yields previously proposed indices when it is performed with Csiszar f-divergences, as well as sensitivity indices which are well-known dependence measures between random variables. This leads us to investigate completely new sensitivity indices based on recent state-of-the-art dependence measures, such as distance correlation and the Hilbert-Schmidt independence criterion. We also emphasize the potential of feature selection techniques relying on such dependence measures as alternatives to screening in high dimension. version:1
arxiv-1303-6746 | Exploiting correlation and budget constraints in Bayesian multi-armed bandit optimization | http://arxiv.org/abs/1303.6746 | id:1303.6746 author:Matthew W. Hoffman, Bobak Shahriari, Nando de Freitas category:stat.ML cs.LG  published:2013-03-27 summary:We address the problem of finding the maximizer of a nonlinear smooth function, that can only be evaluated point-wise, subject to constraints on the number of permitted function evaluations. This problem is also known as fixed-budget best arm identification in the multi-armed bandit literature. We introduce a Bayesian approach for this problem and show that it empirically outperforms both the existing frequentist counterpart and other Bayesian optimization methods. The Bayesian approach places emphasis on detailed modelling, including the modelling of correlations among the arms. As a result, it can perform well in situations where the number of arms is much larger than the number of allowed function evaluation, whereas the frequentist counterpart is inapplicable. This feature enables us to develop and deploy practical applications, such as automatic machine learning toolboxes. The paper presents comprehensive comparisons of the proposed approach, Thompson sampling, classical Bayesian optimization techniques, more recent Bayesian bandit approaches, and state-of-the-art best arm identification methods. This is the first comparison of many of these methods in the literature and allows us to examine the relative merits of their different features. version:4
arxiv-1311-2378 | An Empirical Evaluation of Sequence-Tagging Trainers | http://arxiv.org/abs/1311.2378 | id:1311.2378 author:P. Balamurugan, Shirish Shevade, S. Sundararajan, S. S Keerthi category:cs.LG  published:2013-11-11 summary:The task of assigning label sequences to a set of observed sequences is common in computational linguistics. Several models for sequence labeling have been proposed over the last few years. Here, we focus on discriminative models for sequence labeling. Many batch and online (updating model parameters after visiting each example) learning algorithms have been proposed in the literature. On large datasets, online algorithms are preferred as batch learning methods are slow. These online algorithms were designed to solve either a primal or a dual problem. However, there has been no systematic comparison of these algorithms in terms of their speed, generalization performance (accuracy/likelihood) and their ability to achieve steady state generalization performance fast. With this aim, we compare different algorithms and make recommendations, useful for a practitioner. We conclude that the selection of an algorithm for sequence labeling depends on the evaluation criterion used and its implementation simplicity. version:1
arxiv-1310-6007 | Efficient Optimization for Sparse Gaussian Process Regression | http://arxiv.org/abs/1310.6007 | id:1310.6007 author:Yanshuai Cao, Marcus A. Brubaker, David J. Fleet, Aaron Hertzmann category:cs.LG  published:2013-10-22 summary:We propose an efficient optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates an inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-of-art performance in discrete cases and competitive results in the continuous case. version:3
arxiv-1306-6269 | Active Contour Models for Manifold Valued Image Segmentation | http://arxiv.org/abs/1306.6269 | id:1306.6269 author:Sumukh Bansal, Aditya Tatu category:cs.CV  published:2013-06-26 summary:Image segmentation is the process of partitioning a image into different regions or groups based on some characteristics like color, texture, motion or shape etc. Active contours is a popular variational method for object segmentation in images, in which the user initializes a contour which evolves in order to optimize an objective function designed such that the desired object boundary is the optimal solution. Recently, imaging modalities that produce Manifold valued images have come up, for example, DT-MRI images, vector fields. The traditional active contour model does not work on such images. In this paper, we generalize the active contour model to work on Manifold valued images. As expected, our algorithm detects regions with similar Manifold values in the image. Our algorithm also produces expected results on usual gray-scale images, since these are nothing but trivial examples of Manifold valued images. As another application of our general active contour model, we perform texture segmentation on gray-scale images by first creating an appropriate Manifold valued image. We demonstrate segmentation results for manifold valued images and texture images. version:2
arxiv-1306-1433 | Tight Lower Bound on the Probability of a Binomial Exceeding its Expectation | http://arxiv.org/abs/1306.1433 | id:1306.1433 author:Spencer Greenberg, Mehryar Mohri category:cs.LG stat.ML  published:2013-06-06 summary:We give the proof of a tight lower bound on the probability that a binomial random variable exceeds its expected value. The inequality plays an important role in a variety of contexts, including the analysis of relative deviation bounds in learning theory and generalization bounds for unbounded loss functions. version:3
arxiv-1305-6663 | Generalized Denoising Auto-Encoders as Generative Models | http://arxiv.org/abs/1305.6663 | id:1305.6663 author:Yoshua Bengio, Li Yao, Guillaume Alain, Pascal Vincent category:cs.LG  published:2013-05-29 summary:Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty). version:4
arxiv-1311-2276 | A Quantitative Evaluation Framework for Missing Value Imputation Algorithms | http://arxiv.org/abs/1311.2276 | id:1311.2276 author:Vinod Nair, Rahul Kidambi, Sundararajan Sellamanickam, S. Sathiya Keerthi, Johannes Gehrke, Vijay Narayanan category:cs.LG  published:2013-11-10 summary:We consider the problem of quantitatively evaluating missing value imputation algorithms. Given a dataset with missing values and a choice of several imputation algorithms to fill them in, there is currently no principled way to rank the algorithms using a quantitative metric. We develop a framework based on treating imputation evaluation as a problem of comparing two distributions and show how it can be used to compute quantitative metrics. We present an efficient procedure for applying this framework to practical datasets, demonstrate several metrics derived from the existing literature on comparing distributions, and propose a new metric called Neighborhood-based Dissimilarity Score which is fast to compute and provides similar results. Results are shown on several datasets, metrics, and imputations algorithms. version:1
arxiv-1311-2271 | More data speeds up training time in learning halfspaces over sparse vectors | http://arxiv.org/abs/1311.2271 | id:1311.2271 author:Amit Daniely, Nati Linial, Shai Shalev Shwartz category:cs.LG  published:2013-11-10 summary:The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a {\em computational} resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task? We give the first positive answer to this question for a {\em natural supervised learning problem} --- we consider agnostic PAC learning of halfspaces over $3$-sparse vectors in $\{-1,1,0\}^n$. This class is inefficiently learnable using $O\left(n/\epsilon^2\right)$ examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random $\mathrm{3CNF}$ formulas is hard, it is impossible to efficiently learn this class using only $O\left(n/\epsilon^2\right)$ examples. We further show that under stronger hardness assumptions, even $O\left(n^{1.499}/\epsilon^2\right)$ examples do not suffice. On the other hand, we show a new algorithm that learns this class efficiently using $\tilde{\Omega}\left(n^2/\epsilon^2\right)$ examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem. version:1
arxiv-1311-2252 | Semantic Sort: A Supervised Approach to Personalized Semantic Relatedness | http://arxiv.org/abs/1311.2252 | id:1311.2252 author:Ran El-Yaniv, David Yanay category:cs.CL cs.LG  published:2013-11-10 summary:We propose and study a novel supervised approach to learning statistical semantic relatedness models from subjectively annotated training examples. The proposed semantic model consists of parameterized co-occurrence statistics associated with textual units of a large background knowledge corpus. We present an efficient algorithm for learning such semantic models from a training sample of relatedness preferences. Our method is corpus independent and can essentially rely on any sufficiently large (unstructured) collection of coherent texts. Moreover, the approach facilitates the fitting of semantic models for specific users or groups of users. We present the results of extensive range of experiments from small to large scale, indicating that the proposed method is effective and competitive with the state-of-the-art. version:1
arxiv-1311-2241 | Learning Gaussian Graphical Models with Observed or Latent FVSs | http://arxiv.org/abs/1311.2241 | id:1311.2241 author:Ying Liu, Alan S. Willsky category:cs.LG stat.ML  published:2013-11-10 summary:Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widely used in many applications, and the trade-off between the modeling capacity and the efficiency of learning and inference has been an important research problem. In this paper, we study the family of GGMs with small feedback vertex sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles. Exact inference such as computing the marginal distributions and the partition function has complexity $O(k^{2}n)$ using message-passing algorithms, where k is the size of the FVS, and n is the total number of nodes. We propose efficient structure learning algorithms for two cases: 1) All nodes are observed, which is useful in modeling social or flight networks where the FVS nodes often correspond to a small number of high-degree nodes, or hubs, while the rest of the networks is modeled by a tree. Regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate in $O(kn^2+n^2\log n)$ if the FVS is known or in polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes are latent variables, where structure learning is equivalent to decomposing a inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. By incorporating efficient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank correction with complexity $O(kn^{2}+n^{2}\log n)$ per iteration. We also perform experiments using both synthetic data as well as real data of flight delays to demonstrate the modeling capacity with FVSs of various sizes. version:1
arxiv-1311-2150 | Pattern-Coupled Sparse Bayesian Learning for Recovery of Block-Sparse Signals | http://arxiv.org/abs/1311.2150 | id:1311.2150 author:Jun Fang, Yanning Shen, Hongbin Li, Pu Wang category:cs.IT cs.LG math.IT stat.ML  published:2013-11-09 summary:We consider the problem of recovering block-sparse signals whose structures are unknown \emph{a priori}. Block-sparse signals with nonzero coefficients occurring in clusters arise naturally in many practical scenarios. However, the knowledge of the block structure is usually unavailable in practice. In this paper, we develop a new sparse Bayesian learning method for recovery of block-sparse signals with unknown cluster patterns. Specifically, a pattern-coupled hierarchical Gaussian prior model is introduced to characterize the statistical dependencies among coefficients, in which a set of hyperparameters are employed to control the sparsity of signal coefficients. Unlike the conventional sparse Bayesian learning framework in which each individual hyperparameter is associated independently with each coefficient, in this paper, the prior for each coefficient not only involves its own hyperparameter, but also the hyperparameters of its immediate neighbors. In doing this way, the sparsity patterns of neighboring coefficients are related to each other and the hierarchical model has the potential to encourage structured-sparse solutions. The hyperparameters, along with the sparse signal, are learned by maximizing their posterior probability via an expectation-maximization (EM) algorithm. Numerical results show that the proposed algorithm presents uniform superiority over other existing methods in a series of experiments. version:1
arxiv-1311-2139 | Large Margin Semi-supervised Structured Output Learning | http://arxiv.org/abs/1311.2139 | id:1311.2139 author:P. Balamurugan, Shirish Shevade, Sundararajan Sellamanickam category:cs.LG  published:2013-11-09 summary:In structured output learning, obtaining labelled data for real-world applications is usually costly, while unlabelled examples are available in abundance. Semi-supervised structured classification has been developed to handle large amounts of unlabelled structured data. In this work, we consider semi-supervised structural SVMs with domain constraints. The optimization problem, which in general is not convex, contains the loss terms associated with the labelled and unlabelled examples along with the domain constraints. We propose a simple optimization approach, which alternates between solving a supervised learning problem and a constraint matching problem. Solving the constraint matching problem is difficult for structured prediction, and we propose an efficient and effective hill-climbing method to solve it. The alternating optimization is carried out within a deterministic annealing framework, which helps in effective constraint matching, and avoiding local minima which are not very useful. The algorithm is simple to implement and achieves comparable generalization performance on benchmark datasets. version:1
arxiv-1311-2137 | A Structured Prediction Approach for Missing Value Imputation | http://arxiv.org/abs/1311.2137 | id:1311.2137 author:Rahul Kidambi, Vinod Nair, Sundararajan Sellamanickam, S. Sathiya Keerthi category:cs.LG  published:2013-11-09 summary:Missing value imputation is an important practical problem. There is a large body of work on it, but there does not exist any work that formulates the problem in a structured output setting. Also, most applications have constraints on the imputed data, for example on the distribution associated with each variable. None of the existing imputation methods use these constraints. In this paper we propose a structured output approach for missing value imputation that also incorporates domain constraints. We focus on large margin models, but it is easy to extend the ideas to probabilistic models. We deal with the intractable inference step in learning via a piecewise training technique that is simple, efficient, and effective. Comparison with existing state-of-the-art and baseline imputation methods shows that our method gives significantly improved performance on the Hamming loss measure. version:1
arxiv-1304-4672 | Low-Rank Matrix and Tensor Completion via Adaptive Sampling | http://arxiv.org/abs/1304.4672 | id:1304.4672 author:Akshay Krishnamurthy, Aarti Singh category:stat.ML  published:2013-04-17 summary:We study low rank matrix and tensor completion and propose novel algorithms that employ adaptive sampling schemes to obtain strong performance guarantees. Our algorithms exploit adaptivity to identify entries that are highly informative for learning the column space of the matrix (tensor) and consequently, our results hold even when the row space is highly coherent, in contrast with previous analyses. In the absence of noise, we show that one can exactly recover a $n \times n$ matrix of rank $r$ from merely $\Omega(n r^{3/2}\log(r))$ matrix entries. We also show that one can recover an order $T$ tensor using $\Omega(n r^{T-1/2}T^2 \log(r))$ entries. For noisy recovery, our algorithm consistently estimates a low rank matrix corrupted with noise using $\Omega(n r^{3/2} \textrm{polylog}(n))$ entries. We complement our study with simulations that verify our theory and demonstrate the scalability of our algorithms. version:3
arxiv-1311-2110 | Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions | http://arxiv.org/abs/1311.2110 | id:1311.2110 author:Rishabh Iyer, Stefanie Jegelka, Jeff Bilmes category:cs.DS cs.DM cs.LG  published:2013-11-08 summary:We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [53]), and constrained minimization of submodular functions. We show that the complexity of all three problems depends on the 'curvature' of the submodular function, and provide lower and upper bounds that refine and improve previous results [3, 16, 18, 52]. Our proof techniques are fairly generic. We either use a black-box transformation of the function (for approximation and learning), or a transformation of algorithms to use an appropriate surrogate function (for minimization). Curiously, curvature has been known to influence approximations for submodular maximization [7, 55], but its effect on minimization, approximation and learning has hitherto been open. We complete this picture, and also support our theoretical claims by empirical results. version:1
arxiv-1311-2102 | An Experimental Comparison of Trust Region and Level Sets | http://arxiv.org/abs/1311.2102 | id:1311.2102 author:Lena Gorelick, Ismail BenAyed, Frank R. Schmidt, Yuri Boykov category:cs.CV  published:2013-11-08 summary:High-order (non-linear) functionals have become very popular in segmentation, stereo and other computer vision problems. Level sets is a well established general gradient descent framework, which is directly applicable to optimization of such functionals and widely used in practice. Recently, another general optimization approach based on trust region methodology was proposed for regional non-linear functionals. Our goal is a comprehensive experimental comparison of these two frameworks in regard to practical efficiency, robustness to parameters, and optimality. We experiment on a wide range of problems with non-linear constraints on segment volume, appearance and shape. version:1
arxiv-1311-2079 | Nonparametric Multi-group Membership Model for Dynamic Networks | http://arxiv.org/abs/1311.2079 | id:1311.2079 author:Myunghwan Kim, Jure Leskovec category:cs.SI physics.soc-ph stat.ML  published:2013-11-08 summary:Relational data-like graphs, networks, and matrices-is often dynamic, where the relational structure evolves over time. A fundamental problem in the analysis of time-varying network data is to extract a summary of the common structure and the dynamics of the underlying relations between the entities. Here we build on the intuition that changes in the network structure are driven by the dynamics at the level of groups of nodes. We propose a nonparametric multi-group membership model for dynamic networks. Our model contains three main components: We model the birth and death of individual groups with respect to the dynamics of the network structure via a distance dependent Indian Buffet Process. We capture the evolution of individual node group memberships via a Factorial Hidden Markov model. And, we explain the dynamics of the network structure by explicitly modeling the connectivity structure of groups. We demonstrate our model's capability of identifying the dynamics of latent groups in a number of different types of network data. Experimental results show that our model provides improved predictive performance over existing dynamic network models on future network forecasting and missing link prediction. version:1
arxiv-1311-2014 | A new stopping criterion for the mean shift iterative algorithm | http://arxiv.org/abs/1311.2014 | id:1311.2014 author:Roberto Rodríguez, Esley Torres, Yasel Garcés, Osvaldo Pereira, Humberto Sossa category:cs.CV  published:2013-11-08 summary:The mean shift iterative algorithm was proposed in 2006, for using the entropy as a stopping criterion. From then on, a theoretical base has been developed and a group of applications has been carried out using this algorithm. This paper proposes a new stopping criterion for the mean shift iterative algorithm, where stopping threshold via entropy is used now, but in another way. Many segmentation experiments were carried out by utilizing standard images and it was verified that a better segmentation was reached, and that the algorithm had better stability. An analysis on the convergence, through a theorem, with the new stopping criterion was carried out. The goal of this paper is to compare the new stopping criterion with the old criterion. For this reason, the obtained results were not compared with other segmentation approaches, since with the old stopping criterion were previously carried out. version:1
arxiv-1311-1939 | Fast Tracking via Spatio-Temporal Context Learning | http://arxiv.org/abs/1311.1939 | id:1311.1939 author:Kaihua Zhang, Lei Zhang, Ming-Hsuan Yang, David Zhang category:cs.CV  published:2013-11-08 summary:In this paper, we present a simple yet fast and robust algorithm which exploits the spatio-temporal context for visual tracking. Our approach formulates the spatio-temporal relationships between the object of interest and its local context based on a Bayesian framework, which models the statistical correlation between the low-level features (i.e., image intensity and position) from the target and its surrounding regions. The tracking problem is posed by computing a confidence map, and obtaining the best target location by maximizing an object location likelihood function. The Fast Fourier Transform is adopted for fast learning and detection in this work. Implemented in MATLAB without code optimization, the proposed tracker runs at 350 frames per second on an i7 machine. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods in terms of efficiency, accuracy and robustness. version:1
arxiv-1311-1903 | Moment-based Uniform Deviation Bounds for $k$-means and Friends | http://arxiv.org/abs/1311.1903 | id:1311.1903 author:Matus Telgarsky, Sanjoy Dasgupta category:cs.LG stat.ML  published:2013-11-08 summary:Suppose $k$ centers are fit to $m$ points by heuristically minimizing the $k$-means cost; what is the corresponding fit over the source distribution? This question is resolved here for distributions with $p\geq 4$ bounded moments; in particular, the difference between the sample cost and distribution cost decays with $m$ and $p$ as $m^{\min\{-1/4, -1/2+2/p\}}$. The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions. To further demonstrate this mechanism, a soft clustering variant of $k$-means cost is also considered, namely the log likelihood of a Gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum. Lastly, a rate with refined constants is provided for $k$-means instances possessing some cluster structure. version:1
arxiv-1311-1897 | Logique mathématique et linguistique formelle | http://arxiv.org/abs/1311.1897 | id:1311.1897 author:Christian Retoré category:math.LO cs.CL  published:2013-11-08 summary:As the etymology of the word shows, logic is intimately related to language, as exemplified by the work of philosophers from Antiquity and from the Middle-Age. At the beginning of the XX century, the crisis of the foundations of mathematics invented mathematical logic and imposed logic as a language-based foundation for mathematics. How did the relations between logic and language evolved in this newly defined mathematical framework? After a survey of the history of the relation between logic and linguistics, traditionally focused on semantics, we focus on some present issues: 1) grammar as a deductive system 2) the transformation of the syntactic structure of a sentence to a logical formula representing its meaning 3) taking into account the context when interpreting words. This lecture shows that type theory provides a convenient framework both for natural language syntax and for the interpretation of any of tis level (words, sentences, discourse). version:1
arxiv-1311-1731 | Stochastic blockmodel approximation of a graphon: Theory and consistent estimation | http://arxiv.org/abs/1311.1731 | id:1311.1731 author:Edoardo M Airoldi, Thiago B Costa, Stanley H Chan category:stat.ME cs.LG cs.SI physics.data-an stat.ML  published:2013-11-07 summary:Non-parametric approaches for analyzing network data based on exchangeable graph models (ExGM) have recently gained interest. The key object that defines an ExGM is often referred to as a graphon. This non-parametric perspective on network modeling poses challenging questions on how to make inference on the graphon underlying observed network data. In this paper, we propose a computationally efficient procedure to estimate a graphon from a set of observed networks generated from it. This procedure is based on a stochastic blockmodel approximation (SBA) of the graphon. We show that, by approximating the graphon with a stochastic block model, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches infinity. version:2
arxiv-1311-1869 | Optimization, Learning, and Games with Predictable Sequences | http://arxiv.org/abs/1311.1869 | id:1311.1869 author:Alexander Rakhlin, Karthik Sridharan category:cs.LG cs.GT  published:2013-11-08 summary:We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea of predictable sequences. First, we recover the Mirror Prox algorithm for offline optimization, prove an extension to Holder-smooth functions, and apply the results to saddle-point type problems. Next, we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a finite zero-sum matrix game to converge to the minimax equilibrium at the rate of O((log T)/T). This addresses a question of Daskalakis et al 2011. Further, we consider a partial information version of the problem. We then apply the results to convex programming and exhibit a simple algorithm for the approximate Max Flow problem. version:1
arxiv-1304-8132 | Local Graph Clustering Beyond Cheeger's Inequality | http://arxiv.org/abs/1304.8132 | id:1304.8132 author:Zeyuan Allen Zhu, Silvio Lattanzi, Vahab Mirrokni category:cs.DS cs.LG stat.ML  published:2013-04-30 summary:Motivated by applications of large-scale graph clustering, we study random-walk-based LOCAL algorithms whose running times depend only on the size of the output cluster, rather than the entire graph. All previously known such algorithms guarantee an output conductance of $\tilde{O}(\sqrt{\phi(A)})$ when the target set $A$ has conductance $\phi(A)\in[0,1]$. In this paper, we improve it to $$\tilde{O}\bigg( \min\Big\{\sqrt{\phi(A)}, \frac{\phi(A)}{\sqrt{\mathsf{Conn}(A)}} \Big\} \bigg)\enspace, $$ where the internal connectivity parameter $\mathsf{Conn}(A) \in [0,1]$ is defined as the reciprocal of the mixing time of the random walk over the induced subgraph on $A$. For instance, using $\mathsf{Conn}(A) = \Omega(\lambda(A) / \log n)$ where $\lambda$ is the second eigenvalue of the Laplacian of the induced subgraph on $A$, our conductance guarantee can be as good as $\tilde{O}(\phi(A)/\sqrt{\lambda(A)})$. This builds an interesting connection to the recent advance of the so-called improved Cheeger's Inequality [KKL+13], which says that global spectral algorithms can provide a conductance guarantee of $O(\phi_{\mathsf{opt}}/\sqrt{\lambda_3})$ instead of $O(\sqrt{\phi_{\mathsf{opt}}})$. In addition, we provide theoretical guarantee on the clustering accuracy (in terms of precision and recall) of the output set. We also prove that our analysis is tight, and perform empirical evaluation to support our theory on both synthetic and real data. It is worth noting that, our analysis outperforms prior work when the cluster is well-connected. In fact, the better it is well-connected inside, the more significant improvement (both in terms of conductance and accuracy) we can obtain. Our results shed light on why in practice some random-walk-based algorithms perform better than its previous theory, and help guide future research about local clustering. version:2
arxiv-1311-1761 | Exploring Deep and Recurrent Architectures for Optimal Control | http://arxiv.org/abs/1311.1761 | id:1311.1761 author:Sergey Levine category:cs.LG cs.AI cs.NE cs.RO cs.SY  published:2013-11-07 summary:Sophisticated multilayer neural networks have achieved state of the art results on multiple supervised tasks. However, successful applications of such multilayer networks to control have so far been limited largely to the perception portion of the control pipeline. In this paper, we explore the application of deep and recurrent neural networks to a continuous, high-dimensional locomotion task, where the network is used to represent a control policy that maps the state of the system (represented by joint angles) directly to the torques at each joint. By using a recent reinforcement learning algorithm called guided policy search, we can successfully train neural network controllers with thousands of parameters, allowing us to compare a variety of architectures. We discuss the differences between the locomotion control task and previous supervised perception tasks, present experimental results comparing various architectures, and discuss future directions in the application of techniques from deep learning to the problem of optimal control. version:1
arxiv-1311-1694 | Biometric Signature Processing & Recognition Using Radial Basis Function Network | http://arxiv.org/abs/1311.1694 | id:1311.1694 author:Ankit Chadha, Neha Satam, Vibha Wali category:cs.CV  published:2013-11-07 summary:Automatic recognition of signature is a challenging problem which has received much attention during recent years due to its many applications in different fields. Signature has been used for long time for verification and authentication purpose. Earlier methods were manual but nowadays they are getting digitized. This paper provides an efficient method to signature recognition using Radial Basis Function Network. The network is trained with sample images in database. Feature extraction is performed before using them for training. For testing purpose, an image is made to undergo rotation-translation-scaling correction and then given to network. The network successfully identifies the original image and gives correct output for stored database images also. The method provides recognition rate of approximately 80% for 200 samples. version:1
arxiv-1311-1644 | The Maximum Entropy Relaxation Path | http://arxiv.org/abs/1311.1644 | id:1311.1644 author:Moshe Dubiner, Matan Gavish, Yoram Singer category:cs.LG math.OC stat.ML  published:2013-11-07 summary:The relaxed maximum entropy problem is concerned with finding a probability distribution on a finite set that minimizes the relative entropy to a given prior distribution, while satisfying relaxed max-norm constraints with respect to a third observed multinomial distribution. We study the entire relaxation path for this problem in detail. We show existence and a geometric description of the relaxation path. Specifically, we show that the maximum entropy relaxation path admits a planar geometric description as an increasing, piecewise linear function in the inverse relaxation parameter. We derive fast algorithms for tracking the path. In various realistic settings, our algorithms require $O(n\log(n))$ operations for probability distributions on $n$ points, making it possible to handle large problems. Once the path has been recovered, we show that given a validation set, the family of admissible models is reduced from an infinite family to a small, discrete set. We demonstrate the merits of our approach in experiments with synthetic data and discuss its potential for the estimation of compact n-gram language models. version:1
arxiv-1311-1539 | Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics | http://arxiv.org/abs/1311.1539 | id:1311.1539 author:Edward Grefenstette category:cs.CL cs.LG math.CT math.LO I.2.7  published:2013-11-06 summary:This thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. It models words as distributions over these contexts and represents them as vectors in high dimensional spaces. The problem of compositionality for such models concerns itself with how to produce representations for larger units of text by composing the representations of smaller units of text. This thesis focuses on a particular approach to this compositionality problem, namely using the categorical framework developed by Coecke, Sadrzadeh, and Clark, which combines syntactic analysis formalisms with distributional semantic representations of meaning to produce syntactically motivated composition operations. This thesis shows how this approach can be theoretically extended and practically implemented to produce concrete compositional distributional models of natural language semantics. It furthermore demonstrates that such models can perform on par with, or better than, other competing approaches in the field of natural language processing. There are three principal contributions to computational linguistics in this thesis. The first is to extend the DisCoCat framework on the syntactic front and semantic front, incorporating a number of syntactic analysis formalisms and providing learning procedures allowing for the generation of concrete compositional distributional models. The second contribution is to evaluate the models developed from the procedures presented here, showing that they outperform other compositional distributional models present in the literature. The third contribution is to show how using category theory to solve linguistic problems forms a sound basis for research, illustrated by examples of work on this topic, that also suggest directions for future research. version:1
arxiv-1311-6355 | Exploration in Interactive Personalized Music Recommendation: A Reinforcement Learning Approach | http://arxiv.org/abs/1311.6355 | id:1311.6355 author:Xinxi Wang, Yi Wang, David Hsu, Ye Wang category:cs.MM cs.IR cs.LG H.3.3; H.5.5  published:2013-11-06 summary:Current music recommender systems typically act in a greedy fashion by recommending songs with the highest user ratings. Greedy recommendation, however, is suboptimal over the long term: it does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the needs to explore user preferences and to exploit this information for recommendation. This paper presents a new approach to music recommendation by formulating this exploration-exploitation trade-off as a reinforcement learning task called the multi-armed bandit. To learn user preferences, it uses a Bayesian model, which accounts for both audio content and the novelty of recommendations. A piecewise-linear approximation to the model and a variational inference algorithm are employed to speed up Bayesian inference. One additional benefit of our approach is a single unified model for both music recommendation and playlist generation. Both simulation results and a user study indicate strong potential for the new approach. version:1
arxiv-1311-0830 | The Squared-Error of Generalized LASSO: A Precise Analysis | http://arxiv.org/abs/1311.0830 | id:1311.0830 author:Samet Oymak, Christos Thrampoulidis, Babak Hassibi category:cs.IT math.IT math.OC stat.ML  published:2013-11-04 summary:We consider the problem of estimating an unknown signal $x_0$ from noisy linear observations $y = Ax_0 + z\in R^m$. In many practical instances, $x_0$ has a certain structure that can be captured by a structure inducing convex function $f(\cdot)$. For example, $\ell_1$ norm can be used to encourage a sparse solution. To estimate $x_0$ with the aid of $f(\cdot)$, we consider the well-known LASSO method and provide sharp characterization of its performance. We assume the entries of the measurement matrix $A$ and the noise vector $z$ have zero-mean normal distributions with variances $1$ and $\sigma^2$ respectively. For the LASSO estimator $x^*$, we attempt to calculate the Normalized Square Error (NSE) defined as $\frac{\ x^*-x_0\ _2^2}{\sigma^2}$ as a function of the noise level $\sigma$, the number of observations $m$ and the structure of the signal. We show that, the structure of the signal $x_0$ and choice of the function $f(\cdot)$ enter the error formulae through the summary parameters $D(cone)$ and $D(\lambda)$, which are defined as the Gaussian squared-distances to the subdifferential cone and to the $\lambda$-scaled subdifferential, respectively. The first LASSO estimator assumes a-priori knowledge of $f(x_0)$ and is given by $\arg\min_{x}\{{\ y-Ax\ _2}~\text{subject to}~f(x)\leq f(x_0)\}$. We prove that its worst case NSE is achieved when $\sigma\rightarrow 0$ and concentrates around $\frac{D(cone)}{m-D(cone)}$. Secondly, we consider $\arg\min_{x}\{\ y-Ax\ _2+\lambda f(x)\}$, for some $\lambda\geq 0$. This time the NSE formula depends on the choice of $\lambda$ and is given by $\frac{D(\lambda)}{m-D(\lambda)}$. We then establish a mapping between this and the third estimator $\arg\min_{x}\{\frac{1}{2}\ y-Ax\ _2^2+ \lambda f(x)\}$. Finally, for a number of important structured signal classes, we translate our abstract formulae to closed-form upper bounds on the NSE. version:2
arxiv-1311-1279 | Face Recognition via Globality-Locality Preserving Projections | http://arxiv.org/abs/1311.1279 | id:1311.1279 author:Sheng Huang, Dan Yang, Fei Yang, Yongxin Ge, Xiaohong Zhang, Jiwen Lu category:cs.CV  published:2013-11-06 summary:We present an improved Locality Preserving Projections (LPP) method, named Gloablity-Locality Preserving Projections (GLPP), to preserve both the global and local geometric structures of data. In our approach, an additional constraint of the geometry of classes is imposed to the objective function of conventional LPP for respecting some more global manifold structures. Moreover, we formulate a two-dimensional extension of GLPP (2D-GLPP) as an example to show how to extend GLPP with some other statistical techniques. We apply our works to face recognition on four popular face databases, namely ORL, Yale, FERET and LFW-A databases, and extensive experimental results demonstrate that the considered global manifold information can significantly improve the performance of LPP and the proposed face recognition methods outperform the state-of-the-arts. version:1
arxiv-1211-1642 | Randomized Dimension Reduction on Massive Data | http://arxiv.org/abs/1211.1642 | id:1211.1642 author:Stoyan Georgiev, Sayan Mukherjee category:stat.ML stat.ME  published:2012-11-07 summary:Scalability of statistical estimators is of increasing importance in modern applications and dimension reduction is often used to extract relevant information from data. A variety of popular dimension reduction approaches can be framed as symmetric generalized eigendecomposition problems. In this paper we outline how taking into account the low rank structure assumption implicit in these dimension reduction approaches provides both computational and statistical advantages. We adapt recent randomized low-rank approximation algorithms to provide efficient solutions to three dimension reduction methods: Principal Component Analysis (PCA), Sliced Inverse Regression (SIR), and Localized Sliced Inverse Regression (LSIR). A key observation in this paper is that randomization serves a dual role, improving both computational and statistical performance. This point is highlighted in our experiments on real and simulated data. version:2
arxiv-1311-1223 | Quality Assessment of Pixel-Level ImageFusion Using Fuzzy Logic | http://arxiv.org/abs/1311.1223 | id:1311.1223 author:Srinivasa Rao Dammavalam, Seetha Maddala, M. H. M. Krishna Prasad category:cs.CV  published:2013-11-05 summary:Image fusion is to reduce uncertainty and minimize redundancy in the output while maximizing relevant information from two or more images of a scene into a single composite image that is more informative and is more suitable for visual perception or processing tasks like medical imaging, remote sensing, concealed weapon detection, weather forecasting, biometrics etc. Image fusion combines registered images to produce a high quality fused image with spatial and spectral information. The fused image with more information will improve the performance of image analysis algorithms used in different applications. In this paper, we proposed a fuzzy logic method to fuse images from different sensors, in order to enhance the quality and compared proposed method with two other methods i.e. image fusion using wavelet transform and weighted average discrete wavelet transform based image fusion using genetic algorithm (here onwards abbreviated as GA) along with quality evaluation parameters image quality index (IQI), mutual information measure (MIM), root mean square error (RMSE), peak signal to noise ratio (PSNR), fusion factor (FF), fusion symmetry (FS) and fusion index (FI) and entropy. The results obtained from proposed fuzzy based image fusion approach improves quality of fused image as compared to earlier reported methods, wavelet transform based image fusion and weighted average discrete wavelet transform based image fusion using genetic algorithm. version:1
arxiv-1311-1194 | Identifying Purpose Behind Electoral Tweets | http://arxiv.org/abs/1311.1194 | id:1311.1194 author:Saif M. Mohammad, Svetlana Kiritchenko, Joel Martin category:cs.CL  published:2013-11-05 summary:Tweets pertaining to a single event, such as a national election, can number in the hundreds of millions. Automatically analyzing them is beneficial in many downstream natural language applications such as question answering and summarization. In this paper, we propose a new task: identifying the purpose behind electoral tweets--why do people post election-oriented tweets? We show that identifying purpose is correlated with the related phenomenon of sentiment and emotion detection, but yet significantly different. Detecting purpose has a number of applications including detecting the mood of the electorate, estimating the popularity of policies, identifying key issues of contention, and predicting the course of events. We create a large dataset of electoral tweets and annotate a few thousand tweets for purpose. We develop a system that automatically classifies electoral tweets as per their purpose, obtaining an accuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-class task (both accuracies well above the most-frequent-class baseline). Finally, we show that resources developed for emotion detection are also helpful for detecting purpose. version:1
arxiv-1311-1189 | Statistical Inference in Hidden Markov Models using $k$-segment Constraints | http://arxiv.org/abs/1311.1189 | id:1311.1189 author:Michalis K. Titsias, Christopher Yau, Christopher C. Holmes category:stat.ME cs.LG stat.ML  published:2013-11-05 summary:Hidden Markov models (HMMs) are one of the most widely used statistical methods for analyzing sequence data. However, the reporting of output from HMMs has largely been restricted to the presentation of the most-probable (MAP) hidden state sequence, found via the Viterbi algorithm, or the sequence of most probable marginals using the forward-backward (F-B) algorithm. In this article, we expand the amount of information we could obtain from the posterior distribution of an HMM by introducing linear-time dynamic programming algorithms that, we collectively call $k$-segment algorithms, that allow us to i) find MAP sequences, ii) compute posterior probabilities and iii) simulate sample paths conditional on a user specified number of segments, i.e. contiguous runs in a hidden state, possibly of a particular type. We illustrate the utility of these methods using simulated and real examples and highlight the application of prospective and retrospective use of these methods for fitting HMMs or exploring existing model fits. version:1
arxiv-1311-1169 | Using Robust PCA to estimate regional characteristics of language use from geo-tagged Twitter messages | http://arxiv.org/abs/1311.1169 | id:1311.1169 author:Dániel Kondor, István Csabai, László Dobos, János Szüle, Norbert Barankai, Tamás Hanyecz, Tamás Sebők, Zsófia Kallus, Gábor Vattay category:cs.CL  published:2013-11-05 summary:Principal component analysis (PCA) and related techniques have been successfully employed in natural language processing. Text mining applications in the age of the online social media (OSM) face new challenges due to properties specific to these use cases (e.g. spelling issues specific to texts posted by users, the presence of spammers and bots, service announcements, etc.). In this paper, we employ a Robust PCA technique to separate typical outliers and highly localized topics from the low-dimensional structure present in language use in online social networks. Our focus is on identifying geospatial features among the messages posted by the users of the Twitter microblogging service. Using a dataset which consists of over 200 million geolocated tweets collected over the course of a year, we investigate whether the information present in word usage frequencies can be used to identify regional features of language use and topics of interest. Using the PCA pursuit method, we are able to identify important low-dimensional features, which constitute smoothly varying functions of the geographic location. version:1
arxiv-1311-0222 | Online Learning with Multiple Operator-valued Kernels | http://arxiv.org/abs/1311.0222 | id:1311.0222 author:Julien Audiffren, Hachem Kadri category:cs.LG stat.ML  published:2013-11-01 summary:We consider the problem of learning a vector-valued function f in an online learning setting. The function f is assumed to lie in a reproducing Hilbert space of operator-valued kernels. We describe two online algorithms for learning f while taking into account the output structure. A first contribution is an algorithm, ONORMA, that extends the standard kernel-based online learning algorithm NORMA from scalar-valued to operator-valued setting. We report a cumulative error bound that holds both for classification and regression. We then define a second algorithm, MONORMA, which addresses the limitation of pre-defining the output structure in ONORMA by learning sequentially a linear combination of operator-valued kernels. Our experiments show that the proposed algorithms achieve good performance results with low computational cost. version:2
arxiv-1311-1132 | Motion and audio analysis in mobile devices for remote monitoring of physical activities and user authentication | http://arxiv.org/abs/1311.1132 | id:1311.1132 author:Hamed Ketabdar, Jalaluddin Qureshi, Pan Hui category:cs.HC cs.CV  published:2013-11-05 summary:In this article we propose the use of accelerometer embedded by default in smartphone as a cost-effective, reliable and efficient way to provide remote physical activity monitoring for the elderly and people requiring healthcare service. Mobile phones are regularly carried by users during their day-to-day work routine, physical movement information can be captured by the mobile phone accelerometer, processed and sent to a remote server for monitoring. The acceleration pattern can deliver information related to the pattern of physical activities the user is engaged in. We further show how this technique can be extended to provide implicit real-time security by analysing unexpected movements captured by the phone accelerometer, and automatically locking the phone in such situation to prevent unauthorised access. This technique is also shown to provide implicit continuous user authentication, by capturing regular user movements such as walking, and requesting for re-authentication whenever it detects a non-regular movement. version:1
arxiv-1207-2340 | Pseudo-likelihood methods for community detection in large sparse networks | http://arxiv.org/abs/1207.2340 | id:1207.2340 author:Arash A. Amini, Aiyou Chen, Peter J. Bickel, Elizaveta Levina category:cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH  published:2012-07-10 summary:Many algorithms have been proposed for fitting network models with communities, but most of them do not scale well to large networks, and often fail on sparse networks. Here we propose a new fast pseudo-likelihood method for fitting the stochastic block model for networks, as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees. We show that the algorithms perform well under a range of settings, including on very sparse networks, and illustrate on the example of a network of political blogs. We also propose spectral clustering with perturbations, a method of independent interest, which works well on sparse networks where regular spectral clustering fails, and use it to provide an initial value for pseudo-likelihood. We prove that pseudo-likelihood provides consistent estimates of the communities under a mild condition on the starting value, for the case of a block model with two communities. version:3
arxiv-1305-0258 | Inverting Nonlinear Dimensionality Reduction with Scale-Free Radial Basis Function Interpolation | http://arxiv.org/abs/1305.0258 | id:1305.0258 author:Nathan D. Monnig, Bengt Fornberg, Francois G. Meyer category:math.NA cs.NA physics.data-an stat.ML  published:2013-05-01 summary:Nonlinear dimensionality reduction embeddings computed from datasets do not provide a mechanism to compute the inverse map. In this paper, we address the problem of computing a stable inverse map to such a general bi-Lipschitz map. Our approach relies on radial basis functions (RBFs) to interpolate the inverse map everywhere on the low-dimensional image of the forward map. We demonstrate that the scale-free cubic RBF kernel performs better than the Gaussian kernel: it does not suffer from ill-conditioning, and does not require the choice of a scale. The proposed construction is shown to be similar to the Nystr\"om extension of the eigenvectors of the symmetric normalized graph Laplacian matrix. Based on this observation, we provide a new interpretation of the Nystr\"om extension with suggestions for improvement. version:2
arxiv-1311-1090 | Polyhedrons and Perceptrons Are Functionally Equivalent | http://arxiv.org/abs/1311.1090 | id:1311.1090 author:Daniel Crespin category:cs.NE 68T01 C.1.3; I.2.6  published:2013-11-05 summary:Mathematical definitions of polyhedrons and perceptron networks are discussed. The formalization of polyhedrons is done in a rather traditional way. For networks, previously proposed systems are developed. Perceptron networks in disjunctive normal form (DNF) and conjunctive normal forms (CNF) are introduced. The main theme is that single output perceptron neural networks and characteristic functions of polyhedrons are one and the same class of functions. A rigorous formulation and proof that three layers suffice is obtained. The various constructions and results are among several steps required for algorithms that replace incremental and statistical learning with more efficient, direct and exact geometric methods for calculation of perceptron architecture and weights. version:1
arxiv-1311-1040 | Fifth-order canonical polyadic decomposition with partial symmetry via joint diagonalization for combined independent component analysis and canonical / Parallel factor analysis | http://arxiv.org/abs/1311.1040 | id:1311.1040 author:Xiao-Feng Gong, Cheng-Yuan Wang, Ya-Na Hao, Qiu-Hua Lin category:stat.ML cs.LG  published:2013-11-05 summary:Recently, there has been a trend to combine independent component analysis and canonical / parallel factor analysis (ICA-CPA) for an enhanced robustness for the computation of CPA, and ICA-CPA could be further converted into the problem of canonical polyadic decomposition (CPD) of a 5th-order partially symmetric tensor, by calculating the 4th-order cumulant of a trilinear mixture. In this study, we propose a new 5th-order CPD algorithm constrained with partial symmetry using joint diagonalization. As the main steps involved in the proposed algorithm undergo no updating iterations for the loading matrices, it is much faster than the existing algorithm based on alternating least squares and enhanced line search, and therefore could be used as a nice initialization for the latter. Simulation results are given to examine the performance of the proposed algorithm. version:1
arxiv-1306-5554 | Correlated random features for fast semi-supervised learning | http://arxiv.org/abs/1306.5554 | id:1306.5554 author:Brian McWilliams, David Balduzzi, Joachim M. Buhmann category:stat.ML cs.LG  published:2013-06-24 summary:This paper presents Correlated Nystrom Views (XNV), a fast semi-supervised algorithm for regression and classification. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, XNV applies multiview regression using Canonical Correlation Analysis (CCA) on unlabeled data to bias the regression towards useful features. It has been shown that, if the views contains accurate estimators, CCA regression can substantially reduce variance with a minimal increase in bias. Random views are justified by recent theoretical and empirical work showing that regression with random features closely approximates kernel regression, implying that random views can be expected to contain accurate estimators. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. version:2
arxiv-1311-0914 | A Divide-and-Conquer Solver for Kernel Support Vector Machines | http://arxiv.org/abs/1311.0914 | id:1311.0914 author:Cho-Jui Hsieh, Si Si, Inderjit S. Dhillon category:cs.LG  published:2013-11-04 summary:The kernel support vector machine (SVM) is one of the most widely used classification methods; however, the amount of computation required becomes the bottleneck when facing millions of samples. In this paper, we propose and analyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently. We show theoretically that the support vectors identified by the subproblem solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering. In the conquer step, the local solutions from the subproblems are used to initialize a global coordinate descent solver, which converges quickly as suggested by our analysis. By extending this idea, we develop a multilevel Divide-and-Conquer SVM algorithm with adaptive clustering and early prediction strategy, which outperforms state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on the covtype dataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM in obtaining the exact SVM solution (to within $10^{-6}$ relative error) which achieves 96.15% prediction accuracy. Moreover, with our proposed early prediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes, which is more than 100 times faster than LIBSVM. version:1
arxiv-1311-0833 | A Comparative Study on Linguistic Feature Selection in Sentiment Polarity Classification | http://arxiv.org/abs/1311.0833 | id:1311.0833 author:Zitao Liu category:cs.CL  published:2013-11-04 summary:Sentiment polarity classification is perhaps the most widely studied topic. It classifies an opinionated document as expressing a positive or negative opinion. In this paper, using movie review dataset, we perform a comparative study with different single kind linguistic features and the combinations of these features. We find that the classic topic-based classifier(Naive Bayes and Support Vector Machine) do not perform as well on sentiment polarity classification. And we find that with some combination of different linguistic features, the classification accuracy can be boosted a lot. We give some reasonable explanations about these boosting outcomes. version:1
arxiv-1311-1406 | TOP-SPIN: TOPic discovery via Sparse Principal component INterference | http://arxiv.org/abs/1311.1406 | id:1311.1406 author:Martin Takáč, Selin Damla Ahipaşaoğlu, Ngai-Man Cheung, Peter Richtárik category:cs.CV cs.IR cs.LG  published:2013-11-04 summary:We propose a novel topic discovery algorithm for unlabeled images based on the bag-of-words (BoW) framework. We first extract a dictionary of visual words and subsequently for each image compute a visual word occurrence histogram. We view these histograms as rows of a large matrix from which we extract sparse principal components (PCs). Each PC identifies a sparse combination of visual words which co-occur frequently in some images but seldom appear in others. Each sparse PC corresponds to a topic, and images whose interference with the PC is high belong to that topic, revealing the common parts possessed by the images. We propose to solve the associated sparse PCA problems using an Alternating Maximization (AM) method, which we modify for purpose of efficiently extracting multiple PCs in a deflation scheme. Our approach attacks the maximization problem in sparse PCA directly and is scalable to high-dimensional data. Experiments on automatic topic discovery and category prediction demonstrate encouraging performance of our approach. version:1
arxiv-1311-0800 | Distributed Exploration in Multi-Armed Bandits | http://arxiv.org/abs/1311.0800 | id:1311.0800 author:Eshcar Hillel, Zohar Karnin, Tomer Koren, Ronny Lempel, Oren Somekh category:cs.LG  published:2013-11-04 summary:We study exploration in Multi-Armed Bandits in a setting where $k$ players collaborate in order to identify an $\epsilon$-optimal arm. Our motivation comes from recent employment of bandit algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the $k$ players to communicate only once, they are able to learn $\sqrt{k}$ times faster than a single player. That is, distributing learning to $k$ players gives rise to a factor $\sqrt{k}$ parallel speed-up. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor $k$ speed-up in learning performance, with communication only logarithmic in $1/\epsilon$. version:1
arxiv-1311-0646 | A Parallel Compressive Imaging Architecture for One-Shot Acquisition | http://arxiv.org/abs/1311.0646 | id:1311.0646 author:Tomas Björklund, Enrico Magli category:cs.CV astro-ph.IM  published:2013-11-04 summary:A limitation of many compressive imaging architectures lies in the sequential nature of the sensing process, which leads to long sensing times. In this paper we present a novel architecture that uses fewer detectors than the number of reconstructed pixels and is able to acquire the image in a single acquisition. This paves the way for the development of video architectures that acquire several frames per second. We specifically address the diffraction problem, showing that deconvolution normally used to recover diffraction blur can be replaced by convolution of the sensing matrix, and how measurements of a 0/1 physical sensing matrix can be converted to -1/1 compressive sensing matrix without any extra acquisitions. Simulations of our architecture show that the image quality is comparable to that of a classic Compressive Imaging camera, whereas the proposed architecture avoids long acquisition times due to sequential sensing. This one-shot procedure also allows to employ a fixed sensing matrix instead of a complex device such as a Digital Micro Mirror array or Spatial Light Modulator. It also enables imaging at bandwidths where these are not efficient. version:1
arxiv-1311-0636 | A Parallel SGD method with Strong Convergence | http://arxiv.org/abs/1311.0636 | id:1311.0636 author:Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan, Leon Bottou category:cs.LG cs.DC  published:2013-11-04 summary:This paper proposes a novel parallel stochastic gradient descent (SGD) method that is obtained by applying parallel sets of SGD iterations (each set operating on one node using the data residing in it) for finding the direction in each iteration of a batch descent method. The method has strong convergence properties. Experiments on datasets with high dimensional feature spaces show the value of this method. version:1
arxiv-1306-0811 | A Gang of Bandits | http://arxiv.org/abs/1306.0811 | id:1306.0811 author:Nicolò Cesa-Bianchi, Claudio Gentile, Giovanni Zappella category:cs.LG cs.SI stat.ML  published:2013-06-04 summary:Multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems. In many cases, however, these applications have a strong social component, whose integration in the bandit algorithm could lead to a dramatic performance increase. For instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. In this paper, we introduce novel algorithmic approaches to the solution of such networked bandit problems. More specifically, we design and analyze a global strategy which allocates a bandit algorithm to each network node (user) and allows it to "share" signals (contexts and payoffs) with the neghboring nodes. We then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes. We experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information. Our experiments, carried out on synthetic and real-world datasets, show a marked increase in prediction performance obtained by exploiting the network structure. version:3
arxiv-1311-0622 | Stochastic Dual Coordinate Ascent with Alternating Direction Multiplier Method | http://arxiv.org/abs/1311.0622 | id:1311.0622 author:Taiji Suzuki category:stat.ML  published:2013-11-04 summary:We propose a new stochastic dual coordinate ascent technique that can be applied to a wide range of regularized learning problems. Our method is based on Alternating Direction Multiplier Method (ADMM) to deal with complex regularization functions such as structured regularizations. Although the original ADMM is a batch method, the proposed method offers a stochastic update rule where each iteration requires only one or few sample observations. Moreover, our method can naturally afford mini-batch update and it gives speed up of convergence. We show that, under mild assumptions, our method converges exponentially. The numerical experiments show that our method actually performs efficiently. version:1
arxiv-1311-0598 | Q-Gaussian Swarm Quantum Particle Intelligence on Predicting Global Minimum of Potential Energy Function | http://arxiv.org/abs/1311.0598 | id:1311.0598 author:Hiqmet Kamberaj category:cs.NE  published:2013-11-04 summary:We present a newly developed -Gaussian Swarm Quantum-like Particle Optimization (q-GSQPO) algorithm to determine the global minimum of the potential energy function. Swarm Quantum-like Particle Optimization (SQPO) algorithms have been derived using different attractive potential fields to represent swarm particles moving in a quantum environment, where the one which uses a harmonic oscillator potential as attractive field is considered as an improved version. In this paper, we propose a new SQPO that uses -Gaussian probability density function for the attractive potential field (q-GSQPO) rather than Gaussian one (GSQPO) which corresponds to harmonic potential. The performance of the q-GSQPO is compared against the GSQPO. The new algorithm outperforms the GSQPO on most of the time in convergence to the global optimum by increasing the efficiency of sampling the phase space and avoiding the premature convergence to local minima. Moreover, the computational efforts were comparable for both algorithms. We tested the algorithm to determine the lowest energy configurations of a particle moving in a 2, 5, 10, and 50 dimensional spaces. version:1
arxiv-1307-1739 | Anatomical Feature-guided Volumeric Registration of Multimodal Prostate MRI | http://arxiv.org/abs/1307.1739 | id:1307.1739 author:Xin Zhao, Arie Kaufman category:cs.CV cs.GR  published:2013-07-06 summary:Radiological imaging of prostate is becoming more popular among researchers and clinicians in searching for diseases, primarily cancer. Scans might be acquired at different times, with patient movement between scans, or with different equipment, resulting in multiple datasets that need to be registered. For this issue, we introduce a registration method using anatomical feature-guided mutual information. Prostate scans of the same patient taken in three different orientations are first aligned for the accurate detection of anatomical features in 3D. Then, our pipeline allows for multiple modalities registration through the use of anatomical features, such as the interior urethra of prostate and gland utricle, in a bijective way. The novelty of this approach is the application of anatomical features as the pre-specified corresponding landmarks for prostate registration. We evaluate the registration results through both artificial and clinical datasets. Registration accuracy is evaluated by performing statistical analysis of local intensity differences or spatial differences of anatomical landmarks between various MR datasets. Evaluation results demonstrate that our method statistics-significantly improves the quality of registration. Although this strategy is tested for MRI-guided brachytherapy, the preliminary results from these experiments suggest that it can be also applied to other settings such as transrectal ultrasound-guided or CT-guided therapy, where the integration of preoperative MRI may have a significant impact upon treatment planning and guidance. version:2
arxiv-1311-0468 | Thompson Sampling for Online Learning with Linear Experts | http://arxiv.org/abs/1311.0468 | id:1311.0468 author:Aditya Gopalan category:stat.ML cs.LG  published:2013-11-03 summary:In this note, we present a version of the Thompson sampling algorithm for the problem of online linear generalization with full information (i.e., the experts setting), studied by Kalai and Vempala, 2005. The algorithm uses a Gaussian prior and time-varying Gaussian likelihoods, and we show that it essentially reduces to Kalai and Vempala's Follow-the-Perturbed-Leader strategy, with exponentially distributed noise replaced by Gaussian noise. This implies sqrt(T) regret bounds for Thompson sampling (with time-varying likelihood) for online learning with full information. version:1
arxiv-1311-0466 | Thompson Sampling for Complex Bandit Problems | http://arxiv.org/abs/1311.0466 | id:1311.0466 author:Aditya Gopalan, Shie Mannor, Yishay Mansour category:stat.ML cs.LG  published:2013-11-03 summary:We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic arms' rewards, and the feedback observed may not necessarily be the reward per-arm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets. version:1
arxiv-1311-0460 | An Adaptive Amoeba Algorithm for Shortest Path Tree Computation in Dynamic Graphs | http://arxiv.org/abs/1311.0460 | id:1311.0460 author:Xiaoge Zhang, Qi Liu, Yong Hu, Felix T. S. Chan, Sankaran Mahadevan, Zili Zhang, Yong Deng category:cs.NE  published:2013-11-03 summary:This paper presents an adaptive amoeba algorithm to address the shortest path tree (SPT) problem in dynamic graphs. In dynamic graphs, the edge weight updates consists of three categories: edge weight increases, edge weight decreases, the mixture of them. Existing work on this problem solve this issue through analyzing the nodes influenced by the edge weight updates and recompute these affected vertices. However, when the network becomes big, the process will become complex. The proposed method can overcome the disadvantages of the existing approaches. The most important feature of this algorithm is its adaptivity. When the edge weight changes, the proposed algorithm can recognize the affected vertices and reconstruct them spontaneously. To evaluate the proposed adaptive amoeba algorithm, we compare it with the Label Setting algorithm and Bellman-Ford algorithm. The comparison results demonstrate the effectiveness of the proposed method. version:1
arxiv-1303-4015 | On multi-class learning through the minimization of the confusion matrix norm | http://arxiv.org/abs/1303.4015 | id:1303.4015 author:Sokol Koço, Cécile Capponi category:cs.LG  published:2013-03-16 summary:In imbalanced multi-class classification problems, the misclassification rate as an error measure may not be a relevant choice. Several methods have been developed where the performance measure retained richer information than the mere misclassification rate: misclassification costs, ROC-based information, etc. Following this idea of dealing with alternate measures of performance, we propose to address imbalanced classification problems by using a new measure to be optimized: the norm of the confusion matrix. Indeed, recent results show that using the norm of the confusion matrix as an error measure can be quite interesting due to the fine-grain informations contained in the matrix, especially in the case of imbalanced classes. Our first contribution then consists in showing that optimizing criterion based on the confusion matrix gives rise to a common background for cost-sensitive methods aimed at dealing with imbalanced classes learning problems. As our second contribution, we propose an extension of a recent multi-class boosting method --- namely AdaBoost.MM --- to the imbalanced class problem, by greedily minimizing the empirical norm of the confusion matrix. A theoretical analysis of the properties of the proposed method is presented, while experimental results illustrate the behavior of the algorithm and show the relevancy of the approach compared to other methods. version:2
arxiv-1310-0319 | Second Croatian Computer Vision Workshop (CCVW 2013) | http://arxiv.org/abs/1310.0319 | id:1310.0319 author:Sven Lončarić, Siniša Šegvić category:cs.CV  published:2013-10-01 summary:Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb. version:3
arxiv-1210-3384 | Inferring clonal evolution of tumors from single nucleotide somatic mutations | http://arxiv.org/abs/1210.3384 | id:1210.3384 author:Wei Jiao, Shankar Vembu, Amit G. Deshwar, Lincoln Stein, Quaid Morris category:cs.LG q-bio.PE q-bio.QM stat.ML  published:2012-10-11 summary:High-throughput sequencing allows the detection and quantification of frequencies of somatic single nucleotide variants (SNV) in heterogeneous tumor cell populations. In some cases, the evolutionary history and population frequency of the subclonal lineages of tumor cells present in the sample can be reconstructed from these SNV frequency measurements. However, automated methods to do this reconstruction are not available and the conditions under which reconstruction is possible have not been described. We describe the conditions under which the evolutionary history can be uniquely reconstructed from SNV frequencies from single or multiple samples from the tumor population and we introduce a new statistical model, PhyloSub, that infers the phylogeny and genotype of the major subclonal lineages represented in the population of cancer cells. It uses a Bayesian nonparametric prior over trees that groups SNVs into major subclonal lineages and automatically estimates the number of lineages and their ancestry. We sample from the joint posterior distribution over trees to identify evolutionary histories and cell population frequencies that have the highest probability of generating the observed SNV frequency data. When multiple phylogenies are consistent with a given set of SNV frequencies, PhyloSub represents the uncertainty in the tumor phylogeny using a partial order plot. Experiments on a simulated dataset and two real datasets comprising tumor samples from acute myeloid leukemia and chronic lymphocytic leukemia patients demonstrate that PhyloSub can infer both linear (or chain) and branching lineages and its inferences are in good agreement with ground truth, where it is available. version:4
arxiv-1311-0396 | Data-based approximate policy iteration for nonlinear continuous-time optimal control design | http://arxiv.org/abs/1311.0396 | id:1311.0396 author:Biao Luo, Huai-Ning Wu, Tingwen Huang, Derong Liu category:cs.SY math.OC stat.ML  published:2013-11-02 summary:This paper addresses the model-free nonlinear optimal problem with generalized cost functional, and a data-based reinforcement learning technique is developed. It is known that the nonlinear optimal control problem relies on the solution of the Hamilton-Jacobi-Bellman (HJB) equation, which is a nonlinear partial differential equation that is generally impossible to be solved analytically. Even worse, most of practical systems are too complicated to establish their accurate mathematical model. To overcome these difficulties, we propose a data-based approximate policy iteration (API) method by using real system data rather than system model. Firstly, a model-free policy iteration algorithm is derived for constrained optimal control problem and its convergence is proved, which can learn the solution of HJB equation and optimal control policy without requiring any knowledge of system mathematical model. The implementation of the algorithm is based on the thought of actor-critic structure, where actor and critic neural networks (NNs) are employed to approximate the control policy and cost function, respectively. To update the weights of actor and critic NNs, a least-square approach is developed based on the method of weighted residuals. The whole data-based API method includes two parts, where the first part is implemented online to collect real system information, and the second part is conducting offline policy iteration to learn the solution of HJB equation and the control policy. Then, the data-based API algorithm is simplified for solving unconstrained optimal control problem of nonlinear and linear systems. Finally, we test the efficiency of the data-based API control design method on a simple nonlinear system, and further apply it to a rotational/translational actuator system. The simulation results demonstrate the effectiveness of the proposed method. version:1
arxiv-1302-2767 | Coherence and sufficient sampling densities for reconstruction in compressed sensing | http://arxiv.org/abs/1302.2767 | id:1302.2767 author:Franz J. Király, Louis Theran category:cs.LG cs.IT math.AG math.IT stat.ML  published:2013-02-12 summary:We give a new, very general, formulation of the compressed sensing problem in terms of coordinate projections of an analytic variety, and derive sufficient sampling rates for signal reconstruction. Our bounds are linear in the coherence of the signal space, a geometric parameter independent of the specific signal and measurement, and logarithmic in the ambient dimension where the signal is presented. We exemplify our approach by deriving sufficient sampling densities for low-rank matrix completion and distance matrix completion which are independent of the true matrix. version:2
arxiv-1311-0360 | Multivariate Generalized Gaussian Process Models | http://arxiv.org/abs/1311.0360 | id:1311.0360 author:Antoni B. Chan category:stat.ML  published:2013-11-02 summary:We propose a family of multivariate Gaussian process models for correlated outputs, based on assuming that the likelihood function takes the generic form of the multivariate exponential family distribution (EFD). We denote this model as a multivariate generalized Gaussian process model, and derive Taylor and Laplace algorithms for approximate inference on the generic model. By instantiating the EFD with specific parameter functions, we obtain two novel GP models (and corresponding inference algorithms) for correlated outputs: 1) a Von-Mises GP for angle regression; and 2) a Dirichlet GP for regressing on the multinomial simplex. version:1
arxiv-1310-2479 | Spatio-temporal variation of conversational utterances on Twitter | http://arxiv.org/abs/1310.2479 | id:1310.2479 author:Christian M. Alis, May T. Lim category:physics.soc-ph cs.CL cs.SI  published:2013-10-09 summary:Conversations reflect the existing norms of a language. Previously, we found that utterance lengths in English fictional conversations in books and movies have shortened over a period of 200 years. In this work, we show that this shortening occurs even for a brief period of 3 years (September 2009-December 2012) using 229 million utterances from Twitter. Furthermore, the subset of geographically-tagged tweets from the United States show an inverse proportion between utterance lengths and the state-level percentage of the Black population. We argue that shortening of utterances can be explained by the increasing usage of jargon including coined words. version:3
arxiv-1311-0317 | Parsimonious Shifted Asymmetric Laplace Mixtures | http://arxiv.org/abs/1311.0317 | id:1311.0317 author:Brian C. Franczak, Paul D. McNicholas, Ryan P. Browne, Paula M. Murray category:stat.ME stat.CO stat.ML  published:2013-11-01 summary:A family of parsimonious shifted asymmetric Laplace mixture models is introduced. We extend the mixture of factor analyzers model to the shifted asymmetric Laplace distribution. Imposing constraints on the constitute parts of the resulting decomposed component scale matrices leads to a family of parsimonious models. An explicit two-stage parameter estimation procedure is described, and the Bayesian information criterion and the integrated completed likelihood are compared for model selection. This novel family of models is applied to real data, where it is compared to its Gaussian analogue within clustering and classification paradigms. version:1
arxiv-1311-0274 | Nearly Optimal Sample Size in Hypothesis Testing for High-Dimensional Regression | http://arxiv.org/abs/1311.0274 | id:1311.0274 author:Adel Javanmard, Andrea Montanari category:math.ST cs.IT cs.LG math.IT stat.ME stat.TH  published:2013-11-01 summary:We consider the problem of fitting the parameters of a high-dimensional linear regression model. In the regime where the number of parameters $p$ is comparable to or exceeds the sample size $n$, a successful approach uses an $\ell_1$-penalized least squares estimator, known as Lasso. Unfortunately, unlike for linear estimators (e.g., ordinary least squares), no well-established method exists to compute confidence intervals or p-values on the basis of the Lasso estimator. Very recently, a line of work \cite{javanmard2013hypothesis, confidenceJM, GBR-hypothesis} has addressed this problem by constructing a debiased version of the Lasso estimator. In this paper, we study this approach for random design model, under the assumption that a good estimator exists for the precision matrix of the design. Our analysis improves over the state of the art in that it establishes nearly optimal \emph{average} testing power if the sample size $n$ asymptotically dominates $s_0 (\log p)^2$, with $s_0$ being the sparsity level (number of non-zero coefficients). Earlier work obtains provable guarantees only for much larger sample size, namely it requires $n$ to asymptotically dominate $(s_0 \log p)^2$. In particular, for random designs with a sparse precision matrix we show that an estimator thereof having the required properties can be computed efficiently. Finally, we evaluate this approach on synthetic data and compare it with earlier proposals. version:1
arxiv-1305-6659 | Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture | http://arxiv.org/abs/1305.6659 | id:1305.6659 author:Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin category:cs.LG stat.ML  published:2013-05-28 summary:This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a low-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets. version:2
arxiv-1307-1493 | Dropout Training as Adaptive Regularization | http://arxiv.org/abs/1307.1493 | id:1307.1493 author:Stefan Wager, Sida Wang, Percy Liang category:stat.ML cs.LG stat.ME  published:2013-07-04 summary:Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learning algorithm, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset. version:2
arxiv-1204-0585 | Convergence Properties of Kronecker Graphical Lasso Algorithms | http://arxiv.org/abs/1204.0585 | id:1204.0585 author:Theodoros Tsiligkaridis, Alfred O. Hero III, Shuheng Zhou category:stat.ME stat.ML  published:2012-04-03 summary:This paper studies iteration convergence of Kronecker graphical lasso (KGLasso) algorithms for estimating the covariance of an i.i.d. Gaussian random sample under a sparse Kronecker-product covariance model and MSE convergence rates. The KGlasso model, originally called the transposable regularized covariance model by Allen ["Transposable regularized covariance models with an application to missing data imputation," Ann. Appl. Statist., vol. 4, no. 2, pp. 764-790, 2010], implements a pair of $\ell_1$ penalties on each Kronecker factor to enforce sparsity in the covariance estimator. The KGlasso algorithm generalizes Glasso, introduced by Yuan and Lin ["Model selection and estimation in the Gaussian graphical model," Biometrika, vol. 94, pp. 19-35, 2007] and Banerjee ["Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data," J. Mach. Learn. Res., vol. 9, pp. 485-516, Mar. 2008], to estimate covariances having Kronecker product form. It also generalizes the unpenalized ML flip-flop (FF) algorithm of Dutilleul ["The MLE algorithm for the matrix normal distribution," J. Statist. Comput. Simul., vol. 64, pp. 105-123, 1999] and Werner ["On estimation of covariance matrices with Kronecker product structure," IEEE Trans. Signal Process., vol. 56, no. 2, pp. 478-491, Feb. 2008] to estimation of sparse Kronecker factors. We establish that the KGlasso iterates converge pointwise to a local maximum of the penalized likelihood function. We derive high dimensional rates of convergence to the true covariance as both the number of samples and the number of variables go to infinity. Our results establish that KGlasso has significantly faster asymptotic convergence than Glasso and FF. Simulations are presented that validate the results of our analysis. version:4
arxiv-1311-2889 | Reinforcement Learning for Matrix Computations: PageRank as an Example | http://arxiv.org/abs/1311.2889 | id:1311.2889 author:Vivek S. Borkar, Adwaitvedant S. Mathkar category:cs.LG cs.SI stat.ML  published:2013-11-01 summary:Reinforcement learning has gained wide popularity as a technique for simulation-driven approximate dynamic programming. A less known aspect is that the very reasons that make it effective in dynamic programming can also be leveraged for using it for distributed schemes for certain matrix computations involving non-negative matrices. In this spirit, we propose a reinforcement learning algorithm for PageRank computation that is fashioned after analogous schemes for approximate dynamic programming. The algorithm has the advantage of ease of distributed implementation and more importantly, of being model-free, i.e., not dependent on any specific assumptions about the transition probabilities in the random web-surfer model. We analyze its convergence and finite time behavior and present some supporting numerical experiments. version:1
arxiv-1311-0162 | Iterative Bilateral Filtering of Polarimetric SAR Data | http://arxiv.org/abs/1311.0162 | id:1311.0162 author:Olivier D'Hondt, Stéphane Guillaso, Olaf Hellwich category:cs.CV  published:2013-11-01 summary:In this paper, we introduce an iterative speckle filtering method for polarimetric SAR (PolSAR) images based on the bilateral filter. To locally adapt to the spatial structure of images, this filter relies on pixel similarities in both spatial and radiometric domains. To deal with polarimetric data, we study the use of similarities based on a statistical distance called Kullback-Leibler divergence as well as two geodesic distances on Riemannian manifolds. To cope with speckle, we propose to progressively refine the result thanks to an iterative scheme. Experiments are run over synthetic and experimental data. First, simulations are generated to study the effects of filtering parameters in terms of polarimetric reconstruction error, edge preservation and smoothing of homogeneous areas. Comparison with other methods shows that our approach compares well to other state of the art methods in the extraction of polarimetric information and shows superior performance for edge restoration and noise smoothing. The filter is then applied to experimental data sets from ESAR and FSAR sensors (DLR) at L-band and S-band, respectively. These last experiments show the ability of the filter to restore structures such as buildings and roads and to preserve boundaries between regions while achieving a high amount of smoothing in homogeneous areas. version:1
arxiv-1311-0124 | Reconstruction of Complex-Valued Fractional Brownian Motion Fields Based on Compressive Sampling and Its Application to PSF Interpolation in Weak Lensing Survey | http://arxiv.org/abs/1311.0124 | id:1311.0124 author:Andriyan B. Suksmono category:cs.CV astro-ph.CO  published:2013-11-01 summary:A new reconstruction method of complex-valued fractional Brownian motion (CV-fBm) field based on Compressive Sampling (CS) is proposed. The decay property of Fourier coefficients magnitude of the fBm signals/ fields indicates that fBms are compressible. Therefore, a few numbers of samples will be sufficient for a CS based method to reconstruct the full field. The effectiveness of the proposed method is showed by simulating, random sampling, and reconstructing CV-fBm fields. Performance evaluation shows advantages of the proposed method over boxcar filtering and thin plate methods. It is also found that the reconstruction performance depends on both of the fBm's Hurst parameter and the number of samples, which in fact is consistent with the CS reconstruction theory. In contrast to other fBm or fractal interpolation methods, the proposed CS based method does not require the knowledge of fractal parameters in the reconstruction process; the inherent sparsity is just sufficient for the CS to do the reconstruction. Potential applicability of the proposed method in weak gravitational lensing survey, particularly for interpolating non-smooth PSF (Point Spread Function) distribution representing distortion by a turbulent field is also discussed. version:1
arxiv-1311-0119 | Structure-preserving color transformations using Laplacian commutativity | http://arxiv.org/abs/1311.0119 | id:1311.0119 author:Davide Eynard, Artiom Kovnatsky, Michael M. Bronstein category:cs.CV cs.GR math.SP  published:2013-11-01 summary:Mappings between color spaces are ubiquitous in image processing problems such as gamut mapping, decolorization, and image optimization for color-blind people. Simple color transformations often result in information loss and ambiguities (for example, when mapping from RGB to grayscale), and one wishes to find an image-specific transformation that would preserve as much as possible the structure of the original image in the target color space. In this paper, we propose Laplacian colormaps, a generic framework for structure-preserving color transformations between images. We use the image Laplacian to capture the structural information, and show that if the color transformation between two images preserves the structure, the respective Laplacians have similar eigenvectors, or in other words, are approximately jointly diagonalizable. Employing the relation between joint diagonalizability and commutativity of matrices, we use Laplacians commutativity as a criterion of color mapping quality and minimize it w.r.t. the parameters of a color transformation to achieve optimal structure preservation. We show numerous applications of our approach, including color-to-gray conversion, gamut mapping, multispectral image fusion, and image optimization for color deficient viewers. version:1
arxiv-1311-0072 | Bayesian inference as iterated random functions with applications to sequential inference in graphical models | http://arxiv.org/abs/1311.0072 | id:1311.0072 author:Arash A. Amini, XuanLong Nguyen category:stat.ML math.ST stat.ME stat.TH  published:2013-11-01 summary:We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as specific instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are illustrated by simulated examples. version:1
arxiv-1311-0035 | Parameterless Optimal Approximate Message Passing | http://arxiv.org/abs/1311.0035 | id:1311.0035 author:Ali Mousavi, Arian Maleki, Richard G. Baraniuk category:cs.IT math.IT math.ST stat.ML stat.TH  published:2013-10-31 summary:Iterative thresholding algorithms are well-suited for high-dimensional problems in sparse recovery and compressive sensing. The performance of this class of algorithms depends heavily on the tuning of certain threshold parameters. In particular, both the final reconstruction error and the convergence rate of the algorithm crucially rely on how the threshold parameter is set at each step of the algorithm. In this paper, we propose a parameter-free approximate message passing (AMP) algorithm that sets the threshold parameter at each iteration in a fully automatic way without either having an information about the signal to be reconstructed or needing any tuning from the user. We show that the proposed method attains both the minimum reconstruction error and the highest convergence rate. Our method is based on applying the Stein unbiased risk estimate (SURE) along with a modified gradient descent to find the optimal threshold in each iteration. Motivated by the connections between AMP and LASSO, it could be employed to find the solution of the LASSO for the optimal regularization parameter. To the best of our knowledge, this is the first work concerning parameter tuning that obtains the fastest convergence rate with theoretical guarantees. version:1
arxiv-1310-8618 | Convergence analysis of kernel LMS algorithm with pre-tuned dictionary | http://arxiv.org/abs/1310.8618 | id:1310.8618 author:Jie Chen, Wei Gao, Cédric Richard, Jose-Carlos M. Bermudez category:stat.ML  published:2013-10-31 summary:The kernel least-mean-square (KLMS) algorithm is an appealing tool for online identification of nonlinear systems due to its simplicity and robustness. In addition to choosing a reproducing kernel and setting filter parameters, designing a KLMS adaptive filter requires to select a so-called dictionary in order to get a finite-order model. This dictionary has a significant impact on performance, and requires careful consideration. Theoretical analysis of KLMS as a function of dictionary setting has rarely, if ever, been addressed in the literature. In an analysis previously published by the authors, the dictionary elements were assumed to be governed by the same probability density function of the input data. In this paper, we modify this study by considering the dictionary as part of the filter parameters to be set. This theoretical analysis paves the way for future investigations on KLMS dictionary design. version:1
arxiv-1310-8612 | Nonlinear unmixing of hyperspectral images using a semiparametric model and spatial regularization | http://arxiv.org/abs/1310.8612 | id:1310.8612 author:Jie Chen, Cédric Richard, Alfred O. Hero III category:stat.ML  published:2013-10-31 summary:Incorporating spatial information into hyperspectral unmixing procedures has been shown to have positive effects, due to the inherent spatial-spectral duality in hyperspectral scenes. Current research works that consider spatial information are mainly focused on the linear mixing model. In this paper, we investigate a variational approach to incorporating spatial correlation into a nonlinear unmixing procedure. A nonlinear algorithm operating in reproducing kernel Hilbert spaces, associated with an $\ell_1$ local variation norm as the spatial regularizer, is derived. Experimental results, with both synthetic and real data, illustrate the effectiveness of the proposed scheme. version:1
arxiv-1310-8574 | Spatial statistics, image analysis and percolation theory | http://arxiv.org/abs/1310.8574 | id:1310.8574 author:Mikhail Langovoy, Michael Habeck, Bernhard Schölkopf category:stat.AP stat.ML  published:2013-10-31 summary:We develop a novel method for detection of signals and reconstruction of images in the presence of random noise. The method uses results from percolation theory. We specifically address the problem of detection of multiple objects of unknown shapes in the case of nonparametric noise. The noise density is unknown and can be heavy-tailed. The objects of interest have unknown varying intensities. No boundary shape constraints are imposed on the objects, only a set of weak bulk conditions is required. We view the object detection problem as a multiple hypothesis testing for discrete statistical inverse problems. We present an algorithm that allows to detect greyscale objects of various shapes in noisy images. We prove results on consistency and algorithmic complexity of our procedures. Applications to cryo-electron microscopy are presented. version:1
arxiv-1303-3265 | A dependent partition-valued process for multitask clustering and time evolving network modelling | http://arxiv.org/abs/1303.3265 | id:1303.3265 author:Konstantina Palla, David A. Knowles, Zoubin Ghahramani category:stat.ML  published:2013-03-13 summary:The fundamental aim of clustering algorithms is to partition data points. We consider tasks where the discovered partition is allowed to vary with some covariate such as space or time. One approach would be to use fragmentation-coagulation processes, but these, being Markov processes, are restricted to linear or tree structured covariate spaces. We define a partition-valued process on an arbitrary covariate space using Gaussian processes. We use the process to construct a multitask clustering model which partitions datapoints in a similar way across multiple data sources, and a time series model of network data which allows cluster assignments to vary over time. We describe sampling algorithms for inference and apply our method to defining cancer subtypes based on different types of cellular characteristics, finding regulatory modules from gene expression data from multiple human populations, and discovering time varying community structure in a social network. version:2
arxiv-1310-8467 | Reinforcement Learning Framework for Opportunistic Routing in WSNs | http://arxiv.org/abs/1310.8467 | id:1310.8467 author:G. Srinivas Rao, A. V. Ramana category:cs.NI cs.LG  published:2013-10-31 summary:Routing packets opportunistically is an essential part of multihop ad hoc wireless sensor networks. The existing routing techniques are not adaptive opportunistic. In this paper we have proposed an adaptive opportunistic routing scheme that routes packets opportunistically in order to ensure that packet loss is avoided. Learning and routing are combined in the framework that explores the optimal routing possibilities. In this paper we implemented this Reinforced learning framework using a customer simulator. The experimental results revealed that the scheme is able to exploit the opportunistic to optimize routing of packets even though the network structure is unknown. version:1
arxiv-1310-3399 | An Improved K-means Clustering Based Approach to Detect a DNA Structure in H&E Image of Mouse Tissue Reacted with CD4-Green Antigen | http://arxiv.org/abs/1310.3399 | id:1310.3399 author:B U V Prashanth, P Narahari Sastry, V Rajesh category:cs.CV  published:2013-10-12 summary:In this manuscript we present the technique to detect and analyze the DNA rich structure in Haemotoxylin & Eosin (H&E) image of a tissue treated with anti CD4 green antigen. The detection of DNA rich structure can be considered as a detection of blue nuclei present through the biomedical signal/image processing technique performed on the image of the tissue obtained by the Scanning Electron Microscope(SEM). Earlier the tissue treated with the anti CD4 green antigen, is stained with the H&E staining solution. version:2
arxiv-1307-0805 | Novel Factorization Strategies for Higher Order Tensors: Implications for Compression and Recovery of Multi-linear Data | http://arxiv.org/abs/1307.0805 | id:1307.0805 author:Zemin Zhang, Gregory Ely, Shuchin Aeron, Ning Hao, Misha Kilmer category:cs.IT cs.CV math.IT  published:2013-07-02 summary:In this paper we propose novel methods for compression and recovery of multilinear data under limited sampling. We exploit the recently proposed tensor- Singular Value Decomposition (t-SVD)[1], which is a group theoretic framework for tensor decomposition. In contrast to popular existing tensor decomposition techniques such as higher-order SVD (HOSVD), t-SVD has optimality properties similar to the truncated SVD for matrices. Based on t-SVD, we first construct novel tensor-rank like measures to characterize informational and structural complexity of multilinear data. Following that we outline a complexity penalized algorithm for tensor completion from missing entries. As an application, 3-D and 4-D (color) video data compression and recovery are considered. We show that videos with linear camera motion can be represented more efficiently using t-SVD compared to traditional approaches based on vectorizing or flattening of the tensors. Application of the proposed tensor completion algorithm for video recovery from missing entries is shown to yield a superior performance over existing methods. In conclusion we point out several research directions and implications to online prediction of multilinear data. version:3
arxiv-1310-8320 | Safe and Efficient Screening For Sparse Support Vector Machine | http://arxiv.org/abs/1310.8320 | id:1310.8320 author:Zheng Zhao, Jun Liu category:cs.LG stat.ML  published:2013-10-30 summary:Screening is an effective technique for speeding up the training process of a sparse learning model by removing the features that are guaranteed to be inactive the process. In this paper, we present a efficient screening technique for sparse support vector machine based on variational inequality. The technique is both efficient and safe. version:1
arxiv-1306-0604 | Distributed k-Means and k-Median Clustering on General Topologies | http://arxiv.org/abs/1306.0604 | id:1306.0604 author:Maria Florina Balcan, Steven Ehrlich, Yingyu Liang category:cs.LG cs.DC stat.ML  published:2013-06-03 summary:This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by \cite{har2004coresets}, we reduce the problem of finding a clustering with low cost to the problem of finding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms. version:3
arxiv-1310-8243 | Para-active learning | http://arxiv.org/abs/1310.8243 | id:1310.8243 author:Alekh Agarwal, Leon Bottou, Miroslav Dudik, John Langford category:cs.LG stat.ML  published:2013-10-30 summary:Training examples are not all equally informative. Active learning strategies leverage this observation in order to massively reduce the number of examples that need to be labeled. We leverage the same observation to build a generic strategy for parallelizing learning algorithms. This strategy is effective because the search for informative examples is highly parallelizable and because we show that its performance does not deteriorate when the sifting process relies on a slightly outdated model. Parallel active learning is particularly attractive to train nonlinear models with non-linear representations because there are few practical parallel learning algorithms for such models. We report preliminary experiments using both kernel SVMs and SGD-trained neural networks. version:1
arxiv-1308-6324 | Prediction of breast cancer recurrence using Classification Restricted Boltzmann Machine with Dropping | http://arxiv.org/abs/1308.6324 | id:1308.6324 author:Jakub M. Tomczak category:cs.LG  published:2013-08-28 summary:In this paper, we apply Classification Restricted Boltzmann Machine (ClassRBM) to the problem of predicting breast cancer recurrence. According to the Polish National Cancer Registry, in 2010 only, the breast cancer caused almost 25% of all diagnosed cases of cancer in Poland. We propose how to use ClassRBM for predicting breast cancer return and discovering relevant inputs (symptoms) in illness reappearance. Next, we outline a general probabilistic framework for learning Boltzmann machines with masks, which we refer to as Dropping. The fashion of generating masks leads to different learning methods, i.e., DropOut, DropConnect. We propose a new method called DropPart which is a generalization of DropConnect. In DropPart the Beta distribution instead of Bernoulli distribution in DropConnect is used. At the end, we carry out an experiment using real-life dataset consisting of 949 cases, provided by the Institute of Oncology Ljubljana. version:2
arxiv-1209-0654 | Compressive Optical Deflectometric Tomography: A Constrained Total-Variation Minimization Approach | http://arxiv.org/abs/1209.0654 | id:1209.0654 author:Adriana Gonzalez, Laurent Jacques, Christophe De Vleeschouwer, Philippe Antoine category:cs.CV math.OC  published:2012-09-04 summary:Optical Deflectometric Tomography (ODT) provides an accurate characterization of transparent materials whose complex surfaces present a real challenge for manufacture and control. In ODT, the refractive index map (RIM) of a transparent object is reconstructed by measuring light deflection under multiple orientations. We show that this imaging modality can be made "compressive", i.e., a correct RIM reconstruction is achievable with far less observations than required by traditional Filtered Back Projection (FBP) methods. Assuming a cartoon-shape RIM model, this reconstruction is driven by minimizing the map Total-Variation under a fidelity constraint with the available observations. Moreover, two other realistic assumptions are added to improve the stability of our approach: the map positivity and a frontier condition. Numerically, our method relies on an accurate ODT sensing model and on a primal-dual minimization scheme, including easily the sensing operator and the proposed RIM constraints. We conclude this paper by demonstrating the power of our method on synthetic and experimental data under various compressive scenarios. In particular, the compressiveness of the stabilized ODT problem is demonstrated by observing a typical gain of 20 dB compared to FBP at only 5% of 360 incident light angles for moderately noisy sensing. version:2
arxiv-1311-0262 | Tracking Deformable Parts via Dynamic Conditional Random Fields | http://arxiv.org/abs/1311.0262 | id:1311.0262 author:Suofei Zhang, Zhixin Sun, Xu Cheng, Zhenyang Wu category:cs.CV cs.MM  published:2013-10-30 summary:Despite the success of many advanced tracking methods in this area, tracking targets with drastic variation of appearance such as deformation, view change and partial occlusion in video sequences is still a challenge in practical applications. In this letter, we take these serious tracking problems into account simultaneously, proposing a dynamic graph based model to track object and its deformable parts at multiple resolutions. The method introduces well learned structural object detection models into object tracking applications as prior knowledge to deal with deformation and view change. Meanwhile, it explicitly formulates partial occlusion by integrating spatial potentials and temporal potentials with an unparameterized occlusion handling mechanism in the dynamic conditional random field framework. Empirical results demonstrate that the method outperforms state-of-the-art trackers on different challenging video sequences. version:1
arxiv-1310-8059 | Description and Evaluation of Semantic Similarity Measures Approaches | http://arxiv.org/abs/1310.8059 | id:1310.8059 author:Thabet Slimani category:cs.CL  published:2013-10-30 summary:In recent years, semantic similarity measure has a great interest in Semantic Web and Natural Language Processing (NLP). Several similarity measures have been developed, being given the existence of a structured knowledge representation offered by ontologies and corpus which enable semantic interpretation of terms. Semantic similarity measures compute the similarity between concepts/terms included in knowledge sources in order to perform estimations. This paper discusses the existing semantic similarity methods based on structure, information content and feature approaches. Additionally, we present a critical evaluation of several categories of semantic similarity approaches based on two standard benchmarks. The aim of this paper is to give an efficient evaluation of all these measures which help researcher and practitioners to select the measure that best fit for their requirements. version:1
arxiv-1308-0900 | Trading USDCHF filtered by Gold dynamics via HMM coupling | http://arxiv.org/abs/1308.0900 | id:1308.0900 author:Donny Lee category:stat.ML cs.LG 91G99  60J22  published:2013-08-05 summary:We devise a USDCHF trading strategy using the dynamics of gold as a filter. Our strategy involves modelling both USDCHF and gold using a coupled hidden Markov model (CHMM). The observations will be indicators, RSI and CCI, which will be used as triggers for our trading signals. Upon decoding the model in each iteration, we can get the next most probable state and the next most probable observation. Hopefully by taking advantage of intermarket analysis and the Markov property implicit in the model, trading with these most probable values will produce profitable results. version:2
arxiv-1310-8004 | Online Ensemble Learning for Imbalanced Data Streams | http://arxiv.org/abs/1310.8004 | id:1310.8004 author:Boyu Wang, Joelle Pineau category:cs.LG stat.ML  published:2013-10-30 summary:While both cost-sensitive learning and online learning have been studied extensively, the effort in simultaneously dealing with these two issues is limited. Aiming at this challenge task, a novel learning framework is proposed in this paper. The key idea is based on the fusion of online ensemble algorithms and the state of the art batch mode cost-sensitive bagging/boosting algorithms. Within this framework, two separately developed research areas are bridged together, and a batch of theoretically sound online cost-sensitive bagging and online cost-sensitive boosting algorithms are first proposed. Unlike other online cost-sensitive learning algorithms lacking theoretical analysis of asymptotic properties, the convergence of the proposed algorithms is guaranteed under certain conditions, and the experimental evidence with benchmark data sets also validates the effectiveness and efficiency of the proposed methods. version:1
arxiv-1310-7994 | Necessary and Sufficient Conditions for Novel Word Detection in Separable Topic Models | http://arxiv.org/abs/1310.7994 | id:1310.7994 author:Weicong Ding, Prakash Ishwar, Mohammad H. Rohban, Venkatesh Saligrama category:cs.LG cs.IR stat.ML  published:2013-10-30 summary:The simplicial condition and other stronger conditions that imply it have recently played a central role in developing polynomial time algorithms with provable asymptotic consistency and sample complexity guarantees for topic estimation in separable topic models. Of these algorithms, those that rely solely on the simplicial condition are impractical while the practical ones need stronger conditions. In this paper, we demonstrate, for the first time, that the simplicial condition is a fundamental, algorithm-independent, information-theoretic necessary condition for consistent separable topic estimation. Furthermore, under solely the simplicial condition, we present a practical quadratic-complexity algorithm based on random projections which consistently detects all novel words of all topics using only up to second-order empirical word moments. This algorithm is amenable to distributed implementation making it attractive for 'big-data' scenarios involving a network of large distributed databases. version:1
arxiv-1310-7868 | Automatic Classification of Variable Stars in Catalogs with missing data | http://arxiv.org/abs/1310.7868 | id:1310.7868 author:Karim Pichara, Pavlos Protopapas category:astro-ph.IM cs.LG stat.ML  published:2013-10-29 summary:We present an automatic classification method for astronomical catalogs with missing data. We use Bayesian networks, a probabilistic graphical model, that allows us to perform inference to pre- dict missing values given observed data and dependency relationships between variables. To learn a Bayesian network from incomplete data, we use an iterative algorithm that utilises sampling methods and expectation maximization to estimate the distributions and probabilistic dependencies of variables from data with missing values. To test our model we use three catalogs with missing data (SAGE, 2MASS and UBVI) and one complete catalog (MACHO). We examine how classification accuracy changes when information from missing data catalogs is included, how our method compares to traditional missing data approaches and at what computational cost. Integrating these catalogs with missing data we find that classification of variable objects improves by few percent and by 15% for quasar detection while keeping the computational cost the same. version:1
arxiv-1310-7855 | A comparison of bandwidth selectors for mean shift clustering | http://arxiv.org/abs/1310.7855 | id:1310.7855 author:José E. Chacón, Pablo Monfort category:stat.ML  published:2013-10-29 summary:We explore the performance of several automatic bandwidth selectors, originally designed for density gradient estimation, as data-based procedures for nonparametric, modal clustering. The key tool to obtain a clustering from density gradient estimators is the mean shift algorithm, which allows to obtain a partition not only of the data sample, but also of the whole space. The results of our simulation study suggest that most of the methods considered here, like cross validation and plug in bandwidth selectors, are useful for cluster analysis via the mean shift algorithm. version:1
arxiv-1310-7795 | An Unsupervised Feature Learning Approach to Improve Automatic Incident Detection | http://arxiv.org/abs/1310.7795 | id:1310.7795 author:Jimmy SJ. Ren, Wei Wang, Jiawei Wang, Stephen Liao category:cs.LG  published:2013-10-29 summary:Sophisticated automatic incident detection (AID) technology plays a key role in contemporary transportation systems. Though many papers were devoted to study incident classification algorithms, few study investigated how to enhance feature representation of incidents to improve AID performance. In this paper, we propose to use an unsupervised feature learning algorithm to generate higher level features to represent incidents. We used real incident data in the experiments and found that effective feature mapping function can be learnt from the data crosses the test sites. With the enhanced features, detection rate (DR), false alarm rate (FAR) and mean time to detect (MTTD) are significantly improved in all of the three representative cases. This approach also provides an alternative way to reduce the amount of labeled data, which is expensive to obtain, required in training better incident classifiers since the feature learning is unsupervised. version:1
arxiv-1310-7679 | Structured Optimal Transmission Control in Network-coded Two-way Relay Channels | http://arxiv.org/abs/1310.7679 | id:1310.7679 author:Ni Ding, Parastoo Sadeghi, Rodney A. Kennedy category:cs.SY stat.ML  published:2013-10-29 summary:This paper considers a transmission control problem in network-coded two-way relay channels (NC-TWRC), where the relay buffers random symbol arrivals from two users, and the channels are assumed to be fading. The problem is modeled by a discounted infinite horizon Markov decision process (MDP). The objective is to find a transmission control policy that minimizes the symbol delay, buffer overflow and transmission power consumption and error rate simultaneously and in the long run. By using the concepts of submodularity, multimodularity and L-natural convexity, we study the structure of the optimal policy searched by dynamic programming (DP) algorithm. We show that the optimal transmission policy is nondecreasing in queue occupancies or/and channel states under certain conditions such as the chosen values of parameters in the MDP model, channel modeling method, modulation scheme and the preservation of stochastic dominance in the transitions of system states. The results derived in this paper can be used to relieve the high complexity of DP and facilitate real-time control. version:1
arxiv-1306-2967 | Optimization of Clustering for Clustering-based Image Denoising | http://arxiv.org/abs/1306.2967 | id:1306.2967 author:Mohsen Joneidi, Mostafa Sadeghi category:cs.CV  published:2013-06-12 summary:In this paper, the problem of de-noising of an image contaminated with additive white Gaussian noise (AWGN) is studied. This subject has been continued to be an open problem in signal processing for more than 50 years. In the present paper, we suggest a method based on global clustering of image constructing blocks. Noting that the type of clustering plays an important role in clustering-based de-noising methods, we address two questions about the clustering. First, which parts of data should be considered for clustering? Second, what data clustering method is suitable for de-noising? Clustering is exploited to learn an over complete dictionary. By obtaining sparse decomposition of the noisy image blocks in terms of the dictionary atoms, the de-noised version is achieved. Experimental results show that our dictionary learning framework outperforms traditional dictionary learning methods such as K-SVD. version:3
arxiv-1309-0719 | Understanding Evolutionary Potential in Virtual CPU Instruction Set Architectures | http://arxiv.org/abs/1309.0719 | id:1309.0719 author:David M. Bryson, Charles Ofria category:cs.NE  published:2013-09-03 summary:We investigate fundamental decisions in the design of instruction set architectures for linear genetic programs that are used as both model systems in evolutionary biology and underlying solution representations in evolutionary computation. We subjected digital organisms with each tested architecture to seven different computational environments designed to present a range of evolutionary challenges. Our goal was to engineer a general purpose architecture that would be effective under a broad range of evolutionary conditions. We evaluated six different types of architectural features for the virtual CPUs: (1) genetic flexibility: we allowed digital organisms to more precisely modify the function of genetic instructions, (2) memory: we provided an increased number of registers in the virtual CPUs, (3) decoupled sensors and actuators: we separated input and output operations to enable greater control over data flow. We also tested a variety of methods to regulate expression: (4) explicit labels that allow programs to dynamically refer to specific genome positions, (5) position-relative search instructions, and (6) multiple new flow control instructions, including conditionals and jumps. Each of these features also adds complication to the instruction set and risks slowing evolution due to epistatic interactions. Two features (multiple argument specification and separated I/O) demonstrated substantial improvements int the majority of test environments. Some of the remaining tested modifications were detrimental, thought most exhibit no systematic effects on evolutionary potential, highlighting the robustness of digital evolution. Combined, these observations enhance our understanding of how instruction architecture impacts evolutionary potential, enabling the creation of architectures that support more rapid evolution of complex solutions to a broad range of challenges. version:2
arxiv-1107-0789 | Distributed Matrix Completion and Robust Factorization | http://arxiv.org/abs/1107.0789 | id:1107.0789 author:Lester Mackey, Ameet Talwalkar, Michael I. Jordan category:cs.LG cs.DS cs.NA math.NA stat.ML  published:2011-07-05 summary:If learning methods are to scale to the massive sizes of modern datasets, it is essential for the field of machine learning to embrace parallel and distributed computing. Inspired by the recent development of matrix factorization methods with rich theory but poor computational complexity and by the relative ease of mapping matrices onto distributed architectures, we introduce a scalable divide-and-conquer framework for noisy matrix factorization. We present a thorough theoretical analysis of this framework in which we characterize the statistical errors introduced by the "divide" step and control their magnitude in the "conquer" step, so that the overall algorithm enjoys high-probability estimation guarantees comparable to those of its base algorithm. We also present experiments in collaborative filtering and video background modeling that demonstrate the near-linear to superlinear speed-ups attainable with this approach. version:7
arxiv-1310-7217 | Compressed Sensing SAR Imaging with Multilook Processing | http://arxiv.org/abs/1310.7217 | id:1310.7217 author:Jian Fang, Zongben Xu, Bingchen Zhang, Wen Hong, Yirong Wu category:cs.IT cs.CV math.IT  published:2013-10-27 summary:Multilook processing is a widely used speckle reduction approach in synthetic aperture radar (SAR) imaging. Conventionally, it is achieved by incoherently summing of some independent low-resolution images formulated from overlapping subbands of the SAR signal. However, in the context of compressive sensing (CS) SAR imaging, where the samples are collected at sub-Nyquist rate, the data spectrum is highly aliased that hinders the direct application of the existing multilook techniques. In this letter, we propose a new CS-SAR imaging method that can realize multilook processing simultaneously during image reconstruction. The main idea is to replace the SAR observation matrix by the inverse of multilook procedures, which is then combined with random sampling matrix to yield a multilook CS-SAR observation model. Then a joint sparse regularization model, considering pixel dependency of subimages, is derived to form multilook images. The suggested SAR imaging method can not only reconstruct sparse scene efficiently below Nyquist rate, but is also able to achieve a comparable reduction of speckles during reconstruction. Simulation results are finally provided to demonstrate the effectiveness of the proposed method. version:1
arxiv-1310-7163 | Generalized Thompson Sampling for Contextual Bandits | http://arxiv.org/abs/1310.7163 | id:1310.7163 author:Lihong Li category:cs.LG cs.AI stat.ML stat.OT 62L05 I.2.6  published:2013-10-27 summary:Thompson Sampling, one of the oldest heuristics for solving multi-armed bandits, has recently been shown to demonstrate state-of-the-art performance. The empirical success has led to great interests in theoretical understanding of this heuristic. In this paper, we approach this problem in a way very different from existing efforts. In particular, motivated by the connection between Thompson Sampling and exponentiated updates, we propose a new family of algorithms called Generalized Thompson Sampling in the expert-learning framework, which includes Thompson Sampling as a special case. Similar to most expert-learning algorithms, Generalized Thompson Sampling uses a loss function to adjust the experts' weights. General regret bounds are derived, which are also instantiated to two important loss functions: square loss and logarithmic loss. In contrast to existing bounds, our results apply to quite general contextual bandits. More importantly, they quantify the effect of the "prior" distribution on the regret bounds. version:1
arxiv-1310-7115 | Studying a Chaotic Spiking Neural Model | http://arxiv.org/abs/1310.7115 | id:1310.7115 author:Mohammad Alhawarat, Waleed Nazih, Mohammad Eldesouki category:cs.AI cs.NE  published:2013-10-26 summary:Dynamics of a chaotic spiking neuron model are being studied mathematically and experimentally. The Nonlinear Dynamic State neuron (NDS) is analysed to further understand the model and improve it. Chaos has many interesting properties such as sensitivity to initial conditions, space filling, control and synchronization. As suggested by biologists, these properties may be exploited and play vital role in carrying out computational tasks in human brain. The NDS model has some limitations; in thus paper the model is investigated to overcome some of these limitations in order to enhance the model. Therefore, the models parameters are tuned and the resulted dynamics are studied. Also, the discretization method of the model is considered. Moreover, a mathematical analysis is carried out to reveal the underlying dynamics of the model after tuning of its parameters. The results of the aforementioned methods revealed some facts regarding the NDS attractor and suggest the stabilization of a large number of unstable periodic orbits (UPOs) which might correspond to memories in phase space. version:1
arxiv-1310-7114 | Efficient Information Theoretic Clustering on Discrete Lattices | http://arxiv.org/abs/1310.7114 | id:1310.7114 author:Christian Bauckhage, Kristian Kersting category:cs.CV  published:2013-10-26 summary:We consider the problem of clustering data that reside on discrete, low dimensional lattices. Canonical examples for this setting are found in image segmentation and key point extraction. Our solution is based on a recent approach to information theoretic clustering where clusters result from an iterative procedure that minimizes a divergence measure. We replace costly processing steps in the original algorithm by means of convolutions. These allow for highly efficient implementations and thus significantly reduce runtime. This paper therefore bridges a gap between machine learning and signal processing. version:1
arxiv-1211-0906 | Algorithm Runtime Prediction: Methods & Evaluation | http://arxiv.org/abs/1211.0906 | id:1211.0906 author:Frank Hutter, Lin Xu, Holger H. Hoos, Kevin Leyton-Brown category:cs.AI cs.LG cs.PF stat.ML 68T20 I.2.8; I.2.6  published:2012-11-05 summary:Perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm's runtime as a function of problem-specific instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic configuration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and -- perhaps most importantly -- a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisfiability (SAT), travelling salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of SAT, MIP, and TSP instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously. version:2
arxiv-1310-7048 | Scaling SVM and Least Absolute Deviations via Exact Data Reduction | http://arxiv.org/abs/1310.7048 | id:1310.7048 author:Jie Wang, Peter Wonka, Jieping Ye category:cs.LG stat.ML  published:2013-10-25 summary:The support vector machine (SVM) is a widely used method for classification. Although many efforts have been devoted to develop efficient solvers, it remains challenging to apply SVM to large-scale problems. A nice property of SVM is that the non-support vectors have no effect on the resulting classifier. Motivated by this observation, we present fast and efficient screening rules to discard non-support vectors by analyzing the dual problem of SVM via variational inequalities (DVI). As a result, the number of data instances to be entered into the optimization can be substantially reduced. Some appealing features of our screening method are: (1) DVI is safe in the sense that the vectors discarded by DVI are guaranteed to be non-support vectors; (2) the data set needs to be scanned only once to run the screening, whose computational cost is negligible compared to that of solving the SVM problem; (3) DVI is independent of the solvers and can be integrated with any existing efficient solvers. We also show that the DVI technique can be extended to detect non-support vectors in the least absolute deviations regression (LAD). To the best of our knowledge, there are currently no screening methods for LAD. We have evaluated DVI on both synthetic and real data sets. Experiments indicate that DVI significantly outperforms the existing state-of-the-art screening rules for SVM, and is very effective in discarding non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude. version:1
arxiv-1310-5426 | MLI: An API for Distributed Machine Learning | http://arxiv.org/abs/1310.5426 | id:1310.5426 author:Evan R. Sparks, Ameet Talwalkar, Virginia Smith, Jey Kottalam, Xinghao Pan, Joseph Gonzalez, Michael J. Franklin, Michael I. Jordan, Tim Kraska category:cs.LG cs.DC stat.ML  published:2013-10-21 summary:MLI is an Application Programming Interface designed to address the challenges of building Machine Learn- ing algorithms in a distributed setting based on data-centric computing. Its primary goal is to simplify the development of high-performance, scalable, distributed algorithms. Our initial results show that, relative to existing systems, this interface can be used to build distributed implementations of a wide variety of common Machine Learning algorithms with minimal complexity and highly competitive performance and scalability. version:2
arxiv-1310-7033 | A feasible roadmap for unsupervised deconvolution of two-source mixed gene expressions | http://arxiv.org/abs/1310.7033 | id:1310.7033 author:Niya Wang, Eric P. Hoffman, Robert Clarke, Zhen Zhang, David M. Herrington, Ie-Ming Shih, Douglas A. Levine, Guoqiang Yu, Jianhua Xuan, Yue Wang category:stat.ML q-bio.GN q-bio.QM stat.AP  published:2013-10-25 summary:Tissue heterogeneity is a major confounding factor in studying individual populations that cannot be resolved directly by global profiling. Experimental solutions to mitigate tissue heterogeneity are expensive, time consuming, inapplicable to existing data, and may alter the original gene expression patterns. Here we ask whether it is possible to deconvolute two-source mixed expressions (estimating both proportions and cell-specific profiles) from two or more heterogeneous samples without requiring any prior knowledge. Supported by a well-grounded mathematical framework, we argue that both constituent proportions and cell-specific expressions can be estimated in a completely unsupervised mode when cell-specific marker genes exist, which do not have to be known a priori, for each of constituent cell types. We demonstrate the performance of unsupervised deconvolution on both simulation and real gene expression data, together with perspective discussions. version:1
arxiv-1310-6998 | Predicting the NFL using Twitter | http://arxiv.org/abs/1310.6998 | id:1310.6998 author:Shiladitya Sinha, Chris Dyer, Kevin Gimpel, Noah A. Smith category:cs.SI cs.LG physics.soc-ph stat.ML  published:2013-10-25 summary:We study the relationship between social media output and National Football League (NFL) games, using a dataset containing messages from Twitter and NFL game statistics. Specifically, we consider tweets pertaining to specific teams and games in the NFL season and use them alongside statistical game data to build predictive models for future game outcomes (which team will win?) and sports betting outcomes (which team will win with the point spread? will the total points be over/under the line?). We experiment with several feature sets and find that simple features using large volumes of tweets can match or exceed the performance of more traditional features that use game statistics. version:1
arxiv-1301-2724 | Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models | http://arxiv.org/abs/1301.2724 | id:1301.2724 author:Manfred Opper, Ulrich Paquet, Ole Winther category:stat.ML  published:2013-01-12 summary:Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian field, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model's partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP's local matching of moments. Through the expansion, we see that EP is correct to first order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. version:2
arxiv-1307-6417 | Boosting the concordance index for survival data - a unified framework to derive and evaluate biomarker combinations | http://arxiv.org/abs/1307.6417 | id:1307.6417 author:Andreas Mayr, Matthias Schmid category:stat.AP stat.ME stat.ML  published:2013-07-24 summary:The development of molecular signatures for the prediction of time-to-event outcomes is a methodologically challenging task in bioinformatics and biostatistics. Although there are numerous approaches for the derivation of marker combinations and their evaluation, the underlying methodology often suffers from the problem that different optimization criteria are mixed during the feature selection, estimation and evaluation steps. This might result in marker combinations that are only suboptimal regarding the evaluation criterion of interest. To address this issue, we propose a unified framework to derive and evaluate biomarker combinations. Our approach is based on the concordance index for time-to-event data, which is a non-parametric measure to quantify the discrimatory power of a prediction rule. Specifically, we propose a component-wise boosting algorithm that results in linear biomarker combinations that are optimal with respect to a smoothed version of the concordance index. We investigate the performance of our algorithm in a large-scale simulation study and in two molecular data sets for the prediction of survival in breast cancer patients. Our numerical results show that the new approach is not only methodologically sound but can also lead to a higher discriminatory power than traditional approaches for the derivation of gene signatures. version:2
