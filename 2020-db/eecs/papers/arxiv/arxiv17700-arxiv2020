arxiv-1605-06778 | openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit | http://arxiv.org/abs/1605.06778 | id:1605.06778 author:Maximilian Schmitt, Björn W. Schuller category:cs.CV cs.CL cs.IR  published:2016-05-22 summary:We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e.g., acoustic or visual low-level descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed subbags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool are exemplified in two sample scenarios: time-continuous speech-based emotion recognition and sentiment analysis in tweets where improved results over other feature representation forms were observed. version:1
arxiv-1605-06776 | Sparse Signal Reconstruction with Multiple Side Information using Adaptive Weights for Multiview Sources | http://arxiv.org/abs/1605.06776 | id:1605.06776 author:Huynh Van Luong, Jürgen Seiler, André Kaup, Søren Forchhammer category:cs.CV math.OC  published:2016-05-22 summary:This work considers reconstructing a target signal in a context of distributed sparse sources. We propose an efficient reconstruction algorithm with the aid of other given sources as multiple side information (SI). The proposed algorithm takes advantage of compressive sensing (CS) with SI and adaptive weights by solving a proposed weighted $n$-$\ell_{1}$ minimization. The proposed algorithm computes the adaptive weights in two levels, first each individual intra-SI and then inter-SI weights are iteratively updated at every reconstructed iteration. This two-level optimization leads the proposed reconstruction algorithm with multiple SI using adaptive weights (RAMSIA) to robustly exploit the multiple SIs with different qualities. We experimentally perform our algorithm on generated sparse signals and also correlated feature histograms as multiview sparse sources from a multiview image database. The results show that RAMSIA significantly outperforms both classical CS and CS with single SI, and RAMSIA with higher number of SIs gained more than the one with smaller number of SIs. version:1
arxiv-1605-06770 | Automatic Construction of Discourse Corpora for Dialogue Translation | http://arxiv.org/abs/1605.06770 | id:1605.06770 author:Longyue Wang, Xiaojun Zhang, Zhaopeng Tu, Andy Way, Qun Liu category:cs.CL  published:2016-05-22 summary:In this paper, a novel approach is proposed to automatically construct parallel discourse corpus for dialogue machine translation. Firstly, the parallel subtitle data and its corresponding monolingual movie script data are crawled and collected from Internet. Then tags such as speaker and discourse boundary from the script data are projected to its subtitle data via an information retrieval approach in order to map monolingual discourse to bilingual texts. We not only evaluate the mapping results, but also integrate speaker information into the translation. Experiments show our proposed method can achieve 81.79% and 98.64% accuracy on speaker and dialogue boundary annotation, and speaker-based language model adaptation can obtain around 0.5 BLEU points improvement in translation qualities. Finally, we publicly release around 100K parallel discourse data with manual speaker and dialogue boundary annotation. version:1
arxiv-1605-06764 | 3D Face Tracking and Texture Fusion in the Wild | http://arxiv.org/abs/1605.06764 | id:1605.06764 author:Patrik Huber, Philipp Kopp, Matthias Rätsch, William Christmas, Josef Kittler category:cs.CV 68T45  published:2016-05-22 summary:We present a fully automatic approach to real-time 3D face reconstruction from monocular in-the-wild videos. With the use of a cascaded-regressor based face tracking and a 3D Morphable Face Model shape fitting, we obtain a semi-dense 3D face shape. We further use the texture information from multiple frames to build a holistic 3D face representation from the video frames. Our system is able to capture facial expressions and does not require any person-specific training. We demonstrate the robustness of our approach on the challenging 300 Videos in the Wild (300-VW) dataset. Our real-time fitting framework is available as an open source library at http://4dface.org. version:1
arxiv-1603-03678 | Nonstationary Distance Metric Learning | http://arxiv.org/abs/1603.03678 | id:1603.03678 author:Kristjan Greenewald, Stephen Kelley, Alfred Hero category:stat.ML cs.LG  published:2016-03-11 summary:Recent work in distance metric learning has focused on learning transformations of data that best align with provided sets of pairwise similarity and dissimilarity constraints. The learned transformations lead to improved retrieval, classification, and clustering algorithms due to the better adapted distance or similarity measures. Here, we introduce the problem of learning these transformations when the underlying constraint generation process is nonstationary. This nonstationarity can be due to changes in either the ground-truth clustering used to generate constraints or changes to the feature subspaces in which the class structure is apparent. We propose and evaluate COMID-SADL, an adaptive, online approach for learning and tracking optimal metrics as they change over time that is highly robust to a variety of nonstationary behaviors in the changing metric. We demonstrate COMID-SADL on both real and synthetic data sets and show significant performance improvements relative to previously proposed batch and online distance metric learning algorithms. version:2
arxiv-1605-06743 | Inductive Bias of Deep Convolutional Networks through Pooling Geometry | http://arxiv.org/abs/1605.06743 | id:1605.06743 author:Nadav Cohen, Amnon Shashua category:cs.NE cs.LG  published:2016-05-22 summary:Our formal understanding of the inductive bias that drives the success of convolutional networks on computer vision tasks is limited. In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images. In this paper we study the ability of convolutional arithmetic circuits to model correlations among regions of their input. Correlations are formalized through the notion of separation rank, which for a given input partition, measures how far a function is from being separable. We show that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial separation ranks for others. The network's pooling geometry effectively determines which input partitions are favored, thus serves as a means for controlling the inductive bias. Contiguous pooling windows as commonly employed in practice favor interleaved partitions over coarse ones, orienting the inductive bias towards the statistics of natural images. In addition to analyzing deep networks, we show that shallow ones support only linear separation ranks, and by this gain insight into the benefit of functions brought forth by depth - they are able to efficiently model strong correlation under favored partitions of the input. version:1
arxiv-1605-06742 | A Rapid Pattern-Recognition Method for Driving Types Using Clustering-Based Support Vector Machines | http://arxiv.org/abs/1605.06742 | id:1605.06742 author:Wenshuo Wang, Junqiang Xi category:stat.ML cs.CV cs.LG  published:2016-05-22 summary:A rapid pattern-recognition approach to characterize driver's curve-negotiating behavior is proposed. To shorten the recognition time and improve the recognition of driving styles, a k-means clustering-based support vector machine ( kMC-SVM) method is developed and used for classifying drivers into two types: aggressive and moderate. First, vehicle speed and throttle opening are treated as the feature parameters to reflect the driving styles. Second, to discriminate driver curve-negotiating behaviors and reduce the number of support vectors, the k-means clustering method is used to extract and gather the two types of driving data and shorten the recognition time. Then, based on the clustering results, a support vector machine approach is utilized to generate the hyperplane for judging and predicting to which types the human driver are subject. Lastly, to verify the validity of the kMC-SVM method, a cross-validation experiment is designed and conducted. The research results show that the $ k $MC-SVM is an effective method to classify driving styles with a short time, compared with SVM method. version:1
arxiv-1605-06722 | Hybrid evolutionary algorithm with extreme machine learning fitness function evaluation for two-stage capacitated facility location problem | http://arxiv.org/abs/1605.06722 | id:1605.06722 author:Peng Guo, Wenming Cheng, Yi Wang category:math.OC cs.NE  published:2016-05-22 summary:This paper considers the two-stage capacitated facility location problem (TSCFLP) in which products manufactured in plants are delivered to customers via storage depots. Customer demands are satisfied subject to limited plant production and limited depot storage capacity. The objective is to determine the locations of plants and depots in order to minimize the total cost including the fixed cost and transportation cost. A hybrid evolutionary algorithm (HEA) with genetic operations and local search is proposed. To avoid the expensive calculation of fitness of population in terms of computational time, the HEA uses extreme machine learning to approximate the fitness of most of the individuals. Moreover, two heuristics based on the characteristic of the problem is incorporated to generate a good initial population. Computational experiments are performed on two sets of test instances from the recent literature. The performance of the proposed algorithm is evaluated and analyzed. Compared with the state-of-the-art genetic algorithm, the proposed algorithm can find the optimal or near-optimal solutions in a reasonable computational time. version:1
arxiv-1306-5993 | Stochastic Modeling and Estimation of Stationary Complex-Valued Signals | http://arxiv.org/abs/1306.5993 | id:1306.5993 author:Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly, Jeffrey J. Early category:stat.ME stat.AP stat.CO stat.ML  published:2013-06-25 summary:This paper provides a stochastic modeling framework for the power spectral representations of stationary complex-valued signals. We specify how complex-valued signals can be modeled stochastically in terms of their rotary components, which decompose a bivariate signal according to direction of rotation. The necessary relationships are provided to map between complex-rotary and bivariate-Cartesian representations. We demonstrate how by modeling in rotary components we can infer useful features from application datasets---in particular for capturing the improper or anisotropic structure of a signal---by implementing our methodology on fluid dynamic simulations of turbulence. In addition, we detail how parameters of a chosen stochastic model can be efficiently estimated in the frequency domain, by extending the Whittle likelihood to complex-valued signals. We also provide a new method of testing for complex structure such as impropriety, as well as procedures for model choice and semi-parametric modeling. version:3
arxiv-1605-06718 | The De-Biased Whittle Likelihood for Second-Order Stationary Stochastic Processes | http://arxiv.org/abs/1605.06718 | id:1605.06718 author:Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly category:stat.ME math.ST stat.CO stat.ML stat.TH  published:2016-05-22 summary:The Whittle likelihood is a computationally efficient pseudo-maximum likelihood inference procedure which is known to produce biased parameter estimates for large classes of time series models. We propose a method for de-biasing Whittle likelihood parameter estimates for second-order stationary stochastic processes. We demonstrate how to compute the de-biased Whittle likelihood in the same $\mathcal{O}(n\log n)$ computational efficiency as standard Whittle likelihood. We prove that the method is consistent, and demonstrate its superior performance in simulation studies. We also demonstrate how the method can be easily combined with standard methods of bias reduction, such as tapering and differencing, to further reduce bias in parameter estimates. version:1
arxiv-1605-06715 | Factored Temporal Sigmoid Belief Networks for Sequence Learning | http://arxiv.org/abs/1605.06715 | id:1605.06715 author:Jiaming Song, Zhe Gan, Lawrence Carin category:stat.ML cs.LG  published:2016-05-22 summary:Deep conditional generative models are developed to simultaneously learn the temporal dependencies of multiple sequences. The model is designed by introducing a three-way weight tensor to capture the multiplicative interactions between side information and sequences. The proposed model builds on the Temporal Sigmoid Belief Network (TSBN), a sequential stack of Sigmoid Belief Networks (SBNs). The transition matrices are further factored to reduce the number of parameters and improve generalization. When side information is not available, a general framework for semi-supervised learning based on the proposed model is constituted, allowing robust sequence classification. Experimental results show that the proposed approach achieves state-of-the-art predictive and classification performance on sequential data, and has the capacity to synthesize sequences, with controlled style transitioning and blending. version:1
arxiv-1605-06714 | Evolutionary Demographic Algorithms | http://arxiv.org/abs/1605.06714 | id:1605.06714 author:Marco AR Erra, Pedro MM Mitra, Agostinho C Rosa category:cs.NE  published:2016-05-22 summary:Most of the problems in genetic algorithms are very complex and demand a large amount of resources that current technology can not offer. Our purpose was to develop a Java-JINI distributed library that implements Genetic Algorithms with sub-populations (coarse grain) and a graphical interface in order to configure and follow the evolution of the search. The sub-populations are simulated/evaluated in personal computers connected trough a network, keeping in mind different models of sub-populations, migration policies and network topologies. We show that this model delays the convergence of the population keeping a higher level of genetic diversity and allows a much greater number of evaluations since they are distributed among several computers compared with the traditional Genetic Algorithms. version:1
arxiv-1604-07928 | Distributed Flexible Nonlinear Tensor Factorization | http://arxiv.org/abs/1604.07928 | id:1604.07928 author:Shandian Zhe, Kai Zhang, Pengyuan Wang, Kuang-chih Lee, Zenglin Xu, Yuan Qi, Zoubin Ghahramani category:cs.LG cs.AI cs.DC stat.ML I.5.1; I.5.4  published:2016-04-27 summary:Tensor factorization is a powerful tool to analyse multi-way data. Compared with traditional multi-linear methods, nonlinear tensor factorization models are capable of capturing more complex relationships in the data. However, they are computationally expensive and may suffer severe learning bias in case of extreme data sparsity. To overcome these limitations, in this paper we propose a distributed, flexible nonlinear tensor factorization model. Our model can effectively avoid the expensive computations and structural restrictions of the Kronecker-product in existing TGP formulations, allowing an arbitrary subset of tensorial entries to be selected to contribute to the training. At the same time, we derive a tractable and tight variational evidence lower bound (ELBO) that enables highly decoupled, parallel computations and high-quality inference. Based on the new bound, we develop a distributed inference algorithm in the MapReduce framework, which is key-value-free and can fully exploit the memory cache mechanism in fast MapReduce systems such as SPARK. Experimental results fully demonstrate the advantages of our method over several state-of-the-art approaches, in terms of both predictive performance and computational efficiency. Moreover, our approach shows a promising potential in the application of Click-Through-Rate (CTR) prediction for online advertising. version:2
arxiv-1605-06711 | Learning From Hidden Traits: Joint Factor Analysis and Latent Clustering | http://arxiv.org/abs/1605.06711 | id:1605.06711 author:Bo Yang, Xiao Fu, Nicholas D. Sidiropoulos category:cs.LG stat.ML  published:2016-05-21 summary:Dimensionality reduction techniques play an essential role in data analytics, signal processing and machine learning. Dimensionality reduction is usually performed in a preprocessing stage that is separate from subsequent data analysis, such as clustering or classification. Finding reduced-dimension representations that are well-suited for the intended task is more appealing. This paper proposes a joint factor analysis and latent clustering framework, which aims at learning cluster-aware low-dimensional representations of matrix and tensor data. The proposed approach leverages matrix and tensor factorization models that produce essentially unique latent representations of the data to unravel latent cluster structure -- which is otherwise obscured because of the freedom to apply an oblique transformation in latent space. At the same time, latent cluster structure is used as prior information to enhance the performance of factorization. Specific contributions include several custom-built problem formulations, corresponding algorithms, and discussion of associated convergence properties. Besides extensive simulations, real-world datasets such as Reuters document data and MNIST image data are also employed to showcase the effectiveness of the proposed approaches. version:1
arxiv-1605-06710 | Chess Player by Co-Evolutionary Algorithm | http://arxiv.org/abs/1605.06710 | id:1605.06710 author:Nuno Ramos, Sergio Salgado, Agostinho C Rosa category:cs.NE  published:2016-05-21 summary:A co-evolutionary algorithm (CA) based chess player is presented. Implementation details of the algorithms, namely coding, population, variation operators are described. The alpha-beta or mini-max like behaviour of the player is achieved through two competitive or cooperative populations. Special attention is given to the fitness function evaluation (the heart of the solution). Test results on algorithms vs. algorithms or human player is provided. version:1
arxiv-1605-06708 | Automatic Detection of Epileptiform Discharges in the EEG | http://arxiv.org/abs/1605.06708 | id:1605.06708 author:Andre Rosado, Agostinho C Rosa category:cs.CV q-bio.NC  published:2016-05-21 summary:The diagnosis of epilepsy generally includes a visual inspection of EEG recorded data by the Neurologist, with the purpose of checking the occurrence of transient waveforms called interictal epileptiform discharges. These waveforms have short duration (less than 100 ms), so the inspection process is usually time-consuming, particularly for ambulatory long term EEG records. Therefore, an automatic detection system of epileptiform discharges can be a valuable tool for a Neurology service. The proposed approach is the development of a multi stage detection algorithm, which processes the complete EEG signals and applies decision criteria to selected waveforms. It employs EEG analysis techniques such as Wavelet Transform and Mimetic Analysis, complemented with a classification based on Fuzzy Logic. In order to evaluate the algorithm's performance, data were collected from several epileptic patients, with epileptiform activity marked by a Neurologist. The average values obtained for both Sensitivity and Specificity were respectively higher than 80 and 70 percent. version:1
arxiv-1601-06303 | Undecidability of the Lambek calculus with a relevant modality | http://arxiv.org/abs/1601.06303 | id:1601.06303 author:Max Kanovich, Stepan Kuznetsov, Andre Scedrov category:math.LO cs.CL 03B47  published:2016-01-23 summary:Morrill and Valentin in the paper "Computational coverage of TLG: Nonlinearity" considered an extension of the Lambek calculus enriched by a so-called "exponential" modality. This modality behaves in the "relevant" style, that is, it allows contraction and permutation, but not weakening. Morrill and Valentin stated an open problem whether this system is decidable. Here we show its undecidability. Our result remains valid if we consider the fragment where all division operations have one direction. We also show that the derivability problem in a restricted case, where the modality can be applied only to variables (primitive types), is decidable and belongs to the NP class. version:3
arxiv-1605-06695 | Fine-to-coarse Knowledge Transfer For Low-Res Image Classification | http://arxiv.org/abs/1605.06695 | id:1605.06695 author:Xingchao Peng, Judy Hoffman, Stella X. Yu, Kate Saenko category:cs.CV  published:2016-05-21 summary:We address the difficult problem of distinguishing fine-grained object categories in low resolution images. Wepropose a simple an effective deep learning approach that transfers fine-grained knowledge gained from high resolution training data to the coarse low-resolution test scenario. Such fine-to-coarse knowledge transfer has many real world applications, such as identifying objects in surveillance photos or satellite images where the image resolution at the test time is very low but plenty of high resolution photos of similar objects are available. Our extensive experiments on two standard benchmark datasets containing fine-grained car models and bird species demonstrate that our approach can effectively transfer fine-detail knowledge to coarse-detail imagery. version:1
arxiv-1605-06693 | Efficient Document Indexing Using Pivot Tree | http://arxiv.org/abs/1605.06693 | id:1605.06693 author:Gaurav Singh, Benjamin Piwowarski category:cs.IR cs.LG  published:2016-05-21 summary:We present a novel method for efficiently searching top-k neighbors for documents represented in high dimensional space of terms based on the cosine similarity. Mostly, documents are stored as bag-of-words tf-idf representation. One of the most used ways of computing similarity between a pair of documents is cosine similarity between the vector representations, but cosine similarity is not a metric distance measure as it doesn't follow triangle inequality, therefore most metric searching methods can not be applied directly. We propose an efficient method for indexing documents using a pivot tree that leads to efficient retrieval. We also study the relation between precision and efficiency for the proposed method and compare it with a state of the art in the area of document searching based on inner product. version:1
arxiv-1510-05956 | Optimal Cluster Recovery in the Labeled Stochastic Block Model | http://arxiv.org/abs/1510.05956 | id:1510.05956 author:Se-Young Yun, Alexandre Proutiere category:math.PR cs.LG cs.SI stat.ML  published:2015-10-20 summary:We consider the problem of community detection or clustering in the labeled Stochastic Block Model (LSBM) with a finite number $K$ of clusters of sizes linearly growing with the global population of items $n$. Every pair of items is labeled independently at random, and label $\ell$ appears with probability $p(i,j,\ell)$ between two items in clusters indexed by $i$ and $j$, respectively. The objective is to reconstruct the clusters from the observation of these random labels. Clustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters. We find the set of parameters such that there exists a clustering algorithm with at most $s$ misclassified items in average under the general LSBM and for any $s=o(n)$, which solves one open problem raised in \cite{abbe2015community}. We further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within $O(n \mbox{polylog}(n))$ computations and without the a-priori knowledge of the model parameters. version:6
arxiv-1604-02336 | Back to the Basics: Bayesian extensions of IRT outperform neural networks for proficiency estimation | http://arxiv.org/abs/1604.02336 | id:1604.02336 author:Kevin H. Wilson, Yan Karklin, Bojian Han, Chaitanya Ekanadham category:cs.AI cs.LG  published:2016-04-08 summary:Estimating student proficiency is an important task for computer based learning systems. We compare a family of IRT-based proficiency estimation methods to Deep Knowledge Tracing (DKT), a recently proposed recurrent neural network model with promising initial results. We evaluate how well each model predicts a student's future response given previous responses using two publicly available and one proprietary data set. We find that IRT-based methods consistently matched or outperformed DKT across all data sets at the finest level of content granularity that was tractable for them to be trained on. A hierarchical extension of IRT that captured item grouping structure performed best overall. When data sets included non-trivial autocorrelations in student response patterns, a temporal extension of IRT improved performance over standard IRT while the RNN-based method did not. We conclude that IRT-based models provide a simpler, better-performing alternative to existing RNN-based models of student interaction data while also affording more interpretability and guarantees due to their formulation as Bayesian probabilistic models. version:2
arxiv-1605-06673 | Cross Domain Adaptation by Learning Partially Shared Classifiers and Weighting Source Data Points in the Shared Subspaces | http://arxiv.org/abs/1605.06673 | id:1605.06673 author:Hongqi Wang, Anfeng Xu, Shanshan Wang, Sunny Chughtai category:cs.LG  published:2016-05-21 summary:Transfer learning is a problem defined over two domains. These two domains share the same feature space and class label space, but have significantly different distributions. One domain has sufficient labels, named as source domain, and the other domain has few labels, named as target do- main. The problem is to learn a effective classifier for the target domain. In this paper, we propose a novel transfer learning method for this problem by learning a partially shared classifier for the target domain, and weighting the source domain data points. We learn some shared subspaces for both the data points of the two domains, and a shared classifier in the shared subspaces. We hope that in the shared subspaces, the distributions of two domain can match each other well, and to match the distributions, we weight the source domain data points with different weighting factors. Moreover, we adapt the shared classifier to each domain by learning different adaptation functions. To learn the subspace transformation matrices, the classifier parameters, and the adaptation parameters, we build a objective function with weighted clas- sification errors, parameter regularization, local reconstruction regularization, and distribution matching. This objective function is minimized by an itera- tive algorithm. Experiments show its effectiveness over benchmark data sets, including travel destination review data set, face expression data set, spam email data set, etc. version:1
arxiv-1602-02202 | Efficient Second Order Online Learning by Sketching | http://arxiv.org/abs/1602.02202 | id:1602.02202 author:Haipeng Luo, Alekh Agarwal, Nicolo Cesa-Bianchi, John Langford category:cs.LG  published:2016-02-06 summary:We propose Sketched Online Newton (SON), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step, which, via sketching techniques enjoys a running time linear in the dimension and sketch size. We further develop sparse forms of the sketching methods (such as Oja's rule), making the computation linear in the sparsity of features. Together, the algorithm eliminates all computational obstacles in previous second order online learning approaches. version:2
arxiv-1605-06652 | Bending the Curve: Improving the ROC Curve Through Error Redistribution | http://arxiv.org/abs/1605.06652 | id:1605.06652 author:Oran Richman, Shie Mannor category:cs.LG  published:2016-05-21 summary:Classification performance is often not uniform over the data. Some areas in the input space are easier to classify than others. Features that hold information about the "difficulty" of the data may be non-discriminative and are therefore disregarded in the classification process. We propose a meta-learning approach where performance may be improved by post-processing. This improvement is done by establishing a dynamic threshold on the base-classifier results. Since the base-classifier is treated as a "black box" the method presented can be used on any state of the art classifier in order to try an improve its performance. We focus our attention on how to better control the true-positive/false-positive trade-off known as the ROC curve. We propose an algorithm for the derivation of optimal thresholds by redistributing the error depending on features that hold information about difficulty. We demonstrate the resulting benefit on both synthetic and real-life data. version:1
arxiv-1605-06651 | Gambler's Ruin Bandit Problem | http://arxiv.org/abs/1605.06651 | id:1605.06651 author:Nima Akbarzadeh, Cem Tekin category:cs.LG  published:2016-05-21 summary:In this paper, we propose a new sequential decision making problem called {\em gambler's ruin bandit problem} (GRBP). In each round of the GRBP the learner faces a gambler's ruin problem with two possible actions: a {\em continuation action} that moves the learner randomly over the state space around the current state; and a {\em terminal action} that moves the learner directly into one of the two terminal states (goal and dead-end state). The current round ends when a terminal state is reached. We first formulate GRBP as an optimization problem, and prove that the optimal policy is characterized by a simple threshold rule. The problem is solved for infinite time budget. Then, we consider the case when the state transition probabilities are unknown and provide logarithmic problem specific regret bounds. We also identify a condition under which the learner only incurs finite regret. Numerous applications including optimal medical treatment assignment can be formulated as a GRBP, in which the continuation action corresponds to the conservative treatment and the terminal action corresponds to the surgery. version:1
arxiv-1605-06650 | Latent Tree Models for Hierarchical Topic Detection | http://arxiv.org/abs/1605.06650 | id:1605.06650 author:Peixian Chen, Nevin L. Zhang, Tengfei Liu, Leonard K. M. Poon, Zhourong Chen category:cs.CL cs.IR cs.LG stat.ML  published:2016-05-21 summary:We propose a novel method for hierarchical topic detection where topics are obtained by clustering documents in multiple ways. Specifically, we model document collections using a class of graphical models called hierarchical latent tree models (HLTMs). The variables at the bottom level of an HLTM are observed binary variables that represent the presence/absence of words in a document. The variables at other levels are discrete latent variables, with those at the second level representing word co-occurrence patterns and those at higher levels representing co-occurrence of patterns at the level below. Each latent variable gives a soft partition of the documents, and document clusters in the partitions are interpreted as topics. Latent variables at high levels of the hierarchy capture long-range word co-occurrence patterns and hence give thematically more general topics, while those at low levels of the hierarchy capture short-range word co-occurrence patterns and give thematically more specific topics. Compared with LDA-based topic models, a key advantage of HLTMs is that they, as graphical models, explicitly model the dependence and independence structure among topics and words, which is conducive to the discovery of meaningful topics and topic hierarchies. version:1
arxiv-1605-06640 | Programming with a Differentiable Forth Interpreter | http://arxiv.org/abs/1605.06640 | id:1605.06640 author:Sebastian Riedel, Matko Bošnjak, Tim Rocktäschel category:cs.NE cs.AI cs.LG  published:2016-05-21 summary:There are families of neural networks that can learn to compute any function, provided sufficient training data. However, given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. Here we consider the case of prior procedural knowledge such as knowing the overall recursive structure of a sequence transduction program or the fact that a program will likely use arithmetic operations on real numbers to solve a task. To this end we present a differentiable interpreter for the programming language Forth. Through a neural implementation of the dual stack machine that underlies Forth, programmers can write program sketches with slots that can be filled with learnable behaviour. As the program interpreter is end-to-end differentiable, we can optimize this behaviour directly through gradient descent techniques on user specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex transduction tasks such as sequence sorting or addition with substantially less data and better generalisation over problem sizes. In addition, we introduce neural program optimisations based on symbolic computation and parallel branching that lead to significant speed improvements. version:1
arxiv-1605-06636 | Deep Transfer Learning with Joint Adaptation Networks | http://arxiv.org/abs/1605.06636 | id:1605.06636 author:Mingsheng Long, Jianmin Wang, Michael I. Jordan category:cs.LG stat.ML  published:2016-05-21 summary:Deep networks rely on massive amounts of labeled data to learn powerful models. For a target task short of labeled data, transfer learning enables model adaptation from a different source domain. This paper addresses deep transfer learning under a more general scenario that the joint distributions of features and labels may change substantially across domains. Based on the theory of Hilbert space embedding of distributions, a novel joint distribution discrepancy is proposed to directly compare joint distributions across domains, eliminating the need of marginal-conditional factorization. Transfer learning is enabled in deep convolutional networks, where the dataset shifts may linger in multiple task-specific feature layers and the classifier layer. A set of joint adaptation networks are crafted to match the joint distributions of these layers across domains by minimizing the joint distribution discrepancy, which can be trained efficiently using back-propagation. Experiments show that the new approach yields state of the art results on standard domain adaptation datasets. version:1
arxiv-1602-01582 | SDCA without Duality, Regularization, and Individual Convexity | http://arxiv.org/abs/1602.01582 | id:1602.01582 author:Shai Shalev-Shwartz category:cs.LG  published:2016-02-04 summary:Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularization and do not rely on duality. We prove linear convergence rates even if individual loss functions are non-convex, as long as the expected loss is strongly convex. version:2
arxiv-1605-06619 | Make Workers Work Harder: Decoupled Asynchronous Proximal Stochastic Gradient Descent | http://arxiv.org/abs/1605.06619 | id:1605.06619 author:Yitan Li, Linli Xu, Xiaowei Zhong, Qing Ling category:math.OC cs.DC cs.LG stat.ML  published:2016-05-21 summary:Asynchronous parallel optimization algorithms for solving large-scale machine learning problems have drawn significant attention from academia to industry recently. This paper proposes a novel algorithm, decoupled asynchronous proximal stochastic gradient descent (DAP-SGD), to minimize an objective function that is the composite of the average of multiple empirical losses and a regularization term. Unlike the traditional asynchronous proximal stochastic gradient descent (TAP-SGD) in which the master carries much of the computation load, the proposed algorithm off-loads the majority of computation tasks from the master to workers, and leaves the master to conduct simple addition operations. This strategy yields an easy-to-parallelize algorithm, whose performance is justified by theoretical convergence analyses. To be specific, DAP-SGD achieves an $O(\log T/T)$ rate when the step-size is diminishing and an ergodic $O(1/\sqrt{T})$ rate when the step-size is constant, where $T$ is the number of total iterations. version:1
arxiv-1505-05972 | Instant Learning: Parallel Deep Neural Networks and Convolutional Bootstrapping | http://arxiv.org/abs/1505.05972 | id:1505.05972 author:Andrew J. R. Simpson category:cs.LG 68Txx  published:2015-05-22 summary:Although deep neural networks (DNN) are able to scale with direct advances in computational power (e.g., memory and processing speed), they are not well suited to exploit the recent trends for parallel architectures. In particular, gradient descent is a sequential process and the resulting serial dependencies mean that DNN training cannot be parallelized effectively. Here, we show that a DNN may be replicated over a massive parallel architecture and used to provide a cumulative sampling of local solution space which results in rapid and robust learning. We introduce a complimentary convolutional bootstrapping approach that enhances performance of the parallel architecture further. Our parallelized convolutional bootstrapping DNN out-performs an identical fully-trained traditional DNN after only a single iteration of training. version:2
arxiv-1602-04474 | Generalization Properties of Learning with Random Features | http://arxiv.org/abs/1602.04474 | id:1602.04474 author:Alessandro Rudi, Raffaello Camoriano, Lorenzo Rosasco category:stat.ML cs.LG  published:2016-02-14 summary:We study the generalization properties of regularized learning with random features in the statistical learning theory framework. We show that optimal learning errors can be achieved with a number of features smaller than the number of examples. As a byproduct, we also show that learning with random features can be seen as a form of regularization, rather than only a way to speed up computations. version:2
arxiv-1602-01925 | Massively Multilingual Word Embeddings | http://arxiv.org/abs/1602.01925 | id:1602.01925 author:Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, Noah A. Smith category:cs.CL  published:2016-02-05 summary:We introduce new methods for estimating and evaluating embeddings of words in more than fifty languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVEC-CCA, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods. version:2
arxiv-1603-09320 | Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs | http://arxiv.org/abs/1603.09320 | id:1603.09320 author:Yu. A. Malkov, D. A. Yashunin category:cs.DS cs.CV cs.IR cs.SI  published:2016-03-30 summary:We present a new algorithm for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW). The proposed approach is fully graph-based, without any need for additional search structures which are typically used at coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to the NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space method is able to strongly outperform many previous state-of-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation. version:2
arxiv-1605-06597 | Adaptive Algorithm and Platform Selection for Visual Detection and Tracking | http://arxiv.org/abs/1605.06597 | id:1605.06597 author:Shu Zhang, Qi Zhu, Amit Roy-Chowdhury category:cs.CV  published:2016-05-21 summary:Computer vision algorithms are known to be extremely sensitive to the environmental conditions in which the data is captured, e.g., lighting conditions and target density. Tuning of parameters or choosing a completely new algorithm is often needed to achieve a certain performance level, especially when there is a limitation of the computation source. In this paper, we focus on this problem and propose a framework to adaptively select the "best" algorithm-parameter combination and the computation platform under performance and cost constraints at design time, and adapt the algorithms at runtime based on real-time inputs. This necessitates developing a mechanism to switch between different algorithms as the nature of the input video changes. Our proposed algorithm calculates a similarity function between a test video scenario and each training scenario, where the similarity calculation is based on learning a manifold of image features that is shared by both the training and test datasets. Similarity between training and test dataset indicates the same algorithm can be applied to both of them and achieve similar performance. We design a cost function with this similarity measure to find the most similar training scenario to the test data. The "best" algorithm under a given platform is obtained by selecting the algorithm with a specific parameter combination that performs the best on the corresponding training data. The proposed framework can be used first offline to choose the platform based on performance and cost constraints, and then online whereby the "best" algorithm is selected for each new incoming video segment for a given platform. In the experiments, we apply our algorithm to the problems of pedestrian detection and tracking. We show how to adaptively select platforms and algorithm-parameter combinations. Our results provide optimal performance on 3 publicly available datasets. version:1
arxiv-1605-06595 | WAHRSIS: A Low-cost, High-resolution Whole Sky Imager With Near-Infrared Capabilities | http://arxiv.org/abs/1605.06595 | id:1605.06595 author:Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler category:astro-ph.IM cs.CV  published:2016-05-21 summary:Cloud imaging using ground-based whole sky imagers is essential for a fine-grained understanding of the effects of cloud formations, which can be useful in many applications. Some such imagers are available commercially, but their cost is relatively high, and their flexibility is limited. Therefore, we built a new daytime Whole Sky Imager (WSI) called Wide Angle High-Resolution Sky Imaging System. The strengths of our new design are its simplicity, low manufacturing cost and high resolution. Our imager captures the entire hemisphere in a single high-resolution picture via a digital camera using a fish-eye lens. The camera was modified to capture light across the visible as well as the near-infrared spectral ranges. This paper describes the design of the device as well as the geometric and radiometric calibration of the imaging system. version:1
arxiv-1605-06593 | Influence Maximization with Semi-Bandit Feedback | http://arxiv.org/abs/1605.06593 | id:1605.06593 author:Zheng Wen, Branislav Kveton, Michal Valko category:cs.LG cs.SI stat.ML  published:2016-05-21 summary:We study a stochastic online problem of learning to influence in a social network with semi-bandit feedback, individual observations of how influenced users influence others. Our problem combines challenges of partial monitoring, because the learning agent only observes the influenced portion of the network, and combinatorial bandits, because the cardinality of the feasible set is exponential in the maximum number of influencers. We propose a computationally efficient UCB-like algorithm for solving our problem, IMLinUCB, and analyze it on forests. Our regret bounds are polynomial in all quantities of interest; reflect the structure of the network; and do not depend on inherently large quantities, such as the reciprocal of the minimum probability of being influenced and the cardinality of the action set. To the best of our knowledge, these are the first such results. IMLinUCB permits linear generalization and therefore is suitable for large-scale problems. We evaluate IMLinUCB on several synthetic problems and observe that the regret of IMLinUCB scales as suggested by our upper bounds. A special form of our problem can be viewed as a linear bandit and we match the regret bounds of LinUCB in this case. version:1
arxiv-1605-04469 | Rationale-Augmented Convolutional Neural Networks for Text Classification | http://arxiv.org/abs/1605.04469 | id:1605.04469 author:Ye Zhang, Iain Marshall, Byron C. Wallace category:cs.CL  published:2016-05-14 summary:We present a new Convolutional Neural Network (CNN) model for text classification that jointly exploits labels on documents and their component sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model uses such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its constituent sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on five classification datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong baselines. Moreover, our model naturally provides explanations for its predictions. version:2
arxiv-1605-06561 | DynaNewton - Accelerating Newton's Method for Machine Learning | http://arxiv.org/abs/1605.06561 | id:1605.06561 author:Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann category:cs.LG  published:2016-05-20 summary:Newton's method is a fundamental technique in optimization with quadratic convergence within a neighborhood around the optimum. However reaching this neighborhood is often slow and dominates the computational costs. We exploit two properties specific to empirical risk minimization problems to accelerate Newton's method, namely, subsampling training data and increasing strong convexity through regularization. We propose a novel continuation method, where we define a family of objectives over increasing sample sizes and with decreasing regularization strength. Solutions on this path are tracked such that the minimizer of the previous objective is guaranteed to be within the quadratic convergence region of the next objective to be optimized. Thereby every Newton iteration is guaranteed to achieve super-linear contractions with regard to the chosen objective, which becomes a moving target. We provide a theoretical analysis that motivates our algorithm, called DynaNewton, and characterizes its speed of convergence. Experiments on a wide range of data sets and problems consistently confirm the predicted computational savings. version:1
arxiv-1605-06560 | Functional Hashing for Compressing Neural Networks | http://arxiv.org/abs/1605.06560 | id:1605.06560 author:Lei Shi, Shikun Feng, ZhifanZhu category:cs.LG cs.NE  published:2016-05-20 summary:As the complexity of deep neural networks (DNNs) trend to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on functional hashing to compress DNNs, namely FunHashNN. For each entry in a deep net, FunHashNN uses multiple low-cost hash functions to fetch values in the compression space, and then employs a small reconstruction network to recover that entry. The reconstruction network is plugged into the whole network and trained jointly. FunHashNN includes the recently proposed HashedNets as a degenerated case, and benefits from larger value capacity and less reconstruction loss. We further discuss extensions with dual space hashing and multi-hops. On several benchmark datasets, FunHashNN demonstrates high compression ratios with little loss on prediction accuracy. version:1
arxiv-1604-06162 | The Extended Littlestone's Dimension for Learning with Mistakes and Abstentions | http://arxiv.org/abs/1604.06162 | id:1604.06162 author:Chicheng Zhang, Kamalika Chaudhuri category:cs.LG  published:2016-04-21 summary:This paper studies classification with an abstention option in the online setting. In this setting, examples arrive sequentially, the learner is given a hypothesis class $\mathcal H$, and the goal of the learner is to either predict a label on each example or abstain, while ensuring that it does not make more than a pre-specified number of mistakes when it does predict a label. Previous work on this problem has left open two main challenges. First, not much is known about the optimality of algorithms, and in particular, about what an optimal algorithmic strategy is for any individual hypothesis class. Second, while the realizable case has been studied, the more realistic non-realizable scenario is not well-understood. In this paper, we address both challenges. First, we provide a novel measure, called the Extended Littlestone's Dimension, which captures the number of abstentions needed to ensure a certain number of mistakes. Second, we explore the non-realizable case, and provide upper and lower bounds on the number of abstentions required by an algorithm to guarantee a specified number of mistakes. version:2
arxiv-1509-02957 | Semismooth Newton Coordinate Descent Algorithm for Elastic-Net Penalized Huber Loss Regression and Quantile Regression | http://arxiv.org/abs/1509.02957 | id:1509.02957 author:Congrui Yi, Jian Huang category:stat.CO stat.ML  published:2015-09-09 summary:We propose an algorithm, semismooth Newton coordinate descent (SNCD), for the elastic-net penalized Huber loss regression and quantile regression in high dimensional settings. Unlike existing coordinate descent type algorithms, the SNCD updates each regression coefficient and its corresponding subgradient simultaneously in each iteration. It combines the strengths of the coordinate descent and the semismooth Newton algorithm, and effectively solves the computational challenges posed by dimensionality and nonsmoothness. We establish the convergence properties of the algorithm. In addition, we present an adaptive version of the "strong rule" for screening predictors to gain extra efficiency. Through numerical experiments, we demonstrate that the proposed algorithm is very efficient and scalable to ultra-high dimensions. We illustrate the application via a real data example. version:2
arxiv-1508-07933 | Coordinate Dual Averaging for Decentralized Online Optimization with Nonseparable Global Objectives | http://arxiv.org/abs/1508.07933 | id:1508.07933 author:Soomin Lee, Angelia Nedić, Maxim Raginsky category:math.OC cs.LG cs.SY  published:2015-08-31 summary:We consider a decentralized online convex optimization problem in a network of agents, where each agent controls only a coordinate (or a part) of the global decision vector. For such a problem, we propose two decentralized variants (ODA-C and ODA-PS) of Nesterov's primal-dual algorithm with dual averaging. In ODA-C, to mitigate the disagreements on the primal-vector updates, the agents implement a generalization of the local information-exchange dynamics recently proposed by Li and Marden over a static undirected graph. In ODA-PS, the agents implement the broadcast-based push-sum dynamics over a time-varying sequence of uniformly connected digraphs. We show that the regret bounds in both cases have sublinear growth of $O(\sqrt{T})$, with the time horizon $T$, when the stepsize is of the form $1/\sqrt{t}$ and the objective functions are Lipschitz-continuous convex functions with Lipschitz gradients. We also implement the proposed algorithms on a sensor network to complement our theoretical analysis. version:2
arxiv-1605-06523 | TensorLog: A Differentiable Deductive Database | http://arxiv.org/abs/1605.06523 | id:1605.06523 author:William W. Cohen category:cs.AI cs.DB cs.LG  published:2016-05-20 summary:Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into "deep" gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are "unrolled" into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efficient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other first-order probabilistic logics. version:1
arxiv-1605-06492 | Linear-memory and Decomposition-invariant Linearly Convergent Conditional Gradient Algorithm for Structured Polytopes | http://arxiv.org/abs/1605.06492 | id:1605.06492 author:Dan Garber, Ofer Meshi category:math.OC cs.LG  published:2016-05-20 summary:Recently, several works have shown that natural modifications of the classical conditional gradient method (aka Frank-Wolfe algorithm) for constrained convex optimization, provably converge with a linear rate when: i) the feasible set is a polytope, and ii) the objective is smooth and strongly-convex. However, all of these results suffer from two significant shortcomings: large memory requirement due to the need to store an explicit convex decomposition of the current iterate, and as a consequence, large running-time overhead per iteration, and worst case convergence rate that depends unfavorably on the dimension. In this work we present a new conditional gradient variant and a corresponding analysis that improves on both of the above shortcomings. In particular: both memory and computation overheads are only linear in the dimension. Moreover, in case the optimal solution is sparse, the new convergence rate replaces a factor which is at least linear in the dimension in previous works, with a linear dependence on the number of non-zeros in the optimal solution. At the heart of our method, and corresponding analysis, is a novel way to compute decomposition-invariant away-steps. While our theoretical guarantees do not apply to any polytope, they apply to several important structured polytopes that capture central concepts such as paths in graphs, perfect matchings in bipartite graphs, marginal distributions that arise in structured prediction tasks, and more. Our theoretical findings are complemented by empirical evidence which shows that our method delivers state-of-the-art performance. version:1
arxiv-1605-06489 | Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups | http://arxiv.org/abs/1605.06489 | id:1605.06489 author:Yani Ioannou, Duncan Robertson, Roberto Cipolla, Antonio Criminisi category:cs.NE cs.CV cs.LG  published:2016-05-20 summary:We propose a new method for training computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. Our sparse connection structure facilitates a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less compute, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU). version:1
arxiv-1605-06477 | Regression with n$\to$1 by Expert Knowledge Elicitation | http://arxiv.org/abs/1605.06477 | id:1605.06477 author:Marta Soare, Muhammad Ammad-ud-din, Samuel Kaski category:cs.LG  published:2016-05-20 summary:We consider regression under the "extremely small $n$ large $p$" condition. In particular, we focus on problems with so small sample sizes $n$ compared to the dimensionality $p$, even $n\to 1$, that predictors cannot be estimated without prior knowledge. Furthermore, we assume all prior knowledge that can be automatically extracted from databases has already been taken into account. This setup occurs in personalized medicine, for instance, when predicting treatment outcomes for an individual patient based on noisy high-dimensional genomics data. A remaining source of information is expert knowledge which has received relatively little attention in recent years. We formulate the inference problem of asking expert feedback on features on a budget, present experimental results for two setups: "small $n$" and "n=1 with similar data available", and derive conditions under which the elicitation strategy is optimal. Experiments on simulated experts, both on simulated and genomics data, demonstrate that the proposed strategy can drastically improve prediction accuracy. version:1
arxiv-1605-06474 | X-ray image separation via coupled dictionary learning | http://arxiv.org/abs/1605.06474 | id:1605.06474 author:Nikos Deligiannis, João F. C. Mota, Bruno Cornelis, Miguel R. D. Rodrigues, Ingrid Daubechies category:cs.CV  published:2016-05-20 summary:In support of art investigation, we propose a new source sepa- ration method that unmixes a single X-ray scan acquired from double-sided paintings. Unlike prior source separation meth- ods, which are based on statistical or structural incoherence of the sources, we use visual images taken from the front- and back-side of the panel to drive the separation process. The coupling of the two imaging modalities is achieved via a new multi-scale dictionary learning method. Experimental results demonstrate that our method succeeds in the discrimination of the sources, while state-of-the-art methods fail to do so. version:1
arxiv-1605-06465 | Swapout: Learning an ensemble of deep architectures | http://arxiv.org/abs/1605.06465 | id:1605.06465 author:Saurabh Singh, Derek Hoiem, David Forsyth category:cs.CV cs.LG cs.NE  published:2016-05-20 summary:We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a rich set of architectures including dropout, stochastic depth and residual architectures as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model. version:1
arxiv-1605-06457 | Virtual Worlds as Proxy for Multi-Object Tracking Analysis | http://arxiv.org/abs/1605.06457 | id:1605.06457 author:Adrien Gaidon, Qiao Wang, Yohann Cabon, Eleonora Vig category:cs.CV cs.LG cs.NE stat.ML  published:2016-05-20 summary:Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking. version:1
arxiv-1605-06451 | Fixed Points of Belief Propagation -- An Analysis via Polynomial Homotopy Continuation | http://arxiv.org/abs/1605.06451 | id:1605.06451 author:Christian Knoll, Franz Pernkopf, Dhagash Mehta, Tianran Chen category:stat.ML math.AG  published:2016-05-20 summary:Belief propagation (BP) is an iterative method to perform approximate inference on arbitrary graphical models. Whether BP converges and if the solution is a unique fixed point depends on both, the structure and the parametrization of the model. To understand this dependence we are interested in finding \emph{all} fixed points. In this work, we formulate BP as a set of polynomial equations, the solutions of which correspond to the BP fixed points. We apply the numerical polynomial-homotopy-continuation (NPHC) method to solve such systems. It is commonly believed that uniqueness of BP fixed points implies convergence to this fixed point. Contrary to this conjecture, we find graphs for which BP fails to converge, even though a unique fixed point exists. Moreover, we show that this fixed point gives a good approximation of the exact marginal distribution. version:1
arxiv-1605-06450 | Query-Efficient Imitation Learning for End-to-End Autonomous Driving | http://arxiv.org/abs/1605.06450 | id:1605.06450 author:Jiakai Zhang, Kyunghyun Cho category:cs.LG cs.AI cs.RO  published:2016-05-20 summary:One way to approach end-to-end autonomous driving is to learn a policy function that maps from a sensory input, such as an image frame from a front-facing camera, to a driving action, by imitating an expert driver, or a reference policy. This can be done by supervised learning, where a policy function is tuned to minimize the difference between the predicted and ground-truth actions. A policy function trained in this way however is known to suffer from unexpected behaviours due to the mismatch between the states reachable by the reference policy and trained policy functions. More advanced algorithms for imitation learning, such as DAgger, addresses this issue by iteratively collecting training examples from both reference and trained policies. These algorithms often requires a large number of queries to a reference policy, which is undesirable as the reference policy is often expensive. In this paper, we propose an extension of the DAgger, called SafeDAgger, that is query-efficient and more suitable for end-to-end autonomous driving. We evaluate the proposed SafeDAgger in a car racing simulator and show that it indeed requires less queries to a reference policy. We observe a significant speed up in convergence, which we conjecture to be due to the effect of automated curriculum learning. version:1
arxiv-1512-06452 | ATD: Anomalous Topic Discovery in High Dimensional Discrete Data | http://arxiv.org/abs/1512.06452 | id:1512.06452 author:Hossein Soleimani, David J. Miller category:stat.ML cs.LG  published:2015-12-20 summary:We propose an algorithm for detecting patterns exhibited by anomalous clusters in high dimensional discrete data. Unlike most anomaly detection (AD) methods, which detect individual anomalies, our proposed method detects groups (clusters) of anomalies; i.e. sets of points which collectively exhibit abnormal patterns. In many applications this can lead to better understanding of the nature of the atypical behavior and to identifying the sources of the anomalies. Moreover, we consider the case where the atypical patterns exhibit on only a small (salient) subset of the very high dimensional feature space. Individual AD techniques and techniques that detect anomalies using all the features typically fail to detect such anomalies, but our method can detect such instances collectively, discover the shared anomalous patterns exhibited by them, and identify the subsets of salient features. In this paper, we focus on detecting anomalous topics in a batch of text documents, developing our algorithm based on topic models. Results of our experiments show that our method can accurately detect anomalous topics and salient features (words) under each such topic in a synthetic data set and two real-world text corpora and achieves better performance compared to both standard group AD and individual AD techniques. All required code to reproduce our experiments is available from https://github.com/hsoleimani/ATD version:2
arxiv-1605-06444 | Unreasonable Effectiveness of Learning Neural Nets: Accessible States and Robust Ensembles | http://arxiv.org/abs/1605.06444 | id:1605.06444 author:Carlo Baldassi, Christian Borgs, Jennifer Chayes, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, Riccardo Zecchina category:stat.ML cond-mat.dis-nn cs.LG  published:2016-05-20 summary:In artificial neural networks, learning from data is a computationally demanding task in which a large number of connection weights are iteratively tuned through stochastic-gradient-based heuristic processes over a cost-function. It is not well understood how learning occurs in these systems, in particular how they avoid getting trapped in configurations with poor computational performance. Here we study the difficult case of networks with discrete weights, where the optimization landscape is very rough even for simple architectures, and provide theoretical and numerical evidence of the existence of rare---but extremely dense and accessible---regions of configurations in the network weight space. We define a novel measure, which we call the \emph{robust ensemble} (RE), which suppresses trapping by isolated configurations and amplifies the role of these dense regions. We analytically compute the RE in some exactly solvable models, and also provide a general algorithmic scheme which is straightforward to implement: define a cost-function given by a sum of a finite number of replicas of the original cost-function, with a constraint centering the replicas around a driving assignment. To illustrate this, we derive several powerful new algorithms, ranging from Markov Chains to message passing to gradient descent processes, where the algorithms target the robust dense states, resulting in substantial improvements in performance. The weak dependence on the number of precision bits of the weights leads us to conjecture that very similar reasoning applies to more conventional neural networks. Analogous algorithmic schemes can also be applied to other optimization problems. version:1
arxiv-1605-06443 | Structured Prediction Theory and Voted Risk Minimization | http://arxiv.org/abs/1605.06443 | id:1605.06443 author:Corinna Cortes, Mehryar Mohri, Vitaly Kuznetsov, Scott Yang category:stat.ML cs.LG  published:2016-05-20 summary:We present a general theoretical analysis of structured prediction. By introducing a new complexity measure that explicitly factors in the structure of the output space and the loss function, we are able to derive new data-dependent learning guarantees for a broad family of losses and for hypothesis sets with an arbitrary factor graph decomposition. We extend this theory by leveraging the principle of Voted Risk Minimization (VRM) and showing that learning is possible with complex factor graphs. We both present new learning bounds in this advanced setting as well as derive two new families of algorithms, \emph{Voted Conditional Random Fields} and \emph{Voted Structured Boosting}, which can make use of very complex features and factor graphs without overfitting. Finally, we also validate our theory through experiments on several datasets. version:1
arxiv-1605-06439 | Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning | http://arxiv.org/abs/1605.06439 | id:1605.06439 author:Wouter M. Koolen, Peter Grünwald, Tim van Erven category:cs.LG  published:2016-05-20 summary:We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability. version:1
arxiv-1603-05642 | Optimal Black-Box Reductions Between Optimization Objectives | http://arxiv.org/abs/1603.05642 | id:1603.05642 author:Zeyuan Allen-Zhu, Elad Hazan category:math.OC cs.DS cs.LG stat.ML  published:2016-03-17 summary:The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the specific regression or classification task at hand. We reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications. Furthermore, unlike existing results, our new reductions are OPTIMAL and more PRACTICAL. We show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions, and conclude with experiments showing their successes also in practice. version:3
arxiv-1605-06437 | Learning shape correspondence with anisotropic convolutional neural networks | http://arxiv.org/abs/1605.06437 | id:1605.06437 author:Davide Boscaini, Jonathan Masci, Emanuele Rodolà, Michael M. Bronstein category:cs.CV  published:2016-05-20 summary:Establishing correspondence between shapes is a fundamental problem in geometry processing, arising in a wide variety of applications. The problem is especially difficult in the setting of non-isometric deformations, as well as in the presence of topological noise and missing parts, mainly due to the limited capability to model such deformations axiomatically. Several recent works showed that invariance to complex shape transformations can be learned from examples. In this paper, we introduce an intrinsic convolutional neural network architecture based on anisotropic diffusion kernels, which we term Anisotropic Convolutional Neural Network (ACNN). In our construction, we generalize convolutions to non-Euclidean domains by constructing a set of oriented anisotropic diffusion kernels, creating in this way a local intrinsic polar representation of the data (`patch'), which is then correlated with a filter. Several cascades of such filters, linear, and non-linear operators are stacked to form a deep neural network whose parameters are learned by minimizing a task-specific cost. We use ACNNs to effectively learn intrinsic dense correspondences between deformable shapes in very challenging settings, achieving state-of-the-art results on some of the most difficult recent correspondence benchmarks. version:1
arxiv-1605-06432 | Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data | http://arxiv.org/abs/1605.06432 | id:1605.06432 author:Maximilian Karl, Maximilian Soelch, Justin Bayer, Patrick van der Smagt category:stat.ML cs.LG cs.SY  published:2016-05-20 summary:We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions by means of variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction. version:1
arxiv-1512-07876 | An unsupervised spatiotemporal graphical modeling approach to anomaly detection in distributed CPS | http://arxiv.org/abs/1512.07876 | id:1512.07876 author:Chao Liu, Sambuddha Ghosal, Zhanhong Jiang, Soumik Sarkar category:cs.LG  published:2015-12-24 summary:Modern distributed cyber-physical systems (CPSs) encounter a large variety of physical faults and cyber anomalies and in many cases, they are vulnerable to catastrophic fault propagation scenarios due to strong connectivity among the sub-systems. This paper presents a new data-driven framework for system-wide anomaly detection for addressing such issues. The framework is based on a spatiotemporal feature extraction scheme built on the concept of symbolic dynamics for discovering and representing causal interactions among the subsystems of a CPS. The extracted spatiotemporal features are then used to learn system-wide patterns via a Restricted Boltzmann Machine (RBM). The results show that: (1) the RBM free energy in the off-nominal conditions is different from that in the nominal conditions and can be used for anomaly detection; (2) the framework can capture multiple nominal modes with one graphical model; (3) the case studies with simulated data and an integrated building system validate the proposed approach. version:2
arxiv-1605-06431 | Residual Networks are Exponential Ensembles of Relatively Shallow Networks | http://arxiv.org/abs/1605.06431 | id:1605.06431 author:Andreas Veit, Michael Wilber, Serge Belongie category:cs.CV cs.AI cs.LG cs.NE  published:2016-05-20 summary:In this work, we introduce a novel interpretation of residual networks showing they are exponential ensembles. This observation is supported by a large-scale lesion study that demonstrates they behave just like ensembles at test time. Subsequently, we perform an analysis showing these ensembles mostly consist of networks that are each relatively shallow. For example, contrary to our expectations, most of the gradient in a residual network with 110 layers comes from an ensemble of very short networks, i.e., only 10-34 layers deep. This suggests that in addition to describing neural networks in terms of width and depth, there is a third dimension: multiplicity, the size of the implicit ensemble. Ultimately, residual networks do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network - rather, they avoid the problem simply by ensembling many short networks together. This insight reveals that depth is still an open research question and invites the exploration of the related notion of multiplicity. version:1
arxiv-1605-06423 | Coresets for Scalable Bayesian Logistic Regression | http://arxiv.org/abs/1605.06423 | id:1605.06423 author:Jonathan H. Huggins, Trevor Campbell, Tamara Broderick category:stat.CO cs.DS stat.ML  published:2016-05-20 summary:The use of Bayesian models in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. The proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. version:1
arxiv-1605-06421 | Root-cause analysis for time-series anomalies via spatiotemporal causal graphical modeling | http://arxiv.org/abs/1605.06421 | id:1605.06421 author:Chao Liu, Kin Gwn Lore, Soumik Sarkar category:cs.LG  published:2016-05-20 summary:Modern distributed cyber-physical systems encounter a large variety of anomalies and in many cases, they are vulnerable to catastrophic fault propagation scenarios due to strong connectivity among the sub-systems. In this regard, root-cause analysis becomes highly intractable due to complex fault propagation mechanisms in combination with diverse operating modes. This paper presents a new data-driven framework for root-cause analysis for addressing such issues. The framework is based on a spatiotemporal feature extraction scheme for multivariate time series built on the concept of symbolic dynamics for discovering and representing causal interactions among subsystems of a complex system. We propose sequential state switching ($S^3$) and artificial anomaly association ($A^3$) methods to implement root-cause analysis in an unsupervised and semi-supervised manner respectively. Synthetic data from cases with failed pattern(s) and anomalous node are simulated to validate the proposed approaches, then compared with the performance of vector autoregressive (VAR) model-based root-cause analysis. The results show that: (1) $S^3$ and $A^3$ approaches can obtain high accuracy in root-cause analysis and successfully handle multiple nominal operation modes, and (2) the proposed tool-chain is shown to be scalable while maintaining high accuracy. version:1
arxiv-1605-06420 | Quantifying the accuracy of approximate diffusions and Markov chains | http://arxiv.org/abs/1605.06420 | id:1605.06420 author:Jonathan H. Huggins, James Zou category:math.ST math.PR stat.CO stat.ML stat.TH  published:2016-05-20 summary:Diffusions and their discretizations as Markov chains are a workhorse for inference, sampling and modeling. With the growth of large-scale datasets, the computational cost associated with simulating these stochastic processes can be considerable, and many algorithms have been proposed to approximate the underlying Markov chain or diffusion. A fundamental question is how the computational savings trade off against the statistical error incurred due to approximations. This paper develops general results to investigate this question. We bound the Wasserstein distance between the equilibrium distributions of two diffusions as a function of their mixing rates and the deviation in their drifts. We show that this error bound is exact in simple Gaussian settings. This general result on continuous diffusions can be discretized to provide insights on the computational--statistical trade-off of Markov chains. As an illustration, we apply our framework to derive finite-sample error bounds of approximate unadjusted Langevin dynamics. We characterize computation-constrained settings where, by using fast-to-compute approximate gradients in the Langevin dynamics, we obtain more accurate samples compared to using the exact gradients. Our theoretical analyses are supported by simulation experiments. version:1
arxiv-1605-06417 | Shape Recognition by Bag of Skeleton-associated Contour Parts | http://arxiv.org/abs/1605.06417 | id:1605.06417 author:Wei Shen, Yuan Jiang, Wenjing Gao, Dan Zeng, Xinggang Wang category:cs.CV  published:2016-05-20 summary:Contour and skeleton are two complementary representations for shape recognition. However combining them in a principal way is nontrivial, as they are generally abstracted by different structures (closed string vs graph), respectively. This paper aims at addressing the shape recognition problem by combining contour and skeleton according to the correspondence between them. The correspondence provides a straightforward way to associate skeletal information with a shape contour. More specifically, we propose a new shape descriptor. named Skeleton-associated Shape Context (SSC), which captures the features of a contour fragment associated with skeletal information. Benefited from the association, the proposed shape descriptor provides the complementary geometric information from both contour and skeleton parts, including the spatial distribution and the thickness change along the shape part. To form a meaningful shape feature vector for an overall shape, the Bag of Features framework is applied to the SSC descriptors extracted from it. Finally, the shape feature vector is fed into a linear SVM classifier to recognize the shape. The encouraging experimental results demonstrate that the proposed way to combine contour and skeleton is effective for shape recognition, which achieves the state-of-the-art performances on several standard shape benchmarks. version:1
arxiv-1605-06416 | Statistical Inference for Cluster Trees | http://arxiv.org/abs/1605.06416 | id:1605.06416 author:Yen-Chi Chen, Jisu Kim, Sivaraman Balakrishnan, Alessandro Rinaldo, Larry Wasserman category:math.ST stat.ME stat.ML stat.TH  published:2016-05-20 summary:A cluster tree provides a highly-interpretable summary of a density function by representing the hierarchy of its high-density clusters. It is estimated using the empirical tree, which is the cluster tree constructed from a density estimator. This paper addresses the basic question of quantifying our uncertainty by assessing the statistical significance of features of an empirical cluster tree. We first study a variety of metrics that can be used to compare different trees, analyze their properties and assess their suitability for inference. We then propose methods to construct and summarize confidence sets for the unknown true cluster tree. We introduce a partial ordering on cluster trees which we use to prune some of the statistically insignificant features of the empirical tree, yielding interpretable and parsimonious cluster trees. Finally, we illustrate the proposed methods on a variety of synthetic examples and furthermore demonstrate their utility in the analysis of a Graft-versus-Host Disease (GvHD) data set. version:1
arxiv-1605-06409 | R-FCN: Object Detection via Region-based Fully Convolutional Networks | http://arxiv.org/abs/1605.06409 | id:1605.06409 author:Jifeng Dai, Yi Li, Kaiming He, Jian Sun category:cs.CV  published:2016-05-20 summary:We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code will be made publicly available. version:1
arxiv-1605-06402 | Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks | http://arxiv.org/abs/1605.06402 | id:1605.06402 author:Philipp Gysel category:cs.CV cs.LG cs.NE  published:2016-05-20 summary:Convolutional neural networks (CNN) have achieved major breakthroughs in recent years. Their performance in computer vision have matched and in some areas even surpassed human capabilities. Deep neural networks can capture complex non-linear features; however this ability comes at the cost of high computational and memory requirements. State-of-art networks require billions of arithmetic operations and millions of parameters. To enable embedded devices such as smartphones, Google glasses and monitoring cameras with the astonishing power of deep learning, dedicated hardware accelerators can be used to decrease both execution time and power consumption. In applications where fast connection to the cloud is not guaranteed or where privacy is important, computation needs to be done locally. Many hardware accelerators for deep neural networks have been proposed recently. A first important step of accelerator design is hardware-oriented approximation of deep networks, which enables energy-efficient inference. We present Ristretto, a fast and automated framework for CNN approximation. Ristretto simulates the hardware arithmetic of a custom hardware accelerator. The framework reduces the bit-width of network parameters and outputs of resource-intense layers, which reduces the chip area for multiplication units significantly. Alternatively, Ristretto can remove the need for multipliers altogether, resulting in an adder-only arithmetic. The tool fine-tunes trimmed networks to achieve high classification accuracy. Since training of deep neural networks can be time-consuming, Ristretto uses highly optimized routines which run on the GPU. This enables fast compression of any given network. Given a maximum tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available. version:1
arxiv-1605-06398 | Stochastic Variance Reduction Methods for Saddle-Point Problems | http://arxiv.org/abs/1605.06398 | id:1605.06398 author:P Balamurugan, Francis Bach category:cs.LG math.OC  published:2016-05-20 summary:We consider convex-concave saddle-point problems where the objective functions may be split in many components, and extend recent stochastic variance reduction methods (such as SVRG or SAGA) to provide the first large-scale linearly convergent algorithms for this class of problems which is common in machine learning. While the algorithmic extension is straightforward, it comes with challenges and opportunities: (a) the convex minimization analysis does not apply and we use the notion of monotone operators to prove convergence, showing in particular that the same algorithm applies to a larger class of problems, such as variational inequalities, (b) there are two notions of splits, in terms of functions, or in terms of partial derivatives, (c) the split does need to be done with convex-concave terms, (d) non-uniform sampling is key to an efficient algorithm, both in theory and practice, and (e) these incremental algorithms can be easily accelerated using a simple extension of the "catalyst" framework, leading to an algorithm which is always superior to accelerated batch algorithms. version:1
arxiv-1605-06394 | Bayesian Hyperparameter Optimization for Ensemble Learning | http://arxiv.org/abs/1605.06394 | id:1605.06394 author:Julien-Charles Lévesque, Christian Gagné, Robert Sabourin category:cs.LG  published:2016-05-20 summary:In this paper, we bridge the gap between hyperparameter optimization and ensemble learning by performing Bayesian optimization of an ensemble with regards to its hyperparameters. Our method consists in building a fixed-size ensemble, optimizing the configuration of one classifier of the ensemble at each iteration of the hyperparameter optimization algorithm, taking into consideration the interaction with the other models when evaluating potential performances. We also consider the case where the ensemble is to be reconstructed at the end of the hyperparameter optimization phase, through a greedy selection over the pool of models generated during the optimization. We study the performance of our proposed method on three different hyperparameter spaces, showing that our approach is better than both the best single model and a greedy ensemble construction over the models produced by a standard Bayesian optimization. version:1
arxiv-1605-06391 | Deep Multi-task Representation Learning: A Tensor Factorisation Approach | http://arxiv.org/abs/1605.06391 | id:1605.06391 author:Yongxin Yang, Timothy Hospedales category:cs.LG  published:2016-05-20 summary:Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices. version:1
arxiv-1605-06377 | Towards Automation of Knowledge Understanding: An Approach for Probabilistic Generative Classifiers | http://arxiv.org/abs/1605.06377 | id:1605.06377 author:Dominik Fisch, Christian Gruhl, Edgar Kalkowski, Bernhard Sick, Seppo J. Ovaska category:cs.LG cs.AI  published:2016-05-20 summary:After data selection, pre-processing, transformation, and feature extraction, knowledge extraction is not the final step in a data mining process. It is then necessary to understand this knowledge in order to apply it efficiently and effectively. Up to now, there is a lack of appropriate techniques that support this significant step. This is partly due to the fact that the assessment of knowledge is often highly subjective, e.g., regarding aspects such as novelty or usefulness. These aspects depend on the specific knowledge and requirements of the data miner. There are, however, a number of aspects that are objective and for which it is possible to provide appropriate measures. In this article we focus on classification problems and use probabilistic generative classifiers based on mixture density models that are quite common in data mining applications. We define objective measures to assess the informativeness, uniqueness, importance, discrimination, representativity, uncertainty, and distinguishability of rules contained in these classifiers numerically. These measures not only support a data miner in evaluating results of a data mining process based on such classifiers. As we will see in illustrative case studies, they may also be used to improve the data mining process itself or to support the later application of the extracted knowledge. version:1
arxiv-1604-04562 | A Network-based End-to-End Trainable Task-oriented Dialogue System | http://arxiv.org/abs/1604.04562 | id:1604.04562 author:Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, Steve Young category:cs.CL cs.AI cs.NE stat.ML  published:2016-04-15 summary:Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring labelled datasets and solving a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable dialogue system along with a new way of collecting task-oriented dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain. version:2
arxiv-1502-02077 | Quantum Energy Regression using Scattering Transforms | http://arxiv.org/abs/1502.02077 | id:1502.02077 author:Matthew Hirn, Nicolas Poilvert, Stéphane Mallat category:cs.LG cs.CV physics.chem-ph physics.comp-ph quant-ph  published:2015-02-06 summary:We present a novel approach to the regression of quantum mechanical energies based on a scattering transform of an intermediate electron density representation. A scattering transform is a deep convolution network computed with a cascade of multiscale wavelet transforms. It possesses appropriate invariant and stability properties for quantum energy regression. This new framework removes fundamental limitations of Coulomb matrix based energy regressions, and numerical experiments give state-of-the-art accuracy over planar molecules. version:3
arxiv-1605-06353 | Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction | http://arxiv.org/abs/1605.06353 | id:1605.06353 author:Marcin Junczys-Dowmunt, Roman Grundkiewicz category:cs.CL  published:2016-05-20 summary:In this work, we study parameter tuning towards the M$^2$ metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M$^2$ as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M$^2$ and offer partial solutions. To our surprise, we find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M$^2$ over previously 40.56%, by a neural encoder-decoder model) while being trained on the same data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M$^2$. version:1
arxiv-1603-06743 | Localized Lasso for High-Dimensional Regression | http://arxiv.org/abs/1603.06743 | id:1603.06743 author:Makoto Yamada, Koh Takeuchi, Tomoharu Iwata, John Shawe-Taylor, Samuel Kaski category:stat.ML cs.LG stat.ME  published:2016-03-22 summary:We introduce the localized Lasso, which is suited for learning models that are both interpretable and have a high predictive power in problems with high dimensionality $d$ and small sample size $n$. More specifically, we consider a function defined by local sparse models, one at each data point. We introduce sample-wise network regularization to borrow strength across the models, and sample-wise exclusive group sparsity (a.k.a., $\ell_{1,2}^2$ norm) to introduce diversity into the choice of feature sets in the local models. The local models are interpretable in terms of similarity of their sparsity patterns. The cost function is convex, and thus has a globally optimal solution. Moreover, we propose a simple yet efficient iterative least-squares based optimization procedure for the localized Lasso, which does not need a tuning parameter, and is guaranteed to converge to a globally optimal solution. The solution is empirically shown to outperform alternatives for both simulated and genomic personalized medicine data. version:2
arxiv-1506-04158 | A Spectral Algorithm with Additive Clustering for the Recovery of Overlapping Communities in Networks | http://arxiv.org/abs/1506.04158 | id:1506.04158 author:Emilie Kaufmann, Thomas Bonald, Marc Lelarge category:stat.ML  published:2015-06-12 summary:This paper presents a novel spectral algorithm with additive clustering, designed to identify overlapping communities in networks. The algorithm is based on geometric properties of the spectrum of the expected adjacency matrix in a random graph model that we call stochastic blockmodel withoverlap (SBMO). An adaptive version of the algorithm, that does not require the knowledge of the number of hidden communities, is proved to be consistent under the SBMO when the degrees in the graph are (slightly more than) logarithmic. The algorithm is shown to perform well on simulateddata and on real-world graphs with known overlapping communities. version:2
arxiv-1605-06336 | Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA | http://arxiv.org/abs/1605.06336 | id:1605.06336 author:Aapo Hyvarinen, Hiroshi Morioka category:stat.ML cs.LG  published:2016-05-20 summary:Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL), finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique --- thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general. version:1
arxiv-1605-06325 | Superpixel Hierarchy | http://arxiv.org/abs/1605.06325 | id:1605.06325 author:Xing Wei, Qingxiong Yang, Yihong Gong, Ming-Hsuan Yang, Narendra Ahuja category:cs.CV  published:2016-05-20 summary:Superpixel segmentation is becoming ubiquitous in computer vision. In practice, an object can either be represented by a number of segments in finer levels of detail or included in a surrounding region at coarser levels of detail, and thus a superpixel segmentation hierarchy is useful for applications that require different levels of image segmentation detail depending on the particular image objects segmented. Unfortunately, there is no method that can generate all scales of superpixels accurately in real-time. As a result, a simple yet effective algorithm named Super Hierarchy (SH) is proposed in this paper. It is as accurate as the state-of-the-art but 1-2 orders of magnitude faster. The proposed method can be directly integrated with recent efficient edge detectors like the structured forest edges to significantly outperforms the state-of-the-art in terms of segmentation accuracy. Quantitative and qualitative evaluation on a number of computer vision applications was conducted, demonstrating that the proposed method is the top performer. version:1
arxiv-1511-05118 | Random sampling of bandlimited signals on graphs | http://arxiv.org/abs/1511.05118 | id:1511.05118 author:Gilles Puy, Nicolas Tremblay, Rémi Gribonval, Pierre Vandergheynst category:cs.SI cs.LG stat.ML  published:2015-11-16 summary:We study the problem of sampling k-bandlimited signals on graphs. We propose two sampling strategies that consist in selecting a small subset of nodes at random. The first strategy is non-adaptive, i.e., independent of the graph structure, and its performance depends on a parameter called the graph coherence. On the contrary, the second strategy is adaptive but yields optimal results. Indeed, no more than O(k log(k)) measurements are sufficient to ensure an accurate and stable recovery of all k-bandlimited signals. This second strategy is based on a careful choice of the sampling distribution, which can be estimated quickly. Then, we propose a computationally efficient decoder to reconstruct k-bandlimited signals from their samples. We prove that it yields accurate reconstructions and that it is also stable to noise. Finally, we conduct several experiments to test these techniques. version:2
arxiv-1601-00013 | A single hidden layer feedforward network with only one neuron in the hidden layer can approximate any univariate function | http://arxiv.org/abs/1601.00013 | id:1601.00013 author:Namig J. Guliyev, Vugar E. Ismailov category:cs.NE cs.IT math.IT math.NA 41A30  65D15  92B20  published:2015-12-31 summary:The possibility of approximating a continuous function on a compact subset of the real line by a feedforward single hidden layer neural network with a sigmoidal activation function has been studied in many papers. Such networks can approximate an arbitrary continuous function provided that an unlimited number of neurons in a hidden layer is permitted. In this paper, we consider constructive approximation on any finite interval of $\mathbb{R}$ by neural networks with only one neuron in the hidden layer. We construct algorithmically a smooth, sigmoidal, almost monotone activation function $\sigma$ providing approximation to an arbitrary continuous function within any degree of accuracy. This algorithm is implemented in a computer program, which computes the value of $\sigma$ at any reasonable point of the real axis. version:2
arxiv-1605-06319 | As Cool as a Cucumber: Towards a Corpus of Contemporary Similes in Serbian | http://arxiv.org/abs/1605.06319 | id:1605.06319 author:Nikola Milosevic, Goran Nenadic category:cs.CL cs.AI  published:2016-05-20 summary:Similes are natural language expressions used to compare unlikely things, where the comparison is not taken literally. They are often used in everyday communication and are an important part of cultural heritage. Having an up-to-date corpus of similes is challenging, as they are constantly coined and/or adapted to the contemporary times. In this paper we present a methodology for semi-automated collection of similes from the world wide web using text mining techniques. We expanded an existing corpus of traditional similes (containing 333 similes) by collecting 446 additional expressions. We, also, explore how crowdsourcing can be used to extract and curate new similes. version:1
arxiv-1605-06311 | Poisson multi-Bernoulli conjugate prior for multiple extended object estimation | http://arxiv.org/abs/1605.06311 | id:1605.06311 author:Karl Granstrom, Maryam Fatemi, Lennart Svensson category:stat.CO cs.CV cs.SY  published:2016-05-20 summary:This paper presents a Poisson multi-Bernoulli mixture (PMBM) conjugate prior for multiple extended object estimation. A Poisson point process is used to describe the existence of yet undetected targets, while a multi-Bernoulli mixture describes the distribution of the targets that have been detected. The conjugacy property allows the posterior PMBM density to be computed exactly, meaning that given enough computational power the PMBM filter is correct. However, in practice, the data association problem requires approximations. The update and the prediction of the PMBM density parameters are presented and are given interpretations, and a simple linear Gaussian implementation is presented along with methods to handle the data association problem. A simulation study shows that the extended target PMBM filter outperforms the extended target cardinalized probability hypothesis density (CPHD) filter in scenarios where the expected number of detections per target per time step is low. version:1
arxiv-1604-01999 | Online Optimization of Smoothed Piecewise Constant Functions | http://arxiv.org/abs/1604.01999 | id:1604.01999 author:Vincent Cohen-Addad, Varun Kanade category:cs.LG stat.ML  published:2016-04-07 summary:We study online optimization of smoothed piecewise constant functions over the domain [0, 1). This is motivated by the problem of adaptively picking parameters of learning algorithms as in the recently introduced framework by Gupta and Roughgarden (2016). Majority of the machine learning literature has focused on Lipschitz-continuous functions or functions with bounded gradients. 1 This is with good reason---any learning algorithm suffers linear regret even against piecewise constant functions that are chosen adversarially, arguably the simplest of non-Lipschitz continuous functions. The smoothed setting we consider is inspired by the seminal work of Spielman and Teng (2004) and the recent work of Gupta and Roughgarden---in this setting, the sequence of functions may be chosen by an adversary, however, with some uncertainty in the location of discontinuities. We give algorithms that achieve sublinear regret in the full information and bandit settings. version:2
arxiv-1605-06304 | Local communities obstruct global consensus: Naming game on multi-local-world networks | http://arxiv.org/abs/1605.06304 | id:1605.06304 author:Yang Lou, Guanrong Chen, Zhengping Fan, Luna Xiang category:cs.SI cs.CL physics.soc-ph  published:2016-05-20 summary:Community structure is essential for social communications, where individuals belonging to the same community are much more actively interacting and communicating with each other than those in different communities within the human society. Naming game, on the other hand, is a social communication model that simulates the process of learning a name of an object within a community of humans, where the individuals can reach global consensus on naming an object asymptotically through iterative pair-wise conversations. The underlying communication network indicates the relationships among the individuals. In this paper, three typical topologies of human communication networks, namely random-graph, small-world and scale-free networks, are employed, which are embedded with the multi-local-world community structure, to study the naming game. Simulations show that 1) when the intra-community connections increase while the inter-community connections remain to be unchanged, the convergence to global consensus is slow and eventually might fail; 2) when the inter-community connections are sufficiently dense, both the number and the size of the communities do not affect the convergence process; and 3) for different topologies with the same average node-degree, local clustering of individuals obstruct or prohibit global consensus to take place. The results reveal the role of local communities in a global naming game in social network studies. version:1
arxiv-1605-06296 | On the Robustness of Decision Tree Learning under Label Noise | http://arxiv.org/abs/1605.06296 | id:1605.06296 author:Aritra Ghosh, Naresh Manwani, P. S. Sastry category:cs.LG  published:2016-05-20 summary:In most practical problems of classifier learning, the training data suffers from the label noise. Hence, it is important to understand how robust is a learning algorithm to such label noise. Experimentally, Decision trees have been found to be more robust against label noise than SVM and logistic regression. This paper presents some theoretical results to show that decision tree algorithms are robust to symmetric label noise under the assumption of large sample size. We also present some sample complexity results for this robustness. Through extensive simulations we illustrate this robustness. version:1
arxiv-1507-06120 | Towards Storytelling from Visual Lifelogging: An Overview | http://arxiv.org/abs/1507.06120 | id:1507.06120 author:Marc Bolaños, Mariella Dimiccoli, Petia Radeva category:cs.CV  published:2015-07-22 summary:Visual lifelogging consists of acquiring images that capture the daily experiences of the user by wearing a camera over a long period of time. The pictures taken offer considerable potential for knowledge mining concerning how people live their lives, hence, they open up new opportunities for many potential applications in fields including healthcare, security, leisure and the quantified self. However, automatically building a story from a huge collection of unstructured egocentric data presents major challenges. This paper provides a thorough review of advances made so far in egocentric data analysis, and in view of the current state of the art, indicates new lines of research to move us towards storytelling from visual lifelogging. version:4
arxiv-1605-06276 | Piece-wise quadratic lego set for constructing arbitrary error potentials and their fast optimization | http://arxiv.org/abs/1605.06276 | id:1605.06276 author:A. N. Gorban, E. M. Mirkes, A. Zinovyev category:cs.LG stat.ML  published:2016-05-20 summary:Most of machine learning approaches have stemmed from the application of minimizing the mean squared distance principle, based on the computationally efficient quadratic optimization methods. However, when faced with high-dimensional and noisy data, the quadratic error functionals demonstrate many weaknesses including high sensitivity to contaminating factors and dimensionality curse. Therefore, a lot of recent applications in machine learning exploited the properties of non-quadratic error functionals based on L1 norm or even sub-linear potentials corresponding to fractional norms. The back side of these approaches is tremendous increase in computational cost for optimization. Till so far, no approaches have been suggested to deal with {\it arbitrary} error functionals, in a flexible and computationally efficient framework. In this paper, we develop the theory and basic universal data approximation algorithms ($k$-means, principal components, principal manifolds and graphs), based on piece-wise quadratic error potentials of subquadratic growth (PQSQ potentials). We develop a new and universal framework to minimize {\it arbitrary sub-quadratic error potentials} using an algorithm with guaranteed fast convergence to the local or global error minimum. The approach can be applied in most of existing machine learning methods, including methods of data approximation and regularized regression, leading to the improvement in the computational cost/accuracy trade-off. version:1
arxiv-1605-06265 | End-to-End Kernel Learning with Supervised Convolutional Kernel Networks | http://arxiv.org/abs/1605.06265 | id:1605.06265 author:Julien Mairal category:stat.ML cs.CV cs.LG  published:2016-05-20 summary:In this paper, we propose a new image representation based on a multilayer kernel machine that performs end-to-end learning. Unlike traditional kernel methods, where the kernel is handcrafted or adapted to data in an unsupervised manner, we learn how to shape the kernel for a supervised prediction problem. We proceed by generalizing convolutional kernel networks, which originally provide unsupervised image representations, and we derive backpropagation rules to optimize model parameters. As a result, we obtain a new type of convolutional neural network with the following properties: (i) at each layer, learning filters is equivalent to optimizing a linear subspace in a reproducing kernel Hilbert space (RKHS), where we project data, (ii) the network may be learned with supervision or without, (iii) the model comes with a natural regularization function (the norm in the RKHS). We show that our method achieves reasonably competitive performance on some standard "deep learning" image classification datasets such as CIFAR-10 and SVHN, and also state-of-the-art results for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks. version:1
arxiv-1602-02311 | Rényi Divergence Variational Inference | http://arxiv.org/abs/1602.02311 | id:1602.02311 author:Yingzhen Li, Richard E. Turner category:stat.ML cs.LG  published:2016-02-06 summary:This paper introduces the variational R\'enyi bound (VR) that extends traditional variational inference to R\'enyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log marginal likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound. version:2
arxiv-1605-06240 | FPNN: Field Probing Neural Networks for 3D Data | http://arxiv.org/abs/1605.06240 | id:1605.06240 author:Yangyan Li, Soeren Pirk, Hao Su, Charles R. Qi, Leonidas J. Guibas category:cs.CV I.5.1  I.2.10  published:2016-05-20 summary:Building discriminative representations for 3D data has been an important task in computer graphics and computer vision research. Convolutional Neural Networks (CNNs) have shown to operate on 2D images with great success for a variety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like a plausible and promising next step. Unfortunately, the computational complexity of 3D CNNs grows cubically with respect to voxel resolution. Moreover, since most 3D geometry representations are boundary based, occupied regions do not increase proportionately with the size of the discretization, resulting in wasted computation. In this work, we represent 3D spaces as volumetric fields, and propose a novel design that employs field probing filters to efficiently extract features from them. Each field probing filter is a set of probing points --- sensors that perceive the space. Our learning algorithm optimizes not only the weights associated with the probing points, but also their locations, which deforms the shape of the probing filters and adaptively distributes them in 3D space. The optimized probing points sense the 3D space "intelligently", rather than operating blindly over the entire domain. We show that field probing is significantly more efficient than 3DCNNs, while providing state-of-the-art performance, on classification tasks for 3D object recognition benchmark datasets. version:1
arxiv-1406-5675 | SPSD Matrix Approximation vis Column Selection: Theories, Algorithms, and Extensions | http://arxiv.org/abs/1406.5675 | id:1406.5675 author:Shusen Wang, Luo Luo, Zhihua Zhang category:cs.LG  published:2014-06-22 summary:Symmetric positive semidefinite (SPSD) matrix approximation is an important problem with applications in kernel methods. However, existing SPSD matrix approximation methods such as the Nystr\"om method only have weak error bounds. In this paper we conduct in-depth studies of an SPSD matrix approximation model and establish strong relative-error bounds. We call it the prototype model for it has more efficient and effective extensions, and some of its extensions have high scalability. Though the prototype model itself is not suitable for large-scale data, it is still useful to study its properties, on which the analysis of its extensions relies. This paper offers novel theoretical analysis, efficient algorithms, and a highly accurate extension. First, we establish a lower error bound for the prototype model and improve the error bound of an existing column selection algorithm to match the lower bound. In this way, we obtain the first optimal column selection algorithm for the prototype model. We also prove that the prototype model is exact under certain conditions. Second, we develop a simple column selection algorithm with a provable error bound. Third, we propose a so-called spectral shifting model to make the approximation more accurate when the eigenvalues of the matrix decay slowly, and the improvement is theoretically quantified. The spectral shifting method can also be applied to improve other SPSD matrix approximation models. version:6
arxiv-1505-03540 | Brain Tumor Segmentation with Deep Neural Networks | http://arxiv.org/abs/1505.03540 | id:1505.03540 author:Mohammad Havaei, Axel Davy, David Warde-Farley, Antoine Biard, Aaron Courville, Yoshua Bengio, Chris Pal, Pierre-Marc Jodoin, Hugo Larochelle category:cs.CV cs.AI  published:2015-05-13 summary:In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test dataset reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster. version:3
arxiv-1605-06220 | Convergence of Contrastive Divergence with Annealed Learning Rate in Exponential Family | http://arxiv.org/abs/1605.06220 | id:1605.06220 author:Bai Jiang, Tung-yu Wu, Wing H. Wong category:stat.ML cs.LG  published:2016-05-20 summary:In our recent paper, we showed that in exponential family, contrastive divergence (CD) with fixed learning rate will give asymptotically consistent estimates \cite{wu2016convergence}. In this paper, we establish consistency and convergence rate of CD with annealed learning rate $\eta_t$. Specifically, suppose CD-$m$ generates the sequence of parameters $\{\theta_t\}_{t \ge 0}$ using an i.i.d. data sample $\mathbf{X}_1^n \sim p_{\theta^*}$ of size $n$, then $\delta_n(\mathbf{X}_1^n) = \limsup_{t \to \infty} \Vert \sum_{s=t_0}^t \eta_s \theta_s / \sum_{s=t_0}^t \eta_s - \theta^* \Vert$ converges in probability to 0 at a rate of $1/\sqrt[3]{n}$. The number ($m$) of MCMC transitions in CD only affects the coefficient factor of convergence rate. Our proof is not a simple extension of the one in \cite{wu2016convergence}. which depends critically on the fact that $\{\theta_t\}_{t \ge 0}$ is a homogeneous Markov chain conditional on the observed sample $\mathbf{X}_1^n$. Under annealed learning rate, the homogeneous Markov property is not available and we have to develop an alternative approach based on super-martingales. Experiment results of CD on a fully-visible $2\times 2$ Boltzmann Machine are provided to demonstrate our theoretical results. version:1
arxiv-1605-06215 | TRIM: Triangulating Images for Efficient Registration | http://arxiv.org/abs/1605.06215 | id:1605.06215 author:Chun Pang Yung, Gary Pui-Tung Choi, Ke Chen, Lok Ming Lui category:cs.GR cs.CG cs.CV  published:2016-05-20 summary:With the advancement in the digital camera technology, the use of high resolution images and videos has been widespread in the modern society. In particular, image and video frame registration is frequently applied in computer graphics and film production. However, the conventional registration approaches usually require long computational time for high quality images and video frames. This hinders the applications of the registration approaches in the modern industries. In this work, we propose a novel approach called {\em TRIM} to accelerate the computations of the registration by triangulating the images. More specifically, given a high resolution image or video frame, we compute an optimal coarse triangulation which captures the important features of the image. Then, the computation of the registration can be simplified with the aid of the coarse triangulation. Experimental results suggest that the computational time of the registration is significantly reduced using our triangulation-based approach, meanwhile the accuracy of the registration is well retained when compared with the conventional grid-based approach. version:1
arxiv-1605-03639 | Facial Expression Recognition from World Wild Web | http://arxiv.org/abs/1605.03639 | id:1605.03639 author:Ali Mollahosseini, Behzad Hassani, Michelle J. Salvador, Hojjat Abdollahi, David Chan, Mohammad H. Mahoor category:cs.CV cs.NE  published:2016-05-11 summary:Recognizing facial expression in a wild setting has remained a challenging task in computer vision. The World Wide Web is a good source of facial images which most of them are captured in uncontrolled conditions. In fact, the Internet is a Word Wild Web of facial images with expressions. This paper presents the results of a new study on collecting, annotating, and analyzing wild facial expressions from the web. Three search engines were queried using 1250 emotion related keywords in six different languages and the retrieved images were mapped by two annotators to six basic expressions and neutral. Deep neural networks and noise modeling were used in three different training scenarios to find how accurately facial expressions can be recognized when trained on noisy images collected from the web using query terms (e.g. happy face, laughing man, etc)? The results of our experiments show that deep neural networks can recognize wild facial expressions with an accuracy of 82.12%. version:2
arxiv-1605-06211 | Fully Convolutional Networks for Semantic Segmentation | http://arxiv.org/abs/1605.06211 | id:1605.06211 author:Evan Shelhamer, Jonathan Long, Trevor Darrell category:cs.CV  published:2016-05-20 summary:Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image. version:1
arxiv-1604-01870 | Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis | http://arxiv.org/abs/1604.01870 | id:1604.01870 author:Weiran Wang, Jialei Wang, Dan Garber, Nathan Srebro category:cs.LG  published:2016-04-07 summary:We study the stochastic optimization of canonical correlation analysis (CCA), whose objective is nonconvex and does not decouple over training samples. Although several stochastic gradient based optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them. Inspired by the alternating least squares/power iterations formulation of CCA, and the shift-and-invert preconditioning method for PCA, we propose two globally convergent meta-algorithms for CCA, both of which transform the original problem into sequences of least squares problems that need only be solved approximately. We instantiate the meta-algorithms with state-of-the-art SGD methods and obtain time complexities that significantly improve upon that of previous work. Experimental results demonstrate their superior performance. version:3
arxiv-1605-06203 | Faster Projection-free Convex Optimization over the Spectrahedron | http://arxiv.org/abs/1605.06203 | id:1605.06203 author:Dan Garber category:math.OC cs.LG  published:2016-05-20 summary:Minimizing a convex function over the spectrahedron, i.e., the set of all positive semidefinite matrices with unit trace, is an important optimization task with many applications in optimization, machine learning, and signal processing. It is also notoriously difficult to solve in large-scale since standard techniques require expensive matrix decompositions. An alternative, is the conditional gradient method (aka Frank-Wolfe algorithm) that regained much interest in recent years, mostly due to its application to this specific setting. The key benefit of the CG method is that it avoids expensive matrix decompositions all together, and simply requires a single eigenvector computation per iteration, which is much more efficient. On the downside, the CG method, in general, converges with an inferior rate. The error for minimizing a $\beta$-smooth function after $t$ iterations scales like $\beta/t$. This convergence rate does not improve even if the function is also strongly convex. In this work we present a modification of the CG method tailored for convex optimization over the spectrahedron. The per-iteration complexity of the method is essentially identical to that of the standard CG method: only a single eigenvecor computation is required. For minimizing an $\alpha$-strongly convex and $\beta$-smooth function, the expected approximation error of the method after $t$ iterations is: $$O\left({\min\{\frac{\beta{}}{t} ,\left({\frac{\beta\sqrt{\textrm{rank}(\textbf{X}^*)}}{\alpha^{1/4}t}}\right)^{4/3}, \left({\frac{\beta}{\sqrt{\alpha}\lambda_{\min}(\textbf{X}^*)t}}\right)^{2}\}}\right) ,$$ where $\textbf{X}^*$ is the optimal solution. To the best of our knowledge, this is the first result that attains provably faster convergence rates for a CG variant for optimization over the spectrahedron. We also present encouraging preliminary empirical results. version:1
arxiv-1605-06201 | Adversarial Delays in Online Strongly-Convex Optimization | http://arxiv.org/abs/1605.06201 | id:1605.06201 author:Daniel Khashabi, Kent Quanrud, Amirhossein Taghvaei category:cs.LG cs.AI stat.ML  published:2016-05-20 summary:We consider the problem of strongly-convex online optimization in presence of adversarial delays; in a T-iteration online game, the feedback of the player's query at time t is arbitrarily delayed by an adversary for d_t rounds and delivered before the game ends, at iteration t+d_t-1. Specifically for \algo{online-gradient-descent} algorithm we show it has a simple regret bound of \Oh{\sum_{t=1}^T \log (1+ \frac{d_t}{t})}. This gives a clear and simple bound without resorting any distributional and limiting assumptions on the delays. We further show how this result encompasses and generalizes several of the existing known results in the literature. Specifically it matches the celebrated logarithmic regret \Oh{\log T} when there are no delays (i.e. d_t = 1) and regret bound of \Oh{\tau \log T} for constant delays d_t = \tau. version:1
arxiv-1604-06045 | Dialog-based Language Learning | http://arxiv.org/abs/1604.06045 | id:1604.06045 author:Jason Weston category:cs.CL  published:2016-04-20 summary:A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of (Weston et al., 2015) and large-scale question answering from (Dodge et al., 2015). We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all. version:4
arxiv-1605-06197 | Deep Generative Models with Stick-Breaking Priors | http://arxiv.org/abs/1605.06197 | id:1605.06197 author:Eric Nalisnick, Padhraic Smyth category:stat.ML  published:2016-05-20 summary:Bayesian nonparametric models are attractive for their data-dependent capacity, but their implementation can be problematic due to computational or analytical obstacles. We make progress on this problem by extending Stochastic Gradient Variational Bayes (Kingma & Welling, 2013), a 'black box' method for approximate posterior inference, to stick-breaking priors (Ishwaran & James, 2001). This innovation allows us to define deep generative models (DGMs) with infinite dimensional latent variables. We experimentally demonstrate that DGMs with Dirichlet process priors learn highly discriminative latent representations that are well suited for semi-supervised settings and often outperform the popular Gaussian alternative. version:1
arxiv-1605-05368 | Deep Action Sequence Learning for Causal Shape Transformation | http://arxiv.org/abs/1605.05368 | id:1605.05368 author:Kin Gwn Lore, Daniel Stoecklein, Michael Davies, Baskar Ganapathysubramanian, Soumik Sarkar category:cs.LG cs.CV cs.NE  published:2016-05-17 summary:Deep learning (DL) became the method of choice in recent years for solving problems ranging from object recognition and speech recognition to robotic perception and human disease prediction. In this paper, we present a hybrid architecture of convolutional neural networks (CNN) and stacked autoencoders (SAE) to learn a sequence of actions that nonlinearly transforms an input shape or distribution into a target shape or distribution with the same support. While such a framework can be useful in a variety of problems such as robotic path planning, sequential decision-making in games and identifying material processing pathways to achieve desired microstructures, this paper focuses on controlling fluid deformations in a microfluidic channel by deliberately placing a sequence of pillars, which has a significant impact on manufacturing for biomedical and textile applications where highly targeted shapes are desired. We propose an architecture which simultaneously predicts the intermediate shape lying in the nonlinear transformation pathway between the undeformed and desired flow shape, then learns the causal action--the single pillar which results in the deformation of the flow--one at a time. The learning of stage-wise transformations provides deep insights into the physical flow deformation. Results show that under the current framework, our model is able to predict a sequence of pillars that reconstructs the flow shape which highly resembles the desired shape. version:2
arxiv-1605-05904 | Re-ranking Object Proposals for Object Detection in Automatic Driving | http://arxiv.org/abs/1605.05904 | id:1605.05904 author:Zhun Zhong, Mingyi Lei, Shaozi Li, Jianping Fan category:cs.CV  published:2016-05-19 summary:Object detection often suffers from a plenty of bootless proposals, selecting high quality proposals remains a great challenge. In this paper, we propose a semantic, class-specific approach to re-rank object proposals, which can consistently improve the recall performance even with less proposals. We first extract features for each proposal including semantic segmentation, stereo information, contextual information, CNN-based objectness and low-level cue, and then score them using class-specific weights learnt by Structured SVM. The advantages of the proposed model are twofold: 1) it can be easily merged to existing generators with few computational costs, and 2) it can achieve high recall rate uner strict critical even using less proposals. Experimental evaluation on the KITTI benchmark demonstrates that our approach significantly improves existing popular generators on recall performance. Moreover, in the experiment conducted for object detection, even with 1,500 proposals, our approach can still have higher average precision (AP) than baselines with 5,000 proposals. version:2
arxiv-1605-05573 | Modelling Interaction of Sentence Pair with coupled-LSTMs | http://arxiv.org/abs/1605.05573 | id:1605.05573 author:Pengfei Liu, Xipeng Qiu, Xuanjing Huang category:cs.CL  published:2016-05-18 summary:Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state-of-the-art methods. version:2
arxiv-1605-06182 | Dimensionality Reduction on SPD Manifolds: The Emergence of Geometry-Aware Methods | http://arxiv.org/abs/1605.06182 | id:1605.06182 author:Mehrtash Harandi, Mathieu Salzmann, Richard Hartley category:cs.CV  published:2016-05-20 summary:Representing images and videos with Symmetric Positive Definite (SPD) matrices, and considering the Riemannian geometry of the resulting space, has been shown to yield high discriminative power in many visual recognition tasks. Unfortunately, computation on the Riemannian manifold of SPD matrices -especially of high-dimensional ones- comes at a high cost that limits the applicability of existing techniques. In this paper, we introduce algorithms able to handle high-dimensional SPD matrices by constructing a lower-dimensional SPD manifold. To this end, we propose to model the mapping from the high-dimensional SPD manifold to the low-dimensional one with an orthonormal projection. This lets us formulate dimensionality reduction as the problem of finding a projection that yields a low-dimensional manifold either with maximum discriminative power in the supervised scenario, or with maximum variance of the data in the unsupervised one. We show that learning can be expressed as an optimization problem on a Grassmann manifold and discuss fast solutions for special cases. Our evaluation on several classification tasks evidences that our approach leads to a significant accuracy gain over state-of-the-art methods. version:1
arxiv-1605-06181 | Variational hybridization and transformation for large inaccurate noisy-or networks | http://arxiv.org/abs/1605.06181 | id:1605.06181 author:Yusheng Xie, Nan Du, Wei Fan, Jing Zhai, Weicheng Zhu category:cs.LG cs.AI stat.ML  published:2016-05-20 summary:Variational inference provides approximations to the computationally intractable posterior distribution in Bayesian networks. A prominent medical application of noisy-or Bayesian network is to infer potential diseases given observed symptoms. Previous studies focus on approximating a handful of complicated pathological cases using variational transformation. Our goal is to use variational transformation as part of a novel hybridized inference for serving reliable and real time diagnosis at web scale. We propose a hybridized inference that allows variational parameters to be estimated without disease posteriors or priors, making the inference faster and much of its computation recyclable. In addition, we propose a transformation ranking algorithm that is very stable to large variances in network prior probabilities, a common issue that arises in medical applications of Bayesian networks. In experiments, we perform comparative study on a large real life medical network and scalability study on a much larger (36,000x) synthesized network. version:1
arxiv-1605-06177 | Fine-Grained Classification of Pedestrians in Video: Benchmark and State of the Art | http://arxiv.org/abs/1605.06177 | id:1605.06177 author:David Hall, Pietro Perona category:cs.CV  published:2016-05-20 summary:A video dataset that is designed to study fine-grained categorisation of pedestrians is introduced. Pedestrians were recorded "in-the-wild" from a moving vehicle. Annotations include bounding boxes, tracks, 14 keypoints with occlusion information and the fine-grained categories of age (5 classes), sex (2 classes), weight (3 classes) and clothing style (4 classes). There are a total of 27,454 bounding box and pose labels across 4222 tracks. This dataset is designed to train and test algorithms for fine-grained categorisation of people, it is also useful for benchmarking tracking, detection and pose estimation of pedestrians. State-of-the-art algorithms for fine-grained classification and pose estimation were tested using the dataset and the results are reported as a useful performance baseline. version:1
arxiv-1605-06170 | Evaluation System for a Bayesian Optimization Service | http://arxiv.org/abs/1605.06170 | id:1605.06170 author:Ian Dewancker, Michael McCourt, Scott Clark, Patrick Hayes, Alexandra Johnson, George Ke category:cs.LG  published:2016-05-19 summary:Bayesian optimization is an elegant solution to the hyperparameter optimization problem in machine learning. Building a reliable and robust Bayesian optimization service requires careful testing methodology and sound statistical analysis. In this talk we will outline our development of an evaluation framework to rigorously test and measure the impact of changes to the SigOpt optimization service. We present an overview of our evaluation system and discuss how this framework empowers our research engineers to confidently and quickly make changes to our core optimization engine version:1
arxiv-1409-0473 | Neural Machine Translation by Jointly Learning to Align and Translate | http://arxiv.org/abs/1409.0473 | id:1409.0473 author:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio category:cs.CL cs.LG cs.NE stat.ML  published:2014-09-01 summary:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. version:7
arxiv-1605-06155 | Inter-Battery Topic Representation Learning | http://arxiv.org/abs/1605.06155 | id:1605.06155 author:Cheng Zhang, Hedvig Kjellstrom, Carl Henrik Ek category:cs.LG cs.CV  published:2016-05-19 summary:In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach extends traditional topic models by learning a factorized latent variable representation. The structured representation leads to a model that marries benefits traditionally associated with a discriminative approach, such as feature selection, with those of a generative model, such as principled regularization and ability to handle missing data. The factorization is provided by representing data in terms of aligned pairs of observations as different views. This provides means for selecting a representation that separately models topics that exist in both views from the topics that are unique to a single view. This structured consolidation allows for efficient and robust inference and provides a compact and efficient representation. Learning is performed in a Bayesian fashion by maximizing a rigorous bound on the log-likelihood. Firstly, we illustrate the benefits of the model on a synthetic dataset,. The model is then evaluated in both uni- and multi-modality settings on two different classification tasks with off-the-shelf convolutional neural network (CNN) features which generate state-of-the-art results with extremely compact representations. version:1
arxiv-1602-07377 | How Deep Neural Networks Can Improve Emotion Recognition on Video Data | http://arxiv.org/abs/1602.07377 | id:1602.07377 author:Pooya Khorrami, Tom Le Paine, Kevin Brady, Charlie Dagli, Thomas S. Huang category:cs.CV  published:2016-02-24 summary:We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods. version:2
arxiv-1605-06106 | Development of a 3D tongue motion visualization platform based on ultrasound image sequences | http://arxiv.org/abs/1605.06106 | id:1605.06106 author:Kele Xu, Yin Yang, Aurore Jaumard-Hakoun, Clemence Leboullenger, Gerard Dreyfus, Pierre Roussel, Maureen Stone, Bruce Denby category:cs.CV  published:2016-05-19 summary:This article describes the development of a platform designed to visualize the 3D motion of the tongue using ultrasound image sequences. An overview of the system design is given and promising results are presented. Compared to the analysis of motion in 2D image sequences, such a system can provide additional visual information and a quantitative description of the tongue 3D motion. The platform can be useful in a variety of fields, such as speech production, articulation training, etc. version:1
arxiv-1605-06094 | Automatic Selection of the Optimal Local Feature Detector | http://arxiv.org/abs/1605.06094 | id:1605.06094 author:Bruno Ferrarini, Shoaib Ehsan, Naveed Ur Rehman, Ales Leonardis, Klaus D. McDonald-Maier category:cs.CV  published:2016-05-19 summary:A large number of different feature detectors has been proposed so far. Any existing approach presents strengths and weaknesses, which make a detector optimal only for a limited range of applications. A tool capable of selecting the optimal feature detector in relation to the operating conditions is presented in this paper. The input images are quickly analyzed to determine what type of image transformation is applied to them and at which amount. Finally, the detector that is expected to obtain the highest repeatability under such conditions, is chosen to extract features from the input images. The efficiency and the good accuracy in determining the optimal feature detector for any operating condition, make the proposed tool suitable to be utilized in real visual applications. %A large number of different feature detectors has been proposed so far. Any existing approach presents strengths and weaknesses, which make a detector optimal only for a limited range of applications. A large number of different local feature detectors have been proposed in the last few years. However, each feature detector has its own strengths ad weaknesses that limit its use to a specific range of applications. In this paper is presented a tool capable of quickly analysing input images to determine which type and amount of transformation is applied to them and then selecting the optimal feature detector, which is expected to perform the best. The results show that the performance and the fast execution time render the proposed tool suitable for real-world vision applications. version:1
arxiv-1603-03116 | Low-rank passthrough neural networks | http://arxiv.org/abs/1603.03116 | id:1603.03116 author:Antonio Valerio Miceli Barone category:cs.LG cs.NE  published:2016-03-10 summary:Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. Effective learning in this setting is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem of naive deep networks. Many of these architectures, such as LSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single structural principle: the state passthrough. We observe that these architectures, hereby characterized as Passthrough Networks, in addition to the mitigation of the vanishing gradient problem, enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on synthetic tasks and a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data. version:2
arxiv-1605-06083 | Stereotyping and Bias in the Flickr30K Dataset | http://arxiv.org/abs/1605.06083 | id:1605.06083 author:Emiel van Miltenburg category:cs.CL cs.CV  published:2016-05-19 summary:An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they "focus only on the information that can be obtained from the image alone" (Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications. version:1
arxiv-1605-06076 | On a convergent off -policy temporal difference learning algorithm in on-line learning environment | http://arxiv.org/abs/1605.06076 | id:1605.06076 author:Prasenjit Karmakar, Rajkumar Maity, Shalabh Bhatnagar category:cs.LG  published:2016-05-19 summary:In this paper we provide a rigorous convergence analysis of a "off"-policy temporal difference learning algorithm with linear function approximation and per time-step linear computational complexity in "online" learning environment. The algorithm considered here is TDC with importance weighting introduced by Maei et al. We support our theoretical results by providing suitable empirical results for standard off-policy counterexamples. version:1
arxiv-1512-02743 | Perfect Recovery Conditions For Non-Negative Sparse Modeling | http://arxiv.org/abs/1512.02743 | id:1512.02743 author:Yuki Itoh, Marco F. Duarte, Mario Parente category:cs.IT cs.LG math.IT  published:2015-12-09 summary:Sparse modeling has been widely and successfully used in many applications such as computer vision, machine learning, and pattern recognition and, accompanied with those applications, significant research has studied the theoretical limits and algorithm design for convex relaxations in sparse modeling. However, only little has been done for theoretical limits of non-negative versions of sparse modeling. The behavior is expected to be similar as the general sparse modeling, but a precise analysis has not been explored. This paper studies the performance of non-negative sparse modeling, especially for non-negativity constrained and $\ell_1$-penalized least squares, and gives an exact bound for which this problem can recover the correct signal elements. We pose two conditions to guarantee the correct signal recovery: minimum coefficient condition (MCC) and non-linearity vs. subset coherence condition (NSCC). The former defines the minimum weight for each of the correct atoms present in the signal and the latter defines the tolerable deviation from the linear model relative to the positive subset coherence (PSC), a novel type of "coherence" metric. We provide rigorous performance guarantees based on these conditions and experimentally verify their precise predictive power in a hyperspectral data unmixing application. version:2
arxiv-1605-06065 | One-shot Learning with Memory-Augmented Neural Networks | http://arxiv.org/abs/1605.06065 | id:1605.06065 author:Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap category:cs.LG  published:2016-05-19 summary:Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms. version:1
arxiv-1605-06052 | Hierarchical Clustering in Face Similarity Score Space | http://arxiv.org/abs/1605.06052 | id:1605.06052 author:Jason Grant, Patrick Flynn category:cs.CV  published:2016-05-19 summary:Similarity scores in face recognition represent the proximity between pairs of images as computed by a matching algorithm. Given a large set of images and the proximities between all pairs, a similarity score space is defined. Cluster analysis was applied to the similarity score space to develop various taxonomies. Given the number of subjects in the dataset, we used hierarchical methods to aggregate images of the same subject. We also explored the hierarchy above and below the subject level, including clusters that reflect gender and ethnicity. Evidence supports the existence of clustering by race, gender, subject, and illumination condition. version:1
arxiv-1604-03584 | Asynchronous Stochastic Gradient Descent with Variance Reduction for Non-Convex Optimization | http://arxiv.org/abs/1604.03584 | id:1604.03584 author:Zhouyuan Huo, Heng Huang category:cs.LG math.OC  published:2016-04-12 summary:We provide the first theoretical analysis on the convergence rate of the asynchronous stochastic variance reduced gradient (SVRG) descent algorithm on non-convex optimization. Recent studies have shown that the asynchronous stochastic gradient descent (SGD) based algorithms with variance reduction converge with a linear convergent rate on convex problems. However, there is no work to analyze asynchronous SGD with variance reduction technique on non-convex problem. In this paper, we study two asynchronous parallel implementations of SVRG: one is on a distributed memory system and the other is on a shared memory system. We provide the theoretical analysis that both algorithms can obtain a convergence rate of $O(1/T)$, and linear speed up is achievable if the number of workers is upper bounded. version:3
arxiv-1605-06049 | A Multi-Batch L-BFGS Method for Machine Learning | http://arxiv.org/abs/1605.06049 | id:1605.06049 author:Albert S. Berahas, Jorge Nocedal, Martin Takáč category:math.OC cs.LG stat.ML  published:2016-05-19 summary:The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This inherently gives the algorithm a stochastic flavor that can cause instability in L-BFGS, a popular batch method in machine learning. These difficulties arise because L-BFGS employs gradient differences to update the Hessian approximations; when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases. version:1
arxiv-1605-06047 | AMSOM: Adaptive Moving Self-organizing Map for Clustering and Visualization | http://arxiv.org/abs/1605.06047 | id:1605.06047 author:Gerasimos Spanakis, Gerhard Weiss category:cs.AI cs.NE  published:2016-05-19 summary:Self-Organizing Map (SOM) is a neural network model which is used to obtain a topology-preserving mapping from the (usually high dimensional) input/feature space to an output/map space of fewer dimensions (usually two or three in order to facilitate visualization). Neurons in the output space are connected with each other but this structure remains fixed throughout training and learning is achieved through the updating of neuron reference vectors in feature space. Despite the fact that growing variants of SOM overcome the fixed structure limitation they increase computational cost and also do not allow the removal of a neuron after its introduction. In this paper, a variant of SOM is proposed called AMSOM (Adaptive Moving Self-Organizing Map) that on the one hand creates a more flexible structure where neuron positions are dynamically altered during training and on the other hand tackles the drawback of having a predefined grid by allowing neuron addition and/or removal during training. Experiments using multiple literature datasets show that the proposed method improves training performance of SOM, leads to a better visualization of the input dataset and provides a framework for determining the optimal number and structure of neurons. version:1
arxiv-1604-05529 | Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss | http://arxiv.org/abs/1604.05529 | id:1604.05529 author:Barbara Plank, Anders Søgaard, Yoav Goldberg category:cs.CL  published:2016-04-19 summary:Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence modeling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed. version:2
arxiv-1507-08449 | One model, two languages: training bilingual parsers with harmonized treebanks | http://arxiv.org/abs/1507.08449 | id:1507.08449 author:David Vilares, Carlos Gómez-Rodríguez, Miguel A. Alonso category:cs.CL  published:2015-07-30 summary:We introduce an approach to train lexicalized parsers using bilingual corpora obtained by merging harmonized treebanks of different languages, producing parsers that can analyze sentences in either of the learned languages, or even sentences that mix both. We test the approach on the Universal Dependency Treebanks, training with MaltParser and MaltOptimizer. The results show that these bilingual parsers are more than competitive, as most combinations not only preserve accuracy, but some even achieve significant improvements over the corresponding monolingual parsers. Preliminary experiments also show the approach to be promising on texts with code-switching and when more languages are added. version:2
arxiv-1605-05977 | A Geometric Approach to Color Image Regularization | http://arxiv.org/abs/1605.05977 | id:1605.05977 author:Freddie Åström, Christoph Schnörr category:cs.CV  published:2016-05-19 summary:We present a new vectorial total variation method that addresses the problem of color consistent image filtering. Our approach is inspired from the double-opponent cell representation in the human visual cortex. Existing methods of vectorial total variation regularizers have insufficient (or no) coupling between the color channels and thus may introduce color artifacts. We address this problem by introducing a novel coupling between the color channels related to a pullback-metric from the opponent space to the data (RGB color) space. Our energy is a non-convex, non-smooth higher-order vectorial total variation approach and promotes color consistent image filtering via a coupling term. For a convex variant, we show well-posedness and existence of a solution in the space of vectorial bounded variation. For the higher-order scheme we employ a half-quadratic strategy, which model the non-convex energy terms as the infimum of a sequence of quadratic functions. In experiments, we elaborate on traditional image restoration applications of inpainting, deblurring and denoising. Regarding the latter, we demonstrate state of the art restoration quality with respect to structure coherence and color consistency. version:1
arxiv-1602-03032 | Associative Long Short-Term Memory | http://arxiv.org/abs/1602.03032 | id:1602.03032 author:Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, Alex Graves category:cs.NE  published:2016-02-09 summary:We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks. version:2
arxiv-1605-05967 | Contour-based 3d tongue motion visualization using ultrasound image sequences | http://arxiv.org/abs/1605.05967 | id:1605.05967 author:Kele Xu, Yin Yang, Clémence Leboullenger, Pierre Roussel, Bruce Denby category:cs.CV  published:2016-05-19 summary:This article describes a contour-based 3D tongue deformation visualization framework using B-mode ultrasound image sequences. A robust, automatic tracking algorithm characterizes tongue motion via a contour, which is then used to drive a generic 3D Finite Element Model (FEM). A novel contour-based 3D dynamic modeling method is presented. Modal reduction and modal warping techniques are applied to model the deformation of the tongue physically and efficiently. This work can be helpful in a variety of fields, such as speech production, silent speech recognition, articulation training, speech disorder study, etc. version:1
arxiv-1603-00810 | Character-based Neural Machine Translation | http://arxiv.org/abs/1603.00810 | id:1603.00810 author:Marta R. Costa-Jussà, José A. R. Fonollosa category:cs.CL cs.LG cs.NE stat.ML  published:2016-03-02 summary:Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task. version:2
arxiv-1604-02532 | T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos | http://arxiv.org/abs/1604.02532 | id:1604.02532 author:Kai Kang, Hongsheng Li, Junjie Yan, Xingyu Zeng, Bin Yang, Tong Xiao, Cong Zhang, Zhe Wang, Ruohui Wang, Xiaogang Wang, Wanli Ouyang category:cs.CV  published:2016-04-09 summary:The state-of-the-art performance for object detection has been significantly improved over the past two years. Besides the introduction of powerful deep neural networks such as GoogleNet and VGG, novel object detection frameworks such as R-CNN and its successors, Fast R-CNN and Faster R-CNN, play an essential role in improving the state-of-the-art. Despite their effectiveness on still images, those frameworks are not specifically designed for object detection from videos. Temporal and contextual information of videos are not fully investigated and utilized. In this work, we propose a deep learning framework that incorporates temporal and contextual information from tubelets obtained in videos, which dramatically improves the baseline performance of existing still-image detection frameworks when they are applied to videos. It is called T-CNN, i.e. tubelets with convolutional neueral networks. The proposed framework won the recently introduced object-detection-from-video (VID) task with provided data in the ImageNet Large-Scale Visual Recognition Challenge 2015 (ILSVRC2015). version:2
arxiv-1605-05937 | Hierarchical Piecewise-Constant Super-regions | http://arxiv.org/abs/1605.05937 | id:1605.05937 author:Imanol Luengo, Mark Basham, Andrew P. French category:cs.CV  published:2016-05-19 summary:Recent applications in computer vision have come to heavily rely on superpixel over-segmentation as a pre-processing step for higher level vision tasks, such as object recognition, image labelling or image segmentation. Here we present a new superpixel algorithm called Hierarchical Piecewise-Constant Super-regions (HPCS), which not only obtains superpixels comparable to the state-of-the-art, but can also be applied hierarchically to form what we call n-th order super-regions. In essence, a Markov Random Field (MRF)-based anisotropic denoising formulation over the quantized feature space is adopted to form piecewise-constant image regions, which are then combined with a graph-based split & merge post-processing step to form superpixels. The graph and quantized feature based formulation of the problem allows us to generalize it hierarchically to preserve boundary adherence with fewer superpixels. Experimental results show that, despite the simplicity of our framework, it is able to provide high quality superpixels, and to hierarchically apply them to form layers of over-segmentation, each with a decreasing number of superpixels, while maintaining the same desired properties (such as adherence to strong image edges). The algorithm is also memory efficient and has a low computational cost. version:1
arxiv-1605-05923 | Matching Handwritten Document Images | http://arxiv.org/abs/1605.05923 | id:1605.05923 author:Praveen Krishnan, C. V. Jawahar category:cs.CV  published:2016-05-19 summary:We address the problem of predicting similarity between a pair of handwritten document images written by different individuals. This has applications related to matching and mining in image collections containing handwritten content. A similarity score is computed by detecting patterns of text re-usages between document images irrespective of the minor variations in word morphology, word ordering, layout and paraphrasing of the content. Our method does not depend on an accurate segmentation of words and lines. We formulate the document matching problem as a structured comparison of the word distributions across two document images. To match two word images, we propose a convolutional neural network (CNN) based feature descriptor. Performance of this representation surpasses the state-of-the-art on handwritten word spotting. Finally, we demonstrate the applicability of our method on a practical problem of matching handwritten assignments. version:1
arxiv-1605-05918 | Bayesian Variable Selection for Globally Sparse Probabilistic PCA | http://arxiv.org/abs/1605.05918 | id:1605.05918 author:Charles Bouveyron, Pierre Latouche, Pierre-Alexandre Mattei category:stat.ML  published:2016-05-19 summary:With the flourishing development of high-dimensional data, sparse versions of principal component analysis (PCA) have imposed themselves as simple, yet powerful ways of selecting relevant features in an unsupervised manner. However, when several sparse principal components are computed, the interpretation of the selected variables may be difficult since each axis has its own sparsity pattern and has to be interpreted separately. To overcome this drawback, we propose a Bayesian procedure that allows to obtain several sparse components with the same sparsity pattern. This allows the practitioner to identify the original variables which are relevant to describe the data. To this end, using Roweis' probabilistic interpretation of PCA and an isotropic Gaussian prior on the loading matrix, we provide the first exact computation of the marginal likelihood of a Bayesian PCA model. In order to avoid the drawbacks of discrete model selection, we propose a simple relaxation of our framework which allows to find a path of models using a variational expectation-maximization algorithm. The exact marginal likelihood can eventually be maximized over this path, relying on Occam's razor to select the relevant variables. Since the sparsity pattern is common to all components, we call this approach globally sparse probabilistic PCA (GSPPCA). Its usefulness is illustrated on synthetic data sets and on several real unsupervised feature selection problems. version:1
arxiv-1605-05912 | Tongue contour extraction from ultrasound images based on deep neural network | http://arxiv.org/abs/1605.05912 | id:1605.05912 author:Aurore Jaumard-Hakoun, Kele Xu, Pierre Roussel-Ragot, Gérard Dreyfus, Bruce Denby category:cs.CV  published:2016-05-19 summary:Studying tongue motion during speech using ultrasound is a standard procedure, but automatic ultrasound image labelling remains a challenge, as standard tongue shape extraction methods typically require human intervention. This article presents a method based on deep neural networks to automatically extract tongue contour from ultrasound images on a speech dataset. We use a deep autoencoder trained to learn the relationship between an image and its related contour, so that the model is able to automatically reconstruct contours from the ultrasound image alone. In this paper, we use an automatic labelling algorithm instead of time-consuming hand-labelling during the training process, and estimate the performances of both automatic labelling and contour extraction as compared to hand-labelling. Observed results show quality scores comparable to the state of the art. version:1
arxiv-1605-05906 | Automatic TM Cleaning through MT and POS Tagging: Autodesk's Submission to the NLP4TM 2016 Shared Task | http://arxiv.org/abs/1605.05906 | id:1605.05906 author:Alena Zwahlen, Olivier Carnal, Samuel Läubli category:cs.CL  published:2016-05-19 summary:We describe a machine learning based method to identify incorrect entries in translation memories. It extends previous work by Barbu (2015) through incorporating recall-based machine translation and part-of-speech-tagging features. Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English-Italian and English-Spanish. version:1
arxiv-1603-08776 | COCO: The Experimental Procedure | http://arxiv.org/abs/1603.08776 | id:1603.08776 author:Nikolaus Hansen, Tea Tusar, Olaf Mersmann, Anne Auger, Dimo Brockhoff category:cs.AI cs.NE  published:2016-03-29 summary:We present a budget-free experimental setup and procedure for benchmarking numericaloptimization algorithms in a black-box scenario. This procedure can be applied with the COCO benchmarking platform. We describe initialization of and input to the algorithm and touch upon therelevance of termination and restarts. version:2
arxiv-1412-4056 | Blind system identification using kernel-based methods | http://arxiv.org/abs/1412.4056 | id:1412.4056 author:Giulio Bottegal, Riccardo S. Risuleo, Håkan Hjalmarsson category:cs.SY stat.ML  published:2014-12-12 summary:We propose a new method for blind system identification. Resorting to a Gaussian regression framework, we model the impulse response of the unknown linear system as a realization of a Gaussian process. The structure of the covariance matrix (or kernel) of such a process is given by the stable spline kernel, which has been recently introduced for system identification purposes and depends on an unknown hyperparameter. We assume that the input can be linearly described by few parameters. We estimate these parameters, together with the kernel hyperparameter and the noise variance, using an empirical Bayes approach. The related optimization problem is efficiently solved with a novel iterative scheme based on the Expectation-Maximization method. In particular, we show that each iteration consists of a set of simple update rules. We show, through some numerical experiments, very promising performance of the proposed method. version:2
arxiv-1504-08196 | On the estimation of initial conditions in kernel-based system identification | http://arxiv.org/abs/1504.08196 | id:1504.08196 author:Riccardo Sven Risuleo, Giulio Bottegal, Håkan Hjalmarsson category:cs.SY stat.ML  published:2015-04-30 summary:Recent developments in system identification have brought attention to regularized kernel-based methods, where, adopting the recently introduced stable spline kernel, prior information on the unknown process is enforced. This reduces the variance of the estimates and thus makes kernel-based methods particularly attractive when few input-output data samples are available. In such cases however, the influence of the system initial conditions may have a significant impact on the output dynamics. In this paper, we specifically address this point. We propose three methods that deal with the estimation of initial conditions using different types of information. The methods consist in various mixed maximum likelihood--a posteriori estimators which estimate the initial conditions and tune the hyperparameters characterizing the stable spline kernel. To solve the related optimization problems, we resort to the expectation-maximization method, showing that the solutions can be attained by iterating among simple update steps. Numerical experiments show the advantages, in terms of accuracy in reconstructing the system impulse response, of the proposed strategies, compared to other kernel-based schemes not accounting for the effect initial conditions. version:3
arxiv-1605-05863 | Siamese Instance Search for Tracking | http://arxiv.org/abs/1605.05863 | id:1605.05863 author:Ran Tao, Efstratios Gavves, Arnold W. M. Smeulders category:cs.CV  published:2016-05-19 summary:In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-the-art tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function. The strength of the matching function comes from being extensively trained generically, i.e., without any data of the target, using a Siamese deep neural network, which we design for tracking. Once learned, the matching function is used as is, without any adapting, to track previously unseen targets. It turns out that the learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-the-art performance. Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot. version:1
arxiv-1605-04569 | Syntactically Guided Neural Machine Translation | http://arxiv.org/abs/1605.04569 | id:1605.04569 author:Felix Stahlberg, Eva Hasler, Aurelien Waite, Bill Byrne category:cs.CL  published:2016-05-15 summary:We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full n-gram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies. version:2
arxiv-1605-01141 | Texture Synthesis Through Convolutional Neural Networks and Spectrum Constraints | http://arxiv.org/abs/1605.01141 | id:1605.01141 author:Gang Liu, Yann Gousseau, Gui-Song Xia category:cs.CV  published:2016-05-04 summary:This paper presents a significant improvement for the synthesis of texture images using convolutional neural networks (CNNs), making use of constraints on the Fourier spectrum of the results. More precisely, the texture synthesis is regarded as a constrained optimization problem, with constraints conditioning both the Fourier spectrum and statistical features learned by CNNs. In contrast with existing methods, the presented method inherits from previous CNN approaches the ability to depict local structures and fine scale details, and at the same time yields coherent large scale structures, even in the case of quasi-periodic images. This is done at no extra computational cost. Synthesis experiments on various images show a clear improvement compared to a recent state-of-the art method relying on CNN constraints only. version:3
arxiv-1605-05829 | On the Sampling Strategy for Evaluation of Spectral-spatial Methods in Hyperspectral Image Classification | http://arxiv.org/abs/1605.05829 | id:1605.05829 author:Jie Liang, Jun Zhou, Yuntao Qian, Lian Wen, Xiao Bai, Yongsheng Gao category:cs.CV  published:2016-05-19 summary:Spectral-spatial processing has been increasingly explored in remote sensing hyperspectral image classification. While extensive studies have focused on developing methods to improve the classification accuracy, experimental setting and design for method evaluation have drawn little attention. In the scope of supervised classification, we find that traditional experimental designs for spectral processing are often improperly used in the spectral-spatial processing context, leading to unfair or biased performance evaluation. This is especially the case when training and testing samples are randomly drawn from the same image - a practice that has been commonly adopted in the experiments. Under such setting, the dependence caused by overlap between the training and testing samples may be artificially enhanced by some spatial information processing methods such as spatial filtering and morphological operation. Such interaction between training and testing sets has violated data independence assumption that is abided by supervised learning theory and performance evaluation mechanism. Therefore, the widely adopted pixel-based random sampling strategy is not always suitable to evaluate spectral-spatial classification algorithms because it is difficult to determine whether the improvement of classification accuracy is caused by incorporating spatial information into classifier or by increasing the overlap between training and testing samples. To partially solve this problem, we propose a novel controlled random sampling strategy for spectral-spatial methods. It can greatly reduce the overlap between training and testing samples and provides more objective and accurate evaluation. version:1
arxiv-1605-05826 | Declarative Machine Learning - A Classification of Basic Properties and Types | http://arxiv.org/abs/1605.05826 | id:1605.05826 author:Matthias Boehm, Alexandre V. Evfimievski, Niketan Pansare, Berthold Reinwald category:cs.DB cs.DC cs.LG cs.PL  published:2016-05-19 summary:Declarative machine learning (ML) aims at the high-level specification of ML tasks or algorithms, and automatic generation of optimized execution plans from these specifications. The fundamental goal is to simplify the usage and/or development of ML algorithms, which is especially important in the context of large-scale computations. However, ML systems at different abstraction levels have emerged over time and accordingly there has been a controversy about the meaning of this general definition of declarative ML. Specification alternatives range from ML algorithms expressed in domain-specific languages (DSLs) with optimization for performance, to ML task (learning problem) specifications with optimization for performance and accuracy. We argue that these different types of declarative ML complement each other as they address different users (data scientists and end users). This paper makes an attempt to create a taxonomy for declarative ML, including a definition of essential basic properties and types of declarative ML. Along the way, we provide insights into implications of these properties. We also use this taxonomy to classify existing systems. Finally, we draw conclusions on defining appropriate benchmarks and specification languages for declarative ML. version:1
arxiv-1601-06683 | Clustering from Sparse Pairwise Measurements | http://arxiv.org/abs/1601.06683 | id:1601.06683 author:Alaa Saade, Marc Lelarge, Florent Krzakala, Lenka Zdeborová category:cs.SI cond-mat.dis-nn cs.LG  published:2016-01-25 summary:We consider the problem of grouping items into clusters based on few random pairwise comparisons between the items. We introduce three closely related algorithms for this task: a belief propagation algorithm approximating the Bayes optimal solution, and two spectral algorithms based on the non-backtracking and Bethe Hessian operators. For the case of two symmetric clusters, we conjecture that these algorithms are asymptotically optimal in that they detect the clusters as soon as it is information theoretically possible to do so. We substantiate this claim for one of the spectral approaches we introduce. version:2
arxiv-1605-05815 | Bacterial foraging optimization based brain magnetic resonance image segmentation | http://arxiv.org/abs/1605.05815 | id:1605.05815 author:Abdul kayom Md Khairuzzaman category:cs.CV cs.NE  published:2016-05-19 summary:Segmentation partitions an image into its constituent parts. It is essentially the pre-processing stage of image analysis and computer vision. In this work, T1 and T2 weighted brain magnetic resonance images are segmented using multilevel thresholding and bacterial foraging optimization (BFO) algorithm. The thresholds are obtained by maximizing the between class variance (multilevel Otsu method) of the image. The BFO algorithm is used to optimize the threshold searching process. The edges are then obtained from the thresholded image by comparing the intensity of each pixel with its eight connected neighbourhood. Post processing is performed to remove spurious responses in the segmented image. The proposed segmentation technique is evaluated using edge detector evaluation parameters such as figure of merit, Rand Index and variation of information. The proposed brain MR image segmentation technique outperforms the traditional edge detectors such as canny and sobel. version:1
arxiv-1605-05799 | Recurrent Exponential-Family Harmoniums without Backprop-Through-Time | http://arxiv.org/abs/1605.05799 | id:1605.05799 author:Joseph G. Makin, Benjamin K. Dichter, Philip N. Sabes category:cs.LG stat.ML  published:2016-05-19 summary:Exponential-family harmoniums (EFHs), which extend restricted Boltzmann machines (RBMs) from Bernoulli random variables to other exponential families (Welling et al., 2005), are generative models that can be trained with unsupervised-learning techniques, like contrastive divergence (Hinton et al. 2006; Hinton, 2002), as density estimators for static data. Methods for extending RBMs--and likewise EFHs--to data with temporal dependencies have been proposed previously (Sutskever and Hinton, 2007; Sutskever et al., 2009), the learning procedure being validated by qualitative assessment of the generative model. Here we propose and justify, from a very different perspective, an alternative training procedure, proving sufficient conditions for optimal inference under that procedure. The resulting algorithm can be learned with only forward passes through the data--backprop-through-time is not required, as in previous approaches. The proof exploits a recent result about information retention in density estimators (Makin and Sabes, 2015), and applies it to a "recurrent EFH" (rEFH) by induction. Finally, we demonstrate optimality by simulation, testing the rEFH: (1) as a filter on training data generated with a linear dynamical system, the position of which is noisily reported by a population of "neurons" with Poisson-distributed spike counts; and (2) with the qualitative experiments proposed by Sutskever et al. (2009). version:1
arxiv-1605-05791 | A Generic Framework for Assessing the Performance Bounds of Image Feature Detectors | http://arxiv.org/abs/1605.05791 | id:1605.05791 author:Shoaib Ehsan, Adrian F. Clark, Ales Leonardis, Naveed ur Rehman, Klaus D. McDonald-Maier category:cs.CV  published:2016-05-19 summary:Since local feature detection has been one of the most active research areas in computer vision during the last decade, a large number of detectors have been proposed. The interest in feature-based applications continues to grow and has thus rendered the task of characterizing the performance of various feature detection methods an important issue in vision research. Inspired by the good practices of electronic system design, a generic framework based on the repeatability measure is presented in this paper that allows assessment of the upper and lower bounds of detector performance and finds statistically significant performance differences between detectors as a function of image transformation amount by introducing a new variant of McNemars test in an effort to design more reliable and effective vision systems. The proposed framework is then employed to establish operating and guarantee regions for several state-of-the-art detectors and to identify their statistical performance differences for three specific image transformations: JPEG compression, uniform light changes and blurring. The results are obtained using a newly acquired, large image database (20482) images with 539 different scenes. These results provide new insights into the behaviour of detectors and are also useful from the vision systems design perspective. version:1
arxiv-1605-05785 | Efficient Nonparametric Smoothness Estimation | http://arxiv.org/abs/1605.05785 | id:1605.05785 author:Shashank Singh, Simon S. Du, Barnabás Póczos category:math.ST cs.IT math.IT stat.ML stat.TH  published:2016-05-19 summary:Sobolev quantities (norms, inner products, and distances) of probability density functions are important in the theory of nonparametric statistics, but have rarely been used in practice, partly due to a lack of practical estimators. They also include, as special cases, $L^2$ quantities which are used in many applications. We propose and analyze a family of estimators for Sobolev quantities of unknown probability density functions. We bound the bias and variance of our estimators over finite samples, finding that they are generally minimax rate-optimal. Our estimators are significantly more computationally tractable than previous estimators, and exhibit a statistical/computational trade-off allowing them to adapt to computational constraints. We also draw theoretical connections to recent work on fast two-sample testing. Finally, we empirically validate our estimators on synthetic data. version:1
arxiv-1605-05782 | A comparison of semi-deterministic and stochastic search techniques | http://arxiv.org/abs/1605.05782 | id:1605.05782 author:Andy M. Connor, Kristina Shea category:cs.NE math.OC  published:2016-05-18 summary:This paper presents an investigation of two search techniques, tabu search (TS) and simulated annealing (SA), to assess their relative merits when applied to engineering design optimisation. Design optimisation problems are generally characterised as having multi-modal search spaces and discontinuities making global optimisation techniques beneficial. Both techniques claim to be capable of locating globally optimum solutions on a range of problems but this capability is derived from different underlying philosophies. While tabu search uses a semi-deterministic approach to escape local optima, simulated annealing uses a complete stochastic approach. The performance of each technique is investigated using a structural optimisation problem. These performances are then compared to each other as and to a steepest descent (SD) method. version:1
arxiv-1507-08184 | Beamforming through regularized inverse problems in ultrasound medical imaging | http://arxiv.org/abs/1507.08184 | id:1507.08184 author:Teodora Szasz, Adrian Basarab, Denis Kouamé category:cs.CV  published:2015-07-29 summary:Beamforming in ultrasound imaging has significant impact on the quality of the final image, controlling its resolution and contrast. Despite its low spatial resolution and contrast, delay-and-sum is still extensively used nowadays in clinical applications, due to its real-time capabilities. The most common alternatives are minimum variance method and its variants, which overcome the drawbacks of delay-and-sum, at the cost of higher computational complexity that limits its utilization in real-time applications. In this paper, we propose to perform beamforming in ultrasound imaging through a regularized inverse problem based on a linear model relating the reflected echoes to the signal to be recovered. Our approach presents two major advantages: i) its flexibility in the choice of statistical assumptions on the signal to be beamformed (Laplacian and Gaussian statistics are tested herein) and ii) its robustness to a reduced number of pulse emissions. The proposed framework is flexible and allows for choosing the right trade-off between noise suppression and sharpness of the resulted image. We illustrate the performance of our approach on both simulated and experimental data, with \textit{in vivo} examples of carotid and thyroid. Compared to delay-and-sum, minimimum variance and two other recently published beamforming techniques, our method offers better spatial resolution, respectively contrast, when using Laplacian and Gaussian priors. version:2
arxiv-1605-05776 | The Quality of the Covariance Selection Through Detection Problem and AUC Bounds | http://arxiv.org/abs/1605.05776 | id:1605.05776 author:Navid Tafaghodi Khajavi, Anthony Kuh category:cs.IT math.IT stat.ML  published:2016-05-18 summary:We consider the problem of quantifying the quality of a model selection problem for a graphical model. We discuss this by formulating the problem as a detection problem. Model selection problems usually minimize the distance with the model distribution. For the special case of Gaussian distributions, this problem simplifies to the covariance selection problem which is widely discussed in literature by Dempster [1] where the Kullback-Leibler (KL) divergence is minimized or equivalently the likelihood criterion maximized to compute the model covariance matrix. While this solution is optimal for Gaussian distributions in the sense of the KL divergence, it is not optimal when compared with other information divergences and criteria such as Area Under the Curve (AUC). In this paper, we discuss the quality of model approximation using the AUC and its bounds as an average measure of accuracy in detection problem. We compute upper and lower bounds for the AUC. We define the correlation approximation matrix (CAM) and show that the KL divergence and AUC and its upper and lower bounds depend on the eigenvalues of the CAM. We also show the relationship between the AUC, the KL divergence and the ROC curve by optimizing with respect to the ROC curve. In the examples provided, we pick tree structures as the simplest graphical models. We perform simulations on fully-connected graphs and compute the tree structured models by applying the widely used Chow-Liu algorithm. Examples show that the quality of tree approximation models are not good in general based on information divergences, AUC and its bounds when the number of nodes in the graphical model is large. Specially for 1-AUC, it is shown both in theory and using simulations that the 1-AUC for the tree approximation model decays exponentially as the dimension of the graphical model increases. version:1
arxiv-1605-05775 | Supervised Learning with Quantum-Inspired Tensor Networks | http://arxiv.org/abs/1605.05775 | id:1605.05775 author:E. Miles Stoudenmire, David J. Schwab category:stat.ML cond-mat.str-el cs.LG  published:2016-05-18 summary:Tensor networks are efficient representations of high-dimensional tensors which have been very successful for physics and mathematics applications. We demonstrate how algorithms for optimizing such networks can be adapted to supervised learning tasks by using matrix product states (tensor trains) to parameterize models for classifying images. For the MNIST data set we obtain less than 1% test set classification error. We discuss how the tensor network form imparts additional structure to the learned model and suggest a possible generative interpretation. version:1
arxiv-1605-05757 | Robust Image Descriptors for Real-Time Inter-Examination Retargeting in Gastrointestinal Endoscopy | http://arxiv.org/abs/1605.05757 | id:1605.05757 author:Menglong Ye, Edward Johns, Benjamin Walter, Alexander Meining, Guang-Zhong Yang category:cs.CV  published:2016-05-18 summary:For early diagnosis of malignancies in the gastrointestinal tract, surveillance endoscopy is increasingly used to monitor abnormal tissue changes in serial examinations of the same patient. Despite successes with optical biopsy for in vivo and in situ tissue characterisation, biopsy retargeting for serial examinations is challenging because tissue may change in appearance between examinations. In this paper, we propose an inter-examination retargeting framework for optical biopsy, based on an image descriptor designed for matching between endoscopic scenes over significant time intervals. Each scene is described by a hierarchy of regional intensity comparisons at various scales, offering tolerance to long-term change in tissue appearance whilst remaining discriminative. Binary coding is then used to compress the descriptor via a novel random forests approach, providing fast comparisons in Hamming space and real-time retargeting. Extensive validation conducted on 13 in vivo gastrointestinal videos, collected from six patients, show that our approach outperforms state-of-the-art methods. version:1
arxiv-1510-05970 | Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches | http://arxiv.org/abs/1510.05970 | id:1510.05970 author:Jure Žbontar, Yann LeCun category:cs.CV cs.LG cs.NE  published:2015-10-20 summary:We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets. version:2
arxiv-1605-05710 | Active Learning On Weighted Graphs Using Adaptive And Non-adaptive Approaches | http://arxiv.org/abs/1605.05710 | id:1605.05710 author:Eyal En Gad, Akshay Gadde, A. Salman Avestimehr, Antonio Ortega category:cs.LG  published:2016-05-18 summary:This paper studies graph-based active learning, where the goal is to reconstruct a binary signal defined on the nodes of a weighted graph, by sampling it on a small subset of the nodes. A new sampling algorithm is proposed, which sequentially selects the graph nodes to be sampled, based on an aggressive search for the boundary of the signal over the graph. The algorithm generalizes a recent method for sampling nodes in unweighted graphs. The generalization improves the sampling performance using the information gained from the available graph weights. An analysis of the number of samples required by the proposed algorithm is provided, and the gain over the unweighted method is further demonstrated in simulations. Additionally, the proposed method is compared with an alternative state of-the-art method, which is based on the graph's spectral properties. It is shown that the proposed method significantly outperforms the spectral sampling method, if the signal needs to be predicted with high accuracy. On the other hand, if a higher level of inaccuracy is tolerable, then the spectral method outperforms the proposed aggressive search method. Consequently, we propose a hybrid method, which is shown to combine the advantages of both approaches. version:1
arxiv-1605-05697 | Online Algorithms For Parameter Mean And Variance Estimation In Dynamic Regression Models | http://arxiv.org/abs/1605.05697 | id:1605.05697 author:Carlos Alberto Gomez-Uribe category:stat.ML  published:2016-05-18 summary:We study the problem of estimating the parameters of a regression model from a set of observations, each consisting of a response and a predictor. The response is assumed to be related to the predictor via a regression model of unknown parameters. Often, in such models the parameters to be estimated are assumed to be constant. Here we consider the more general scenario where the parameters are allowed to evolve over time, a more natural assumption for many applications. We model these dynamics via a linear update equation with additive noise that is often used in a wide range of engineering applications, particularly in the well-known and widely used Kalman filter (where the system state it seeks to estimate maps to the parameter values here). We derive an approximate algorithm to estimate both the mean and the variance of the parameter estimates in an online fashion for a generic regression model. This algorithm turns out to be equivalent to the extended Kalman filter. We specialize our algorithm to the multivariate exponential family distribution to obtain a generalization of the generalized linear model (GLM). Because the common regression models encountered in practice such as logistic, exponential and multinomial all have observations modeled through an exponential family distribution, our results are used to easily obtain algorithms for online mean and variance parameter estimation for all these regression models in the context of time-dependent parameters. Lastly, we propose to use these algorithms in the contextual multi-armed bandit scenario, where so far model parameters are assumed static and observations univariate and Gaussian or Bernoulli. Both of these restrictions can be relaxed using the algorithms described here, which we combine with Thompson sampling to show the resulting performance on a simulation. version:1
arxiv-1605-05652 | Low dimensional manifold model in hyperspectral image reconstruction | http://arxiv.org/abs/1605.05652 | id:1605.05652 author:Zuoqiang Shi, Wei Zhu, Stanley Osher category:cs.CV cs.IT math.IT  published:2016-05-18 summary:We present the application of a low dimensional manifold model (LDMM) on hyperspectral image (HSI) reconstruction. An important property of hyperspectral images is that the patch manifold, which is sampled by the three-dimensional blocks in the data cube, is generally of a low dimensional nature. This is a generalization of low-rank models in that hyperspectral images with nonlinear mixing terms can also fit in this framework. The point integral method (PIM) is used to solve a Laplace-Beltrami equation over a point cloud sampling the patch manifold in LDMM. Both numerical simulations and theoretical analysis show that the sample points constraint is correctly enforced by PIM. The framework is demonstrated by experiments on the reconstruction of both linear and nonlinear mixed hyperspectral images with a significant number of missing voxels and several entirely missing spectral bands. version:1
arxiv-1605-05628 | Detecting Novel Processes with CANDIES -- An Holistic Novelty Detection Technique based on Probabilistic Models | http://arxiv.org/abs/1605.05628 | id:1605.05628 author:Christian Gruhl, Bernhard Sick category:cs.LG  published:2016-05-18 summary:In this article, we propose CANDIES (Combined Approach for Novelty Detection in Intelligent Embedded Systems), a new approach to novelty detection in technical systems. We assume that in a technical system several processes interact. If we observe these processes with sensors, we are able to model the observations (samples) with a probabilistic model, where, in an ideal case, the components of the parametric mixture density model we use, correspond to the processes in the real world. Eventually, at run-time, novel processes emerge in the technical systems such as in the case of an unpredictable failure. As a consequence, new kinds of samples are observed that require an adaptation of the model. CANDIES relies on mixtures of Gaussians which can be used for classification purposes, too. New processes may emerge in regions of the models' input spaces where few samples were observed before (low-density regions) or in regions where already many samples were available (high-density regions). The latter case is more difficult, but most existing solutions focus on the former. Novelty detection in low- and high-density regions requires different detection strategies. With CANDIES, we introduce a new technique to detect novel processes in high-density regions by means of a fast online goodness-of-fit test. For detection in low-density regions we combine this approach with a 2SND (Two-Stage-Novelty-Detector) which we presented in preliminary work. The properties of CANDIES are evaluated using artificial data and benchmark data from the field of intrusion detection in computer networks, where the task is to detect new kinds of attacks. version:1
arxiv-1605-05622 | Gaussian variational approximation with sparse precision matrix | http://arxiv.org/abs/1605.05622 | id:1605.05622 author:Linda S. L. Tan, David J. Nott category:stat.CO stat.ML  published:2016-05-18 summary:We consider the problem of learning a Gaussian variational approximation to the posterior distribution for a high-dimensional parameter, where we impose sparsity in the precision matrix to reflect appropriate conditional independence structure in the model. Incorporating sparsity in the precision matrix allows the Gaussian variational distribution to be both flexible and parsimonious, and the sparsity is achieved through parameterization in terms of the Cholesky factor. Efficient stochastic gradient methods which make appropriate use of gradient information for the target distribution are developed for the optimization. We consider alternative estimators of the stochastic gradients which have lower variation and are more stable. Our approach is illustrated using generalized linear mixed models and state space models for time series. version:1
arxiv-1506-05860 | Variational Gaussian Copula Inference | http://arxiv.org/abs/1506.05860 | id:1506.05860 author:Shaobo Han, Xuejun Liao, David B. Dunson, Lawrence Carin category:stat.ML cs.LG stat.CO  published:2015-06-19 summary:We utilize copulas to constitute a unified framework for constructing and optimizing variational proposals in hierarchical Bayesian models. For models with continuous and non-Gaussian hidden variables, we propose a semiparametric and automated variational Gaussian copula approach, in which the parametric Gaussian copula family is able to preserve multivariate posterior dependence, and the nonparametric transformations based on Bernstein polynomials provide ample flexibility in characterizing the univariate marginal posteriors. version:3
arxiv-1601-05647 | On Structured Sparsity of Phonological Posteriors for Linguistic Parsing | http://arxiv.org/abs/1601.05647 | id:1601.05647 author:Milos Cernak, Afsaneh Asaei, Hervé Bourlard category:cs.CL  published:2016-01-21 summary:The speech signal conveys information on different time scales from short time scale or segmental, associated to phonological and phonetic information to long time scale or supra segmental, associated to syllabic and prosodic information. Linguistic and neurocognitive studies recognize the phonological classes at segmental level as the essential and invariant representations used in speech temporal organization. In the context of speech processing, a deep neural network (DNN) is an effective computational method to infer the probability of individual phonological classes from a short segment of speech signal. A vector of all phonological class probabilities is referred to as phonological posterior. There are only very few classes comprising a short term speech signal; hence, the phonological posterior is a sparse vector. Although the phonological posteriors are estimated at segmental level, we claim that they convey supra-segmental information. Specifically, we demonstrate that phonological posteriors are indicative of syllabic and prosodic events. Building on findings from converging linguistic evidence on the gestural model of Articulatory Phonology as well as the neural basis of speech perception, we hypothesize that phonological posteriors convey properties of linguistic classes at multiple time scales, and this information is embedded in their support (index) of active coefficients. To verify this hypothesis, we obtain a binary representation of phonological posteriors at the segmental level which is referred to as first-order sparsity structure; the high-order structures are obtained by the concatenation of first-order binary vectors. It is then confirmed that the classification of supra-segmental linguistic events, the problem known as linguistic parsing, can be achieved with high accuracy using asimple binary pattern matching of first-order or high-order structures. version:2
arxiv-1605-04809 | The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based SMT | http://arxiv.org/abs/1605.04809 | id:1605.04809 author:Marcin Junczys-Dowmunt, Tomasz Dwojak, Rico Sennrich category:cs.CL  published:2016-05-16 summary:This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task on news translation. We explore methods of decode-time integration of attention-based neural translation models with phrase-based statistical machine translation. Efficient batch-algorithms for GPU-querying are proposed and implemented. For English-Russian, the phrase-based system cannot surpass state-of-the-art pure neural models. For the Russian-English task, our submission achieves the top BLEU result, outperforming the best pure neural system by 1.1 BLEU points and our own phrase-based baseline by 1.6 BLEU. In follow-up experiments we improve these results by additional 0.7 BLEU. version:2
arxiv-1605-05543 | A deep learning approach to single-particle recognition in cryo-electron microscopy | http://arxiv.org/abs/1605.05543 | id:1605.05543 author:Yanan Zhu, Qi Ouyang, Youdong Mao category:physics.data-an cs.CV  published:2016-05-18 summary:Particle extraction represents a major practical bottleneck in the structure determination of biological macromolecular complexes by single-particle cryo-electron microscopy (cryo-EM). We developed a deep learning-based algorithmic framework, DeepEM, for single-particle recognition from noisy cryo-EM micrographs, enabling automated particle picking, selection and verification in an integrated fashion. Our approach exhibits improved performance and high accuracy when tested on the standard KLH dataset as well as several challenging experimental cryo-EM datasets. version:1
arxiv-1605-05538 | Improving Weakly-Supervised Object Localization By Micro-Annotation | http://arxiv.org/abs/1605.05538 | id:1605.05538 author:Alexander Kolesnikov, Christoph H. Lampert category:cs.CV  published:2016-05-18 summary:Weakly-supervised object localization methods tend to fail for object classes that consistently co-occur with the same background elements, e.g. trains on tracks. We propose a method to overcome these failures by adding a very small amount of model-specific additional annotation. The main idea is to cluster a deep network's mid-level representations and assign object or distractor labels to each cluster. Experiments show substantially improved localization results on the challenging ILSVC2014 dataset for bounding box detection and the PASCAL VOC2012 dataset for semantic segmentation. version:1
arxiv-1605-05537 | ABC random forests for Bayesian parameter inference | http://arxiv.org/abs/1605.05537 | id:1605.05537 author:Jean-Michel Marin, Louis Raynal, Pierre Pudlo, Mathieu Ribatet, Christian P. Robert category:stat.ME stat.CO stat.ML  published:2016-05-18 summary:Approximate Bayesian Computation (ABC) has grown into a standard methodology to handle Bayesian inference in models associated with intractable likelihood functions. Most ABC implementations require the selection of a summary statistic as the data itself is too large or too complex to be compared to simulated realisations from the assumed model. The dimension of this statistic is generally constrained to be close to the dimension of the model parameter for efficiency reasons. Furthermore, the tolerance level that governs the acceptance or rejection of parameter values needs to be calibrated and the range of calibration techniques available so far is mostly based on asymptotic arguments. We propose here to conduct Bayesian inference based on an arbitrarily large vector of summary statistics without imposing a selection of the relevant components and bypassing the derivation of a tolerance. The approach relies on the random forest methodology of Breiman (2001) when applied to regression. We advocate the derivation of a new random forest for each component of the parameter vector, a tool from which an approximation to the marginal posterior distribution can be derived. Correlations between parameter components are handled by separate random forests. This technology offers significant gains in terms of robustness to the choice of the summary statistics and of computing time, when compared with more standard ABC solutions. version:1
arxiv-1403-7304 | Characteristic Kernels and Infinitely Divisible Distributions | http://arxiv.org/abs/1403.7304 | id:1403.7304 author:Yu Nishiyama, Kenji Fukumizu category:stat.ML  published:2014-03-28 summary:We connect shift-invariant characteristic kernels to infinitely divisible distributions on $\mathbb{R}^{d}$. Characteristic kernels play an important role in machine learning applications with their kernel means to distinguish any two probability measures. The contribution of this paper is two-fold. First, we show, using the L\'evy-Khintchine formula, that any shift-invariant kernel given by a bounded, continuous and symmetric probability density function (pdf) of an infinitely divisible distribution on $\mathbb{R}^d$ is characteristic. We also present some closure property of such characteristic kernels under addition, pointwise product, and convolution. Second, in developing various kernel mean algorithms, it is fundamental to compute the following values: (i) kernel mean values $m_P(x)$, $x \in \mathcal{X}$, and (ii) kernel mean RKHS inner products ${\left\langle m_P, m_Q \right\rangle _{\mathcal{H}}}$, for probability measures $P, Q$. If $P, Q$, and kernel $k$ are Gaussians, then computation (i) and (ii) results in Gaussian pdfs that is tractable. We generalize this Gaussian combination to more general cases in the class of infinitely divisible distributions. We then introduce a {\it conjugate} kernel and {\it convolution trick}, so that the above (i) and (ii) have the same pdf form, expecting tractable computation at least in some cases. As specific instances, we explore $\alpha$-stable distributions and a rich class of generalized hyperbolic distributions, where the Laplace, Cauchy and Student-t distributions are included. version:2
arxiv-1605-05509 | Learning activation functions from data using cubic spline interpolation | http://arxiv.org/abs/1605.05509 | id:1605.05509 author:Simone Scardapane, Michele Scarpiniti, Danilo Comminiello, Aurelio Uncini category:stat.ML cs.LG cs.NE  published:2016-05-18 summary:Neural networks require a careful design in order to perform properly on a given task. In particular, selecting a good activation function (possibly in a data-dependent fashion) is a crucial step, which remains an open problem in the research community. Despite a large amount of investigations, most current implementations simply select one fixed function from a small set of candidates, which is not adapted during training, and is shared among all neurons throughout the different layers. However, neither two of these assumptions can be supposed optimal in practice. In this paper, we present a principled way to have data-dependent adaptation of the activation functions, which is performed independently for each neuron. This is achieved by leveraging over past and present advances on cubic spline interpolation, allowing for local adaptation of the functions around their regions of use. The resulting algorithm is relatively cheap to implement, and overfitting is counterbalanced by the inclusion of a novel damping criterion, which penalizes unwanted oscillations from a predefined shape. Experimental results validate the proposal over two well-known benchmarks. version:1
arxiv-1504-08190 | A new kernel-based approach for overparameterized Hammerstein system identification | http://arxiv.org/abs/1504.08190 | id:1504.08190 author:Riccardo Sven Risuleo, Giulio Bottegal, Håkan Hjalmarsson category:cs.SY stat.ML  published:2015-04-30 summary:In this paper we propose a new identification scheme for Hammerstein systems, which are dynamic systems consisting of a static nonlinearity and a linear time-invariant dynamic system in cascade. We assume that the nonlinear function can be described as a linear combination of $p$ basis functions. We reconstruct the $p$ coefficients of the nonlinearity together with the first $n$ samples of the impulse response of the linear system by estimating an $np$-dimensional overparameterized vector, which contains all the combinations of the unknown variables. To avoid high variance in these estimates, we adopt a regularized kernel-based approach and, in particular, we introduce a new kernel tailored for Hammerstein system identification. We show that the resulting scheme provides an estimate of the overparameterized vector that can be uniquely decomposed as the combination of an impulse response and $p$ coefficients of the static nonlinearity. We also show, through several numerical experiments, that the proposed method compares very favorably with two standard methods for Hammerstein system identification. version:2
arxiv-1506-05600 | Causality on Cross-Sectional Data: Stable Specification Search in Constrained Structural Equation Modeling | http://arxiv.org/abs/1506.05600 | id:1506.05600 author:Ridho Rahmadi, Perry Groot, Marianne Heins, Hans Knoop, Tom Heskes, The OPTIMISTIC consortium category:stat.ML cs.LG  published:2015-06-18 summary:Causal modeling has long been an attractive topic for many researchers and in recent decades there has seen a surge in theoretical development and discovery algorithms. Generally discovery algorithms can be divided into two approaches: constraint-based and score-based. The constraint-based approach is able to detect common causes of the observed variables but the use of independence tests makes it less reliable. The score-based approach produces a result that is easier to interpret as it also measures the reliability of the inferred causal relationships, but it is unable to detect common confounders of the observed variables. A drawback of both score-based and constrained-based approaches is the inherent instability in structure estimation. With finite samples small changes in the data can lead to completely different optimal structures. The present work introduces a new hypothesis-free score-based causal discovery algorithm that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms. Structure search is performed over Structural Equation Models. Our approach uses exploratory search but allows incorporation of prior background knowledge. We validated our approach on one simulated data set, which we compare to the known the ground truth, and two real-world data sets for Chronic Fatigue Syndrome and Attention Deficit Hyperactivity Disorder, which we compare to earlier medical studies. The result on the simulated data set shows accurate structure estimates and the results on the real-word data sets show consistency with the hypothesis driven models constructed by medical experts. version:2
arxiv-1605-05466 | Image segmentation with superpixel-based covariance descriptors in low-rank representation | http://arxiv.org/abs/1605.05466 | id:1605.05466 author:Xianbin Gu, Jeremiah D. Deng, Martin K. Purvis category:cs.CV  published:2016-05-18 summary:This paper investigates the problem of image segmentation using superpixels. We propose two approaches to enhance the discriminative ability of the superpixel's covariance descriptors. In the first one, we employ the Log-Euclidean distance as the metric on the covariance manifolds, and then use the RBF kernel to measure the similarities between covariance descriptors. The second method is focused on extracting the subspace structure of the set of covariance descriptors by extending a low rank representation algorithm on to the covariance manifolds. Experiments are carried out with the Berkly Segmentation Dataset, and compared with the state-of-the-art segmentation algorithms, both methods are competitive. version:1
arxiv-1605-05462 | Dual Local-Global Contextual Pathways for Recognition in Aerial Imagery | http://arxiv.org/abs/1605.05462 | id:1605.05462 author:Alina Marcu, Marius Leordeanu category:cs.CV  published:2016-05-18 summary:Visual context is important in object recognition and it is still an open problem in computer vision. Along with the advent of deep convolutional neural networks (CNN), using contextual information with such systems starts to receive attention in the literature. At the same time, aerial imagery is gaining momentum. While advances in deep learning make good progress in aerial image analysis, this problem still poses many great challenges. Aerial images are often taken under poor lighting conditions and contain low resolution objects, many times occluded by trees or taller buildings. In this domain, in particular, visual context could be of great help, but there are still very few papers that consider context in aerial image understanding. Here we introduce context as a complementary way of recognizing objects. We propose a dual-stream deep neural network model that processes information along two independent pathways, one for local and another for global visual reasoning. The two are later combined in the final layers of processing. Our model learns to combine local object appearance as well as information from the larger scene at the same time and in a complementary way, such that together they form a powerful classifier. We test our dual-stream network on the task of segmentation of buildings and roads in aerial images and obtain state-of-the-art results on the Massachusetts Buildings Dataset. We also introduce two new datasets, for buildings and road segmentation, respectively, and study the relative importance of local appearance vs. the larger scene, as well as their performance in combination. While our local-global model could also be useful in general recognition tasks, we clearly demonstrate the effectiveness of visual context in conjunction with deep nets for aerial image understanding. version:1
arxiv-1605-05448 | The Bees Algorithm for the Vehicle Routing Problem | http://arxiv.org/abs/1605.05448 | id:1605.05448 author:Aish Fenton category:cs.NE cs.AI  published:2016-05-18 summary:In this thesis we present a new algorithm for the Vehicle Routing Problem called the Enhanced Bees Algorithm. It is adapted from a fairly recent algorithm, the Bees Algorithm, which was developed for continuous optimisation problems. We show that the results obtained by the Enhanced Bees Algorithm are competitive with the best meta-heuristics available for the Vehicle Routing Problem (within 0.5% of the optimal solution for common benchmark problems). We show that the algorithm has good runtime performance, producing results within 2% of the optimal solution within 60 seconds, making it suitable for use within real world dispatch scenarios. version:1
arxiv-1509-08067 | Online Object Tracking, Learning and Parsing with And-Or Graphs | http://arxiv.org/abs/1509.08067 | id:1509.08067 author:Tianfu Wu, Yang Lu, Song-Chun Zhu category:cs.CV cs.LG  published:2015-09-27 summary:This paper presents a method, called AOGTracker, for simultaneously tracking, learning and parsing (TLP) unknown objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. %The AOG captures both structural and appearance variations of a target object in a principled way. The TLP method is formulated in the Bayesian framework with a spatial and a temporal dynamic programming (DP) algorithms inferring object bounding boxes on-the-fly. During online learning, the AOG is discriminatively learned using latent SVM to account for appearance (e.g., lighting and partial occlusion) and structural (e.g., different poses and viewpoints) variations of a tracked object, as well as distractors (e.g., similar objects) in background. Three key issues in online inference and learning are addressed: (i) maintaining purity of positive and negative examples collected online, (ii) controling model complexity in latent structure learning, and (iii) identifying critical moments to re-learn the structure of AOG based on its intrackability. The intrackability measures uncertainty of an AOG based on its score maps in a frame. In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks, and the VOT benchmarks --- VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking). In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network. In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015. Reproducibility: The source code is released with this paper for reproducing all results, which is available at https://github.com/tfwu/RGM-AOGTracker. version:5
arxiv-1605-05440 | Beyond Caption To Narrative: Video Captioning With Multiple Sentences | http://arxiv.org/abs/1605.05440 | id:1605.05440 author:Andrew Shin, Katsunori Ohnishi, Tatsuya Harada category:cs.CV  published:2016-05-18 summary:Recent advances in image captioning task have led to increasing interests in video captioning task. However, most works on video captioning are focused on generating single input of aggregated features, which hardly deviates from image captioning process and does not fully take advantage of dynamic contents present in videos. We attempt to generate video captions that convey richer contents by temporally segmenting the video with action localization, generating multiple captions from multiple frames, and connecting them with natural language processing techniques, in order to generate a story-like caption. We show that our proposed method can generate captions that are richer in contents and can compete with state-of-the-art method without explicitly using video-level features as input. version:1
arxiv-1605-05433 | Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment | http://arxiv.org/abs/1605.05433 | id:1605.05433 author:Stephen Roller, Katrin Erk category:cs.CL cs.AI  published:2016-05-18 summary:We consider the task of predicting lexical entailment using distributional vectors. We focus experiments on one previous classifier which was shown to only learn to detect prototypicality of a word pair. Analysis shows that the model single-mindedly learns to detect Hearst Patterns, which are well known to be predictive of lexical relations. We present a new model which exploits this Hearst Detector functionality, matching or outperforming prior work on multiple data sets. version:1
arxiv-1605-05416 | Leveraging Lexical Resources for Learning Entity Embeddings in Multi-Relational Data | http://arxiv.org/abs/1605.05416 | id:1605.05416 author:Teng Long, Ryan Lowe, Jackie Chi Kit Cheung, Doina Precup category:cs.CL  published:2016-05-18 summary:Recent work in learning vector-space embeddings for multi-relational data has focused on combining relational information derived from knowledge bases with distributional information derived from large text corpora. We propose a simple approach that leverages the descriptions of entities or phrases available in lexical resources, in conjunction with distributional semantics, in order to derive a better initialization for training relational models. Applying this initialization to the TransE model results in significant new state-of-the-art performances on the WordNet dataset, decreasing the mean rank from the previous best of 212 to 51. It also results in faster convergence of the entity representations. We find that there is a trade-off between improving the mean rank and the hits@10 with this approach. This illustrates that much remains to be understood regarding performance improvements in relational models. version:1
arxiv-1605-05415 | Relative distance features for gait recognition with Kinect | http://arxiv.org/abs/1605.05415 | id:1605.05415 author:Ke Yang, Yong Dou, Shaohe Lv, Fei Zhang, Qi Lv category:cs.CV  published:2016-05-18 summary:Gait and static body measurement are important biometric technologies for passive human recognition. Many previous works argue that recognition performance based completely on the gait feature is limited. The reason for this limited performance remains unclear. This study focuses on human recognition with gait feature obtained by Kinect and shows that gait feature can effectively distinguish from different human beings through a novel representation -- relative distance-based gait features. Experimental results show that the recognition accuracy with relative distance features reaches up to 85%, which is comparable with that of anthropometric features. The combination of relative distance features and anthropometric features can provide an accuracy of more than 95%. Results indicate that the relative distance feature is quite effective and worthy of further study in more general scenarios (e.g., without Kinect). version:1
arxiv-1605-05414 | On the Evaluation of Dialogue Systems with Next Utterance Classification | http://arxiv.org/abs/1605.05414 | id:1605.05414 author:Ryan Lowe, Iulian V. Serban, Mike Noseworthy, Laurent Charlin, Joelle Pineau category:cs.CL cs.LG  published:2016-05-18 summary:An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data. Recent work has proposed Next-Utterance-Classification (NUC) as a surrogate task for building dialogue systems from text data. In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation. Our results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems. version:1
arxiv-1605-05411 | Are Facial Attributes Adversarially Robust? | http://arxiv.org/abs/1605.05411 | id:1605.05411 author:Andras Rozsa, Manuel Günther, Ethan M. Rudd, Terrance E. Boult category:cs.CV  published:2016-05-18 summary:Facial attributes are emerging soft biometrics that have the potential to reject non-matches, for example, based on on mismatching gender. To be usable in stand-alone systems, facial attributes must be extracted from images automatically and reliably. In this paper we propose a simple yet effective solution for automatic facial attribute extraction by training a deep convolutional neural network (DCNN) for each facial attribute separately, without using any pre-training or dataset augmentation, and we obtain new state-of-the-art facial attribute classification results on the CelebA benchmark. To test the stability of the networks, we generated adversarial images via a novel fast flipping attribute (FFA) technique. We show that FFA generates more adversarials than other related algorithms, and that the DCNNs for certain attributes are generally robust to adversarial inputs, while DCNNs for other attributes are not. This result is surprising because no DCNNs tested to date have exhibited robustness to adversarial images without explicit augmentation in the training procedure to account for adversarial examples. Finally, we introduce the concept of natural adversarial images, i.e., images that are misclassified but can be easily turned into correctly classified images by applying small perturbations. We demonstrate that natural adversarials commonly occur, even within the training set, and show that most of these images remain misclassified even with additional training epochs. This phenomenon is surprising because correcting the misclassification, particularly when guided by training data, should require only a small adjustment to the DCNN parameters. version:1
arxiv-1506-06112 | The Extreme Value Machine | http://arxiv.org/abs/1506.06112 | id:1506.06112 author:Ethan M. Rudd, Lalit P. Jain, Walter J. Scheirer, Terrance E. Boult category:cs.LG  published:2015-06-19 summary:It is often desirable to be able to recognize when inputs to a recognition function correspond to classes unseen at training time. With this ability, these inputs could be re-labeled by a human, and later incorporated into the recognition function -- ideally under an efficient incremental update mechanism. While good models that assume inputs from a fixed set of classes exist, e.g., artificial neural networks and kernel machines, it is not immediately obvious how to extend them to perform incremental learning in the presence of unknown query classes. Models that do so take little other distributional information into account when constructing recognition functions and lack strong theoretical foundations. We take steps to address this gap by formulating a novel, theoretically grounded classifier -- the Extreme Value Machine (EVM) -- which is capable of performing open world recognition. The EVM has a well-grounded interpretation derived from statistical extreme value theory (EVT), and is the first classifier of its kind to be able to perform nonlinear, kernel-free, variable bandwidth, incremental learning. We demonstrate experimentally that, compared to other classifiers in the same deep network derived feature space, the EVM is accurate and efficient on an established benchmark partition of the ImageNet dataset. version:3
arxiv-1602-00310 | Learning a low-rank shared dictionary for object classification | http://arxiv.org/abs/1602.00310 | id:1602.00310 author:Tiep H. Vu, Vishal Monga category:cs.CV  published:2016-01-31 summary:Despite the fact that different objects possess distinct class-specific features, they also usually share common patterns. Inspired by this observation, we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification. Our dictionary learning framework is hence characterized by both a shared dictionary and particular (class-specific) dictionaries. For the shared dictionary, we enforce a low-rank constraint, i.e. claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar. For the particular dictionaries, we impose on them the well-known constraints stated in the Fisher discrimination dictionary learning (FDDL). Further, we propose a new fast and accurate algorithm to solve the sparse coding problems in the learning step, accelerating its convergence. The said algorithm could also be applied to FDDL and its extensions. Experimental results on widely used image databases establish the advantages of our method over state-of-the-art dictionary learning methods. version:2
arxiv-1605-00287 | Detecting Burnscar from Hyperspectral Imagery via Sparse Representation with Low-Rank Interference | http://arxiv.org/abs/1605.00287 | id:1605.00287 author:Minh Dao, Xiang Xiang, Bulent Ayhan, Chiman Kwan, Trac D. Tran category:cs.CV  published:2016-05-01 summary:In this paper, we propose a burnscar detection model for hyperspectral imaging (HSI) data. The proposed model contains two-processing steps in which the first step separate and then suppress the cloud information presenting in the data set using an RPCA algorithm and the second step detect the burnscar area in the low-rank component output of the first step. Experiments are conducted on the public MODIS dataset available at NASA official website. version:2
arxiv-1605-05395 | Learning Deep Representations of Fine-grained Visual Descriptions | http://arxiv.org/abs/1605.05395 | id:1605.05395 author:Scott Reed, Zeynep Akata, Bernt Schiele, Honglak Lee category:cs.CV  published:2016-05-17 summary:State-of-the-art methods for zero-shot visual recognition formulate learning as a joint embedding problem of images and side information. In these formulations the current best complement to visual features are attributes: manually encoded vectors describing shared characteristics among categories. Despite good performance, attributes have limitations: (1) finer-grained recognition requires commensurately more attributes, and (2) attributes do not provide a natural language interface. We propose to overcome these limitations by training neural language models from scratch; i.e. without pre-training and only consuming words and characters. Our proposed models train end-to-end to align with the fine-grained and category-specific content of images. Natural language provides a flexible and compact way of encoding only the salient visual aspects for distinguishing categories. By training on raw text, our model can do inference on raw text as well, providing humans a familiar mode both for annotation and retrieval. Our model achieves strong performance on zero-shot text-based image retrieval and significantly outperforms the attribute-based state-of-the-art for zero-shot classification on the Caltech UCSD Birds 200-2011 dataset. version:1
arxiv-1605-05362 | Yelp Dataset Challenge: Review Rating Prediction | http://arxiv.org/abs/1605.05362 | id:1605.05362 author:Nabiha Asghar category:cs.CL cs.IR cs.LG  published:2016-05-17 summary:Review websites, such as TripAdvisor and Yelp, allow users to post online reviews for various businesses, products and services, and have been recently shown to have a significant influence on consumer shopping behaviour. An online review typically consists of free-form text and a star rating out of 5. The problem of predicting a user's star rating for a product, given the user's text review for that product, is called Review Rating Prediction and has lately become a popular, albeit hard, problem in machine learning. In this paper, we treat Review Rating Prediction as a multi-class classification problem, and build sixteen different prediction models by combining four feature extraction methods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic Indexing, with four machine learning algorithms, (i) logistic regression, (ii) Naive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector Classification. We analyse the performance of each of these sixteen models to come up with the best model for predicting the ratings from reviews. We use the dataset provided by Yelp for training and testing the models. version:1
arxiv-1605-01368 | Unsupervised Total Variation Loss for Semi-supervised Deep Learning of Semantic Segmentation | http://arxiv.org/abs/1605.01368 | id:1605.01368 author:Mehran Javanmardi, Mehdi Sajjadi, Ting Liu, Tolga Tasdizen category:cs.CV  published:2016-05-04 summary:We introduce a novel unsupervised loss function for learning semantic segmentation with deep convolutional neural nets (ConvNet) when densely labeled training images are not available. More specifically, the proposed loss function penalizes the L1-norm of the gradient of the label probability vector image , i.e. total variation, produced by the ConvNet. This can be seen as a regularization term that promotes piecewise smoothness of the label probability vector image produced by the ConvNet during learning. The unsupervised loss function is combined with a supervised loss in a semi-supervised setting to learn ConvNets that can achieve high semantic segmentation accuracy even when only a tiny percentage of the pixels in the training images are labeled. We demonstrate significant improvements over the purely supervised setting in the Weizmann horse, Stanford background and Sift Flow datasets. Furthermore, we show that using the proposed piecewise smoothness constraint in the learning phase significantly outperforms post-processing results from a purely supervised approach with Markov Random Fields (MRF). Finally, we note that the framework we introduce is general and can be used to learn to label other types of structures such as curvilinear structures by modifying the unsupervised loss function accordingly. version:2
arxiv-1605-05359 | Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks | http://arxiv.org/abs/1605.05359 | id:1605.05359 author:Ramnandan Krishnamurthy, Aravind S. Lakshminarayanan, Peeyush Kumar, Balaraman Ravindran category:cs.LG cs.AI cs.CV cs.NE  published:2016-05-17 summary:This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. The skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over a common state space. Another major advantage of the approach is that it does not need a prior model of the MDP and can work well even when the MDPs are constructed from sampled trajectories. Finally, we present our attempts to extend the automated skills acquisition framework to complex tasks such as learning to play video games where we use deep learning techniques for representation learning to aid our spatio-temporal abstraction framework. version:1
arxiv-1605-05349 | Orthogonal symmetric non-negative matrix factorization under the stochastic block model | http://arxiv.org/abs/1605.05349 | id:1605.05349 author:Subhadeep Paul, Yuguo Chen category:stat.ML  published:2016-05-17 summary:We present a method based on the orthogonal symmetric non-negative matrix tri-factorization of the normalized Laplacian matrix for community detection in complex networks. While the exact factorization of a given order may not exist and is NP hard to compute, we obtain an approximate factorization by solving an optimization problem. We establish the connection of the factors obtained through the factorization to a non-negative basis of an invariant subspace of the estimated matrix, drawing parallel with the spectral clustering. Using such factorization for clustering in networks is motivated by analyzing a block-diagonal Laplacian matrix with the blocks representing the connected components of a graph. The method is shown to be consistent for community detection in graphs generated from the stochastic block model and the degree corrected stochastic block model. Simulation results and real data analysis show the effectiveness of these methods under a wide variety of situations, including sparse and highly heterogeneous graphs where the usual spectral clustering is known to fail. Our method also performs better than the state of the art in popular benchmark network datasets, e.g., the political web blogs and the karate club data. version:1
arxiv-1605-04652 | Fast and Accurate Performance Analysis of LTE Radio Access Networks | http://arxiv.org/abs/1605.04652 | id:1605.04652 author:Anand Padmanabha Iyer, Ion Stoica, Mosharaf Chowdhury, Li Erran Li category:cs.DC cs.LG cs.NI  published:2016-05-16 summary:An increasing amount of analytics is performed on data that is procured in a real-time fashion to make real-time decisions. Such tasks include simple reporting on streams to sophisticated model building. However, the practicality of such analyses are impeded in several domains because they are faced with a fundamental trade-off between data collection latency and analysis accuracy. In this paper, we study this trade-off in the context of a specific domain, Cellular Radio Access Networks (RAN). Our choice of this domain is influenced by its commonalities with several other domains that produce real-time data, our access to a large live dataset, and their real-time nature and dimensionality which makes it a natural fit for a popular analysis technique, machine learning (ML). We find that the latency accuracy trade-off can be resolved using two broad, general techniques: intelligent data grouping and task formulations that leverage domain characteristics. Based on this, we present CellScope, a system that addresses this challenge by applying a domain specific formulation and application of Multi-task Learning (MTL) to RAN performance analysis. It achieves this goal using three techniques: feature engineering to transform raw data into effective features, a PCA inspired similarity metric to group data from geographically nearby base stations sharing performance commonalities, and a hybrid online-offline model for efficient model updates. Our evaluation of CellScope shows that its accuracy improvements over direct application of ML range from 2.5x to 4.4x while reducing the model update overhead by up to 4.8x. We have also used CellScope to analyze a live LTE consisting of over 2 million subscribers for a period of over 10 months, where it uncovered several problems and insights, some of them previously unknown. version:2
arxiv-1605-05303 | Fuzzy Sets Across the Natural Language Generation Pipeline | http://arxiv.org/abs/1605.05303 | id:1605.05303 author:A. Ramos-Soto, A. Bugarín, S. Barro category:cs.AI cs.CL  published:2016-05-17 summary:We explore the implications of using fuzzy techniques (mainly those commonly used in the linguistic description/summarization of data discipline) from a natural language generation perspective. For this, we provide an extensive discussion of some general convergence points and an exploration of the relationship between the different tasks involved in the standard NLG system pipeline architecture and the most common fuzzy approaches used in linguistic summarization/description of data, such as fuzzy quantified statements, evaluation criteria or aggregation operators. Each individual discussion is illustrated with a related use case. Recent work made in the context of cross-fertilization of both research fields is also referenced. This paper encompasses general ideas that emerged as part of the PhD thesis "Application of fuzzy sets in data-to-text systems". It does not present a specific application or a formal approach, but rather discusses current high-level issues and potential usages of fuzzy sets (focused on linguistic summarization of data) in natural language generation. version:1
arxiv-1605-05296 | Dataflow matrix machines as programmable, dynamically expandable, self-referential generalized recurrent neural networks | http://arxiv.org/abs/1605.05296 | id:1605.05296 author:Michael Bukatin, Steve Matthews, Andrey Radul category:cs.NE cs.PL  published:2016-05-17 summary:Dataflow matrix machines are a powerful generalization of recurrent neural networks. They work with multiple types of linear streams and multiple types of neurons, including higher-order neurons which dynamically update the matrix describing weights and topology of the network in question while the network is running. It seems that the power of dataflow matrix machines is sufficient for them to be a convenient general purpose programming platform. This paper explores a number of useful programming idioms and constructions arising in this context. version:1
arxiv-1605-05284 | Minimax Lower Bounds for Kronecker-Structured Dictionary Learning | http://arxiv.org/abs/1605.05284 | id:1605.05284 author:Zahra Shakeri, Waheed U. Bajwa, Anand D. Sarwate category:cs.IT cs.LG math.IT stat.ML  published:2016-05-17 summary:Dictionary learning is the problem of estimating the collection of atomic elements that provide a sparse representation of measured/collected signals or data. This paper finds fundamental limits on the sample complexity of estimating dictionaries for tensor data by proving a lower bound on the minimax risk. This lower bound depends on the dimensions of the tensor and parameters of the generative model. The focus of this paper is on second-order tensor data, with the underlying dictionaries constructed by taking the Kronecker product of two smaller dictionaries and the observed data generated by sparse linear combinations of dictionary atoms observed through white Gaussian noise. In this regard, the paper provides a general lower bound on the minimax risk and also adapts the proof techniques for equivalent results using sparse and Gaussian coefficient models. The reported results suggest that the sample complexity of dictionary learning for tensor data can be significantly lower than that for unstructured data. version:1
arxiv-1605-05278 | Exact Simulation of Noncircular or Improper Complex-Valued Stationary Gaussian Processes using Circulant Embedding | http://arxiv.org/abs/1605.05278 | id:1605.05278 author:Adam M. Sykulski, Donald B. Percival category:stat.ME stat.CO stat.ML  published:2016-05-17 summary:This paper provides an algorithm for simulating improper (or noncircular) complex-valued stationary Gaussian processes. The technique utilizes recently developed methods for multivariate Gaussian processes from the circulant embedding literature. The method can be performed in $\mathcal{O}(nlog_2n)$ operations, where n is the length of the desired sequence. The method is exact, except when eigenvalues of prescribed circulant matrices are negative. We evaluate the performance of the algorithm empirically, and provide a practical example where the method is guaranteed to be exact for all $n$, with an improper fractional Gaussian noise process. version:1
arxiv-1605-05272 | Fast and Accurate Algorithm for Eye Localization for Gaze Tracking in Low Resolution Images | http://arxiv.org/abs/1605.05272 | id:1605.05272 author:Anjith George, Aurobinda Routray category:cs.CV  published:2016-05-17 summary:Iris centre localization in low-resolution visible images is a challenging problem in computer vision community due to noise, shadows, occlusions, pose variations, eye blinks, etc. This paper proposes an efficient method for determining iris centre in low-resolution images in the visible spectrum. Even low-cost consumer-grade webcams can be used for gaze tracking without any additional hardware. A two-stage algorithm is proposed for iris centre localization. The proposed method uses geometrical characteristics of the eye. In the first stage, a fast convolution based approach is used for obtaining the coarse location of iris centre (IC). The IC location is further refined in the second stage using boundary tracing and ellipse fitting. The algorithm has been evaluated in public databases like BioID, Gi4E and is found to outperform the state of the art methods. version:1
arxiv-1605-05258 | Real-time Eye Gaze Direction Classification Using Convolutional Neural Network | http://arxiv.org/abs/1605.05258 | id:1605.05258 author:Anjith George, Aurobinda Routray category:cs.CV  published:2016-05-17 summary:Estimation eye gaze direction is useful in various human-computer interaction tasks. Knowledge of gaze direction can give valuable information regarding users point of attention. Certain patterns of eye movements known as eye accessing cues are reported to be related to the cognitive processes in the human brain. We propose a real-time framework for the classification of eye gaze direction and estimation of eye accessing cues. In the first stage, the algorithm detects faces using a modified version of the Viola-Jones algorithm. A rough eye region is obtained using geometric relations and facial landmarks. The eye region obtained is used in the subsequent stage to classify the eye gaze direction. A convolutional neural network is employed in this work for the classification of eye gaze direction. The proposed algorithm was tested on Eye Chimera database and found to outperform state of the art methods. The computational complexity of the algorithm is very less in the testing phase. The algorithm achieved an average frame rate of 24 fps in the desktop environment. version:1
arxiv-1605-05239 | Biologically Inspired Radio Signal Feature Extraction with Sparse Denoising Autoencoders | http://arxiv.org/abs/1605.05239 | id:1605.05239 author:Benjamin Migliori, Riley Zeller-Townson, Daniel Grady, Daniel Gebhardt category:stat.ML cs.LG cs.NE  published:2016-05-17 summary:Automatic modulation classification (AMC) is an important task for modern communication systems; however, it is a challenging problem when signal features and precise models for generating each modulation may be unknown. We present a new biologically-inspired AMC method without the need for models or manually specified features --- thus removing the requirement for expert prior knowledge. We accomplish this task using regularized stacked sparse denoising autoencoders (SSDAs). Our method selects efficient classification features directly from raw in-phase/quadrature (I/Q) radio signals in an unsupervised manner. These features are then used to construct higher-complexity abstract features which can be used for automatic modulation classification. We demonstrate this process using a dataset generated with a software defined radio, consisting of random input bits encoded in 100-sample segments of various common digital radio modulations. Our results show correct classification rates of > 99% at 7.5 dB signal-to-noise ratio (SNR) and > 92% at 0 dB SNR in a 6-way classification test. Our experiments demonstrate a dramatically new and broadly applicable mechanism for performing AMC and related tasks without the need for expert-defined or modulation-specific signal information. version:1
arxiv-1605-05223 | On the boosting ability of top-down decision tree learning algorithm for multiclass classification | http://arxiv.org/abs/1605.05223 | id:1605.05223 author:Anna Choromanska, Krzysztof Choromanski, Mariusz Bojarski category:cs.LG  published:2016-05-17 summary:We analyze the performance of the top-down multiclass classification algorithm for decision tree learning called LOMtree, recently proposed in the literature Choromanska and Langford (2014) for solving efficiently classification problems with very large number of classes. The algorithm online optimizes the objective function which simultaneously controls the depth of the tree and its statistical accuracy. We prove important properties of this objective and explore its connection to three well-known entropy-based decision tree objectives, i.e. Shannon entropy, Gini-entropy and its modified version, for which instead online optimization schemes were not yet developed. We show, via boosting-type guarantees, that maximizing the considered objective leads also to the reduction of all of these entropy-based objectives. The bounds we obtain critically depend on the strong-concavity properties of the entropy-based criteria, where the mildest dependence on the number of classes (only logarithmic) corresponds to the Shannon entropy. version:1
arxiv-1601-00311 | A simple and low redundancy method of image compressed sampling | http://arxiv.org/abs/1601.00311 | id:1601.00311 author:Leonid Yaroslavsky category:cs.CV physics.optics  published:2016-01-03 summary:A problem is addressed of minimization of the number of measurements needed for image acquisition and reconstruction with a given accuracy. In last several years, the compressed sensing approach to solving this problem was advanced, which promises reducing the number of required measurements by means of obtaining sparse approximations of images. However, the number of measurements required by compressive sensing substantially exceeds the theoretical minimum defined by sparsity of the image sparse approximation. In the paper, a sampling theory based method of image sampling is suggested that represents a practical and substantially more economical alternative to the compressed sensing approach. Presented and discussed are also results of experimental verification of the method, its possible applicability extensions and some its limitations. version:3
arxiv-1605-05216 | Combinatorially Generated Piecewise Activation Functions | http://arxiv.org/abs/1605.05216 | id:1605.05216 author:Justin Chen category:cs.NE  published:2016-05-17 summary:In the neuroevolution literature, research has primarily focused on evolving the number of nodes, connections, and weights in artificial neural networks. Few attempts have been made to evolve activation functions. Research in evolving activation functions has mainly focused on evolving function parameters, and developing heterogeneous networks by selecting from a fixed pool of activation functions. This paper introduces a novel technique for evolving heterogeneous artificial neural networks through combinatorially generating piecewise activation functions to enhance expressive power. I demonstrate this technique on NeuroEvolution of Augmenting Topologies using ArcTan and Sigmoid, and show that it outperforms the original algorithm on non-Markovian double pole balancing. This technique expands the landscape of unconventional activation functions by demonstrating that they are competitive with canonical choices, and introduces a purview for further exploration of automatic model selection for artificial neural networks. version:1
arxiv-1605-05212 | Multimodal Sparse Coding for Event Detection | http://arxiv.org/abs/1605.05212 | id:1605.05212 author:Youngjune Gwon, William Campbell, Kevin Brady, Douglas Sturim, Miriam Cha, H. T. Kung category:cs.LG cs.CV  published:2016-05-17 summary:Unsupervised feature learning methods have proven effective for classification tasks based on a single modality. We present multimodal sparse coding for learning feature representations shared across multiple modalities. The shared representations are applied to multimedia event detection (MED) and evaluated in comparison to unimodal counterparts, as well as other feature learning methods such as GMM supervectors and sparse RBM. We report the cross-validated classification accuracy and mean average precision of the MED system trained on features learned from our unimodal and multimodal settings for a subset of the TRECVID MED 2014 dataset. version:1
arxiv-1605-03481 | Tweet2Vec: Character-Based Distributed Representations for Social Media | http://arxiv.org/abs/1605.03481 | id:1605.03481 author:Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl, William W. Cohen category:cs.LG cs.CL  published:2016-05-11 summary:Text from social media provides a set of challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts, leading to a prohibitively large vocabulary size for word-level approaches. We propose a character composition model, tweet2vec, which finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many out-of-vocabulary words or unusual character sequences. Our tweet2vec encoder is publicly available. version:2
arxiv-1605-05197 | Towards Weakly-Supervised Action Localization | http://arxiv.org/abs/1605.05197 | id:1605.05197 author:Philippe Weinzaepfel, Xavier Martin, Cordelia Schmid category:cs.CV  published:2016-05-17 summary:This paper presents a novel approach for weakly-supervised action localization, i.e., that does not require per-frame spatial annotations for training. We first introduce an effective method for extracting human tubes by combining a state-of-the-art human detector with a tracking-by-detection approach. Our tube extraction leverages the large amount of annotated humans available today and outperforms the state of the art by an order of magnitude: with less than 5 tubes per video, we obtain a recall of 95% on the UCF-Sports and J-HMDB datasets. Given these human tubes, we perform weakly-supervised selection based on multi-fold Multiple Instance Learning (MIL) with improved dense trajectories and achieve excellent results. We obtain a mAP of 84% on UCF-Sports, 54% on J-HMDB and 45% on UCF-101, which outperforms the state of the art for weakly-supervised action localization and is close to the performance of the best fully-supervised approaches. The second contribution of this paper is a new realistic dataset for action localization, named DALY (Daily Action Localization in YouTube). It contains high quality temporal and spatial annotations for 10 actions in 31 hours of videos (3.3M frames), which is an order of magnitude larger than standard action localization datasets. On the DALY dataset, our tubes have a spatial recall of 82%, but the detection task is extremely challenging, we obtain 10.8% mAP. version:1
arxiv-1605-05195 | Enhanced Twitter Sentiment Classification Using Contextual Information | http://arxiv.org/abs/1605.05195 | id:1605.05195 author:Soroush Vosoughi, Helen Zhou, Deb Roy category:cs.SI cs.AI cs.CL cs.IR  published:2016-05-17 summary:The rise in popularity and ubiquity of Twitter has made sentiment analysis of tweets an important and well-covered area of research. However, the 140 character limit imposed on tweets makes it hard to use standard linguistic methods for sentiment classification. On the other hand, what tweets lack in structure they make up with sheer volume and rich metadata. This metadata includes geolocation, temporal and author information. We hypothesize that sentiment is dependent on all these contextual factors. Different locations, times and authors have different emotional valences. In this paper, we explored this hypothesis by utilizing distant supervision to collect millions of labelled tweets from different locations, times and authors. We used this data to analyse the variation of tweet sentiments across different authors, times and locations. Once we explored and understood the relationship between these variables and sentiment, we used a Bayesian approach to combine these variables with more standard linguistic features such as n-grams to create a Twitter sentiment classifier. This combined classifier outperforms the purely linguistic classifier, showing that integrating the rich contextual information available on Twitter into sentiment classification is a promising direction of research. version:1
arxiv-1603-04838 | Hierarchical image simplification and segmentation based on Mumford-Shah-salient level line selection | http://arxiv.org/abs/1603.04838 | id:1603.04838 author:Yongchao Xu, Thierry Géraud, Laurent Najman category:cs.CV  published:2016-03-15 summary:Hierarchies, such as the tree of shapes, are popular representations for image simplification and segmentation thanks to their multiscale structures. Selecting meaningful level lines (boundaries of shapes) yields to simplify image while preserving intact salient structures. Many image simplification and segmentation methods are driven by the optimization of an energy functional, for instance the celebrated Mumford-Shah functional. In this paper, we propose an efficient approach to hierarchical image simplification and segmentation based on the minimization of the piecewise-constant Mumford-Shah functional. This method conforms to the current trend that consists in producing hierarchical results rather than a unique partition. Contrary to classical approaches which compute optimal hierarchical segmentations from an input hierarchy of segmentations, we rely on the tree of shapes, a unique and well-defined representation equivalent to the image. Simply put, we compute for each level line of the image an attribute function that characterizes its persistence under the energy minimization. Then we stack the level lines from meaningless ones to salient ones through a saliency map based on extinction values defined on the tree-based shape space. Qualitative illustrations and quantitative evaluation on Weizmann segmentation evaluation database demonstrate the state-of-the-art performance of our method. version:2
arxiv-1605-05180 | Structured Prediction of 3D Human Pose with Deep Neural Networks | http://arxiv.org/abs/1605.05180 | id:1605.05180 author:Bugra Tekin, Isinsu Katircioglu, Mathieu Salzmann, Vincent Lepetit, Pascal Fua category:cs.CV  published:2016-05-17 summary:Most recent approaches to monocular 3D pose estimation rely on Deep Learning. They either train a Convolutional Neural Network to directly regress from image to 3D pose, which ignores the dependencies between human joints, or model these dependencies via a max-margin structured learning framework, which involves a high computational cost at inference time. In this paper, we introduce a Deep Learning regression architecture for structured prediction of 3D human pose from monocular images that relies on an overcomplete auto-encoder to learn a high-dimensional latent pose representation and account for joint dependencies. We demonstrate that our approach outperforms state-of-the-art ones both in terms of structure preservation and prediction accuracy. version:1
arxiv-1603-06127 | Sentence Pair Scoring: Towards Unified Framework for Text Comprehension | http://arxiv.org/abs/1603.06127 | id:1603.06127 author:Petr Baudiš, Jan Pichl, Tomáš Vyskočil, Jan Šedivý category:cs.CL cs.AI cs.LG cs.NE  published:2016-03-19 summary:We review the task of Sentence Pair Scoring, popular in the literature in various forms - viewed as Answer Sentence Selection, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a component of Memory Networks. We argue that all such tasks are similar from the model perspective and propose new baselines by comparing the performance of common IR metrics and popular convolutional, recurrent and attention-based neural models across many Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating randomized models, propose a statistically grounded methodology, and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored benchmarks. We introduce a unified open source software framework with easily pluggable models and tasks, which enables us to experiment with multi-task reusability of trained sentence model. We set a new state-of-art in performance on the Ubuntu Dialogue dataset. version:4
arxiv-1605-05172 | Siamese convolutional networks based on phonetic features for cognate identification | http://arxiv.org/abs/1605.05172 | id:1605.05172 author:Taraka Rama category:cs.CL  published:2016-05-17 summary:In this paper, we explore the use of convolutional networks (ConvNets) for the purpose of cognate identification. We compare our architecture with binary classifiers based on string similarity measures on different language families. Our experiments show that convolutional networks achieve competitive results across concepts and across language families at the task of cognate identification. version:1
arxiv-1605-05166 | Digital Stylometry: Linking Profiles Across Social Networks | http://arxiv.org/abs/1605.05166 | id:1605.05166 author:Soroush Vosoughi, Helen Zhou, Deb Roy category:cs.SI cs.AI cs.CL cs.IR  published:2016-05-17 summary:There is an ever growing number of users with accounts on multiple social media and networking sites. Consequently, there is increasing interest in matching user accounts and profiles across different social networks in order to create aggregate profiles of users. In this paper, we present models for Digital Stylometry, which is a method for matching users through stylometry inspired techniques. We experimented with linguistic, temporal, and combined temporal-linguistic models for matching user accounts, using standard and novel techniques. Using publicly available data, our best model, a combined temporal-linguistic one, was able to correctly match the accounts of 31% of 5,612 distinct users across Twitter and Facebook. version:1
arxiv-1605-05156 | Tweet Acts: A Speech Act Classifier for Twitter | http://arxiv.org/abs/1605.05156 | id:1605.05156 author:Soroush Vosoughi, Deb Roy category:cs.CL cs.SI  published:2016-05-17 summary:Speech acts are a way to conceptualize speech as action. This holds true for communication on any platform, including social media platforms such as Twitter. In this paper, we explored speech act recognition on Twitter by treating it as a multi-class classification problem. We created a taxonomy of six speech acts for Twitter and proposed a set of semantic and syntactic features. We trained and tested a logistic regression classifier using a data set of manually labelled tweets. Our method achieved a state-of-the-art performance with an average F1 score of more than $0.70$. We also explored classifiers with three different granularities (Twitter-wide, type-specific and topic-specific) in order to find the right balance between generalization and overfitting for our task. version:1
arxiv-1605-05150 | Automatic Detection and Categorization of Election-Related Tweets | http://arxiv.org/abs/1605.05150 | id:1605.05150 author:Prashanth Vijayaraghavan, Soroush Vosoughi, Deb Roy category:cs.CL cs.IT cs.SI math.IT  published:2016-05-17 summary:With the rise in popularity of public social media and micro-blogging services, most notably Twitter, the people have found a venue to hear and be heard by their peers without an intermediary. As a consequence, and aided by the public nature of Twitter, political scientists now potentially have the means to analyse and understand the narratives that organically form, spread and decline among the public in a political campaign. However, the volume and diversity of the conversation on Twitter, combined with its noisy and idiosyncratic nature, make this a hard task. Thus, advanced data mining and language processing techniques are required to process and analyse the data. In this paper, we present and evaluate a technical framework, based on recent advances in deep neural networks, for identifying and analysing election-related conversation on Twitter on a continuous, longitudinal basis. Our models can detect election-related tweets with an F-score of 0.92 and can categorize these tweets into 22 topics with an F-score of 0.90. version:1
arxiv-1605-05142 | Automatic Classification of Irregularly Sampled Time Series with Unequal Lengths: A Case Study on Estimated Glomerular Filtration Rate | http://arxiv.org/abs/1605.05142 | id:1605.05142 author:Santosh Tirunagari, Simon Bull, Norman Poh category:cs.LG cs.CE  published:2016-05-17 summary:A patient's estimated glomerular filtration rate (eGFR) can provide important information about disease progression and kidney function. Traditionally, an eGFR time series is interpreted by a human expert labelling it as stable or unstable. While this approach works for individual patients, the time consuming nature of it precludes the quick evaluation of risk in large numbers of patients. However, automating this process poses significant challenges as eGFR measurements are usually recorded at irregular intervals and the series of measurements differs in length between patients. Here we present a two-tier system to automatically classify an eGFR trend. First, we model the time series using Gaussian process regression (GPR) to fill in `gaps' by resampling a fixed size vector of fifty time-dependent observations. Second, we classify the resampled eGFR time series using a K-NN/SVM classifier, and evaluate its performance via 5-fold cross validation. Using this approach we achieved an F-score of 0.90, compared to 0.96 for 5 human experts when scored amongst themselves. version:1
arxiv-1605-05134 | A Semi-automatic Method for Efficient Detection of Stories on Social Media | http://arxiv.org/abs/1605.05134 | id:1605.05134 author:Soroush Vosoughi, Deb Roy category:cs.SI cs.CL cs.IR  published:2016-05-17 summary:Twitter has become one of the main sources of news for many people. As real-world events and emergencies unfold, Twitter is abuzz with hundreds of thousands of stories about the events. Some of these stories are harmless, while others could potentially be life-saving or sources of malicious rumors. Thus, it is critically important to be able to efficiently track stories that spread on Twitter during these events. In this paper, we present a novel semi-automatic tool that enables users to efficiently identify and track stories about real-world events on Twitter. We ran a user study with 25 participants, demonstrating that compared to more conventional methods, our tool can increase the speed and the accuracy with which users can track stories about real-world events. version:1
arxiv-1605-05110 | Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling | http://arxiv.org/abs/1605.05110 | id:1605.05110 author:Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun, Xiaolong Wang category:cs.CL  published:2016-05-17 summary:Modeling human conversations is the essence for building satisfying chat-bots with multi-turn dialog ability. Conversation modeling will notably benefit from domain knowledge since the relationships between sentences can be clarified due to semantic hints introduced by knowledge. In this paper, a deep neural network is proposed to incorporate background knowledge for conversation modeling. Through a specially designed Recall gate, domain knowledge can be transformed into the extra global memory of Long Short-Term Memory (LSTM), so as to enhance LSTM by cooperating with its local memory to capture the implicit semantic relevance between sentences within conversations. In addition, this paper introduces the loose structured domain knowledge base, which can be built with slight amount of manual work and easily adopted by the Recall gate. Our model is evaluated on the context-oriented response selecting task, and experimental results on both two datasets have shown that our approach is promising for modeling human conversations and building key components of automatic chatting systems. version:1
arxiv-1605-05106 | Detecting Violent Crowds using Temporal Analysis of GLCM Texture | http://arxiv.org/abs/1605.05106 | id:1605.05106 author:Kaelon Lloyd, David Marshall, Simon C. Moore, Paul L. Rosin category:cs.CV  published:2016-05-17 summary:The severity of sustained injury resulting from assault-related violence can be minimized by reducing detection time. However, it has been shown that human operators perform poorly at detecting events found in video footage when presented with simultaneous feeds. We utilize computer vision techniques to develop an automated method of violence detection that can aid a human operator. We observed that violence in city centre environments often occur in crowded areas, resulting in individual actions being occluded by other crowd members. Measures of visual texture have shown to be effective at encoding crowd appearance. Therefore, we propose modelling crowd dynamics using changes in crowd texture. We refer to this approach as Violent Crowd Texture (VCT). Real-world surveillance footage of night time environments and the violent flows dataset were tested using a random forest classifier to evaluate the ability of the VCT method at discriminating between violent and non-violent behaviour. Our method achieves ROC values of 0.98 and 0.91 on our own real world CCTV dataset and the violent flows dataset respectively. version:1
arxiv-1605-05101 | Recurrent Neural Network for Text Classification with Multi-Task Learning | http://arxiv.org/abs/1605.05101 | id:1605.05101 author:Pengfei Liu, Xipeng Qiu, Xuanjing Huang category:cs.CL  published:2016-05-17 summary:Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks. version:1
arxiv-1605-05087 | Word2Vec is only a special case of Kernel Correspondence Analysis and Kernels for Natural Language Processing | http://arxiv.org/abs/1605.05087 | id:1605.05087 author:Hirotaka Niitsuma category:cs.LG cs.CL  published:2016-05-17 summary:We show Correspondence Analysis (CA) is equivalent to defining Gini-index with appropriate scaled one-hot encoding. Using this relation, we introduce non-linear kernel extension of CA. The extended CA gives well-known analysis for categorical data (CD) and natural language processing by specializing kernels. For example, our formulation can give G-test, skip-gram with negative-sampling (SGNS), and GloVe as a special case. We introduce two kernels for natural language processing based on our formulation. First is a stop word(SW) kernel. Second is word similarity(WS) kernel. The SW kernel is the system introducing appropriate weights for SW. The WS kernel enables to use WS test data as training data for vector space representations of words. We show these kernels enhances accuracy when training data is not sufficiently large. version:1
arxiv-1605-05054 | HARRISON: A Benchmark on HAshtag Recommendation for Real-world Images in Social Networks | http://arxiv.org/abs/1605.05054 | id:1605.05054 author:Minseok Park, Hanxiang Li, Junmo Kim category:cs.CV cs.IR cs.SI  published:2016-05-17 summary:Simple, short, and compact hashtags cover a wide range of information on social networks. Although many works in the field of natural language processing (NLP) have demonstrated the importance of hashtag recommendation, hashtag recommendation for images has barely been studied. In this paper, we introduce the HARRISON dataset, a benchmark on hashtag recommendation for real world images in social networks. The HARRISON dataset is a realistic dataset, composed of 57,383 photos from Instagram and an average of 4.5 associated hashtags for each photo. To evaluate our dataset, we design a baseline framework consisting of visual feature extractor based on convolutional neural network (CNN) and multi-label classifier based on neural network. Based on this framework, two single feature-based models, object-based and scene-based model, and an integrated model of them are evaluated on the HARRISON dataset. Our dataset shows that hashtag recommendation task requires a wide and contextual understanding of the situation conveyed in the image. As far as we know, this work is the first vision-only attempt at hashtag recommendation for real world images in social networks. We expect this benchmark to accelerate the advancement of hashtag recommendation. version:1
arxiv-1601-07661 | DehazeNet: An End-to-End System for Single Image Haze Removal | http://arxiv.org/abs/1601.07661 | id:1601.07661 author:Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, Dacheng Tao category:cs.CV  published:2016-01-28 summary:Single image haze removal is a challenging ill-posed problem. Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image. In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model. DehazeNet adopts Convolutional Neural Networks (CNN) based deep architecture, whose layers are specially designed to embody the established assumptions/priors in image dehazing. Specifically, layers of Maxout units are used for feature extraction, which can generate almost all haze-relevant features. We also propose a novel nonlinear activation function in DehazeNet, called Bilateral Rectified Linear Unit (BReLU), which is able to improve the quality of recovered haze-free image. We establish connections between components of the proposed DehazeNet and those used in existing methods. Experiments on benchmark images show that DehazeNet achieves superior performance over existing methods, yet keeps efficient and easy to use. version:2
arxiv-1605-05045 | Incremental Object Recognition in Robotics with Extension to New Classes in Constant Time | http://arxiv.org/abs/1605.05045 | id:1605.05045 author:Raffaello Camoriano, Giulia Pasquale, Carlo Ciliberto, Lorenzo Natale, Lorenzo Rosasco, Giorgio Metta category:stat.ML cs.CV cs.LG cs.RO  published:2016-05-17 summary:We consider object recognition in the context of lifelong learning, where a robotic agent learns to discriminate between a growing number of object classes as it accumulates experience about the environment. We propose an incremental variant of the Regularized Least Squares for Classification (RLSC) algorithm, and exploit its structure to seamlessly add new classes to the learned model. The presented algorithm addresses the problem of having unbalanced proportion of training examples per class, which occurs when new objects are presented to the system for the first time. We evaluate our algorithm on both a machine learning benchmark dataset and a challenging object recognition task in a robotic setting. Empirical evidence on both problems shows that our approach is significantly faster than its batch counterparts while achieving comparable or better classification performance when classes are unbalanced. version:1
arxiv-1605-05019 | Image stitching with perspective-preserving warping | http://arxiv.org/abs/1605.05019 | id:1605.05019 author:Tianzhu Xiang, Gui-Song Xia, Liangpei Zhang category:cs.CV  published:2016-05-17 summary:Image stitching algorithms often adopt the global transformation, such as homography, and work well for planar scenes or parallax free camera motions. However, these conditions are easily violated in practice. With casual camera motions, variable taken views, large depth change, or complex structures, it is a challenging task for stitching these images. The global transformation model often provides dreadful stitching results, such as misalignments or projective distortions, especially perspective distortion. To this end, we suggest a perspective-preserving warping for image stitching, which spatially combines local projective transformations and similarity transformation. By weighted combination scheme, our approach gradually extrapolates the local projective transformations of the overlapping regions into the non-overlapping regions, and thus the final warping can smoothly change from projective to similarity. The proposed method can provide satisfactory alignment accuracy as well as reduce the projective distortions and maintain the multi-perspective view. Experiments on a variety of challenging images confirm the efficiency of the approach. version:1
arxiv-1605-04227 | Relation Schema Induction using Tensor Factorization with Side Information | http://arxiv.org/abs/1605.04227 | id:1605.04227 author:Madhav Nimishakavi, Uday Singh Saini, Partha Talukdar category:cs.IR cs.CL cs.DB  published:2016-05-12 summary:Given a set of documents from a specific domain (e.g., medical research journals), how do we automatically identify the schema of relations i.e., type signature of arguments of relations (e.g., undergo (Patient, Surgery)) - a necessary first step towards building a Knowledge Graph (KG) out of the given set of documents? We refer to this problem as Relation Schema Induction (RSI). While Open Information Extraction (OIE) techniques aim at extracting surface level triples of the form (John, underwent, Angioplasty), they don't induce the yet unknown schema of relations themselves. Tensors provide a natural representation for such triples, and factorization of such tensors provide a plausible solution for the RSI problem. To the best of our knowledge, tensor factorization methods have not been used for the RSI problem. We fill this gap and propose Coupled Non-negative Tensor Factorization (CNTF), a tensor factorization method which is able to incorporate additional side information in a principled way for more effective Relation Schema Induction. We report our findings on multiple real-world datasets and demonstrate CNTF's effectiveness over state-of-the-art baselines both in terms of accuracy and speed. version:2
arxiv-1511-06359 | FRIST - Flipping and Rotation Invariant Sparsifying Transform Learning and Applications to Inverse Problems | http://arxiv.org/abs/1511.06359 | id:1511.06359 author:Bihan Wen, Saiprasad Ravishankar, Yoram Bresler category:cs.LG cs.CV  published:2015-11-19 summary:Features based on sparse representation, especially using the synthesis dictionary model, have been heavily exploited in signal processing and computer vision. However, synthesis dictionary learning typically involves NP-hard sparse coding and expensive learning steps. Recently, sparsifying transform learning received interest for its cheap computation and its optimal updates in the alternating algorithms. In this work, we develop a methodology for learning of Flipping and Rotation Invariant Sparsifying Transforms, dubbed FRIST, to better represent natural images that contain textures with various geometrical directions. The proposed alternating learning algorithm involves efficient optimal updates. We provide a convergence guarantee, and demonstrate the empirical convergence behavior of the proposed FRIST learning algorithm. Preliminary experiments show the usefulness of adaptive sparse representation by FRIST for image sparse representation, segmentation, denoising, robust inpainting, and MRI reconstruction with promising performances. version:3
arxiv-1605-05011 | Locally Weighted Ensemble Clustering | http://arxiv.org/abs/1605.05011 | id:1605.05011 author:Dong Huang, Chang-Dong Wang, Jian-Huang Lai category:cs.LG  published:2016-05-17 summary:Due to its ability to combine multiple base clusterings into a probably better and more robust clustering, the ensemble clustering technique has been attracting increasing attention in recent years. Despite the significant success, one limitation to most of the existing ensemble clustering methods is that they generally treat all base clusterings equally regardless of their reliability, which makes them vulnerable to low-quality base clusterings. Although some efforts have been made to (globally) evaluate and weight the base clusterings, yet these methods tend to view each base clustering as an individual and neglect the local diversity of clusters inside the same base clustering. It remains an open problem how to evaluate the reliability of clusters and exploit the local diversity in the ensemble to enhance the consensus performance, without access to data features or specific assumptions on data distribution. To address this, in this paper, we propose a novel ensemble clustering approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. In particular, the uncertainty of each cluster is estimated by considering the cluster labels in the entire ensemble via an entropic criterion. A novel ensemble-driven cluster validity measure is introduced, and a locally weighted co-association matrix is presented to serve as a summary for the ensemble of diverse clusters. With the local diversity in ensembles exploited, two novel consensus functions are further proposed. Extensive experiments on a variety of real-world datasets demonstrate the superiority of the proposed approach over the state-of-the-art. version:1
arxiv-1605-01775 | Adversarial Diversity and Hard Positive Generation | http://arxiv.org/abs/1605.01775 | id:1605.01775 author:Andras Rozsa, Ethan M. Rudd, Terrance E. Boult category:cs.CV  published:2016-05-05 summary:State-of-the-art deep neural networks suffer from a fundamental problem - they misclassify adversarial examples formed by applying small perturbations to inputs. In this paper, we present a new psychometric perceptual adversarial similarity score (PASS) measure for quantifying adversarial images, introduce the notion of hard positive generation, and use a diverse set of adversarial perturbations - not just the closest ones - for data augmentation. We introduce a novel hot/cold approach for adversarial example generation, which provides multiple possible adversarial perturbations for every single image. The perturbations generated by our novel approach often correspond to semantically meaningful image structures, and allow greater flexibility to scale perturbation-amplitudes, which yields an increased diversity of adversarial images. We present adversarial images on several network topologies and datasets, including LeNet on the MNIST dataset, and GoogLeNet and ResidualNet on the ImageNet dataset. Finally, we demonstrate on LeNet and GoogLeNet that fine-tuning with a diverse set of hard positives improves the robustness of these networks compared to training with prior methods of generating adversarial images. version:2
arxiv-1605-04996 | SemiContour: A Semi-supervised Learning Approach for Contour Detection | http://arxiv.org/abs/1605.04996 | id:1605.04996 author:Zizhao Zhang, Fuyong Xing, Xiaoshuang Shi, Lin Yang category:cs.CV  published:2016-05-17 summary:Supervised contour detection methods usually require many labeled training images to obtain satisfactory performance. However, a large set of annotated data might be unavailable or extremely labor intensive. In this paper, we investigate the usage of semi-supervised learning (SSL) to obtain competitive detection accuracy with very limited training data (three labeled images). Specifically, we propose a semi-supervised structured ensemble learning approach for contour detection built on structured random forests (SRF). To allow SRF to be applicable to unlabeled data, we present an effective sparse representation approach to capture inherent structure in image patches by finding a compact and discriminative low-dimensional subspace representation in an unsupervised manner, enabling the incorporation of abundant unlabeled patches with their estimated structured labels to help SRF perform better node splitting. We re-examine the role of sparsity and propose a novel and fast sparse coding algorithm to boost the overall learning efficiency. To the best of our knowledge, this is the first attempt to apply SSL for contour detection. Extensive experiments on the BSDS500 segmentation dataset and the NYU Depth dataset demonstrate the superiority of the proposed method. version:1
arxiv-1605-04988 | Going Deeper into Action Recognition: A Survey | http://arxiv.org/abs/1605.04988 | id:1605.04988 author:Samitha Herath, Mehrtash Harandi, Fatih Porikli category:cs.CV  published:2016-05-16 summary:We provide a detailed review of the work on human action recognition over the past decade. We refer to "actions" as meaningful human motions. Starting with methods that are based on handcrafted representations, we review the impact of revamped deep neural networks on action recognition. We follow a systematic taxonomy of action recognition approaches to present a coherent discussion over their improvements and fall-backs. version:1
arxiv-1605-04986 | A Constant-Factor Bi-Criteria Approximation Guarantee for $k$-means++ | http://arxiv.org/abs/1605.04986 | id:1605.04986 author:Dennis Wei category:cs.LG cs.CG I.5.3; G.1.6  published:2016-05-16 summary:This paper studies the $k$-means++ algorithm for clustering as well as the class of $D^\ell$ sampling algorithms to which $k$-means++ belongs. It is shown that for any constant factor $\beta > 1$, selecting $\beta k$ cluster centers by $D^\ell$ sampling yields a constant-factor approximation to the optimal clustering with $k$ centers, in expectation and without conditions on the dataset. This result extends the previously known $O(\log k)$ guarantee for the case $\beta = 1$ to the constant-factor bi-criteria regime. It also improves upon an existing constant-factor bi-criteria result that holds only with constant probability. version:1
arxiv-1602-04302 | Convex Optimization for Linear Query Processing under Approximate Differential Privacy | http://arxiv.org/abs/1602.04302 | id:1602.04302 author:Ganzhao Yuan, Yin Yang, Zhenjie Zhang, Zhifeng Hao category:cs.DB cs.LG stat.ML  published:2016-02-13 summary:Differential privacy enables organizations to collect accurate aggregates over sensitive data with strong, rigorous guarantees on individuals' privacy. Previous work has found that under differential privacy, computing multiple correlated aggregates as a batch, using an appropriate \emph{strategy}, may yield higher accuracy than computing each of them independently. However, finding the best strategy that maximizes result accuracy is non-trivial, as it involves solving a complex constrained optimization program that appears to be non-linear and non-convex. Hence, in the past much effort has been devoted in solving this non-convex optimization program. Existing approaches include various sophisticated heuristics and expensive numerical solutions. None of them, however, guarantees to find the optimal solution of this optimization problem. This paper points out that under ($\epsilon$, $\delta$)-differential privacy, the optimal solution of the above constrained optimization problem in search of a suitable strategy can be found, rather surprisingly, by solving a simple and elegant convex optimization program. Then, we propose an efficient algorithm based on Newton's method, which we prove to always converge to the optimal solution with linear global convergence rate and quadratic local convergence rate. Empirical evaluations demonstrate the accuracy and efficiency of the proposed solution. version:3
arxiv-1605-04955 | Probing the Geometry of Data with Diffusion Fréchet Functions | http://arxiv.org/abs/1605.04955 | id:1605.04955 author:Diego Hernán Díaz Martínez, Christine H. Lee, Peter T. Kim, Washington Mio category:stat.ML 62-07  92C50  published:2016-05-16 summary:Many complex ecosystems, such as those formed by multiple microbial taxa, involve intricate interactions amongst various sub-communities. The most basic relationships are frequently modeled as co-occurrence networks in which the nodes represent the various players in the community and the weighted edges encode levels of interaction. In this setting, the composition of a community may be viewed as a probability distribution on the nodes of the network. This paper develops methods for modeling the organization of such data, as well as their Euclidean counterparts, across spatial scales. Using the notion of diffusion distance, we introduce diffusion Fr\'echet functions and diffusion Fr\'echet vectors associated with probability distributions on Euclidean spaces and the vertex set of a weighted network, respectively. We prove that these functional statistics are stable with respect to the Wasserstein distance between probability measures, thus yielding robust descriptors of their shapes. We apply the methodology to investigate bacterial communities in the human gut, seeking to characterize divergence from intestinal homeostasis in patients with Clostridium difficile infection (CDI) and the effects of fecal microbiota transplantation, a treatment used in CDI patients that has proven to be significantly more effective than traditional treatment with antibiotics. The proposed method proves useful in deriving a biomarker that might help elucidate the mechanisms that drive these processes. version:1
arxiv-1605-04932 | Classification of Big Data with Application to Imaging Genetics | http://arxiv.org/abs/1605.04932 | id:1605.04932 author:Magnus O. Ulfarsson, Frosti Palsson, Jakob Sigurdsson, Johannes R. Sveinsson category:physics.data-an cs.CV stat.ML  published:2016-05-16 summary:Big data applications, such as medical imaging and genetics, typically generate datasets that consist of few observations n on many more variables p, a scenario that we denote as p>>n. Traditional data processing methods are often insufficient for extracting information out of big data. This calls for the development of new algorithms that can deal with the size, complexity, and the special structure of such datasets. In this paper, we consider the problem of classifying p>>n data and propose a classification method based on linear discriminant analysis (LDA). Traditional LDA depends on the covariance estimate of the data, but when p>>n the sample covariance estimate is singular. The proposed method estimates the covariance by using a sparse version of noisy principal component analysis (nPCA). The use of sparsity in this setting aims at automatically selecting variables that are relevant for classification. In experiments, the new method is compared to state-of-the art methods for big data problems using both simulated datasets and imaging genetics datasets. version:1
arxiv-1512-01752 | Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation | http://arxiv.org/abs/1512.01752 | id:1512.01752 author:Sujith Ravi, Qiming Diao category:cs.LG cs.AI  published:2015-12-06 summary:Traditional graph-based semi-supervised learning (SSL) approaches, even though widely applied, are not suited for massive data and large label scenarios since they scale linearly with the number of edges $ E $ and distinct labels $m$. To deal with the large label size problem, recent works propose sketch-based methods to approximate the distribution on labels per node thereby achieving a space reduction from $O(m)$ to $O(\log m)$, under certain conditions. In this paper, we present a novel streaming graph-based SSL approximation that captures the sparsity of the label distribution and ensures the algorithm propagates labels accurately, and further reduces the space complexity per node to $O(1)$. We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. We also study different graph construction mechanisms for natural language applications and propose a robust graph augmentation strategy trained using state-of-the-art unsupervised deep learning architectures that yields further significant quality gains. version:2
arxiv-1512-04387 | Data-driven Sequential Monte Carlo in Probabilistic Programming | http://arxiv.org/abs/1512.04387 | id:1512.04387 author:Yura N Perov, Tuan Anh Le, Frank Wood category:cs.AI stat.AP stat.ML  published:2015-12-14 summary:Most of Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC) algorithms in existing probabilistic programming systems suboptimally use only model priors as proposal distributions. In this work, we describe an approach for training a discriminative model, namely a neural network, in order to approximate the optimal proposal by using posterior estimates from previous runs of inference. We show an example that incorporates a data-driven proposal for use in a non-parametric model in the Anglican probabilistic programming system. Our results show that data-driven proposals can significantly improve inference performance so that considerably fewer particles are necessary to perform a good posterior estimation. version:2
arxiv-1512-06888 | On Distributed Cooperative Decision-Making in Multiarmed Bandits | http://arxiv.org/abs/1512.06888 | id:1512.06888 author:Peter Landgren, Vaibhav Srivastava, Naomi Ehrich Leonard category:cs.SY cs.MA math.OC stat.ML  published:2015-12-21 summary:We study the explore-exploit tradeoff in distributed cooperative decision-making using the context of the multiarmed bandit (MAB) problem. For the distributed cooperative MAB problem, we design the cooperative UCB algorithm that comprises two interleaved distributed processes: (i) running consensus algorithms for estimation of rewards, and (ii) upper-confidence-bound-based heuristics for selection of arms. We rigorously analyze the performance of the cooperative UCB algorithm and characterize the influence of communication graph structure on the decision-making performance of the group. version:2
arxiv-1605-04859 | Reducing the Model Order of Deep Neural Networks Using Information Theory | http://arxiv.org/abs/1605.04859 | id:1605.04859 author:Ming Tu, Visar Berisha, Yu Cao, Jae-sun Seo category:cs.LG cs.NE  published:2016-05-16 summary:Deep neural networks are typically represented by a much larger number of parameters than shallow models, making them prohibitive for small footprint devices. Recent research shows that there is considerable redundancy in the parameter space of deep neural networks. In this paper, we propose a method to compress deep neural networks by using the Fisher Information metric, which we estimate through a stochastic optimization method that keeps track of second-order information in the network. We first remove unimportant parameters and then use non-uniform fixed point quantization to assign more bits to parameters with higher Fisher Information estimates. We evaluate our method on a classification task with a convolutional neural network trained on the MNIST data set. Experimental results show that our method outperforms existing methods for both network pruning and quantization. version:1
arxiv-1605-04850 | Video2GIF: Automatic Generation of Animated GIFs from Video | http://arxiv.org/abs/1605.04850 | id:1605.04850 author:Michael Gygli, Yale Song, Liangliang Cao category:cs.CV cs.MM  published:2016-05-16 summary:We introduce the novel problem of automatically generating animated GIFs from video. GIFs are short looping video with no sound, and a perfect combination between image and video that really capture our attention. GIFs tell a story, express emotion, turn events into humorous moments, and are the new wave of photojournalism. We pose the question: Can we automate the entirely manual and elaborate process of GIF creation by leveraging the plethora of user generated GIF content? We propose a Robust Deep RankNet that, given a video, generates a ranked list of its segments according to their suitability as GIF. We train our model to learn what visual content is often selected for GIFs by using over 100K user generated GIFs and their corresponding video sources. We effectively deal with the noisy web data by proposing a novel adaptive Huber loss in the ranking formulation. We show that our approach is robust to outliers and picks up several patterns that are frequently present in popular animated GIFs. On our new large-scale benchmark dataset, we show the advantage of our approach over several state-of-the-art methods. version:1
arxiv-1504-00680 | Antisocial Behavior in Online Discussion Communities | http://arxiv.org/abs/1504.00680 | id:1504.00680 author:Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure Leskovec category:cs.SI cs.CY stat.AP stat.ML  published:2015-04-02 summary:User contributions in the form of posts, comments, and votes are essential to the success of online communities. However, allowing user participation also invites undesirable behavior such as trolling. In this paper, we characterize antisocial behavior in three large online discussion communities by analyzing users who were banned from these communities. We find that such users tend to concentrate their efforts in a small number of threads, are more likely to post irrelevantly, and are more successful at garnering responses from other users. Studying the evolution of these users from the moment they join a community up to when they get banned, we find that not only do they write worse than other users over time, but they also become increasingly less tolerated by the community. Further, we discover that antisocial behavior is exacerbated when community feedback is overly harsh. Our analysis also reveals distinct groups of users with different levels of antisocial behavior that can change over time. We use these insights to identify antisocial users early on, a task of high practical importance to community maintainers. version:2
arxiv-1511-06397 | Compressing Word Embeddings | http://arxiv.org/abs/1511.06397 | id:1511.06397 author:Martin Andrews category:cs.CL cs.LG  published:2015-11-19 summary:Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic. However, these vector space representations (created through large-scale text analysis) are typically stored verbatim, since their internal structure is opaque. Using word-analogy tests to monitor the level of detail stored in compressed re-representations of the same vector space, the trade-offs between the reduction in memory usage and expressiveness are investigated. A simple scheme is outlined that can reduce the memory footprint of a state-of-the-art embedding by a factor of 10, with only minimal impact on performance. Then, using the same `bit budget', a binary (approximate) factorisation of the same space is also explored, with the aim of creating an equivalent representation with better interpretability. version:2
arxiv-1605-04806 | Multilevel Thresholding Segmentation of T2 weighted Brain MRI images using Convergent Heterogeneous Particle Swarm Optimization | http://arxiv.org/abs/1605.04806 | id:1605.04806 author:Mohammad Hamed Mozaffari, Won-Sook Lee category:cs.CV  published:2016-05-16 summary:This paper proposes a new image thresholding segmentation approach using the heuristic method, Convergent Heterogeneous Particle Swarm Optimization algorithm. The proposed algorithm incorporates a new strategy of searching the problem space by dividing the swarm into subswarms. Each subswarm particles search for better solution separately lead to better exploitation while they cooperate with each other to find the best global position. The consequence of the aforementioned cooperation is better exploration, convergence and it able the algorithm to jump from local optimal solution to the better spots. A practical application of this method is demonstrated for the problem of medical image thresholding segmentation. We considered two classical thresholding techniques of Otsu and Kapur separately as the objective function for the optimization method and applied on a set of brain MR images. Comparative experimental results reveal that the proposed method outperforms another state of the art method from the literature in terms of accuracy, computation time and stable results. version:1
arxiv-1605-03805 | Detecting Relative Anomaly | http://arxiv.org/abs/1605.03805 | id:1605.03805 author:Richard Neuberg, Yixin Shi category:stat.ML cs.LG  published:2016-05-12 summary:System states that are anomalous from the perspective of a domain expert occur frequently in some anomaly detection problems. The performance of commonly used unsupervised anomaly detection methods may suffer in that setting, because they use frequency as a proxy for anomaly. We propose a novel concept for anomaly detection, called relative anomaly detection. It is tailored to be robust towards anomalies that occur frequently, by taking into account their location relative to the most typical observations. The approaches we develop are computationally feasible even for large data sets, and they allow real-time detection. We illustrate using data sets of potential scraping attempts and Wi-Fi channel utilization, both from Google, Inc. version:2
arxiv-1605-04800 | Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing | http://arxiv.org/abs/1605.04800 | id:1605.04800 author:Marcin Junczys-Dowmunt, Roman Grundkiewicz category:cs.CL  published:2016-05-16 summary:This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs (the MT-output and the source) that are decoded to the same target language (post-edited translations). A simple string-matching penalty integrated within the log-linear model can be used to control for higher faithfulness with regard to the to-be-corrected machine translation input. Our submission outperforms the uncorrected baseline on the unseen test set by -3.2% TER and +5.5% BLEU. version:1
arxiv-1605-04785 | An Alternative Matting Laplacian | http://arxiv.org/abs/1605.04785 | id:1605.04785 author:François Pitié category:cs.CV  published:2016-05-16 summary:Cutting out and object and estimate its transparency mask is a key task in many applications. We take on the work on closed-form matting by Levin et al., that is used at the core of many matting techniques, and propose an alternative formulation that offers more flexible controls over the matting priors. We also show that this new approach is efficient at upscaling transparency maps from coarse estimates. version:1
arxiv-1604-07090 | A Review of Co-saliency Detection Technique: Fundamentals, Applications, and Challenges | http://arxiv.org/abs/1604.07090 | id:1604.07090 author:Dingwen Zhang, Huazhu Fu, Junwei Han, Feng Wu category:cs.CV  published:2016-04-24 summary:Co-saliency detection is a newly emerging and rapidly growing research area in computer vision community. As a novel branch of visual saliency, co-saliency detection refers to discovery of the common and salient foregrounds existed in two or more relevant images, and can be more widely used in many computer vision tasks. The existing co-saliency detection algorithms mainly consist of three components: extracting effective features to represent the image regions, exploring the informative cues or factors to characterize co-saliency, and designing effective computational framework to formulate co-saliency. Although enormous methods have been developed, a deep review of the literatures concerning about the co-saliency detection technique is still lacking. In this paper, we aim to provide a comprehensive review of the fundamentals, challenges, and applications in co-saliency detection area. Specifically, this paper provides the overview of some related computer vision works, reviews the history of co-saliency detection briefly, summarizes and categorizes the major algorithms in this research area, presents the potential applications of co-saliency detection, discusses some open issues in this research area, and finally points out some unsolved challenges and promising future works. It is our hope that this review will be beneficial for both the fresh and senior researchers in this field as well as researchers working in other relevant fields to have a better understanding about what they can do with co-saliency detection in the future. version:2
arxiv-1605-04770 | Automatic Image Annotation via Label Transfer in the Semantic Space | http://arxiv.org/abs/1605.04770 | id:1605.04770 author:Tiberio Uricchio, Lamberto Ballan, Lorenzo Seidenari, Alberto Del Bimbo category:cs.CV cs.IR cs.MM  published:2016-05-16 summary:While most automatic image annotation methods rely solely on visual features, we consider integrating additional information into an unified embedding comprised of visual and textual information. We propose an approach based on Kernel Canonical Correlation Analysis, which builds a latent semantic space where correlation of visual and textual features are well preserved into a semantic embedding. Images in the semantic space have reduced semantic gap and thus they are likely to give better annotation performance. The proposed approach is robust and can work either when the training set is well annotated by experts, as well as when it is noisy such as in the case of user-generated tags in social media. We evaluate our framework on four popular datasets. Our results show that our KCCA-based approach can be applied to several state-of-the-art label transfer methods to obtain significant improvements. In particular, nearest neighbor methods for label transfer get the most benefit and enable our approach to scale on never seen labels at training time. Our approach works even with the noisy tags of social users, provided that appropriate denoising is performed. Experiments on a large scale setting show that our method can provide some benefits even when the semantic space is estimated on a subset of training images. version:1
arxiv-1605-04764 | Geometry Aware Mappings for High Dimensional Sparse Factors | http://arxiv.org/abs/1605.04764 | id:1605.04764 author:Avradeep Bhowmik, Nathan Liu, Erheng Zhong, Badri Narayan Bhaskar, Suju Rajan category:cs.LG cs.IR stat.ML  published:2016-05-16 summary:While matrix factorisation models are ubiquitous in large scale recommendation and search, real time application of such models requires inner product computations over an intractably large set of item factors. In this manuscript we present a novel framework that uses the inverted index representation to exploit structural properties of sparse vectors to significantly reduce the run time computational cost of factorisation models. We develop techniques that use geometry aware permutation maps on a tessellated unit sphere to obtain high dimensional sparse embeddings for latent factors with sparsity patterns related to angular closeness of the original latent factors. We also design several efficient and deterministic realisations within this framework and demonstrate with experiments that our techniques lead to faster run time operation with minimal loss of accuracy. version:1
arxiv-1507-06977 | String and Membrane Gaussian Processes | http://arxiv.org/abs/1507.06977 | id:1507.06977 author:Yves-Laurent Kom Samo, Stephen Roberts category:stat.ML 60G15  published:2015-07-24 summary:In this paper we introduce a novel framework for making exact nonparametric Bayesian inference on latent functions, that is particularly suitable for Big Data tasks. Firstly, we introduce a class of stochastic processes we refer to as string Gaussian processes (string GPs), which are not to be mistaken for Gaussian processes operating on text. We construct string GPs so that their finite-dimensional marginals exhibit suitable local conditional independence structures, which allow for scalable, distributed, and flexible nonparametric Bayesian inference, without resorting to approximations, and while ensuring some mild global regularity constraints. Furthermore, string GP priors naturally cope with heterogeneous input data, and the gradient of the learned latent function is readily available for explanatory analysis. Secondly, we provide some theoretical results relating our approach to the standard GP paradigm. In particular, we prove that some string GPs are Gaussian processes, which provides a complementary global perspective on our framework. Finally, we derive a scalable and distributed MCMC scheme for supervised learning tasks under string GP priors. The proposed MCMC scheme has computational time complexity $\mathcal{O}(N)$ and memory requirement $\mathcal{O}(dN)$, where $N$ is the data size and $d$ the dimension of the input space. We illustrate the efficacy of the proposed approach on several synthetic and real-world datasets, including a dataset with $6$ millions input points and $8$ attributes. version:3
arxiv-1605-04731 | CNN based texture synthesize with Semantic segment | http://arxiv.org/abs/1605.04731 | id:1605.04731 author:Xianye Liang, Bocheng Zhuo, Peijie Li, Liangju He category:cs.CV cs.GR cs.LG  published:2016-05-16 summary:Deep learning algorithm display powerful ability in Computer Vision area, in recent year, the CNN has been applied to solve problems in the subarea of Image-generating, which has been widely applied in areas such as photo editing, image design, computer animation, real-time rendering for large scale of scenes and for visual effects in movies. However in the texture synthesize procedure. The state-of-art CNN can not capture the spatial location of texture in image, lead to significant distortion after texture synthesize, we propose a new way to generating-image by adding the semantic segment step with deep learning algorithm as Pre-Processing and analyze the outcome. version:1
arxiv-1512-07839 | Implementing a Bayes Filter in a Neural Circuit: The Case of Unknown, Nonlinear Stimulus Dynamics | http://arxiv.org/abs/1512.07839 | id:1512.07839 author:Sacha Sokoloski category:cs.LG stat.ML  published:2015-12-22 summary:In order to interact intelligently with objects in the world, animals must first transform neural population responses into estimates of the unknown stimuli which caused them. The Bayesian solution to this problem is known as a Bayes filter, and previous work has shown how to exactly implement a Bayes filter in a theoretical neural circuit when the stimulus dynamics are known and linear. In this paper we develop a method for approximating a Bayes filter when the stimulus dynamics are unknown and nonlinear, by training a recurrent neural network to approximate the predictions of the Bayes filter. To train the network, we use a combination of contrastive divergence minimization and backpropagation, in order to maximize the likelihood of the parameters of the network given the population responses. We demonstrate this method on a problem where the stimulus is a stochastic pendulum, and show how the learned network displays many of the characteristic properties found in research on populations of neurons. version:2
arxiv-1605-04711 | Ternary Weight Networks | http://arxiv.org/abs/1605.04711 | id:1605.04711 author:Fengfu Li, Bin Liu category:cs.CV  published:2016-05-16 summary:We introduce Ternary Weight Networks (TWNs) - neural networks with weights constrained to +1, 0 and -1. The L2 distance between the full (float or double) precision weights and the ternary weights along with a scaling factor is minimized. With the optimization, the TWNs own high capacity of model expression that is good enough to approximate the Full Precision Weight Networks (FPWNs) counterpart. Besides, the TWNs achieve up to 16x or 32x model compression rate and own much fewer multiplications compared with the FPWNs. Compared with recently proposed Binary Precision Weight Networks (BPWNs), the TWNs own nearly 38x more power of expression in a 3$\times$3 size filter, which is commonly used in most of the state-of-the-art CNN models like residual networks or VGG. Besides, the TWNs eliminate the singularity at zero and converge faster and more stablely at training time. Benchmarks on MNIST, CIFAR-10, and the large scale ImageNet dataset show that TWNs achieve state-of-the-art performance which is only slightly worse than the FPWNs counterpart but outperforms the analogous BPWNs. version:1
arxiv-1601-04115 | Estimation of Fiber Orientations Using Neighborhood Information | http://arxiv.org/abs/1601.04115 | id:1601.04115 author:Chuyang Ye, Jiachen Zhuo, Rao P. Gullapalli, Jerry L. Prince category:cs.CV  published:2016-01-16 summary:Data from diffusion magnetic resonance imaging (dMRI) can be used to reconstruct fiber tracts, for example, in muscle and white matter. Estimation of fiber orientations (FOs) is a crucial step in the reconstruction process and these estimates can be corrupted by noise. In this paper, a new method called Fiber Orientation Reconstruction using Neighborhood Information (FORNI) is described and shown to reduce the effects of noise and improve FO estimation performance by incorporating spatial consistency. FORNI uses a fixed tensor basis to model the diffusion weighted signals, which has the advantage of providing an explicit relationship between the basis vectors and the FOs. FO spatial coherence is encouraged using weighted l1-norm regularization terms, which contain the interaction of directional information between neighbor voxels. Data fidelity is encouraged using a squared error between the observed and reconstructed diffusion weighted signals. After appropriate weighting of these competing objectives, the resulting objective function is minimized using a block coordinate descent algorithm, and a straightforward parallelization strategy is used to speed up processing. Experiments were performed on a digital crossing phantom, ex vivo tongue dMRI data, and in vivo brain dMRI data for both qualitative and quantitative evaluation. The results demonstrate that FORNI improves the quality of FO estimation over other state of the art algorithms. version:2
arxiv-1605-04672 | A Critical Examination of RESCAL for Completion of Knowledge Bases with Transitive Relations | http://arxiv.org/abs/1605.04672 | id:1605.04672 author:Pushpendre Rastogi, Benjamin Van Durme category:stat.ML cs.AI cs.DB cs.LG  published:2016-05-16 summary:Link prediction in large knowledge graphs has received a lot of attention recently because of its importance for inferring missing relations and for completing and improving noisily extracted knowledge graphs. Over the years a number of machine learning researchers have presented various models for predicting the presence of missing relations in a knowledge base. Although all the previous methods are presented with empirical results that show high performance on select datasets, there is almost no previous work on understanding the connection between properties of a knowledge base and the performance of a model. In this paper we analyze the RESCAL method and prove that it can not encode asymmetric transitive relations in knowledge bases. version:1
arxiv-1603-08318 | Exclusivity Regularized Machine | http://arxiv.org/abs/1603.08318 | id:1603.08318 author:Xiaojie Guo category:cs.LG  published:2016-03-28 summary:It has been recognized that the diversity of base learners is of utmost importance to a good ensemble. This paper defines a novel measurement of diversity, termed as exclusivity. With the designed exclusivity, we further propose an ensemble model, namely Exclusivity Regularized Machine (ERM), to jointly suppress the training error of ensemble and enhance the diversity between bases. Moreover, an Augmented Lagrange Multiplier based algorithm is customized to effectively and efficiently seek the optimal solution of ERM. Theoretical analysis on convergence and global optimality of the proposed algorithm, as well as experiments are provided to reveal the efficacy of our method and show its superiority over state-of-the-art alternatives in terms of accuracy and efficiency. version:2
arxiv-1605-04657 | Solve-Select-Scale: A Three Step Process For Sparse Signal Estimation | http://arxiv.org/abs/1605.04657 | id:1605.04657 author:Mithun Das Gupta category:cs.IT cs.LG math.IT stat.ML  published:2016-05-16 summary:In the theory of compressed sensing (CS), the sparsity $\ x\ _0$ of the unknown signal $\mathbf{x} \in \mathcal{R}^n$ is of prime importance and the focus of reconstruction algorithms has mainly been either $\ x\ _0$ or its convex relaxation (via $\ x\ _1$). However, it is typically unknown in practice and has remained a challenge when nothing about the size of the support is known. As pointed recently, $\ x\ _0$ might not be the best metric to minimize directly, both due to its inherent complexity as well as its noise performance. Recently a novel stable measure of sparsity $s(\mathbf{x}) := \ \mathbf{x}\ _1^2/\ \mathbf{x}\ _2^2$ has been investigated by Lopes \cite{Lopes2012}, which is a sharp lower bound on $\ \mathbf{x}\ _0$. The estimation procedure for this measure uses only a small number of linear measurements, does not rely on any sparsity assumptions, and requires very little computation. The usage of the quantity $s(\mathbf{x})$ in sparse signal estimation problems has not received much importance yet. We develop the idea of incorporating $s(\mathbf{x})$ into the signal estimation framework. We also provide a three step algorithm to solve problems of the form $\mathbf{Ax=b}$ with no additional assumptions on the original signal $\mathbf{x}$. version:1
arxiv-1605-04655 | Joint Learning of Sentence Embeddings for Relevance and Entailment | http://arxiv.org/abs/1605.04655 | id:1605.04655 author:Petr Baudis, Silvestr Stanko, Jan Sedivy category:cs.CL cs.LG cs.NE  published:2016-05-16 summary:We consider the problem of Recognizing Textual Entailment within an Information Retrieval context, where we must simultaneously determine the relevancy as well as degree of entailment for individual pieces of evidence to determine a yes/no answer to a binary natural language question. We compare several variants of neural networks for sentence embeddings in a setting of decision-making based on evidence of varying relevance. We propose a basic model to integrate evidence for entailment, show that joint training of the sentence embeddings to model relevance and entailment is feasible even with no explicit per-evidence supervision, and show the importance of evaluating strong baselines. We also demonstrate the benefit of carrying over text comprehension model trained on an unrelated task for our small datasets. Our research is motivated primarily by a new open dataset we introduce, consisting of binary questions and news-based evidence snippets. We also apply the proposed relevance-entailment model on a similar task of ranking multiple-choice test answers, evaluating it on a preliminary dataset of school test questions as well as the standard MCTest dataset, where we improve the neural model state-of-art. version:1
arxiv-1605-04654 | Wavelet Scattering Regression of Quantum Chemical Energies | http://arxiv.org/abs/1605.04654 | id:1605.04654 author:Matthew Hirn, Stéphane Mallat, Nicolas Poilvert category:math.CA physics.chem-ph quant-ph stat.ML  published:2016-05-16 summary:We introduce multiscale invariant dictionaries to estimate quantum chemical energies of organic molecules, from training databases. Molecular energies are invariant to isometric atomic displacements, and are Lipschitz continuous to molecular deformations. Similarly to density functional theory (DFT), the molecule is represented by an electronic density function. A multiscale invariant dictionary is calculated with wavelet scattering invariants. It cascades a first wavelet transform which separates scales, with a second wavelet transform which computes interactions across scales. Sparse scattering regressions give state of the art results over two databases of organic planar molecules. On these databases, the regression error is of the order of the error produced by DFT codes, but at a fraction of the computational cost. version:1
arxiv-1605-04639 | Alternating optimization method based on nonnegative matrix factorizations for deep neural networks | http://arxiv.org/abs/1605.04639 | id:1605.04639 author:Tetsuya Sakurai, Akira Imakura, Yuto Inoue, Yasunori Futamura category:cs.LG cs.NE stat.ML  published:2016-05-16 summary:The backpropagation algorithm for calculating gradients has been widely used in computation of weights for deep neural networks (DNNs). This method requires derivatives of objective functions and has some difficulties finding appropriate parameters such as learning rate. In this paper, we propose a novel approach for computing weight matrices of fully-connected DNNs by using two types of semi-nonnegative matrix factorizations (semi-NMFs). In this method, optimization processes are performed by calculating weight matrices alternately, and backpropagation (BP) is not used. We also present a method to calculate stacked autoencoder using a NMF. The output results of the autoencoder are used as pre-training data for DNNs. The experimental results show that our method using three types of NMFs attains similar error rates to the conventional DNNs with BP. version:1
arxiv-1605-04638 | Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient | http://arxiv.org/abs/1605.04638 | id:1605.04638 author:Tianbao Yang, Lijun Zhang, Rong Jin, Jinfeng Yi category:cs.LG math.OC stat.ML  published:2016-05-16 summary:This work focuses on dynamic regret of online convex optimization that compares the performance of online learning to a clairvoyant who knows the sequence of loss functions in advance and hence selects the minimizer of the loss function at each step. By assuming that the clairvoyant moves slowly (i.e., the minimizers change slowly), we present several improved variation-based upper bounds of the dynamic regret under the true and noisy gradient feedback, which are {\it optimal} in light of the presented lower bounds. The key to our analysis is to explore a regularity metric that measures the temporal changes in the clairvoyant's minimizers, to which we refer as {\it path variation}. Firstly, we present a general lower bound in terms of the path variation, and then show that under full information or gradient feedback we are able to achieve an optimal dynamic regret. Secondly, we present a lower bound with noisy gradient feedback and then show that we can achieve optimal dynamic regrets under a stochastic gradient feedback and two-point bandit feedback. Moreover, for a sequence of smooth loss functions that admit a small variation in the gradients, our dynamic regret under the two-point bandit feedback matches what is achieved with full information. version:1
arxiv-1605-04634 | Heart Beat Characterization from Ballistocardiogram Signals using Extended Functions of Multiple Instances | http://arxiv.org/abs/1605.04634 | id:1605.04634 author:Changzhe Jiao, Princess Lyons, Alina Zare, Licet Rosales, Marjorie Skubic category:cs.CV  published:2016-05-16 summary:A multiple instance learning (MIL) method, extended Function of Multiple Instances ($e$FUMI), is applied to ballistocardiogram (BCG) signals produced by a hydraulic bed sensor. The goal of this approach is to learn a personalized heartbeat "concept" for an individual. This heartbeat concept is a prototype (or "signature") that characterizes the heartbeat pattern for an individual in ballistocardiogram data. The $e$FUMI method models the problem of learning a heartbeat concept from a BCG signal as a MIL problem. This approach elegantly addresses the uncertainty inherent in a BCG signal e. g., misalignment between training data and ground truth, mis-collection of heartbeat by some transducers, etc. Given a BCG training signal coupled with a ground truth signal (e.g., a pulse finger sensor), training "bags" labeled with only binary labels denoting if a training bag contains a heartbeat signal or not can be generated. Then, using these bags, $e$FUMI learns a personalized concept of heartbeat for a subject as well as several non-heartbeat background concepts. After learning the heartbeat concept, heartbeat detection and heart rate estimation can be applied to test data. Experimental results show that the estimated heartbeat concept found by $e$FUMI is more representative and a more discriminative prototype of the heartbeat signals than those found by comparison MIL methods in the literature. version:1
arxiv-1605-04624 | Learning to Rank Personalized Search Results in Professional Networks | http://arxiv.org/abs/1605.04624 | id:1605.04624 author:Viet Ha-Thuc, Shakti Sinha category:cs.IR cs.LG  published:2016-05-16 summary:LinkedIn search is deeply personalized - for the same queries, different searchers expect completely different results. This paper presents our approach to achieving this by mining various data sources available in LinkedIn to infer searchers' intents (such as hiring, job seeking, etc.), as well as extending the concept of homophily to capture the searcher-result similarities on many aspects. Then, learning-to-rank (LTR) is applied to combine these signals with standard search features. version:1
arxiv-1512-01848 | Rank Pooling for Action Recognition | http://arxiv.org/abs/1512.01848 | id:1512.01848 author:Basura Fernando, Efstratios Gavves, Jose Oramas, Amir Ghodrati, Tinne Tuytelaars category:cs.CV  published:2015-12-06 summary:We propose a function-based temporal pooling method that captures the latent structure of the video sequence data - e.g. how frame-level features evolve over time in a video. We show how the parameters of a function that has been fit to the video data can serve as a robust new video representation. As a specific example, we learn a pooling function via ranking machines. By learning to rank the frame-level features of a video in chronological order, we obtain a new representation that captures the video-wide temporal dynamics of a video, suitable for action recognition. Other than ranking functions, we explore different parametric models that could also explain the temporal changes in videos. The proposed functional pooling methods, and rank pooling in particular, is easy to interpret and implement, fast to compute and effective in recognizing a wide variety of actions. We evaluate our method on various benchmarks for generic action, fine-grained action and gesture recognition. Results show that rank pooling brings an absolute improvement of 7-10 average pooling baseline. At the same time, rank pooling is compatible with and complementary to several appearance and local motion based methods and features, such as improved trajectories and deep learning features. version:2
arxiv-1512-06473 | Quantized Convolutional Neural Networks for Mobile Devices | http://arxiv.org/abs/1512.06473 | id:1512.06473 author:Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng category:cs.CV  published:2015-12-21 summary:Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4~6x speed-up and 15~20x compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second. version:3
arxiv-1605-04614 | DeepLearningKit - an GPU Optimized Deep Learning Framework for Apple's iOS, OS X and tvOS developed in Metal and Swift | http://arxiv.org/abs/1605.04614 | id:1605.04614 author:Amund Tveit, Torbjørn Morland, Thomas Brox Røst category:cs.LG cs.DC cs.NE  published:2016-05-15 summary:In this paper we present DeepLearningKit - an open source framework that supports using pretrained deep learning models (convolutional neural networks) for iOS, OS X and tvOS. DeepLearningKit is developed in Metal in order to utilize the GPU efficiently and Swift for integration with applications, e.g. iOS-based mobile apps on iPhone/iPad, tvOS-based apps for the big screen, or OS X desktop applications. The goal is to support using deep learning models trained with popular frameworks such as Caffe, Torch, TensorFlow, Theano, Pylearn, Deeplearning4J and Mocha. Given the massive GPU resources and time required to train Deep Learning models we suggest an App Store like model to distribute and download pretrained and reusable Deep Learning models. version:1
arxiv-1605-04603 | Improving the Neural Algorithm of Artistic Style | http://arxiv.org/abs/1605.04603 | id:1605.04603 author:Roman Novak, Yaroslav Nikulin category:cs.CV  published:2016-05-15 summary:In this work we investigate different avenues of improving the Neural Algorithm of Artistic Style (by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge, arXiv:1508.06576). While showing great results when transferring homogeneous and repetitive patterns, the original style representation often fails to capture more complex properties, like having separate styles of foreground and background. This leads to visual artifacts and undesirable textures appearing in unexpected regions when performing style transfer. We tackle this issue with a variety of approaches, mostly by modifying the style representation in order for it to capture more information and impose a tighter constraint on the style transfer result. In our experiments, we subjectively evaluate our best method as producing from barely noticeable to significant improvements in the quality of style transfer. version:1
arxiv-1605-05588 | A Distributed Quaternion Kalman Filter With Applications to Fly-by-Wire Systems | http://arxiv.org/abs/1605.05588 | id:1605.05588 author:Sayed Pouria Talebi category:cs.SY stat.AP stat.ML  published:2016-05-15 summary:The introduction of automated flight control and management systems have made possible aircraft designs that sacrifice arodynamic stability in order to incorporate stealth technology intro their shape, operate more efficiently, and are highly maneuverable. Therefore, modern flight management systems are reliant on multiple redundant sensors to monitor and control the rotations of the aircraft. To this end, a novel distributed quaternion Kalman filtering algorithm is developed for tracking the rotation and orientation of an aircraft in the three-dimensional space. The algorithm is developed to distribute computation among the sensors in a manner that forces them to consent to a unique solution while being robust to sensor and link failure, a desirable characteristic for flight management systems. In addition, the underlying quaternion-valued state space model allows to avoid problems associated with gimbal lock. The performance of the developed algorithm is verified through simulations. version:1
arxiv-1604-06242 | Novelty Detection in MultiClass Scenarios with Incomplete Set of Class Labels | http://arxiv.org/abs/1604.06242 | id:1604.06242 author:Nomi Vinokurov, Daphna Weinshall category:cs.CV  published:2016-04-21 summary:We address the problem of novelty detection in multiclass scenarios where some class labels are missing from the training set. Our method is based on the initial assignment of confidence values, which measure the affinity between a new test point and each known class. We first compare the values of the two top elements in this vector of confidence values. In the heart of our method lies the training of an ensemble of classifiers, each trained to discriminate known from novel classes based on some partition of the training data into presumed-known and presumednovel classes. Our final novelty score is derived from the output of this ensemble of classifiers. We evaluated our method on two datasets of images containing a relatively large number of classes - the Caltech-256 and Cifar-100 datasets. We compared our method to 3 alternative methods which represent commonly used approaches, including the one-class SVM, novelty based on k-NN, novelty based on maximal confidence, and the recent KNFST method. The results show a very clear and marked advantage for our method over all alternative methods, in an experimental setup where class labels are missing during training. version:2
arxiv-1604-02646 | Visualization Regularizers for Neural Network based Image Recognition | http://arxiv.org/abs/1604.02646 | id:1604.02646 author:Biswajit Paria, Anirban Santara, Pabitra Mitra category:cs.LG cs.CV cs.NE  published:2016-04-10 summary:The success of deep neural networks is mostly due their ability to learn meaningful features from the data. Features learned in the hidden layers of deep neural networks trained in computer vision tasks have been shown to be similar to mid-level vision features. We leverage this fact in this work and propose the visualization regularizer for image tasks. The proposed regularization technique enforces smoothness of the features learned by hidden nodes and turns out to be a special case of Tikhonov regularization. We achieve higher classification accuracy as compared to existing regularizers such as the L2 norm regularizer and dropout, on benchmark datasets with no change in the training computational complexity. version:2
arxiv-1605-04553 | Treating Similarity with Respect: How to Evaluate Models of Meaning? | http://arxiv.org/abs/1605.04553 | id:1605.04553 author:Dmitrijs Milajevs, Sascha Griffiths category:cs.CL  published:2016-05-15 summary:Similarity is a core notion that is used in psychology, theoretical and computational linguistics. The similarity datasets that come from the two fields differ in design: psychological datasets are focused around a certain topic such as fruit names; linguistic datasets contain words from various categories. The later makes humans assign low similarity scores to the words that have nothing in common and to the words that have contrast in meaning, making similarity scores ambiguous. In this work we discuss the similarity collection procedure for a multi-category dataset that avoids score ambiguity and suggest changes to the evaluation procedure to reflect the insights of psychological literature for word, phrase and sentence similarity. We suggest to ask humans to provide a list of commonalities and differences instead of numerical similarity scores and employ the structure of human judgements beyond pairwise similarity. We believe that the proposed approach will give rise to datasets that test meaning representation models more thoroughly with respect to the human treatment of similarity. version:1
arxiv-1603-09631 | Data Collection for Interactive Learning through the Dialog | http://arxiv.org/abs/1603.09631 | id:1603.09631 author:Miroslav Vodolán, Filip Jurčíček category:cs.CL cs.LG  published:2016-03-31 summary:This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning. version:2
arxiv-1603-08199 | Recurrent Mixture Density Network for Spatiotemporal Visual Attention | http://arxiv.org/abs/1603.08199 | id:1603.08199 author:Loris Bazzani, Hugo Larochelle, Lorenzo Torresani category:cs.CV  published:2016-03-27 summary:The high-dimensional and redundant nature of video have pushed researchers to seek the design of attentional models that can dynamically focus computations on the spatiotemporal volumes that are most relevant. Specifically, these models have been used to eliminate or down-weight background pixels that are not important for the task at hand. In order to deal with this problem, we propose an attentional model that learns where to look in a video directly from human fixation data. The proposed model leverages deep 3D convolutional features to represent clip segments in videos. This clip-level representation is aggregated over time by a long short-term memory network that connects into a mixture density network model of the likely positions of fixations in each frame. The resulting model is trained end to end using backpropagation. Our experiments show state-of-the-art performance on saliency prediction for videos. Experiments on Hollywood2 and UCF101 also show that the saliency can be used to improve classification accuracy on action recognition tasks. version:3
arxiv-1601-01467 | On Some Properties of Calibrated Trifocal Tensors | http://arxiv.org/abs/1601.01467 | id:1601.01467 author:Evgeniy Martyushev category:cs.CV  published:2016-01-07 summary:In two-view geometry, the essential matrix describes the relative position and orientation of two calibrated images. In three views, a similar role is assigned to the calibrated trifocal tensor. It is a particular case of the (uncalibrated) trifocal tensor and thus it inherits all its properties but, due to the smaller degrees of freedom, satisfies a number of additional algebraic constraints. Some of them are described in this paper. More specifically, we define a new notion --- the trifocal essential matrix. On the one hand, it is a generalization of the ordinary (bifocal) essential matrix, and, on the other hand, it is closely related to the calibrated trifocal tensor. We prove the two necessary and sufficient conditions that characterize the set of trifocal essential matrices. Based on these characterizations, we propose three necessary conditions on a calibrated trifocal tensor. They have a form of 15 quartic and 99 quintic polynomial equations. We show that in the practically significant real case the 15 quartic constraints are also sufficient. version:3
arxiv-1605-04502 | Joint Learning of Convolutional Neural Networks and Temporally Constrained Metrics for Tracklet Association Based on Large-Scale Datasets | http://arxiv.org/abs/1605.04502 | id:1605.04502 author:Bing Wang, Kap Luk Chan, Li Wang, Bing Shuai, Zhen Zuo, Ting Liu, Gang Wang category:cs.CV  published:2016-05-15 summary:In this paper, we study the challenging problem of multi-object tracking in a complex scene captured by a single camera. Different from the existing tracklet association-based tracking methods, we propose a novel and efficient way to obtain discriminative appearance-based tracklet affinity models. Our proposed method jointly learns the convolutional neural networks (CNNs) and temporally constrained metrics. In our method, a siamese convolutional neural network (CNN) is first pre-trained on the auxiliary data. Then the siamese CNN and temporally constrained metrics are jointly learned online to construct the appearance-based tracklet affinity models. The proposed method can jointly learn the hierarchical deep features and temporally constrained segment-wise metrics under a unified framework. For reliable association between tracklets, a novel loss function incorporating temporally constrained multi-task learning mechanism is proposed. By employing the proposed method, tracklet association can be accomplished even in challenging situations. Moreover, a large-scale dataset with 40 fully annotated sequences is created to facilitate the tracking evaluation. Experimental results on five public datasets and the new large-scale dataset show that our method outperforms several state-of-the-art approaches in multi-object tracking. version:1
arxiv-1605-04478 | Gabor Barcodes for Medical Image Retrieval | http://arxiv.org/abs/1605.04478 | id:1605.04478 author:Mina Nouredanesh, Hamid R. Tizhoosh, Ershad Banijamali category:cs.CV  published:2016-05-14 summary:In recent years, advances in medical imaging have led to the emergence of massive databases, containing images from a diverse range of modalities. This has significantly heightened the need for automated annotation of the images on one side, and fast and memory-efficient content-based image retrieval systems on the other side. Binary descriptors have recently gained more attention as a potential vehicle to achieve these goals. One of the recently introduced binary descriptors for tagging of medical images are Radon barcodes (RBCs) that are driven from Radon transform via local thresholding. Gabor transform is also a powerful transform to extract texture-based information. Gabor features have exhibited robustness against rotation, scale, and also photometric disturbances, such as illumination changes and image noise in many applications. This paper introduces Gabor Barcodes (GBCs), as a novel framework for the image annotation. To find the most discriminative GBC for a given query image, the effects of employing Gabor filters with different parameters, i.e., different sets of scales and orientations, are investigated, resulting in different barcode lengths and retrieval performances. The proposed method has been evaluated on the IRMA dataset with 193 classes comprising of 12,677 x-ray images for indexing, and 1,733 x-rays images for testing. A total error score as low as $351$ ($\approx 80\%$ accuracy for the first hit) was achieved. version:1
arxiv-1605-04475 | Capturing divergence in dependency trees to improve syntactic projection | http://arxiv.org/abs/1605.04475 | id:1605.04475 author:Ryan Georgi, Fei Xia, William D. Lewis category:cs.CL  published:2016-05-14 summary:Obtaining syntactic parses is a crucial part of many NLP pipelines. However, most of the world's languages do not have large amounts of syntactically annotated corpora available for building parsers. Syntactic projection techniques attempt to address this issue by using parallel corpora consisting of resource-poor and resource-rich language pairs, taking advantage of a parser for the resource-rich language and word alignment between the languages to project the parses onto the data for the resource-poor language. These projection methods can suffer, however, when the two languages are divergent. In this paper, we investigate the possibility of using small, parallel, annotated corpora to automatically detect divergent structural patterns between two languages. These patterns can then be used to improve structural projection algorithms, allowing for better performing NLP tools for resource-poor languages, in particular those that may not have large amounts of annotated data necessary for traditional, fully-supervised methods. While this detection process is not exhaustive, we demonstrate that common patterns of divergence can be identified automatically without prior knowledge of a given language pair, and the patterns can be used to improve performance of projection algorithms. version:1
arxiv-1605-04466 | Generalized Linear Models for Aggregated Data | http://arxiv.org/abs/1605.04466 | id:1605.04466 author:Avradeep Bhowmik, Joydeep Ghosh, Oluwasanmi Koyejo category:stat.ML cs.AI cs.LG  published:2016-05-14 summary:Databases in domains such as healthcare are routinely released to the public in aggregated form. Unfortunately, naive modeling with aggregated data may significantly diminish the accuracy of inferences at the individual level. This paper addresses the scenario where features are provided at the individual level, but the target variables are only available as histogram aggregates or order statistics. We consider a limiting case of generalized linear modeling when the target variables are only known up to permutation, and explore how this relates to permutation testing; a standard technique for assessing statistical dependency. Based on this relationship, we propose a simple algorithm to estimate the model parameters and individual level inferences via alternating imputation and standard generalized linear model fitting. Our results suggest the effectiveness of the proposed approach when, in the original data, permutation testing accurately ascertains the veracity of the linear relationship. The framework is extended to general histogram data with larger bins - with order statistics such as the median as a limiting case. Our experimental results on simulated data and aggregated healthcare data suggest a diminishing returns property with respect to the granularity of the histogram - when a linear relationship holds in the original data, the targets can be predicted accurately given relatively coarse histograms. version:1
arxiv-1605-04465 | Monotone Retargeting for Unsupervised Rank Aggregation with Object Features | http://arxiv.org/abs/1605.04465 | id:1605.04465 author:Avradeep Bhowmik, Joydeep Ghosh category:stat.ML cs.AI cs.LG  published:2016-05-14 summary:Learning the true ordering between objects by aggregating a set of expert opinion rank order lists is an important and ubiquitous problem in many applications ranging from social choice theory to natural language processing and search aggregation. We study the problem of unsupervised rank aggregation where no ground truth ordering information in available, neither about the true preference ordering between any set of objects nor about the quality of individual rank lists. Aggregating the often inconsistent and poor quality rank lists in such an unsupervised manner is a highly challenging problem, and standard consensus-based methods are often ill-defined, and difficult to solve. In this manuscript we propose a novel framework to bypass these issues by using object attributes to augment the standard rank aggregation framework. We design algorithms that learn joint models on both rank lists and object features to obtain an aggregated rank ordering that is more accurate and robust, and also helps weed out rank lists of dubious validity. We validate our techniques on synthetic datasets where our algorithm is able to estimate the true rank ordering even when the rank lists are corrupted. Experiments on three real datasets, MQ2008, MQ2008 and OHSUMED, show that using object features can result in significant improvement in performance over existing rank aggregation methods that do not use object information. Furthermore, when at least some of the rank lists are of high quality, our methods are able to effectively exploit their high expertise to output an aggregated rank ordering of great accuracy. version:1
arxiv-1605-04462 | Natural Language Processing for Mental Health: Large Scale Discourse Analysis of Counseling Conversations | http://arxiv.org/abs/1605.04462 | id:1605.04462 author:Tim Althoff, Kevin Clark, Jure Leskovec category:cs.CL cs.CY cs.SI  published:2016-05-14 summary:Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes. version:1
arxiv-1605-04435 | Proceedings of the 5th Workshop on Machine Learning and Interpretation in Neuroimaging (MLINI) at NIPS 2015 | http://arxiv.org/abs/1605.04435 | id:1605.04435 author:I. Rish, L. Wehbe, G. Langs, M. Grosse-Wentrup, B. Murphy, G. Cecchi category:stat.ML  published:2016-05-14 summary:This volume is a collection of contributions from the 5th Workshop on Machine Learning and Interpretation in Neuroimaging (MLINI) at the Neural Information Processing Systems (NIPS 2015) conference. Modern multivariate statistical methods developed in the rapidly growing field of machine learning are being increasingly applied to various problems in neuroimaging, from cognitive state detection to clinical diagnosis and prognosis. Multivariate pattern analysis methods are designed to examine complex relationships between high-dimensional signals, such as brain images, and outcomes of interest, such as the category of a stimulus, a type of a mental state of a subject, or a specific mental disorder. Such techniques are in contrast with the traditional mass-univariate approaches that dominated neuroimaging in the past and treated each individual imaging measurement in isolation. We believe that machine learning has a prominent role in shaping how questions in neuroscience are framed, and that the machine-learning mind set is now entering modern psychology and behavioral studies. It is also equally important that practical applications in these fields motivate a rapidly evolving line or research in the machine learning community. In parallel, there is an intense interest in learning more about brain function in the context of rich naturalistic environments and scenes. Efforts to go beyond highly specific paradigms that pinpoint a single function, towards schemes for measuring the interaction with natural and more varied scene are made. The goal of the workshop is to pinpoint the most pressing issues and common challenges across the neuroscience, neuroimaging, psychology and machine learning fields, and to sketch future directions and open questions in the light of novel methodology. version:1
arxiv-1602-01052 | Better safe than sorry: Risky function exploitation through safe optimization | http://arxiv.org/abs/1602.01052 | id:1602.01052 author:Eric Schulz, Quentin J. M. Huys, Dominik R. Bach, Maarten Speekenbrink, Andreas Krause category:stat.AP cs.LG stat.ML  published:2016-02-02 summary:Exploration-exploitation of functions, that is learning and optimizing a mapping between inputs and expected outputs, is ubiquitous to many real world situations. These situations sometimes require us to avoid certain outcomes at all cost, for example because they are poisonous, harmful, or otherwise dangerous. We test participants' behavior in scenarios in which they have to find the optimum of a function while at the same time avoid outputs below a certain threshold. In two experiments, we find that Safe-Optimization, a Gaussian Process-based exploration-exploitation algorithm, describes participants' behavior well and that participants seem to care firstly whether a point is safe and then try to pick the optimal point from all such safe points. This means that their trade-off between exploration and exploitation can be seen as an intelligent, approximate, and homeostasis-driven strategy. version:2
arxiv-1406-3190 | Online Optimization for Large-Scale Max-Norm Regularization | http://arxiv.org/abs/1406.3190 | id:1406.3190 author:Jie Shen, Huan Xu, Ping Li category:stat.ML cs.LG  published:2014-06-12 summary:Max-norm regularizer has been extensively studied in the last decade as it promotes an effective low-rank estimation for the underlying data. However, such max-norm regularized problems are typically formulated and solved in a batch manner, which prevents it from processing big data due to possible memory budget. In this paper, hence, we propose an online algorithm that is scalable to large-scale setting. Particularly, we consider the matrix decomposition problem as an example, although a simple variant of the algorithm and analysis can be adapted to other important problems such as matrix completion. The crucial technique in our implementation is to reformulating the max-norm to an equivalent matrix factorization form, where the factors consist of a (possibly overcomplete) basis component and a coefficients one. In this way, we may maintain the basis component in the memory and optimize over it and the coefficients for each sample alternatively. Since the memory footprint of the basis component is independent of the sample size, our algorithm is appealing when manipulating a large collection of samples. We prove that the sequence of the solutions (i.e., the basis component) produced by our algorithm converges to a stationary point of the expected loss function asymptotically. Numerical study demonstrates encouraging results for the efficacy and robustness of our algorithm compared to the widely used nuclear norm solvers. version:4
arxiv-1605-04369 | Neural Dataset Generality | http://arxiv.org/abs/1605.04369 | id:1605.04369 author:Ragav Venkatesan, Vijetha Gattupalli, Baoxin Li category:cs.CV  published:2016-05-14 summary:Often the filters learned by Convolutional Neural Networks (CNNs) from different datasets appear similar. This is prominent in the first few layers. This similarity of filters is being exploited for the purposes of transfer learning and some studies have been made to analyse such transferability of features. This is also being used as an initialization technique for different tasks in the same dataset or for the same task in similar datasets. Off-the-shelf CNN features have capitalized on this idea to promote their networks as best transferable and most general and are used in a cavalier manner in day-to-day computer vision tasks. It is curious that while the filters learned by these CNNs are related to the atomic structures of the images from which they are learnt, all datasets learn similar looking low-level filters. With the understanding that a dataset that contains many such atomic structures learn general filters and are therefore useful to initialize other networks with, we propose a way to analyse and quantify generality among datasets from their accuracies on transferred filters. We applied this metric on several popular character recognition, natural image and a medical image dataset, and arrived at some interesting conclusions. On further experimentation we also discovered that particular classes in a dataset themselves are more general than others. version:1
arxiv-1602-01595 | Many Languages, One Parser | http://arxiv.org/abs/1602.01595 | id:1602.01595 author:Waleed Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, Noah A. Smith category:cs.CL  published:2016-02-04 summary:We train one multilingual model for dependency parsing and use it to parse sentences in several languages. The parsing model uses (i) multilingual word clusters and embeddings; (ii) token-level language information; and (iii) language-specific features (fine-grained POS tags). This input representation enables the parser not only to parse effectively in multiple languages, but also to generalize across languages based on linguistic universals and typological similarities, making it more effective to learn from limited annotations. Our parser's performance compares favorably to strong baselines in a range of data scenarios, including when the target language has a large treebank, a small treebank, or no treebank for training. version:3
arxiv-1605-04359 | Occurrence Statistics of Entities, Relations and Types on the Web | http://arxiv.org/abs/1605.04359 | id:1605.04359 author:Aman Madaan, Sunita Sarawagi category:cs.CL  published:2016-05-14 summary:The problem of collecting reliable estimates of occurrence of entities on the open web forms the premise for this report. The models learned for tagging entities cannot be expected to perform well when deployed on the web. This is owing to the severe mismatch in the distributions of such entities on the web and in the relatively diminutive training data. In this report, we build up the case for maximum mean discrepancy for estimation of occurrence statistics of entities on the web, taking a review of named entity disambiguation techniques and related concepts along the way. version:1
arxiv-1605-04337 | Support Vector Algorithms for Optimizing the Partial Area Under the ROC Curve | http://arxiv.org/abs/1605.04337 | id:1605.04337 author:Harikrishna Narasimhan, Shivani Agarwal category:cs.LG  published:2016-05-13 summary:The area under the ROC curve (AUC) is a widely used performance measure in machine learning. Increasingly, however, in several applications, ranging from ranking to biometric screening to medicine, performance is measured not in terms of the full area under the ROC curve, but in terms of the \emph{partial} area under the ROC curve between two false positive rates. In this paper, we develop support vector algorithms for directly optimizing the partial AUC between any two false positive rates. Our methods are based on minimizing a suitable proxy or surrogate objective for the partial AUC error. In the case of the full AUC, one can readily construct and optimize convex surrogates by expressing the performance measure as a summation of pairwise terms. The partial AUC, on the other hand, does not admit such a simple decomposable structure, making it more challenging to design and optimize (tight) convex surrogates for this measure. Our approach builds on the structural SVM framework of Joachims (2005) to design convex surrogates for partial AUC, and solves the resulting optimization problem using a cutting plane solver. Unlike the full AUC, where the combinatorial optimization needed in each iteration of the cutting plane solver can be decomposed and solved efficiently, the corresponding problem for the partial AUC is harder to decompose. One of our main contributions is a polynomial time algorithm for solving the combinatorial optimization problem associated with partial AUC. We also develop an approach for optimizing a tighter non-convex hinge loss based surrogate for the partial AUC using difference-of-convex programming. Our experiments on a variety of real-world and benchmark tasks confirm the efficacy of the proposed methods. version:1
arxiv-1603-04419 | Modeling and Estimation of Discrete-Time Reciprocal Processes via Probabilistic Graphical Models | http://arxiv.org/abs/1603.04419 | id:1603.04419 author:Francesca Paola Carli category:stat.ML math.OC  published:2016-03-14 summary:Reciprocal processes are acausal generalizations of Markov processes introduced by Bernstein in 1932. In the literature, a significant amount of attention has been focused on developing dynamical models for reciprocal processes. In this paper, we provide a probabilistic graphical model for reciprocal processes. This leads to a principled solution of the smoothing problem via message passing algorithms. For the finite state space case, convergence analysis is revisited via the Hilbert metric. version:3
arxiv-1605-04262 | ABtree: An Algorithm for Subgroup-Based Treatment Assignment | http://arxiv.org/abs/1605.04262 | id:1605.04262 author:Derek Feng, Xiaofei Wang category:stat.ML  published:2016-05-13 summary:Given two possible treatments, there may exist subgroups who benefit greater from one treatment than the other. This problem is relevant to the field of marketing, where treatments may correspond to different ways of selling a product. It is similarly relevant to the field of public policy, where treatments may correspond to specific government programs. And finally, personalized medicine is a field wholly devoted to understanding which subgroups of individuals will benefit from particular medical treatments. We present a computationally fast tree-based method, ABtree, for treatment effect differentiation. Unlike other methods, ABtree specifically produces decision rules for optimal treatment assignment on a per-individual basis. The treatment choices are selected for maximizing the overall occurrence of a desired binary outcome, conditional on a set of covariates. In this poster, we present the methodology on tree growth and pruning, and show performance results when applied to simulated data as well as real data. version:1
arxiv-1605-04253 | An Empirical Study and Analysis of Generalized Zero-Shot Learning for Object Recognition in the Wild | http://arxiv.org/abs/1605.04253 | id:1605.04253 author:Wei-Lun Chao, Soravit Changpinyo, Boqing Gong, Fei Sha category:cs.CV  published:2016-05-13 summary:We investigate the problem of generalized zero-shot learning (GZSL). GZSL relaxes the unrealistic assumption in conventional ZSL that test data belong only to unseen novel classes. In GZSL, test data might also come from seen classes and the labeling space is the union of both types of classes. We show empirically that a straightforward application of the classifiers provided by existing ZSL approaches does not perform well in the setting of GZSL. Motivated by this, we propose a surprisingly simple but effective method to adapt ZSL approaches for GZSL. The main idea is to introduce a calibration factor to calibrate the classifiers for both seen and unseen classes so as to balance two conflicting forces: recognizing data from seen classes and those from unseen ones. We develop a new performance metric called the Area Under Seen-Unseen accuracy Curve to characterize this tradeoff. We demonstrate the utility of this metric by analyzing existing ZSL approaches applied to the generalized setting. Extensive empirical studies reveal strengths and weaknesses of those approaches on three well-studied benchmark datasets, including the large-scale ImageNet Full 2011 with 21,000 unseen categories. We complement our comparative studies in learning methods by further establishing an upper-bound on the performance limit of GZSL. There, our idea is to use class-representative visual features as the idealized semantic embeddings. We show that there is a large gap between the performance of existing approaches and the performance limit, suggesting that improving the quality of class semantic embeddings is vital to improving zero-shot learning. version:1
arxiv-1605-04250 | Color Homography | http://arxiv.org/abs/1605.04250 | id:1605.04250 author:Graham Finlayson, Han Gong, Robert Fisher category:cs.CV  published:2016-05-13 summary:We show the surprising result that colors across a change in viewing condition (changing light color, shading and camera) are related by a homography. Our homography color correction application delivers improved color fidelity compared with the linear least-square. version:1
arxiv-1605-01988 | LSTM with Working Memory | http://arxiv.org/abs/1605.01988 | id:1605.01988 author:Andrew Pulver, Siwei Lyu category:cs.NE  published:2016-05-06 summary:LSTM is arguably the most successful RNN architecture for many tasks that involve sequential information. In the past few years there have been several proposed improvements to LSTM. We propose an improvement to LSTM which allows communication between memory cells in different blocks and allows an LSTM layer to carry out internal computation within its memory. version:2
arxiv-1509-08327 | Unbiased Bayesian Inference for Population Markov Jump Processes via Random Truncations | http://arxiv.org/abs/1509.08327 | id:1509.08327 author:Anastasis Georgoulas, Jane Hillston, Guido Sanguinetti category:stat.ML  published:2015-09-28 summary:We consider continuous time Markovian processes where populations of individual agents interact stochastically according to kinetic rules. Despite the increasing prominence of such models in fields ranging from biology to smart cities, Bayesian inference for such systems remains challenging, as these are continuous time, discrete state systems with potentially infinite state-space. Here we propose a novel efficient algorithm for joint state / parameter posterior sampling in population Markov Jump processes. We introduce a class of pseudo-marginal sampling algorithms based on a random truncation method which enables a principled treatment of infinite state spaces. Extensive evaluation on a number of benchmark models shows that this approach achieves considerable savings compared to state of the art methods, retaining accuracy and fast convergence. We also present results on a synthetic biology data set showing the potential for practical usefulness of our work. version:2
arxiv-1605-04243 | Simultaneous Surface Reflectance and Fluorescence Spectra Estimation | http://arxiv.org/abs/1605.04243 | id:1605.04243 author:Henryk Blasinski, Joyce Farrell, Brian Wandell category:cs.CV  published:2016-05-13 summary:There is widespread interest in estimating the fluorescence properties of natural materials in an image. However, the separation between reflected and fluoresced components is difficult, because it is impossible to distinguish reflected and fluoresced photons without controlling the illuminant spectrum. We show how to jointly estimate the reflectance and fluorescence from a single set of images acquired under multiple illuminants. We present a framework based on a linear approximation to the physical equations describing image formation in terms of surface spectral reflectance and fluorescence due to multiple fluorophores. We relax the non-convex, inverse estimation problem in order to jointly estimate the reflectance and fluorescence properties in a single optimization step and we use the Alternating Direction Method of Multipliers (ADMM) approach to efficiently find a solution. We provide a software implementation of the solver for our method and prior methods. We evaluate the accuracy and reliability of the method using both simulations and experimental data. To acquire data to test the methods, we built a custom imaging system using a monochrome camera, a filter wheel with bandpass transmissive filters and a small number of light emitting diodes. We compared the system and algorithm performance with the ground truth as well as with prior methods. Our approach produces lower errors compared to earlier algorithms. version:1
arxiv-1605-04238 | Semantic Spaces | http://arxiv.org/abs/1605.04238 | id:1605.04238 author:Yuri Manin, Matilde Marcolli category:cs.CL 68Q55  14M15  published:2016-05-13 summary:Any natural language can be considered as a tool for producing large databases (consisting of texts, written, or discursive). This tool for its description in turn requires other large databases (dictionaries, grammars etc.). Nowadays, the notion of database is associated with computer processing and computer memory. However, a natural language resides also in human brains and functions in human communication, from interpersonal to intergenerational one. We discuss in this survey/research paper mathematical, in particular geometric, constructions, which help to bridge these two worlds. In particular, in this paper we consider the Vector Space Model of semantics based on frequency matrices, as used in Natural Language Processing. We investigate underlying geometries, formulated in terms of Grassmannians, projective spaces, and flag varieties. We formulate the relation between vector space models and semantic spaces based on semic axes in terms of projectability of subvarieties in Grassmannians and projective spaces. We interpret Latent Semantics as a geometric flow on Grassmannians. We also discuss how to formulate G\"ardenfors' notion of "meeting of minds" in our geometric setting. version:1
arxiv-1512-08422 | Natural Language Inference by Tree-Based Convolution and Heuristic Matching | http://arxiv.org/abs/1512.08422 | id:1512.08422 author:Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, Zhi Jin category:cs.CL cs.LG  published:2015-12-28 summary:In this paper, we propose the TBCNN-pair model to recognize entailment and contradiction between two sentences. In our model, a tree-based convolutional neural network (TBCNN) captures sentence-level semantics; then heuristic matching layers like concatenation, element-wise product/difference combine the information in individual sentences. Experimental results show that our model outperforms existing sentence encoding-based approaches by a large margin. version:3
arxiv-1511-01158 | Distributed Deep Learning for Answer Selection | http://arxiv.org/abs/1511.01158 | id:1511.01158 author:Minwei Feng, Bing Xiang, Bowen Zhou category:cs.LG cs.CL cs.DC  published:2015-11-03 summary:This paper is an empirical study of the distributed deep learning for a question answering subtask: answer selection. Comparison studies of SGD, MSGD, DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results show that the message passing interface based distributed framework can accelerate the convergence speed at a sublinear scale. This paper demonstrates the importance of distributed training: with 120 workers, an 83x speedup is achievable and running time is decreased from 107.9 hours to 1.3 hours, which will benefit the productivity significantly. version:2
arxiv-1503-01673 | High Dimensional Bayesian Optimisation and Bandits via Additive Models | http://arxiv.org/abs/1503.01673 | id:1503.01673 author:Kirthevasan Kandasamy, Jeff Schneider, Barnabas Poczos category:stat.ML cs.LG  published:2015-03-05 summary:Bayesian Optimisation (BO) is a technique used in optimising a $D$-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on $D$ even though the function depends on all $D$ dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive. version:3
arxiv-1602-01255 | Learning scale-variant and scale-invariant features for deep image classification | http://arxiv.org/abs/1602.01255 | id:1602.01255 author:Nanne van Noord, Eric Postma category:cs.CV  published:2016-02-03 summary:Convolutional Neural Networks (CNNs) require large image corpora to be trained on classification tasks. The variation in image resolutions, sizes of objects and patterns depicted, and image scales, hampers CNN training and performance, because the task-relevant information varies over spatial scales. Previous work attempting to deal with such scale variations focused on encouraging scale-invariant CNN representations. However, scale-invariant representations are incomplete representations of images, because images contain scale-variant information as well. This paper addresses the combined development of scale-invariant and scale-variant representations. We propose a multi- scale CNN method to encourage the recognition of both types of features and evaluate it on a challenging image classification task involving task-relevant characteristics at multiple scales. The results show that our multi-scale CNN outperforms single-scale CNN. This leads to the conclusion that encouraging the combined development of a scale-invariant and scale-variant representation in CNNs is beneficial to image recognition performance. version:2
arxiv-1604-02910 | Deep Gate Recurrent Neural Network | http://arxiv.org/abs/1604.02910 | id:1604.02910 author:Yuan Gao, Dorota Glowacka category:cs.NE  published:2016-04-11 summary:This paper introduces two recurrent neural network structures called Simple Gated Unit (SGU) and Deep Simple Gated Unit (DSGU), which are general structures for learning long term dependencies. Compared to traditional Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), both structures require fewer parameters and less computation time in sequence classification tasks. Unlike GRU and LSTM, which require more than one gates to control information flow in the network, SGU and DSGU only use one multiplicative gate to control the flow of information. We show that this difference can accelerate the learning speed in tasks that require long dependency information. We also show that DSGU is more numerically stable than SGU. In addition, we also propose a standard way of representing inner structure of RNN called RNN Conventional Graph (RCG), which helps analyzing the relationship between input units and hidden units of RNN. version:3
arxiv-1604-06602 | Clustering with Missing Features: A Penalized Dissimilarity Measure based approach | http://arxiv.org/abs/1604.06602 | id:1604.06602 author:Shounak Datta, Supritam Bhattacharjee, Swagatam Das category:cs.LG 62H30  published:2016-04-22 summary:Many real-world clustering problems are plagued by incomplete data characterized by missing or absent features for some or all of the data instances. Traditional clustering methods cannot be directly applied to such data without preprocessing by imputation or marginalization techniques. In this article, we put forth the concept of Penalized Dissimilarity Measures which estimate the actual distance between two data points (the distance between them if they were to be fully observed) by adding a penalty to the distance due to the observed features common to both the instances. We then propose such a dissimilarity measure called the Feature Weighted Penalty based Dissimilarity (FWPD) measure. Using the proposed dissimilarity measure, we also modify the traditional k-means clustering algorithm and the standard hierarchical agglomerative clustering techniques so as to make them directly applicable to datasets with missing features. We present time complexity analyses for these new techniques and also present a detailed analysis showing that the new FWPD based k-means algorithm converges to a local optimum within a finite number of iterations. We have also conducted extensive experiments on various benchmark datasets showing that the proposed clustering techniques have generally better results compared to some of the popular imputation methods which are commonly used to handle such incomplete data. We have appended a possible extension of the proposed dissimilarity measure to the case of absent features (where the unobserved features are known to be non-existent). version:2
arxiv-1605-02971 | Structured Receptive Fields in CNNs | http://arxiv.org/abs/1605.02971 | id:1605.02971 author:Jörn-Henrik Jacobsen, Jan van Gemert, Zhongyu Lou, Arnold W. M. Smeulders category:cs.CV  published:2016-05-10 summary:Learning powerful feature representations with CNNs is hard when training data are limited. Pre-training is one way to overcome this, but it requires large datasets sufficiently similar to the target domain. Another option is to design priors into the model, which can range from tuned hyperparameters to fully engineered representations like Scattering Networks. We combine these ideas into structured receptive field networks, a model which has a fixed filter basis and yet retains the flexibility of CNNs. This flexibility is achieved by expressing receptive fields in CNNs as a weighted sum over a fixed basis which is similar in spirit to Scattering Networks. The key difference is that we learn arbitrary effective filter sets from the basis rather than modeling the filters. This approach explicitly connects classical multiscale image analysis with general CNNs. With structured receptive field networks, we improve considerably over unstructured CNNs for small and medium dataset scenarios as well as over Scattering for large datasets. We validate our findings on ILSVRC2012, Cifar-10, Cifar-100 and MNIST. As a realistic small dataset example, we show state-of-the-art classification results on popular 3D MRI brain-disease datasets where pre-training is difficult due to a lack of large public datasets in a similar domain. version:2
arxiv-1605-04129 | With Whom Do I Interact? Detecting Social Interactions in Egocentric Photo-streams | http://arxiv.org/abs/1605.04129 | id:1605.04129 author:Maedeh Aghaei, Mariella Dimiccoli, Petia Radeva category:cs.CV  published:2016-05-13 summary:Given a user wearing a low frame rate wearable camera during a day, this work aims to automatically detect the moments when the user gets engaged into a social interaction solely by reviewing the automatically captured photos by the worn camera. The proposed method, inspired by the sociological concept of F-formation, exploits distance and orientation of the appearing individuals -with respect to the user- in the scene from a bird-view perspective. As a result, the interaction pattern over the sequence can be understood as a two-dimensional time series that corresponds to the temporal evolution of the distance and orientation features over time. A Long-Short Term Memory-based Recurrent Neural Network is then trained to classify each time series. Experimental evaluation over a dataset of 30.000 images has shown promising results on the proposed method for social interaction detection in egocentric photo-streams. version:1
arxiv-1605-04122 | Natural Language Semantics and Computability | http://arxiv.org/abs/1605.04122 | id:1605.04122 author:Richard Moot, Christian Retoré category:cs.CL cs.AI cs.CC  published:2016-05-13 summary:This paper is a reflexion on the computability of natural language semantics. It does not contain a new model or new results in the formal semantics of natural language: it is rather a computational analysis of the logical models and algorithms currently used in natural language semantics, defined as the mapping of a statement to logical formulas - formulas, because a statement can be ambiguous. We argue that as long as possible world semantics is left out, one can compute the semantic representation(s) of a given statement, including aspects of lexical meaning. We also discuss the algorithmic complexity of this process. version:1
arxiv-1604-01250 | Fast methods for training Gaussian processes on large data sets | http://arxiv.org/abs/1604.01250 | id:1604.01250 author:Christopher J. Moore, Alvin J. K. Chua, Christopher P. L. Berry, Jonathan R. Gair category:stat.ML stat.CO stat.ME  published:2016-04-05 summary:Gaussian process regression (GPR) is a non-parametric Bayesian technique for interpolating or fitting data. The main barrier to further uptake of this powerful tool rests in the computational costs associated with the matrices which arise when dealing with large data sets. Here, we derive some simple results which we have found useful for speeding up the learning stage in the GPR algorithm, and especially for performing Bayesian model comparison between different covariance functions. We apply our techniques to both synthetic and real data and quantify the speed-up relative to using nested sampling to numerically evaluate model evidences. version:2
arxiv-1605-04074 | Wisdom of Crowds cluster ensemble | http://arxiv.org/abs/1605.04074 | id:1605.04074 author:Hosein Alizadeh, Muhammad Yousefnezhad, Behrouz Minaei Bidgoli category:stat.ML cs.AI cs.SI  published:2016-05-13 summary:The Wisdom of Crowds is a phenomenon described in social science that suggests four criteria applicable to groups of people. It is claimed that, if these criteria are satisfied, then the aggregate decisions made by a group will often be better than those of its individual members. Inspired by this concept, we present a novel feedback framework for the cluster ensemble problem, which we call Wisdom of Crowds Cluster Ensemble (WOCCE). Although many conventional cluster ensemble methods focusing on diversity have recently been proposed, WOCCE analyzes the conditions necessary for a crowd to exhibit this collective wisdom. These include decentralization criteria for generating primary results, independence criteria for the base algorithms, and diversity criteria for the ensemble members. We suggest appropriate procedures for evaluating these measures, and propose a new measure to assess the diversity. We evaluate the performance of WOCCE against some other traditional base algorithms as well as state-of-the-art ensemble methods. The results demonstrate the efficiency of WOCCE's aggregate decision-making compared to other algorithms. version:1
