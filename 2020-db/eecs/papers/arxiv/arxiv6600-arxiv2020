arxiv-1312-7715 | Constrained Parametric Proposals and Pooling Methods for Semantic Segmentation in RGB-D Images | http://arxiv.org/abs/1312.7715 | id:1312.7715 author:Dan Banica, Cristian Sminchisescu category:cs.CV  published:2013-12-30 summary:We focus on the problem of semantic segmentation based on RGB-D data, with emphasis on analyzing cluttered indoor scenes containing many instances from many visual categories. Our approach is based on a parametric figure-ground intensity and depth-constrained proposal process that generates spatial layout hypotheses at multiple locations and scales in the image followed by a sequential inference algorithm that integrates the proposals into a complete scene estimate. Our contributions can be summarized as proposing the following: (1) a generalization of parametric max flow figure-ground proposal methodology to take advantage of intensity and depth information, in order to systematically and efficiently generate the breakpoints of an underlying spatial model in polynomial time, (2) new region description methods based on second-order pooling over multiple features constructed using both intensity and depth channels, (3) an inference procedure that can resolve conflicts in overlapping spatial partitions, and handles scenes with a large number of objects category instances, of very different scales, (4) extensive evaluation of the impact of depth, as well as the effectiveness of a large number of descriptors, both pre-designed and automatically obtained using deep learning, in a difficult RGB-D semantic segmentation problem with 92 classes. We report state of the art results in the challenging NYU Depth v2 dataset, extended for RMRC 2013 Indoor Segmentation Challenge, where currently the proposed model ranks first, with an average score of 24.61% and a number of 39 classes won. Moreover, we show that by combining second-order and deep learning features, over 15% relative accuracy improvements can be additionally achieved. In a scene classification benchmark, our methodology further improves the state of the art by 24%. version:2
arxiv-1407-8337 | A New Model of Array Grammar for generating Connected Patterns on an Image Neighborhood | http://arxiv.org/abs/1407.8337 | id:1407.8337 author:G. Vishnu Murthy, Pavan Kumar C., Vakulabharanam Vijaya Kumar category:cs.FL cs.CV  published:2014-07-31 summary:Study of patterns on images is recognized as an important step in characterization and classification of image. The ability to efficiently analyze and describe image patterns is thus of fundamental importance. The study of syntactic methods of describing pictures has been of interest for researchers. Array Grammars can be used to represent and recognize connected patterns. In any image the patterns are recognized using connected patterns. It is very difficult to represent all connected patterns (CP) even on a small 3 x 3 neighborhood in a pictorial way. The present paper proposes the model of array grammar capable of generating any kind of simple or complex pattern and derivation of connected pattern in an image neighborhood using the proposed grammar is discussed. version:1
arxiv-1311-2547 | Learning Mixtures of Linear Classifiers | http://arxiv.org/abs/1311.2547 | id:1311.2547 author:Yuekai Sun, Stratis Ioannidis, Andrea Montanari category:cs.LG stat.ML  published:2013-11-11 summary:We consider a discriminative learning (regression) problem, whereby the regression function is a convex combination of k linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a `mirroring' trick, that discovers the subspace spanned by the classifiers' parameter vectors. Under a probabilistic assumption on the feature vector distribution, we prove that this approach has nearly optimal statistical efficiency. version:4
arxiv-1311-6802 | Recommending with an Agenda: Active Learning of Private Attributes using Matrix Factorization | http://arxiv.org/abs/1311.6802 | id:1311.6802 author:Smriti Bhagat, Udi Weinsberg, Stratis Ioannidis, Nina Taft category:cs.LG cs.CY  published:2013-11-26 summary:Recommender systems leverage user demographic information, such as age, gender, etc., to personalize recommendations and better place their targeted ads. Oftentimes, users do not volunteer this information due to privacy concerns, or due to a lack of initiative in filling out their online profiles. We illustrate a new threat in which a recommender learns private attributes of users who do not voluntarily disclose them. We design both passive and active attacks that solicit ratings for strategically selected items, and could thus be used by a recommender system to pursue this hidden agenda. Our methods are based on a novel usage of Bayesian matrix factorization in an active learning setting. Evaluations on multiple datasets illustrate that such attacks are indeed feasible and use significantly fewer rated items than static inference methods. Importantly, they succeed without sacrificing the quality of recommendations to users. version:2
arxiv-1407-8215 | Two-pass Discourse Segmentation with Pairing and Global Features | http://arxiv.org/abs/1407.8215 | id:1407.8215 author:Vanessa Wei Feng, Graeme Hirst category:cs.CL  published:2014-07-30 summary:Previous attempts at RST-style discourse segmentation typically adopt features centered on a single token to predict whether to insert a boundary before that token. In contrast, we develop a discourse segmenter utilizing a set of pairing features, which are centered on a pair of adjacent tokens in the sentence, by equally taking into account the information from both tokens. Moreover, we propose a novel set of global features, which encode characteristics of the segmentation as a whole, once we have an initial segmentation. We show that both the pairing and global features are useful on their own, and their combination achieved an $F_1$ of 92.6% of identifying in-sentence discourse boundaries, which is a 17.8% error-rate reduction over the state-of-the-art performance, approaching 95% of human performance. In addition, similar improvement is observed across different classification frameworks. version:1
arxiv-1407-8187 | Fast Bayesian Feature Selection for High Dimensional Linear Regression in Genomics via the Ising Approximation | http://arxiv.org/abs/1407.8187 | id:1407.8187 author:Charles K. Fisher, Pankaj Mehta category:q-bio.QM cs.LG stat.ML  published:2014-07-30 summary:Feature selection, identifying a subset of variables that are relevant for predicting a response, is an important and challenging component of many methods in statistics and machine learning. Feature selection is especially difficult and computationally intensive when the number of variables approaches or exceeds the number of samples, as is often the case for many genomic datasets. Here, we introduce a new approach -- the Bayesian Ising Approximation (BIA) -- to rapidly calculate posterior probabilities for feature relevance in L2 penalized linear regression. In the regime where the regression problem is strongly regularized by the prior, we show that computing the marginal posterior probabilities for features is equivalent to computing the magnetizations of an Ising model. Using a mean field approximation, we show it is possible to rapidly compute the feature selection path described by the posterior probabilities as a function of the L2 penalty. We present simulations and analytical results illustrating the accuracy of the BIA on some simple regression problems. Finally, we demonstrate the applicability of the BIA to high dimensional regression by analyzing a gene expression dataset with nearly 30,000 features. version:1
arxiv-1407-8123 | Merging and Shifting of Images with Prominence Coefficient for Predictive Analysis using Combined Image | http://arxiv.org/abs/1407.8123 | id:1407.8123 author:T. R. Gopalakrishnan Nair, Richa Sharma category:cs.CV  published:2014-07-30 summary:Shifting of objects in an image and merging many images after appropriate shifting is being used in several engineering and scientific applications which require complex perception development. A method has been presented here which could be used in precision engineering and biological applications where more precise prediction is required of a combined phenomenon with varying prominence of each phenomenon. Accurate merging of intended pixels can be achieved in high quality using frequency domain techniques even though initial properties of the original pixels are lost in this process. This paper introduces a technique to shift and merge various images with varying prominence of each image. A coefficient named prominence coefficient has been introduced which is capable of making some of the images transparent and highlighting the rest as per requirement of merging process which can be used as a simple but effective technique for overlapped view of a set of images. version:1
arxiv-1407-8121 | Clustering Approach Towards Image Segmentation: An Analytical Study | http://arxiv.org/abs/1407.8121 | id:1407.8121 author:Dibya Jyoti Bora, Anil Kumar Gupta category:cs.CV  published:2014-07-30 summary:Image processing is an important research area in computer vision. Image segmentation plays the vital rule in image processing research. There exist so many methods for image segmentation. Clustering is an unsupervised study. Clustering can also be used for image segmentation. In this paper, an in-depth study is done on different clustering techniques that can be used for image segmentation with their pros and cons. An experiment for color image segmentation based on clustering with K-Means algorithm is performed to observe the accuracy of clustering technique for the segmentation purpose. version:1
arxiv-1407-8088 | The Grow-Shrink strategy for learning Markov network structures constrained by context-specific independences | http://arxiv.org/abs/1407.8088 | id:1407.8088 author:Alejandro Edera, Yanela Strappa, Facundo Bromberg category:cs.LG cs.DS  published:2014-07-30 summary:Markov networks are models for compactly representing complex probability distributions. They are composed by a structure and a set of numerical weights. The structure qualitatively describes independences in the distribution, which can be exploited to factorize the distribution into a set of compact functions. A key application for learning structures from data is to automatically discover knowledge. In practice, structure learning algorithms focused on "knowledge discovery" present a limitation: they use a coarse-grained representation of the structure. As a result, this representation cannot describe context-specific independences. Very recently, an algorithm called CSPC was designed to overcome this limitation, but it has a high computational complexity. This work tries to mitigate this downside presenting CSGS, an algorithm that uses the Grow-Shrink strategy for reducing unnecessary computations. On an empirical evaluation, the structures learned by CSGS achieve competitive accuracies and lower computational complexity with respect to those obtained by CSPC. version:1
arxiv-1407-8067 | Differentially-Private Logistic Regression for Detecting Multiple-SNP Association in GWAS Databases | http://arxiv.org/abs/1407.8067 | id:1407.8067 author:Fei Yu, Michal Rybar, Caroline Uhler, Stephen E. Fienberg category:stat.ML cs.LG stat.AP 62P10  published:2014-07-30 summary:Following the publication of an attack on genome-wide association studies (GWAS) data proposed by Homer et al., considerable attention has been given to developing methods for releasing GWAS data in a privacy-preserving way. Here, we develop an end-to-end differentially private method for solving regression problems with convex penalty functions and selecting the penalty parameters by cross-validation. In particular, we focus on penalized logistic regression with elastic-net regularization, a method widely used to in GWAS analyses to identify disease-causing genes. We show how a differentially private procedure for penalized logistic regression with elastic-net regularization can be applied to the analysis of GWAS data and evaluate our method's performance. version:1
arxiv-1407-8042 | Targeting Optimal Active Learning via Example Quality | http://arxiv.org/abs/1407.8042 | id:1407.8042 author:Lewis P. G. Evans, Niall M. Adams, Christoforos Anagnostopoulos category:stat.ML cs.LG  published:2014-07-30 summary:In many classification problems unlabelled data is abundant and a subset can be chosen for labelling. This defines the context of active learning (AL), where methods systematically select that subset, to improve a classifier by retraining. Given a classification problem, and a classifier trained on a small number of labelled examples, consider the selection of a single further example. This example will be labelled by the oracle and then used to retrain the classifier. This example selection raises a central question: given a fully specified stochastic description of the classification problem, which example is the optimal selection? If optimality is defined in terms of loss, this definition directly produces expected loss reduction (ELR), a central quantity whose maximum yields the optimal example selection. This work presents a new theoretical approach to AL, example quality, which defines optimal AL behaviour in terms of ELR. Once optimal AL behaviour is defined mathematically, reasoning about this abstraction provides insights into AL. In a theoretical context the optimal selection is compared to existing AL methods, showing that heuristics can make sub-optimal selections. Algorithms are constructed to estimate example quality directly. A large-scale experimental study shows these algorithms to be competitive with standard AL methods. version:1
arxiv-1312-3240 | Associative embeddings for large-scale knowledge transfer with self-assessment | http://arxiv.org/abs/1312.3240 | id:1312.3240 author:Alexander Vezhnevets, Vittorio Ferrari category:cs.CV  published:2013-12-11 summary:We propose a method for knowledge transfer between semantically related classes in ImageNet. By transferring knowledge from the images that have bounding-box annotations to the others, our method is capable of automatically populating ImageNet with many more bounding-boxes and even pixel-level segmentations. The underlying assumption that objects from semantically related classes look alike is formalized in our novel Associative Embedding (AE) representation. AE recovers the latent low-dimensional space of appearance variations among image windows. The dimensions of AE space tend to correspond to aspects of window appearance (e.g. side view, close up, background). We model the overlap of a window with an object using Gaussian Processes (GP) regression, which spreads annotation smoothly through AE space. The probabilistic nature of GP allows our method to perform self-assessment, i.e. assigning a quality estimate to its own output. It enables trading off the amount of returned annotations for their quality. A large scale experiment on 219 classes and 0.5 million images demonstrates that our method outperforms state-of-the-art methods and baselines for both object localization and segmentation. Using self-assessment we can automatically return bounding-box annotations for 30% of all images with high localization accuracy (i.e.~73% average overlap with ground-truth). version:2
arxiv-1407-7969 | Automated Machine Learning on Big Data using Stochastic Algorithm Tuning | http://arxiv.org/abs/1407.7969 | id:1407.7969 author:Thomas Nickson, Michael A Osborne, Steven Reece, Stephen J Roberts category:stat.ML  published:2014-07-30 summary:We introduce a means of automating machine learning (ML) for big data tasks, by performing scalable stochastic Bayesian optimisation of ML algorithm parameters and hyper-parameters. More often than not, the critical tuning of ML algorithm parameters has relied on domain expertise from experts, along with laborious hand-tuning, brute search or lengthy sampling runs. Against this background, Bayesian optimisation is finding increasing use in automating parameter tuning, making ML algorithms accessible even to non-experts. However, the state of the art in Bayesian optimisation is incapable of scaling to the large number of evaluations of algorithm performance required to fit realistic models to complex, big data. We here describe a stochastic, sparse, Bayesian optimisation strategy to solve this problem, using many thousands of noisy evaluations of algorithm performance on subsets of data in order to effectively train algorithms for big data. We provide a comprehensive benchmarking of possible sparsification strategies for Bayesian optimisation, concluding that a Nystrom approximation offers the best scaling and performance for real tasks. Our proposed algorithm demonstrates substantial improvement over the state of the art in tuning the parameters of a Gaussian Process time series prediction task on real, big data. version:1
arxiv-1407-8176 | Accurate merging of images for predictive analysis using combined image | http://arxiv.org/abs/1407.8176 | id:1407.8176 author:T. R. Gopalakrishnan Nair, Richa Sharma category:cs.CV  published:2014-07-30 summary:Several Scientific and engineering applications require merging of sampled images for complex perception development. In most cases, for such requirements, images are merged at intensity level. Even though it gives fairly good perception of combined scenario of objects and scenes, it is found that they are not sufficient enough to analyze certain engineering cases. The main problem is incoherent modulation of intensity arising out of phase properties being lost. In order to compensate these losses, combined phase and amplitude merge is demanded. We present here a method which could be used in precision engineering and biological applications where more precise prediction is required of a combined phenomenon. When pixels are added, its original property is lost but accurate merging of intended pixels can be achieved in high quality using frequency domain properties of an image. This paper introduces a technique to merge various images which can be used as a simple but effective technique for overlapped view of a set of images and producing reduced dataset for review purposes. version:1
arxiv-1407-7937 | Learning Economic Parameters from Revealed Preferences | http://arxiv.org/abs/1407.7937 | id:1407.7937 author:Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, Vijay V. Vazirani category:cs.GT cs.LG  published:2014-07-30 summary:A recent line of work, starting with Beigman and Vohra (2006) and Zadimoghaddam and Roth (2012), has addressed the problem of {\em learning} a utility function from revealed preference data. The goal here is to make use of past data describing the purchases of a utility maximizing agent when faced with certain prices and budget constraints in order to produce a hypothesis function that can accurately forecast the {\em future} behavior of the agent. In this work we advance this line of work by providing sample complexity guarantees and efficient algorithms for a number of important classes. By drawing a connection to recent advances in multi-class learning, we provide a computationally efficient algorithm with tight sample complexity guarantees ($\Theta(d/\epsilon)$ for the case of $d$ goods) for learning linear utility functions under a linear price model. This solves an open question in Zadimoghaddam and Roth (2012). Our technique yields numerous generalizations including the ability to learn other well-studied classes of utility functions, to deal with a misspecified model, and with non-linear prices. version:1
arxiv-1309-3832 | Sequential Design for Optimal Stopping Problems | http://arxiv.org/abs/1309.3832 | id:1309.3832 author:Robert B. Gramacy, Mike Ludkovski category:q-fin.CP q-fin.PR stat.ML  published:2013-09-16 summary:We propose a new approach to solve optimal stopping problems via simulation. Working within the backward dynamic programming/Snell envelope framework, we augment the methodology of Longstaff-Schwartz that focuses on approximating the stopping strategy. Namely, we introduce adaptive generation of the stochastic grids anchoring the simulated sample paths of the underlying state process. This allows for active learning of the classifiers partitioning the state space into the continuation and stopping regions. To this end, we examine sequential design schemes that adaptively place new design points close to the stopping boundaries. We then discuss dynamic regression algorithms that can implement such recursive estimation and local refinement of the classifiers. The new algorithm is illustrated with a variety of numerical experiments, showing that an order of magnitude savings in terms of design size can be achieved. We also compare with existing benchmarks in the context of pricing multi-dimensional Bermudan options. version:2
arxiv-1407-7840 | Bayesian Probabilistic Matrix Factorization: A User Frequency Analysis | http://arxiv.org/abs/1407.7840 | id:1407.7840 author:Cody Severinski, Ruslan Salakhutdinov category:stat.ML  published:2014-07-29 summary:Matrix factorization (MF) has become a common approach to collaborative filtering, due to ease of implementation and scalability to large data sets. Two existing drawbacks of the basic model is that it does not incorporate side information on either users or items, and assumes a common variance for all users. We extend the work of constrained probabilistic matrix factorization by deriving the Gibbs updates for the side feature vectors for items (Salakhutdinov and Minh, 2008). We show that this Bayesian treatment to the constrained PMF model outperforms simple MAP estimation. We also consider extensions to heteroskedastic precision introduced in the literature (Lakshminarayanan, Bouchard, and Archambeau, 2011). We show that this tends result in overfitting for deterministic approximation algorithms (ex: Variational inference) when the observed entries in the user / item matrix are distributed in an non-uniform manner. In light of this, we propose a truncated precision model. Our experimental results suggest that this model tends to delay overfitting. version:1
arxiv-1407-7819 | Sure Screening for Gaussian Graphical Models | http://arxiv.org/abs/1407.7819 | id:1407.7819 author:Shikai Luo, Rui Song, Daniela Witten category:stat.ML cs.LG  published:2014-07-29 summary:We propose {graphical sure screening}, or GRASS, a very simple and computationally-efficient screening procedure for recovering the structure of a Gaussian graphical model in the high-dimensional setting. The GRASS estimate of the conditional dependence graph is obtained by thresholding the elements of the sample covariance matrix. The proposed approach possesses the sure screening property: with very high probability, the GRASS estimated edge set contains the true edge set. Furthermore, with high probability, the size of the estimated edge set is controlled. We provide a choice of threshold for GRASS that can control the expected false positive rate. We illustrate the performance of GRASS in a simulation study and on a gene expression data set, and show that in practice it performs quite competitively with more complex and computationally-demanding techniques for graph estimation. version:1
arxiv-1404-1592 | The Power of Online Learning in Stochastic Network Optimization | http://arxiv.org/abs/1404.1592 | id:1404.1592 author:Longbo Huang, Xin Liu, Xiaohong Hao category:math.OC cs.LG cs.SY  published:2014-04-06 summary:In this paper, we investigate the power of online learning in stochastic network optimization with unknown system statistics {\it a priori}. We are interested in understanding how information and learning can be efficiently incorporated into system control techniques, and what are the fundamental benefits of doing so. We propose two \emph{Online Learning-Aided Control} techniques, $\mathtt{OLAC}$ and $\mathtt{OLAC2}$, that explicitly utilize the past system information in current system control via a learning procedure called \emph{dual learning}. We prove strong performance guarantees of the proposed algorithms: $\mathtt{OLAC}$ and $\mathtt{OLAC2}$ achieve the near-optimal $[O(\epsilon), O([\log(1/\epsilon)]^2)]$ utility-delay tradeoff and $\mathtt{OLAC2}$ possesses an $O(\epsilon^{-2/3})$ convergence time. $\mathtt{OLAC}$ and $\mathtt{OLAC2}$ are probably the first algorithms that simultaneously possess explicit near-optimal delay guarantee and sub-linear convergence time. Simulation results also confirm the superior performance of the proposed algorithms in practice. To the best of our knowledge, our attempt is the first to explicitly incorporate online learning into stochastic network optimization and to demonstrate its power in both theory and practice. version:2
arxiv-1407-7753 | A Hash-based Co-Clustering Algorithm for Categorical Data | http://arxiv.org/abs/1407.7753 | id:1407.7753 author:Fabricio Olivetti de França category:cs.LG  published:2014-07-29 summary:Many real-life data are described by categorical attributes without a pre-classification. A common data mining method used to extract information from this type of data is clustering. This method group together the samples from the data that are more similar than all other samples. But, categorical data pose a challenge when extracting information because: the calculation of two objects similarity is usually done by measuring the number of common features, but ignore a possible importance weighting; if the data may be divided differently according to different subsets of the features, the algorithm may find clusters with different meanings from each other, difficulting the post analysis. Data Co-Clustering of categorical data is the technique that tries to find subsets of samples that share a subset of features in common. By doing so, not only a sample may belong to more than one cluster but, the feature selection of each cluster describe its own characteristics. In this paper a novel Co-Clustering technique for categorical data is proposed by using Locality Sensitive Hashing technique in order to preprocess a list of Co-Clusters seeds based on a previous research. Results indicate this technique is capable of finding high quality Co-Clusters in many different categorical data sets and scales linearly with the data set size. version:1
arxiv-1407-7737 | A CUDA-Based Real Parameter Optimization Benchmark | http://arxiv.org/abs/1407.7737 | id:1407.7737 author:Ke Ding, Ying Tan category:cs.NE  published:2014-07-29 summary:Benchmarking is key for developing and comparing optimization algorithms. In this paper, a CUDA-based real parameter optimization benchmark (cuROB) is introduced. Test functions of diverse properties are included within cuROB and implemented efficiently with CUDA. Speedup of one order of magnitude can be achieved in comparison with CPU-based benchmark of CEC'14. version:1
arxiv-1407-7736 | A Latent Space Analysis of Editor Lifecycles in Wikipedia | http://arxiv.org/abs/1407.7736 | id:1407.7736 author:Xiangju Qin, Derek Greene, Pádraig Cunningham category:cs.SI cs.CL cs.CY physics.soc-ph  published:2014-07-29 summary:Collaborations such as Wikipedia are a key part of the value of the modern Internet. At the same time there is concern that these collaborations are threatened by high levels of member turnover. In this paper we borrow ideas from topic analysis to editor activity on Wikipedia over time into a latent space that offers an insight into the evolving patterns of editor behavior. This latent space representation reveals a number of different categories of editor (e.g. content experts, social networkers) and we show that it does provide a signal that predicts an editor's departure from the community. We also show that long term editors gradually diversify their participation by shifting edit preference from one or two namespaces to multiple namespaces and experience relatively soft evolution in their editor profiles, while short term editors generally distribute their contribution randomly among the namespaces and experience considerably fluctuated evolution in their editor profiles. version:1
arxiv-1407-7691 | NMF with Sparse Regularizations in Transformed Domains | http://arxiv.org/abs/1407.7691 | id:1407.7691 author:Jérémy Rapin, Jérôme Bobin, Anthony Larue, Jean-Luc Starck category:stat.ML cs.LG 94A12 I.5.4  published:2014-07-29 summary:Non-negative blind source separation (non-negative BSS), which is also referred to as non-negative matrix factorization (NMF), is a very active field in domains as different as astrophysics, audio processing or biomedical signal processing. In this context, the efficient retrieval of the sources requires the use of signal priors such as sparsity. If NMF has now been well studied with sparse constraints in the direct domain, only very few algorithms can encompass non-negativity together with sparsity in a transformed domain since simultaneously dealing with two priors in two different domains is challenging. In this article, we show how a sparse NMF algorithm coined non-negative generalized morphological component analysis (nGMCA) can be extended to impose non-negativity in the direct domain along with sparsity in a transformed domain, with both analysis and synthesis formulations. To our knowledge, this work presents the first comparison of analysis and synthesis priors ---as well as their reweighted versions--- in the context of blind source separation. Comparisons with state-of-the-art NMF algorithms on realistic data show the efficiency as well as the robustness of the proposed algorithms. version:1
arxiv-1407-7686 | Hyperspectral Imaging and Analysis for Sparse Reconstruction and Recognition | http://arxiv.org/abs/1407.7686 | id:1407.7686 author:Zohaib Khan category:cs.CV  published:2014-07-29 summary:This thesis proposes spatio-spectral techniques for hyperspectral image analysis. Adaptive spatio-spectral support and variable exposure hyperspectral imaging is demonstrated to improve spectral reflectance recovery from hyperspectral images. Novel spectral dimensionality reduction techniques have been proposed from the perspective of spectral only and spatio-spectral information preservation. It was found that the joint sparse and joint group sparse hyperspectral image models achieve lower reconstruction error and higher recognition accuracy using only a small subset of bands. Hyperspectral image databases have been developed and made publicly available for further research in compressed hyperspectral imaging, forensic document analysis and spectral reflectance recovery. version:1
arxiv-1407-7663 | Level-based Analysis of Genetic Algorithms and other Search Processes | http://arxiv.org/abs/1407.7663 | id:1407.7663 author:Dogan Corus, Duc-Cuong Dang, Anton V. Eremeev, Per Kristian Lehre category:cs.NE q-bio.PE  published:2014-07-29 summary:The fitness-level technique is a simple and old way to derive upper bounds for the expected runtime of simple elitist evolutionary algorithms (EAs). Recently, the technique has been adapted to deduce the runtime of algorithms with non-elitist populations and unary variation operators. In this paper, we show that the restriction to unary variation operators can be removed. This gives rise to a much more general analytical tool which is applicable to a wide range of search processes. As introductory examples, we provide simple runtime analyses of many variants of the Genetic Algorithm on well-known benchmark functions, such as OneMax, LeadingOnes, and the sorting problem. version:1
arxiv-1407-7635 | Chasing Ghosts: Competing with Stateful Policies | http://arxiv.org/abs/1407.7635 | id:1407.7635 author:Uriel Feige, Tomer Koren, Moshe Tennenholtz category:cs.LG  published:2014-07-29 summary:We consider sequential decision making in a setting where regret is measured with respect to a set of stateful reference policies, and feedback is limited to observing the rewards of the actions performed (the so called "bandit" setting). If either the reference policies are stateless rather than stateful, or the feedback includes the rewards of all actions (the so called "expert" setting), previous work shows that the optimal regret grows like $\Theta(\sqrt{T})$ in terms of the number of decision rounds $T$. The difficulty in our setting is that the decision maker unavoidably loses track of the internal states of the reference policies, and thus cannot reliably attribute rewards observed in a certain round to any of the reference policies. In fact, in this setting it is impossible for the algorithm to estimate which policy gives the highest (or even approximately highest) total reward. Nevertheless, we design an algorithm that achieves expected regret that is sublinear in $T$, of the form $O( T/\log^{1/4}{T})$. Our algorithm is based on a certain local repetition lemma that may be of independent interest. We also show that no algorithm can guarantee expected regret better than $O( T/\log^{3/2} T)$. version:1
arxiv-1407-7626 | A Survey on Two Dimensional Cellular Automata and Its Application in Image Processing | http://arxiv.org/abs/1407.7626 | id:1407.7626 author:Deepak Ranjan Nayak, Prashanta Kumar Patra, Amitav Mahapatra category:cs.CV  published:2014-07-29 summary:Parallel algorithms for solving any image processing task is a highly demanded approach in the modern world. Cellular Automata (CA) are the most common and simple models of parallel computation. So, CA has been successfully used in the domain of image processing for the last couple of years. This paper provides a survey of available literatures of some methodologies employed by different researchers to utilize the cellular automata for solving some important problems of image processing. The survey includes some important image processing tasks such as rotation, zooming, translation, segmentation, edge detection, compression and noise reduction of images. Finally, the experimental results of some methodologies are presented. version:1
arxiv-1310-7991 | Learning Sparsely Used Overcomplete Dictionaries via Alternating Minimization | http://arxiv.org/abs/1310.7991 | id:1310.7991 author:Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli category:cs.LG math.OC stat.ML  published:2013-10-30 summary:We consider the problem of sparse coding, where each sample consists of a sparse linear combination of a set of dictionary atoms, and the task is to learn both the dictionary elements and the mixing coefficients. Alternating minimization is a popular heuristic for sparse coding, where the dictionary and the coefficients are estimated in alternate steps, keeping the other fixed. Typically, the coefficients are estimated via $\ell_1$ minimization, keeping the dictionary fixed, and the dictionary is estimated through least squares, keeping the coefficients fixed. In this paper, we establish local linear convergence for this variant of alternating minimization and establish that the basin of attraction for the global optimum (corresponding to the true dictionary and the coefficients) is $\order{1/s^2}$, where $s$ is the sparsity level in each sample and the dictionary satisfies RIP. Combined with the recent results of approximate dictionary estimation, this yields provable guarantees for exact recovery of both the dictionary elements and the coefficients, when the dictionary elements are incoherent. version:2
arxiv-1407-7584 | Dynamic Feature Scaling for Online Learning of Binary Classifiers | http://arxiv.org/abs/1407.7584 | id:1407.7584 author:Danushka Bollegala category:cs.LG stat.ML  published:2014-07-28 summary:Scaling feature values is an important step in numerous machine learning tasks. Different features can have different value ranges and some form of a feature scaling is often required in order to learn an accurate classifier. However, feature scaling is conducted as a preprocessing task prior to learning. This is problematic in an online setting because of two reasons. First, it might not be possible to accurately determine the value range of a feature at the initial stages of learning when we have observed only a few number of training instances. Second, the distribution of data can change over the time, which render obsolete any feature scaling that we perform in a pre-processing step. We propose a simple but an effective method to dynamically scale features at train time, thereby quickly adapting to any changes in the data stream. We compare the proposed dynamic feature scaling method against more complex methods for estimating scaling parameters using several benchmark datasets for binary classification. Our proposed feature scaling method consistently outperforms more complex methods on all of the benchmark datasets and improves classification accuracy of a state-of-the-art online binary classifier algorithm. version:1
arxiv-1407-7566 | Dependence versus Conditional Dependence in Local Causal Discovery from Gene Expression Data | http://arxiv.org/abs/1407.7566 | id:1407.7566 author:Eric V. Strobl, Shyam Visweswaran category:q-bio.QM cs.LG stat.ML  published:2014-07-28 summary:Motivation: Algorithms that discover variables which are causally related to a target may inform the design of experiments. With observational gene expression data, many methods discover causal variables by measuring each variable's degree of statistical dependence with the target using dependence measures (DMs). However, other methods measure each variable's ability to explain the statistical dependence between the target and the remaining variables in the data using conditional dependence measures (CDMs), since this strategy is guaranteed to find the target's direct causes, direct effects, and direct causes of the direct effects in the infinite sample limit. In this paper, we design a new algorithm in order to systematically compare the relative abilities of DMs and CDMs in discovering causal variables from gene expression data. Results: The proposed algorithm using a CDM is sample efficient, since it consistently outperforms other state-of-the-art local causal discovery algorithms when samples sizes are small. However, the proposed algorithm using a CDM outperforms the proposed algorithm using a DM only when sample sizes are above several hundred. These results suggest that accurate causal discovery from gene expression data using current CDM-based algorithms requires datasets with at least several hundred samples. Availability: The proposed algorithm is freely available at https://github.com/ericstrobl/DvCD. version:1
arxiv-1407-7508 | Efficient Regularized Regression for Variable Selection with L0 Penalty | http://arxiv.org/abs/1407.7508 | id:1407.7508 author:Zhenqiu Liu, Gang Li category:cs.LG stat.ML  published:2014-07-28 summary:Variable (feature, gene, model, which we use interchangeably) selections for regression with high-dimensional BIGDATA have found many applications in bioinformatics, computational biology, image processing, and engineering. One appealing approach is the L0 regularized regression which penalizes the number of nonzero features in the model directly. L0 is known as the most essential sparsity measure and has nice theoretical properties, while the popular L1 regularization is only a best convex relaxation of L0. Therefore, it is natural to expect that L0 regularized regression performs better than LASSO. However, it is well-known that L0 optimization is NP-hard and computationally challenging. Instead of solving the L0 problems directly, most publications so far have tried to solve an approximation problem that closely resembles L0 regularization. In this paper, we propose an efficient EM algorithm (L0EM) that directly solves the L0 optimization problem. $L_0$EM is efficient with high dimensional data. It also provides a natural solution to all Lp p in [0,2] problems. The regularized parameter can be either determined through cross-validation or AIC and BIC. Theoretical properties of the L0-regularized estimator are given under mild conditions that permit the number of variables to be much larger than the sample size. We demonstrate our methods through simulation and high-dimensional genomic data. The results indicate that L0 has better performance than LASSO and L0 with AIC or BIC has similar performance as computationally intensive cross-validation. The proposed algorithms are efficient in identifying the non-zero variables with less-bias and selecting biologically important genes and pathways with high dimensional BIGDATA. version:1
arxiv-1407-7504 | A Fast Hierarchical Method for Multi-script and Arbitrary Oriented Scene Text Extraction | http://arxiv.org/abs/1407.7504 | id:1407.7504 author:Lluis Gomez, Dimosthenis Karatzas category:cs.CV  published:2014-07-28 summary:Typography and layout lead to the hierarchical organisation of text in words, text lines, paragraphs. This inherent structure is a key property of text in any script and language, which has nonetheless been minimally leveraged by existing text detection methods. This paper addresses the problem of text segmentation in natural scenes from a hierarchical perspective. Contrary to existing methods, we make explicit use of text structure, aiming directly to the detection of region groupings corresponding to text within a hierarchy produced by an agglomerative similarity clustering process over individual regions. We propose an optimal way to construct such an hierarchy introducing a feature space designed to produce text group hypotheses with high recall and a novel stopping rule combining a discriminative classifier and a probabilistic measure of group meaningfulness based in perceptual organization. Results obtained over four standard datasets, covering text in variable orientations and different languages, demonstrate that our algorithm, while being trained in a single mixed dataset, outperforms state of the art methods in unconstrained scenarios. version:1
arxiv-1406-1881 | Fine-grained Activity Recognition with Holistic and Pose based Features | http://arxiv.org/abs/1406.1881 | id:1406.1881 author:Leonid Pishchulin, Mykhaylo Andriluka, Bernt Schiele category:cs.CV  published:2014-06-07 summary:Holistic methods based on dense trajectories are currently the de facto standard for recognition of human activities in video. Whether holistic representations will sustain or will be superseded by higher level video encoding in terms of body pose and motion is the subject of an ongoing debate. In this paper we aim to clarify the underlying factors responsible for good performance of holistic and pose-based representations. To that end we build on our recent dataset leveraging the existing taxonomy of human activities. This dataset includes 24,920 video snippets covering 410 human activities in total. Our analysis reveals that holistic and pose-based methods are highly complementary, and their performance varies significantly depending on the activity. We find that holistic methods are mostly affected by the number and speed of trajectories, whereas pose-based methods are mostly influenced by viewpoint of the person. We observe striking performance differences across activities: for certain activities results with pose-based features are more than twice as accurate compared to holistic features, and vice versa. The best performing approach in our comparison is based on the combination of holistic and pose-based approaches, which again underlines their complementarity. version:2
arxiv-1404-5772 | Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks | http://arxiv.org/abs/1404.5772 | id:1404.5772 author:Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin Wang, Tie-Yan Liu category:cs.IR cs.LG cs.NE  published:2014-04-23 summary:Click prediction is one of the fundamental problems in sponsored search. Most of existing studies took advantage of machine learning approaches to predict ad click for each event of ad view independently. However, as observed in the real-world sponsored search system, user's behaviors on ads yield high dependency on how the user behaved along with the past time, especially in terms of what queries she submitted, what ads she clicked or ignored, and how long she spent on the landing pages of clicked ads, etc. Inspired by these observations, we introduce a novel framework based on Recurrent Neural Networks (RNN). Compared to traditional methods, this framework directly models the dependency on user's sequential behaviors into the click prediction process through the recurrent structure in RNN. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that our approach can significantly improve the click prediction accuracy, compared to sequence-independent approaches. version:3
arxiv-1407-7417 | 'Almost Sure' Chaotic Properties of Machine Learning Methods | http://arxiv.org/abs/1407.7417 | id:1407.7417 author:Nabarun Mondal, Partha P. Ghosh category:cs.LG cs.AI  published:2014-07-28 summary:It has been demonstrated earlier that universal computation is 'almost surely' chaotic. Machine learning is a form of computational fixed point iteration, iterating over the computable function space. We showcase some properties of this iteration, and establish in general that the iteration is 'almost surely' of chaotic nature. This theory explains the observation in the counter intuitive properties of deep learning methods. This paper demonstrates that these properties are going to be universal to any learning method. version:1
arxiv-1205-3981 | kLog: A Language for Logical and Relational Learning with Kernels | http://arxiv.org/abs/1205.3981 | id:1205.3981 author:Paolo Frasconi, Fabrizio Costa, Luc De Raedt, Kurt De Grave category:cs.AI cs.LG cs.PL I.2.6; I.2.3; D.3.2  published:2012-05-17 summary:We introduce kLog, a novel approach to statistical relational learning. Unlike standard approaches, kLog does not represent a probability distribution directly. It is rather a language to perform kernel-based learning on expressive logical and relational representations. kLog allows users to specify learning problems declaratively. It builds on simple but powerful concepts: learning from interpretations, entity/relationship data modeling, logic programming, and deductive databases. Access by the kernel to the rich representation is mediated by a technique we call graphicalization: the relational representation is first transformed into a graph --- in particular, a grounded entity/relationship diagram. Subsequently, a choice of graph kernel defines the feature space. kLog supports mixed numerical and symbolic data, as well as background knowledge in the form of Prolog or Datalog programs as in inductive logic programming systems. The kLog framework can be applied to tackle the same range of tasks that has made statistical relational learning so popular, including classification, regression, multitask learning, and collective classification. We also report about empirical comparisons, showing that kLog can be either more accurate, or much faster at the same level of accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at http://klog.dinfo.unifi.it along with tutorials. version:5
arxiv-1407-7399 | A Numerical Optimization Algorithm Inspired by the Strawberry Plant | http://arxiv.org/abs/1407.7399 | id:1407.7399 author:F. Merrikh-Bayat category:cs.NE  published:2014-07-28 summary:This paper proposes a new numerical optimization algorithm inspired by the strawberry plant for solving complicated engineering problems. Plants like strawberry develop both runners and roots for propagation and search for water resources and minerals. In these plants, runners and roots can be thought of as tools for global and local searches, respectively. The proposed algorithm has three main differences with the trivial nature-inspired optimization algorithms: duplication-elimination of the computational agents at all iterations, subjecting all agents to both small and large movements from the beginning to end, and the lack of communication (information exchange) between agents. Moreover, it has the advantage of using only three parameters to be tuned by user. This algorithm is applied to standard test functions and the results are compared with GA and PSO. The proposed algorithm is also used to solve an open problem in the field of robust control theory. These simulations show that the proposed algorithm can very effectively solve complicated optimization problems. version:1
arxiv-1410-0226 | Non-parametric Image Registration of Airborne LiDAR, Hyperspectral and Photographic Imagery of Forests | http://arxiv.org/abs/1410.0226 | id:1410.0226 author:Juheon Lee, Xiaohao Cai, Carola-Bibiane Schonlieb, David Coomes category:cs.CV I.4.8  published:2014-07-28 summary:There is much current interest in using multi-sensor airborne remote sensing to monitor the structure and biodiversity of forests. This paper addresses the application of non-parametric image registration techniques to precisely align images obtained from multimodal imaging, which is critical for the successful identification of individual trees using object recognition approaches. Non-parametric image registration, in particular the technique of optimizing one objective function containing data fidelity and regularization terms, provides flexible algorithms for image registration. Using a survey of woodlands in southern Spain as an example, we show that non-parametric image registration can be successful at fusing datasets when there is little prior knowledge about how the datasets are interrelated (i.e. in the absence of ground control points). The validity of non-parametric registration methods in airborne remote sensing is demonstrated by a series of experiments. Precise data fusion is a prerequisite to accurate recognition of objects within airborne imagery, so non-parametric image registration could make a valuable contribution to the analysis pipeline. version:1
arxiv-1206-6679 | Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression | http://arxiv.org/abs/1206.6679 | id:1206.6679 author:Tim Salimans, David A. Knowles category:stat.CO cs.CV stat.ML 62F15  published:2012-06-28 summary:We propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribution. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approximation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several examples illustrate the speed and accuracy of our approximation method in practice. version:6
arxiv-1312-5354 | Classification of Human Ventricular Arrhythmia in High Dimensional Representation Spaces | http://arxiv.org/abs/1312.5354 | id:1312.5354 author:Yaqub Alwan, Zoran Cvetkovic, Michael Curtis category:cs.CE cs.LG  published:2013-12-18 summary:We studied classification of human ECGs labelled as normal sinus rhythm, ventricular fibrillation and ventricular tachycardia by means of support vector machines in different representation spaces, using different observation lengths. ECG waveform segments of duration 0.5-4 s, their Fourier magnitude spectra, and lower dimensional projections of Fourier magnitude spectra were used for classification. All considered representations were of much higher dimension than in published studies. Classification accuracy improved with segment duration up to 2 s, with 4 s providing little improvement. We found that it is possible to discriminate between ventricular tachycardia and ventricular fibrillation by the present approach with much shorter runs of ECG (2 s, minimum 86% sensitivity per class) than previously imagined. Ensembles of classifiers acting on 1 s segments taken over 5 s observation windows gave best results, with sensitivities of detection for all classes exceeding 93%. version:2
arxiv-1407-8518 | Beyond KernelBoost | http://arxiv.org/abs/1407.8518 | id:1407.8518 author:Roberto Rigamonti, Vincent Lepetit, Pascal Fua category:cs.CV cs.LG  published:2014-07-28 summary:In this Technical Report we propose a set of improvements with respect to the KernelBoost classifier presented in [Becker et al., MICCAI 2013]. We start with a scheme inspired by Auto-Context, but that is suitable in situations where the lack of large training sets poses a potential problem of overfitting. The aim is to capture the interactions between neighboring image pixels to better regularize the boundaries of segmented regions. As in Auto-Context [Tu et al., PAMI 2009] the segmentation process is iterative and, at each iteration, the segmentation results for the previous iterations are taken into account in conjunction with the image itself. However, unlike in [Tu et al., PAMI 2009], we organize our recursion so that the classifiers can progressively focus on difficult-to-classify locations. This lets us exploit the power of the decision-tree paradigm while avoiding over-fitting. In the context of this architecture, KernelBoost represents a powerful building block due to its ability to learn on the score maps coming from previous iterations. We first introduce two important mechanisms to empower the KernelBoost classifier, namely pooling and the clustering of positive samples based on the appearance of the corresponding ground-truth. These operations significantly contribute to increase the effectiveness of the system on biomedical images, where texture plays a major role in the recognition of the different image components. We then present some other techniques that can be easily integrated in the KernelBoost framework to further improve the accuracy of the final segmentation. We show extensive results on different medical image datasets, including some multi-label tasks, on which our method is shown to outperform state-of-the-art approaches. The resulting segmentations display high accuracy, neat contours, and reduced noise. version:1
arxiv-1407-7357 | Text Classification Using Association Rules, Dependency Pruning and Hyperonymization | http://arxiv.org/abs/1407.7357 | id:1407.7357 author:Yannis Haralambous, Philippe Lenca category:cs.IR cs.CL  published:2014-07-28 summary:We present new methods for pruning and enhancing item- sets for text classification via association rule mining. Pruning methods are based on dependency syntax and enhancing methods are based on replacing words by their hyperonyms of various orders. We discuss the impact of these methods, compared to pruning based on tfidf rank of words. version:1
arxiv-1407-3068 | Deep Networks with Internal Selective Attention through Feedback Connections | http://arxiv.org/abs/1407.3068 | id:1407.3068 author:Marijn Stollenga, Jonathan Masci, Faustino Gomez, Juergen Schmidhuber category:cs.CV cs.LG cs.NE 68T45  published:2014-07-11 summary:Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model. version:2
arxiv-1407-7330 | Discovering Discriminative Cell Attributes for HEp-2 Specimen Image Classification | http://arxiv.org/abs/1407.7330 | id:1407.7330 author:Arnold Wiliem, Peter Hobson, Brian C. Lovell category:cs.CV cs.CE  published:2014-07-28 summary:Recently, there has been a growing interest in developing Computer Aided Diagnostic (CAD) systems for improving the reliability and consistency of pathology test results. This paper describes a novel CAD system for the Anti-Nuclear Antibody (ANA) test via Indirect Immunofluorescence protocol on Human Epithelial Type 2 (HEp-2) cells. While prior works have primarily focused on classifying cell images extracted from ANA specimen images, this work takes a further step by focussing on the specimen image classification problem itself. Our system is able to efficiently classify specimen images as well as producing meaningful descriptions of ANA pattern class which helps physicians to understand the differences between various ANA patterns. We achieve this goal by designing a specimen-level image descriptor that: (1) is highly discriminative; (2) has small descriptor length and (3) is semantically meaningful at the cell level. In our work, a specimen image descriptor is represented by its overall cell attribute descriptors. As such, we propose two max-margin based learning schemes to discover cell attributes whilst still maintaining the discrimination of the specimen image descriptor. Our learning schemes differ from the existing discriminative attribute learning approaches as they primarily focus on discovering image-level attributes. Comparative evaluations were undertaken to contrast the proposed approach to various state-of-the-art approaches on a novel HEp-2 cell dataset which was specifically proposed for the specimen-level classification. Finally, we showcase the ability of the proposed approach to provide textual descriptions to explain ANA patterns. version:1
arxiv-1407-7317 | A unified framework for thermal face recognition | http://arxiv.org/abs/1407.7317 | id:1407.7317 author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV  published:2014-07-28 summary:The reduction of the cost of infrared (IR) cameras in recent years has made IR imaging a highly viable modality for face recognition in practice. A particularly attractive advantage of IR-based over conventional, visible spectrum-based face recognition stems from its invariance to visible illumination. In this paper we argue that the main limitation of previous work on face recognition using IR lies in its ad hoc approach to treating different nuisance factors which affect appearance, prohibiting a unified approach that is capable of handling concurrent changes in multiple (or indeed all) major extrinsic sources of variability, which is needed in practice. We describe the first approach that attempts to achieve this - the framework we propose achieves outstanding recognition performance in the presence of variable (i) pose, (ii) facial expression, (iii) physiological state, (iv) partial occlusion due to eye-wear, and (v) quasi-occlusion due to facial hair growth. version:1
arxiv-1206-4631 | A Poisson convolution model for characterizing topical content with word frequency and exclusivity | http://arxiv.org/abs/1206.4631 | id:1206.4631 author:Edoardo M Airoldi, Jonathan M Bischof category:cs.LG cs.CL cs.IR stat.ME stat.ML  published:2012-06-18 summary:An ongoing challenge in the analysis of document collections is how to summarize content in terms of a set of inferred themes that can be interpreted substantively in terms of topics. The current practice of parametrizing the themes in terms of most frequent words limits interpretability by ignoring the differential use of words across topics. We argue that words that are both common and exclusive to a theme are more effective at characterizing topical content. We consider a setting where professional editors have annotated documents to a collection of topic categories, organized into a tree, in which leaf-nodes correspond to the most specific topics. Each document is annotated to multiple categories, at different levels of the tree. We introduce a hierarchical Poisson convolution model to analyze annotated documents in this setting. The model leverages the structure among categories defined by professional editors to infer a clear semantic description for each topic in terms of words that are both frequent and exclusive. We carry out a large randomized experiment on Amazon Turk to demonstrate that topic summaries based on the FREX score are more interpretable than currently established frequency based summaries, and that the proposed model produces more efficient estimates of exclusivity than with currently models. We also develop a parallelized Hamiltonian Monte Carlo sampler that allows the inference to scale to millions of documents. version:3
arxiv-1407-7299 | Algorithms, Initializations, and Convergence for the Nonnegative Matrix Factorization | http://arxiv.org/abs/1407.7299 | id:1407.7299 author:Amy N. Langville, Carl D. Meyer, Russell Albright, James Cox, David Duling category:cs.NA cs.LG stat.ML 65F30  published:2014-07-28 summary:It is well known that good initializations can improve the speed and accuracy of the solutions of many nonnegative matrix factorization (NMF) algorithms. Many NMF algorithms are sensitive with respect to the initialization of W or H or both. This is especially true of algorithms of the alternating least squares (ALS) type, including the two new ALS algorithms that we present in this paper. We compare the results of six initialization procedures (two standard and four new) on our ALS algorithms. Lastly, we discuss the practical issue of choosing an appropriate convergence criterion. version:1
arxiv-1407-7260 | Leveraging user profile attributes for improving pedagogical accuracy of learning pathways | http://arxiv.org/abs/1407.7260 | id:1407.7260 author:Tanmay Sinha, Ankit Banka, Dae Ki Kang category:cs.CY cs.LG  published:2014-07-27 summary:In recent years, with the enormous explosion of web based learning resources, personalization has become a critical factor for the success of services that wish to leverage the power of Web 2.0. However, the relevance, significance and impact of tailored content delivery in the learning domain is still questionable. Apart from considering only interaction based features like ratings and inferring learner preferences from them, if these services were to incorporate innate user profile attributes which affect learning activities, the quality of recommendations produced could be vastly improved. Recognizing the crucial role of effective guidance in informal educational settings, we provide a principled way of utilizing multiple sources of information from the user profile itself for the recommendation task. We explore factors that affect the choice of learning resources and explain in what way are they helpful to improve the pedagogical accuracy of learning objects recommended. Through a systematical application of machine learning techniques, we further provide a technological solution to convert these indirectly mapped learner specific attributes into a direct mapping with the learning resources. This mapping has a distinct advantage of tagging learning resources to make their metadata more informative. The results of our empirical study depict the similarity of nominal learning attributes with respect to each other. We further succeed in capturing the learner subset, whose preferences are most likely to be an indication of learning resource usage. Our novel system filters learner profile attributes to discover a tag that links them with learning resources. version:1
arxiv-1405-5300 | Fast Distributed Coordinate Descent for Non-Strongly Convex Losses | http://arxiv.org/abs/1405.5300 | id:1405.5300 author:Olivier Fercoq, Zheng Qu, Peter Richtárik, Martin Takáč category:math.OC cs.LG  published:2014-05-21 summary:We propose an efficient distributed randomized coordinate descent method for minimizing regularized non-strongly convex loss functions. The method attains the optimal $O(1/k^2)$ convergence rate, where $k$ is the iteration counter. The core of the work is the theoretical study of stepsize parameters. We have implemented the method on Archer - the largest supercomputer in the UK - and show that the method is capable of solving a (synthetic) LASSO optimization problem with 50 billion variables. version:2
arxiv-1407-7211 | An evolutionary solver for linear integer programming | http://arxiv.org/abs/1407.7211 | id:1407.7211 author:João Pedro Pedroso category:cs.NE cs.AI math.OC 80M50 G.1.6  I.2.8  published:2014-07-27 summary:In this paper we introduce an evolutionary algorithm for the solution of linear integer programs. The strategy is based on the separation of the variables into the integer subset and the continuous subset; the integer variables are fixed by the evolutionary system, and the continuous ones are determined in function of them, by a linear program solver. We report results obtained for some standard benchmark problems, and compare them with those obtained by branch-and-bound. The performance of the evolutionary algorithm is promising. Good feasible solutions were generally obtained, and in some of the difficult benchmark tests it outperformed branch-and-bound. version:1
arxiv-1407-7169 | Principles and Parameters: a coding theory perspective | http://arxiv.org/abs/1407.7169 | id:1407.7169 author:Matilde Marcolli category:cs.CL cs.IT math.IT 91F20  68P30  published:2014-07-26 summary:We propose an approach to Longobardi's parametric comparison method (PCM) via the theory of error-correcting codes. One associates to a collection of languages to be analyzed with the PCM a binary (or ternary) code with one code words for each language in the family and each word consisting of the binary values of the syntactic parameters of the language, with the ternary case allowing for an additional parameter state that takes into account phenomena of entailment of parameters. The code parameters of the resulting code can be compared with some classical bounds in coding theory: the asymptotic bound, the Gilbert-Varshamov bound, etc. The position of the code parameters with respect to some of these bounds provides quantitative information on the variability of syntactic parameters within and across historical-linguistic families. While computations carried out for languages belonging to the same family yield codes below the GV curve, comparisons across different historical families can give examples of isolated codes lying above the asymptotic bound. version:1
arxiv-1212-5760 | Mixture Model Averaging for Clustering | http://arxiv.org/abs/1212.5760 | id:1212.5760 author:Yuhong Wei, Paul D. McNicholas category:stat.ME stat.CO stat.ML  published:2012-12-23 summary:In mixture model-based clustering applications, it is common to fit several models from a family and report clustering results from only the `best' one. In such circumstances, selection of this best model is achieved using a model selection criterion, most often the Bayesian information criterion. Rather than throw away all but the best model, we average multiple models that are in some sense close to the best one, thereby producing a weighted average of clustering results. Two (weighted) averaging approaches are considered: averaging the component membership probabilities and averaging models. In both cases, Occam's window is used to determine closeness to the best model and weights are computed within a Bayesian model averaging paradigm. In some cases, we need to merge components before averaging; we introduce a method for merging mixture components based on the adjusted Rand index. The effectiveness of our model-based clustering averaging approaches is illustrated using a family of Gaussian mixture models on real and simulated data. version:3
arxiv-1407-7159 | Pairwise Correlations in Layered Close-Packed Structures | http://arxiv.org/abs/1407.7159 | id:1407.7159 author:P. M. Riechers, D. P. Varn, J. P. Crutchfield category:cond-mat.mtrl-sci cs.LG  published:2014-07-26 summary:Given a description of the stacking statistics of layered close-packed structures in the form of a hidden Markov model, we develop analytical expressions for the pairwise correlation functions between the layers. These may be calculated analytically as explicit functions of model parameters or the expressions may be used as a fast, accurate, and efficient way to obtain numerical values. We present several examples, finding agreement with previous work as well as deriving new relations. version:1
arxiv-1407-0107 | Randomized Block Coordinate Descent for Online and Stochastic Optimization | http://arxiv.org/abs/1407.0107 | id:1407.0107 author:Huahua Wang, Arindam Banerjee category:cs.LG  published:2014-07-01 summary:Two types of low cost-per-iteration gradient descent methods have been extensively studied in parallel. One is online or stochastic gradient descent (OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this paper, we combine the two types of methods together and propose online randomized block coordinate descent (ORBCD). At each iteration, ORBCD only computes the partial gradient of one block coordinate of one mini-batch samples. ORBCD is well suited for the composite minimization problem where one function is the average of the losses of a large number of samples and the other is a simple regularizer defined on high dimensional variables. We show that the iteration complexity of ORBCD has the same order as OGD or SGD. For strongly convex functions, by reducing the variance of stochastic gradients, we show that ORBCD can converge at a geometric rate in expectation, matching the convergence rate of SGD with variance reduction and RBCD. version:3
arxiv-1309-2796 | Decision Trees for Function Evaluation - Simultaneous Optimization of Worst and Expected Cost | http://arxiv.org/abs/1309.2796 | id:1309.2796 author:Ferdinando Cicalese, Eduardo Laber, Aline Medeiros Saettler category:cs.DS cs.AI cs.LG  published:2013-09-11 summary:In several applications of automatic diagnosis and active learning a central problem is the evaluation of a discrete function by adaptively querying the values of its variables until the values read uniquely determine the value of the function. In general, the process of reading the value of a variable might involve some cost, computational or even a fee to be paid for the experiment required for obtaining the value. This cost should be taken into account when deciding the next variable to read. The goal is to design a strategy for evaluating the function incurring little cost (in the worst case or in expectation according to a prior distribution on the possible variables' assignments). Our algorithm builds a strategy (decision tree) which attains a logarithmic approxima- tion simultaneously for the expected and worst cost spent. This is best possible under the assumption that $P \neq NP.$ version:2
arxiv-1302-2712 | Bayesian Nonparametric Dictionary Learning for Compressed Sensing MRI | http://arxiv.org/abs/1302.2712 | id:1302.2712 author:Yue Huang, John Paisley, Qin Lin, Xinghao Ding, Xueyang Fu, Xiao-ping Zhang category:cs.CV physics.med-ph stat.AP  published:2013-02-12 summary:We develop a Bayesian nonparametric model for reconstructing magnetic resonance images (MRI) from highly undersampled k-space data. We perform dictionary learning as part of the image reconstruction process. To this end, we use the beta process as a nonparametric dictionary learning prior for representing an image patch as a sparse combination of dictionary elements. The size of the dictionary and the patch-specific sparsity pattern are inferred from the data, in addition to other dictionary learning variables. Dictionary learning is performed directly on the compressed image, and so is tailored to the MRI being considered. In addition, we investigate a total variation penalty term in combination with the dictionary learning model, and show how the denoising property of dictionary learning removes dependence on regularization parameters in the noisy setting. We derive a stochastic optimization algorithm based on Markov Chain Monte Carlo (MCMC) for the Bayesian model, and use the alternating direction method of multipliers (ADMM) for efficiently performing total variation minimization. We present empirical results on several MRI, which show that the proposed regularization framework can improve reconstruction accuracy over other methods. version:3
arxiv-1407-7094 | Crowdsourcing Dialect Characterization through Twitter | http://arxiv.org/abs/1407.7094 | id:1407.7094 author:Bruno Gonçalves, David Sánchez category:physics.soc-ph cs.CL cs.SI stat.ML  published:2014-07-26 summary:We perform a large-scale analysis of language diatopic variation using geotagged microblogging datasets. By collecting all Twitter messages written in Spanish over more than two years, we build a corpus from which a carefully selected list of concepts allows us to characterize Spanish varieties on a global scale. A cluster analysis proves the existence of well defined macroregions sharing common lexical properties. Remarkably enough, we find that Spanish language is split into two superdialects, namely, an urban speech used across major American and Spanish citites and a diverse form that encompasses rural areas and small towns. The latter can be further clustered into smaller varieties with a stronger regional character. version:1
arxiv-1407-7091 | Pushbroom Stereo for High-Speed Navigation in Cluttered Environments | http://arxiv.org/abs/1407.7091 | id:1407.7091 author:Andrew J. Barry, Russ Tedrake category:cs.RO cs.CV  published:2014-07-26 summary:We present a novel stereo vision algorithm that is capable of obstacle detection on a mobile-CPU processor at 120 frames per second. Our system performs a subset of standard block-matching stereo processing, searching only for obstacles at a single depth. By using an onboard IMU and state-estimator, we can recover the position of obstacles at all other depths, building and updating a full depth-map at framerate. Here, we describe both the algorithm and our implementation on a high-speed, small UAV, flying at over 20 MPH (9 m/s) close to obstacles. The system requires no external sensing or computation and is, to the best of our knowledge, the first high-framerate stereo detection system running onboard a small UAV. version:1
arxiv-1407-6949 | Efficient Bayesian Nonparametric Modelling of Structured Point Processes | http://arxiv.org/abs/1407.6949 | id:1407.6949 author:Tom Gunter, Chris Lloyd, Michael A. Osborne, Stephen J. Roberts category:stat.ML  published:2014-07-25 summary:This paper presents a Bayesian generative model for dependent Cox point processes, alongside an efficient inference scheme which scales as if the point processes were modelled independently. We can handle missing data naturally, infer latent structure, and cope with large numbers of observed processes. A further novel contribution enables the model to work effectively in higher dimensional spaces. Using this method, we achieve vastly improved predictive performance on both 2D and 1D real data, validating our structured approach. version:1
arxiv-1404-1129 | An Efficient Two-Stage Sparse Representation Method | http://arxiv.org/abs/1404.1129 | id:1404.1129 author:Chengyu Peng, Hong Cheng, Manchor Ko category:cs.CV G.1.6; I.4.10  published:2014-04-04 summary:There are a large number of methods for solving under-determined linear inverse problem. Many of them have very high time complexity for large datasets. We propose a new method called Two-Stage Sparse Representation (TSSR) to tackle this problem. We decompose the representing space of signals into two parts, the measurement dictionary and the sparsifying basis. The dictionary is designed to approximate a sub-Gaussian distribution to exploit its concentration property. We apply sparse coding to the signals on the dictionary in the first stage, and obtain the training and testing coefficients respectively. Then we design the basis to approach an identity matrix in the second stage, to acquire the Restricted Isometry Property (RIP) and universality property. The testing coefficients are encoded over the basis and the final representing coefficients are obtained. We verify that the projection of testing coefficients onto the basis is a good approximation of the signal onto the representing space. Since the projection is conducted on a much sparser space, the runtime is greatly reduced. For concrete realization, we provide an instance for the proposed TSSR. Experiments on four biometrics databases show that TSSR is effective and efficient, comparing with several classical methods for solving linear inverse problem. version:2
arxiv-1407-6872 | Interpretable Low-Rank Document Representations with Label-Dependent Sparsity Patterns | http://arxiv.org/abs/1407.6872 | id:1407.6872 author:Ivan Ivek category:cs.CL cs.IR cs.LG  published:2014-07-25 summary:In context of document classification, where in a corpus of documents their label tags are readily known, an opportunity lies in utilizing label information to learn document representation spaces with better discriminative properties. To this end, in this paper application of a Variational Bayesian Supervised Nonnegative Matrix Factorization (supervised vbNMF) with label-driven sparsity structure of coefficients is proposed for learning of discriminative nonsubtractive latent semantic components occuring in TF-IDF document representations. Constraints are such that the components pursued are made to be frequently occuring in a small set of labels only, making it possible to yield document representations with distinctive label-specific sparse activation patterns. A simple measure of quality of this kind of sparsity structure, dubbed inter-label sparsity, is introduced and experimentally brought into tight connection with classification performance. Representing a great practical convenience, inter-label sparsity is shown to be easily controlled in supervised vbNMF by a single parameter. version:1
arxiv-1407-6853 | Substitute Based SCODE Word Embeddings in Supervised NLP Tasks | http://arxiv.org/abs/1407.6853 | id:1407.6853 author:Volkan Cirik, Deniz Yuret category:cs.CL  published:2014-07-25 summary:We analyze a word embedding method in supervised tasks. It maps words on a sphere such that words co-occurring in similar contexts lie closely. The similarity of contexts is measured by the distribution of substitutes that can fill them. We compared word embeddings, including more recent representations, in Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine our framework in multilingual dependency parsing as well. The results show that the proposed method achieves as good as or better results compared to the other word embeddings in the tasks we investigate. It achieves state-of-the-art results in multilingual dependency parsing. Word embeddings in 7 languages are available for public use. version:1
arxiv-1207-2940 | Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version | http://arxiv.org/abs/1207.2940 | id:1207.2940 author:Marc Peter Deisenroth, Shakir Mohamed category:stat.ML cs.LG cs.SY  published:2012-07-12 summary:Rich and complex time-series data, such as those generated from engineering systems, financial markets, videos or neural recordings, are now a common feature of modern data analysis. Explaining the phenomena underlying these diverse data sets requires flexible and accurate models. In this paper, we promote Gaussian process dynamical systems (GPDS) as a rich model class that is appropriate for such analysis. In particular, we present a message passing algorithm for approximate inference in GPDSs based on expectation propagation. By posing inference as a general message passing problem, we iterate forward-backward smoothing. Thus, we obtain more accurate posterior distributions over latent structures, resulting in improved predictive performance compared to state-of-the-art GPDS smoothers, which are special cases of our general message passing algorithm. Hence, we provide a unifying approach within which to contextualize message passing in GPDSs. version:4
arxiv-1104-4298 | Curved Gabor Filters for Fingerprint Image Enhancement | http://arxiv.org/abs/1104.4298 | id:1104.4298 author:Carsten Gottschlich category:cs.CV  published:2011-04-21 summary:Gabor filters play an important role in many application areas for the enhancement of various types of images and the extraction of Gabor features. For the purpose of enhancing curved structures in noisy images, we introduce curved Gabor filters which locally adapt their shape to the direction of flow. These curved Gabor filters enable the choice of filter parameters which increase the smoothing power without creating artifacts in the enhanced image. In this paper, curved Gabor filters are applied to the curved ridge and valley structure of low-quality fingerprint images. First, we combine two orientation field estimation methods in order to obtain a more robust estimation for very noisy images. Next, curved regions are constructed by following the respective local orientation and they are used for estimating the local ridge frequency. Lastly, curved Gabor filters are defined based on curved regions and they are applied for the enhancement of low-quality fingerprint images. Experimental results on the FVC2004 databases show improvements of this approach in comparison to state-of-the-art enhancement methods. version:2
arxiv-1407-3535 | Optimizing Auto-correlation for Fast Target Search in Large Search Space | http://arxiv.org/abs/1407.3535 | id:1407.3535 author:Arif Mahmood, Ajmal Mian, Robyn Owens category:cs.CV  published:2014-07-14 summary:In remote sensing image-blurring is induced by many sources such as atmospheric scatter, optical aberration, spatial and temporal sensor integration. The natural blurring can be exploited to speed up target search by fast template matching. In this paper, we synthetically induce additional non-uniform blurring to further increase the speed of the matching process. To avoid loss of accuracy, the amount of synthetic blurring is varied spatially over the image according to the underlying content. We extend transitive algorithm for fast template matching by incorporating controlled image blur. To this end we propose an Efficient Group Size (EGS) algorithm which minimizes the number of similarity computations for a particular search image. A larger efficient group size guarantees less computations and more speedup. EGS algorithm is used as a component in our proposed Optimizing auto-correlation (OptA) algorithm. In OptA a search image is iteratively non-uniformly blurred while ensuring no accuracy degradation at any image location. In each iteration efficient group size and overall computations are estimated by using the proposed EGS algorithm. The OptA algorithm stops when the number of computations cannot be further decreased without accuracy degradation. The proposed algorithm is compared with six existing state of the art exhaustive accuracy techniques using correlation coefficient as the similarity measure. Experiments on satellite and aerial image datasets demonstrate the effectiveness of the proposed algorithm. version:2
arxiv-1001-1027 | An Unsupervised Algorithm For Learning Lie Group Transformations | http://arxiv.org/abs/1001.1027 | id:1001.1027 author:Jascha Sohl-Dickstein, Ching Ming Wang, Bruno A. Olshausen category:cs.CV cs.LG  published:2010-01-07 summary:We present several theoretical contributions which allow Lie groups to be fit to high dimensional datasets. Transformation operators are represented in their eigen-basis, reducing the computational complexity of parameter estimation to that of training a linear transformation model. A transformation specific "blurring" operator is introduced that allows inference to escape local minima via a smoothing of the transformation space. A penalty on traversed manifold distance is added which encourages the discovery of sparse, minimal distance, transformations between states. Both learning and inference are demonstrated using these methods for the full set of affine transformations on natural image patches. Transformation operators are then trained on natural video sequences. It is shown that the learned video transformations provide a better description of inter-frame differences than the standard motion model based on rigid translation. version:4
arxiv-1407-6748 | Enhancing the Accuracy of Biometric Feature Extraction Fusion Using Gabor Filter and Mahalanobis Distance Algorithm | http://arxiv.org/abs/1407.6748 | id:1407.6748 author:Ayodeji S. Makinde, Yaw Nkansah-Gyekye, Loserian S. Laizer category:cs.CV  published:2014-07-24 summary:Biometric recognition systems have advanced significantly in the last decade and their use in specific applications will increase in the near future. The ability to conduct meaningful comparisons and assessments will be crucial to successful deployment and increasing biometric adoption. The best modality used as unimodal biometric systems are unable to fully address the problem of higher recognition rate. Multimodal biometric systems are able to mitigate some of the limitations encountered in unimodal biometric systems, such as non-universality, distinctiveness, non-acceptability, noisy sensor data, spoof attacks, and performance. More reliable recognition accuracy and performance are achievable as different modalities were being combined together and different algorithms or techniques were being used. The work presented in this paper focuses on a bimodal biometric system using face and fingerprint. An image enhancement technique (histogram equalization) is used to enhance the face and fingerprint images. Salient features of the face and fingerprint were extracted using the Gabor filter technique. A dimensionality reduction technique was carried out on both images extracted features using a principal component analysis technique. A feature level fusion algorithm (Mahalanobis distance technique) is used to combine each unimodal feature together. The performance of the proposed approach is validated and is effective. version:1
arxiv-1407-6637 | Trainable and Dynamic Computing: Error Backpropagation through Physical Media | http://arxiv.org/abs/1407.6637 | id:1407.6637 author:Michiel Hermans, Michaël Burm, Joni Dambre, Peter Bienstman category:cs.NE  published:2014-07-24 summary:Machine learning algorithms, and more in particular neural networks, arguably experience a revolution in terms of performance. Currently, the best systems we have for speech recognition, computer vision and similar problems are based on neural networks, trained using the half-century old backpropagation algorithm. Despite the fact that neural networks are a form of analog computers, they are still implemented digitally for reasons of convenience and availability. In this paper we demonstrate how we can design physical linear dynamic systems with non-linear feedback as a generic platform for dynamic, neuro-inspired analog computing. We show that a crucial advantage of this setup is that the error backpropagation can be performed physically as well, which greatly speeds up the optimisation process. As we show in this paper, using one experimentally validated and one conceptual example, such systems may be the key to providing a relatively straightforward mechanism for constructing highly scalable, fully dynamic analog computers. version:1
arxiv-1403-1343 | Ubic: Bridging the gap between digital cryptography and the physical world | http://arxiv.org/abs/1403.1343 | id:1403.1343 author:Mark Simkin, Dominique Schroeder, Andreas Bulling, Mario Fritz category:cs.CR cs.CV  published:2014-03-06 summary:Advances in computing technology increasingly blur the boundary between the digital domain and the physical world. Although the research community has developed a large number of cryptographic primitives and has demonstrated their usability in all-digital communication, many of them have not yet made their way into the real world due to usability aspects. We aim to make another step towards a tighter integration of digital cryptography into real world interactions. We describe Ubic, a framework that allows users to bridge the gap between digital cryptography and the physical world. Ubic relies on head-mounted displays, like Google Glass, resource-friendly computer vision techniques as well as mathematically sound cryptographic primitives to provide users with better security and privacy guarantees. The framework covers key cryptographic primitives, such as secure identification, document verification using a novel secure physical document format, as well as content hiding. To make a contribution of practical value, we focused on making Ubic as simple, easily deployable, and user friendly as possible. version:3
arxiv-1407-6513 | Convolutional Neural Associative Memories: Massive Capacity with Noise Tolerance | http://arxiv.org/abs/1407.6513 | id:1407.6513 author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi category:cs.NE cs.AI  published:2014-07-24 summary:The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions using a network of neurons. An ideal network should have the ability to 1) learn a set of patterns as they arrive, 2) retrieve the correct patterns from noisy queries, and 3) maximize the pattern retrieval capacity while maintaining the reliability in responding to queries. The majority of work on neural associative memories has focused on designing networks capable of memorizing any set of randomly chosen patterns at the expense of limiting the retrieval capacity. In this paper, we show that if we target memorizing only those patterns that have inherent redundancy (i.e., belong to a subspace), we can obtain all the aforementioned properties. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we propose framework based on a convolutional neural network along with an iterative algorithm that learns the redundancy among the patterns. The resulting network has a retrieval capacity that is exponential in the size of the network. Moreover, the asymptotic error correction performance of our network is linear in the size of the patterns. We then ex- tend our approach to deal with patterns lie approximately in a subspace. This extension allows us to memorize datasets containing natural patterns (e.g., images). Finally, we report experimental results on both synthetic and real datasets to support our claims. version:1
arxiv-1008-4063 | Nonlinear Quality of Life Index | http://arxiv.org/abs/1008.4063 | id:1008.4063 author:A. Zinovyev, A. N. Gorban category:cs.NE stat.AP  published:2010-08-24 summary:We present details of the analysis of the nonlinear quality of life index for 171 countries. This index is based on four indicators: GDP per capita by Purchasing Power Parities, Life expectancy at birth, Infant mortality rate, and Tuberculosis incidence. We analyze the structure of the data in order to find the optimal and independent on expert's opinion way to map several numerical indicators from a multidimensional space onto the one-dimensional space of the quality of life. In the 4D space we found a principal curve that goes "through the middle" of the dataset and project the data points on this curve. The order along this principal curve gives us the ranking of countries. Projection onto the principal curve provides a solution to the classical problem of unsupervised ranking of objects. It allows us to find the independent on expert's opinion way to project several numerical indicators from a multidimensional space onto the one-dimensional space of the index values. This projection is, in some sense, optimal and preserves as much information as possible. For computation we used ViDaExpert, a tool for visualization and analysis of multidimensional vectorial data (arXiv:1406.5550). version:3
arxiv-1407-6510 | New Method for Optimization of License Plate Recognition system with Use of Edge Detection and Connected Component | http://arxiv.org/abs/1407.6510 | id:1407.6510 author:Reza Azad, Hamid Reza Shayegh category:cs.CV  published:2014-07-24 summary:License Plate recognition plays an important role on the traffic monitoring and parking management systems. In this paper, a fast and real time method has been proposed which has an appropriate application to find tilt and poor quality plates. In the proposed method, at the beginning, the image is converted into binary mode using adaptive threshold. Then, by using some edge detection and morphology operations, plate number location has been specified. Finally, if the plat has tilt, its tilt is removed away. This method has been tested on another paper data set that has different images of the background, considering distance, and angel of view so that the correct extraction rate of plate reached at 98.66%. version:1
arxiv-1407-6506 | Novel and Tuneable Method for Skin Detection Based on Hybrid Color Space and Color Statistical Features | http://arxiv.org/abs/1407.6506 | id:1407.6506 author:Reza Azad, Hamid Reza Shayegh category:cs.CV  published:2014-07-24 summary:Skin detection is one of the most important and primary stages in some of image processing applications such as face detection and human tracking. So far, many approaches are proposed to done this case. Near all of these methods have tried to find best match intensity distribution with skin pixels based on popular color spaces such as RGB, CMYK or YCbCr. Results show these methods cannot provide an accurate approach for every kinds of skin. In this paper, an approach is proposed to solve this problem using statistical features technique. This approach is including two stages. In the first one, from pure skin statistical features were extracted and at the second stage, the skin pixels are detected using HSV and YCbCr color spaces. In the result part, the proposed approach is applied on FEI database and the accuracy rate reached 99.25 + 0.2. Further proposed method is applied on complex background database and accuracy rate obtained 95.40+0.31%. The proposed approach can be used for all kinds of skin using train stage which is the main advantages of it. Low noise sensitivity and low computational complexity are some of other advantages. version:1
arxiv-1407-6498 | Real-Time and Efficient Method for Accuracy Enhancement of Edge Based License Plate Recognition System | http://arxiv.org/abs/1407.6498 | id:1407.6498 author:Reza Azad, Babak Azad, Hamid Reza Shayegh category:cs.CV  published:2014-07-24 summary:License Plate Recognition plays an important role on the traffic monitoring and parking management. Administration and restriction of those transportation tools for their better service becomes very essential. In this paper, a fast and real time method has an appropriate application to find plates that the plat has tilt and the picture quality is poor. In the proposed method, at the beginning, the image is converted into binary mode with use of adaptive threshold. And with use of edge detection and morphology operation, plate number location has been specified and if the plat has tilt; its tilt is removed away. Then its characters are distinguished using image processing techniques. Finally, K Nearest Neighbour (KNN) classifier was used for character recognition. This method has been tested on available data set that has different images of the background, considering distance, and angel of view so that the correct extraction rate of plate reached at 98% and character recognition rate achieved at 99.12%. Further we tested our character recognition stage on Persian vehicle data set and we achieved 99% correct recognition rate. version:1
arxiv-1407-5978 | Sequential Changepoint Approach for Online Community Detection | http://arxiv.org/abs/1407.5978 | id:1407.5978 author:David Marangoni-Simonsen, Yao Xie category:stat.ML cs.LG cs.SI math.ST stat.TH  published:2014-07-22 summary:We present new algorithms for detecting the emergence of a community in large networks from sequential observations. The networks are modeled using Erdos-Renyi random graphs with edges forming between nodes in the community with higher probability. Based on statistical changepoint detection methodology, we develop three algorithms: the Exhaustive Search (ES), the mixture, and the Hierarchical Mixture (H-Mix) methods. Performance of these methods is evaluated by the average run length (ARL), which captures the frequency of false alarms, and the detection delay. Numerical comparisons show that the ES method performs the best; however, it is exponentially complex. The mixture method is polynomially complex by exploiting the fact that the size of the community is typically small in a large network. However, it may react to a group of active edges that do not form a community. This issue is resolved by the H-Mix method, which is based on a dendrogram decomposition of the network. We present an asymptotic analytical expression for ARL of the mixture method when the threshold is large. Numerical simulation verifies that our approximation is accurate even in the non-asymptotic regime. Hence, it can be used to determine a desired threshold efficiently. Finally, numerical examples show that the mixture and the H-Mix methods can both detect a community quickly with a lower complexity than the ES method. version:3
arxiv-1407-6432 | Learning Structured Outputs from Partial Labels using Forest Ensemble | http://arxiv.org/abs/1407.6432 | id:1407.6432 author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.CV cs.LG  published:2014-07-24 summary:Learning structured outputs with general structures is computationally challenging, except for tree-structured models. Thus we propose an efficient boosting-based algorithm AdaBoost.MRF for this task. The idea is based on the realization that a graph is a superimposition of trees. Different from most existing work, our algorithm can handle partial labelling, and thus is particularly attractive in practice where reliable labels are often sparsely observed. In addition, our method works exclusively on trees and thus is guaranteed to converge. We apply the AdaBoost.MRF algorithm to an indoor video surveillance scenario, where activities are modelled at multiple levels. version:1
arxiv-1407-6423 | Performance evaluation of wavelet scattering network in image texture classification in various color spaces | http://arxiv.org/abs/1407.6423 | id:1407.6423 author:Jiasong Wu, Longyu Jiang, Xu Han, Lotfi Senhadji, Huazhong Shu category:cs.CV  published:2014-07-24 summary:Texture plays an important role in many image analysis applications. In this paper, we give a performance evaluation of color texture classification by performing wavelet scattering network in various color spaces. Experimental results on the KTH_TIPS_COL database show that opponent RGB based wavelet scattering network outperforms other color spaces. Therefore, when dealing with the problem of color texture classification, opponent RGB based wavelet scattering network is recommended. version:1
arxiv-1407-6288 | Subspace Learning From Bits | http://arxiv.org/abs/1407.6288 | id:1407.6288 author:Yuejie Chi category:stat.ML cs.IT math.IT  published:2014-07-23 summary:This paper proposes a simple sensing and estimation framework to faithfully recover the principal subspace of high-dimensional datasets or data streams from a collection of one-bit measurements from distributed sensors based on comparing accumulated energy projections of their data samples of dimension n over pairs of randomly selected directions. By leveraging low-dimensional structures, the top eigenvectors of a properly designed surrogate matrix is shown to recover the principal subspace of rank $r$ as soon as the number of bit measurements exceeds the order of $nr^2 \log n$, which can be much smaller than the ambient dimension of the covariance matrix. The sample complexity to obtain reliable comparison outcomes is also obtained. Furthermore, we develop a low-complexity online algorithm to track the principal subspace that allows new bit measurements arrive sequentially. Numerical examples are provided to validate the proposed approach. version:2
arxiv-1210-7477 | Parallel MCMC with Generalized Elliptical Slice Sampling | http://arxiv.org/abs/1210.7477 | id:1210.7477 author:Robert Nishihara, Iain Murray, Ryan P. Adams category:stat.CO stat.ML  published:2012-10-28 summary:Probabilistic models are conceptually powerful tools for finding structure in data, but their practical effectiveness is often limited by our ability to perform inference in them. Exact inference is frequently intractable, so approximate inference is often performed using Markov chain Monte Carlo (MCMC). To achieve the best possible results from MCMC, we want to efficiently simulate many steps of a rapidly mixing Markov chain which leaves the target distribution invariant. Of particular interest in this regard is how to take advantage of multi-core computing to speed up MCMC-based inference, both to improve mixing and to distribute the computational load. In this paper, we present a parallelizable Markov chain Monte Carlo algorithm for efficiently sampling from continuous probability distributions that can take advantage of hundreds of cores. This method shares information between parallel Markov chains to build a scale-mixture of Gaussians approximation to the density function of the target distribution. We combine this approximation with a recent method known as elliptical slice sampling to create a Markov chain with no step-size parameters that can mix rapidly without requiring gradient or curvature computations. version:2
arxiv-1104-4803 | Clustering Partially Observed Graphs via Convex Optimization | http://arxiv.org/abs/1104.4803 | id:1104.4803 author:Yudong Chen, Ali Jalali, Sujay Sanghavi, Huan Xu category:cs.LG stat.ML  published:2011-04-25 summary:This paper considers the problem of clustering a partially observed unweighted graph---i.e., one where for some node pairs we know there is an edge between them, for some others we know there is no edge, and for the remaining we do not know whether or not there is an edge. We want to organize the nodes into disjoint clusters so that there is relatively dense (observed) connectivity within clusters, and sparse across clusters. We take a novel yet natural approach to this problem, by focusing on finding the clustering that minimizes the number of "disagreements"---i.e., the sum of the number of (observed) missing edges within clusters, and (observed) present edges across clusters. Our algorithm uses convex optimization; its basis is a reduction of disagreement minimization to the problem of recovering an (unknown) low-rank matrix and an (unknown) sparse matrix from their partially observed sum. We evaluate the performance of our algorithm on the classical Planted Partition/Stochastic Block Model. Our main theorem provides sufficient conditions for the success of our algorithm as a function of the minimum cluster size, edge density and observation probability; in particular, the results characterize the tradeoff between the observation probability and the edge density gap. When there are a constant number of clusters of equal size, our results are optimal up to logarithmic factors. version:4
arxiv-1406-6038 | Exact fit of simple finite mixture models | http://arxiv.org/abs/1406.6038 | id:1406.6038 author:Dirk Tasche category:stat.ML q-fin.RM 62P30  62F10  published:2014-06-23 summary:How to forecast next year's portfolio-wide credit default rate based on last year's default observations and the current score distribution? A classical approach to this problem consists of fitting a mixture of the conditional score distributions observed last year to the current score distribution. This is a special (simple) case of a finite mixture model where the mixture components are fixed and only the weights of the components are estimated. The optimum weights provide a forecast of next year's portfolio-wide default rate. We point out that the maximum-likelihood (ML) approach to fitting the mixture distribution not only gives an optimum but even an exact fit if we allow the mixture components to vary but keep their density ratio fix. From this observation we can conclude that the standard default rate forecast based on last year's conditional default rates will always be located between last year's portfolio-wide default rate and the ML forecast for next year. As an application example, then cost quantification is discussed. We also discuss how the mixture model based estimation methods can be used to forecast total loss. This involves the reinterpretation of an individual classification problem as a collective quantification problem. version:2
arxiv-1407-6321 | Novel and Automatic Parking Inventory System Based on Pattern Recognition and Directional Chain Code | http://arxiv.org/abs/1407.6321 | id:1407.6321 author:Reza Azad, Majid Nazari category:cs.CV  published:2014-07-23 summary:The objective of this paper is to design an efficient vehicle license plate recognition System and to implement it for automatic parking inventory system. The system detects the vehicle first and then captures the image of the front view of the vehicle. Vehicle license plate is localized and characters are segmented. For finding the place of plate, a novel and real time method is expressed. A new and robust technique based on directional chain code is used for character recognition. The resulting vehicle number is then compared with the available database of all the vehicles so as to come up with information about the vehicle type and to charge entrance cost accordingly. The system is then allowed to open parking barrier for the vehicle and generate entrance cost receipt. The vehicle information (such as entrance time, date, and cost amount) is also stored in the database to maintain the record. The hardware and software integrated system is implemented and a working prototype model is developed. Under the available database, the average accuracy of locating vehicle license plate obtained 100%. Using 70% samples of character for training, we tested our scheme on whole samples and obtained 100% correct recognition rate. Further we tested our character recognition stage on Persian vehicle data set and we achieved 99% correct recognition. version:1
arxiv-1407-6318 | A robust and adaptable method for face detection based on Color Probabilistic Estimation Technique | http://arxiv.org/abs/1407.6318 | id:1407.6318 author:Reza Azad, Fatemeh Davami category:cs.CV  published:2014-07-23 summary:Human face perception is currently an active research area in the computer vision community. Skin detection is one of the most important and primary stages for this purpose. So far, many approaches are proposed to done this case. Near all of these methods have tried to find best match intensity distribution with skin pixels based on popular color spaces such as RGB, HSI or YCBCR. Results show that these methods cannot provide an accurate approach for every kind of skin. In this paper, an approach is proposed to solve this problem using a color probabilistic estimation technique. This approach is including two stages. In the first one, the skin intensity distribution is estimated using some train photos of pure skin, and at the second stage, the skin pixels are detected using Gaussian model and optimal threshold tuning. Then from the skin region facial features have been extracted to get the face from the skin region. In the results section, the proposed approach is applied on FEI database and the accuracy rate reached 99.25%. The proposed approach can be used for all kinds of skin using train stage which is the main advantage among the other advantages, such as Low noise sensitivity and low computational complexity. version:1
arxiv-1407-6315 | Quadratically constrained quadratic programming for classification using particle swarms and applications | http://arxiv.org/abs/1407.6315 | id:1407.6315 author:Deepak Kumar, A G Ramakrishnan category:cs.AI cs.LG cs.NE math.OC  published:2014-07-23 summary:Particle swarm optimization is used in several combinatorial optimization problems. In this work, particle swarms are used to solve quadratic programming problems with quadratic constraints. The approach of particle swarms is an example for interior point methods in optimization as an iterative technique. This approach is novel and deals with classification problems without the use of a traditional classifier. Our method determines the optimal hyperplane or classification boundary for a data set. In a binary classification problem, we constrain each class as a cluster, which is enclosed by an ellipsoid. The estimation of the optimal hyperplane between the two clusters is posed as a quadratically constrained quadratic problem. The optimization problem is solved in distributed format using modified particle swarms. Our method has the advantage of using the direction towards optimal solution rather than searching the entire feasible region. Our results on the Iris, Pima, Wine, and Thyroid datasets show that the proposed method works better than a neural network and the performance is close to that of SVM. version:1
arxiv-1407-6245 | scikit-image: Image processing in Python | http://arxiv.org/abs/1407.6245 | id:1407.6245 author:Stefan van der Walt, Johannes L. Schönberger, Juan Nunez-Iglesias, François Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu, the scikit-image contributors category:cs.MS cs.CV  published:2014-07-23 summary:scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal "Modified BSD" open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image. version:1
arxiv-1312-6995 | Towards Using Unlabeled Data in a Sparse-coding Framework for Human Activity Recognition | http://arxiv.org/abs/1312.6995 | id:1312.6995 author:Sourav Bhattacharya, Petteri Nurmi, Nils Hammerla, Thomas Plötz category:cs.LG cs.AI stat.ML  published:2013-12-25 summary:We propose a sparse-coding framework for activity recognition in ubiquitous and mobile computing that alleviates two fundamental problems of current supervised learning approaches. (i) It automatically derives a compact, sparse and meaningful feature representation of sensor data that does not rely on prior expert knowledge and generalizes extremely well across domain boundaries. (ii) It exploits unlabeled sample data for bootstrapping effective activity recognizers, i.e., substantially reduces the amount of ground truth annotation required for model estimation. Such unlabeled data is trivial to obtain, e.g., through contemporary smartphones carried by users as they go about their everyday activities. Based on the self-taught learning paradigm we automatically derive an over-complete set of basis vectors from unlabeled data that captures inherent patterns present within activity data. Through projecting raw sensor data onto the feature space defined by such over-complete sets of basis vectors effective feature extraction is pursued. Given these learned feature representations, classification backends are then trained using small amounts of labeled training data. We study the new approach in detail using two datasets which differ in terms of the recognition tasks and sensor modalities. Primarily we focus on transportation mode analysis task, a popular task in mobile-phone based sensing. The sparse-coding framework significantly outperforms the state-of-the-art in supervised learning approaches. Furthermore, we demonstrate the great practical potential of the new approach by successfully evaluating its generalization capabilities across both domain and sensor modalities by considering the popular Opportunity dataset. Our feature learning approach outperforms state-of-the-art approaches to analyzing activities in daily living. version:3
arxiv-1407-6174 | Visual Word Selection without Re-Coding and Re-Pooling | http://arxiv.org/abs/1407.6174 | id:1407.6174 author:Fatih Cakir, Stan Sclaroff category:cs.CV  published:2014-07-23 summary:The Bag-of-Words (BoW) representation is widely used in computer vision. The size of the codebook impacts the time and space complexity of the applications that use BoW. Thus, given a training set for a particular computer vision task, a key problem is pruning a large codebook to select only a subset of visual words. Evaluating possible selections of words to be included in the pruned codebook can be computationally prohibitive; in a brute-force scheme, evaluating each pruned codebook requires re-coding of all features extracted from training images to words in the candidate codebook and then re-pooling the words to obtain a representation of each image, e.g., histogram of visual word frequencies. In this paper, a method is proposed that selects and evaluates a subset of words from an initially large codebook, without the need for re-coding or re-pooling. Formulations are proposed for two commonly-used schemes: hard and soft (kernel) coding of visual words with average-pooling. The effectiveness of these formulations is evaluated on the 15 Scenes and Caltech 10 benchmarks. version:1
arxiv-1407-6154 | Content-Level Selective Offloading in Heterogeneous Networks: Multi-armed Bandit Optimization and Regret Bounds | http://arxiv.org/abs/1407.6154 | id:1407.6154 author:Pol Blasco, Deniz Gündüz category:cs.IT cs.LG math.IT  published:2014-07-23 summary:We consider content-level selective offloading of cellular downlink traffic to a wireless infostation terminal which stores high data-rate content in its cache memory. Cellular users in the vicinity of the infostation can directly download the stored content from the infostation through a broadband connection (e.g., WiFi), reducing the latency and load on the cellular network. The goal of the infostation cache controller (CC) is to store the most popular content in the cache memory such that the maximum amount of traffic is offloaded to the infostation. In practice, the popularity profile of the files is not known by the CC, which observes only the instantaneous demands for those contents stored in the cache. Hence, the cache content placement is optimised based on the demand history and on the cost associated to placing each content in the cache. By refreshing the cache content at regular time intervals, the CC gradually learns the popularity profile, while at the same time exploiting the limited cache capacity in the best way possible. This is formulated as a multi-armed bandit (MAB) problem with switching cost. Several algorithms are presented to decide on the cache content over time. The performance is measured in terms of cache efficiency, defined as the amount of net traffic that is offloaded to the infostation. In addition to theoretical regret bounds, the proposed algorithms are analysed through numerical simulations. In particular, the impact of system parameters, such as the number of files, number of users, cache size, and skewness of the popularity profile, on the performance is studied numerically. It is shown that the proposed algorithms learn the popularity profile quickly for a wide range of system parameters. version:1
arxiv-1407-7449 | A Fast Synchronization Clustering Algorithm | http://arxiv.org/abs/1407.7449 | id:1407.7449 author:Xinquan Chen category:cs.LG  published:2014-07-23 summary:This paper presents a Fast Synchronization Clustering algorithm (FSynC), which is an improved version of SynC algorithm. In order to decrease the time complexity of the original SynC algorithm, we combine grid cell partitioning method and Red-Black tree to construct the near neighbor point set of every point. By simulated experiments of some artificial data sets and several real data sets, we observe that FSynC algorithm can often get less time than SynC algorithm for many kinds of data sets. At last, it gives some research expectations to popularize this algorithm. version:1
arxiv-1407-6128 | Permutation Models for Collaborative Ranking | http://arxiv.org/abs/1407.6128 | id:1407.6128 author:Truyen Tran, Svetha Venkatesh category:cs.IR cs.LG stat.ML  published:2014-07-23 summary:We study the problem of collaborative filtering where ranking information is available. Focusing on the core of the collaborative ranking process, the user and their community, we propose new models for representation of the underlying permutations and prediction of ranks. The first approach is based on the assumption that the user makes successive choice of items in a stage-wise manner. In particular, we extend the Plackett-Luce model in two ways - introducing parameter factoring to account for user-specific contribution, and modelling the latent community in a generative setting. The second approach relies on log-linear parameterisation, which relaxes the discrete-choice assumption, but makes learning and inference much more involved. We propose MCMC-based learning and inference methods and derive linear-time prediction algorithms. version:1
arxiv-1311-3715 | Recognizing Image Style | http://arxiv.org/abs/1311.3715 | id:1311.3715 author:Sergey Karayev, Matthew Trentacoste, Helen Han, Aseem Agarwala, Trevor Darrell, Aaron Hertzmann, Holger Winnemoeller category:cs.CV  published:2013-11-15 summary:The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best -- even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style. version:3
arxiv-1407-6116 | A Genetic Algorithm for Software Design Migration from Structured to Object Oriented Paradigm | http://arxiv.org/abs/1407.6116 | id:1407.6116 author:Md. Selim, Saeed Siddik, Alim Ul Gias, M. Abdullah-Al-Wadud, Shah Mostafa Khaled category:cs.SE cs.NE  published:2014-07-23 summary:The potential benefit of migrating software design from Structured to Object Oriented Paradigm is manifolded including modularity, manageability and extendability. This design migration should be automated as it will reduce the time required in manual process. Our previous work has addressed this issue in terms of optimal graph clustering problem formulated by a quadratic Integer Program (IP). However, it has been realized that solution to the IP is computationally hard and thus heuristic based methods are required to get a near optimal solution. This paper presents a Genetic Algorithm (GA) for optimal clustering with an objective of maximizing intra-cluster edges whereas minimizing the inter-cluster ones. The proposed algorithm relies on fitness based parent selection and cross-overing cluster elements to reach an optimal solution step by step. The scheme was implemented and tested against a set of real and synthetic data. The experimental results show that GA outperforms our previous works based on Greedy and Monte Carlo approaches by 40% and 49.5%. version:1
arxiv-1407-6099 | Autonomous requirements specification processing using natural language processing | http://arxiv.org/abs/1407.6099 | id:1407.6099 author:S. G. Macdonell, K. Min, A. M. Connor category:cs.CL cs.SE  published:2014-07-23 summary:We describe our ongoing research that centres on the application of natural language processing (NLP) to software engineering and systems development activities. In particular, this paper addresses the use of NLP in the requirements analysis and systems design processes. We have developed a prototype toolset that can assist the systems analyst or software engineer to select and verify terms relevant to a project. In this paper we describe the processes employed by the system to extract and classify objects of interest from requirements documents. These processes are illustrated using a small example. version:1
arxiv-1407-6094 | Stabilizing Sparse Cox Model using Clinical Structures in Electronic Medical Records | http://arxiv.org/abs/1407.6094 | id:1407.6094 author:Shivapratap Gopakumar, Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG  published:2014-07-23 summary:Stability in clinical prediction models is crucial for transferability between studies, yet has received little attention. The problem is paramount in high dimensional data which invites sparse models with feature selection capability. We introduce an effective method to stabilize sparse Cox model of time-to-events using clinical structures inherent in Electronic Medical Records. Model estimation is stabilized using a feature graph derived from two types of EMR structures: temporal structure of disease and intervention recurrences, and hierarchical structure of medical knowledge and practices. We demonstrate the efficacy of the method in predicting time-to-readmission of heart failure patients. On two stability measures - the Jaccard index and the Consistency index - the use of clinical structures significantly increased feature stability without hurting discriminative power. Our model reported a competitive AUC of 0.64 (95% CIs: [0.58,0.69]) for 6 months prediction. version:1
arxiv-1407-6082 | Joint Energy-based Detection and Classificationon of Multilingual Text Lines | http://arxiv.org/abs/1407.6082 | id:1407.6082 author:Igor Milevskiy, Yuri Boykov category:cs.CV  published:2014-07-23 summary:This paper proposes a new hierarchical MDL-based model for a joint detection and classi?cation of multilingual text lines in im- ages taken by hand-held cameras. The majority of related text detec- tion methods assume alphabet-based writing in a single language, e.g. in Latin. They use simple clustering heuristics speci?c to such texts: prox- imity between letters within one line, larger distance between separate lines, etc. We are interested in a significantly more ambiguous problem where images combine alphabet and logographic characters from multiple languages and typographic rules vary a lot (e.g. English, Korean, and Chinese). Complexity of detecting and classifying text lines in multiple languages calls for a more principled approach based on information- theoretic principles. Our new MDL model includes data costs combining geometric errors with classi?cation likelihoods and a hierarchical sparsity term based on label costs. This energy model can be e?ciently minimized by fusion moves. We demonstrate robustness of the proposed algorithm on a large new database of multilingual text images collected in the pub- lic transit system of Seoul. version:1
arxiv-1407-6067 | The U-curve optimization problem: improvements on the original algorithm and time complexity analysis | http://arxiv.org/abs/1407.6067 | id:1407.6067 author:Marcelo S. Reis, Carlos E. Ferreira, Junior Barrera category:cs.LG cs.CV 68T10 I.5.2  published:2014-07-22 summary:The U-curve optimization problem is characterized by a decomposable in U-shaped curves cost function over the chains of a Boolean lattice. This problem can be applied to model the classical feature selection problem in Machine Learning. Recently, the U-Curve algorithm was proposed to give optimal solutions to the U-curve problem. In this article, we point out that the U-Curve algorithm is in fact suboptimal, and introduce the U-Curve-Search (UCS) algorithm, which is actually optimal. We also present the results of optimal and suboptimal experiments, in which UCS is compared with the UBB optimal branch-and-bound algorithm and the SFFS heuristic, respectively. We show that, in both experiments, $\proc{UCS}$ had a better performance than its competitor. Finally, we analyze the obtained results and point out improvements on UCS that might enhance the performance of this algorithm. version:1
arxiv-1407-6027 | Modeling languages from graph networks | http://arxiv.org/abs/1407.6027 | id:1407.6027 author:Alberto Besana, Cristina Martínez category:cs.CL math.CO  published:2014-07-22 summary:We model and compute the probability distribution of the letters in random generated words in a language by using the theory of set partitions, Young tableaux and graph theoretical representation methods. This has been of interest for several application areas such as network systems, bioinformatics, internet search, data mining and computacional linguistics. version:1
arxiv-1407-5976 | Detection of Sclerotic Spine Metastases via Random Aggregation of Deep Convolutional Neural Network Classifications | http://arxiv.org/abs/1407.5976 | id:1407.5976 author:Holger R. Roth, Jianhua Yao, Le Lu, James Stieger, Joseph E. Burns, Ronald M. Summers category:cs.CV  published:2014-07-22 summary:Automated detection of sclerotic metastases (bone lesions) in Computed Tomography (CT) images has potential to be an important tool in clinical practice and research. State-of-the-art methods show performance of 79% sensitivity or true-positive (TP) rate, at 10 false-positives (FP) per volume. We design a two-tiered coarse-to-fine cascade framework to first operate a highly sensitive candidate generation system at a maximum sensitivity of ~92% but with high FP level (~50 per patient). Regions of interest (ROI) for lesion candidates are generated in this step and function as input for the second tier. In the second tier we generate N 2D views, via scale, random translations, and rotations with respect to each ROI centroid coordinates. These random views are used to train a deep Convolutional Neural Network (CNN) classifier. In testing, the CNN is employed to assign individual probabilities for a new set of N random views that are averaged at each ROI to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. We validate the approach on CT images of 59 patients (49 with sclerotic metastases and 10 normal controls). The proposed method reduces the number of FP/vol. from 4 to 1.2, 7 to 3, and 12 to 9.5 when comparing a sensitivity rates of 60%, 70%, and 80% respectively in testing. The Area-Under-the-Curve (AUC) is 0.834. The results show marked improvement upon previous work. version:1
arxiv-1403-5556 | Learning to Optimize via Information-Directed Sampling | http://arxiv.org/abs/1403.5556 | id:1403.5556 author:Daniel Russo, Benjamin Van Roy category:cs.LG  published:2014-03-21 summary:We propose information-directed sampling -- a new algorithm for online optimization problems in which a decision-maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between squared expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation. We establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. For the widely studied Bernoulli, Gaussian, and linear bandit problems, we demonstrate simulation performance surpassing popular approaches, including upper confidence bound algorithms, Thompson sampling, and the knowledge gradient algorithm. Further, we present simple analytic examples illustrating that, due to the way it measures information gain, information-directed sampling can dramatically outperform upper confidence bound algorithms and Thompson sampling. version:4
arxiv-1404-4467 | Cube-Cut: Vertebral Body Segmentation in MRI-Data through Cubic-Shaped Divergences | http://arxiv.org/abs/1404.4467 | id:1404.4467 author:Robert Schwarzenberg, Bernd Freisleben, Christopher Nimsky, Jan Egger category:cs.CV  published:2014-04-17 summary:In this article, we present a graph-based method using a cubic template for volumetric segmentation of vertebrae in magnetic resonance imaging (MRI) acquisitions. The user can define the degree of deviation from a regular cube via a smoothness value Delta. The Cube-Cut algorithm generates a directed graph with two terminal nodes (s-t-network), where the nodes of the graph correspond to a cubic-shaped subset of the image's voxels. The weightings of the graph's terminal edges, which connect every node with a virtual source s or a virtual sink t, represent the affinity of a voxel to the vertebra (source) and to the background (sink). Furthermore, a set of infinite weighted and non-terminal edges implements the smoothness term. After graph construction, a minimal s-t-cut is calculated within polynomial computation time, which splits the nodes into two disjoint units. Subsequently, the segmentation result is determined out of the source-set. A quantitative evaluation of a C++ implementation of the algorithm resulted in an average Dice Similarity Coefficient (DSC) of 81.33% and a running time of less than a minute. version:2
arxiv-1308-3383 | Axioms for graph clustering quality functions | http://arxiv.org/abs/1308.3383 | id:1308.3383 author:Twan van Laarhoven, Elena Marchiori category:cs.CV cs.LG stat.ML  published:2013-08-15 summary:We investigate properties that intuitively ought to be satisfied by graph clustering quality functions, that is, functions that assign a score to a clustering of a graph. Graph clustering, also known as network community detection, is often performed by optimizing such a function. Two axioms tailored for graph clustering quality functions are introduced, and the four axioms introduced in previous work on distance based clustering are reformulated and generalized for the graph setting. We show that modularity, a standard quality function for graph clustering, does not satisfy all of these six properties. This motivates the derivation of a new family of quality functions, adaptive scale modularity, which does satisfy the proposed axioms. Adaptive scale modularity has two parameters, which give greater flexibility in the kinds of clusterings that can be found. Standard graph clustering quality functions, such as normalized cut and unnormalized cut, are obtained as special cases of adaptive scale modularity. In general, the results of our investigation indicate that the considered axiomatic framework covers existing `good' quality functions for graph clustering, and can be used to derive an interesting new family of quality functions. version:2
arxiv-1407-5924 | Resolution-limit-free and local Non-negative Matrix Factorization quality functions for graph clustering | http://arxiv.org/abs/1407.5924 | id:1407.5924 author:Twan van Laarhoven, Elena Marchiori category:stat.ML  published:2014-07-22 summary:Many graph clustering quality functions suffer from a resolution limit, the inability to find small clusters in large graphs. So called resolution-limit-free quality functions do not have this limit. This property was previously introduced for hard clustering, that is, graph partitioning. We investigate the resolution-limit-free property in the context of Non-negative Matrix Factorization (NMF) for hard and soft graph clustering. To use NMF in the hard clustering setting, a common approach is to assign each node to its highest membership cluster. We show that in this case symmetric NMF is not resolution-limit-free, but that it becomes so when hardness constraints are used as part of the optimization. The resulting function is strongly linked to the Constant Potts Model. In soft clustering, nodes can belong to more than one cluster, with varying degrees of membership. In this setting resolution-limit-free turns out to be too strong a property. Therefore we introduce locality, which roughly states that changing one part of the graph does not affect the clustering of other parts of the graph. We argue that this is a desirable property, provide conditions under which NMF quality functions are local, and propose a novel class of local probabilistic NMF quality functions for soft graph clustering. version:1
arxiv-1107-5850 | Confidence-Based Dynamic Classifier Combination For Mean-Shift Tracking | http://arxiv.org/abs/1107.5850 | id:1107.5850 author:Ibrahim Saygin Topkaya, Hakan Erdogan category:cs.CV I.4.8  published:2011-07-29 summary:We introduce a novel tracking technique which uses dynamic confidence-based fusion of two different information sources for robust and efficient tracking of visual objects. Mean-shift tracking is a popular and well known method used in object tracking problems. Originally, the algorithm uses a similarity measure which is optimized by shifting a search area to the center of a generated weight image to track objects. Recent improvements on the original mean-shift algorithm involves using a classifier that differentiates the object from its surroundings. We adopt this classifier-based approach and propose an application of a classifier fusion technique within this classifier-based context in this work. We use two different classifiers, where one comes from a background modeling method, to generate the weight image and we calculate contributions of the classifiers dynamically using their confidences to generate a final weight image to be used in tracking. The contributions of the classifiers are calculated by using correlations between histograms of their weight images and histogram of a defined ideal weight image in the previous frame. We show with experiments that our dynamic combination scheme selects good contributions for classifiers for different cases and improves tracking accuracy significantly. version:2
arxiv-1406-6962 | How good are detection proposals, really? | http://arxiv.org/abs/1406.6962 | id:1406.6962 author:Jan Hosang, Rodrigo Benenson, Bernt Schiele category:cs.CV  published:2014-06-26 summary:Current top performing Pascal VOC object detectors employ detection proposals to guide the search for objects thereby avoiding exhaustive sliding window search across images. Despite the popularity of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in depth analysis of ten object proposal methods along with four baselines regarding ground truth annotation recall (on Pascal VOC 2007 and ImageNet 2013), repeatability, and impact on DPM detector performance. Our findings show common weaknesses of existing methods, and provide insights to choose the most adequate method for different settings. version:2
arxiv-1407-5820 | Approximate Regularization Path for Nuclear Norm Based H2 Model Reduction | http://arxiv.org/abs/1407.5820 | id:1407.5820 author:Niclas Blomberg, Cristian R. Rojas, Bo Wahlberg category:cs.SY math.OC stat.ML  published:2014-07-22 summary:This paper concerns model reduction of dynamical systems using the nuclear norm of the Hankel matrix to make a trade-off between model fit and model complexity. This results in a convex optimization problem where this trade-off is determined by one crucial design parameter. The main contribution is a methodology to approximately calculate all solutions up to a certain tolerance to the model reduction problem as a function of the design parameter. This is called the regularization path in sparse estimation and is a very important tool in order to find the appropriate balance between fit and complexity. We extend this to the more complicated nuclear norm case. The key idea is to determine when to exactly calculate the optimal solution using an upper bound based on the so-called duality gap. Hence, by solving a fixed number of optimization problems the whole regularization path up to a given tolerance can be efficiently computed. We illustrate this approach on some numerical examples. version:1
arxiv-1407-5807 | Multi-agents adaptive estimation and coverage control using Gaussian regression | http://arxiv.org/abs/1407.5807 | id:1407.5807 author:Andrea Carron, Marco Todescato, Ruggero Carli, Luca Schenato, Gianluigi Pillonetto category:cs.MA cs.SY stat.ML  published:2014-07-22 summary:We consider a scenario where the aim of a group of agents is to perform the optimal coverage of a region according to a sensory function. In particular, centroidal Voronoi partitions have to be computed. The difficulty of the task is that the sensory function is unknown and has to be reconstructed on line from noisy measurements. Hence, estimation and coverage needs to be performed at the same time. We cast the problem in a Bayesian regression framework, where the sensory function is seen as a Gaussian random field. Then, we design a set of control inputs which try to well balance coverage and estimation, also discussing convergence properties of the algorithm. Numerical experiments show the effectivness of the new approach. version:1
arxiv-1010-2457 | Optimal designs for Lasso and Dantzig selector using Expander Codes | http://arxiv.org/abs/1010.2457 | id:1010.2457 author:Yohann de Castro category:math.ST cs.IT math.IT math.PR stat.ME stat.ML stat.TH 62G05  62J05  62J12  published:2010-10-12 summary:We investigate the high-dimensional regression problem using adjacency matrices of unbalanced expander graphs. In this frame, we prove that the $\ell_{2}$-prediction error and the $\ell_{1}$-risk of the lasso and the Dantzig selector are optimal up to an explicit multiplicative constant. Thus we can estimate a high-dimensional target vector with an error term similar to the one obtained in a situation where one knows the support of the largest coordinates in advance. Moreover, we show that these design matrices have an explicit restricted eigenvalue. Precisely, they satisfy the restricted eigenvalue assumption and the compatibility condition with an explicit constant. Eventually, we capitalize on the recent construction of unbalanced expander graphs due to Guruswami, Umans, and Vadhan, to provide a deterministic polynomial time construction of these design matrices. version:6
arxiv-1407-5759 | Aggregation of local parametric candidates with exemplar-based occlusion handling for optical flow | http://arxiv.org/abs/1407.5759 | id:1407.5759 author:Denis Fortun, Patrick Bouthemy, Charles Kervrann category:cs.CV  published:2014-07-22 summary:Handling all together large displacements, motion details and occlusions remains an open issue for reliable computation of optical flow in a video sequence. We propose a two-step aggregation paradigm to address this problem. The idea is to supply local motion candidates at every pixel in a first step, and then to combine them to determine the global optical flow field in a second step. We exploit local parametric estimations combined with patch correspondences and we experimentally demonstrate that they are sufficient to produce highly accurate motion candidates. The aggregation step is designed as the discrete optimization of a global regularized energy. The occlusion map is estimated jointly with the flow field throughout the two steps. We propose a generic exemplar-based approach for occlusion filling with motion vectors. We achieve state-of-the-art results in computer vision benchmarks, with particularly significant improvements in the case of large displacements and occlusions. version:1
arxiv-1407-5754 | Tree-based iterated local search for Markov random fields with applications in image analysis | http://arxiv.org/abs/1407.5754 | id:1407.5754 author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:cs.AI cs.CV math.OC  published:2014-07-22 summary:The \emph{maximum a posteriori} (MAP) assignment for general structure Markov random fields (MRFs) is computationally intractable. In this paper, we exploit tree-based methods to efficiently address this problem. Our novel method, named Tree-based Iterated Local Search (T-ILS) takes advantage of the tractability of tree-structures embedded within MRFs to derive strong local search in an ILS framework. The method efficiently explores exponentially large neighborhood and does so with limited memory without any requirement on the cost functions. We evaluate the T-ILS in a simulation of Ising model and two real-world problems in computer vision: stereo matching, image denoising. Experimental results demonstrate that our methods are competitive against state-of-the-art rivals with a significant computational gain. version:1
arxiv-1407-5753 | Improved Onlooker Bee Phase in Artificial Bee Colony Algorithm | http://arxiv.org/abs/1407.5753 | id:1407.5753 author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.NE  published:2014-07-22 summary:Artificial Bee Colony (ABC) is a distinguished optimization strategy that can resolve nonlinear and multifaceted problems. It is comparatively a straightforward and modern population based probabilistic approach for comprehensive optimization. In the vein of the other population based algorithms, ABC is moreover computationally classy due to its slow nature of search procedure. The solution exploration equation of ABC is extensively influenced by a arbitrary quantity which helps in exploration at the cost of exploitation of the better search space. In the solution exploration equation of ABC due to the outsized step size the chance of skipping the factual solution is high. Therefore, here this paper improve onlooker bee phase with help of a local search strategy inspired by memetic algorithm to balance the diversity and convergence capability of the ABC. The proposed algorithm is named as Improved Onlooker Bee Phase in ABC (IoABC). It is tested over 12 well known un-biased test problems of diverse complexities and two engineering optimization problems; results show that the anticipated algorithm go one better than the basic ABC and its recent deviations in a good number of the experiments. version:1
arxiv-1407-5739 | Global optimization using Lévy flights | http://arxiv.org/abs/1407.5739 | id:1407.5739 author:Truyen Tran, Trung Thanh Nguyen, Hoang Linh Nguyen category:cs.NE  published:2014-07-22 summary:This paper studies a class of enhanced diffusion processes in which random walkers perform L\'evy flights and apply it for global optimization. L\'evy flights offer controlled balance between exploitation and exploration. We develop four optimization algorithms based on such properties. We compare new algorithms with the well-known Simulated Annealing on hard test functions and the results are very promising. version:1
arxiv-1407-5736 | Learning Rich Features from RGB-D Images for Object Detection and Segmentation | http://arxiv.org/abs/1407.5736 | id:1407.5736 author:Saurabh Gupta, Ross Girshick, Pablo Arbeláez, Jitendra Malik category:cs.CV cs.RO  published:2014-07-22 summary:In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3%, which is a 56% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics. version:1
arxiv-1407-5719 | Artificial Life and the Web: WebAL Comes of Age | http://arxiv.org/abs/1407.5719 | id:1407.5719 author:Tim Taylor category:cs.NE cs.MA  published:2014-07-22 summary:A brief survey is presented of the first 18 years of web-based Artificial Life ("WebAL") research and applications, covering the period 1995-2013. The survey is followed by a short discussion of common methodologies employed and current technologies relevant to WebAL research. The paper concludes with a quick look at what the future may hold for work in this exciting area. version:1
arxiv-1407-5536 | Multichannel Compressive Sensing MRI Using Noiselet Encoding | http://arxiv.org/abs/1407.5536 | id:1407.5536 author:Kamlesh Pawar, Gary F. Egan, Jingxin Zhang category:physics.med-ph cs.CV  published:2014-07-21 summary:The incoherence between measurement and sparsifying transform matrices and the restricted isometry property (RIP) of measurement matrix are two of the key factors in determining the performance of compressive sensing (CS). In CS-MRI, the randomly under-sampled Fourier matrix is used as the measurement matrix and the wavelet transform is usually used as sparsifying transform matrix. However, the incoherence between the randomly under-sampled Fourier matrix and the wavelet matrix is not optimal, which can deteriorate the performance of CS-MRI. Using the mathematical result that noiselets are maximally incoherent with wavelets, this paper introduces the noiselet unitary bases as the measurement matrix to improve the incoherence and RIP in CS-MRI, and presents a method to design the pulse sequence for the noiselet encoding. This novel encoding scheme is combined with the multichannel compressive sensing (MCS) framework to take the advantage of multichannel data acquisition used in MRI scanners. An empirical RIP analysis is presented to compare the multichannel noiselet and multichannel Fourier measurement matrices in MCS. Simulations are presented in the MCS framework to compare the performance of noiselet encoding reconstructions and Fourier encoding reconstructions at different acceleration factors. The comparisons indicate that multichannel noiselet measurement matrix has better RIP than that of its Fourier counterpart, and that noiselet encoded MCS-MRI outperforms Fourier encoded MCS-MRI in preserving image resolution and can achieve higher acceleration factors. To demonstrate the feasibility of the proposed noiselet encoding scheme, two pulse sequences with tailored spatially selective RF excitation pulses was designed and implemented on a 3T scanner to acquire the data in the noiselet domain from a phantom and a human brain. version:2
arxiv-1312-7446 | Shape Primitive Histogram: A Novel Low-Level Face Representation for Face Recognition | http://arxiv.org/abs/1312.7446 | id:1312.7446 author:Sheng Huang, Dan Yang, Haopeng Zhang, Luwen Huangfu, Xiaohong Zhang category:cs.CV  published:2013-12-28 summary:We further exploit the representational power of Haar wavelet and present a novel low-level face representation named Shape Primitives Histogram (SPH) for face recognition. Since human faces exist abundant shape features, we address the face representation issue from the perspective of the shape feature extraction. In our approach, we divide faces into a number of tiny shape fragments and reduce these shape fragments to several uniform atomic shape patterns called Shape Primitives. A convolution with Haar Wavelet templates is applied to each shape fragment to identify its belonging shape primitive. After that, we do a histogram statistic of shape primitives in each spatial local image patch for incorporating the spatial information. Finally, each face is represented as a feature vector via concatenating all the local histograms of shape primitives. Four popular face databases, namely ORL, AR, Yale-B and LFW-a databases, are employed to evaluate SPH and experimentally study the choices of the parameters. Extensive experimental results demonstrate that the proposed approach outperform the state-of-the-arts. version:3
arxiv-1212-3669 | A metric for software vulnerabilities classification | http://arxiv.org/abs/1212.3669 | id:1212.3669 author:Gabriele Modena category:cs.SE cs.LG  published:2012-12-15 summary:Vulnerability discovery and exploits detection are two wide areas of study in software engineering. This preliminary work tries to combine existing methods with machine learning techniques to define a metric classification of vulnerable computer programs. First a feature set has been defined and later two models have been tested against real world vulnerabilities. A relation between the classifier choice and the features has also been outlined. version:2
arxiv-1407-5602 | Predictive support recovery with TV-Elastic Net penalty and logistic regression: an application to structural MRI | http://arxiv.org/abs/1407.5602 | id:1407.5602 author:Mathieu Dubois, Fouad Hadj-Selem, Tommy Lofstedt, Matthieu Perrot, Clara Fischer, Vincent Frouin, Edouard Duchesnay category:stat.ML  published:2014-07-21 summary:The use of machine-learning in neuroimaging offers new perspectives in early diagnosis and prognosis of brain diseases. Although such multivariate methods can capture complex relationships in the data, traditional approaches provide irregular (l2 penalty) or scattered (l1 penalty) predictive pattern with a very limited relevance. A penalty like Total Variation (TV) that exploits the natural 3D structure of the images can increase the spatial coherence of the weight map. However, TV penalization leads to non-smooth optimization problems that are hard to minimize. We propose an optimization framework that minimizes any combination of l1, l2, and TV penalties while preserving the exact l1 penalty. This algorithm uses Nesterov's smoothing technique to approximate the TV penalty with a smooth function such that the loss and the penalties are minimized with an exact accelerated proximal gradient algorithm. We propose an original continuation algorithm that uses successively smaller values of the smoothing parameter to reach a prescribed precision while achieving the best possible convergence rate. This algorithm can be used with other losses or penalties. The algorithm is applied on a classification problem on the ADNI dataset. We observe that the TV penalty does not necessarily improve the prediction but provides a major breakthrough in terms of support recovery of the predictive brain regions. version:1
arxiv-1407-5574 | A Novel Hybrid Crossover based Artificial Bee Colony Algorithm for Optimization Problem | http://arxiv.org/abs/1407.5574 | id:1407.5574 author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.AI cs.NE  published:2014-07-21 summary:Artificial bee colony (ABC) algorithm has proved its importance in solving a number of problems including engineering optimization problems. ABC algorithm is one of the most popular and youngest member of the family of population based nature inspired meta-heuristic swarm intelligence method. ABC has been proved its superiority over some other Nature Inspired Algorithms (NIA) when applied for both benchmark functions and real world problems. The performance of search process of ABC depends on a random value which tries to balance exploration and exploitation phase. In order to increase the performance it is required to balance the exploration of search space and exploitation of optimal solution of the ABC. This paper outlines a new hybrid of ABC algorithm with Genetic Algorithm. The proposed method integrates crossover operation from Genetic Algorithm (GA) with original ABC algorithm. The proposed method is named as Crossover based ABC (CbABC). The CbABC strengthens the exploitation phase of ABC as crossover enhances exploration of search space. The CbABC tested over four standard benchmark functions and a popular continuous optimization problem. version:1
arxiv-1211-3601 | Statistical inference on errorfully observed graphs | http://arxiv.org/abs/1211.3601 | id:1211.3601 author:Carey E. Priebe, Daniel L. Sussman, Minh Tang, Joshua T. Vogelstein category:stat.ML  published:2012-11-15 summary:Statistical inference on graphs is a burgeoning field in the applied and theoretical statistics communities, as well as throughout the wider world of science, engineering, business, etc. In many applications, we are faced with the reality of errorfully observed graphs. That is, the existence of an edge between two vertices is based on some imperfect assessment. In this paper, we consider a graph $G = (V,E)$. We wish to perform an inference task -- the inference task considered here is "vertex classification". However, we do not observe $G$; rather, for each potential edge $uv \in {{V}\choose{2}}$ we observe an "edge-feature" which we use to classify $uv$ as edge/not-edge. Thus we errorfully observe $G$ when we observe the graph $\widetilde{G} = (V,\widetilde{E})$ as the edges in $\widetilde{E}$ arise from the classifications of the "edge-features", and are expected to be errorful. Moreover, we face a quantity/quality trade-off regarding the edge-features we observe -- more informative edge-features are more expensive, and hence the number of potential edges that can be assessed decreases with the quality of the edge-features. We studied this problem by formulating a quantity/quality tradeoff for a simple class of random graphs model, namely the stochastic blockmodel. We then consider a simple but optimal vertex classifier for classifying $v$ and we derive the optimal quantity/quality operating point for subsequent graph inference in the face of this trade-off. The optimal operating points for the quantity/quality trade-off are surprising and illustrate the issue that methods for intermediate tasks should be chosen to maximize performance for the ultimate inference task. Finally, we investigate the quantity/quality tradeoff for errorful obesrvations of the {\it C.\ elegans} connectome graph. version:4
arxiv-1306-2979 | Completing Any Low-rank Matrix, Provably | http://arxiv.org/abs/1306.2979 | id:1306.2979 author:Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, Rachel Ward category:stat.ML cs.IT cs.LG math.IT  published:2013-06-12 summary:Matrix completion, i.e., the exact and provable recovery of a low-rank matrix from a small subset of its elements, is currently only known to be possible if the matrix satisfies a restrictive structural constraint---known as {\em incoherence}---on its row and column spaces. In these cases, the subset of elements is sampled uniformly at random. In this paper, we show that {\em any} rank-$ r $ $ n$-by-$ n $ matrix can be exactly recovered from as few as $O(nr \log^2 n)$ randomly chosen elements, provided this random choice is made according to a {\em specific biased distribution}: the probability of any element being sampled should be proportional to the sum of the leverage scores of the corresponding row, and column. Perhaps equally important, we show that this specific form of sampling is nearly necessary, in a natural precise sense; this implies that other perhaps more intuitive sampling schemes fail. We further establish three ways to use the above result for the setting when leverage scores are not known \textit{a priori}: (a) a sampling strategy for the case when only one of the row or column spaces are incoherent, (b) a two-phase sampling procedure for general matrices that first samples to estimate leverage scores followed by sampling for exact recovery, and (c) an analysis showing the advantages of weighted nuclear/trace-norm minimization over the vanilla un-weighted formulation for the case of non-uniform sampling. version:4
arxiv-1407-5397 | Are There Good Mistakes? A Theoretical Analysis of CEGIS | http://arxiv.org/abs/1407.5397 | id:1407.5397 author:Susmit Jha, Sanjit A. Seshia category:cs.LO cs.AI cs.LG cs.PL  published:2014-07-21 summary:Counterexample-guided inductive synthesis CEGIS is used to synthesize programs from a candidate space of programs. The technique is guaranteed to terminate and synthesize the correct program if the space of candidate programs is finite. But the technique may or may not terminate with the correct program if the candidate space of programs is infinite. In this paper, we perform a theoretical analysis of counterexample-guided inductive synthesis technique. We investigate whether the set of candidate spaces for which the correct program can be synthesized using CEGIS depends on the counterexamples used in inductive synthesis, that is, whether there are good mistakes which would increase the synthesis power. We investigate whether the use of minimal counterexamples instead of arbitrary counterexamples expands the set of candidate spaces of programs for which inductive synthesis can successfully synthesize a correct program. We consider two kinds of counterexamples: minimal counterexamples and history bounded counterexamples. The history bounded counterexample used in any iteration of CEGIS is bounded by the examples used in previous iterations of inductive synthesis. We examine the relative change in power of inductive synthesis in both cases. We show that the synthesis technique using minimal counterexamples MinCEGIS has the same synthesis power as CEGIS but the synthesis technique using history bounded counterexamples HCEGIS has different power than that of CEGIS, but none dominates the other. version:1
arxiv-1312-1733 | Impact of regularization on Spectral Clustering | http://arxiv.org/abs/1312.1733 | id:1312.1733 author:Antony Joseph, Bin Yu category:stat.ML  published:2013-12-05 summary:The performance of spectral clustering can be considerably improved via regularization, as demonstrated empirically in Amini et. al (2012). Here, we provide an attempt at quantifying this improvement through theoretical analysis. Under the stochastic block model (SBM), and its extensions, previous results on spectral clustering relied on the minimum degree of the graph being sufficiently large for its good performance. By examining the scenario where the regularization parameter $\tau$ is large we show that the minimum degree assumption can potentially be removed. As a special case, for an SBM with two blocks, the results require the maximum degree to be large (grow faster than $\log n$) as opposed to the minimum degree. More importantly, we show the usefulness of regularization in situations where not all nodes belong to well-defined clusters. Our results rely on a `bias-variance'-like trade-off that arises from understanding the concentration of the sample Laplacian and the eigen gap as a function of the regularization parameter. As a byproduct of our bounds, we propose a data-driven technique \textit{DKest} (standing for estimated Davis-Kahan bounds) for choosing the regularization parameter. This technique is shown to work well through simulations and on a real data set. version:2
arxiv-1407-5367 | Certifying the Existence of Epipolar Matrices | http://arxiv.org/abs/1407.5367 | id:1407.5367 author:Sameer Agarwal, Hon-leung Lee, Bernd Sturmfels, Rekha R. Thomas category:cs.CV math.AG  published:2014-07-21 summary:Given a set of point correspondences in two images, the existence of a fundamental matrix is a necessary condition for the points to be the images of a 3-dimensional scene imaged with two pinhole cameras. If the camera calibration is known then one requires the existence of an essential matrix. We present an efficient algorithm, using exact linear algebra, for testing the existence of a fundamental matrix. The input is any number of point correspondences. For essential matrices, we characterize the solvability of the Demazure polynomials. In both scenarios, we determine which linear subspaces intersect a fixed set defined by non-linear polynomials. The conditions we derive are polynomials stated purely in terms of image coordinates. They represent a new class of two-view invariants, free of fundamental (resp.~essential)~matrices. version:1
arxiv-1407-5358 | Practical Kernel-Based Reinforcement Learning | http://arxiv.org/abs/1407.5358 | id:1407.5358 author:André M. S. Barreto, Doina Precup, Joelle Pineau category:cs.LG cs.AI stat.ML I.2.8; I.2.6; G.3  published:2014-07-21 summary:Kernel-based reinforcement learning (KBRL) stands out among reinforcement learning algorithms for its strong theoretical guarantees. By casting the learning problem as a local kernel approximation, KBRL provides a way of computing a decision policy which is statistically consistent and converges to a unique solution. Unfortunately, the model constructed by KBRL grows with the number of sample transitions, resulting in a computational cost that precludes its application to large-scale or on-line domains. In this paper we introduce an algorithm that turns KBRL into a practical reinforcement learning tool. Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a transition matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix, potentially much smaller, which retains some fundamental properties of its precursor. KBSF exploits such an insight to compress the information contained in KBRL's model into an approximator of fixed size. This makes it possible to build an approximation that takes into account both the difficulty of the problem and the associated computational cost. KBSF's computational complexity is linear in the number of sample transitions, which is the best one can do without discarding data. Moreover, the algorithm's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions. The result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We derive upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data. We also illustrate the potential of our algorithm in an extensive empirical study in which KBSF is applied to difficult tasks based on real-world data. version:1
arxiv-1304-8126 | Robust Spectral Compressed Sensing via Structured Matrix Completion | http://arxiv.org/abs/1304.8126 | id:1304.8126 author:Yuxin Chen, Yuejie Chi category:cs.IT cs.SY math.IT math.NA stat.ML  published:2013-04-30 summary:The paper explores the problem of \emph{spectral compressed sensing}, which aims to recover a spectrally sparse signal from a small random subset of its $n$ time domain samples. The signal of interest is assumed to be a superposition of $r$ multi-dimensional complex sinusoids, while the underlying frequencies can assume any \emph{continuous} values in the normalized frequency domain. Conventional compressed sensing paradigms suffer from the basis mismatch issue when imposing a discrete dictionary on the Fourier representation. To address this issue, we develop a novel algorithm, called \emph{Enhanced Matrix Completion (EMaC)}, based on structured matrix completion that does not require prior knowledge of the model order. The algorithm starts by arranging the data into a low-rank enhanced form exhibiting multi-fold Hankel structure, and then attempts recovery via nuclear norm minimization. Under mild incoherence conditions, EMaC allows perfect recovery as soon as the number of samples exceeds the order of $r\log^{4}n$, and is stable against bounded noise. Even if a constant portion of samples are corrupted with arbitrary magnitude, EMaC still allows exact recovery, provided that the sample complexity exceeds the order of $r^{2}\log^{3}n$. Along the way, our results demonstrate the power of convex relaxation in completing a low-rank multi-fold Hankel or Toeplitz matrix from minimal observed entries. The performance of our algorithm and its applicability to super resolution are further validated by numerical experiments. version:5
arxiv-1312-0286 | Efficient Learning and Planning with Compressed Predictive States | http://arxiv.org/abs/1312.0286 | id:1312.0286 author:William L. Hamilton, Mahdi Milani Fard, Joelle Pineau category:cs.LG stat.ML  published:2013-12-01 summary:Predictive state representations (PSRs) offer an expressive framework for modelling partially observable systems. By compactly representing systems as functions of observable quantities, the PSR learning approach avoids using local-minima prone expectation-maximization and instead employs a globally optimal moment-based algorithm. Moreover, since PSRs do not require a predetermined latent state structure as an input, they offer an attractive framework for model-based reinforcement learning when agents must plan without a priori access to a system model. Unfortunately, the expressiveness of PSRs comes with significant computational cost, and this cost is a major factor inhibiting the use of PSRs in applications. In order to alleviate this shortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSR learning approach combines recent advancements in dimensionality reduction, incremental matrix decomposition, and compressed sensing. We show how this approach provides a principled avenue for learning accurate approximations of PSRs, drastically reducing the computational costs associated with learning while also providing effective regularization. Going further, we propose a planning framework which exploits these learned models. And we show that this approach facilitates model-learning and planning in large complex partially observable domains, a task that is infeasible without the principled use of compression. version:2
arxiv-1407-5327 | A Comparative Analysis for Determining the Optimal Path using PSO and GA | http://arxiv.org/abs/1407.5327 | id:1407.5327 author:Kavitha Sooda, T. R. Gopalakrishnan Nair category:cs.NI cs.NE  published:2014-07-20 summary:Significant research has been carried out recently to find the optimal path in network routing. Among them, the evolutionary algorithm approach is an area where work is carried out extensively. We in this paper have used particle swarm optimization (PSO) and genetic algorithm (GA) for finding the optimal path and the concept of region based network is introduced along with the use of indirect encoding. We demonstrate the advantage of fitness value and hop count in both PSO and GA. A comparative study of PSO and genetic algorithm (GA) is carried out, and it was found that PSO converged to arrive at the optimal path much faster than GA. version:1
arxiv-1407-5324 | Optimized Method for Iranian Road Signs Detection and recognition system | http://arxiv.org/abs/1407.5324 | id:1407.5324 author:Reza Azad, Babak Azad, Iman Tavakoli Kazerooni category:cs.CV  published:2014-07-20 summary:Road sign recognition is one of the core technologies in Intelligent Transport Systems. In the current study, a robust and real-time method is presented to identify and detect the roads speed signs in road image in different situations. In our proposed method, first, the connected components are created in the main image using the edge detection and mathematical morphology and the location of the road signs extracted by the geometric and color data; then the letters are segmented and recognized by Multiclass Support Vector Machine (SVMs) classifiers. Regarding that the geometric and color features ate properly used in detection the location of the road signs, so it is not sensitive to the distance and noise and has higher speed and efficiency. In the result part, the proposed approach is applied on Iranian road speed sign database and the detection and recognition accuracy rate achieved 98.66% and 100% respectively. version:1
arxiv-1407-5242 | Object Proposal Generation using Two-Stage Cascade SVMs | http://arxiv.org/abs/1407.5242 | id:1407.5242 author:Ziming Zhang, Philip H. S. Torr category:cs.CV  published:2014-07-20 summary:Object proposal algorithms have shown great promise as a first step for object recognition and detection. Good object proposal generation algorithms require high object recall rate as well as low computational cost, because generating object proposals is usually utilized as a preprocessing step. The problem of how to accelerate the object proposal generation and evaluation process without decreasing recall is thus of great interest. In this paper, we propose a new object proposal generation method using two-stage cascade SVMs, where in the first stage linear filters are learned for predefined quantized scales/aspect-ratios independently, and in the second stage a global linear classifier is learned across all the quantized scales/aspect-ratios for calibration, so that all the proposals can be compared properly. The proposals with highest scores are our final output. Specifically, we explain our scale/aspect-ratio quantization scheme, and investigate the effects of combinations of $\ell_1$ and $\ell_2$ regularizers in cascade SVMs with/without ranking constraints in learning. Comprehensive experiments on VOC2007 dataset are conducted, and our results achieve the state-of-the-art performance with high object recall rate and high computational efficiency. Besides, our method has been demonstrated to be suitable for not only class-specific but also generic object proposal generation. version:1
arxiv-1312-1492 | A fast and robust algorithm to count topologically persistent holes in noisy clouds | http://arxiv.org/abs/1312.1492 | id:1312.1492 author:Vitaliy Kurlin category:cs.CG cs.CV math.AT  published:2013-12-05 summary:Preprocessing a 2D image often produces a noisy cloud of interest points. We study the problem of counting holes in unorganized clouds in the plane. The holes in a given cloud are quantified by the topological persistence of their boundary contours when the cloud is analyzed at all possible scales. We design the algorithm to count holes that are most persistent in the filtration of offsets (neighborhoods) around given points. The input is a cloud of $n$ points in the plane without any user-defined parameters. The algorithm has $O(n\log n)$ time and $O(n)$ space. The output is the array (number of holes, relative persistence in the filtration). We prove theoretical guarantees when the algorithm finds the correct number of holes (components in the complement) of an unknown shape approximated by a cloud. version:3
arxiv-1407-5212 | Context Aware Dynamic Traffic Signal Optimization | http://arxiv.org/abs/1407.5212 | id:1407.5212 author:Kandarp Khandwala, Rudra Sharma, Snehal Rao category:cs.AI cs.NE  published:2014-07-19 summary:Conventional urban traffic control systems have been based on historical traffic data. Later advancements made use of detectors, which enabled the gathering of real time traffic data, in order to reorganize and calibrate traffic signalization programs. Further evolvement provided the ability to forecast traffic conditions, in order to develop traffic signalization programs and strategies precomputed and applied at the most appropriate time frame for the optimal control of the current traffic conditions. We, propose the next generation of traffic control systems based on principles of Artificial Intelligence and Context Awareness. Most of the existing algorithms use average waiting time or length of the queue to assess an algorithms performance. However, a low average waiting time may come at the cost of delaying other vehicles indefinitely. In our algorithm, besides the vehicle queue, we use fairness also as an important performance metric to assess an algorithms performance. version:1
arxiv-1406-2963 | A machine-compiled macroevolutionary history of Phanerozoic life | http://arxiv.org/abs/1406.2963 | id:1406.2963 author:Shanan E. Peters, Ce Zhang, Miron Livny, Christopher Ré category:cs.DB cs.CL cs.LG q-bio.PE  published:2014-06-11 summary:Many aspects of macroevolutionary theory and our understanding of biotic responses to global environmental change derive from literature-based compilations of palaeontological data. Existing manually assembled databases are, however, incomplete and difficult to assess and enhance. Here, we develop and validate the quality of a machine reading system, PaleoDeepDive, that automatically locates and extracts data from heterogeneous text, tables, and figures in publications. PaleoDeepDive performs comparably to humans in complex data extraction and inference tasks and generates congruent synthetic macroevolutionary results. Unlike traditional databases, PaleoDeepDive produces a probabilistic database that systematically improves as information is added. We also show that the system can readily accommodate sophisticated data types, such as morphological data in biological illustrations and associated textual descriptions. Our machine reading approach to scientific data integration and synthesis brings within reach many questions that are currently underdetermined and does so in ways that may stimulate entirely new modes of inquiry. version:2
arxiv-1407-5908 | Exploiting Smoothness in Statistical Learning, Sequential Prediction, and Stochastic Optimization | http://arxiv.org/abs/1407.5908 | id:1407.5908 author:Mehrdad Mahdavi category:cs.LG  published:2014-07-19 summary:In the last several years, the intimate connection between convex optimization and learning problems, in both statistical and sequential frameworks, has shifted the focus of algorithmic machine learning to examine this interplay. In particular, on one hand, this intertwinement brings forward new challenges in reassessment of the performance of learning algorithms including generalization and regret bounds under the assumptions imposed by convexity such as analytical properties of loss functions (e.g., Lipschitzness, strong convexity, and smoothness). On the other hand, emergence of datasets of an unprecedented size, demands the development of novel and more efficient optimization algorithms to tackle large-scale learning problems. The overarching goal of this thesis is to reassess the smoothness of loss functions in statistical learning, sequential prediction/online learning, and stochastic optimization and explicate its consequences. In particular we examine how smoothness of loss function could be beneficial or detrimental in these settings in terms of sample complexity, statistical consistency, regret analysis, and convergence rate, and investigate how smoothness can be leveraged to devise more efficient learning algorithms. version:1
arxiv-1307-1058 | On the minimal teaching sets of two-dimensional threshold functions | http://arxiv.org/abs/1307.1058 | id:1307.1058 author:Max A. Alekseyev, Marina G. Basova, Nikolai Yu. Zolotykh category:math.CO cs.LG math.NT  published:2013-07-03 summary:It is known that a minimal teaching set of any threshold function on the twodimensional rectangular grid consists of 3 or 4 points. We derive exact formulae for the numbers of functions corresponding to these values and further refine them in the case of a minimal teaching set of size 3. We also prove that the average cardinality of the minimal teaching sets of threshold functions is asymptotically 7/2. We further present corollaries of these results concerning some special arrangements of lines in the plane. version:2
arxiv-1407-5104 | Pixels to Voxels: Modeling Visual Representation in the Human Brain | http://arxiv.org/abs/1407.5104 | id:1407.5104 author:Pulkit Agrawal, Dustin Stansbury, Jitendra Malik, Jack L. Gallant category:q-bio.NC cs.CV cs.NE  published:2014-07-18 summary:The human brain is adept at solving difficult high-level visual processing problems such as image interpretation and object recognition in natural scenes. Over the past few years neuroscientists have made remarkable progress in understanding how the human brain represents categories of objects and actions in natural scenes. However, all current models of high-level human vision operate on hand annotated images in which the objects and actions have been assigned semantic tags by a human operator. No current models can account for high-level visual function directly in terms of low-level visual input (i.e., pixels). To overcome this fundamental limitation we sought to develop a new class of models that can predict human brain activity directly from low-level visual input (i.e., pixels). We explored two classes of models drawn from computer vision and machine learning. The first class of models was based on Fisher Vectors (FV) and the second was based on Convolutional Neural Networks (ConvNets). We find that both classes of models accurately predict brain activity in high-level visual areas, directly from pixels and without the need for any semantic tags or hand annotation of images. This is the first time that such a mapping has been obtained. The fit models provide a new platform for exploring the functional principles of human vision, and they show that modern methods of computer vision and machine learning provide important tools for characterizing brain function. version:1
arxiv-1407-5017 | Bayesian Nonparametric Crowdsourcing | http://arxiv.org/abs/1407.5017 | id:1407.5017 author:Pablo G. Moreno, Yee Whye Teh, Fernando Perez-Cruz, Antonio Artés-Rodríguez category:stat.ML stat.AP  published:2014-07-18 summary:Crowdsourcing has been proven to be an effective and efficient tool to annotate large datasets. User annotations are often noisy, so methods to combine the annotations to produce reliable estimates of the ground truth are necessary. We claim that considering the existence of clusters of users in this combination step can improve the performance. This is especially important in early stages of crowdsourcing implementations, where the number of annotations is low. At this stage there is not enough information to accurately estimate the bias introduced by each annotator separately, so we have to resort to models that consider the statistical links among them. In addition, finding these clusters is interesting in itself as knowing the behavior of the pool of annotators allows implementing efficient active learning strategies. Based on this, we propose in this paper two new fully unsupervised models based on a Chinese Restaurant Process (CRP) prior and a hierarchical structure that allows inferring these groups jointly with the ground truth and the properties of the users. Efficient inference algorithms based on Gibbs sampling with auxiliary variables are proposed. Finally, we perform experiments, both on synthetic and real databases, to show the advantages of our models over state-of-the-art algorithms. version:1
arxiv-1407-7027 | Motor Learning Mechanism on the Neuron Scale | http://arxiv.org/abs/1407.7027 | id:1407.7027 author:Peilei Liu, Ting Wang category:q-bio.NC cs.NE physics.bio-ph  published:2014-07-18 summary:Based on existing data, we wish to put forward a biological model of motor system on the neuron scale. Then we indicate its implications in statistics and learning. Specifically, neuron firing frequency and synaptic strength are probability estimates in essence. And the lateral inhibition also has statistical implications. From the standpoint of learning, dendritic competition through retrograde messengers is the foundation of conditional reflex and grandmother cell coding. And they are the kernel mechanisms of motor learning and sensory motor integration respectively. Finally, we compare motor system with sensory system. In short, we would like to bridge the gap between molecule evidences and computational models. version:1
arxiv-1407-4979 | Deep Metric Learning for Practical Person Re-Identification | http://arxiv.org/abs/1407.4979 | id:1407.4979 author:Dong Yi, Zhen Lei, Stan Z. Li category:cs.CV cs.LG cs.NE  published:2014-07-18 summary:Various hand-crafted features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this paper proposes a more general way that can learn a similarity metric from image pixels directly. By using a "siamese" deep neural network, the proposed method can jointly learn the color feature, texture feature and metric in a unified framework. The network has a symmetry structure with two sub-networks which are connected by Cosine function. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and labels, which is proved to be robust to outliers. Compared to existing researches, a more practical setting is studied in the experiments that is training and test on different datasets (cross dataset person re-identification). Both in "intra dataset" and "cross dataset" settings, the superiorities of the proposed method are illustrated on VIPeR and PRID. version:1
arxiv-1407-4898 | Hand Pointing Detection Using Live Histogram Template of Forehead Skin | http://arxiv.org/abs/1407.4898 | id:1407.4898 author:Ghassem Tofighi, Nasser Ali Afarin, Kamraan Raahemifar, Anastasios N. Venetsanopoulos category:cs.CV  published:2014-07-18 summary:Hand pointing detection has multiple applications in many fields such as virtual reality and control devices in smart homes. In this paper, we proposed a novel approach to detect pointing vector in 2D space of a room. After background subtraction, face and forehead is detected. In the second step, forehead skin H-S plane histograms in HSV space is calculated. By using these histogram templates of users skin, and back projection method, skin areas are detected. The contours of hand are extracted using Freeman chain code algorithm. Next step is finding fingertips. Points in hand contour which are candidates for the fingertip can be found in convex defects of convex hull and contour. We introduced a novel method for finding the fingertip based on the special points on the contour and their relationships. Our approach detects hand-pointing vectors in live video from a common webcam with 94%TP and 85%TN. version:1
arxiv-1407-5093 | Classification of Passes in Football Matches using Spatiotemporal Data | http://arxiv.org/abs/1407.5093 | id:1407.5093 author:Michael Horton, Joachim Gudmundsson, Sanjay Chawla, Joël Estephan category:cs.LG cs.CG I.5.2  published:2014-07-18 summary:A knowledgeable observer of a game of football (soccer) can make a subjective evaluation of the quality of passes made between players during the game. We investigate the problem of producing an automated system to make the same evaluation of passes. We present a model that constructs numerical predictor variables from spatiotemporal match data using feature functions based on methods from computational geometry, and then learns a classification function from labelled examples of the predictor variables. Furthermore, the learned classifiers are analysed to determine if there is a relationship between the complexity of the algorithm that computed the predictor variable and the importance of the variable to the classifier. Experimental results show that we are able to produce a classifier with 85.8% accuracy on classifying passes as Good, OK or Bad, and that the predictor variables computed using complex methods from computational geometry are of moderate importance to the learned classifiers. Finally, we show that the inter-rater agreement on pass classification between the machine classifier and a human observer is of similar magnitude to the agreement between two observers. version:1
arxiv-1407-4874 | Affine Subspace Representation for Feature Description | http://arxiv.org/abs/1407.4874 | id:1407.4874 author:Zhenhua Wang, Bin Fan, Fuchao Wu category:cs.CV  published:2014-07-18 summary:This paper proposes a novel Affine Subspace Representation (ASR) descriptor to deal with affine distortions induced by viewpoint changes. Unlike the traditional local descriptors such as SIFT, ASR inherently encodes local information of multi-view patches, making it robust to affine distortions while maintaining a high discriminative ability. To this end, PCA is used to represent affine-warped patches as PCA-patch vectors for its compactness and efficiency. Then according to the subspace assumption, which implies that the PCA-patch vectors of various affine-warped patches of the same keypoint can be represented by a low-dimensional linear subspace, the ASR descriptor is obtained by using a simple subspace-to-point mapping. Such a linear subspace representation could accurately capture the underlying information of a keypoint (local structure) under multiple views without sacrificing its distinctiveness. To accelerate the computation of ASR descriptor, a fast approximate algorithm is proposed by moving the most computational part (ie, warp patch under various affine transformations) to an offline training stage. Experimental results show that ASR is not only better than the state-of-the-art descriptors under various image transformations, but also performs well without a dedicated affine invariant detector when dealing with viewpoint changes. version:1
arxiv-1407-4863 | A Comparative Study of Meta-heuristic Algorithms for Solving Quadratic Assignment Problem | http://arxiv.org/abs/1407.4863 | id:1407.4863 author:Gamal Abd El-Nasser A. Said, Abeer M. Mahmoud, El-Sayed M. El-Horbaty category:cs.AI cs.NE  published:2014-07-18 summary:Quadratic Assignment Problem (QAP) is an NP-hard combinatorial optimization problem, therefore, solving the QAP requires applying one or more of the meta-heuristic algorithms. This paper presents a comparative study between Meta-heuristic algorithms: Genetic Algorithm, Tabu Search, and Simulated annealing for solving a real-life (QAP) and analyze their performance in terms of both runtime efficiency and solution quality. The results show that Genetic Algorithm has a better solution quality while Tabu Search has a faster execution time in comparison with other Meta-heuristic algorithms for solving QAP. version:1
arxiv-1407-4739 | An landcover fuzzy logic classification by maximumlikelihood | http://arxiv.org/abs/1407.4739 | id:1407.4739 author:T. Sarath, G. Nagalakshmi category:cs.CV cs.LG  published:2014-07-17 summary:In present days remote sensing is most used application in many sectors. This remote sensing uses different images like multispectral, hyper spectral or ultra spectral. The remote sensing image classification is one of the significant method to classify image. In this state we classify the maximum likelihood classification with fuzzy logic. In this we experimenting fuzzy logic like spatial, spectral texture methods in that different sub methods to be used for image classification. version:1
arxiv-1407-4729 | Sparse Partially Linear Additive Models | http://arxiv.org/abs/1407.4729 | id:1407.4729 author:Yin Lou, Jacob Bien, Rich Caruana, Johannes Gehrke category:stat.ME cs.LG stat.ML  published:2014-07-17 summary:The generalized partially linear additive model (GPLAM) is a flexible and interpretable approach to building predictive models. It combines features in an additive manner, allowing them to have either a linear or nonlinear effect on the response. However, the assignment of features to the linear and nonlinear groups is typically assumed known. Thus, to make a GPLAM a viable approach in situations in which little is known $apriori$ about the features, one must overcome two primary model selection challenges: deciding which features to include in the model and determining which features to treat nonlinearly. We introduce sparse partially linear additive models (SPLAMs), which combine model fitting and $both$ of these model selection challenges into a single convex optimization problem. SPLAM provides a bridge between the Lasso and sparse additive models. Through a statistical oracle inequality and thorough simulation, we demonstrate that SPLAM can outperform other methods across a broad spectrum of statistical regimes, including the high-dimensional ($p\gg N$) setting. We develop efficient algorithms that are applied to real data sets with half a million samples and over 45,000 features with excellent predictive performance. version:1
arxiv-1405-1893 | Initial Comparison of Linguistic Networks Measures for Parallel Texts | http://arxiv.org/abs/1405.1893 | id:1405.1893 author:Kristina Ban, Ana Meštrović, Sanda Martinčić-Ipšić category:cs.CL cs.SI physics.soc-ph  published:2014-05-08 summary:This paper presents preliminary results of Croatian syllable networks analysis. Syllable network is a network in which nodes are syllables and links between them are constructed according to their connections within words. In this paper we analyze networks of syllables generated from texts collected from the Croatian Wikipedia and Blogs. As a main tool we use complex network analysis methods which provide mechanisms that can reveal new patterns in a language structure. We aim to show that syllable networks have much higher clustering coefficient in comparison to Erd\"os-Renyi random networks. The results indicate that Croatian syllable networks exhibit certain properties of a small world networks. Furthermore, we compared Croatian syllable networks with Portuguese and Chinese syllable networks and we showed that they have similar properties. version:2
arxiv-1405-4097 | A preliminary study of Croatian Language Syllable Networks | http://arxiv.org/abs/1405.4097 | id:1405.4097 author:Kristina Ban, Ivan Ivakić, Ana Meštrović category:cs.CL  published:2014-05-16 summary:This paper presents preliminary results of Croatian syllable networks analysis. Syllable network is a network in which nodes are syllables and links between them are constructed according to their connections within words. In this paper we analyze networks of syllables generated from texts collected from the Croatian Wikipedia and Blogs. As a main tool we use complex network analysis methods which provide mechanisms that can reveal new patterns in a language structure. We aim to show that syllable networks have much higher clustering coefficient in comparison to Erd\"os-Renyi random networks. The results indicate that Croatian syllable networks exhibit certain properties of a small world networks. Furthermore, we compared Croatian syllable networks with Portuguese and Chinese syllable networks and we showed that they have similar properties. version:2
arxiv-1405-2702 | Comparison of the language networks from literature and blogs | http://arxiv.org/abs/1405.2702 | id:1405.2702 author:Sabina Šišović, Sanda Martinčić-Ipšić, Ana Meštrović category:cs.CL cs.SI physics.soc-ph  published:2014-05-12 summary:In this paper we present the comparison of the linguistic networks from literature and blog texts. The linguistic networks are constructed from texts as directed and weighted co-occurrence networks of words. Words are nodes and links are established between two nodes if they are directly co-occurring within the sentence. The comparison of the networks structure is performed at global level (network) in terms of: average node degree, average shortest path length, diameter, clustering coefficient, density and number of components. Furthermore, we perform analysis on the local level (node) by comparing the rank plots of in and out degree, strength and selectivity. The selectivity-based results point out that there are differences between the structure of the networks constructed from literature and blogs. version:2
arxiv-1407-4723 | Toward Selectivity Based Keyword Extraction for Croatian News | http://arxiv.org/abs/1407.4723 | id:1407.4723 author:Slobodan Beliga, Ana Meštrović, Sanda Martinčcić-Ipšić category:cs.CL cs.IR cs.SI  published:2014-07-17 summary:Preliminary report on network based keyword extraction for Croatian is an unsupervised method for keyword extraction from the complex network. We build our approach with a new network measure the node selectivity, motivated by the research of the graph based centrality approaches. The node selectivity is defined as the average weight distribution on the links of the single node. We extract nodes (keyword candidates) based on the selectivity value. Furthermore, we expand extracted nodes to word-tuples ranked with the highest in/out selectivity values. Selectivity based extraction does not require linguistic knowledge while it is purely derived from statistical and structural information en-compassed in the source text which is reflected into the structure of the network. Obtained sets are evaluated on a manually annotated keywords: for the set of extracted keyword candidates average F1 score is 24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidates average F1 score is 25,9% and average F2 score is 24,47%. version:1
arxiv-1407-4668 | A feature construction framework based on outlier detection and discriminative pattern mining | http://arxiv.org/abs/1407.4668 | id:1407.4668 author:Albrecht Zimmermann category:cs.LG  published:2014-07-17 summary:No matter the expressive power and sophistication of supervised learning algorithms, their effectiveness is restricted by the features describing the data. This is not a new insight in ML and many methods for feature selection, transformation, and construction have been developed. But while this is on-going for general techniques for feature selection and transformation, i.e. dimensionality reduction, work on feature construction, i.e. enriching the data, is by now mainly the domain of image, particularly character, recognition, and NLP. In this work, we propose a new general framework for feature construction. The need for feature construction in a data set is indicated by class outliers and discriminative pattern mining used to derive features on their k-neighborhoods. We instantiate the framework with LOF and C4.5-Rules, and evaluate the usefulness of the derived features on a diverse collection of UCI data sets. The derived features are more often useful than ones derived by DC-Fringe, and our approach is much less likely to overfit. But while a weak learner, Naive Bayes, benefits strongly from the feature construction, the effect is less pronounced for C4.5, and almost vanishes for an SVM leaner. Keywords: feature construction, classification, outlier detection version:1
arxiv-1407-4636 | Optimization Under Uncertainty Using the Generalized Inverse Distribution Function | http://arxiv.org/abs/1407.4636 | id:1407.4636 author:Domenico Quagliarella, Giovanni Petrone, Gianluca Iaccarino category:math.OC cs.NE  published:2014-07-17 summary:A framework for robust optimization under uncertainty based on the use of the generalized inverse distribution function (GIDF), also called quantile function, is here proposed. Compared to more classical approaches that rely on the usage of statistical moments as deterministic attributes that define the objectives of the optimization process, the inverse cumulative distribution function allows for the use of all the possible information available in the probabilistic domain. Furthermore, the use of a quantile based approach leads naturally to a multi-objective methodology which allows an a-posteriori selection of the candidate design based on risk/opportunity criteria defined by the designer. Finally, the error on the estimation of the objectives due to the resolution of the GIDF will be proven to be quantifiable version:1
arxiv-1407-4543 | Sparse Quadratic Discriminant Analysis and Community Bayes | http://arxiv.org/abs/1407.4543 | id:1407.4543 author:Ya Le, Trevor Hastie category:stat.ML stat.CO  published:2014-07-17 summary:We develop a class of rules spanning the range between quadratic discriminant analysis and naive Bayes, through a path of sparse graphical models. A group lasso penalty is used to introduce shrinkage and encourage a similar pattern of sparsity across precision matrices. It gives sparse estimates of interactions and produces interpretable models. Inspired by the connected-components structure of the estimated precision matrices, we propose the community Bayes model, which partitions features into several conditional independent communities and splits the classification problem into separate smaller ones. The community Bayes idea is quite general and can be applied to non-gaussian data and likelihood-based classifiers. version:1
arxiv-1407-4443 | On the Complexity of Best Arm Identification in Multi-Armed Bandit Models | http://arxiv.org/abs/1407.4443 | id:1407.4443 author:Emilie Kaufmann, Olivier Cappé, Aurélien Garivier category:stat.ML cs.LG  published:2014-07-16 summary:The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m is larger than 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixed-confidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 19) and a novel change of measure inequality for bandit models (Lemma 1). version:1
arxiv-1407-4430 | Sequential Logistic Principal Component Analysis (SLPCA): Dimensional Reduction in Streaming Multivariate Binary-State System | http://arxiv.org/abs/1407.4430 | id:1407.4430 author:Zhaoyi Kang, Costas J. Spanos category:stat.ML cs.LG stat.AP  published:2014-07-16 summary:Sequential or online dimensional reduction is of interests due to the explosion of streaming data based applications and the requirement of adaptive statistical modeling, in many emerging fields, such as the modeling of energy end-use profile. Principal Component Analysis (PCA), is the classical way of dimensional reduction. However, traditional Singular Value Decomposition (SVD) based PCA fails to model data which largely deviates from Gaussian distribution. The Bregman Divergence was recently introduced to achieve a generalized PCA framework. If the random variable under dimensional reduction follows Bernoulli distribution, which occurs in many emerging fields, the generalized PCA is called Logistic PCA (LPCA). In this paper, we extend the batch LPCA to a sequential version (i.e. SLPCA), based on the sequential convex optimization theory. The convergence property of this algorithm is discussed compared to the batch version of LPCA (i.e. BLPCA), as well as its performance in reducing the dimension for multivariate binary-state systems. Its application in building energy end-use profile modeling is also investigated. version:1
arxiv-1407-4422 | Subspace Restricted Boltzmann Machine | http://arxiv.org/abs/1407.4422 | id:1407.4422 author:Jakub M. Tomczak, Adam Gonczarek category:cs.LG  published:2014-07-16 summary:The subspace Restricted Boltzmann Machine (subspaceRBM) is a third-order Boltzmann machine where multiplicative interactions are between one visible and two hidden units. There are two kinds of hidden units, namely, gate units and subspace units. The subspace units reflect variations of a pattern in data and the gate unit is responsible for activating the subspace units. Additionally, the gate unit can be seen as a pooling feature. We evaluate the behavior of subspaceRBM through experiments with MNIST digit recognition task, measuring reconstruction error and classification error. version:1
arxiv-1407-4416 | In Defense of MinHash Over SimHash | http://arxiv.org/abs/1407.4416 | id:1407.4416 author:Anshumali Shrivastava, Ping Li category:stat.CO cs.DS cs.IR cs.LG stat.ML  published:2014-07-16 summary:MinHash and SimHash are the two widely adopted Locality Sensitive Hashing (LSH) algorithms for large-scale data processing applications. Deciding which LSH to use for a particular problem at hand is an important question, which has no clear answer in the existing literature. In this study, we provide a theoretical answer (validated by experiments) that MinHash virtually always outperforms SimHash when the data are binary, as common in practice such as search. The collision probability of MinHash is a function of resemblance similarity ($\mathcal{R}$), while the collision probability of SimHash is a function of cosine similarity ($\mathcal{S}$). To provide a common basis for comparison, we evaluate retrieval results in terms of $\mathcal{S}$ for both MinHash and SimHash. This evaluation is valid as we can prove that MinHash is a valid LSH with respect to $\mathcal{S}$, by using a general inequality $\mathcal{S}^2\leq \mathcal{R}\leq \frac{\mathcal{S}}{2-\mathcal{S}}$. Our worst case analysis can show that MinHash significantly outperforms SimHash in high similarity region. Interestingly, our intensive experiments reveal that MinHash is also substantially better than SimHash even in datasets where most of the data points are not too similar to each other. This is partly because, in practical data, often $\mathcal{R}\geq \frac{\mathcal{S}}{z-\mathcal{S}}$ holds where $z$ is only slightly larger than 2 (e.g., $z\leq 2.1$). Our restricted worst case analysis by assuming $\frac{\mathcal{S}}{z-\mathcal{S}}\leq \mathcal{R}\leq \frac{\mathcal{S}}{2-\mathcal{S}}$ shows that MinHash indeed significantly outperforms SimHash even in low similarity region. We believe the results in this paper will provide valuable guidelines for search in practice, especially when the data are sparse. version:1
arxiv-1404-4646 | Advancing Matrix Completion by Modeling Extra Structures beyond Low-Rankness | http://arxiv.org/abs/1404.4646 | id:1404.4646 author:Guangcan Liu, Ping Li category:stat.ME cs.IT cs.LG math.IT math.ST stat.TH  published:2014-04-17 summary:A well-known method for completing low-rank matrices based on convex optimization has been established by Cand{\`e}s and Recht. Although theoretically complete, the method may not entirely solve the low-rank matrix completion problem. This is because the method captures only the low-rankness property which gives merely a rough constraint that the data points locate on some low-dimensional subspace, but generally ignores the extra structures which specify in more detail how the data points locate on the subspace. Whenever the geometric distribution of the data points is not uniform, the coherence parameters of data might be large and, accordingly, the method might fail even if the latent matrix we want to recover is fairly low-rank. To better handle non-uniform data, in this paper we propose a method termed Low-Rank Factor Decomposition (LRFD), which imposes an additional restriction that the data points must be represented as linear combinations of the bases in a dictionary constructed or learnt in advance. We show that LRFD can well handle non-uniform data, provided that the dictionary is configured properly: We mathematically prove that if the dictionary itself is low-rank then LRFD is immune to the coherence parameters which might be large on non-uniform data. This provides an elementary principle for learning the dictionary in LRFD and, naturally, leads to a practical algorithm for advancing matrix completion. Extensive experiments on randomly generated matrices and motion datasets show encouraging results. version:2
arxiv-1404-4032 | Recovery of Coherent Data via Low-Rank Dictionary Pursuit | http://arxiv.org/abs/1404.4032 | id:1404.4032 author:Guangcan Liu, Ping Li category:stat.ME cs.IT cs.LG math.IT math.ST stat.TH  published:2014-04-15 summary:The recently established RPCA method provides us a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality, RPCA may be not an ultimate solution to the low-rank matrix recovery problem. Indeed, its performance may not be perfect even when data are strictly low-rank. This is because conventional RPCA ignores the clustering structures of the data which are ubiquitous in modern applications. As the number of cluster grows, the coherence of data keeps increasing, and accordingly, the recovery performance of RPCA degrades. We show that the challenges raised by coherent data (i.e., the data with high coherence) could be alleviated by Low-Rank Representation (LRR), provided that the dictionary in LRR is configured appropriately. More precisely, we mathematically prove that if the dictionary itself is low-rank then LRR is immune to the coherence parameter which increases with the underlying cluster number. This provides an elementary principle for dealing with coherent data. Subsequently, we devise a practical algorithm to obtain proper dictionaries in unsupervised environments. Our extensive experiments on randomly generated matrices verify our claims. version:2
arxiv-1407-4373 | Online Asynchronous Distributed Regression | http://arxiv.org/abs/1407.4373 | id:1407.4373 author:Gérard Biau, Ryad Zenine category:math.ST stat.ML stat.TH  published:2014-07-16 summary:Distributed computing offers a high degree of flexibility to accommodate modern learning constraints and the ever increasing size of datasets involved in massive data issues. Drawing inspiration from the theory of distributed computation models developed in the context of gradient-type optimization algorithms, we present a consensus-based asynchronous distributed approach for nonparametric online regression and analyze some of its asymptotic properties. Substantial numerical evidence involving up to 28 parallel processors is provided on synthetic datasets to assess the excellent performance of our method, both in terms of computation time and prediction accuracy. version:1
arxiv-1311-5871 | Finding sparse solutions of systems of polynomial equations via group-sparsity optimization | http://arxiv.org/abs/1311.5871 | id:1311.5871 author:Fabien Lauer, Henrik Ohlsson category:cs.IT cs.LG math.IT math.OC stat.ML  published:2013-11-22 summary:The paper deals with the problem of finding sparse solutions to systems of polynomial equations possibly perturbed by noise. In particular, we show how these solutions can be recovered from group-sparse solutions of a derived system of linear equations. Then, two approaches are considered to find these group-sparse solutions. The first one is based on a convex relaxation resulting in a second-order cone programming formulation which can benefit from efficient reweighting techniques for sparsity enhancement. For this approach, sufficient conditions for the exact recovery of the sparsest solution to the polynomial system are derived in the noiseless setting, while stable recovery results are obtained for the noisy case. Though lacking a similar analysis, the second approach provides a more computationally efficient algorithm based on a greedy strategy adding the groups one-by-one. With respect to previous work, the proposed methods recover the sparsest solution in a very short computing time while remaining at least as accurate in terms of the probability of success. This probability is empirically analyzed to emphasize the relationship between the ability of the methods to solve the polynomial system and the sparsity of the solution. version:2
arxiv-1405-7910 | Optimal CUR Matrix Decompositions | http://arxiv.org/abs/1405.7910 | id:1405.7910 author:Christos Boutsidis, David P. Woodruff category:cs.DS cs.LG math.NA  published:2014-05-30 summary:The CUR decomposition of an $m \times n$ matrix $A$ finds an $m \times c$ matrix $C$ with a subset of $c < n$ columns of $A,$ together with an $r \times n$ matrix $R$ with a subset of $r < m$ rows of $A,$ as well as a $c \times r$ low-rank matrix $U$ such that the matrix $C U R$ approximates the matrix $A,$ that is, $ A - CUR _F^2 \le (1+\epsilon) A - A_k _F^2$, where $ . _F$ denotes the Frobenius norm and $A_k$ is the best $m \times n$ matrix of rank $k$ constructed via the SVD. We present input-sparsity-time and deterministic algorithms for constructing such a CUR decomposition where $c=O(k/\epsilon)$ and $r=O(k/\epsilon)$ and rank$(U) = k$. Up to constant factors, our algorithms are simultaneously optimal in $c, r,$ and rank$(U)$. version:2
arxiv-1407-4832 | Collaborative Filtering Ensemble for Personalized Name Recommendation | http://arxiv.org/abs/1407.4832 | id:1407.4832 author:Bernat Coma-Puig, Ernesto Diaz-Aviles, Wolfgang Nejdl category:cs.IR cs.AI cs.LG H.3.3; I.2.6  published:2014-07-16 summary:Out of thousands of names to choose from, picking the right one for your child is a daunting task. In this work, our objective is to help parents making an informed decision while choosing a name for their baby. We follow a recommender system approach and combine, in an ensemble, the individual rankings produced by simple collaborative filtering algorithms in order to produce a personalized list of names that meets the individual parents' taste. Our experiments were conducted using real-world data collected from the query logs of 'nameling' (nameling.net), an online portal for searching and exploring names, which corresponds to the dataset released in the context of the ECML PKDD Discover Challenge 2013. Our approach is intuitive, easy to implement, and features fast training and prediction steps. version:1
arxiv-1407-4206 | Mobile Camera Array Calibration for Light Field Acquisition | http://arxiv.org/abs/1407.4206 | id:1407.4206 author:Yichao Xu, Kazuki Maeno, Hajime Nagahara, Rin-ichiro Taniguchi category:cs.CV  published:2014-07-16 summary:The light field camera is useful for computer graphics and vision applications. Calibration is an essential step for these applications. After calibration, we can rectify the captured image by using the calibrated camera parameters. However, the large camera array calibration method, which assumes that all cameras are on the same plane, ignores the orientation and intrinsic parameters. The multi-camera calibration technique usually assumes that the working volume and viewpoints are fixed. In this paper, we describe a calibration algorithm suitable for a mobile camera array based light field acquisition system. The algorithm performs in Zhang's style by moving a checkerboard, and computes the initial parameters in closed form. Global optimization is then applied to refine all the parameters simultaneously. Our implementation is rather flexible in that users can assign the number of viewpoints and refinement of intrinsic parameters is optional. Experiments on both simulated data and real data acquired by a commercial product show that our method yields good results. Digital refocusing application shows the calibrated light field can well focus to the target object we desired. version:1
arxiv-1407-4137 | Automatic discovery of cell types and microcircuitry from neural connectomics | http://arxiv.org/abs/1407.4137 | id:1407.4137 author:Eric Jonas, Konrad Kording category:q-bio.NC stat.ML  published:2014-07-15 summary:Neural connectomics has begun producing massive amounts of data, necessitating new analysis methods to discover the biological and computational structure. It has long been assumed that discovering neuron types and their relation to microcircuitry is crucial to understanding neural function. Here we developed a nonparametric Bayesian technique that identifies neuron types and microcircuitry patterns in connectomics data. It combines the information traditionally used by biologists, including connectivity, cell body location and the spatial distribution of synapses, in a principled and probabilistically-coherent manner. We show that the approach recovers known neuron types in the retina and enables predictions of connectivity, better than simpler algorithms. It also can reveal interesting structure in the nervous system of C. elegans, and automatically discovers the structure of a microprocessor. Our approach extracts structural meaning from connectomics, enabling new approaches of automatically deriving anatomical insights from these emerging datasets. version:1
arxiv-1407-4070 | Fast matrix completion without the condition number | http://arxiv.org/abs/1407.4070 | id:1407.4070 author:Moritz Hardt, Mary Wootters category:cs.LG cs.DS stat.ML  published:2014-07-15 summary:We give the first algorithm for Matrix Completion whose running time and sample complexity is polynomial in the rank of the unknown target matrix, linear in the dimension of the matrix, and logarithmic in the condition number of the matrix. To the best of our knowledge, all previous algorithms either incurred a quadratic dependence on the condition number of the unknown matrix or a quadratic dependence on the dimension of the matrix in the running time. Our algorithm is based on a novel extension of Alternating Minimization which we show has theoretical guarantees under standard assumptions even in the presence of noise. version:1
arxiv-1405-3612 | Global disease monitoring and forecasting with Wikipedia | http://arxiv.org/abs/1405.3612 | id:1405.3612 author:Nicholas Generous, Geoffrey Fairchild, Alina Deshpande, Sara Y. Del Valle, Reid Priedhorsky category:cs.SI cs.LG physics.soc-ph H.5.3; H.3.5; J.3  published:2014-05-14 summary:Infectious disease is a leading threat to public health, economic stability, and other key social structures. Efforts to mitigate these impacts depend on accurate and timely monitoring to measure the risk and progress of disease. Traditional, biologically-focused monitoring techniques are accurate but costly and slow; in response, new techniques based on social internet data such as social media and search queries are emerging. These efforts are promising, but important challenges in the areas of scientific peer review, breadth of diseases and countries, and forecasting hamper their operational usefulness. We examine a freely available, open data source for this use: access logs from the online encyclopedia Wikipedia. Using linear models, language as a proxy for location, and a systematic yet simple article selection procedure, we tested 14 location-disease combinations and demonstrate that these data feasibly support an approach that overcomes these challenges. Specifically, our proof-of-concept yields models with $r^2$ up to 0.92, forecasting value up to the 28 days tested, and several pairs of models similar enough to suggest that transferring models from one location to another without re-training is feasible. Based on these preliminary results, we close with a research agenda designed to overcome these challenges and produce a disease monitoring and forecasting system that is significantly more effective, robust, and globally comprehensive than the current state of the art. version:2
arxiv-1208-5003 | Identification of Probabilities of Languages | http://arxiv.org/abs/1208.5003 | id:1208.5003 author:Paul M. B. Vitanyi, Nick Chater category:cs.LG math.PR 68  published:2012-08-24 summary:We consider the problem of inferring the probability distribution associated with a language, given data consisting of an infinite sequence of elements of the languge. We do this under two assumptions on the algorithms concerned: (i) like a real-life algorothm it has round-off errors, and (ii) it has no round-off errors. Assuming (i) we (a) consider a probability mass function of the elements of the language if the data are drawn independent identically distributed (i.i.d.), provided the probability mass function is computable and has a finite expectation. We give an effective procedure to almost surely identify in the limit the target probability mass function using the Strong Law of Large Numbers. Second (b) we treat the case of possibly incomputable probabilistic mass functions in the above setting. In this case we can only pointswize converge to the target probability mass function almost surely. Third (c) we consider the case where the data are dependent assuming they are typical for at least one computable measure and the language is finite. There is an effective procedure to identify by infinite recurrence a nonempty subset of the computable measures according to which the data is typical. Here we use the theory of Kolmogorov complexity. Assuming (ii) we obtain the weaker result for (a) that the target distribution is identified by infinite recurrence almost surely; (b) stays the same as under assumption (i). We consider the associated predictions. version:3
arxiv-1407-3969 | An iterative approach to Hough transform without re-voting | http://arxiv.org/abs/1407.3969 | id:1407.3969 author:Giorgio Ricca, Mauro C. Beltrametti, Anna Maria Massone category:cs.CV 68T45  68U10  published:2014-07-15 summary:Many bone shapes in the human skeleton are characterized by profiles that can be associated to equations of algebraic curves. Fixing the parameters in the curve equation, by means of a classical pattern recognition procedure like the Hough transform technique, it is then possible to associate an equation to a specific bone profile. However, most skeleton districts are more accurately described by piecewise defined curves. This paper utilizes an iterative approach of the Hough transform without re-voting, to provide an efficient procedure for describing the profile of a bone in the human skeleton as a collection of different but continuously attached curves. version:1
arxiv-1407-3939 | Analysis of purely random forests bias | http://arxiv.org/abs/1407.3939 | id:1407.3939 author:Sylvain Arlot, Robin Genuer category:math.ST cs.LG stat.ME stat.TH  published:2014-07-15 summary:Random forests are a very effective and commonly used statistical method, but their full theoretical analysis is still an open problem. As a first step, simplified models such as purely random forests have been introduced, in order to shed light on the good performance of random forests. In this paper, we study the approximation error (the bias) of some purely random forest models in a regression framework, focusing in particular on the influence of the number of trees in the forest. Under some regularity assumptions on the regression function, we show that the bias of an infinite forest decreases at a faster rate (with respect to the size of each tree) than a single tree. As a consequence, infinite forests attain a strictly better risk rate (with respect to the sample size) than single trees. Furthermore, our results allow to derive a minimum number of trees sufficient to reach the same rate as an infinite forest. As a by-product of our analysis, we also show a link between the bias of purely random forests and the bias of some kernel estimators. version:1
arxiv-1303-2184 | Complex Support Vector Machines for Regression and Quaternary Classification | http://arxiv.org/abs/1303.2184 | id:1303.2184 author:Pantelis Bouboulis, Sergios Theodoridis, Charalampos Mavroforakis, Leoni Dalla category:cs.LG stat.ML  published:2013-03-09 summary:The paper presents a new framework for complex Support Vector Regression as well as Support Vector Machines for quaternary classification. The method exploits the notion of widely linear estimation to model the input-out relation for complex-valued data and considers two cases: a) the complex data are split into their real and imaginary parts and a typical real kernel is employed to map the complex data to a complexified feature space and b) a pure complex kernel is used to directly map the data to the induced complex feature space. The recently developed Wirtinger's calculus on complex reproducing kernel Hilbert spaces (RKHS) is employed in order to compute the Lagrangian and derive the dual optimization problem. As one of our major results, we prove that any complex SVM/SVR task is equivalent with solving two real SVM/SVR tasks exploiting a specific real kernel which is generated by the chosen complex kernel. In particular, the case of pure complex kernels leads to the generation of new kernels, which have not been considered before. In the classification case, the proposed framework inherently splits the complex space into four parts. This leads naturally in solving the four class-task (quaternary classification), instead of the typical two classes of the real SVM. In turn, this rationale can be used in a multiclass problem as a split-class scenario based on four classes, as opposed to the one-versus-all method; this can lead to significant computational savings. Experiments demonstrate the effectiveness of the proposed framework for regression and classification tasks that involve complex data. version:3
arxiv-1312-6430 | Growing Regression Forests by Classification: Applications to Object Pose Estimation | http://arxiv.org/abs/1312.6430 | id:1312.6430 author:Kota Hara, Rama Chellappa category:cs.CV cs.LG stat.ML  published:2013-12-22 summary:In this work, we propose a novel node splitting method for regression trees and incorporate it into the regression forest framework. Unlike traditional binary splitting, where the splitting rule is selected from a predefined set of binary splitting rules via trial-and-error, the proposed node splitting method first finds clusters of the training data which at least locally minimize the empirical loss without considering the input space. Then splitting rules which preserve the found clusters as much as possible are determined by casting the problem into a classification problem. Consequently, our new node splitting method enjoys more freedom in choosing the splitting rules, resulting in more efficient tree structures. In addition to the Euclidean target space, we present a variant which can naturally deal with a circular target space by the proper use of circular statistics. We apply the regression forest employing our node splitting to head pose estimation (Euclidean target space) and car direction estimation (circular target space) and demonstrate that the proposed method significantly outperforms state-of-the-art methods (38.5% and 22.5% error reduction respectively). version:2
arxiv-1407-3867 | Part-based R-CNNs for Fine-grained Category Detection | http://arxiv.org/abs/1407.3867 | id:1407.3867 author:Ning Zhang, Jeff Donahue, Ross Girshick, Trevor Darrell category:cs.CV  published:2014-07-15 summary:Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time. version:1
arxiv-1408-2466 | Controlled Natural Language Processing as Answer Set Programming: an Experiment | http://arxiv.org/abs/1408.2466 | id:1408.2466 author:Rolf Schwitter category:cs.CL cs.AI  published:2014-07-15 summary:Most controlled natural languages (CNLs) are processed with the help of a pipeline architecture that relies on different software components. We investigate in this paper in an experimental way how well answer set programming (ASP) is suited as a unifying framework for parsing a CNL, deriving a formal representation for the resulting syntax trees, and for reasoning with that representation. We start from a list of input tokens in ASP notation and show how this input can be transformed into a syntax tree using an ASP grammar and then into reified ASP rules in form of a set of facts. These facts are then processed by an ASP meta-interpreter that allows us to infer new knowledge. version:1
arxiv-1404-7765 | A semantic network-based evolutionary algorithm for computational creativity | http://arxiv.org/abs/1404.7765 | id:1404.7765 author:Atilim Gunes Baydin, Ramon Lopez de Mantaras, Santiago Ontanon category:cs.NE  published:2014-04-30 summary:We introduce a novel evolutionary algorithm (EA) with a semantic network-based representation. For enabling this, we establish new formulations of EA variation operators, crossover and mutation, that we adapt to work on semantic networks. The algorithm employs commonsense reasoning to ensure all operations preserve the meaningfulness of the networks, using ConceptNet and WordNet knowledge bases. The algorithm can be interpreted as a novel memetic algorithm (MA), given that (1) individuals represent pieces of information that undergo evolution, as in the original sense of memetics as it was introduced by Dawkins; and (2) this is different from existing MA, where the word "memetic" has been used as a synonym for local refinement after global optimization. For evaluating the approach, we introduce an analogical similarity-based fitness measure that is computed through structure mapping. This setup enables the open-ended generation of networks analogous to a given base network. version:2
arxiv-1407-3809 | A Framework for Exploring Non-Linear Functional Connectivity and Causality in the Human Brain: Mutual Connectivity Analysis (MCA) of Resting-State Functional MRI with Convergent Cross-Mapping and Non-Metric Clustering | http://arxiv.org/abs/1407.3809 | id:1407.3809 author:Axel Wismüller, Xixi Wang, Adora M. DSouza, Mahesh B. Nagarajan category:cs.NE q-bio.NC  published:2014-07-14 summary:We present a computational framework for analysis and visualization of non-linear functional connectivity in the human brain from resting state functional MRI (fMRI) data for purposes of recovering the underlying network community structure and exploring causality between network components. Our proposed methodology of non-linear mutual connectivity analysis (MCA) involves two computational steps. First, the pair-wise cross-prediction performance between resting state fMRI pixel time series within the brain is evaluated. The underlying network structure is subsequently recovered from the affinity matrix constructed through MCA using non-metric network partitioning/clustering with the so-called Louvain method. We demonstrate our methodology in the task of identifying regions of the motor cortex associated with hand movement on resting state fMRI data acquired from eight slice locations in four subjects. For comparison, we also localized regions of the motor cortex through a task-based fMRI sequence involving a finger-tapping stimulus paradigm. Finally, we integrate convergent cross mapping (CCM) into the first step of MCA for investigating causality between regions of the motor cortex. Results regarding causation between regions of the motor cortex revealed a significant directional variability and were not readily interpretable in a consistent manner across all subjects. However, our results on whole-slice fMRI analysis demonstrate that MCA-based model-free recovery of regions associated with the primary motor cortex and supplementary motor area are in close agreement with localization of similar regions achieved with a task-based fMRI acquisition. Thus, we conclude that our computational framework MCA can extract and visualize valuable information concerning the underlying network structure and causation between different regions of the brain in resting state fMRI. version:1
arxiv-1407-3751 | Benchmarking Named Entity Disambiguation approaches for Streaming Graphs | http://arxiv.org/abs/1407.3751 | id:1407.3751 author:Sutanay Choudhury, Chase Dowling category:cs.CL cs.IR  published:2014-07-14 summary:Named Entity Disambiaguation (NED) is a central task for applications dealing with natural language text. Assume that we have a graph based knowledge base (subsequently referred as Knowledge Graph) where nodes represent various real world entities such as people, location, organization and concepts. Given data sources such as social media streams and web pages Entity Linking is the task of mapping named entities that are extracted from the data to those present in the Knowledge Graph. This is an inherently difficult task due to several reasons. Almost all these data sources are generated without any formal ontology; the unstructured nature of the input, limited context and the ambiguity involved when multiple entities are mapped to the same name make this a hard task. This report looks at two state of the art systems employing two distinctive approaches: graph based Accurate Online Disambiguation of Entities (AIDA) and Mined Evidence Named Entity Disambiguation (MENED), which employs a statistical inference approach. We compare both approaches using the data set and queries provided by the Knowledge Base Population (KBP) track at 2011 NIST Text Analytics Conference (TAC). This report begins with an overview of the respective approaches, followed by detailed description of the experimental setup. It concludes with our findings from the benchmarking exercise. version:1
arxiv-1407-4075 | Finding representative sets of optimizations for adaptive multiversioning applications | http://arxiv.org/abs/1407.4075 | id:1407.4075 author:Lianjie Luo, Yang Chen, Chengyong Wu, Shun Long, Grigori Fursin category:cs.PL cs.LG  published:2014-07-14 summary:Iterative compilation is a widely adopted technique to optimize programs for different constraints such as performance, code size and power consumption in rapidly evolving hardware and software environments. However, in case of statically compiled programs, it is often restricted to optimizations for a specific dataset and may not be applicable to applications that exhibit different run-time behavior across program phases, multiple datasets or when executed in heterogeneous, reconfigurable and virtual environments. Several frameworks have been recently introduced to tackle these problems and enable run-time optimization and adaptation for statically compiled programs based on static function multiversioning and monitoring of online program behavior. In this article, we present a novel technique to select a minimal set of representative optimization variants (function versions) for such frameworks while avoiding performance loss across available datasets and code-size explosion. We developed a novel mapping mechanism using popular decision tree or rule induction based machine learning techniques to rapidly select best code versions at run-time based on dataset features and minimize selection overhead. These techniques enable creation of self-tuning static binaries or libraries adaptable to changing behavior and environments at run-time using staged compilation that do not require complex recompilation frameworks while effectively outperforming traditional single-version non-adaptable code. version:1
arxiv-1407-3686 | Spatiotemporal Stacked Sequential Learning for Pedestrian Detection | http://arxiv.org/abs/1407.3686 | id:1407.3686 author:Alejandro González, Sebastian Ramos, David Vázquez, Antonio M. López, Jaume Amores category:cs.CV  published:2014-07-14 summary:Pedestrian classifiers decide which image windows contain a pedestrian. In practice, such classifiers provide a relatively high response at neighbor windows overlapping a pedestrian, while the responses around potential false positives are expected to be lower. An analogous reasoning applies for image sequences. If there is a pedestrian located within a frame, the same pedestrian is expected to appear close to the same location in neighbor frames. Therefore, such a location has chances of receiving high classification scores during several frames, while false positives are expected to be more spurious. In this paper we propose to exploit such correlations for improving the accuracy of base pedestrian classifiers. In particular, we propose to use two-stage classifiers which not only rely on the image descriptors required by the base classifiers but also on the response of such base classifiers in a given spatiotemporal neighborhood. More specifically, we train pedestrian classifiers using a stacked sequential learning (SSL) paradigm. We use a new pedestrian dataset we have acquired from a car to evaluate our proposal at different frame rates. We also test on a well known dataset: Caltech. The obtained results show that our SSL proposal boosts detection accuracy significantly with a minimal impact on the computational cost. Interestingly, SSL improves more the accuracy at the most dangerous situations, i.e. when a pedestrian is close to the camera. version:1
arxiv-1407-3685 | Finding Motif Sets in Time Series | http://arxiv.org/abs/1407.3685 | id:1407.3685 author:Anthony Bagnall, Jon Hills, Jason Lines category:cs.LG cs.DB  published:2014-07-14 summary:Time-series motifs are representative subsequences that occur frequently in a time series; a motif set is the set of subsequences deemed to be instances of a given motif. We focus on finding motif sets. Our motivation is to detect motif sets in household electricity-usage profiles, representing repeated patterns of household usage. We propose three algorithms for finding motif sets. Two are greedy algorithms based on pairwise comparison, and the third uses a heuristic measure of set quality to find the motif set directly. We compare these algorithms on simulated datasets and on electricity-usage data. We show that Scan MK, the simplest way of using the best-matching pair to find motif sets, is less accurate on our synthetic data than Set Finder and Cluster MK, although the latter is very sensitive to parameter settings. We qualitatively analyse the outputs for the electricity-usage data and demonstrate that both Scan MK and Set Finder can discover useful motif sets in such data. version:1
arxiv-1406-2210 | Memristor models for machine learning | http://arxiv.org/abs/1406.2210 | id:1406.2210 author:Juan Pablo Carbajal, Joni Dambre, Michiel Hermans, Benjamin Schrauwen category:cs.LG cond-mat.mtrl-sci  published:2014-06-09 summary:In the quest for alternatives to traditional CMOS, it is being suggested that digital computing efficiency and power can be improved by matching the precision to the application. Many applications do not need the high precision that is being used today. In particular, large gains in area- and power efficiency could be achieved by dedicated analog realizations of approximate computing engines. In this work, we explore the use of memristor networks for analog approximate computation, based on a machine learning framework called reservoir computing. Most experimental investigations on the dynamics of memristors focus on their nonvolatile behavior. Hence, the volatility that is present in the developed technologies is usually unwanted and it is not included in simulation models. In contrast, in reservoir computing, volatility is not only desirable but necessary. Therefore, in this work, we propose two different ways to incorporate it into memristor simulation models. The first is an extension of Strukov's model and the second is an equivalent Wiener model approximation. We analyze and compare the dynamical properties of these models and discuss their implications for the memory and the nonlinear processing capacity of memristor networks. Our results indicate that device variability, increasingly causing problems in traditional computer design, is an asset in the context of reservoir computing. We conclude that, although both models could lead to useful memristor based reservoir computing systems, their computational performance will differ. Therefore, experimental modeling research is required for the development of accurate volatile memristor models. version:2
arxiv-1407-3664 | An Enhancement Neighborhood connected Segmentation for 2D-Cellular Image | http://arxiv.org/abs/1407.3664 | id:1407.3664 author:Mohammed M. Abdelsamea category:cs.CV  published:2014-07-14 summary:A good segmentation result depends on a set of "correct" choice for the seeds. When the input images are noisy, the seeds may fall on atypical pixels that are not representative of the region statistics. This can lead to erroneous segmentation results. In this paper, an automatic seeded region growing algorithm is proposed for cellular image segmentation. First, the regions of interest (ROIs) extracted from the preprocessed image. Second, the initial seeds are automatically selected based on ROIs extracted from the image. Third, the most reprehensive seeds are selected using a machine learning algorithm. Finally, the cellular image is segmented into regions where each region corresponds to a seed. The aim of the proposed is to automatically extract the Region of Interests (ROI) from in the cellular images in terms of overcoming the explosion, under segmentation and over segmentation problems. Experimental results show that the proposed algorithm can improve the segmented image and the segmented results are less noisy as compared to some existing algorithms. version:1
arxiv-1408-4143 | Self Organization Map based Texture Feature Extraction for Efficient Medical Image Categorization | http://arxiv.org/abs/1408.4143 | id:1408.4143 author:Marghny H. Mohamed, Mohammed M. Abdelsamea category:cs.CV cs.NE  published:2014-07-14 summary:Texture is one of the most important properties of visual surface that helps in discriminating one object from another or an object from background. The self-organizing map (SOM) is an excellent tool in exploratory phase of data mining. It projects its input space on prototypes of a low-dimensional regular grid that can be effectively utilized to visualize and explore properties of the data. This paper proposes an enhancement extraction method for accurate extracting features for efficient image representation it based on SOM neural network. In this approach, we apply three different partitioning approaches as a region of interested (ROI) selection methods for extracting different accurate textural features from medical image as a primary step of our extraction method. Fisherfaces feature selection is used, for selecting discriminated features form extracted textural features. Experimental result showed the high accuracy of medical image categorization with our proposed extraction method. Experiments held on Mammographic Image Analysis Society (MIAS) dataset. version:1
arxiv-1407-3636 | Toward Network-based Keyword Extraction from Multitopic Web Documents | http://arxiv.org/abs/1407.3636 | id:1407.3636 author:Sabina Šišović, Sanda Martinčić-Ipšić, Ana Meštrović category:cs.CL cs.IR  published:2014-07-14 summary:In this paper we analyse the selectivity measure calculated from the complex network in the task of the automatic keyword extraction. Texts, collected from different web sources (portals, forums), are represented as directed and weighted co-occurrence complex networks of words. Words are nodes and links are established between two nodes if they are directly co-occurring within the sentence. We test different centrality measures for ranking nodes - keyword candidates. The promising results are achieved using the selectivity measure. Then we propose an approach which enables extracting word pairs according to the values of the in/out selectivity and weight measures combined with filtering. version:1
arxiv-1407-3619 | On the Power of Adaptivity in Matrix Completion and Approximation | http://arxiv.org/abs/1407.3619 | id:1407.3619 author:Akshay Krishnamurthy, Aarti Singh category:stat.ML cs.LG  published:2014-07-14 summary:We consider the related tasks of matrix completion and matrix approximation from missing data and propose adaptive sampling procedures for both problems. We show that adaptive sampling allows one to eliminate standard incoherence assumptions on the matrix row space that are necessary for passive sampling procedures. For exact recovery of a low-rank matrix, our algorithm judiciously selects a few columns to observe in full and, with few additional measurements, projects the remaining columns onto their span. This algorithm exactly recovers an $n \times n$ rank $r$ matrix using $O(nr\mu_0 \log^2(r))$ observations, where $\mu_0$ is a coherence parameter on the column space of the matrix. In addition to completely eliminating any row space assumptions that have pervaded the literature, this algorithm enjoys a better sample complexity than any existing matrix completion algorithm. To certify that this improvement is due to adaptive sampling, we establish that row space coherence is necessary for passive sampling algorithms to achieve non-trivial sample complexity bounds. For constructing a low-rank approximation to a high-rank input matrix, we propose a simple algorithm that thresholds the singular values of a zero-filled version of the input matrix. The algorithm computes an approximation that is nearly as good as the best rank-$r$ approximation using $O(nr\mu \log^2(n))$ samples, where $\mu$ is a slightly different coherence parameter on the matrix columns. Again we eliminate assumptions on the row space. version:1
arxiv-1407-3540 | Measuring Atmospheric Scattering from Digital Images of Urban Scenery using Temporal Polarization-Based Vision | http://arxiv.org/abs/1407.3540 | id:1407.3540 author:Tarek El-Gaaly, Joshua Gluckman category:cs.CV  published:2014-07-14 summary:Particulate Matter (PM) is a form of air pollution that visually degrades urban scenery and is hazardous to human health and the environment. Current monitoring devices are limited in measuring average PM over large areas. Quantifying the visual effects of haze in digital images of urban scenery and correlating these effects to PM levels is a vital step in more practically monitoring our environment. Current image haze extraction algorithms remove haze from the scene for the sole purpose of enhancing vision. We present two algorithms which bridge the gap between image haze extraction and environmental monitoring. We provide a means of measuring atmospheric scattering from images of urban scenery by incorporating temporal knowledge. In doing so, we also present a method of recovering an accurate depthmap of the scene and recovering the scene without the visual effects of haze. We compare our algorithm to three known haze removal methods. The algorithms are composed of an optimization over a model of haze formation in images and an optimization using a constraint of constant depth over a sequence of images taken over time. These algorithms not only measure atmospheric scattering, but also recover a more accurate depthmap and dehazed image. The measurements of atmospheric scattering this research produces, can be directly correlated to PM levels and therefore pave the way to monitoring the health of the environment by visual means. Accurate atmospheric sensing from digital images is a challenging and under-researched problem. This work provides an important step towards a more practical and accurate visual means of measuring PM from digital images. version:1
arxiv-1402-6951 | Modeling the Complex Dynamics and Changing Correlations of Epileptic Events | http://arxiv.org/abs/1402.6951 | id:1402.6951 author:Drausin F. Wulsin, Emily B. Fox, Brian Litt category:stat.ML q-bio.NC stat.AP  published:2014-02-27 summary:Patients with epilepsy can manifest short, sub-clinical epileptic "bursts" in addition to full-blown clinical seizures. We believe the relationship between these two classes of events---something not previously studied quantitatively---could yield important insights into the nature and intrinsic dynamics of seizures. A goal of our work is to parse these complex epileptic events into distinct dynamic regimes. A challenge posed by the intracranial EEG (iEEG) data we study is the fact that the number and placement of electrodes can vary between patients. We develop a Bayesian nonparametric Markov switching process that allows for (i) shared dynamic regimes between a variable number of channels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary of dynamic regimes. We encode a sparse and changing set of dependencies between the channels using a Markov-switching Gaussian graphical model for the innovations process driving the channel dynamics and demonstrate the importance of this model in parsing and out-of-sample predictions of iEEG data. We show that our model produces intuitive state assignments that can help automate clinical analysis of seizures and enable the comparison of sub-clinical bursts and full clinical seizures. version:2
arxiv-1304-2994 | A Generalized Online Mirror Descent with Applications to Classification and Regression | http://arxiv.org/abs/1304.2994 | id:1304.2994 author:Francesco Orabona, Koby Crammer, Nicolò Cesa-Bianchi category:cs.LG  published:2013-04-10 summary:Online learning algorithms are fast, memory-efficient, easy to implement, and applicable to many prediction problems, including classification, regression, and ranking. Several online algorithms were proposed in the past few decades, some based on additive updates, like the Perceptron, and some on multiplicative updates, like Winnow. A unifying perspective on the design and the analysis of online algorithms is provided by online mirror descent, a general prediction strategy from which most first-order algorithms can be obtained as special cases. We generalize online mirror descent to time-varying regularizers with generic updates. Unlike standard mirror descent, our more general formulation also captures second order algorithms, algorithms for composite losses and algorithms for adaptive filtering. Moreover, we recover, and sometimes improve, known regret bounds as special cases of our analysis using specific regularizers. Finally, we show the power of our approach by deriving a new second order algorithm with a regret bound invariant with respect to arbitrary rescalings of individual features. version:3
arxiv-1407-2676 | A New Optimal Stepsize For Approximate Dynamic Programming | http://arxiv.org/abs/1407.2676 | id:1407.2676 author:Ilya O. Ryzhov, Peter I. Frazier, Warren B. Powell category:math.OC cs.AI cs.LG cs.SY stat.ML  published:2014-07-10 summary:Approximate dynamic programming (ADP) has proven itself in a wide range of applications spanning large-scale transportation problems, health care, revenue management, and energy systems. The design of effective ADP algorithms has many dimensions, but one crucial factor is the stepsize rule used to update a value function approximation. Many operations research applications are computationally intensive, and it is important to obtain good results quickly. Furthermore, the most popular stepsize formulas use tunable parameters and can produce very poor results if tuned improperly. We derive a new stepsize rule that optimizes the prediction error in order to improve the short-term performance of an ADP algorithm. With only one, relatively insensitive tunable parameter, the new rule adapts to the level of noise in the problem and produces faster convergence in numerical experiments. version:2
arxiv-1406-4951 | Brain-like associative learning using a nanoscale non-volatile phase change synaptic device array | http://arxiv.org/abs/1406.4951 | id:1406.4951 author:Sukru Burc Eryilmaz, Duygu Kuzum, Rakesh Jeyasingh, SangBum Kim, Matthew BrightSky, Chung Lam, H. -S. Philip Wong category:cs.NE cond-mat.mtrl-sci cs.LG  published:2014-06-19 summary:Recent advances in neuroscience together with nanoscale electronic device technology have resulted in huge interests in realizing brain-like computing hardwares using emerging nanoscale memory devices as synaptic elements. Although there has been experimental work that demonstrated the operation of nanoscale synaptic element at the single device level, network level studies have been limited to simulations. In this work, we demonstrate, using experiments, array level associative learning using phase change synaptic devices connected in a grid like configuration similar to the organization of the biological brain. Implementing Hebbian learning with phase change memory cells, the synaptic grid was able to store presented patterns and recall missing patterns in an associative brain-like fashion. We found that the system is robust to device variations, and large variations in cell resistance states can be accommodated by increasing the number of training epochs. We illustrated the tradeoff between variation tolerance of the network and the overall energy consumption, and found that energy consumption is decreased significantly for lower variation tolerance. version:4
arxiv-1401-4425 | Monte Carlo Simulation for Lasso-Type Problems by Estimator Augmentation | http://arxiv.org/abs/1401.4425 | id:1401.4425 author:Qing Zhou category:stat.ME stat.ML  published:2014-01-17 summary:Regularized linear regression under the $\ell_1$ penalty, such as the Lasso, has been shown to be effective in variable selection and sparse modeling. The sampling distribution of an $\ell_1$-penalized estimator $\hat{\beta}$ is hard to determine as the estimator is defined by an optimization problem that in general can only be solved numerically and many of its components may be exactly zero. Let $S$ be the subgradient of the $\ell_1$ norm of the coefficient vector $\beta$ evaluated at $\hat{\beta}$. We find that the joint sampling distribution of $\hat{\beta}$ and $S$, together called an augmented estimator, is much more tractable and has a closed-form density under a normal error distribution in both low-dimensional ($p\leq n$) and high-dimensional ($p>n$) settings. Given $\beta$ and the error variance $\sigma^2$, one may employ standard Monte Carlo methods, such as Markov chain Monte Carlo and importance sampling, to draw samples from the distribution of the augmented estimator and calculate expectations with respect to the sampling distribution of $\hat{\beta}$. We develop a few concrete Monte Carlo algorithms and demonstrate with numerical examples that our approach may offer huge advantages and great flexibility in studying sampling distributions in $\ell_1$-penalized linear regression. We also establish nonasymptotic bounds on the difference between the true sampling distribution of $\hat{\beta}$ and its estimator obtained by plugging in estimated parameters, which justifies the validity of Monte Carlo simulation from an estimated sampling distribution even when $p\gg n\to \infty$. version:2
arxiv-1309-7733 | Regression Trees for Longitudinal Data | http://arxiv.org/abs/1309.7733 | id:1309.7733 author:Madan Gopal Kundu, Jaroslaw Harezlak category:stat.ME stat.ML  published:2013-09-30 summary:While studying response trajectory, often the population of interest may be diverse enough to exist distinct subgroups within it and the longitudinal change in response may not be uniform in these subgroups. That is, the timeslope and/or influence of covariates in longitudinal profile may vary among these different subgroups. For example, Raudenbush (2001) used depression as an example to argue that it is incorrect to assume that all the people in a given population would be experiencing either increasing or decreasing levels of depression. In such cases, traditional linear mixed effects model (assuming common parametric form for covariates and time) is not directly applicable for the entire population as a group-averaged trajectory can mask important subgroup differences. Our aim is to identify and characterize longitudinally homogeneous subgroups based on the combination of baseline covariates in the most parsimonious way. This goal can be achieved via constructing regression tree for longitudinal data using baseline covariates as partitioning variables. We have proposed LongCART algorithm to construct regression tree for the longitudinal data. In each node, the proposed LongCART algorithm determines the need for further splitting (i.e. whether parameter(s) of longitudinal profile is influenced by any baseline attributes) via parameter instability tests and thus the decision of further splitting is type-I error controlled. We have obtained the asymptotic results for the proposed instability test and examined finite sample behavior of the whole algorithm through simulation studies. Finally, we have applied the LongCART algorithm to study the longitudinal changes in choline level among HIV patients. version:3
arxiv-1404-0427 | Learning Two-input Linear and Nonlinear Analog Functions with a Simple Chemical System | http://arxiv.org/abs/1404.0427 | id:1404.0427 author:Peter Banda, Christof Teuscher category:q-bio.MN cs.LG  published:2014-04-02 summary:The current biochemical information processing systems behave in a predetermined manner because all features are defined during the design phase. To make such unconventional computing systems reusable and programmable for biomedical applications, adaptation, learning, and self-modification based on external stimuli would be highly desirable. However, so far, it has been too challenging to implement these in wet chemistries. In this paper we extend the chemical perceptron, a model previously proposed by the authors, to function as an analog instead of a binary system. The new analog asymmetric signal perceptron learns through feedback and supports Michaelis-Menten kinetics. The results show that our perceptron is able to learn linear and nonlinear (quadratic) functions of two inputs. To the best of our knowledge, it is the first simulated chemical system capable of doing so. The small number of species and reactions and their simplicity allows for a mapping to an actual wet implementation using DNA-strand displacement or deoxyribozymes. Our results are an important step toward actual biochemical systems that can learn and adapt. version:2
arxiv-1401-0247 | Robust Hierarchical Clustering | http://arxiv.org/abs/1401.0247 | id:1401.0247 author:Maria-Florina Balcan, Yingyu Liang, Pramod Gupta category:cs.LG cs.DS  published:2014-01-01 summary:One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm achieves better performance than other hierarchical algorithms in the presence of noise. version:2
arxiv-1310-0097 | Analysis of Amoeba Active Contours | http://arxiv.org/abs/1310.0097 | id:1310.0097 author:Martin Welk category:cs.CV I.4.6; I.4.3; G.1.8  published:2013-09-30 summary:Subject of this paper is the theoretical analysis of structure-adaptive median filter algorithms that approximate curvature-based PDEs for image filtering and segmentation. These so-called morphological amoeba filters are based on a concept introduced by Lerallut et al. They achieve similar results as the well-known geodesic active contour and self-snakes PDEs. In the present work, the PDE approximated by amoeba active contours is derived for a general geometric situation and general amoeba metric. This PDE is structurally similar but not identical to the geodesic active contour equation. It reproduces the previous PDE approximation results for amoeba median filters as special cases. Furthermore, modifications of the basic amoeba active contour algorithm are analysed that are related to the morphological force terms frequently used with geodesic active contours. Experiments demonstrate the basic behaviour of amoeba active contours and its similarity to geodesic active contours. version:2
arxiv-1407-2483 | Counting Markov Blanket Structures | http://arxiv.org/abs/1407.2483 | id:1407.2483 author:Shyam Visweswaran, Gregory F. Cooper category:stat.ML cs.AI cs.LG  published:2014-07-09 summary:Learning Markov blanket (MB) structures has proven useful in performing feature selection, learning Bayesian networks (BNs), and discovering causal relationships. We present a formula for efficiently determining the number of MB structures given a target variable and a set of other variables. As expected, the number of MB structures grows exponentially. However, we show quantitatively that there are many fewer MB structures that contain the target variable than there are BN structures that contain it. In particular, the ratio of BN structures to MB structures appears to increase exponentially in the number of variables. version:2
arxiv-1407-3341 | Extreme State Aggregation Beyond MDPs | http://arxiv.org/abs/1407.3341 | id:1407.3341 author:Marcus Hutter category:cs.AI cs.LG  published:2014-07-12 summary:We consider a Reinforcement Learning setup where an agent interacts with an environment in observation-reward-action cycles without any (esp.\ MDP) assumptions on the environment. State aggregation and more generally feature reinforcement learning is concerned with mapping histories/raw-states to reduced/aggregated states. The idea behind both is that the resulting reduced process (approximately) forms a small stationary finite-state MDP, which can then be efficiently solved or learnt. We considerably generalize existing aggregation results by showing that even if the reduced process is not an MDP, the (q-)value functions and (optimal) policies of an associated MDP with same state-space size solve the original problem, as long as the solution can approximately be represented as a function of the reduced states. This implies an upper bound on the required state space size that holds uniformly for all RL problems. It may also explain why RL algorithms designed for MDPs sometimes perform well beyond MDPs. version:1
arxiv-1407-3334 | Offline to Online Conversion | http://arxiv.org/abs/1407.3334 | id:1407.3334 author:Marcus Hutter category:cs.LG cs.IT math.IT math.ST stat.CO stat.TH  published:2014-07-12 summary:We consider the problem of converting offline estimators into an online predictor or estimator with small extra regret. Formally this is the problem of merging a collection of probability measures over strings of length 1,2,3,... into a single probability measure over infinite sequences. We describe various approaches and their pros and cons on various examples. As a side-result we give an elementary non-heuristic purely combinatoric derivation of Turing's famous estimator. Our main technical contribution is to determine the computational complexity of online estimators with good guarantees in general. version:1
arxiv-1407-2657 | Beyond Disagreement-based Agnostic Active Learning | http://arxiv.org/abs/1407.2657 | id:1407.2657 author:Chicheng Zhang, Kamalika Chaudhuri category:cs.LG stat.ML  published:2014-07-10 summary:We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are {\em{disagreement-based active learning}}, which has a high label requirement, and {\em{margin-based active learning}}, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions -- a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and a novel confidence-rated predictor. version:2
arxiv-1407-3242 | Density Adaptive Parallel Clustering | http://arxiv.org/abs/1407.3242 | id:1407.3242 author:Marcello La Rocca category:cs.DS cs.LG stat.ML 91C20 H.3.3; D.1.3; G.2.2  published:2014-07-11 summary:In this paper we are going to introduce a new nearest neighbours based approach to clustering, and compare it with previous solutions; the resulting algorithm, which takes inspiration from both DBscan and minimum spanning tree approaches, is deterministic but proves simpler, faster and doesnt require to set in advance a value for k, the number of clusters. version:1
arxiv-1407-3234 | Image Inpainting Using Directional Tensor Product Complex Tight Framelets | http://arxiv.org/abs/1407.3234 | id:1407.3234 author:Yi Shen, Bin Han, Elena Braverman category:cs.IT cs.CV math.IT  published:2014-07-11 summary:In this paper we are particularly interested in the image inpainting problem using directional complex tight wavelet frames. Under the assumption that frame coefficients of images are sparse, several iterative thresholding algorithms for the image inpainting problem have been proposed in the literature. The outputs of such iterative algorithms are closely linked to solutions of several convex minimization models using the balanced approach which simultaneously combines the $l_1$-regularization for sparsity of frame coefficients and the $l_2$-regularization for smoothness of the solution. Due to the redundancy of a tight frame, elements of a tight frame could be highly correlated and therefore, their corresponding frame coefficients of an image are expected to close to each other. This is called the grouping effect in statistics. In this paper, we establish the grouping effect property for frame-based convex minimization models using the balanced approach. This result on grouping effect partially explains the effectiveness of models using the balanced approach for several image restoration problems. Inspired by recent development on directional tensor product complex tight framelets (TP-CTFs) and their impressive performance for the image denoising problem, in this paper we propose an iterative thresholding algorithm using a single tight frame derived from TP-CTFs for the image inpainting problem. Experimental results show that our proposed algorithm can handle well both cartoons and textures simultaneously and performs comparably and often better than several well-known frame-based iterative thresholding algorithms for the image inpainting problem without noise. For the image inpainting problem with additive zero-mean i.i.d. Gaussian noise, our proposed algorithm using TP-CTFs performs superior than other known state-of-the-art frame-based image inpainting algorithms. version:1
arxiv-1311-7385 | Algorithmic Identification of Probabilities | http://arxiv.org/abs/1311.7385 | id:1311.7385 author:Paul M. B. Vitanyi, Nick Chater category:cs.LG  published:2013-11-28 summary:TThe problem is to identify a probability associated with a set of natural numbers, given an infinite data sequence of elements from the set. If the given sequence is drawn i.i.d. and the probability mass function involved (the target) belongs to a computably enumerable (c.e.) or co-computably enumerable (co-c.e.) set of computable probability mass functions, then there is an algorithm to almost surely identify the target in the limit. The technical tool is the strong law of large numbers. If the set is finite and the elements of the sequence are dependent while the sequence is typical in the sense of Martin-L\"of for at least one measure belonging to a c.e. or co-c.e. set of computable measures, then there is an algorithm to identify in the limit a computable measure for which the sequence is typical (there may be more than one such measure). The technical tool is the theory of Kolmogorov complexity. We give the algorithms and consider the associated predictions. version:3
arxiv-1402-2324 | Universal Matrix Completion | http://arxiv.org/abs/1402.2324 | id:1402.2324 author:Srinadh Bhojanapalli, Prateek Jain category:stat.ML cs.IT cs.LG math.IT  published:2014-02-10 summary:The problem of low-rank matrix completion has recently generated a lot of interest leading to several results that offer exact solutions to the problem. However, in order to do so, these methods make assumptions that can be quite restrictive in practice. More specifically, the methods assume that: a) the observed indices are sampled uniformly at random, and b) for every new matrix, the observed indices are sampled afresh. In this work, we address these issues by providing a universal recovery guarantee for matrix completion that works for a variety of sampling schemes. In particular, we show that if the set of sampled indices come from the edges of a bipartite graph with large spectral gap (i.e. gap between the first and the second singular value), then the nuclear norm minimization based method exactly recovers all low-rank matrices that satisfy certain incoherence properties. Moreover, we also show that under certain stricter incoherence conditions, $O(nr^2)$ uniformly sampled entries are enough to recover any rank-$r$ $n\times n$ matrix, in contrast to the $O(nr\log n)$ sample complexity required by other matrix completion algorithms as well as existing analyses of the nuclear norm method. version:2
arxiv-1407-3193 | Optimally Stabilized PET Image Denoising Using Trilateral Filtering | http://arxiv.org/abs/1407.3193 | id:1407.3193 author:Awais Mansoor, Ulas Bagci, Daniel J. Mollura category:cs.CV  published:2014-07-11 summary:Low-resolution and signal-dependent noise distribution in positron emission tomography (PET) images makes denoising process an inevitable step prior to qualitative and quantitative image analysis tasks. Conventional PET denoising methods either over-smooth small-sized structures due to resolution limitation or make incorrect assumptions about the noise characteristics. Therefore, clinically important quantitative information may be corrupted. To address these challenges, we introduced a novel approach to remove signal-dependent noise in the PET images where the noise distribution was considered as Poisson-Gaussian mixed. Meanwhile, the generalized Anscombe's transformation (GAT) was used to stabilize varying nature of the PET noise. Other than noise stabilization, it is also desirable for the noise removal filter to preserve the boundaries of the structures while smoothing the noisy regions. Indeed, it is important to avoid significant loss of quantitative information such as standard uptake value (SUV)-based metrics as well as metabolic lesion volume. To satisfy all these properties, we extended bilateral filtering method into trilateral filtering through multiscaling and optimal Gaussianization process. The proposed method was tested on more than 50 PET-CT images from various patients having different cancers and achieved the superior performance compared to the widely used denoising techniques in the literature. version:1
arxiv-1407-3179 | Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation | http://arxiv.org/abs/1407.3179 | id:1407.3179 author:Awais Mansoor, Ulas Bagci, Daniel J. Mollura category:cs.CV  published:2014-07-11 summary:Accurate delineation of pathological lungs from computed tomography (CT) images remains mostly unsolved because available methods fail to provide a reliable generic solution due to high variability of abnormality appearance. Local descriptor-based classification methods have shown to work well in annotating pathologies; however, these methods are usually computationally intensive which restricts their widespread use in real-time or near-real-time clinical applications. In this paper, we present a novel approach for fast, accurate, reliable segmentation of pathological lungs from CT scans by combining region-based segmentation method with local descriptor classification that is performed on an optimized sampling grid. Our method works in two stages; during stage one, we adapted the fuzzy connectedness (FC) image segmentation algorithm to perform initial lung parenchyma extraction. In the second stage, texture-based local descriptors are utilized to segment abnormal imaging patterns using a near optimal keypoint analysis by employing centroid of supervoxel as grid points. The quantitative results show that our pathological lung segmentation method is fast, robust, and improves on current standards and has potential to enhance the performance of routine clinical tasks. version:1
arxiv-1407-3176 | CIDI-Lung-Seg: A Single-Click Annotation Tool for Automatic Delineation of Lungs from CT Scans | http://arxiv.org/abs/1407.3176 | id:1407.3176 author:Awais Mansoor, Ulas Bagci, Brent Foster, Ziyue Xu, Deborah Douglas, Jeffrey M. Solomon, Jayaram K. Udupa, Daniel J. Mollura category:cs.CV  published:2014-07-11 summary:Accurate and fast extraction of lung volumes from computed tomography (CT) scans remains in a great demand in the clinical environment because the available methods fail to provide a generic solution due to wide anatomical variations of lungs and existence of pathologies. Manual annotation, current gold standard, is time consuming and often subject to human bias. On the other hand, current state-of-the-art fully automated lung segmentation methods fail to make their way into the clinical practice due to their inability to efficiently incorporate human input for handling misclassifications and praxis. This paper presents a lung annotation tool for CT images that is interactive, efficient, and robust. The proposed annotation tool produces an "as accurate as possible" initial annotation based on the fuzzy-connectedness image segmentation, followed by efficient manual fixation of the initial extraction if deemed necessary by the practitioner. To provide maximum flexibility to the users, our annotation tool is supported in three major operating systems (Windows, Linux, and the Mac OS X). The quantitative results comparing our free software with commercially available lung segmentation tools show higher degree of consistency and precision of our software with a considerable potential to enhance the performance of routine clinical tasks. version:1
arxiv-1407-3269 | Multiple chaotic central pattern generators with learning for legged locomotion and malfunction compensation | http://arxiv.org/abs/1407.3269 | id:1407.3269 author:Guanjiao Ren, Weihai Chen, Sakyasingha Dasgupta, Christoph Kolodziejski, Florentin Wörgötter, Poramate Manoonpong category:cs.AI cs.LG cs.NE cs.RO I.2.9; I.2.6  published:2014-07-11 summary:An originally chaotic system can be controlled into various periodic dynamics. When it is implemented into a legged robot's locomotion control as a central pattern generator (CPG), sophisticated gait patterns arise so that the robot can perform various walking behaviors. However, such a single chaotic CPG controller has difficulties dealing with leg malfunction. Specifically, in the scenarios presented here, its movement permanently deviates from the desired trajectory. To address this problem, we extend the single chaotic CPG to multiple CPGs with learning. The learning mechanism is based on a simulated annealing algorithm. In a normal situation, the CPGs synchronize and their dynamics are identical. With leg malfunction or disability, the CPGs lose synchronization leading to independent dynamics. In this case, the learning mechanism is applied to automatically adjust the remaining legs' oscillation frequencies so that the robot adapts its locomotion to deal with the malfunction. As a consequence, the trajectory produced by the multiple chaotic CPGs resembles the original trajectory far better than the one produced by only a single CPG. The performance of the system is evaluated first in a physical simulation of a quadruped as well as a hexapod robot and finally in a real six-legged walking machine called AMOSII. The experimental results presented here reveal that using multiple CPGs with learning is an effective approach for adaptive locomotion generation where, for instance, different body parts have to perform independent movements for malfunction compensation. version:1
arxiv-1407-3077 | Charge Scheduling of an Energy Storage System under Time-of-use Pricing and a Demand Charge | http://arxiv.org/abs/1407.3077 | id:1407.3077 author:Yourim Yoon, Yong-Hyuk Kim category:cs.NE  published:2014-07-11 summary:A real-coded genetic algorithm is used to schedule the charging of an energy storage system (ESS), operated in tandem with renewable power by an electricity consumer who is subject to time-of-use pricing and a demand charge. Simulations based on load and generation profiles of typical residential customers show that an ESS scheduled by our algorithm can reduce electricity costs by approximately 17%, compared to a system without an ESS, and by 8% compared to a scheduling algorithm based on net power. version:1
arxiv-1404-4960 | Agent Behavior Prediction and Its Generalization Analysis | http://arxiv.org/abs/1404.4960 | id:1404.4960 author:Fei Tian, Haifang Li, Wei Chen, Tao Qin, Enhong Chen, Tie-Yan Liu category:cs.LG  published:2014-04-19 summary:Machine learning algorithms have been applied to predict agent behaviors in real-world dynamic systems, such as advertiser behaviors in sponsored search and worker behaviors in crowdsourcing. The behavior data in these systems are generated by live agents: once the systems change due to the adoption of the prediction models learnt from the behavior data, agents will observe and respond to these changes by changing their own behaviors accordingly. As a result, the behavior data will evolve and will not be identically and independently distributed, posing great challenges to the theoretical analysis on the machine learning algorithms for behavior prediction. To tackle this challenge, in this paper, we propose to use Markov Chain in Random Environments (MCRE) to describe the behavior data, and perform generalization analysis of the machine learning algorithms on its basis. Since the one-step transition probability matrix of MCRE depends on both previous states and the random environment, conventional techniques for generalization analysis cannot be directly applied. To address this issue, we propose a novel technique that transforms the original MCRE into a higher-dimensional time-homogeneous Markov chain. The new Markov chain involves more variables but is more regular, and thus easier to deal with. We prove the convergence of the new Markov chain when time approaches infinity. Then we prove a generalization bound for the machine learning algorithms on the behavior data generated by the new Markov chain, which depends on both the Markovian parameters and the covering number of the function class compounded by the loss function for behavior prediction and the behavior prediction model. To the best of our knowledge, this is the first work that performs the generalization analysis on data generated by complex processes in real-world dynamic systems. version:2
arxiv-1407-3026 | An SVM Based Approach for Cardiac View Planning | http://arxiv.org/abs/1407.3026 | id:1407.3026 author:Ramasubramanian Sundararajan, Hima Patel, Dattesh Shanbhag, Vivek Vaidya category:cs.LG cs.CV  published:2014-07-11 summary:We consider the problem of automatically prescribing oblique planes (short axis, 4 chamber and 2 chamber views) in Cardiac Magnetic Resonance Imaging (MRI). A concern with technologist-driven acquisitions of these planes is the quality and time taken for the total examination. We propose an automated solution incorporating anatomical features external to the cardiac region. The solution uses support vector machine regression models wherein complexity and feature selection are optimized using multi-objective genetic algorithms. Additionally, we examine the robustness of our approach by training our models on images with additive Rician-Gaussian mixtures at varying Signal to Noise (SNR) levels. Our approach has shown promising results, with an angular deviation of less than 15 degrees on 90% cases across oblique planes, measured in terms of average 6-fold cross validation performance -- this is generally within acceptable bounds of variation as specified by clinicians. version:1
arxiv-1407-3010 | Biclustering Via Sparse Clustering | http://arxiv.org/abs/1407.3010 | id:1407.3010 author:Qian Liu, Guanhua Chen, Michael R. Kosorok, Eric Bair category:stat.ME stat.ML  published:2014-07-11 summary:In many situations it is desirable to identify clusters that differ with respect to only a subset of features. Such clusters may represent homogeneous subgroups of patients with a disease, such as cancer or chronic pain. We define a bicluster to be a submatrix U of a larger data matrix X such that the features and observations in U differ from those not contained in U. For example, the observations in U could have different means or variances with respect to the features in U. We propose a general framework for biclustering based on the sparse clustering method of Witten and Tibshirani (2010). We develop a method for identifying features that belong to biclusters. This framework can be used to identify biclusters that differ with respect to the means of the features, the variance of the features, or more general differences. We apply these methods to several simulated and real-world data sets and compare the results of our method with several previously published methods. The results of our method compare favorably with existing methods with respect to both predictive accuracy and computing time. version:1
arxiv-1406-3337 | Evolutionary Robotics on the Web with WebGL and Javascript | http://arxiv.org/abs/1406.3337 | id:1406.3337 author:Jared Moore, Anthony Clark, Philip McKinley category:cs.NE cs.HC  published:2014-06-12 summary:Web-based applications are highly accessible to users, providing rich, interactive content while eliminating the need to install software locally. However, evolutionary robotics (ER) has faced challenges in this domain as web-based technologies have not been amenable to 3D physics simulations. Traditionally, physics-based simulations require a local installation and a high degree of user knowledge to configure an environment, but the emergence of Javascript-based physics engines enables complex simulations to be executed in web browsers. These developments create opportunities for ER research to reach new audiences by increasing accessibility. In this work, we introduce two web-based tools we have built to facilitate the exchange of ideas with other researchers as well as outreach to K-12 students and the general public. The first tool is intended to distribute and exchange ER research results, while the second is a completely browser-based implementation of an ER environment. version:3
arxiv-1407-3000 | A Proposed Infrastructure for Adding Online Interaction to Any Evolutionary Domain | http://arxiv.org/abs/1407.3000 | id:1407.3000 author:Paul Szerlip, Kenneth O. Stanley category:cs.NE  published:2014-07-11 summary:To address the difficulty of creating online collaborative evolutionary systems, this paper presents a new prototype library called Worldwide Infrastructure for Neuroevolution (WIN) and its accompanying site WIN Online (http://winark.org/). The WIN library is a collection of software packages built on top of Node.js that reduce the complexity of creating fully persistent, online, and interactive (or automated) evolutionary platforms around any domain. WIN Online is the public interface for WIN, providing an online collection of domains built with the WIN library that lets novice and expert users browse and meaningfully contribute to ongoing experiments. The long term goal of WIN is to make it trivial to connect any platform to the world, providing both a stream of online users, and archives of data and discoveries for later extension by humans or computers. version:1
arxiv-1407-2989 | Hidden Markov Model Based Part of Speech Tagger for Sinhala Language | http://arxiv.org/abs/1407.2989 | id:1407.2989 author:A. J. P. M. P. Jayaweera, N. G. J. Dias category:cs.CL I.2.7  published:2014-07-10 summary:In this paper we present a fundamental lexical semantics of Sinhala language and a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhala language. In any Natural Language processing task, Part of Speech is a very vital topic, which involves analysing of the construction, behaviour and the dynamics of the language, which the knowledge could utilized in computational linguistics analysis and automation applications. Though Sinhala is a morphologically rich and agglutinative language, in which words are inflected with various grammatical features, tagging is very essential for further analysis of the language. Our research is based on statistical based approach, in which the tagging process is done by computing the tag sequence probability and the word-likelihood probability from the given corpus, where the linguistic knowledge is automatically extracted from the annotated corpus. The current tagger could reach more than 90% of accuracy for known words. version:1
arxiv-1407-2987 | FAME: Face Association through Model Evolution | http://arxiv.org/abs/1407.2987 | id:1407.2987 author:Eren Golge, Pinar Duygulu category:cs.CV cs.AI cs.IR cs.LG  published:2014-07-10 summary:We attack the problem of learning face models for public faces from weakly-labelled images collected from web through querying a name. The data is very noisy even after face detection, with several irrelevant faces corresponding to other people. We propose a novel method, Face Association through Model Evolution (FAME), that is able to prune the data in an iterative way, for the face models associated to a name to evolve. The idea is based on capturing discriminativeness and representativeness of each instance and eliminating the outliers. The final models are used to classify faces on novel datasets with possibly different characteristics. On benchmark datasets, our results are comparable to or better than state-of-the-art studies for the task of face identification. version:1
arxiv-1403-2933 | Efficiently inferring community structure in bipartite networks | http://arxiv.org/abs/1403.2933 | id:1403.2933 author:Daniel B. Larremore, Aaron Clauset, Abigail Z. Jacobs category:cs.SI physics.data-an physics.soc-ph q-bio.QM stat.ML  published:2014-03-12 summary:Bipartite networks are a common type of network data in which there are two types of vertices, and only vertices of different types can be connected. While bipartite networks exhibit community structure like their unipartite counterparts, existing approaches to bipartite community detection have drawbacks, including implicit parameter choices, loss of information through one-mode projections, and lack of interpretability. Here we solve the community detection problem for bipartite networks by formulating a bipartite stochastic block model, which explicitly includes vertex type information and may be trivially extended to $k$-partite networks. This bipartite stochastic block model yields a projection-free and statistically principled method for community detection that makes clear assumptions and parameter choices and yields interpretable results. We demonstrate this model's ability to efficiently and accurately find community structure in synthetic bipartite networks with known structure and in real-world bipartite networks with unknown structure, and we characterize its performance in practical contexts. version:2
arxiv-1306-2547 | Efficient Classification for Metric Data | http://arxiv.org/abs/1306.2547 | id:1306.2547 author:Lee-Ad Gottlieb, Aryeh Kontorovich, Robert Krauthgamer category:cs.LG cs.DS stat.ML  published:2013-06-11 summary:Recent advances in large-margin classification of data residing in general metric spaces (rather than Hilbert spaces) enable classification under various natural metrics, such as string edit and earthmover distance. A general framework developed for this purpose by von Luxburg and Bousquet [JMLR, 2004] left open the questions of computational efficiency and of providing direct bounds on generalization error. We design a new algorithm for classification in general metric spaces, whose runtime and accuracy depend on the doubling dimension of the data points, and can thus achieve superior classification performance in many common scenarios. The algorithmic core of our approach is an approximate (rather than exact) solution to the classical problems of Lipschitz extension and of Nearest Neighbor Search. The algorithm's generalization performance is guaranteed via the fat-shattering dimension of Lipschitz classifiers, and we present experimental evidence of its superiority to some common kernel methods. As a by-product, we offer a new perspective on the nearest neighbor classifier, which yields significantly sharper risk asymptotics than the classic analysis of Cover and Hart [IEEE Trans. Info. Theory, 1967]. version:3
arxiv-1408-3139 | Real-Time Impulse Noise Suppression from Images Using an Efficient Weighted-Average Filtering | http://arxiv.org/abs/1408.3139 | id:1408.3139 author:Hossein Hosseini, Farzad Hessar, Farokh Marvasti category:cs.CV  published:2014-07-10 summary:In this paper, we propose a method for real-time high density impulse noise suppression from images. In our method, we first apply an impulse detector to identify the corrupted pixels and then employ an innovative weighted-average filter to restore them. The filter takes the nearest neighboring interpolated image as the initial image and computes the weights according to the relative positions of the corrupted and uncorrupted pixels. Experimental results show that the proposed method outperforms the best existing methods in both PSNR measure and visual quality and is quite suitable for real-time applications. version:1
arxiv-1407-2961 | On the Convergence of the Mean Shift Algorithm in the One-Dimensional Space | http://arxiv.org/abs/1407.2961 | id:1407.2961 author:Youness Aliyari Ghassabeh category:cs.CV  published:2014-07-10 summary:The mean shift algorithm is a non-parametric and iterative technique that has been used for finding modes of an estimated probability density function. It has been successfully employed in many applications in specific areas of machine vision, pattern recognition, and image processing. Although the mean shift algorithm has been used in many applications, a rigorous proof of its convergence is still missing in the literature. In this paper we address the convergence of the mean shift algorithm in the one-dimensional space and prove that the sequence generated by the mean shift algorithm is a monotone and convergent sequence. version:1
arxiv-1407-2904 | An eigenanalysis of data centering in machine learning | http://arxiv.org/abs/1407.2904 | id:1407.2904 author:Paul Honeine category:stat.ML cs.CV cs.LG math.SP math.ST stat.TH  published:2014-07-10 summary:Many pattern recognition methods rely on statistical information from centered data, with the eigenanalysis of an empirical central moment, such as the covariance matrix in principal component analysis (PCA), as well as partial least squares regression, canonical-correlation analysis and Fisher discriminant analysis. Recently, many researchers advocate working on non-centered data. This is the case for instance with the singular value decomposition approach, with the (kernel) entropy component analysis, with the information-theoretic learning framework, and even with nonnegative matrix factorization. Moreover, one can also consider a non-centered PCA by using the second-order non-central moment. The main purpose of this paper is to bridge the gap between these two viewpoints in designing machine learning methods. To provide a study at the cornerstone of kernel-based machines, we conduct an eigenanalysis of the inner product matrices from centered and non-centered data. We derive several results connecting their eigenvalues and their eigenvectors. Furthermore, we explore the outer product matrices, by providing several results connecting the largest eigenvectors of the covariance matrix and its non-centered counterpart. These results lay the groundwork to several extensions beyond conventional centering, with the weighted mean shift, the rank-one update, and the multidimensional scaling. Experiments conducted on simulated and real data illustrate the relevance of this work. version:1
arxiv-1403-0504 | A Compilation Target for Probabilistic Programming Languages | http://arxiv.org/abs/1403.0504 | id:1403.0504 author:Brooks Paige, Frank Wood category:cs.AI cs.PL stat.ML  published:2014-03-03 summary:Forward inference techniques such as sequential Monte Carlo and particle Markov chain Monte Carlo for probabilistic programming can be implemented in any programming language by creative use of standardized operating system functionality including processes, forking, mutexes, and shared memory. Exploiting this we have defined, developed, and tested a probabilistic programming language intermediate representation language we call probabilistic C, which itself can be compiled to machine code by standard compilers and linked to operating system libraries yielding an efficient, scalable, portable probabilistic programming compilation target. This opens up a new hardware and systems research path for optimizing probabilistic programming systems. version:2
arxiv-1407-2864 | Asynchronous Anytime Sequential Monte Carlo | http://arxiv.org/abs/1407.2864 | id:1407.2864 author:Brooks Paige, Frank Wood, Arnaud Doucet, Yee Whye Teh category:stat.CO stat.ML  published:2014-07-10 summary:We introduce a new sequential Monte Carlo algorithm we call the particle cascade. The particle cascade is an asynchronous, anytime alternative to traditional particle filtering algorithms. It uses no barrier synchronizations which leads to improved particle throughput and memory efficiency. It is an anytime algorithm in the sense that it can be run forever to emit an unbounded number of particles while keeping within a fixed memory budget. We prove that the particle cascade is an unbiased marginal likelihood estimator which means that it can be straightforwardly plugged into existing pseudomarginal methods. version:1
arxiv-1407-2845 | XML Matchers: approaches and challenges | http://arxiv.org/abs/1407.2845 | id:1407.2845 author:Santa Agreste, Pasquale De Meo, Emilio Ferrara, Domenico Ursino category:cs.DB cs.AI cs.IR cs.LG  published:2014-07-10 summary:Schema Matching, i.e. the process of discovering semantic correspondences between concepts adopted in different data source schemas, has been a key topic in Database and Artificial Intelligence research areas for many years. In the past, it was largely investigated especially for classical database models (e.g., E/R schemas, relational databases, etc.). However, in the latest years, the widespread adoption of XML in the most disparate application fields pushed a growing number of researchers to design XML-specific Schema Matching approaches, called XML Matchers, aiming at finding semantic matchings between concepts defined in DTDs and XSDs. XML Matchers do not just take well-known techniques originally designed for other data models and apply them on DTDs/XSDs, but they exploit specific XML features (e.g., the hierarchical structure of a DTD/XSD) to improve the performance of the Schema Matching process. The design of XML Matchers is currently a well-established research area. The main goal of this paper is to provide a detailed description and classification of XML Matchers. We first describe to what extent the specificities of DTDs/XSDs impact on the Schema Matching task. Then we introduce a template, called XML Matcher Template, that describes the main components of an XML Matcher, their role and behavior. We illustrate how each of these components has been implemented in some popular XML Matchers. We consider our XML Matcher Template as the baseline for objectively comparing approaches that, at first glance, might appear as unrelated. The introduction of this template can be useful in the design of future XML Matchers. Finally, we analyze commercial tools implementing XML Matchers and introduce two challenging issues strictly related to this topic, namely XML source clustering and uncertainty management in XML Matchers. version:1
arxiv-1407-2812 | Rate-Optimal Detection of Very Short Signal Segments | http://arxiv.org/abs/1407.2812 | id:1407.2812 author:T. Tony Cai, Ming Yuan category:stat.ML cs.IT math.IT math.ST stat.TH  published:2014-07-10 summary:Motivated by a range of applications in engineering and genomics, we consider in this paper detection of very short signal segments in three settings: signals with known shape, arbitrary signals, and smooth signals. Optimal rates of detection are established for the three cases and rate-optimal detectors are constructed. The detectors are easily implementable and are based on scanning with linear and quadratic statistics. Our analysis reveals both similarities and differences in the strategy and fundamental difficulty of detection among these three settings. version:1
arxiv-1407-2806 | Bandits Warm-up Cold Recommender Systems | http://arxiv.org/abs/1407.2806 | id:1407.2806 author:Jérémie Mary, Romaric Gaudel, Preux Philippe category:cs.LG cs.IR stat.ML  published:2014-07-10 summary:We address the cold start problem in recommendation systems assuming no contextual information is available neither about users, nor items. We consider the case in which we only have access to a set of ratings of items by users. Most of the existing works consider a batch setting, and use cross-validation to tune parameters. The classical method consists in minimizing the root mean square error over a training subset of the ratings which provides a factorization of the matrix of ratings, interpreted as a latent representation of items and users. Our contribution in this paper is 5-fold. First, we explicit the issues raised by this kind of batch setting for users or items with very few ratings. Then, we propose an online setting closer to the actual use of recommender systems; this setting is inspired by the bandit framework. The proposed methodology can be used to turn any recommender system dataset (such as Netflix, MovieLens,...) into a sequential dataset. Then, we explicit a strong and insightful link between contextual bandit algorithms and matrix factorization; this leads us to a new algorithm that tackles the exploration/exploitation dilemma associated to the cold start problem in a strikingly new perspective. Finally, experimental evidence confirm that our algorithm is effective in dealing with the cold start problem on publicly available datasets. Overall, the goal of this paper is to bridge the gap between recommender systems based on matrix factorizations and those based on contextual bandits. version:1
arxiv-1407-2776 | What you need to know about the state-of-the-art computational models of object-vision: A tour through the models | http://arxiv.org/abs/1407.2776 | id:1407.2776 author:Seyed-Mahdi Khaligh-Razavi category:cs.CV cs.AI cs.LG q-bio.NC  published:2014-07-10 summary:Models of object vision have been of great interest in computer vision and visual neuroscience. During the last decades, several models have been developed to extract visual features from images for object recognition tasks. Some of these were inspired by the hierarchical structure of primate visual system, and some others were engineered models. The models are varied in several aspects: models that are trained by supervision, models trained without supervision, and models (e.g. feature extractors) that are fully hard-wired and do not need training. Some of the models come with a deep hierarchical structure consisting of several layers, and some others are shallow and come with only one or two layers of processing. More recently, new models have been developed that are not hand-tuned but trained using millions of images, through which they learn how to extract informative task-related features. Here I will survey all these different models and provide the reader with an intuitive, as well as a more detailed, understanding of the underlying computations in each of the models. version:1
arxiv-1407-2736 | A multi-instance learning algorithm based on a stacked ensemble of lazy learners | http://arxiv.org/abs/1407.2736 | id:1407.2736 author:Ramasubramanian Sundararajan, Hima Patel, Manisha Srivastava category:cs.LG  published:2014-07-10 summary:This document describes a novel learning algorithm that classifies "bags" of instances rather than individual instances. A bag is labeled positive if it contains at least one positive instance (which may or may not be specifically identified), and negative otherwise. This class of problems is known as multi-instance learning problems, and is useful in situations where the class label at an instance level may be unavailable or imprecise or difficult to obtain, or in situations where the problem is naturally posed as one of classifying instance groups. The algorithm described here is an ensemble-based method, wherein the members of the ensemble are lazy learning classifiers learnt using the Citation Nearest Neighbour method. Diversity among the ensemble members is achieved by optimizing their parameters using a multi-objective optimization method, with the objectives being to maximize Class 1 accuracy and minimize false positive rate. The method has been found to be effective on the Musk1 benchmark dataset. version:1
arxiv-1407-2710 | Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems | http://arxiv.org/abs/1407.2710 | id:1407.2710 author:Aaron J. Defazio, Tibério S. Caetano, Justin Domke category:cs.LG stat.ML  published:2014-07-10 summary:Recent advances in optimization theory have shown that smooth strongly convex finite sums can be minimized faster than by treating them as a black box "batch" problem. In this work we introduce a new method in this class with a theoretical convergence rate four times faster than existing methods, for sums with sufficiently many terms. This method is also amendable to a sampling without replacement scheme that in practice gives further speed-ups. We give empirical results showing state of the art performance. version:1
arxiv-1407-2700 | Offline handwritten signature identification using adaptive window positioning techniques | http://arxiv.org/abs/1407.2700 | id:1407.2700 author:Ghazali Sulong, Anwar Yahy Ebrahim, Muhammad Jehanzeb category:cs.CV  published:2014-07-10 summary:The paper presents to address this challenge, we have proposed the use of Adaptive Window Positioning technique which focuses on not just the meaning of the handwritten signature but also on the individuality of the writer. This innovative technique divides the handwritten signature into 13 small windows of size nxn(13x13).This size should be large enough to contain ample information about the style of the author and small enough to ensure a good identification performance.The process was tested with a GPDS data set containing 4870 signature samples from 90 different writers by comparing the robust features of the test signature with that of the user signature using an appropriate classifier. Experimental results reveal that adaptive window positioning technique proved to be the efficient and reliable method for accurate signature feature extraction for the identification of offline handwritten signatures.The contribution of this technique can be used to detect signatures signed under emotional duress. version:1
arxiv-1407-2697 | A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation | http://arxiv.org/abs/1407.2697 | id:1407.2697 author:Aaron J. Defazio, Tiberio S. Caetano category:cs.LG stat.ML  published:2014-07-10 summary:A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lov\'asz extension to obtain a convex relaxation. For tractable classes such as Gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset. version:1
arxiv-1407-2694 | Quality Estimation Of Machine Translation Outputs Through Stemming | http://arxiv.org/abs/1407.2694 | id:1407.2694 author:Pooja Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2014-07-10 summary:Machine Translation is the challenging problem for Indian languages. Every day we can see some machine translators being developed, but getting a high quality automatic translation is still a very distant dream . The correct translated sentence for Hindi language is rarely found. In this paper, we are emphasizing on English-Hindi language pair, so in order to preserve the correct MT output we present a ranking system, which employs some machine learning techniques and morphological features. In ranking no human intervention is required. We have also validated our results by comparing it with human ranking. version:1
arxiv-1307-0471 | Quantum support vector machine for big data classification | http://arxiv.org/abs/1307.0471 | id:1307.0471 author:Patrick Rebentrost, Masoud Mohseni, Seth Lloyd category:quant-ph cs.LG  published:2013-07-01 summary:Supervised machine learning is the classification of new data based on already classified training examples. In this work, we show that the support vector machine, an optimized binary classifier, can be implemented on a quantum computer, with complexity logarithmic in the size of the vectors and the number of training examples. In cases when classical sampling algorithms require polynomial time, an exponential speed-up is obtained. At the core of this quantum big data algorithm is a non-sparse matrix exponentiation technique for efficiently performing a matrix inversion of the training data inner-product (kernel) matrix. version:3
arxiv-1407-2674 | Private Learning and Sanitization: Pure vs. Approximate Differential Privacy | http://arxiv.org/abs/1407.2674 | id:1407.2674 author:Amos Beimel, Kobbi Nissim, Uri Stemmer category:cs.LG cs.CR stat.ML  published:2014-07-10 summary:We compare the sample complexity of private learning [Kasiviswanathan et al. 2008] and sanitization~[Blum et al. 2008] under pure $\epsilon$-differential privacy [Dwork et al. TCC 2006] and approximate $(\epsilon,\delta)$-differential privacy [Dwork et al. Eurocrypt 2006]. We show that the sample complexity of these tasks under approximate differential privacy can be significantly lower than that under pure differential privacy. We define a family of optimization problems, which we call Quasi-Concave Promise Problems, that generalizes some of our considered tasks. We observe that a quasi-concave promise problem can be privately approximated using a solution to a smaller instance of a quasi-concave promise problem. This allows us to construct an efficient recursive algorithm solving such problems privately. Specifically, we construct private learners for point functions, threshold functions, and axis-aligned rectangles in high dimension. Similarly, we construct sanitizers for point functions and threshold functions. We also examine the sample complexity of label-private learners, a relaxation of private learning where the learner is required to only protect the privacy of the labels in the sample. We show that the VC dimension completely characterizes the sample complexity of such learners, that is, the sample complexity of learning with label privacy is equal (up to constants) to learning without privacy. version:1
arxiv-1407-2649 | Classifying Fonts and Calligraphy Styles Using Complex Wavelet Transform | http://arxiv.org/abs/1407.2649 | id:1407.2649 author:Alican Bozkurt, Pinar Duygulu, A. Enis Cetin category:cs.CV  published:2014-07-09 summary:Recognizing fonts has become an important task in document analysis, due to the increasing number of available digital documents in different fonts and emphases. A generic font-recognition system independent of language, script and content is desirable for processing various types of documents. At the same time, categorizing calligraphy styles in handwritten manuscripts is important for palaeographic analysis, but has not been studied sufficiently in the literature. We address the font-recognition problem as analysis and categorization of textures. We extract features using complex wavelet transform and use support vector machines for classification. Extensive experimental evaluations on different datasets in four languages and comparisons with state-of-the-art studies show that our proposed method achieves higher recognition accuracy while being computationally simpler. Furthermore, on a new dataset generated from Ottoman manuscripts, we show that the proposed method can also be used for categorizing Ottoman calligraphy with high accuracy. version:1
arxiv-1407-2646 | Learning Probabilistic Programs | http://arxiv.org/abs/1407.2646 | id:1407.2646 author:Yura N. Perov, Frank D. Wood category:cs.AI cs.LG stat.ML  published:2014-07-09 summary:We develop a technique for generalising from data in which models are samplers represented as program text. We establish encouraging empirical results that suggest that Markov chain Monte Carlo probabilistic programming inference techniques coupled with higher-order probabilistic programming languages are now sufficiently powerful to enable successful inference of this kind in nontrivial domains. We also introduce a new notion of probabilistic program compilation and show how the same machinery might be used in the future to compile probabilistic programs for efficient reusable predictive inference. version:1
arxiv-1407-2630 | A Statistical Modeling Approach to Computer-Aided Quantification of Dental Biofilm | http://arxiv.org/abs/1407.2630 | id:1407.2630 author:Awais Mansoor, Valery Patsekin, Dale Scherl, J. Paul Robinson, Bartlomiej Rajwa category:cs.CV  published:2014-07-09 summary:Biofilm is a formation of microbial material on tooth substrata. Several methods to quantify dental biofilm coverage have recently been reported in the literature, but at best they provide a semi-automated approach to quantification with significant input from a human grader that comes with the graders bias of what are foreground, background, biofilm, and tooth. Additionally, human assessment indices limit the resolution of the quantification scale; most commercial scales use five levels of quantification for biofilm coverage (0%, 25%, 50%, 75%, and 100%). On the other hand, current state-of-the-art techniques in automatic plaque quantification fail to make their way into practical applications owing to their inability to incorporate human input to handle misclassifications. This paper proposes a new interactive method for biofilm quantification in Quantitative light-induced fluorescence (QLF) images of canine teeth that is independent of the perceptual bias of the grader. The method partitions a QLF image into segments of uniform texture and intensity called superpixels; every superpixel is statistically modeled as a realization of a single 2D Gaussian Markov random field (GMRF) whose parameters are estimated; the superpixel is then assigned to one of three classes (background, biofilm, tooth substratum) based on the training set of data. The quantification results show a high degree of consistency and precision. At the same time, the proposed method gives pathologists full control to post-process the automatic quantification by flipping misclassified superpixels to a different state (background, tooth, biofilm) with a single click, providing greater usability than simply marking the boundaries of biofilm and tooth as done by current state-of-the-art methods. version:1
arxiv-1405-5268 | Approximate resilience, monotonicity, and the complexity of agnostic learning | http://arxiv.org/abs/1405.5268 | id:1405.5268 author:Dana Dachman-Soled, Vitaly Feldman, Li-Yang Tan, Andrew Wan, Karl Wimmer category:cs.LG cs.CC cs.DM  published:2014-05-21 summary:A function $f$ is $d$-resilient if all its Fourier coefficients of degree at most $d$ are zero, i.e., $f$ is uncorrelated with all low-degree parities. We study the notion of $\mathit{approximate}$ $\mathit{resilience}$ of Boolean functions, where we say that $f$ is $\alpha$-approximately $d$-resilient if $f$ is $\alpha$-close to a $[-1,1]$-valued $d$-resilient function in $\ell_1$ distance. We show that approximate resilience essentially characterizes the complexity of agnostic learning of a concept class $C$ over the uniform distribution. Roughly speaking, if all functions in a class $C$ are far from being $d$-resilient then $C$ can be learned agnostically in time $n^{O(d)}$ and conversely, if $C$ contains a function close to being $d$-resilient then agnostic learning of $C$ in the statistical query (SQ) framework of Kearns has complexity of at least $n^{\Omega(d)}$. This characterization is based on the duality between $\ell_1$ approximation by degree-$d$ polynomials and approximate $d$-resilience that we establish. In particular, it implies that $\ell_1$ approximation by low-degree polynomials, known to be sufficient for agnostic learning over product distributions, is in fact necessary. Focusing on monotone Boolean functions, we exhibit the existence of near-optimal $\alpha$-approximately $\widetilde{\Omega}(\alpha\sqrt{n})$-resilient monotone functions for all $\alpha>0$. Prior to our work, it was conceivable even that every monotone function is $\Omega(1)$-far from any $1$-resilient function. Furthermore, we construct simple, explicit monotone functions based on ${\sf Tribes}$ and ${\sf CycleRun}$ that are close to highly resilient functions. Our constructions are based on a fairly general resilience analysis and amplification. These structural results, together with the characterization, imply nearly optimal lower bounds for agnostic learning of monotone juntas. version:2
arxiv-1305-0030 | A least-squares method for sparse low rank approximation of multivariate functions | http://arxiv.org/abs/1305.0030 | id:1305.0030 author:Mathilde Chevreuil, Régis Lebrun, Anthony Nouy, Prashant Rai category:math.NA stat.ML 65D15  62J02  15A69  published:2013-04-30 summary:In this paper, we propose a low-rank approximation method based on discrete least-squares for the approximation of a multivariate function from random, noisy-free observations. Sparsity inducing regularization techniques are used within classical algorithms for low-rank approximation in order to exploit the possible sparsity of low-rank approximations. Sparse low-rank approximations are constructed with a robust updated greedy algorithm which includes an optimal selection of regularization parameters and approximation ranks using cross validation techniques. Numerical examples demonstrate the capability of approximating functions of many variables even when very few function evaluations are available, thus proving the interest of the proposed algorithm for the propagation of uncertainties through complex computational models. version:2
arxiv-1407-2918 | A Survey of Named Entity Recognition in Assamese and other Indian Languages | http://arxiv.org/abs/1407.2918 | id:1407.2918 author:Gitimoni Talukdar, Pranjal Protim Borah, Arup Baruah category:cs.CL  published:2014-07-09 summary:Named Entity Recognition is always important when dealing with major Natural Language Processing tasks such as information extraction, question-answering, machine translation, document summarization etc so in this paper we put forward a survey of Named Entities in Indian Languages with particular reference to Assamese. There are various rule-based and machine learning approaches available for Named Entity Recognition. At the very first of the paper we give an idea of the available approaches for Named Entity Recognition and then we discuss about the related research in this field. Assamese like other Indian languages is agglutinative and suffers from lack of appropriate resources as Named Entity Recognition requires large data sets, gazetteer list, dictionary etc and some useful feature like capitalization as found in English cannot be found in Assamese. Apart from this we also describe some of the issues faced in Assamese while doing Named Entity Recognition. version:1
arxiv-1406-5726 | CNN: Single-label to Multi-label | http://arxiv.org/abs/1406.5726 | id:1406.5726 author:Yunchao Wei, Wei Xia, Junshi Huang, Bingbing Ni, Jian Dong, Yao Zhao, Shuicheng Yan category:cs.CV  published:2014-06-22 summary:Convolutional Neural Network (CNN) has demonstrated promising performance in single-label image classification tasks. However, how CNN best copes with multi-label images still remains an open problem, mainly due to the complex underlying object layouts and insufficient multi-label training images. In this work, we propose a flexible deep CNN infrastructure, called Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment hypotheses are taken as the inputs, then a shared CNN is connected with each hypothesis, and finally the CNN output results from different hypotheses are aggregated with max pooling to produce the ultimate multi-label predictions. Some unique characteristics of this flexible deep CNN infrastructure include: 1) no ground truth bounding box information is required for training; 2) the whole HCP infrastructure is robust to possibly noisy and/or redundant hypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN may be well pre-trained with a large-scale single-label image dataset, e.g. ImageNet; and 5) it may naturally output multi-label prediction results. Experimental results on Pascal VOC2007 and VOC2012 multi-label image datasets well demonstrate the superiority of the proposed HCP infrastructure over other state-of-the-arts. In particular, the mAP reaches 84.2% by HCP only and 90.3% after the fusion with our complementary result in [47] based on hand-crafted features on the VOC2012 dataset, which significantly outperforms the state-of-the-arts with a large margin of more than 7%. version:3
arxiv-1407-2390 | Online Stroke and Akshara Recognition GUI in Assamese Language Using Hidden Markov Model | http://arxiv.org/abs/1407.2390 | id:1407.2390 author:SRM Prasanna, Rituparna Devi, Deepjoy Das, Subhankar Ghosh, Krishna Naik category:cs.CV  published:2014-07-09 summary:The work describes the development of Online Assamese Stroke & Akshara Recognizer based on a set of language rules. In handwriting literature strokes are composed of two coordinate trace in between pen down and pen up labels. The Assamese aksharas are combination of a number of strokes, the maximum number of strokes taken to make a combination being eight. Based on these combinations eight language rule models have been made which are used to test if a set of strokes form a valid akshara. A Hidden Markov Model is used to train 181 different stroke patterns which generates a model used during stroke level testing. Akshara level testing is performed by integrating a GUI (provided by CDAC-Pune) with the Binaries of HTK toolkit classifier, HMM train model and the language rules using a dynamic linked library (dll). We have got a stroke level performance of 94.14% and akshara level performance of 84.2%. version:1
arxiv-1404-4108 | Representation as a Service | http://arxiv.org/abs/1404.4108 | id:1404.4108 author:Ouais Alsharif, Philip Bachman, Joelle Pineau category:cs.LG  published:2014-02-24 summary:Consider a Machine Learning Service Provider (MLSP) designed to rapidly create highly accurate learners for a never-ending stream of new tasks. The challenge is to produce task-specific learners that can be trained from few labeled samples, even if tasks are not uniquely identified, and the number of tasks and input dimensionality are large. In this paper, we argue that the MLSP should exploit knowledge from previous tasks to build a good representation of the environment it is in, and more precisely, that useful representations for such a service are ones that minimize generalization error for a new hypothesis trained on a new task. We formalize this intuition with a novel method that minimizes an empirical proxy of the intra-task small-sample generalization error. We present several empirical results showing state-of-the art performance on single-task transfer, multitask learning, and the full lifelong learning problem. version:2
arxiv-1407-2343 | PatchLift: Fast and Exact Computation of Patch Distances using Lifting, with Applications to Non-Local Means | http://arxiv.org/abs/1407.2343 | id:1407.2343 author:Kunal Narayan Chaudhury category:cs.CV  published:2014-07-09 summary:In this paper, we propose a fast algorithm called PatchLift for computing distances between patches extracted from a one-dimensional signal. PatchLift is based on the observation that the patch distances can be expressed in terms of simple moving sums of an image, which is derived from the one-dimensional signal via lifting. We apply PatchLift to develop a separable extension of the classical Non-Local Means (NLM) algorithm which is at least 100 times faster than NLM for standard parameter settings. The PSNR obtained using the proposed extension is typically close to (and often larger than) the PSNRs obtained using the original NLM. We provide some simulations results to demonstrate the acceleration achieved using separability and PatchLift. version:1
arxiv-1407-2334 | The jump set under geometric regularisation. Part 2: Higher-order approaches | http://arxiv.org/abs/1407.2334 | id:1407.2334 author:Tuomo Valkonen category:math.FA cs.CV 26B30  49Q20  65J20  published:2014-07-09 summary:In Part 1, we developed a new technique based on Lipschitz pushforwards for proving the jump set containment property $\mathcal{H}^{m-1}(J_u \setminus J_f)=0$ of solutions $u$ to total variation denoising. We demonstrated that the technique also applies to Huber-regularised TV. Now, in this Part 2, we extend the technique to higher-order regularisers. We are not quite able to prove the property for total generalised variation (TGV) based on the symmetrised gradient for the second-order term. We show that the property holds under three conditions: First, the solution $u$ is locally bounded. Second, the second-order variable is of locally bounded variation, $w \in \mbox{BV}_\mbox{loc}(\Omega; \mathbb{R}^m)$, instead of just bounded deformation, $w \in \mbox{BD}(\Omega)$. Third, $w$ does not jump on $J_u$ parallel to it. The second condition can be achieved for non-symmetric TGV. Both the second and third condition can be achieved if we change the Radon (or $L^1$) norm of the symmetrised gradient $Ew$ into an $L^p$ norm, $p>1$, in which case Korn's inequality holds. We also consider the application of the technique to infimal convolution TV, and study the limiting behaviour of the singular part of $D u$, as the second parameter of $\mbox{TGV}^2$ goes to zero. Unsurprisingly, it vanishes, but in numerical discretisations the situation looks quite different. Finally, our work additionally includes a result on TGV-strict approximation in $\mbox{BV}(\Omega)$. version:1
arxiv-1407-2919 | Collaborative Recommendation with Auxiliary Data: A Transfer Learning View | http://arxiv.org/abs/1407.2919 | id:1407.2919 author:Weike Pan category:cs.IR cs.LG  published:2014-07-09 summary:Intelligent recommendation technology has been playing an increasingly important role in various industry applications such as e-commerce product promotion and Internet advertisement display. Besides users' feedbacks (e.g., numerical ratings) on items as usually exploited by some typical recommendation algorithms, there are often some additional data such as users' social circles and other behaviors. Such auxiliary data are usually related to users' preferences on items behind the numerical ratings. Collaborative recommendation with auxiliary data (CRAD) aims to leverage such additional information so as to improve the personalization services, which have received much attention from both researchers and practitioners. Transfer learning (TL) is proposed to extract and transfer knowledge from some auxiliary data in order to assist the learning task on some target data. In this paper, we consider the CRAD problem from a transfer learning view, especially on how to achieve knowledge transfer from some auxiliary data. First, we give a formal definition of transfer learning for CRAD (TL-CRAD). Second, we extend the existing categorization of TL techniques (i.e., adaptive, collective and integrative knowledge transfer algorithm styles) with three knowledge transfer strategies (i.e., prediction rule, regularization and constraint). Third, we propose a novel generic knowledge transfer framework for TL-CRAD. Fourth, we describe some representative works of each specific knowledge transfer strategy of each algorithm style in detail, which are expected to inspire further works. Finally, we conclude the paper with some summary discussions and several future directions. version:1
arxiv-1403-7735 | Optimal Cooperative Cognitive Relaying and Spectrum Access for an Energy Harvesting Cognitive Radio: Reinforcement Learning Approach | http://arxiv.org/abs/1403.7735 | id:1403.7735 author:Ahmed El Shafie, Tamer Khattab, Hussien Saad, Amr Mohamed category:cs.NI cs.IT cs.LG math.IT  published:2014-03-30 summary:In this paper, we consider a cognitive setting under the context of cooperative communications, where the cognitive radio (CR) user is assumed to be a self-organized relay for the network. The CR user and the PU are assumed to be energy harvesters. The CR user cooperatively relays some of the undelivered packets of the primary user (PU). Specifically, the CR user stores a fraction of the undelivered primary packets in a relaying queue (buffer). It manages the flow of the undelivered primary packets to its relaying queue using the appropriate actions over time slots. Moreover, it has the decision of choosing the used queue for channel accessing at idle time slots (slots where the PU's queue is empty). It is assumed that one data packet transmission dissipates one energy packet. The optimal policy changes according to the primary and CR users arrival rates to the data and energy queues as well as the channels connectivity. The CR user saves energy for the PU by taking the responsibility of relaying the undelivered primary packets. It optimally organizes its own energy packets to maximize its payoff as time progresses. version:2
arxiv-1407-2256 | Inferring latent structures via information inequalities | http://arxiv.org/abs/1407.2256 | id:1407.2256 author:R. Chaves, L. Luft, T. O. Maciel, D. Gross, D. Janzing, B. Schölkopf category:stat.ML quant-ph  published:2014-07-08 summary:One of the goals of probabilistic inference is to decide whether an empirically observed distribution is compatible with a candidate Bayesian network. However, Bayesian networks with hidden variables give rise to highly non-trivial constraints on the observed distribution. Here, we propose an information-theoretic approach, based on the insight that conditions on entropies of Bayesian networks take the form of simple linear inequalities. We describe an algorithm for deriving entropic tests for latent structures. The well-known conditional independence tests appear as a special case. While the approach applies for generic Bayesian networks, we presently adopt the causal view, and show the versatility of the framework by treating several relevant problems from that domain: detecting common ancestors, quantifying the strength of causal influence, and inferring the direction of causation from two-variable marginals. version:1
arxiv-1407-2169 | Meteorological time series forecasting with pruned multi-layer perceptron and 2-stage Levenberg-Marquardt method | http://arxiv.org/abs/1407.2169 | id:1407.2169 author:Cyril Voyant, Wani W. Tamas, Marie Laure Nivet, Gilles Notton, Christophe Paoli, Aurélia Balu, Marc Muselli category:cs.NE cs.SY  published:2014-07-08 summary:A Multi-Layer Perceptron (MLP) defines a family of artificial neural networks often used in TS modeling and forecasting. Because of its "black box" aspect, many researchers refuse to use it. Moreover, the optimization (often based on the exhaustive approach where "all" configurations are tested) and learning phases of this artificial intelligence tool (often based on the Levenberg-Marquardt algorithm; LMA) are weaknesses of this approach (exhaustively and local minima). These two tasks must be repeated depending on the knowledge of each new problem studied, making the process, long, laborious and not systematically robust. In this paper a pruning process is proposed. This method allows, during the training phase, to carry out an inputs selecting method activating (or not) inter-nodes connections in order to verify if forecasting is improved. We propose to use iteratively the popular damped least-squares method to activate inputs and neurons. A first pass is applied to 10% of the learning sample to determine weights significantly different from 0 and delete other. Then a classical batch process based on LMA is used with the new MLP. The validation is done using 25 measured meteorological TS and cross-comparing the prediction results of the classical LMA and the 2-stage LMA. version:1
arxiv-1407-2019 | Assamese-English Bilingual Machine Translation | http://arxiv.org/abs/1407.2019 | id:1407.2019 author:Kalyanee Kanchan Baruah, Pranjal Das, Abdul Hannan, Shikhar Kr. Sarma category:cs.CL  published:2014-07-08 summary:Machine translation is the process of translating text from one language to another. In this paper, Statistical Machine Translation is done on Assamese and English language by taking their respective parallel corpus. A statistical phrase based translation toolkit Moses is used here. To develop the language model and to align the words we used two another tools IRSTLM, GIZA respectively. BLEU score is used to check our translation system performance, how good it is. A difference in BLEU scores is obtained while translating sentences from Assamese to English and vice-versa. Since Indian languages are morphologically very rich hence translation is relatively harder from English to Assamese resulting in a low BLEU score. A statistical transliteration system is also introduced with our translation system to deal basically with proper nouns, OOV (out of vocabulary) words which are not present in our corpus. version:1
arxiv-1406-2139 | Log-Euclidean Bag of Words for Human Action Recognition | http://arxiv.org/abs/1406.2139 | id:1406.2139 author:Masoud Faraki, Maziar Palhang, Conrad Sanderson category:cs.CV I.4.9; I.5.4  published:2014-06-09 summary:Representing videos by densely extracted local space-time features has recently become a popular approach for analysing actions. In this paper, we tackle the problem of categorising human actions by devising Bag of Words (BoW) models based on covariance matrices of spatio-temporal features, with the features formed from histograms of optical flow. Since covariance matrices form a special type of Riemannian manifold, the space of Symmetric Positive Definite (SPD) matrices, non-Euclidean geometry should be taken into account while discriminating between covariance matrices. To this end, we propose to embed SPD manifolds to Euclidean spaces via a diffeomorphism and extend the BoW approach to its Riemannian version. The proposed BoW approach takes into account the manifold geometry of SPD matrices during the generation of the codebook and histograms. Experiments on challenging human action datasets show that the proposed method obtains notable improvements in discrimination accuracy, in comparison to several state-of-the-art methods. version:2
arxiv-1407-1993 | A Critical Reassessment of Evolutionary Algorithms on the cryptanalysis of the simplified data encryption standard algorithm | http://arxiv.org/abs/1407.1993 | id:1407.1993 author:Fabien Teytaud, Cyril Fonlupt category:cs.CR cs.NE  published:2014-07-08 summary:In this paper we analyze the cryptanalysis of the simplified data encryption standard algorithm using meta-heuristics and in particular genetic algorithms. The classic fitness function when using such an algorithm is to compare n-gram statistics of a the decrypted message with those of the target message. We show that using such a function is irrelevant in case of Genetic Algorithm, simply because there is no correlation between the distance to the real key (the optimum) and the value of the fitness, in other words, there is no hidden gradient. In order to emphasize this assumption we experimentally show that a genetic algorithm perform worse than a random search on the cryptanalysis of the simplified data encryption standard algorithm. version:1
arxiv-1407-1976 | Inter-Rater Agreement Study on Readability Assessment in Bengali | http://arxiv.org/abs/1407.1976 | id:1407.1976 author:Shanta Phani, Shibamouli Lahiri, Arindam Biswas category:cs.CL  published:2014-07-08 summary:An inter-rater agreement study is performed for readability assessment in Bengali. A 1-7 rating scale was used to indicate different levels of readability. We obtained moderate to fair agreement among seven independent annotators on 30 text passages written by four eminent Bengali authors. As a by product of our study, we obtained a readability-annotated ground truth dataset in Bengali. . version:1
arxiv-1407-1957 | Regression-Based Image Alignment for General Object Categories | http://arxiv.org/abs/1407.1957 | id:1407.1957 author:Hilton Bristow, Simon Lucey category:cs.CV  published:2014-07-08 summary:Gradient-descent methods have exhibited fast and reliable performance for image alignment in the facial domain, but have largely been ignored by the broader vision community. They require the image function be smooth and (numerically) differentiable -- properties that hold for pixel-based representations obeying natural image statistics, but not for more general classes of non-linear feature transforms. We show that transforms such as Dense SIFT can be incorporated into a Lucas Kanade alignment framework by predicting descent directions via regression. This enables robust matching of instances from general object categories whilst maintaining desirable properties of Lucas Kanade such as the capacity to handle high-dimensional warp parametrizations and a fast rate of convergence. We present alignment results on a number of objects from ImageNet, and an extension of the method to unsupervised joint alignment of objects from a corpus of images. version:1
arxiv-1306-3203 | Bregman Alternating Direction Method of Multipliers | http://arxiv.org/abs/1306.3203 | id:1306.3203 author:Huahua Wang, Arindam Banerjee category:math.OC cs.LG stat.ML  published:2013-06-13 summary:The mirror descent algorithm (MDA) generalizes gradient descent by using a Bregman divergence to replace squared Euclidean distance. In this paper, we similarly generalize the alternating direction method of multipliers (ADMM) to Bregman ADMM (BADMM), which allows the choice of different Bregman divergences to exploit the structure of problems. BADMM provides a unified framework for ADMM and its variants, including generalized ADMM, inexact ADMM and Bethe ADMM. We establish the global convergence and the $O(1/T)$ iteration complexity for BADMM. In some cases, BADMM can be faster than ADMM by a factor of $O(n/\log(n))$. In solving the linear program of mass transportation problem, BADMM leads to massive parallelism and can easily run on GPU. BADMM is several times faster than highly optimized commercial software Gurobi. version:3
arxiv-1407-1933 | Lexpresso: a Controlled Natural Language | http://arxiv.org/abs/1407.1933 | id:1407.1933 author:Adam Saulwick category:cs.CL cs.AI  published:2014-07-08 summary:This paper presents an overview of `Lexpresso', a Controlled Natural Language developed at the Defence Science & Technology Organisation as a bidirectional natural language interface to a high-level information fusion system. The paper describes Lexpresso's main features including lexical coverage, expressiveness and range of linguistic syntactic and semantic structures. It also touches on its tight integration with a formal semantic formalism and tentatively classifies it against the PENS system. version:1
arxiv-1404-5344 | A higher-order MRF based variational model for multiplicative noise reduction | http://arxiv.org/abs/1404.5344 | id:1404.5344 author:Yunjin Chen, Wensen Feng, René Ranftl, Hong Qiao, Thomas Pock category:cs.CV  published:2014-04-21 summary:The Fields of Experts (FoE) image prior model, a filter-based higher-order Markov Random Fields (MRF) model, has been shown to be effective for many image restoration problems. Motivated by the successes of FoE-based approaches, in this letter, we propose a novel variational model for multiplicative noise reduction based on the FoE image prior model. The resulted model corresponds to a non-convex minimization problem, which can be solved by a recently published non-convex optimization algorithm. Experimental results based on synthetic speckle noise and real synthetic aperture radar (SAR) images suggest that the performance of our proposed method is on par with the best published despeckling algorithm. Besides, our proposed model comes along with an additional advantage, that the inference is extremely efficient. {Our GPU based implementation takes less than 1s to produce state-of-the-art despeckling performance.} version:3
arxiv-1407-1890 | Recommending Learning Algorithms and Their Associated Hyperparameters | http://arxiv.org/abs/1407.1890 | id:1407.1890 author:Michael R. Smith, Logan Mitchell, Christophe Giraud-Carrier, Tony Martinez category:cs.LG stat.ML  published:2014-07-07 summary:The success of machine learning on a given task dependson, among other things, which learning algorithm is selected and its associated hyperparameters. Selecting an appropriate learning algorithm and setting its hyperparameters for a given data set can be a challenging task, especially for users who are not experts in machine learning. Previous work has examined using meta-features to predict which learning algorithm and hyperparameters should be used. However, choosing a set of meta-features that are predictive of algorithm performance is difficult. Here, we propose to apply collaborative filtering techniques to learning algorithm and hyperparameter selection, and find that doing so avoids determining which meta-features to use and outperforms traditional meta-learning approaches in many cases. version:1
arxiv-1407-1870 | Spectral norm of random tensors | http://arxiv.org/abs/1407.1870 | id:1407.1870 author:Ryota Tomioka, Taiji Suzuki category:math.ST stat.ML stat.TH  published:2014-07-07 summary:We show that the spectral norm of a random $n_1\times n_2\times \cdots \times n_K$ tensor (or higher-order array) scales as $O\left(\sqrt{(\sum_{k=1}^{K}n_k)\log(K)}\right)$ under some sub-Gaussian assumption on the entries. The proof is based on a covering number argument. Since the spectral norm is dual to the tensor nuclear norm (the tightest convex relaxation of the set of rank one tensors), the bound implies that the convex relaxation yields sample complexity that is linear in (the sum of) the number of dimensions, which is much smaller than other recently proposed convex relaxations of tensor rank that use unfolding. version:1
arxiv-1407-1808 | Simultaneous Detection and Segmentation | http://arxiv.org/abs/1407.1808 | id:1407.1808 author:Bharath Hariharan, Pablo Arbeláez, Ross Girshick, Jitendra Malik category:cs.CV  published:2014-07-07 summary:We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-specific, top- down figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work. version:1
arxiv-1403-7550 | DimmWitted: A Study of Main-Memory Statistical Analytics | http://arxiv.org/abs/1403.7550 | id:1403.7550 author:Ce Zhang, Christopher Ré category:cs.DB cs.LG math.OC stat.ML  published:2014-03-28 summary:We perform the first study of the tradeoff space of access methods and replication to support statistical analytics using first-order methods executed in the main memory of a Non-Uniform Memory Access (NUMA) machine. Statistical analytics systems differ from conventional SQL-analytics in the amount and types of memory incoherence they can tolerate. Our goal is to understand tradeoffs in accessing the data in row- or column-order and at what granularity one should share the model and data for a statistical task. We study this new tradeoff space, and discover there are tradeoffs between hardware and statistical efficiency. We argue that our tradeoff study may provide valuable information for designers of analytics engines: for each system we consider, our prototype engine can run at least one popular task at least 100x faster. We conduct our study across five architectures using popular models including SVMs, logistic regression, Gibbs sampling, and neural networks. version:3
arxiv-1407-1768 | Downscaling near-surface atmospheric fields with multi-objective Genetic Programming | http://arxiv.org/abs/1407.1768 | id:1407.1768 author:Tanja Zerenner, Victor Venema, Petra Friederichs, Clemens Simmer category:physics.ao-ph cs.NE  published:2014-07-07 summary:The coupling of models for the different components of the Soil-Vegetation-Atmosphere-System is required to investigate component interactions and feedback processes. However, the component models for atmosphere, land-surface and subsurface are usually operated at different resolutions in space and time owing to the dominant processes. The computationally often more expensive atmospheric models, for instance, are typically employed at a coarser resolution than land-surface and subsurface models. Thus up- and downscaling procedures are required at the interface between the atmospheric model and the land-surface/subsurface models. We apply multi-objective Genetic Programming (GP) to a training data set of high-resolution atmospheric model runs to learn equations or short programs that reconstruct the fine-scale fields (e.g., 400 m resolution) of the near-surface atmospheric state variables from the coarse atmospheric model output (e.g., 2.8 km resolution). Like artificial neural networks, GP can flexibly incorporate multivariate and nonlinear relations, but offers the advantage that the solutions are human readable and thus can be checked for physical consistency. Using the Strength Pareto Approach for multi-objective fitness assignment allows us to consider multiple characteristics of the fine-scale fields during the learning procedure. version:1
arxiv-1407-1723 | The Primal-Dual Hybrid Gradient Method for Semiconvex Splittings | http://arxiv.org/abs/1407.1723 | id:1407.1723 author:Thomas Möllenhoff, Evgeny Strekalovskiy, Michael Moeller, Daniel Cremers category:math.NA cs.CV cs.NA math.OC  published:2014-07-07 summary:This paper deals with the analysis of a recent reformulation of the primal-dual hybrid gradient method [Zhu and Chan 2008, Pock, Cremers, Bischof and Chambolle 2009, Esser, Zhang and Chan 2010, Chambolle and Pock 2011], which allows to apply it to nonconvex regularizers as first proposed for truncated quadratic penalization in [Strekalovskiy and Cremers 2014]. Particularly, it investigates variational problems for which the energy to be minimized can be written as $G(u) + F(Ku)$, where $G$ is convex, $F$ semiconvex, and $K$ is a linear operator. We study the method and prove convergence in the case where the nonconvexity of $F$ is compensated by the strong convexity of the $G$. The convergence proof yields an interesting requirement for the choice of algorithm parameters, which we show to not only be sufficient, but necessary. Additionally, we show boundedness of the iterates under much weaker conditions. Finally, we demonstrate effectiveness and convergence of the algorithm beyond the theoretical guarantees in several numerical experiments. version:1
arxiv-1407-1640 | WordRep: A Benchmark for Research on Learning Word Representations | http://arxiv.org/abs/1407.1640 | id:1407.1640 author:Bin Gao, Jiang Bian, Tie-Yan Liu category:cs.CL cs.LG  published:2014-07-07 summary:WordRep is a benchmark collection for the research on learning distributed word representations (or word embeddings), released by Microsoft Research. In this paper, we describe the details of the WordRep collection and show how to use it in different types of machine learning research related to word embedding. Specifically, we describe how the evaluation tasks in WordRep are selected, how the data are sampled, and how the evaluation tool is built. We then compare several state-of-the-art word representations on WordRep, report their evaluation performance, and make discussions on the results. After that, we discuss new potential research topics that can be supported by WordRep, in addition to algorithm comparison. We hope that this paper can help people gain deeper understanding of WordRep, and enable more interesting research on learning distributed word representations and related topics. version:1
arxiv-1404-1777 | Neural Codes for Image Retrieval | http://arxiv.org/abs/1404.1777 | id:1404.1777 author:Artem Babenko, Anton Slesarev, Alexandr Chigorin, Victor Lempitsky category:cs.CV  published:2014-04-07 summary:It has been shown that the activations invoked by an image within the top layers of a large convolutional neural network provide a high-level descriptor of the visual content of the image. In this paper, we investigate the use of such descriptors (neural codes) within the image retrieval application. In the experiments with several standard retrieval benchmarks, we establish that neural codes perform competitively even when the convolutional neural network has been trained for an unrelated classification task (e.g.\ Image-Net). We also evaluate the improvement in the retrieval performance of neural codes, when the network is retrained on a dataset of images that are similar to images encountered at test time. We further evaluate the performance of the compressed neural codes and show that a simple PCA compression provides very good short codes that give state-of-the-art accuracy on a number of datasets. In general, neural codes turn out to be much more resilient to such compression in comparison other state-of-the-art descriptors. Finally, we show that discriminative dimensionality reduction trained on a dataset of pairs of matched photographs improves the performance of PCA-compressed neural codes even further. Overall, our quantitative experiments demonstrate the promise of neural codes as visual descriptors for image retrieval. version:2
arxiv-1407-1605 | Les noms propres se traduisent-ils ? Étude d'un corpus multilingue | http://arxiv.org/abs/1407.1605 | id:1407.1605 author:Émeline Lecuit, Denis Maurel, Dusko Vitas category:cs.CL  published:2014-07-07 summary:In this paper, we tackle the problem of the translation of proper names. We introduce our hypothesis according to which proper names can be translated more often than most people seem to think. Then, we describe the construction of a parallel multilingual corpus used to illustrate our point. We eventually evaluate both the advantages and limits of this corpus in our study. version:1
arxiv-1309-1952 | A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries | http://arxiv.org/abs/1309.1952 | id:1309.1952 author:Alekh Agarwal, Animashree Anandkumar, Praneeth Netrapalli category:stat.ML cs.LG math.OC  published:2013-09-08 summary:We consider the problem of learning overcomplete dictionaries in the context of sparse coding, where each sample selects a sparse subset of dictionary elements. Our main result is a strategy to approximately recover the unknown dictionary using an efficient algorithm. Our algorithm is a clustering-style procedure, where each cluster is used to estimate a dictionary element. The resulting solution can often be further cleaned up to obtain a high accuracy estimate, and we provide one simple scenario where $\ell_1$-regularized regression can be used for such a second stage. version:2
arxiv-1407-1538 | Large-Scale Multi-Label Learning with Incomplete Label Assignments | http://arxiv.org/abs/1407.1538 | id:1407.1538 author:Xiangnan Kong, Zhaoming Wu, Li-Jia Li, Ruofei Zhang, Philip S. Yu, Hang Wu, Wei Fan category:cs.LG  published:2014-07-06 summary:Multi-label learning deals with the classification problems where each instance can be assigned with multiple labels simultaneously. Conventional multi-label learning approaches mainly focus on exploiting label correlations. It is usually assumed, explicitly or implicitly, that the label sets for training instances are fully labeled without any missing labels. However, in many real-world multi-label datasets, the label assignments for training instances can be incomplete. Some ground-truth labels can be missed by the labeler from the label set. This problem is especially typical when the number instances is very large, and the labeling cost is very high, which makes it almost impossible to get a fully labeled training set. In this paper, we study the problem of large-scale multi-label learning with incomplete label assignments. We propose an approach, called MPU, based upon positive and unlabeled stochastic gradient descent and stacked models. Unlike prior works, our method can effectively and efficiently consider missing labels and label correlations simultaneously, and is very scalable, that has linear time complexities over the size of the data. Extensive experiments on two real-world multi-label datasets show that our MPU model consistently outperform other commonly-used baselines. version:1
arxiv-1312-1530 | Bandit Online Optimization Over the Permutahedron | http://arxiv.org/abs/1312.1530 | id:1312.1530 author:Nir Ailon, Kohei Hatano, Eiji Takimoto category:cs.LG  published:2013-12-05 summary:The permutahedron is the convex polytope with vertex set consisting of the vectors $(\pi(1),\dots, \pi(n))$ for all permutations (bijections) $\pi$ over $\{1,\dots, n\}$. We study a bandit game in which, at each step $t$, an adversary chooses a hidden weight weight vector $s_t$, a player chooses a vertex $\pi_t$ of the permutahedron and suffers an observed loss of $\sum_{i=1}^n \pi(i) s_t(i)$. A previous algorithm CombBand of Cesa-Bianchi et al (2009) guarantees a regret of $O(n\sqrt{T \log n})$ for a time horizon of $T$. Unfortunately, CombBand requires at each step an $n$-by-$n$ matrix permanent approximation to within improved accuracy as $T$ grows, resulting in a total running time that is super linear in $T$, making it impractical for large time horizons. We provide an algorithm of regret $O(n^{3/2}\sqrt{T})$ with total time complexity $O(n^3T)$. The ideas are a combination of CombBand and a recent algorithm by Ailon (2013) for online optimization over the permutahedron in the full information setting. The technical core is a bound on the variance of the Plackett-Luce noisy sorting process's "pseudo loss". The bound is obtained by establishing positive semi-definiteness of a family of 3-by-3 matrices generated from rational functions of exponentials of 3 parameters. version:2
arxiv-1407-1490 | Large-scale Supervised Hierarchical Feature Learning for Face Recognition | http://arxiv.org/abs/1407.1490 | id:1407.1490 author:Jianguo Li, Yurong Chen category:cs.CV  published:2014-07-06 summary:This paper proposes a novel face recognition algorithm based on large-scale supervised hierarchical feature learning. The approach consists of two parts: hierarchical feature learning and large-scale model learning. The hierarchical feature learning searches feature in three levels of granularity in a supervised way. First, face images are modeled by receptive field theory, and the representation is an image with many channels of Gaussian receptive maps. We activate a few most distinguish channels by supervised learning. Second, the face image is further represented by patches of picked channels, and we search from the over-complete patch pool to activate only those most discriminant patches. Third, the feature descriptor of each patch is further projected to lower dimension subspace with discriminant subspace analysis. Learned feature of activated patches are concatenated to get a full face representation.A linear classifier is learned to separate face pairs from same subjects and different subjects. As the number of face pairs are extremely large, we introduce ADMM (alternative direction method of multipliers) to train the linear classifier on a computing cluster. Experiments show that more training samples will bring notable accuracy improvement. We conduct experiments on FRGC and LFW. Results show that the proposed approach outperforms existing algorithms under the same protocol notably. Besides, the proposed approach is small in memory footprint, and low in computing cost, which makes it suitable for embedded applications. version:1
arxiv-1307-5601 | Kinetic Energy Plus Penalty Functions for Sparse Estimation | http://arxiv.org/abs/1307.5601 | id:1307.5601 author:Zhihua Zhang, Shibo Zhao, Zebang Shen, Shuchang Zhou category:stat.ML  published:2013-07-22 summary:In this paper we propose and study a family of sparsity-inducing penalty functions. Since the penalty functions are related to the kinetic energy in special relativity, we call them \emph{kinetic energy plus} (KEP) functions. We construct the KEP function by using the concave conjugate of a $\chi^2$-distance function and present several novel insights into the KEP function with $q=1$. In particular, we derive a thresholding operator based on the KEP function, and prove its mathematical properties and asymptotic properties in sparsity modeling. Moreover, we show that a coordinate descent algorithm is especially appropriate for the KEP function. Additionally, we discuss the relationship of KEP with the penalty functions $\ell_{1/2}$ and MCP. The theoretical and empirical analysis validates that the KEP function is effective and efficient in high-dimensional data modeling. version:3
arxiv-1407-3986 | Image Fusion Using LEP Filtering and Bilinear Interpolation | http://arxiv.org/abs/1407.3986 | id:1407.3986 author:Haritha Raveendran, Deepa Thomas category:cs.CV  published:2014-07-05 summary:Image Fusion is the process in which core information from a set of component images is merged to form a single image, which is more informative and complete than the component input images in quality and appearance. This paper presents a fast and effective image fusion method for creating high quality fused images by merging component images. In the proposed method, the input image is broken down to a two-scale image representation with a base layer having large scale variations in intensity, and a detail layer containing small scale details. Here fusion of the base and detail layers is implemented by means of a Local Edge preserving filtering based technique. The proposed method is an efficient image fusion technique in which the noise component is very low and quality of the resultant image is high so that it can be used for applications like medical image processing, requiring very accurate edge preserved images. Performance is tested by calculating PSNR and SSIM of images. The benefit of the proposed method is that it removes noise without altering the underlying structures of the image. This paper also presents an image zooming technique using bilinear interpolation in which a portion of the input image is cropped and bilinear interpolation is applied. Experimental results showed that the when PSNR value is calculated, the noise is found to be very low for the resultant image portion. version:1
arxiv-1407-3675 | A New Approach for Super resolution by Using Web Images and FFT Based Image Registration | http://arxiv.org/abs/1407.3675 | id:1407.3675 author:Archana Vijayan, Vincy Salam category:cs.CV  published:2014-07-05 summary:Preserving accuracy is a challenging issue in super resolution images. In this paper, we propose a new FFT based image registration algorithm and a sparse based super resolution algorithm to improve the accuracy of super resolution image. Given a low resolution image, our approach initially extracts the local descriptors from the input and then the local descriptors from the whole correlated images using the SIFT algorithm. Once this is completed, it will compare the local descriptors on the basis of a threshold value. The retrieved images could be having different focal length, illumination, inclination and size. To overcome the above differences of the retrieved images, we propose a new FFT based image registration algorithm. After the registration stage, we apply a sparse based super resolution on the images for recreating images with better resolution compared to the input. Based on the PSSNR calculation and SSIM comparison, we can see that the new methodology creates a better image than the traditional methods. version:1
arxiv-1407-1399 | Generalized Higher-Order Tensor Decomposition via Parallel ADMM | http://arxiv.org/abs/1407.1399 | id:1407.1399 author:Fanhua Shang, Yuanyuan Liu, James Cheng category:cs.NA cs.LG  published:2014-07-05 summary:Higher-order tensors are becoming prevalent in many scientific areas such as computer vision, social network analysis, data mining and neuroscience. Traditional tensor decomposition approaches face three major challenges: model selecting, gross corruptions and computational efficiency. To address these problems, we first propose a parallel trace norm regularized tensor decomposition method, and formulate it as a convex optimization problem. This method does not require the rank of each mode to be specified beforehand, and can automatically determine the number of factors in each mode through our optimization scheme. By considering the low-rank structure of the observed tensor, we analyze the equivalent relationship of the trace norm between a low-rank tensor and its core tensor. Then, we cast a non-convex tensor decomposition model into a weighted combination of multiple much smaller-scale matrix trace norm minimization. Finally, we develop two parallel alternating direction methods of multipliers (ADMM) to solve our problems. Experimental results verify that our regularized formulation is effective, and our methods are robust to noise or outliers. version:1
arxiv-1407-1352 | Homophilic Clustering by Locally Asymmetric Geometry | http://arxiv.org/abs/1407.1352 | id:1407.1352 author:Deli Zhao, Xiaoou Tang category:cs.CV 68T10  62H30  91C20  published:2014-07-05 summary:Clustering is indispensable for data analysis in many scientific disciplines. Detecting clusters from heavy noise remains challenging, particularly for high-dimensional sparse data. Based on graph-theoretic framework, the present paper proposes a novel algorithm to address this issue. The locally asymmetric geometries of neighborhoods between data points result in a directed similarity graph to model the structural connectivity of data points. Performing similarity propagation on this directed graph simply by its adjacency matrix powers leads to an interesting discovery, in the sense that if the in-degrees are ordered by the corresponding sorted out-degrees, they will be self-organized to be homophilic layers according to the different distributions of cluster densities, which is dubbed the Homophilic In-degree figure (the HI figure). With the HI figure, we can easily single out all cores of clusters, identify the boundary between cluster and noise, and visualize the intrinsic structures of clusters. Based on the in-degree homophily, we also develop a simple efficient algorithm of linear space complexity to cluster noisy data. Extensive experiments on toy and real-world scientific data validate the effectiveness of our algorithms. version:1
arxiv-1407-1339 | Inverse Graphics with Probabilistic CAD Models | http://arxiv.org/abs/1407.1339 | id:1407.1339 author:Tejas D. Kulkarni, Vikash K. Mansinghka, Pushmeet Kohli, Joshua B. Tenenbaum category:cs.CV cs.AI stat.ML  published:2014-07-04 summary:Recently, multiple formulations of vision problems as probabilistic inversions of generative models based on computer graphics have been proposed. However, applications to 3D perception from natural images have focused on low-dimensional latent scenes, due to challenges in both modeling and inference. Accounting for the enormous variability in 3D object shape and 2D appearance via realistic generative models seems intractable, as does inverting even simple versions of the many-to-many computations that link 3D scenes to 2D images. This paper proposes and evaluates an approach that addresses key aspects of both these challenges. We show that it is possible to solve challenging, real-world 3D vision problems by approximate inference in generative models for images based on rendering the outputs of probabilistic CAD (PCAD) programs. Our PCAD object geometry priors generate deformable 3D meshes corresponding to plausible objects and apply affine transformations to place them in a scene. Image likelihoods are based on similarity in a feature space based on standard mid-level image representations from the vision literature. Our inference algorithm integrates single-site and locally blocked Metropolis-Hastings proposals, Hamiltonian Monte Carlo and discriminative data-driven proposals learned from training data generated from our models. We apply this approach to 3D human pose estimation and object shape reconstruction from single images, achieving quantitative and qualitative performance improvements over state-of-the-art baselines. version:1
arxiv-1407-1267 | Calibration of Multiple Fish-Eye Cameras Using a Wand | http://arxiv.org/abs/1407.1267 | id:1407.1267 author:Qiang Fu, Quan Quan, Kai-Yuan Cai category:cs.CV  published:2014-07-04 summary:Fish-eye cameras are becoming increasingly popular in computer vision, but their use for 3D measurement is limited partly due to the lack of an accurate, efficient and user-friendly calibration procedure. For such a purpose, we propose a method to calibrate the intrinsic and extrinsic parameters (including radial distortion parameters) of two/multiple fish-eye cameras simultaneously by using a wand under general motions. Thanks to the generic camera model used, the proposed calibration method is also suitable for two/multiple conventional cameras and mixed cameras (e.g. two conventional cameras and a fish-eye camera). Simulation and real experiments demonstrate the effectiveness of the proposed method. Moreover, we develop the camera calibration toolbox, which is available online. version:1
arxiv-1407-1208 | Weakly Supervised Action Labeling in Videos Under Ordering Constraints | http://arxiv.org/abs/1407.1208 | id:1407.1208 author:Piotr Bojanowski, Rémi Lajugie, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid, Josef Sivic category:cs.CV cs.LG  published:2014-07-04 summary:We are given a set of video clips, each one annotated with an {\em ordered} list of actions, such as "walk" then "sit" then "answer phone" extracted from, for example, the associated text script. We seek to temporally localize the individual actions in each clip as well as to learn a discriminative classifier for each action. We formulate the problem as a weakly supervised temporal assignment with ordering constraints. Each video clip is divided into small time intervals and each time interval of each video clip is assigned one action label, while respecting the order in which the action labels appear in the given annotations. We show that the action label assignment can be determined together with learning a classifier for each action in a discriminative manner. We evaluate the proposed model on a new and challenging dataset of 937 video clips with a total of 787720 frames containing sequences of 16 different actions from 69 Hollywood movies. version:1
arxiv-1407-1201 | Improving Performance of Self-Organising Maps with Distance Metric Learning Method | http://arxiv.org/abs/1407.1201 | id:1407.1201 author:Piotr Płoński, Krzysztof Zaremba category:cs.LG cs.NE  published:2014-07-04 summary:Self-Organising Maps (SOM) are Artificial Neural Networks used in Pattern Recognition tasks. Their major advantage over other architectures is human readability of a model. However, they often gain poorer accuracy. Mostly used metric in SOM is the Euclidean distance, which is not the best approach to some problems. In this paper, we study an impact of the metric change on the SOM's performance in classification problems. In order to change the metric of the SOM we applied a distance metric learning method, so-called 'Large Margin Nearest Neighbour'. It computes the Mahalanobis matrix, which assures small distance between nearest neighbour points from the same class and separation of points belonging to different classes by large margin. Results are presented on several real data sets, containing for example recognition of written digits, spoken letters or faces. version:1
arxiv-1407-1176 | Identifying Higher-order Combinations of Binary Features | http://arxiv.org/abs/1407.1176 | id:1407.1176 author:Felipe Llinares, Mahito Sugiyama, Karsten M. Borgwardt category:stat.ML cs.LG  published:2014-07-04 summary:Finding statistically significant interactions between binary variables is computationally and statistically challenging in high-dimensional settings, due to the combinatorial explosion in the number of hypotheses. Terada et al. recently showed how to elegantly address this multiple testing problem by excluding non-testable hypotheses. Still, it remains unclear how their approach scales to large datasets. We here proposed strategies to speed up the approach by Terada et al. and evaluate them thoroughly in 11 real-world benchmark datasets. We observe that one approach, incremental search with early stopping, is orders of magnitude faster than the current state-of-the-art approach. version:1
arxiv-1407-1165 | Recognition of Isolated Words using Zernike and MFCC features for Audio Visual Speech Recognition | http://arxiv.org/abs/1407.1165 | id:1407.1165 author:Prashant Bordea, Amarsinh Varpeb, Ramesh Manzac, Pravin Yannawara category:cs.CV cs.CL  published:2014-07-04 summary:Automatic Speech Recognition (ASR) by machine is an attractive research topic in signal processing domain and has attracted many researchers to contribute in this area. In recent year, there have been many advances in automatic speech reading system with the inclusion of audio and visual speech features to recognize words under noisy conditions. The objective of audio-visual speech recognition system is to improve recognition accuracy. In this paper we computed visual features using Zernike moments and audio feature using Mel Frequency Cepstral Coefficients (MFCC) on vVISWa (Visual Vocabulary of Independent Standard Words) dataset which contains collection of isolated set of city names of 10 speakers. The visual features were normalized and dimension of features set was reduced by Principal Component Analysis (PCA) in order to recognize the isolated word utterance on PCA space.The performance of recognition of isolated words based on visual only and audio only features results in 63.88 and 100 respectively. version:1
arxiv-1407-1151 | Optimizing Ranking Measures for Compact Binary Code Learning | http://arxiv.org/abs/1407.1151 | id:1407.1151 author:Guosheng Lin, Chunhua Shen, Jianxin Wu category:cs.LG cs.CV  published:2014-07-04 summary:Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest---multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures. The resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods. version:1
arxiv-1406-7360 | A framework for improving the performance of verification algorithms with a low false positive rate requirement and limited training data | http://arxiv.org/abs/1406.7360 | id:1406.7360 author:Ognjen Arandjelovic category:cs.CV  published:2014-06-28 summary:In this paper we address the problem of matching patterns in the so-called verification setting in which a novel, query pattern is verified against a single training pattern: the decision sought is whether the two match (i.e. belong to the same class) or not. Unlike previous work which has universally focused on the development of more discriminative distance functions between patterns, here we consider the equally important and pervasive task of selecting a distance threshold which fits a particular operational requirement - specifically, the target false positive rate (FPR). First, we argue on theoretical grounds that a data-driven approach is inherently ill-conditioned when the desired FPR is low, because by the very nature of the challenge only a small portion of training data affects or is affected by the desired threshold. This leads us to propose a general, statistical model-based method instead. Our approach is based on the interpretation of an inter-pattern distance as implicitly defining a pattern embedding which approximately distributes patterns according to an isotropic multi-variate normal distribution in some space. This interpretation is then used to show that the distribution of training inter-pattern distances is the non-central chi2 distribution, differently parameterized for each class. Thus, to make the class-specific threshold choice we propose a novel analysis-by-synthesis iterative algorithm which estimates the three free parameters of the model (for each class) using task-specific constraints. The validity of the premises of our work and the effectiveness of the proposed method are demonstrated by applying the method to the task of set-based face verification on a large database of pseudo-random head motion videos. version:2
arxiv-1407-1123 | Expanding the Family of Grassmannian Kernels: An Embedding Perspective | http://arxiv.org/abs/1407.1123 | id:1407.1123 author:Mehrtash T. Harandi, Mathieu Salzmann, Sadeep Jayasumana, Richard Hartley, Hongdong Li category:cs.CV cs.LG stat.ML  published:2014-07-04 summary:Modeling videos and image-sets as linear subspaces has proven beneficial for many visual recognition tasks. However, it also incurs challenges arising from the fact that linear subspaces do not obey Euclidean geometry, but lie on a special type of Riemannian manifolds known as Grassmannian. To leverage the techniques developed for Euclidean spaces (e.g, support vector machines) with subspaces, several recent studies have proposed to embed the Grassmannian into a Hilbert space by making use of a positive definite kernel. Unfortunately, only two Grassmannian kernels are known, none of which -as we will show- is universal, which limits their ability to approximate a target function arbitrarily well. Here, we introduce several positive definite Grassmannian kernels, including universal ones, and demonstrate their superiority over previously-known kernels in various tasks, such as classification, clustering, sparse coding and hashing. version:1
arxiv-1407-1097 | Robust Optimization using Machine Learning for Uncertainty Sets | http://arxiv.org/abs/1407.1097 | id:1407.1097 author:Theja Tulabandhula, Cynthia Rudin category:math.OC cs.LG stat.ML  published:2014-07-04 summary:Our goal is to build robust optimization problems for making decisions based on complex data from the past. In robust optimization (RO) generally, the goal is to create a policy for decision-making that is robust to our uncertainty about the future. In particular, we want our policy to best handle the the worst possible situation that could arise, out of an uncertainty set of possible situations. Classically, the uncertainty set is simply chosen by the user, or it might be estimated in overly simplistic ways with strong assumptions; whereas in this work, we learn the uncertainty set from data collected in the past. The past data are drawn randomly from an (unknown) possibly complicated high-dimensional distribution. We propose a new uncertainty set design and show how tools from statistical learning theory can be employed to provide probabilistic guarantees on the robustness of the policy. version:1
arxiv-1407-1082 | Online Submodular Maximization under a Matroid Constraint with Application to Learning Assignments | http://arxiv.org/abs/1407.1082 | id:1407.1082 author:Daniel Golovin, Andreas Krause, Matthew Streeter category:cs.LG  published:2014-07-03 summary:Which ads should we display in sponsored search in order to maximize our revenue? How should we dynamically rank information sources to maximize the value of the ranking? These applications exhibit strong diminishing returns: Redundancy decreases the marginal utility of each ad or information source. We show that these and other problems can be formalized as repeatedly selecting an assignment of items to positions to maximize a sequence of monotone submodular functions that arrive one by one. We present an efficient algorithm for this general problem and analyze it in the no-regret model. Our algorithm possesses strong theoretical guarantees, such as a performance ratio that converges to the optimal constant of 1 - 1/e. We empirically evaluate our algorithm on two real-world online optimization problems on the web: ad allocation with submodular utilities, and dynamically ranking blogs to detect information cascades. Finally, we present a second algorithm that handles the more general case in which the feasible sets are given by a matroid constraint, while still maintaining a 1 - 1/e asymptotic performance ratio. version:1
arxiv-1404-7527 | A Map of Update Constraints in Inductive Inference | http://arxiv.org/abs/1404.7527 | id:1404.7527 author:Timo Kötzing, Raphaela Palenta category:cs.LG  published:2014-04-29 summary:We investigate how different learning restrictions reduce learning power and how the different restrictions relate to one another. We give a complete map for nine different restrictions both for the cases of complete information learning and set-driven learning. This completes the picture for these well-studied \emph{delayable} learning restrictions. A further insight is gained by different characterizations of \emph{conservative} learning in terms of variants of \emph{cautious} learning. Our analyses greatly benefit from general theorems we give, for example showing that learners with exclusively delayable restrictions can always be assumed total. version:2
arxiv-1407-0935 | Multiple Moving Object Recognitions in video based on Log Gabor-PCA Approach | http://arxiv.org/abs/1407.0935 | id:1407.0935 author:M. T Gopalakrishna, M. Ravishankar, D. R Rameshbabu category:cs.CV 68T45 I.4.8  published:2014-07-03 summary:Object recognition in the video sequence or images is one of the sub-field of computer vision. Moving object recognition from a video sequence is an appealing topic with applications in various areas such as airport safety, intrusion surveillance, video monitoring, intelligent highway, etc. Moving object recognition is the most challenging task in intelligent video surveillance system. In this regard, many techniques have been proposed based on different methods. Despite of its importance, moving object recognition in complex environments is still far from being completely solved for low resolution videos, foggy videos, and also dim video sequences. All in all, these make it necessary to develop exceedingly robust techniques. This paper introduces multiple moving object recognition in the video sequence based on LoG Gabor-PCA approach and Angle based distance Similarity measures techniques used to recognize the object as a human, vehicle etc. Number of experiments are conducted for indoor and outdoor video sequences of standard datasets and also our own collection of video sequences comprising of partial night vision video sequences. Experimental results show that our proposed approach achieves an excellent recognition rate. Results obtained are satisfactory and competent. version:1
arxiv-1407-0921 | Solving QVIs for Image Restoration with Adaptive Constraint Sets | http://arxiv.org/abs/1407.0921 | id:1407.0921 author:Frank Lenzen, Jan Lellmann, Florian Becker, Christoph Schnörr category:math.OC cs.CV math.NA  published:2014-07-03 summary:We consider a class of quasi-variational inequalities (QVIs) for adaptive image restoration, where the adaptivity is described via solution-dependent constraint sets. In previous work we studied both theoretical and numerical issues. While we were able to show the existence of solutions for a relatively broad class of problems, we encountered problems concerning uniqueness of the solution as well as convergence of existing algorithms for solving QVIs. In particular, it seemed that with increasing image size the growing condition number of the involved differential operator poses severe problems. In the present paper we prove uniqueness for a larger class of problems and in particular independent of the image size. Moreover, we provide a numerical algorithm with proved convergence. Experimental results support our theoretical findings. version:1
arxiv-1407-3673 | Enhanced EZW Technique for Compression of Image by Setting Detail Retaining Pass Number | http://arxiv.org/abs/1407.3673 | id:1407.3673 author:Isha Tyagi, Ashish Nautiyal, Vishwanath Bijalwan, Meenu Balodhi category:cs.CV  published:2014-07-03 summary:For keeping the data secured and maintained, compression is most essential aspect. For which efficiency is the important part to be researched continuously until the satisfactory result is achieved. the optimized ratio of data is necessary for compression and embedded transmission. In this paper the main objective is to improve the execution time evolved in EZW compression. version:1
arxiv-1407-0822 | Reducing Offline Evaluation Bias in Recommendation Systems | http://arxiv.org/abs/1407.0822 | id:1407.0822 author:Arnaud De Myttenaere, Bénédicte Le Grand, Boris Golden, Fabrice Rossi category:cs.IR cs.LG stat.ML  published:2014-07-03 summary:Recommendation systems have been integrated into the majority of large online systems. They tailor those systems to individual users by filtering and ranking information according to user profiles. This adaptation process influences the way users interact with the system and, as a consequence, increases the difficulty of evaluating a recommendation algorithm with historical data (via offline evaluation). This paper analyses this evaluation bias and proposes a simple item weighting solution that reduces its impact. The efficiency of the proposed solution is evaluated on real world data extracted from Viadeo professional social network. version:1
arxiv-1406-6558 | $ N^4 $-Fields: Neural Network Nearest Neighbor Fields for Image Transforms | http://arxiv.org/abs/1406.6558 | id:1406.6558 author:Yaroslav Ganin, Victor Lempitsky category:cs.CV  published:2014-06-25 summary:We propose a new architecture for difficult image processing operations, such as natural edge detection or thin object segmentation. The architecture is based on a simple combination of convolutional neural networks with the nearest neighbor search. We focus our attention on the situations when the desired image transformation is too hard for a neural network to learn explicitly. We show that in such situations, the use of the nearest neighbor search on top of the network output allows to improve the results considerably and to account for the underfitting effect during the neural network training. The approach is validated on three challenging benchmarks, where the performance of the proposed architecture matches or exceeds the state-of-the-art. version:2
arxiv-1407-0786 | Strengthening the Effectiveness of Pedestrian Detection with Spatially Pooled Features | http://arxiv.org/abs/1407.0786 | id:1407.0786 author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2014-07-03 summary:We propose a simple yet effective approach to the problem of pedestrian detection which outperforms the current state-of-the-art. Our new features are built on the basis of low-level visual features and spatial pooling. Incorporating spatial pooling improves the translational invariance and thus the robustness of the detection process. We then directly optimise the partial area under the ROC curve (\pAUC) measure, which concentrates detection performance in the range of most practical importance. The combination of these factors leads to a pedestrian detector which outperforms all competitors on all of the standard benchmark datasets. We advance state-of-the-art results by lowering the average miss rate from $13\%$ to $11\%$ on the INRIA benchmark, $41\%$ to $37\%$ on the ETH benchmark, $51\%$ to $42\%$ on the TUD-Brussels benchmark and $36\%$ to $29\%$ on the Caltech-USA benchmark. version:1
arxiv-1206-4832 | Smoothed Functional Algorithms for Stochastic Optimization using q-Gaussian Distributions | http://arxiv.org/abs/1206.4832 | id:1206.4832 author:Debarghya Ghoshdastidar, Ambedkar Dukkipati, Shalabh Bhatnagar category:cs.IT cs.LG math.IT stat.ME G.1.6; I.6.8  published:2012-06-21 summary:Smoothed functional (SF) schemes for gradient estimation are known to be efficient in stochastic optimization algorithms, specially when the objective is to improve the performance of a stochastic system. However, the performance of these methods depends on several parameters, such as the choice of a suitable smoothing kernel. Different kernels have been studied in literature, which include Gaussian, Cauchy and uniform distributions among others. This paper studies a new class of kernels based on the q-Gaussian distribution, that has gained popularity in statistical physics over the last decade. Though the importance of this family of distributions is attributed to its ability to generalize the Gaussian distribution, we observe that this class encompasses almost all existing smoothing kernels. This motivates us to study SF schemes for gradient estimation using the q-Gaussian distribution. Using the derived gradient estimates, we propose two-timescale algorithms for optimization of a stochastic objective function in a constrained setting with projected gradient search approach. We prove the convergence of our algorithms to the set of stationary points of an associated ODE. We also demonstrate their performance numerically through simulations on a queuing model. version:6
arxiv-1407-0765 | BiofilmQuant: A Computer-Assisted Tool for Dental Biofilm Quantification | http://arxiv.org/abs/1407.0765 | id:1407.0765 author:Awais Mansoor, Valery Patsekin, Dale Scherl, J. Paul Robinson, Bartlomiej Rajwa category:cs.CV  published:2014-07-03 summary:Dental biofilm is the deposition of microbial material over a tooth substratum. Several methods have recently been reported in the literature for biofilm quantification; however, at best they provide a barely automated solution requiring significant input needed from the human expert. On the contrary, state-of-the-art automatic biofilm methods fail to make their way into clinical practice because of the lack of effective mechanism to incorporate human input to handle praxis or misclassified regions. Manual delineation, the current gold standard, is time consuming and subject to expert bias. In this paper, we introduce a new semi-automated software tool, BiofilmQuant, for dental biofilm quantification in quantitative light-induced fluorescence (QLF) images. The software uses a robust statistical modeling approach to automatically segment the QLF image into three classes (background, biofilm, and tooth substratum) based on the training data. This initial segmentation has shown a high degree of consistency and precision on more than 200 test QLF dental scans. Further, the proposed software provides the clinicians full control to fix any misclassified areas using a single click. In addition, BiofilmQuant also provides a complete solution for the longitudinal quantitative analysis of biofilm of the full set of teeth, providing greater ease of usability. version:1
arxiv-1407-0754 | Structured Learning via Logistic Regression | http://arxiv.org/abs/1407.0754 | id:1407.0754 author:Justin Domke category:cs.LG stat.ML  published:2014-07-03 summary:A successful approach to structured learning is to write the learning objective as a joint function of linear parameters and inference messages, and iterate between updates to each. This paper observes that if the inference problem is "smoothed" through the addition of entropy terms, for fixed messages, the learning objective reduces to a traditional (non-structured) logistic regression problem with respect to parameters. In these logistic regression problems, each training example has a bias term determined by the current set of messages. Based on this insight, the structured energy function can be extended from linear factors to any function class where an "oracle" exists to minimize a logistic loss. version:1
arxiv-1407-0717 | Deep Poselets for Human Detection | http://arxiv.org/abs/1407.0717 | id:1407.0717 author:Lubomir Bourdev, Fei Yang, Rob Fergus category:cs.CV  published:2014-07-02 summary:We address the problem of detecting people in natural scenes using a part approach based on poselets. We propose a bootstrapping method that allows us to collect millions of weakly labeled examples for each poselet type. We use these examples to train a Convolutional Neural Net to discriminate different poselet types and separate them from the background class. We then use the trained CNN as a way to represent poselet patches with a Pose Discriminative Feature (PDF) vector -- a compact 256-dimensional feature vector that is effective at discriminating pose from appearance. We train the poselet model on top of PDF features and combine them with object-level CNNs for detection and bounding box prediction. The resulting model leads to state-of-the-art performance for human detection on the PASCAL datasets. version:1
arxiv-1407-0698 | Continuous On-line Evolution of Agent Behaviours with Cartesian Genetic Programming | http://arxiv.org/abs/1407.0698 | id:1407.0698 author:Davide Nunes, Luis Antunes category:cs.NE cs.MA  published:2014-07-02 summary:Evolutionary Computation has been successfully used to synthesise controllers for embodied agents and multi-agent systems in general. Notwithstanding this, continuous on-line adaptation by the means of evolutionary algorithms is still under-explored, especially outside the evolutionary robotics domain. In this paper, we present an on-line evolutionary programming algorithm that searches in the agent design space for the appropriate behavioural policies to cope with the underlying environment. We discuss the current problems of continuous agent adaptation, present our on-line evolution testbed for evolutionary simulation. version:1
arxiv-1407-0612 | Nonparametric Hierarchical Clustering of Functional Data | http://arxiv.org/abs/1407.0612 | id:1407.0612 author:Marc Boullé, Romain Guigourès, Fabrice Rossi category:stat.ML cs.LG  published:2014-07-02 summary:In this paper, we deal with the problem of curves clustering. We propose a nonparametric method which partitions the curves into clusters and discretizes the dimensions of the curve points into intervals. The cross-product of these partitions forms a data-grid which is obtained using a Bayesian model selection approach while making no assumptions regarding the curves. Finally, a post-processing technique, aiming at reducing the number of clusters in order to improve the interpretability of the clustering, is proposed. It consists in optimally merging the clusters step by step, which corresponds to an agglomerative hierarchical classification whose dissimilarity measure is the variation of the criterion. Interestingly this measure is none other than the sum of the Kullback-Leibler divergences between clusters distributions before and after the merges. The practical interest of the approach for functional data exploratory analysis is presented and compared with an alternative approach on an artificial and a real world data set. version:1
arxiv-1407-0611 | How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need? | http://arxiv.org/abs/1407.0611 | id:1407.0611 author:Fabrice Rossi category:stat.ML cs.LG cs.NE  published:2014-07-02 summary:In numerous applicative contexts, data are too rich and too complex to be represented by numerical vectors. A general approach to extend machine learning and data mining techniques to such data is to really on a dissimilarity or on a kernel that measures how different or similar two objects are. This approach has been used to define several variants of the Self Organizing Map (SOM). This paper reviews those variants in using a common set of notations in order to outline differences and similarities between them. It discusses the advantages and drawbacks of the variants, as well as the actual relevance of the dissimilarity/kernel SOM for practical applications. version:1
arxiv-1407-0977 | Higher-Order Quantum-Inspired Genetic Algorithms | http://arxiv.org/abs/1407.0977 | id:1407.0977 author:Robert Nowotniak, Jacek Kucharski category:cs.NE quant-ph  published:2014-07-02 summary:This paper presents a theory and an empirical evaluation of Higher-Order Quantum-Inspired Genetic Algorithms. Fundamental notions of the theory have been introduced, and a novel Order-2 Quantum-Inspired Genetic Algorithm (QIGA2) has been presented. Contrary to all QIGA algorithms which represent quantum genes as independent qubits, in higher-order QIGAs quantum registers are used to represent genes strings which allows modelling of genes relations using quantum phenomena. Performance comparison has been conducted on a benchmark of 20 deceptive combinatorial optimization problems. It has been presented that using higher quantum orders is beneficial for genetic algorithm efficiency, and the new QIGA2 algorithm outperforms the old QIGA algorithm which was tuned in highly compute intensive metaoptimization process. version:1
arxiv-1208-0645 | On the Consistency of AUC Pairwise Optimization | http://arxiv.org/abs/1208.0645 | id:1208.0645 author:Wei Gao, Zhi-Hua Zhou category:cs.LG stat.ML  published:2012-08-03 summary:AUC (area under ROC curve) is an important evaluation criterion, which has been popularly used in many learning tasks such as class-imbalance learning, cost-sensitive learning, learning to rank, etc. Many learning approaches try to optimize AUC, while owing to the non-convexity and discontinuousness of AUC, almost all approaches work with surrogate loss functions. Thus, the consistency of AUC is crucial; however, it has been almost untouched before. In this paper, we provide a sufficient condition for the asymptotic consistency of learning approaches based on surrogate loss functions. Based on this result, we prove that exponential loss and logistic loss are consistent with AUC, but hinge loss is inconsistent. Then, we derive the $q$-norm hinge loss and general hinge loss that are consistent with AUC. We also derive the consistent bounds for exponential loss and logistic loss, and obtain the consistent bounds for many surrogate loss functions under the non-noise setting. Further, we disclose an equivalence between the exponential surrogate loss of AUC and exponential surrogate loss of accuracy, and one straightforward consequence of such finding is that AdaBoost and RankBoost are equivalent. version:4
arxiv-1402-3811 | Dropout Rademacher Complexity of Deep Neural Networks | http://arxiv.org/abs/1402.3811 | id:1402.3811 author:Wei Gao, Zhi-Hua Zhou category:cs.NE stat.ML  published:2014-02-16 summary:Great successes of deep neural networks have been witnessed in various real applications. Many algorithmic and implementation techniques have been developed, however, theoretical understanding of many aspects of deep neural networks is far from clear. A particular interesting issue is the usefulness of dropout, which was motivated from the intuition of preventing complex co-adaptation of feature detectors. In this paper, we study the Rademacher complexity of different types of dropout, and our theoretical results disclose that for shallow neural networks (with one or none hidden layer) dropout is able to reduce the Rademacher complexity in polynomial, whereas for deep neural networks it can amazingly lead to an exponential reduction of the Rademacher complexity. version:2
