arxiv-1606-03203 | Causal Bandits: Learning Good Interventions via Causal Inference | http://arxiv.org/abs/1606.03203 | id:1606.03203 author:Finnian Lattimore, Tor Lattimore, Mark D. Reid category:stat.ML cs.LG  published:2016-06-10 summary:We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information. version:1
arxiv-1606-03196 | Phase Retrieval via Incremental Truncated Wirtinger Flow | http://arxiv.org/abs/1606.03196 | id:1606.03196 author:Ritesh Kolte, Ayfer Özgür category:cs.IT cs.LG math.IT  published:2016-06-10 summary:In the phase retrieval problem, an unknown vector is to be recovered given quadratic measurements. This problem has received considerable attention in recent times. In this paper, we present an algorithm to solve a nonconvex formulation of the phase retrieval problem, that we call $\textit{Incremental Truncated Wirtinger Flow}$. Given random Gaussian sensing vectors, we prove that it converges linearly to the solution, with an optimal sample complexity. We also provide stability guarantees of the algorithm under noisy measurements. Performance and comparisons with existing algorithms are illustrated via numerical experiments on simulated and real data, with both random and structured sensing vectors. version:1
arxiv-1606-03192 | PSDVec: a Toolbox for Incremental and Scalable Word Embedding | http://arxiv.org/abs/1606.03192 | id:1606.03192 author:Shaohua Li, Jun Zhu, Chunyan Miao category:cs.CL  published:2016-06-10 summary:PSDVec is a Python/Perl toolbox that learns word embeddings, i.e. the mapping of words in a natural language to continuous vectors which encode the semantic/syntactic regularities between the words. PSDVec implements a word embedding learning method based on a weighted low-rank positive semidefinite approximation. To scale up the learning process, we implement a blockwise online learning algorithm to learn the embeddings incrementally. This strategy greatly reduces the learning time of word embeddings on a large vocabulary, and can learn the embeddings of new words without re-learning the whole vocabulary. On 9 word similarity/analogy benchmark sets and 2 Natural Language Processing (NLP) tasks, PSDVec produces embeddings that has the best average performance among popular word embedding tools. PSDVec provides a new option for NLP practitioners. version:1
arxiv-1606-01541 | Deep Reinforcement Learning for Dialogue Generation | http://arxiv.org/abs/1606.01541 | id:1606.01541 author:Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky category:cs.CL  published:2016-06-05 summary:Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues. version:2
arxiv-1606-03168 | Finding Low-rank Solutions to Matrix Problems, Efficiently and Provably | http://arxiv.org/abs/1606.03168 | id:1606.03168 author:Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, Sujay Sanghavi category:math.OC cs.DS cs.IT cs.LG cs.NA math.IT  published:2016-06-10 summary:A rank-r matrix X \in R^{m x n} can be written as a product UV', where U \in R^{m x r} and V \in R^{n x r}. One could exploit this observation in optimization: e.g., consider the minimization of a convex function f(X) over rank-r matrices, where the scaffold of rank-r matrices is modeled via the factorization in U and V variables. Such heuristic has been widely used before for specific problem instances, where the solution sought is (approximately) low-rank. Though such parameterization reduces the number of variables and is more efficient in computational speed and memory requirement (of particular interest is the case r << min{m, n}), it comes at a cost: f(UV') becomes a non-convex function w.r.t. U and V. In this paper, we study such parameterization in optimization of generic convex f and focus on first-order, gradient descent algorithmic solutions. We propose an algorithm we call the Bi-Factored Gradient Descent (BFGD) algorithm, an efficient first-order method that operates on the U, V factors. We show that when f is smooth, BFGD has local sublinear convergence, and linear convergence when f is both smooth and strongly convex. Moreover, for several key applications, we provide simple and efficient initialization schemes that provide approximate solutions good enough for the above convergence results to hold. version:1
arxiv-1606-01316 | Provable non-convex projected gradient descent for a class of constrained matrix optimization problems | http://arxiv.org/abs/1606.01316 | id:1606.01316 author:Dohyung Park, Anastasios Kyrillidis, Srinadh Bhojanapalli, Constantine Caramanis, Sujay Sanghavi category:stat.ML cs.DS cs.IT cs.NA math.IT math.OC  published:2016-06-04 summary:We propose a simple and scalable non-convex method for low-rank PSD matrix problems with a generic (strongly) convex objective $f$, and additional matrix norm constraints. Such criteria appear in quantum state tomography and phase retrieval applications, among others. However, without careful design, existing methods quickly run into time and memory bottlenecks, as problem dimensions increase. To remedy these shortcomings, we propose the Projected Factored Gradient Descent (ProjFGD) algorithm, that operates on the low-rank factorization of the variable space. Such factorization imputes non-convexity in the optimization; nevertheless, we show that our method favors local linear convergence rate in the non-convex factored space, for a class of convex norm-constrained problems. We build our theory on a novel descent lemma, that extends recent results on the unconstrained version of the problem. Our findings are supported by empirical evidence on quantum state tomography and sparse phase retrieval applications. version:2
arxiv-1503-07810 | Interpretable Classification Models for Recidivism Prediction | http://arxiv.org/abs/1503.07810 | id:1503.07810 author:Jiaming Zeng, Berk Ustun, Cynthia Rudin category:stat.ML stat.AP  published:2015-03-26 summary:We investigate a long-debated question, which is how to create predictive models of recidivism that are sufficiently accurate, transparent, and interpretable to use for decision-making. This question is complicated as these models are used to support different decisions, from sentencing, to determining release on probation, to allocating preventative social services. Each use case might have an objective other than classification accuracy, such as a desired true positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair is a point on the receiver operator characteristic (ROC) curve. We use popular machine learning methods to create models along the full ROC curve on a wide range of recidivism prediction problems. We show that many methods (SVM, Ridge Regression) produce equally accurate models along the full ROC curve. However, methods that designed for interpretability (CART, C5.0) cannot be tuned to produce models that are accurate and/or interpretable. To handle this shortcoming, we use a new method known as SLIM (Supersparse Linear Integer Models) to produce accurate, transparent, and interpretable models along the full ROC curve. These models can be used for decision-making for many different use cases, since they are just as accurate as the most powerful black-box machine learning models, but completely transparent, and highly interpretable. version:5
arxiv-1601-01343 | Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation | http://arxiv.org/abs/1601.01343 | id:1601.01343 author:Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji category:cs.CL  published:2016-01-06 summary:Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset. version:4
arxiv-1606-03153 | Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition | http://arxiv.org/abs/1606.03153 | id:1606.03153 author:Furong Huang, Animashree Anandkumar category:cs.CL cs.LG  published:2016-06-10 summary:Text embeddings have played a key role in obtaining state-of-the-art results in natural language processing. Word2Vec and its variants have successfully mapped words with similar syntactic or semantic meanings to nearby vectors. However, extracting universal embeddings of longer word-sequences remains a challenging task. We employ the convolutional dictionary model for unsupervised learning of embeddings for variable length word-sequences. We propose a two-phase ConvDic+DeconvDec framework that first learns dictionary elements (i.e., phrase templates), and then employs them for decoding the activations. The estimated activations are then used as embeddings for downstream tasks such as sentiment analysis, paraphrase detection, and semantic textual similarity estimation. We propose a convolutional tensor decomposition algorithm for learning the phrase templates. It is shown to be more accurate, and much more efficient than the popular alternating minimization in dictionary learning literature. Our word-sequence embeddings achieve state-of-the-art performance in sentiment classification, semantic textual similarity estimation, and paraphrase detection over eight datasets from various domains, without requiring pre-training or additional features. version:1
arxiv-1606-03152 | Policy Networks with Two-Stage Training for Dialogue Systems | http://arxiv.org/abs/1606.03152 | id:1606.03152 author:Mehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He, Kaheer Suleman category:cs.CL cs.AI  published:2016-06-10 summary:In this paper, we propose to use deep policy networks which are trained with an advantage actor-critic method for statistically optimised dialogue systems. First, we show that, on summary state and action spaces, deep Reinforcement Learning (RL) outperforms Gaussian Processes methods. Summary state and action spaces lead to good performance but require pre-engineering effort, RL knowledge, and domain expertise. In order to remove the need to define such summary spaces, we show that deep RL can also be trained efficiently on the original state and action spaces. Dialogue systems based on partially observable Markov decision processes are known to require many dialogues to train, which makes them unappealing for practical deployment. We show that a deep RL method based on an actor-critic architecture can exploit a small amount of data very efficiently. Indeed, with only a few hundred dialogues collected with a handcrafted policy, the actor-critic deep learner is considerably bootstrapped from a combination of supervised and batch RL. In addition, convergence to an optimal policy is significantly sped up compared to other deep RL methods initialized on the data with batch RL. All experiments are performed on a restaurant domain derived from the Dialogue State Tracking Challenge 2 (DSTC2) dataset. version:1
arxiv-1606-03144 | Sentence Similarity Measures for Fine-Grained Estimation of Topical Relevance in Learner Essays | http://arxiv.org/abs/1606.03144 | id:1606.03144 author:Marek Rei, Ronan Cummins category:cs.CL cs.LG cs.NE  published:2016-06-09 summary:We investigate the task of assessing sentence-level prompt relevance in learner essays. Various systems using word overlap, neural embeddings and neural compositional models are evaluated on two datasets of learner writing. We propose a new method for sentence-level similarity calculation, which learns to adjust the weights of pre-trained word embeddings for a specific task, achieving substantially higher accuracy compared to other relevant baselines. version:1
arxiv-1606-03143 | PerSum: Novel Systems for Document Summarization in Persian | http://arxiv.org/abs/1606.03143 | id:1606.03143 author:Saeid Parvandeh, Shibamouli Lahiri, Fahimeh Boroumand category:cs.CL  published:2016-06-09 summary:In this paper we explore the problem of document summarization in Persian language from two distinct angles. In our first approach, we modify a popular and widely cited Persian document summarization framework to see how it works on a realistic corpus of news articles. Human evaluation on generated summaries shows that graph-based methods perform better than the modified systems. We carry this intuition forward in our second approach, and probe deeper into the nature of graph-based systems by designing several summarizers based on centrality measures. Ad hoc evaluation using ROUGE score on these summarizers suggests that there is a small class of centrality measures that perform better than three strong unsupervised baselines. version:1
arxiv-1606-03141 | Mutual Exclusivity Loss for Semi-Supervised Deep Learning | http://arxiv.org/abs/1606.03141 | id:1606.03141 author:Mehdi Sajjadi, Mehran Javanmardi, Tolga Tasdizen category:cs.CV cs.LG stat.ML  published:2016-06-09 summary:In this paper we consider the problem of semi-supervised learning with deep Convolutional Neural Networks (ConvNets). Semi-supervised learning is motivated on the observation that unlabeled data is cheap and can be used to improve the accuracy of classifiers. In this paper we propose an unsupervised regularization term that explicitly forces the classifier's prediction for multiple classes to be mutually-exclusive and effectively guides the decision boundary to lie on the low density space between the manifolds corresponding to different classes of data. Our proposed approach is general and can be used with any backpropagation-based learning method. We show through different experiments that our method can improve the object recognition performance of ConvNets using unlabeled data. version:1
arxiv-1406-6145 | Fast, Robust and Non-convex Subspace Recovery | http://arxiv.org/abs/1406.6145 | id:1406.6145 author:Gilad Lerman, Tyler Maunu category:cs.LG cs.CV stat.AP stat.ML  published:2014-06-24 summary:This work presents a fast and non-convex algorithm for robust subspace recovery. The data sets considered include inliers drawn around a low-dimensional subspace of a higher dimensional ambient space, and a possibly large portion of outliers that do not lie nearby this subspace. The proposed algorithm, which we refer to as Fast Median Subspace (FMS), is designed to robustly determine the underlying subspace of such data sets, while having lower computational complexity than existing methods. We prove convergence of the FMS iterates to a stationary point. Further, under a special model of data, FMS converges to a point which is near to the global minimum with overwhelming probability. Under this model, we show that the iteration complexity is globally bounded and locally $r$-linear. The latter theorem holds for any fixed fraction of outliers (less than 1) and any fixed positive distance between the limit point and the global minimum. Numerical experiments on synthetic and real data demonstrate its competitive speed and accuracy. version:2
arxiv-1604-06079 | Symmetry-aware Depth Estimation using Deep Neural Networks | http://arxiv.org/abs/1604.06079 | id:1604.06079 author:Guilin Liu, Chao Yang, Zimo Li, Duygu Ceylan, Qixing Huang category:cs.CV  published:2016-04-20 summary:Due to the abundance of 2D product images from the Internet, developing efficient and scalable algorithms to recover the missing depth information is central to many applications. Recent works have addressed the single-view depth estimation problem by utilizing convolutional neural networks. In this paper, we show that exploring symmetry information, which is ubiquitous in man made objects, can significantly boost the quality of such depth predictions. Specifically, we propose a new convolutional neural network architecture to first estimate dense symmetric correspondences in a product image and then propose an optimization which utilizes this information explicitly to significantly improve the quality of single-view depth estimations. We have evaluated our approach extensively, and experimental results show that this approach outperforms state-of-the-art depth estimation techniques. version:2
arxiv-1606-03126 | Key-Value Memory Networks for Directly Reading Documents | http://arxiv.org/abs/1606.03126 | id:1606.03126 author:Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, Jason Weston category:cs.CL  published:2016-06-09 summary:Directly reading documents and being able to answer questions from them is a key problem. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs suffer from often being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, MovieQA, a QA dataset in the domain of movies. Our method closes the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark. version:1
arxiv-1505-04771 | DopeLearning: A Computational Approach to Rap Lyrics Generation | http://arxiv.org/abs/1505.04771 | id:1505.04771 author:Eric Malmi, Pyry Takala, Hannu Toivonen, Tapani Raiko, Aristides Gionis category:cs.LG cs.AI cs.CL cs.NE I.2.7; H.3.3  published:2015-05-18 summary:Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences. version:2
arxiv-1605-07683 | Learning End-to-End Goal-Oriented Dialog | http://arxiv.org/abs/1605.07683 | id:1605.07683 author:Antoine Bordes, Jason Weston category:cs.CL  published:2016-05-24 summary:End-to-end dialog systems, in which all components are learnt simultaneously, have recently obtained encouraging successes. However these were mostly on conversations related to chit-chat with no clear objective and for which evaluation is difficult. This paper proposes a set of tasks to test the capabilities of such systems on goal-oriented dialogs, where goal completion ensures a well-defined measure of performance. Built in the context of restaurant reservation, our tasks require to manipulate sentences and symbols, in order to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). version:2
arxiv-1604-02388 | RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling | http://arxiv.org/abs/1604.02388 | id:1604.02388 author:Yang He, Wei-Chen Chiu, Margret Keuper, Mario Fritz category:cs.CV  published:2016-04-08 summary:Beyond the success in classification, neural networks have recently shown strong results on pixel-wise prediction tasks like image semantic segmentation on RGBD data. However, the commonly used deconvolutional layers for upsampling intermediate representations to the full-resolution output still show different failure modes, like imprecise segmentation boundaries and label mistakes in particular on large, weakly textured objects (e.g. fridge, whiteboard, door). We attribute these errors in part to the rigid way, current network aggregate information, that can be either too local (missing context) or too global (inaccurate boundaries). Therefore we propose a data-driven pooling layer that integrates with fully convolutional architectures and utilizes boundary detection from RGBD image segmentation approaches. We extend our approach to leverage region-level correspondences across images with an additional temporal pooling stage. We evaluate our approach on the NYU-Depth-V2 dataset comprised of indoor RGBD video sequences and compare it to various state-of-the-art baselines. Besides a general improvement over the state-of-the-art, our approach shows particularly good results in terms of accuracy of the predicted boundaries and in segmenting previously problematic classes. version:2
arxiv-1606-03077 | Efficient Robust Proper Learning of Log-concave Distributions | http://arxiv.org/abs/1606.03077 | id:1606.03077 author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.DS cs.LG math.ST stat.TH  published:2016-06-09 summary:We study the {\em robust proper learning} of univariate log-concave distributions (over continuous and discrete domains). Given a set of samples drawn from an unknown target distribution, we want to compute a log-concave hypothesis distribution that is as close as possible to the target, in total variation distance. In this work, we give the first computationally efficient algorithm for this learning problem. Our algorithm achieves the information-theoretically optimal sample size (up to a constant factor), runs in polynomial time, and is robust to model misspecification with nearly-optimal error guarantees. Specifically, we give an algorithm that, on input $n=O(1/\eps^{5/2})$ samples from an unknown distribution $f$, runs in time $\widetilde{O}(n^{8/5})$, and outputs a log-concave hypothesis $h$ that (with high probability) satisfies $\dtv(h, f) = O(\opt)+\eps$, where $\opt$ is the minimum total variation distance between $f$ and the class of log-concave distributions. Our approach to the robust proper learning problem is quite flexible and may be applicable to many other univariate distribution families. version:1
arxiv-1606-03073 | Convolutional Sketch Inversion | http://arxiv.org/abs/1606.03073 | id:1606.03073 author:Yağmur Güçlütürk, Umut Güçlü, Rob van Lier, Marcel A. J. van Gerven category:cs.CV  published:2016-06-09 summary:In this paper, we use deep neural networks for inverting face sketches to synthesize photorealistic face images. We first construct a semi-simulated dataset containing a very large number of computer-generated face sketches with different styles and corresponding face images by expanding existing unconstrained face data sets. We then train models achieving state-of-the-art results on both computer-generated sketches and hand-drawn sketches by leveraging recent advances in deep learning such as batch normalization, deep residual learning, perceptual losses and stochastic optimization in combination with our new dataset. We finally demonstrate potential applications of our models in fine arts and forensic arts. In contrast to existing patch-based approaches, our deep-neural-network-based approach can be used for synthesizing photorealistic face images by inverting face sketches in the wild. version:1
arxiv-1604-07342 | Supervised Incremental Hashing | http://arxiv.org/abs/1604.07342 | id:1604.07342 author:Bahadir Ozdemir, Mahyar Najibi, Larry S. Davis category:cs.CV  published:2016-04-25 summary:We propose an incremental strategy for learning hash functions with kernels for large-scale image search. Our method is based on a two-stage classification framework that treats binary codes as intermediate variables between the feature space and the semantic space. In the first stage of classification, binary codes are considered as class labels by a set of binary SVMs; each corresponds to one bit. In the second stage, binary codes become the input space of a multi-class SVM. Hash functions are learned by an efficient algorithm where the NP-hard problem of finding optimal binary codes is solved via cyclic coordinate descent and SVMs are trained in a parallelized incremental manner. For modifications like adding images from a previously unseen class, we describe an incremental procedure for effective and efficient updates to the previous hash functions. Experiments on three large-scale image datasets demonstrate the effectiveness of the proposed hashing method, Supervised Incremental Hashing (SIH), over the state-of-the-art supervised hashing methods. version:2
arxiv-1606-02307 | Sifting Common Information from Many Variables | http://arxiv.org/abs/1606.02307 | id:1606.02307 author:Greg Ver Steeg, Shuyang Gao, Kyle Reing, Aram Galstyan category:stat.ML cs.IT math.IT  published:2016-06-07 summary:Measuring the relationship between any two variables is a rich and active area of research at the core of the scientific enterprise. In contrast, characterizing the common information among a group of observed variables has remained a speculative undertaking producing no practical methods for high-dimensional data. A promising solution would be a multivariate generalization of the famous Wyner common information, but this approach relies on solving an apparently intractable optimization problem. We formulate an incremental version of this problem called the information sieve that not only admits a simple fixed-point solution, but also empirically exhibits an exponential rate of convergence. We use this scalable method to demonstrate that common information is a useful concept for machine learning. The sieve outperforms standard methods on dimensionality reduction tasks, solves a blind source separation problem involving Gaussian sources that cannot be solved with ICA, and accurately recovers structure in brain imaging data. version:2
arxiv-1606-03021 | Feature-based Recursive Observer Design for Homography Estimation | http://arxiv.org/abs/1606.03021 | id:1606.03021 author:Minh-Duc Hua, Jochen Trumpf, Tarek Hamel, Robert Mahony, Pascal Morin category:cs.CV  published:2016-06-09 summary:This paper presents a new algorithm for online estimation of a sequence of homographies applicable to image sequences obtained from robotic vehicles equipped with vision sensors. The approach taken exploits the underlying Special Linear group structure of the set of homographies along with gyroscope measurements and direct point-feature correspondences between images to develop temporal filter for the homography estimate. Theoretical analysis and experimental results are provided to demonstrate the robustness of the proposed algorithm. The experimental results show excellent performance even in the case of very fast camera motion (relative to frame rate), severe occlusion, and in the presence of specular reflections. version:1
arxiv-1606-02492 | Convolutional Neural Fabrics | http://arxiv.org/abs/1606.02492 | id:1606.02492 author:Shreyas Saxena, Jakob Verbeek category:cs.CV cs.LG cs.NE  published:2016-06-08 summary:Despite the success of convolutional neural networks, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a $"$fabric$"$ that embeds an exponentially large number of CNN architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of the model (nr. of channels and layers) are not critical for performance. While individual CNN architectures can be recovered as paths in the trellis, the trellis can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. The trellis parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset. version:2
arxiv-1606-03014 | Implicit Tubular Surface Generation Guided by Centerline | http://arxiv.org/abs/1606.03014 | id:1606.03014 author:Haoyin Zhou, James K. Min, Guanglei Xiong category:cs.CV  published:2016-06-09 summary:Most machine learning-based coronary artery segmentation methods represent the vascular lumen surface in an implicit way by the centerline and the associated lumen radii, which makes the subsequent modeling process to generate a whole piece of watertight coronary artery tree model difficult. To solve this problem, in this paper, we propose a modeling method with the learning-based segmentation results by (1) considering mesh vertices as physical particles and using interaction force model and particle expansion model to generate uniformly distributed point cloud on the implicit lumen surface and; (2) doing incremental Delaunay-based triangulation. Our method has the advantage of being able to consider the complex shape of the coronary artery tree as a whole piece; hence no extra stitching or intersection removal algorithm is needed to generate a watertight model. Experiment results demonstrate that our method is capable of generating high quality mesh model which is highly consistent with the given implicit vascular lumen surface, with an average error of 0.08 mm. version:1
arxiv-1602-02355 | Hyperparameter optimization with approximate gradient | http://arxiv.org/abs/1602.02355 | id:1602.02355 author:Fabian Pedregosa category:stat.ML cs.LG math.OC  published:2016-02-07 summary:Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods. version:4
arxiv-1606-03002 | MuFuRU: The Multi-Function Recurrent Unit | http://arxiv.org/abs/1606.03002 | id:1606.03002 author:Dirk Weissenborn, Tim Rocktäschel category:cs.NE cs.AI cs.CL  published:2016-06-09 summary:Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-of-the-art results for many tasks. These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. However, they only cover a small subset of potentially useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that allow for arbitrary differentiable functions as composition operations. Furthermore, MuFuRUs allow for an input- and state-dependent choice of these composition operations that is learned. Our experiments demonstrate that the additional functionality helps in different sequence modeling tasks, including the evaluation of propositional logic formulae, language modeling and sentiment analysis. version:1
arxiv-1606-03000 | On Projected Stochastic Gradient Descent Algorithm with Weighted Averaging for Least Squares Regression | http://arxiv.org/abs/1606.03000 | id:1606.03000 author:Kobi Cohen, Angelia Nedic, R. Srikant category:cs.IT cs.LG math.IT  published:2016-06-09 summary:The problem of least squares regression of a $d$-dimensional unknown parameter is considered. A stochastic gradient descent based algorithm with weighted iterate-averaging that uses a single pass over the data is studied and its convergence rate is analyzed. We first consider a bounded constraint set of the unknown parameter. Under some standard regularity assumptions, we provide an explicit $O(1/k)$ upper bound on the convergence rate, depending on the variance (due to the additive noise in the measurements) and the size of the constraint set. We show that the variance term dominates the error and decreases with rate $1/k$, while the term which is related to the size of the constraint set decreases with rate $\log k/k^2$. We then compare the asymptotic ratio $\rho$ between the convergence rate of the proposed scheme and the empirical risk minimizer (ERM) as the number of iterations approaches infinity. We show that $\rho\leq 4$ under some mild conditions for all $d\geq 1$. We further improve the upper bound by showing that $\rho\leq 4/3$ for the case of $d=1$ and unbounded parameter set. Simulation results demonstrate strong performance of the algorithm as compared to existing methods, and coincide with $\rho\leq 4/3$ even for large $d$ in practice. version:1
arxiv-1603-00957 | Question Answering on Freebase via Relation Extraction and Textual Evidence | http://arxiv.org/abs/1603.00957 | id:1603.00957 author:Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang, Dongyan Zhao category:cs.CL  published:2016-03-03 summary:Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a neural network based relation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Experiments on the WebQuestions question answering dataset show that our method achieves an F_1 of 53.3%, a substantial improvement over the state-of-the-art. version:3
arxiv-1606-02979 | Generative Topic Embedding: a Continuous Representation of Documents (Extended Version with Proofs) | http://arxiv.org/abs/1606.02979 | id:1606.02979 author:Shaohua Li, Tat-Seng Chua, Jun Zhu, Chunyan Miao category:cs.CL cs.AI cs.IR cs.LG stat.ML  published:2016-06-09 summary:Word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document. version:1
arxiv-1606-02976 | Large scale biomedical texts classification: a kNN and an ESA-based approaches | http://arxiv.org/abs/1606.02976 | id:1606.02976 author:Khadim Dramé, Fleur Mougin, Gayo Diallo category:cs.IR cs.CL  published:2016-06-09 summary:With the large and increasing volume of textual data, automated methods for identifying significant topics to classify textual documents have received a growing interest. While many efforts have been made in this direction, it still remains a real challenge. Moreover, the issue is even more complex as full texts are not always freely available. Then, using only partial information to annotate these documents is promising but remains a very ambitious issue. MethodsWe propose two classification methods: a k-nearest neighbours (kNN)-based approach and an explicit semantic analysis (ESA)-based approach. Although the kNN-based approach is widely used in text classification, it needs to be improved to perform well in this specific classification problem which deals with partial information. Compared to existing kNN-based methods, our method uses classical Machine Learning (ML) algorithms for ranking the labels. Additional features are also investigated in order to improve the classifiers' performance. In addition, the combination of several learning algorithms with various techniques for fixing the number of relevant topics is performed. On the other hand, ESA seems promising for this classification task as it yielded interesting results in related issues, such as semantic relatedness computation between texts and text classification. Unlike existing works, which use ESA for enriching the bag-of-words approach with additional knowledge-based features, our ESA-based method builds a standalone classifier. Furthermore, we investigate if the results of this method could be useful as a complementary feature of our kNN-based approach.ResultsExperimental evaluations performed on large standard annotated datasets, provided by the BioASQ organizers, show that the kNN-based method with the Random Forest learning algorithm achieves good performances compared with the current state-of-the-art methods, reaching a competitive f-measure of 0.55% while the ESA-based approach surprisingly yielded reserved results.ConclusionsWe have proposed simple classification methods suitable to annotate textual documents using only partial information. They are therefore adequate for large multi-label classification and particularly in the biomedical domain. Thus, our work contributes to the extraction of relevant information from unstructured documents in order to facilitate their automated processing. Consequently, it could be used for various purposes, including document indexing, information retrieval, etc. version:1
arxiv-1603-06812 | Con-Patch: When a Patch Meets its Context | http://arxiv.org/abs/1603.06812 | id:1603.06812 author:Yaniv Romano, Michael Elad category:cs.CV  published:2016-03-22 summary:Measuring the similarity between patches in images is a fundamental building block in various tasks. Naturally, the patch-size has a major impact on the matching quality, and on the consequent application performance. Under the assumption that our patch database is sufficiently sampled, using large patches (e.g. 21-by-21) should be preferred over small ones (e.g. 7-by-7). However, this "dense-sampling" assumption is rarely true; in most cases large patches cannot find relevant nearby examples. This phenomenon is a consequence of the curse of dimensionality, stating that the database-size should grow exponentially with the patch-size to ensure proper matches. This explains the favored choice of small patch-size in most applications. Is there a way to keep the simplicity and work with small patches while getting some of the benefits that large patches provide? In this work we offer such an approach. We propose to concatenate the regular content of a conventional (small) patch with a compact representation of its (large) surroundings - its context. Therefore, with a minor increase of the dimensions (e.g. with additional 10 values to the patch representation), we implicitly/softly describe the information of a large patch. The additional descriptors are computed based on a self-similarity behavior of the patch surrounding. We show that this approach achieves better matches, compared to the use of conventional-size patches, without the need to increase the database-size. Also, the effectiveness of the proposed method is tested on three distinct problems: (i) External natural image denoising, (ii) Depth image super-resolution, and (iii) Motion-compensated frame-rate up-conversion. version:3
arxiv-1603-02518 | A New Method to Visualize Deep Neural Networks | http://arxiv.org/abs/1603.02518 | id:1603.02518 author:Luisa M. Zintgraf, Taco S. Cohen, Max Welling category:cs.CV  published:2016-03-08 summary:We present a method for visualising the response of a deep neural network to a specific input. For image data for instance our method will highlight areas that provide evidence in favor of, and against choosing a certain class. The method overcomes several shortcomings of previous methods and provides great additional insight into the decision making process of convolutional networks, which is important both to improve models and to accelerate the adoption of such methods in e.g. medicine. In experiments on ImageNet data, we illustrate how the method works and can be applied in different ways to understand deep neural nets. version:2
arxiv-1606-02960 | Sequence-to-Sequence Learning as Beam-Search Optimization | http://arxiv.org/abs/1606.02960 | id:1606.02960 author:Sam Wiseman, Alexander M. Rush category:cs.CL cs.LG cs.NE stat.ML  published:2016-06-09 summary:Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation. version:1
arxiv-1606-02909 | Apparent Age Estimation Using Ensemble of Deep Learning Models | http://arxiv.org/abs/1606.02909 | id:1606.02909 author:Refik Can Malli, Mehmet Aygun, Hazim Kemal Ekenel category:cs.CV  published:2016-06-09 summary:In this paper, we address the problem of apparent age estimation. Different from estimating the real age of individuals, in which each face image has a single age label, in this problem, face images have multiple age labels, corresponding to the ages perceived by the annotators, when they look at these images. This provides an intriguing computer vision problem, since in generic image or object classification tasks, it is typical to have a single ground truth label per class. To account for multiple labels per image, instead of using average age of the annotated face image as the class label, we have grouped the face images that are within a specified age range. Using these age groups and their age-shifted groupings, we have trained an ensemble of deep learning models. Before feeding an input face image to a deep learning model, five facial landmark points are detected and used for 2-D alignment. We have employed and fine tuned convolutional neural networks (CNNs) that are based on VGG-16 [24] architecture and pretrained on the IMDB-WIKI dataset [22]. The outputs of these deep learning models are then combined to produce the final estimation. Proposed method achieves 0.3668 error in the final ChaLearn LAP 2016 challenge test set [5]. version:1
arxiv-1606-02894 | A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition | http://arxiv.org/abs/1606.02894 | id:1606.02894 author:Mostafa Mehdipour Ghazi, Hazim Kemal Ekenel category:cs.CV  published:2016-06-09 summary:Deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets. These approaches have been extensively tested on such unconstrained datasets, on the Labeled Faces in the Wild and YouTube Faces, to name a few. However, their capability to handle individual appearance variations caused by factors such as head pose, illumination, occlusion, and misalignment has not been thoroughly assessed till now. In this paper, we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localization. Two successful and publicly available deep learning models, namely VGG-Face and Lightened CNN have been utilized to extract face representations. The obtained results show that although deep learning provides a powerful representation for face recognition, it can still benefit from preprocessing, for example, for pose and illumination normalization in order to achieve better performance under various conditions. Particularly, if these variations are not included in the dataset used to train the deep learning model, the role of preprocessing becomes more crucial. Experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10% of the interocular distance. version:1
arxiv-1606-02892 | Linguistic Input Features Improve Neural Machine Translation | http://arxiv.org/abs/1606.02892 | id:1606.02892 author:Rico Sennrich, Barry Haddow category:cs.CL  published:2016-06-09 summary:Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder--decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to English<->German neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. version:1
arxiv-1606-02891 | Edinburgh Neural Machine Translation Systems for WMT 16 | http://arxiv.org/abs/1606.02891 | id:1606.02891 author:Rico Sennrich, Barry Haddow, Alexandra Birch category:cs.CL  published:2016-06-09 summary:We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English<->Czech, English<->German, English<->Romanian and English<->Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. version:1
arxiv-1605-06914 | Embedding based on function approximation for large scale image search | http://arxiv.org/abs/1605.06914 | id:1605.06914 author:Thanh-Toan Do, Ngai-Man Cheung category:cs.CV  published:2016-05-23 summary:The objective of this paper is to design an embedding method that maps local features describing an image (e.g. SIFT) to a higher dimensional representation useful for the image retrieval problem. First, motivated by the relationship between the linear approximation of a nonlinear function in high dimensional space and the state of-the-art feature representation used in image retrieval, i.e., VLAD, we propose a new approach for the approximation. The embedded vectors resulted by the function approximation process are then aggregated to form a single representation for image retrieval. Second, in order to make the proposed embedding method applicable to large scale problem, we further derive its fast version in which the embedded vectors can be efficiently computed, i.e., in the closed-form. We compare the proposed embedding methods with the state of the art in the context of image search under various settings: when the images are represented by medium length vectors, short vectors, or binary vectors. The experimental results show that the proposed embedding methods outperform existing the state of the art on the standard public image retrieval benchmarks. version:2
arxiv-1606-02861 | Simultaneous Inpainting and Denoising by Directional Global Three-part Decomposition: Connecting Variational and Fourier Domain Based Image Processing | http://arxiv.org/abs/1606.02861 | id:1606.02861 author:Duy Hoang Thai, Carsten Gottschlich category:cs.CV  published:2016-06-09 summary:We consider the very challenging task of restoring images (i) which have a large number of missing pixels, (ii) whose existing pixels are corrupted by noise and (iii) the ideal image to be restored contains both cartoon and texture elements. The combination of these three properties makes this inverse problem a very difficult one. The solution proposed in this manuscript is based on directional global three-part decomposition (DG3PD) [ThaiGottschlich2016] with directional total variation norm, directional G-norm and $\ell_\infty$-norm in curvelet domain as key ingredients of the model. Image decomposition by DG3PD enables a decoupled inpainting and denoising of the cartoon and texture components. A comparison to existing approaches for inpainting and denoising shows the advantages of the proposed method. Moreover, we regard the image restoration problem from the viewpoint of a Bayesian framework and we discuss the connections between the proposed solution by function space and related image representation by harmonic analysis and pyramid decomposition. version:1
arxiv-1606-02858 | A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task | http://arxiv.org/abs/1606.02858 | id:1606.02858 author:Danqi Chen, Jason Bolton, Christopher D. Manning category:cs.CL cs.AI  published:2016-06-09 summary:Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task. version:1
arxiv-1606-02854 | e-Commerce product classification: our participation at cDiscount 2015 challenge | http://arxiv.org/abs/1606.02854 | id:1606.02854 author:Ioannis Partalas, Georgios Balikas category:cs.LG cs.AI  published:2016-06-09 summary:This report describes our participation in the cDiscount 2015 challenge where the goal was to classify product items in a predefined taxonomy of products. Our best submission yielded an accuracy score of 64.20\% in the private part of the leaderboard and we were ranked 10th out of 175 participating teams. We followed a text classification approach employing mainly linear models. The final solution was a weighted voting system which combined a variety of trained models. version:1
arxiv-1510-00921 | Cross-convolutional-layer Pooling for Image Recognition | http://arxiv.org/abs/1510.00921 | id:1510.00921 author:Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2015-10-04 summary:Recent studies have shown that a Deep Convolutional Neural Network (DCNN) pretrained on a large image dataset can be used as a universal image descriptor, and that doing so leads to impressive performance for a variety of image classification tasks. Most of these studies adopt activations from a single DCNN layer, usually the fully-connected layer, as the image representation. In this paper, we proposed a novel way to extract image representations from two consecutive convolutional layers: one layer is utilized for local feature extraction and the other serves as guidance to pool the extracted features. By taking different viewpoints of convolutional layers, we further develop two schemes to realize this idea. The first one directly uses convolutional layers from a DCNN. The second one applies the pretrained CNN on densely sampled image regions and treats the fully-connected activations of each image region as convolutional feature activations. We then train another convolutional layer on top of that as the pooling-guidance convolutional layer. By applying our method to three popular visual classification tasks, we find our first scheme tends to perform better on the applications which need strong discrimination on subtle object patterns within small regions while the latter excels in the cases that require discrimination on category-level patterns. Overall, the proposed method achieves superior performance over existing ways of extracting image representations from a DCNN. version:2
arxiv-1606-00487 | Recurrent Fully Convolutional Networks for Video Segmentation | http://arxiv.org/abs/1606.00487 | id:1606.00487 author:Sepehr Valipour, Mennatullah Siam, Martin Jagersand, Nilanjan Ray category:cs.CV  published:2016-06-01 summary:Image segmentation is an important step in most visual tasks. While convolutional neural networks have shown to perform well on single image segmentation, to our knowledge, no study has been been done on leveraging recurrent gated architectures for video segmentation. Accordingly, we propose a novel method for online segmentation of video sequences that incorporates temporal data. The network is built from fully convolutional element and recurrent unit that works on a sliding window over the temporal data. We also introduce a novel convolutional gated recurrent unit that preserves the spatial information and reduces the parameters learned. Our method has the advantage that it can work in an online fashion instead of operating over the whole input batch of video frames. The network is tested on the change detection dataset, and proved to have 5.5\% improvement in F-measure over a plain fully convolutional network for per frame segmentation. It was also shown to have improvement of 1.4\% for the F-measure compared to our baseline network that we call FCN 12s. version:2
arxiv-1606-02838 | Sketching for Large-Scale Learning of Mixture Models | http://arxiv.org/abs/1606.02838 | id:1606.02838 author:Nicolas Keriven, Anthony Bourrier, Rémi Gribonval, Patrick Pérez category:cs.LG stat.ML  published:2016-06-09 summary:Learning parameters from voluminous data can be prohibitive in terms of memory and computational requirements. We propose a "compressive learning" framework where we estimate model parameters from a sketch of the training data. This sketch is a collection of generalized moments of the underlying probability distribution of the data. It can be computed in a single pass on the training set, and is easily computable on streams or distributed datasets. The proposed framework shares similarities with compressive sensing, which aims at drastically reducing the dimension of high-dimensional signals while preserving the ability to reconstruct them. To perform the estimation task, we derive an iterative algorithm analogous to sparse reconstruction algorithms in the context of linear inverse problems. We exemplify our framework with the compressive estimation of a Gaussian Mixture Model (GMM), providing heuristics on the choice of the sketching procedure and theoretical guarantees of reconstruction. We experimentally show on synthetic data that the proposed algorithm yields results comparable to the classical Expectation-Maximization (EM) technique while requiring significantly less memory and fewer computations when the number of database elements is large. We further demonstrate the potential of the approach on real large-scale data (over 10 8 training samples) for the task of model-based speaker verification. Finally, we draw some connections between the proposed framework and approximate Hilbert space embedding of probability distributions using random features. We show that the proposed sketching operator can be seen as an innovative method to design translation-invariant kernels adapted to the analysis of GMMs. We also use this theoretical framework to derive information preservation guarantees, in the spirit of infinite-dimensional compressive sensing. version:1
arxiv-1602-02706 | Decoy Bandits Dueling on a Poset | http://arxiv.org/abs/1602.02706 | id:1602.02706 author:Julien Audiffren, Ralaivola Liva category:cs.LG cs.AI  published:2016-02-08 summary:We adress the problem of dueling bandits defined on partially ordered sets, or posets. In this setting, arms may not be comparable, and there may be several (incomparable) optimal arms. We propose an algorithm, UnchainedBandits, that efficiently finds the set of optimal arms of any poset even when pairs of comparable arms cannot be distinguished from pairs of incomparable arms, with a set of minimal assumptions. This algorithm relies on the concept of decoys, which stems from social psychology. For the easier case where the incomparability information may be accessible, we propose a second algorithm, SlicingBandits, which takes advantage of this information and achieves a very significant gain of performance compared to UnchainedBandits. We provide theoretical guarantees and experimental evaluation for both algorithms. version:2
arxiv-1604-08504 | Detecting "Smart" Spammers On Social Network: A Topic Model Approach | http://arxiv.org/abs/1604.08504 | id:1604.08504 author:Linqing Liu, Yao Lu, Ye Luo, Renxian Zhang, Laurent Itti, Jianwei Lu category:cs.CL cs.SI  published:2016-04-28 summary:Spammer detection on social network is a challenging problem. The rigid anti-spam rules have resulted in emergence of "smart" spammers. They resemble legitimate users who are difficult to identify. In this paper, we present a novel spammer classification approach based on Latent Dirichlet Allocation(LDA), a topic model. Our approach extracts both the local and the global information of topic distribution patterns, which capture the essence of spamming. Tested on one benchmark dataset and one self-collected dataset, our proposed method outperforms other state-of-the-art methods in terms of averaged F1-score. version:2
arxiv-1511-06728 | Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning | http://arxiv.org/abs/1511.06728 | id:1511.06728 author:Natalia Neverova, Christian Wolf, Florian Nebout, Graham Taylor category:cs.CV cs.AI cs.LG  published:2015-11-20 summary:We propose a method for hand pose estimation based on a deep regressor trained on two different kinds of input. Raw depth data is fused with an intermediate representation in the form of a segmentation of the hand into parts. This intermediate representation contains important topological information and provides useful cues for reasoning about joint locations. The mapping from raw depth to segmentation maps is learned in a semi/weakly-supervised way from two different datasets: (i) a synthetic dataset created through a rendering pipeline including densely labeled ground truth (pixelwise segmentations); and (ii) a dataset with real images for which ground truth joint positions are available, but not dense segmentations. Loss for training on real images is generated from a patch-wise restoration process, which aligns tentative segmentation maps with a large dictionary of synthetic poses. The underlying premise is that the domain shift between synthetic and real data is smaller in the intermediate representation, where labels carry geometric and topological meaning, than in the raw input domain. Experiments on the NYU dataset show that the proposed training method decreases error on joints over direct regression of joints from depth data by 15.7%. version:3
arxiv-1606-02827 | Variational Information Maximization for Feature Selection | http://arxiv.org/abs/1606.02827 | id:1606.02827 author:Shuyang Gao, Greg Ver Steeg, Aram Galstyan category:stat.ML cs.LG  published:2016-06-09 summary:Feature selection is one of the most fundamental problems in machine learning. An extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels. Practical methods are forced to rely on approximations due to the difficulty of estimating mutual information. We demonstrate that approximations made by existing methods are based on unrealistic assumptions. We formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information. These bounds define a novel information-theoretic framework for feature selection, which we prove to be optimal under tree graphical models with proper choice of variational distributions. Our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches. version:1
arxiv-1606-02821 | Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change | http://arxiv.org/abs/1606.02821 | id:1606.02821 author:William L. Hamilton, Jure Leskovec, Dan Jurafsky category:cs.CL  published:2016-06-09 summary:Words shift in meaning for many reasons, including cultural factors like new technologies and regular linguistic processes like subjectification. Understanding the evolution of language and culture requires disentangling these underlying causes. Here we show how two different distributional measures can be used to detect two different types of semantic change. The first measure, which has been used in many previous works, analyzes global shifts in a word's distributional semantics; it is sensitive to changes due to regular processes of linguistic drift, such as the semantic generalization of promise ("I promise." -> "It promised to be exciting."). The second measure, which we develop here, focuses on local changes to a word's nearest semantic neighbors; it is more sensitive to cultural shifts, such as the change in the meaning of cell ("prison cell" -> "cell phone"). Comparing measurements made by these two methods allows researchers to determine whether changes are more cultural or linguistic in nature, a distinction that is essential for work in the digital humanities and historical linguistics. version:1
arxiv-1606-02820 | Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora | http://arxiv.org/abs/1606.02820 | id:1606.02820 author:William L. Hamilton, Kevin Clark, Jure Leskovec, Dan Jurafsky category:cs.CL  published:2016-06-09 summary:A word's sentiment depends on the domain in which it is used. Computational social science research thus requires sentiment lexicons that are specific to the domains being studied. We combine domain-specific word embeddings with a label propagation framework to induce accurate domain-specific sentiment lexicons using small sets of seed words. We show that our approach achieves state-of-the-art performance on inducing sentiment lexicons from domain-specific corpora and that our purely corpus-based approach outperforms methods that rely on hand-curated resources (e.g., WordNet). Using our framework, we induce and release historical sentiment lexicons for 150 years of English and community-specific sentiment lexicons for 250 online communities from the social media forum Reddit. The historical lexicons we induce show that more than 5% of sentiment-bearing (non-neutral) English words completely switched polarity during the last 150 years, and the community-specific lexicons highlight how sentiment varies drastically between different communities. version:1
arxiv-1606-02819 | Low-shot visual object recognition | http://arxiv.org/abs/1606.02819 | id:1606.02819 author:Bharath Hariharan, Ross Girshick category:cs.CV  published:2016-06-09 summary:Low-shot visual learning - the ability to recognize novel object categories from very few examples - is a hallmark of human visual intelligence. Existing machine learning approaches fail to generalize in the same way. To make progress on this foundational problem, we present a novel protocol to evaluate low-shot learning on complex images where the learner is permitted to first build a feature representation. Then, we propose and evaluate representation regularization techniques that improve the effectiveness of convolutional networks at the task of low-shot learning, leading to a 2x reduction in the amount of training data required at equal accuracy rates on the challenging ImageNet dataset. version:1
arxiv-1506-02690 | Adaptive Normalized Risk-Averting Training For Deep Neural Networks | http://arxiv.org/abs/1506.02690 | id:1506.02690 author:Zhiguang Wang, Tim Oates, James Lo category:cs.LG cs.NE stat.ML  published:2015-06-08 summary:This paper proposes a set of new error criteria and learning approaches, Adaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex optimization problem in training deep neural networks (DNNs). Theoretically, we demonstrate its effectiveness on global and local convexity lower-bounded by the standard $L_p$-norm error. By analyzing the gradient on the convexity index $\lambda$, we explain the reason why to learn $\lambda$ adaptively using gradient descent works. In practice, we show how this method improves training of deep neural networks to solve visual recognition tasks on the MNIST and CIFAR-10 datasets. Without using pretraining or other tricks, we obtain results comparable or superior to those reported in recent literature on the same tasks using standard ConvNets + MSE/cross entropy. Performance on deep/shallow multilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can be combined with other quasi-Newton training methods, innovative network variants, regularization techniques and other specific tricks in DNNs. Other than unsupervised pretraining, it provides a new perspective to address the non-convex optimization problem in DNNs. version:3
arxiv-1606-02811 | Machine Learning Techniques and Applications For Ground-based Image Analysis | http://arxiv.org/abs/1606.02811 | id:1606.02811 author:Soumyabrata Dev, Bihan Wen, Yee Hui Lee, Stefan Winkler category:cs.CV  published:2016-06-09 summary:Ground-based whole sky cameras have opened up new opportunities for monitoring the earth's atmosphere. These cameras are an important complement to satellite images by providing geoscientists with cheaper, faster, and more localized data. The images captured by whole sky imagers can have high spatial and temporal resolution, which is an important pre-requisite for applications such as solar energy modeling, cloud attenuation analysis, local weather prediction, etc. Extracting valuable information from the huge amount of image data by detecting and analyzing the various entities in these images is challenging. However, powerful machine learning techniques have become available to aid with the image analysis. This article provides a detailed walk-through of recent developments in these techniques and their applications in ground-based imaging. We aim to bridge the gap between computer vision and remote sensing with the help of illustrative examples. We demonstrate the advantages of using machine learning techniques in ground-based image analysis via three primary applications -- segmentation, classification, and denoising. version:1
arxiv-1603-05953 | Katyusha: The First Direct Acceleration of Stochastic Gradient Methods | http://arxiv.org/abs/1603.05953 | id:1603.05953 author:Zeyuan Allen-Zhu category:math.OC cs.DS stat.ML  published:2016-03-18 summary:We introduce $\mathtt{Katyusha}$, the first direct stochastic gradient method that has an accelerated convergence rate. Given an objective that is an average of $n$ convex and smooth functions, $\mathtt{Katyusha}$ converges to an $\varepsilon$-approximate minimizer using $O((n + \sqrt{n \kappa})\cdot \log\frac{f(x_0)-f(x^*)}{\varepsilon})$ stochastic iterations, where $\kappa$ is the condition number. $\mathtt{Katyusha}$ is a direct primal method. In contrast, previous accelerated stochastic methods are either based on dual coordinate descent which are more restrictive, or based on outer-inner loops which make them "blind" to the underlying stochastic nature of the optimization process. $\mathtt{Katyusha}$ is the first algorithm that incorporates acceleration directly into the stochastic gradient updates. $\mathtt{Katyusha}$ supports proximal updates, non-Euclidean norm smoothness, non-uniform sampling, as well as mini-batch sampling. It also improves the best known convergence rates on many interesting classes of convex objectives, including smooth objectives (e.g., Lasso, Logistic Regression), strongly-convex objectives (e.g., SVM), and non-smooth objectives (e.g., L1SVM). The main ingredient behind our result is Katyusha momentum, a clever "negative momentum on top of momentum" that can be added on top of a variance-reduction based algorithm and speed it up. As a result, since variance reduction has been successfully applied to a fast growing list of practical problems, our paper suggests that in each of such cases, one had better hurry up and give Katyusha a hug. version:3
arxiv-1603-06059 | Generating Natural Questions About an Image | http://arxiv.org/abs/1603.06059 | id:1603.06059 author:Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, Lucy Vanderwende category:cs.CL cs.AI cs.CV  published:2016-03-19 summary:There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription, and answering questions about images. These tasks have focused on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image. In this paper, we introduce the novel task of Visual Question Generation (VQG), where the system is tasked with asking a natural and engaging question when shown an image. We provide three datasets which cover a variety of images from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art captioning systems thus far. We train and test several generative and retrieval models to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions for a variety of images, there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language. version:3
arxiv-1606-02792 | Spontaneous Subtle Expression Detection and Recognition based on Facial Strain | http://arxiv.org/abs/1606.02792 | id:1606.02792 author:Sze-Teng Liong, John See, Raphael Chung-Wei Phan, Yee-Hui Oh, Anh Cat Le Ngo, KokSheik Wong, Su-Wei Tan category:cs.CV  published:2016-06-09 summary:Optical strain is an extension of optical flow that is capable of quantifying subtle changes on faces and representing the minute facial motion intensities at the pixel level. This is computationally essential for the relatively new field of spontaneous micro-expression, where subtle expressions can be technically challenging to pinpoint. In this paper, we present a novel method for detecting and recognizing micro-expressions by utilizing facial optical strain magnitudes to construct optical strain features and optical strain weighted features. The two sets of features are then concatenated to form the resultant feature histogram. Experiments were performed on the CASME II and SMIC databases. We demonstrate on both databases, the usefulness of optical strain information and more importantly, that our best approaches are able to outperform the original baseline results for both detection and recognition tasks. A comparison of the proposed method with other existing spatio-temporal feature extraction approaches is also presented. version:1
arxiv-1605-07157 | Unsupervised Learning for Physical Interaction through Video Prediction | http://arxiv.org/abs/1605.07157 | id:1605.07157 author:Chelsea Finn, Ian Goodfellow, Sergey Levine category:cs.LG cs.AI cs.CV cs.RO  published:2016-05-23 summary:A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 50,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method not only produces more accurate video predictions, but also more accurately predicts object motion, when compared to prior methods. version:3
arxiv-1606-02785 | Neural Network-Based Abstract Generation for Opinions and Arguments | http://arxiv.org/abs/1606.02785 | id:1606.02785 author:Lu Wang, Wang Ling category:cs.CL  published:2016-06-09 summary:We study the problem of generating abstractive summaries for opinionated text. We propose an attention-based neural network model that is able to absorb information from multiple text units to construct informative, concise, and fluent summaries. An importance-based sampling method is designed to allow the encoder to integrate information from an important subset of input. Automatic evaluation indicates that our system outperforms state-of-the-art abstractive and extractive summarization systems on two newly collected datasets of movie reviews and arguments. Our system summaries are also rated as more informative and grammatical in human evaluation. version:1
arxiv-1507-02284 | The Information Sieve | http://arxiv.org/abs/1507.02284 | id:1507.02284 author:Greg Ver Steeg, Aram Galstyan category:stat.ML cs.IT cs.LG math.IT  published:2015-07-08 summary:We introduce a new framework for unsupervised learning of representations based on a novel hierarchical decomposition of information. Intuitively, data is passed through a series of progressively fine-grained sieves. Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data. The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer. Ultimately, we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise. We present a practical implementation of this framework for discrete variables and apply it to a variety of fundamental tasks in unsupervised learning including independent component analysis, lossy and lossless compression, and predicting missing values in data. version:3
arxiv-1603-07294 | On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis | http://arxiv.org/abs/1603.07294 | id:1603.07294 author:James Foulds, Joseph Geumlek, Max Welling, Kamalika Chaudhuri category:cs.LG cs.AI cs.CR stat.ML  published:2016-03-23 summary:Bayesian inference has great promise for the privacy-preserving analysis of sensitive data, as posterior sampling automatically preserves differential privacy, an algorithmic notion of data privacy, under certain conditions (Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample (OPS) approach elegantly provides privacy "for free," it is data inefficient in the sense of asymptotic relative efficiency (ARE). We show that a simple alternative based on the Laplace mechanism, the workhorse of differential privacy, is as asymptotically efficient as non-private posterior inference, under general assumptions. This technique also has practical advantages including efficient use of the privacy budget for MCMC. We demonstrate the practicality of our approach on a time-series analysis of sensitive military records from the Afghanistan and Iraq wars disclosed by the Wikileaks organization. version:2
arxiv-1606-01323 | Improving Coreference Resolution by Learning Entity-Level Distributed Representations | http://arxiv.org/abs/1606.01323 | id:1606.01323 author:Kevin Clark, Christopher D. Manning category:cs.CL  published:2016-06-04 summary:A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features. version:2
arxiv-1605-02029 | Shaping the Future through Innovations: From Medical Imaging to Precision Medicine | http://arxiv.org/abs/1605.02029 | id:1605.02029 author:Dorin Comaniciu, Klaus Engel, Bogdan Georgescu, Tommaso Mansi category:cs.CV cs.CE  published:2016-05-01 summary:Medical images constitute a source of information essential for disease diagnosis, treatment and follow-up. In addition, due to its patient-specific nature, imaging information represents a critical component required for advancing precision medicine into clinical practice. This manuscript describes recently developed technologies for better handling of image information: photorealistic visualization of medical images with Cinematic Rendering, artificial agents for in-depth image understanding, support for minimally invasive procedures, and patient-specific computational models with enhanced predictive power. Throughout the manuscript we will analyze the capabilities of such technologies and extrapolate on their potential impact to advance the quality of medical care, while reducing its cost. version:2
arxiv-1606-02753 | Rotation Invariant Angular Descriptor Via A Bandlimited Gaussian-like Kernel | http://arxiv.org/abs/1606.02753 | id:1606.02753 author:Michael T. McCann, Matthew Fickus, Jelena Kovacevic category:cs.CV  published:2016-06-08 summary:We present a new smooth, Gaussian-like kernel that allows the kernel density estimate for an angular distribution to be exactly represented by a finite number of its Fourier series coefficients. Distributions of angular quantities, such as gradients, are a central part of several state-of-the-art image processing algorithms, but these distributions are usually described via histograms and therefore lack rotation invariance due to binning artifacts. Replacing histograming with kernel density estimation removes these binning artifacts and can provide a finite-dimensional descriptor of the distribution, provided that the kernel is selected to be bandlimited. In this paper, we present a new band-limited kernel that has the added advantage of being Gaussian-like in the angular domain. We then show that it compares favorably to gradient histograms for patch matching, person detection, and texture segmentation. version:1
arxiv-1606-02718 | Learning Thermodynamics with Boltzmann Machines | http://arxiv.org/abs/1606.02718 | id:1606.02718 author:Giacomo Torlai, Roger G. Melko category:cond-mat.stat-mech cond-mat.dis-nn cs.LG  published:2016-06-08 summary:A Boltzmann machine is a stochastic neural network that has been extensively used in the layers of deep architectures for modern machine learning applications. In this paper, we develop a Boltzmann machine that is capable of modelling thermodynamic observables for physical systems in thermal equilibrium. Through unsupervised learning, we train the Boltzmann machine on data sets constructed with spin configurations importance-sampled from the partition function of an Ising Hamiltonian at different temperatures using Monte Carlo (MC) methods. The trained Boltzmann machine is then used to generate spin states, for which we compare thermodynamic observables to those computed by direct MC sampling. We demonstrate that the Boltzmann machine can faithfully reproduce the observables of the physical system. Further, we observe that the number of neurons required to obtain accurate results increases as the system is brought close to criticality. version:1
arxiv-1606-02702 | Efficient Smoothed Concomitant Lasso Estimation for High Dimensional Regression | http://arxiv.org/abs/1606.02702 | id:1606.02702 author:Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Vincent Leclère, Joseph Salmon category:stat.ML cs.LG math.OC 62J05  90C25  90C06  published:2016-06-08 summary:In high dimensional settings, sparse structures are crucial for efficiency, both in term of memory, computation and performance. It is customary to consider $\ell_1$ penalty to enforce sparsity in such scenarios. Sparsity enforcing methods, the Lasso being a canonical example, are popular candidates to address high dimension. For efficiency, they rely on tuning a parameter trading data fitting versus sparsity. For the Lasso theory to hold this tuning parameter should be proportional to the noise level, yet the latter is often unknown in practice. A possible remedy is to jointly optimize over the regression parameter as well as over the noise level. This has been considered under several names in the literature: Scaled-Lasso, Square-root Lasso, Concomitant Lasso estimation for instance, and could be of interest for confidence sets or uncertainty quantification. In this work, after illustrating numerical difficulties for the Smoothed Concomitant Lasso formulation, we propose a modification we coined Smoothed Concomitant Lasso, aimed at increasing numerical stability. We propose an efficient and accurate solver leading to a computational cost no more expansive than the one for the Lasso. We leverage on standard ingredients behind the success of fast Lasso solvers: a coordinate descent algorithm, combined with safe screening rules to achieve speed efficiency, by eliminating early irrelevant features. version:1
arxiv-1502-06952 | Phase Transitions for High Dimensional Clustering and Related Problems | http://arxiv.org/abs/1502.06952 | id:1502.06952 author:Jiashun Jin, Zheng Tracy Ke, Wanjie Wang category:math.ST stat.ML stat.TH  published:2015-02-24 summary:Consider a two-class clustering problem where we observe $X_i = \ell_i \mu + Z_i$, $Z_i \stackrel{iid}{\sim} N(0, I_p)$, $1 \leq i \leq n$. The feature vector $\mu\in R^p$ is unknown but is presumably sparse. The class labels $\ell_i\in\{-1, 1\}$ are also unknown and the main interest is to estimate them. We are interested in the statistical limits. In the two-dimensional phase space calibrating the rarity and strengths of useful features, we find the precise demarcation for the Region of Impossibility and Region of Possibility. In the former, useful features are too rare/weak for successful clustering. In the latter, useful features are strong enough to allow successful clustering. The results are extended to the case of colored noise using Le Cam's idea on comparison of experiments. We also extend the study on statistical limits for clustering to that for signal recovery and that for hypothesis testing. We compare the statistical limits for three problems and expose some interesting insight. We propose classical PCA and Important Features PCA (IF-PCA) for clustering. For a threshold $t > 0$, IF-PCA clusters by applying classical PCA to all columns of $X$ with an $L^2$-norm larger than $t$. We also propose two aggregation methods. For any parameter in the Region of Possibility, some of these methods yield successful clustering. We find an interesting phase transition for IF-PCA. Our results require delicate analysis, especially on post-selection Random Matrix Theory and on lower bound arguments. version:4
arxiv-1604-02993 | Using Sentence-Level LSTM Language Models for Script Inference | http://arxiv.org/abs/1604.02993 | id:1604.02993 author:Karl Pichotta, Raymond J. Mooney category:cs.CL  published:2016-04-11 summary:There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on structured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents. version:2
arxiv-1606-02689 | Continuously Learning Neural Dialogue Management | http://arxiv.org/abs/1606.02689 | id:1606.02689 author:Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young category:cs.CL cs.LG  published:2016-06-08 summary:We describe a two-step approach for dialogue management in task-oriented spoken dialogue systems. A unified neural network framework is proposed to enable the system to first learn by supervision from a set of dialogue data and then continuously improve its behaviour via reinforcement learning, all using gradient-based algorithms on one single model. The experiments demonstrate the supervised model's effectiveness in the corpus-based evaluation, with user simulation, and with paid human subjects. The use of reinforcement learning further improves the model's performance in both interactive settings, especially under higher-noise conditions. version:1
arxiv-1606-02680 | First Result on Arabic Neural Machine Translation | http://arxiv.org/abs/1606.02680 | id:1606.02680 author:Amjad Almahairi, Kyunghyun Cho, Nizar Habash, Aaron Courville category:cs.CL  published:2016-06-08 summary:Neural machine translation has become a major alternative to widely used phrase-based statistical machine translation. We notice however that much of research on neural machine translation has focused on European languages despite its language agnostic nature. In this paper, we apply neural machine translation to the task of Arabic translation (Ar<->En) and compare it against a standard phrase-based translation system. We run extensive comparison using various configurations in preprocessing Arabic script and show that the phrase-based and neural translation systems perform comparably to each other and that proper preprocessing of Arabic script has a similar effect on both of the systems. We however observe that the neural machine translation significantly outperform the phrase-based system on an out-of-domain test set, making it attractive for real-world deployment. version:1
arxiv-1606-02647 | Safe and Efficient Off-Policy Reinforcement Learning | http://arxiv.org/abs/1606.02647 | id:1606.02647 author:Rémi Munos, Tom Stepleton, Anna Harutyunyan, Marc G. Bellemare category:cs.LG cs.AI stat.ML  published:2016-06-08 summary:In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\lambda$), with three desired properties: (1) low variance; (2) safety, as it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) efficiency, as it makes the best use of samples collected from near on-policy behaviour policies. We analyse the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. To our knowledge, this is the first return-based off-policy control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\lambda$), which was still an open problem. We illustrate the benefits of Retrace($\lambda$) on a standard suite of Atari 2600 games. version:1
arxiv-1603-06155 | A Persona-Based Neural Conversation Model | http://arxiv.org/abs/1603.06155 | id:1603.06155 author:Jiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis, Jianfeng Gao, Bill Dolan category:cs.CL  published:2016-03-19 summary:We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges. version:2
arxiv-1605-03661 | Learning Representations for Counterfactual Inference | http://arxiv.org/abs/1605.03661 | id:1605.03661 author:Fredrik D. Johansson, Uri Shalit, David Sontag category:stat.ML cs.AI cs.LG  published:2016-05-12 summary:Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, "Would this patient have lower blood sugar had she received a different medication?". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art. version:2
arxiv-1606-01467 | Deep Q-Networks for Accelerating the Training of Deep Neural Networks | http://arxiv.org/abs/1606.01467 | id:1606.01467 author:Jie Fu, Zichuan Lin, Miao Liu, Nicholas Leonard, Jiashi Feng, Tat-Seng Chua category:cs.LG cs.NE  published:2016-06-05 summary:We present a novel method, called QAN, for improving the generalization ability of a deep neural network (DNN). It achieves this by feeding the weights of the DNN during training into a deep Q-network (DQN) as its states. The actions of the DQN modify different hyperparameters during training. The reward function of this DQN is designed to learn policies to accelerate the training of that DNN. Empirically, this acceleration leads to better generalization performance of the DNN. Compared to other similar works, QAN is thousands of times faster as it takes into account the characteristics of DNNs' training. To the best of our knowledge, QAN is the first practical solution to speeding up the training of a DNN using a DQN on real-world benchmark datasets. The code can be downloaded from https://github.com/bigaidream-projects/qan version:2
arxiv-1606-02638 | Addressing Limited Data for Textual Entailment Across Domains | http://arxiv.org/abs/1606.02638 | id:1606.02638 author:Chaitanya Shivade, Preethi Raghavan, Siddharth Patwardhan category:cs.CL  published:2016-06-08 summary:We seek to address the lack of labeled data (and high cost of annotation) for textual entailment in some domains. To that end, we first create (for experimental purposes) an entailment dataset for the clinical domain, and a highly competitive supervised entailment system, ENT, that is effective (out of the box) on two domains. We then explore self-training and active learning strategies to address the lack of labeled data. With self-training, we successfully exploit unlabeled data to improve over ENT by 15% F-score on the newswire domain, and 13% F-score on clinical data. On the other hand, our active learning experiments demonstrate that we can match (and even beat) ENT using only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain. version:1
arxiv-1511-00792 | Scalable Recommendation from Web Usage Mining using Method of Moments | http://arxiv.org/abs/1511.00792 | id:1511.00792 author:Sayantan Dasgupta category:cs.LG  published:2015-11-03 summary:Building recommendation algorithms is one of the most challenging tasks in Machine Learning. Although most of the recommendation systems are built on explicit feedback available from the users in terms of rating or text, a majority of the applications do not receive such feedback. Here we consider the recommendation task where the only available data is the records of user-item interaction over web applications over time, in terms of subscription or purchase of items; this is known as implicit feedback recommendation. It is very common to draw recommendation from such datasets using Probabilistic Latent Semantic Indexing (PLSI). However, PLSI relies on EM algorithm and suffers from local maxima problem. Also, for any web application, there is massive amount of user-item interaction data available stored across distributed frameworks. Algorithms like PLSI or Matrix Factorization runs several iterations through the dataset, and may not be suitable for large web scale dataset. Here we propose a solution for PLSI using Method of Moments, which unlike EM algorithm does not suffer from local maxima, and provides significant improvement in performance over the standard EM based solution. Further, we show how to scale up the algorithm using a stochastic whitening step. This results in a highly scalable algorithm that scales up to million of users even on a machine with a single-core processor and 8 GB RAM, and produces competitive performance in comparison with existing algorithms. version:9
arxiv-1604-08153 | Classifying Options for Deep Reinforcement Learning | http://arxiv.org/abs/1604.08153 | id:1604.08153 author:Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, Anil Anthony Bharath category:cs.LG cs.AI stat.ML  published:2016-04-27 summary:In this paper we combine one method for hierarchical reinforcement learning - the options framework - with deep Q-networks (DQNs) through the use of different "option heads" on the policy network, and a supervisory network for choosing between the different options. We utilise our setup to investigate the effects of architectural constraints in subtasks with positive and negative transfer, across a range of network capacities. We empirically show that our augmented DQN has lower sample complexity when simultaneously learning subtasks with negative transfer, without degrading performance when learning subtasks with positive transfer. version:2
arxiv-1606-02615 | Specific Differential Entropy Rate Estimation for Continuous-Valued Time Series | http://arxiv.org/abs/1606.02615 | id:1606.02615 author:David Darmon category:cs.LG stat.ME  published:2016-06-08 summary:We introduce a method for quantifying the inherent unpredictability of a continuous-valued time series via an extension of the differential Shannon entropy rate. Our extension, the specific entropy rate, quantifies the amount of predictive uncertainty associated with a specific state, rather than averaged over all states. We relate the specific entropy rate to popular `complexity' measures such as Approximate and Sample Entropies. We provide a data-driven approach for estimating the specific entropy rate of an observed time series. Finally, we consider three case studies of estimating specific entropy rate from synthetic and physiological data relevant to the analysis of heart rate variability. version:1
arxiv-1606-02608 | Fast and Extensible Online Multivariate Kernel Density Estimation | http://arxiv.org/abs/1606.02608 | id:1606.02608 author:Jaime Ferreira, David Martins de Matos, Ricardo Ribeiro category:cs.LG cs.CV I.5.1; I.5.2  published:2016-06-08 summary:We present xokde++, a state-of-the-art online kernel density estimation approach that maintains Gaussian mixture models input data streams. The approach follows state-of-the-art work on online density estimation, but was redesigned with computational efficiency, numerical robustness, and extensibility in mind. Our approach produces comparable or better results than the current state-of-the-art, while achieving significant computational performance gains and improved numerical stability. The use of diagonal covariance Gaussian kernels, which further improve performance and stability, at a small loss of modelling quality, is also explored. Our approach is up to 40 times faster, while requiring 90\% less memory than the closest state-of-the-art counterpart. version:1
arxiv-1606-02601 | A Joint Model for Word Embedding and Word Morphology | http://arxiv.org/abs/1606.02601 | id:1606.02601 author:Kris Cao, Marek Rei category:cs.CL  published:2016-06-08 summary:This paper presents a joint model for performing unsupervised morphological analysis on words, and learning a character-level composition function from morphemes to word embeddings. Our model splits individual words into segments, and weights each segment according to its ability to predict context words. Our morphological analysis is comparable to dedicated morphological analyzers at the task of morpheme boundary recovery, and also performs better than word-based embedding models at the task of syntactic analogy answering. Finally, we show that incorporating morphology explicitly into character-level models help them produce embeddings for unseen words which correlate better with human judgments. version:1
arxiv-1505-06816 | Representing Meaning with a Combination of Logical and Distributional Models | http://arxiv.org/abs/1505.06816 | id:1505.06816 author:I. Beltagy, Stephen Roller, Pengxiang Cheng, Katrin Erk, Raymond J. Mooney category:cs.CL  published:2015-05-26 summary:NLP tasks differ in the semantic information they require, and at this time no single se- mantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not capture sentence structure in the same detail as logic-based approaches. So it has been argued that the two are complementary. We adopt a hybrid approach that combines logic-based and distributional semantics through probabilistic logic inference in Markov Logic Networks (MLNs). In this paper, we focus on the three components of a practical system integrating logical and distributional models: 1) Parsing and task representation is the logic-based part where input problems are represented in probabilistic logic. This is quite different from representing them in standard first-order logic. 2) For knowledge base construction we form weighted inference rules. We integrate and compare distributional information with other sources, notably WordNet and an existing paraphrase collection. In particular, we use our system to evaluate distributional lexical entailment approaches. We use a variant of Robinson resolution to determine the necessary inference rules. More sources can easily be added by mapping them to logical rules; our system learns a resource-specific weight that corrects for scaling differences between resources. 3) In discussing probabilistic inference, we show how to solve the inference problems efficiently. To evaluate our approach, we use the task of textual entailment (RTE), which can utilize the strengths of both logic-based and distributional representations. In particular we focus on the SICK dataset, where we achieve state-of-the-art results. version:5
arxiv-1503-03701 | Hierarchical learning of grids of microtopics | http://arxiv.org/abs/1503.03701 | id:1503.03701 author:Nebojsa Jojic, Alessandro Perina, Dongwoo Kim category:stat.ML cs.IR cs.LG  published:2015-03-12 summary:The counting grid is a grid of microtopics, sparse word/feature distributions. The generative model associated with the grid does not use these microtopics individually. Rather, it groups them in overlapping rectangular windows and uses these grouped microtopics as either mixture or admixture components. This paper builds upon the basic counting grid model and it shows that hierarchical reasoning helps avoid bad local minima, produces better classification accuracy and, most interestingly, allows for extraction of large numbers of coherent microtopics even from small datasets. We evaluate this in terms of consistency, diversity and clarity of the indexed content, as well as in a user study on word intrusion tasks. We demonstrate that these models work well as a technique for embedding raw images and discuss interesting parallels between hierarchical CG models and other deep architectures. version:4
arxiv-1606-02585 | Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery | http://arxiv.org/abs/1606.02585 | id:1606.02585 author:Jamie Sherrah category:cs.CV  published:2016-06-08 summary:The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets. version:1
arxiv-1603-06744 | Latent Predictor Networks for Code Generation | http://arxiv.org/abs/1603.06744 | id:1603.06744 author:Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, Andrew Senior, Fumin Wang, Phil Blunsom category:cs.CL cs.NE  published:2016-03-22 summary:Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks. version:2
arxiv-1606-02580 | Convolution by Evolution: Differentiable Pattern Producing Networks | http://arxiv.org/abs/1606.02580 | id:1606.02580 author:Chrisantha Fernando, Dylan Banarse, Malcolm Reynolds, Frederic Besse, David Pfau, Max Jaderberg, Marc Lanctot, Daan Wierstra category:cs.NE cs.CV cs.LG  published:2016-06-08 summary:In this work we introduce a differentiable version of the Compositional Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the topology of a DPPN is evolved but the weights are learned. A Lamarckian algorithm, that combines evolution and learning, produces DPPNs to reconstruct an image. Our main result is that DPPNs can be evolved/trained to compress the weights of a denoising autoencoder from 157684 to roughly 200 parameters, while achieving a reconstruction accuracy comparable to a fully connected network with more than two orders of magnitude more parameters. The regularization ability of the DPPN allows it to rediscover (approximate) convolutional network architectures embedded within a fully connected architecture. Such convolutional architectures are the current state of the art for many computer vision applications, so it is satisfying that DPPNs are capable of discovering this structure rather than having to build it in by design. DPPNs exhibit better generalization when tested on the Omniglot dataset after being trained on MNIST, than directly encoded fully connected autoencoders. DPPNs are therefore a new framework for integrating learning and evolution. version:1
arxiv-1606-02566 | A moment-matching Ferguson and Klass algorithm | http://arxiv.org/abs/1606.02566 | id:1606.02566 author:Julyan Arbel, Igor Prünster category:stat.CO stat.ML  published:2016-06-08 summary:Completely random measures (CRM) represent the key building block of a wide variety of popular stochastic models and play a pivotal role in modern Bayesian Nonparametrics. A popular representation of CRMs as a random series with decreasing jumps is due to Ferguson and Klass (1972). This can immediately be turned into an algorithm for sampling realizations of CRMs or more elaborate models involving transformed CRMs. However, concrete implementation requires to truncate the random series at some threshold resulting in an approximation error. The goal of this paper is to quantify the quality of the approximation by a moment-matching criterion, which consists in evaluating a measure of discrepancy between actual moments and moments based on the simulation output. Seen as a function of the truncation level, the methodology can be used to determine the truncation level needed to reach a certain level of precision. The resulting moment-matching \FK algorithm is then implemented and illustrated on several popular Bayesian nonparametric models. version:1
arxiv-1602-06886 | Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation | http://arxiv.org/abs/1602.06886 | id:1602.06886 author:Akash Srivastava, James Zou, Charles Sutton category:stat.ML cs.LG  published:2016-02-22 summary:A good clustering can help a data analyst to explore and understand a data set, but what constitutes a good clustering may depend on domain-specific and application-specific criteria. These criteria can be difficult to formalize, even when it is easy for an analyst to know a good clustering when she sees one. We present a new approach to interactive clustering for data exploration, called \ciif, based on a particularly simple feedback mechanism, in which an analyst can choose to reject individual clusters and request new ones. The new clusters should be different from previously rejected clusters while still fitting the data well. We formalize this interaction in a novel Bayesian prior elicitation framework. In each iteration, the prior is adapted to account for all the previous feedback, and a new clustering is then produced from the posterior distribution. To achieve the computational efficiency necessary for an interactive setting, we propose an incremental optimization method over data minibatches using Lagrangian relaxation. Experiments demonstrate that \ciif can produce accurate and diverse clusterings. version:2
arxiv-1606-02562 | DialPort: Connecting the Spoken Dialog Research Community to Real User Data | http://arxiv.org/abs/1606.02562 | id:1606.02562 author:Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi category:cs.AI cs.CL  published:2016-06-08 summary:This paper describes a new spoken dialog portal that connects systems produced by the spoken dialog academic research community and gives them access to real users. We introduce a distributed, multi-modal, multi-agent prototype dialog framework that affords easy integration with various remote resources, ranging from end-to-end dialog systems to external knowledge APIs. To date, the DialPort portal has successfully connected to the multi-domain spoken dialog system at Cambridge University, the NOAA (National Oceanic and Atmospheric Administration) weather API and the Yelp API. version:1
arxiv-1606-02560 | Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning | http://arxiv.org/abs/1606.02560 | id:1606.02560 author:Tiancheng Zhao, Maxine Eskenazi category:cs.AI cs.CL cs.LG  published:2016-06-08 summary:This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state. version:1
arxiv-1603-06393 | Incorporating Copying Mechanism in Sequence-to-Sequence Learning | http://arxiv.org/abs/1603.06393 | id:1603.06393 author:Jiatao Gu, Zhengdong Lu, Hang Li, Victor O. K. Li category:cs.CL cs.AI cs.LG cs.NE  published:2016-03-21 summary:We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks. version:3
arxiv-1606-02555 | Improving Recurrent Neural Networks For Sequence Labelling | http://arxiv.org/abs/1606.02555 | id:1606.02555 author:Marco Dinarelli, Isabelle Tellier category:cs.CL cs.LG cs.NE  published:2016-06-08 summary:In this paper we study different types of Recurrent Neural Networks (RNN) for sequence labeling tasks. We propose two new variants of RNNs integrating improvements for sequence labeling, and we compare them to the more traditional Elman and Jordan RNNs. We compare all models, either traditional or new, on four distinct tasks of sequence labeling: two on Spoken Language Understanding (ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the Penn Treebank (PTB) corpora. The results show that our new variants of RNNs are always more effective than the others. version:1
arxiv-1603-06042 | Globally Normalized Transition-Based Neural Networks | http://arxiv.org/abs/1603.06042 | id:1603.06042 author:Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, Michael Collins category:cs.CL cs.LG cs.NE  published:2016-03-19 summary:We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models. version:2
arxiv-1606-02546 | Estimation of solar irradiance using ground-based whole sky imagers | http://arxiv.org/abs/1606.02546 | id:1606.02546 author:Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler category:astro-ph.IM cs.CV cs.MM  published:2016-06-08 summary:Ground-based whole sky imagers (WSIs) can provide localized images of the sky of high temporal and spatial resolution, which permits fine-grained cloud observation. In this paper, we show how images taken by WSIs can be used to estimate solar radiation. Sky cameras are useful here because they provide additional information about cloud movement and coverage, which are otherwise not available from weather station data. Our setup includes ground-based weather stations at the same location as the imagers. We use their measurements to validate our methods. version:1
arxiv-1606-02529 | Coordination Annotation Extension in the Penn Tree Bank | http://arxiv.org/abs/1606.02529 | id:1606.02529 author:Jessica Ficler, Yoav Goldberg category:cs.CL  published:2016-06-08 summary:Coordination is an important and common syntactic construction which is not handled well by state of the art parsers. Coordinations in the Penn Treebank are missing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsistencies. In this work, we initiated manual annotation process for solving these issues. We identify the different elements in a coordination phrase and label each element with its function. We add phrase boundaries when these are missing, unify inconsistencies, and fix errors. The outcome is an extension of the PTB that includes consistent and detailed structures for coordinations. We make the coordination annotation publicly available, in hope that they will facilitate further research into coordination disambiguation. version:1
arxiv-1602-08952 | Representation of linguistic form and function in recurrent neural networks | http://arxiv.org/abs/1602.08952 | id:1602.08952 author:Ákos Kádár, Grzegorz Chrupała, Afra Alishahi category:cs.CL cs.LG  published:2016-02-29 summary:We present novel methods for analyzing the activation patterns of RNNs from a linguistic point of view and explore the types of linguistic structure they learn. As a case study, we use a multi-task gated recurrent network architecture consisting of two parallel pathways with shared word embeddings trained on predicting the representations of the visual scene corresponding to an input sentence, and predicting the next word in the same sentence. Based on our proposed method to estimate the amount of contribution of individual tokens in the input to the final prediction of the networks we show that the image prediction pathway: a) is sensitive to the information structure of the sentence b) pays selective attention to lexical categories and grammatical functions that carry semantic information c) learns to treat the same input token differently depending on its grammatical functions in the sentence. In contrast the language model is comparatively more sensitive to words with a syntactic function. Furthermore, we propose methods to ex- plore the function of individual hidden units in RNNs and show that the two pathways of the architecture in our case study contain specialized units tuned to patterns informative for the task, some of which can carry activations to later time steps to encode long-term dependencies. version:2
arxiv-1606-02518 | A Locally Adaptive Normal Distribution | http://arxiv.org/abs/1606.02518 | id:1606.02518 author:Georgios Arvanitidis, Lars Kai Hansen, Søren Hauberg category:stat.ML  published:2016-06-08 summary:The multivariate normal density is a monotonic function of the distance to the mean, and its ellipsoidal shape is due to the underlying Euclidean metric. We suggest to replace this metric with a locally adaptive, smoothly changing (Riemannian) metric that favors regions of high local density. The resulting locally adaptive normal distribution (LAND) is a generalization of the normal distribution to the "manifold" setting, where data is assumed to lie near a potentially low-dimensional manifold embedded in $\mathbb{R}^D$. The LAND is parametric, depending only on a mean and a covariance, and is the maximum entropy distribution under the given metric. The underlying metric is, however, non-parametric. We develop a maximum likelihood algorithm to infer the distribution parameters that relies on a combination of gradient descent and Monte Carlo integration. We further extend the LAND to mixture models, and provide the corresponding EM algorithm. We demonstrate the efficiency of the LAND to fit non-trivial probability distributions over both synthetic data, and EEG measurements of human sleep. version:1
arxiv-1605-05273 | Learning Convolutional Neural Networks for Graphs | http://arxiv.org/abs/1605.05273 | id:1605.05273 author:Mathias Niepert, Mohamed Ahmed, Konstantin Kutzkov category:cs.LG cs.AI stat.ML  published:2016-05-17 summary:Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient. version:4
arxiv-1606-02514 | DefExt: A Semi Supervised Definition Extraction Tool | http://arxiv.org/abs/1606.02514 | id:1606.02514 author:Luis Espinosa-Anke, Roberto Carlini, Horacio Saggion, Francesco Ronzano category:cs.CL  published:2016-06-08 summary:We present DefExt, an easy to use semi supervised Definition Extraction Tool. DefExt is designed to extract from a target corpus those textual fragments where a term is explicitly mentioned together with its core features, i.e. its definition. It works on the back of a Conditional Random Fields based sequential labeling algorithm and a bootstrapping approach. Bootstrapping enables the model to gradually become more aware of the idiosyncrasies of the target corpus. In this paper we describe the main components of the toolkit as well as experimental results stemming from both automatic and manual evaluation. We release DefExt as open source along with the necessary files to run it in any Unix machine. We also provide access to training and test data for immediate use. version:1
arxiv-1606-02467 | Point-wise mutual information-based video segmentation with high temporal consistency | http://arxiv.org/abs/1606.02467 | id:1606.02467 author:Margret Keuper, Thomas Brox category:cs.CV  published:2016-06-08 summary:In this paper, we tackle the problem of temporally consistent boundary detection and hierarchical segmentation in videos. While finding the best high-level reasoning of region assignments in videos is the focus of much recent research, temporal consistency in boundary detection has so far only rarely been tackled. We argue that temporally consistent boundaries are a key component to temporally consistent region assignment. The proposed method is based on the point-wise mutual information (PMI) of spatio-temporal voxels. Temporal consistency is established by an evaluation of PMI-based point affinities in the spectral domain over space and time. Thus, the proposed method is independent of any optical flow computation or previously learned motion models. The proposed low-level video segmentation method outperforms the learning-based state of the art in terms of standard region metrics. version:1
arxiv-1606-02461 | Learning Semantically and Additively Compositional Distributional Representations | http://arxiv.org/abs/1606.02461 | id:1606.02461 author:Ran Tian, Naoaki Okazaki, Kentaro Inui category:cs.CL  published:2016-06-08 summary:This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS). We show theoretical evidence that the vector compositions in our model conform to the logic of DCS. Experimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art. version:1
arxiv-1602-05394 | Online optimization and regret guarantees for non-additive long-term constraints | http://arxiv.org/abs/1602.05394 | id:1602.05394 author:Rodolphe Jenatton, Jim Huang, Dominik Csiba, Cedric Archambeau category:stat.ML cs.LG math.OC math.ST stat.TH  published:2016-02-17 summary:We consider online optimization in the 1-lookahead setting, where the objective does not decompose additively over the rounds of the online game. The resulting formulation enables us to deal with non-stationary and/or long-term constraints , which arise, for example, in online display advertising problems. We propose an on-line primal-dual algorithm for which we obtain dynamic cumulative regret guarantees. They depend on the convexity and the smoothness of the non-additive penalty, as well as terms capturing the smoothness with which the residuals of the non-stationary and long-term constraints vary over the rounds. We conduct experiments on synthetic data to illustrate the benefits of the non-additive penalty and show vanishing regret convergence on live traffic data collected by a display advertising platform in production. version:2
arxiv-1603-06075 | Tree-to-Sequence Attentional Neural Machine Translation | http://arxiv.org/abs/1603.06075 | id:1603.06075 author:Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka category:cs.CL  published:2016-03-19 summary:Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system. version:3
arxiv-1606-02448 | Multiple-Play Bandits in the Position-Based Model | http://arxiv.org/abs/1606.02448 | id:1606.02448 author:Paul Lagrée, Claire Vernade, Olivier Cappé category:cs.LG math.ST stat.TH  published:2016-06-08 summary:Sequentially learning to place items in multi-position displays or lists is a task that can be cast into the multiple-play semi-bandit setting. However, a major concern in this context is when the system cannot decide whether the user feedback for each item is actually exploitable. Indeed, much of the content may have been simply ignored by the user. The present work proposes to exploit available information regarding the display position bias under the so-called Position-based click model (PBM). We first discuss how this model differs from the Cascade model and its variants considered in several recent works on multiple-play bandits. We then provide a novel regret lower bound for this model as well as computationally efficient algorithms that display good empirical and theoretical performance. version:1
arxiv-1602-00853 | An analytic comparison of regularization methods for Gaussian Processes | http://arxiv.org/abs/1602.00853 | id:1602.00853 author:Hossein Mohammadi, Rodolphe Le Riche, Nicolas Durrande, Eric Touboul, Xavier Bay category:math.OC math.ST stat.ML stat.TH  published:2016-02-02 summary:Gaussian Processes (GPs) are often used to predict the output of a parameterized deterministic experiment. They have many applications in the field of Computer Experiments, in particular to perform sensitivity analysis, adaptive design of experiments and global optimization. Nearly all of the applications of GPs to Computer Experiments require the inversion of a covariance matrix. Because this matrix is often ill-conditioned, regularization techniques are required. Today, there is still a need to better regularize GPs.The two most classical regularization methods to avoid degeneracy of the covariance matrix are i) pseudoinverse (PI) and ii) adding a small positive constant to the main diagonal, i.e., the case of noisy observations. Herein, we will refer to the second regularization technique with a slight abuse of language as nugget. This paper provides algebraic calculations which allow comparing PI and nugget regularizations. It is proven that pseudoinverse regularization averages the output values and makes the variance null at redundant points. On the opposite, nugget regularization lacks interpolationproperties but preserves a non-zero variance at every point. However, these two regularization techniques become similar as the nugget value decreases. A distribution-wise GP is introduced which interpolates Gaussian distributions instead of data points and mitigates the drawbacks of pseudoinverse and nugget regularized GPs. Finally, data-model discrepancy is discussed and serves as a guide for choosing a regularization technique. version:2
arxiv-1606-02447 | Learning Language Games through Interaction | http://arxiv.org/abs/1606.02447 | id:1606.02447 author:Sida I. Wang, Percy Liang, Christopher D. Manning category:cs.CL cs.AI I.2.6; I.2.7  published:2016-06-08 summary:We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players. version:1
arxiv-1606-02440 | On the Place of Text Data in Lifelogs, and Text Analysis via Semantic Facets | http://arxiv.org/abs/1606.02440 | id:1606.02440 author:Gregory Grefenstette, Lawrence Muchemi category:cs.CL cs.CY cs.HC  published:2016-06-08 summary:Current research in lifelog data has not paid enough attention to analysis of cognitive activities in comparison to physical activities. We argue that as we look into the future, wearable devices are going to be cheaper and more prevalent and textual data will play a more significant role. Data captured by lifelogging devices will increasingly include speech and text, potentially useful in analysis of intellectual activities. Analyzing what a person hears, reads, and sees, we should be able to measure the extent of cognitive activity devoted to a certain topic or subject by a learner. Test-based lifelog records can benefit from semantic analysis tools developed for natural language processing. We show how semantic analysis of such text data can be achieved through the use of taxonomic subject facets and how these facets might be useful in quantifying cognitive activity devoted to various topics in a person's day. We are currently developing a method to automatically create taxonomic topic vocabularies that can be applied to this detection of intellectual activity. version:1
arxiv-1603-06067 | Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings | http://arxiv.org/abs/1603.06067 | id:1603.06067 author:Kazuma Hashimoto, Yoshimasa Tsuruoka category:cs.CL  published:2016-03-19 summary:We present a novel method for jointly learning compositional and non-compositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function. The scoring function is used to quantify the level of compositionality of each phrase, and the parameters of the function are jointly optimized with the objective for learning phrase embeddings. In experiments, we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases, and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality, substantially outperforming the previous state of the art. Moreover, our embeddings improve upon the previous best model on a transitive verb disambiguation task. We also show that a simple ensemble technique further improves the results for both tasks. version:3
arxiv-1601-00496 | Nonparametric Modeling of Dynamic Functional Connectivity in fMRI Data | http://arxiv.org/abs/1601.00496 | id:1601.00496 author:Søren F. V. Nielsen, Kristoffer H. Madsen, Rasmus Røge, Mikkel N. Schmidt, Morten Mørup category:stat.AP q-bio.NC stat.ML  published:2016-01-04 summary:Dynamic functional connectivity (FC) has in recent years become a topic of interest in the neuroimaging community. Several models and methods exist for both functional magnetic resonance imaging (fMRI) and electroencephalography (EEG), and the results point towards the conclusion that FC exhibits dynamic changes. The existing approaches modeling dynamic connectivity have primarily been based on time-windowing the data and k-means clustering. We propose a non-parametric generative model for dynamic FC in fMRI that does not rely on specifying window lengths and number of dynamic states. Rooted in Bayesian statistical modeling we use the predictive likelihood to investigate if the model can discriminate between a motor task and rest both within and across subjects. We further investigate what drives dynamic states using the model on the entire data collated across subjects and task/rest. We find that the number of states extracted are driven by subject variability and preprocessing differences while the individual states are almost purely defined by either task or rest. This questions how we in general interpret dynamic FC and points to the need for more research on what drives dynamic FC. version:2
arxiv-1606-02421 | Gossip Dual Averaging for Decentralized Optimization of Pairwise Functions | http://arxiv.org/abs/1606.02421 | id:1606.02421 author:Igor Colin, Aurélien Bellet, Joseph Salmon, Stéphan Clémençon category:stat.ML cs.DC cs.LG cs.SY  published:2016-06-08 summary:In decentralized networks (of sensors, connected objects, etc.), there is an important need for efficient algorithms to optimize a global cost function, for instance to learn a global model from the local data collected by each computing unit. In this paper, we address the problem of decentralized minimization of pairwise functions of the data points, where these points are distributed over the nodes of a graph defining the communication topology of the network. This general problem finds applications in ranking, distance metric learning and graph inference, among others. We propose new gossip algorithms based on dual averaging which aims at solving such problems both in synchronous and asynchronous settings. The proposed framework is flexible enough to deal with constrained and regularized variants of the optimization problem. Our theoretical analysis reveals that the proposed algorithms preserve the convergence rate of centralized dual averaging up to an additive bias term. We present numerical simulations on Area Under the ROC Curve (AUC) maximization and metric learning problems which illustrate the practical interest of our approach. version:1
arxiv-1606-01672 | Predictive Coding for Dynamic Vision : Development of Functional Hierarchy in a Multiple Spatio-Temporal Scales RNN Model | http://arxiv.org/abs/1606.01672 | id:1606.01672 author:Minkyu Choi, Jun Tani category:cs.CV  published:2016-06-06 summary:The current paper presents a novel recurrent neural network model, the predictive multiple spatio-temporal scales RNN (P-MSTRNN), which can generate as well as recognize dynamic visual patterns in the predictive coding framework. The model is characterized by multiple spatio-temporal scales imposed on neural unit dynamics through which an adequate spatio-temporal hierarchy develops via learning from exemplars. The model was evaluated by conducting an experiment of learning a set of whole body human movement patterns which was generated by following a hierarchically defined movement syntax. The analysis of the trained model clarifies what types of spatio-temporal hierarchy develop in dynamic neural activity as well as how robust generation and recognition of movement patterns can be achieved by using the error minimization principle. version:2
arxiv-1603-06679 | Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis | http://arxiv.org/abs/1603.06679 | id:1603.06679 author:Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, Xiaokui Xiao category:cs.CL cs.IR cs.LG  published:2016-03-22 summary:In aspect-based sentiment analysis, extracting aspect terms along with the opinions being expressed from user-generated content is one of the most important subtasks. Previous studies have shown that exploiting connections between aspect and opinion terms is promising for this task. In this paper, we propose a novel joint model that integrates recursive neural networks and conditional random fields into a unified framework for explicit aspect and opinion terms co-extraction. The proposed model learns high-level discriminative features and double propagate information between aspect and opinion terms, simultaneously. Moreover, it is flexible to incorporate hand-crafted features into the proposed model to further boost its information extraction performance. Experimental results on the SemEval Challenge 2014 dataset show the superiority of our proposed model over several baseline methods as well as the winning systems of the challenge. version:2
arxiv-1511-04636 | Deep Reinforcement Learning with a Natural Language Action Space | http://arxiv.org/abs/1511.04636 | id:1511.04636 author:Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf category:cs.AI cs.CL cs.LG  published:2015-11-14 summary:This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text. version:5
arxiv-1603-08887 | Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints | http://arxiv.org/abs/1603.08887 | id:1603.08887 author:Greg Durrett, Taylor Berg-Kirkpatrick, Dan Klein category:cs.CL  published:2016-03-29 summary:We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun's antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. version:2
arxiv-1606-02407 | Structured Convolution Matrices for Energy-efficient Deep learning | http://arxiv.org/abs/1606.02407 | id:1606.02407 author:Rathinakumar Appuswamy, Tapan Nayak, John Arthur, Steven Esser, Paul Merolla, Jeffrey Mckinstry, Timothy Melano, Myron Flickner, Dharmendra Modha category:cs.NE cs.AI cs.CV cs.LG  published:2016-06-08 summary:We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy-rectified linear units and binary activations. version:1
arxiv-1606-02404 | Clustering with Same-Cluster Queries | http://arxiv.org/abs/1606.02404 | id:1606.02404 author:Hassan Ashtiani, Shrinu Kushagra, Shai Ben-David category:cs.LG stat.ML  published:2016-06-08 summary:We propose a framework for Semi-Supervised Active Clustering framework (SSAC), where the learner is allowed to interact with a domain expert, asking whether two given instances belong to the same cluster or not. We study the query and computational complexity of clustering in this framework. We consider a setting where the expert conforms to a center-based clustering with a notion of margin. We show that there is a trade off between computational complexity and query complexity; We prove that for the case of $k$-means clustering (i.e., when the expert conforms to a solution of $k$-means), having access to relatively few such queries allows efficient solutions to otherwise NP hard problems. In particular, we provide a probabilistic polynomial-time (BPP) algorithm for clustering in this setting that asks $O\big(k^2\log k + k\log n)$ same-cluster queries and runs with time complexity $O\big(kn\log n)$ (where $k$ is the number of clusters and $n$ is the number of instances). The success of the algorithm is guaranteed for data satisfying margin conditions under which, without queries, we show that the problem is NP hard. We also prove a lower bound on the number of queries needed to have a computationally efficient clustering algorithm in this setting. version:1
arxiv-1606-02401 | On estimating a mixture on graphons | http://arxiv.org/abs/1606.02401 | id:1606.02401 author:Soumendu Sundar Mukherjee, Purnamrita Sarkar, Lizhen Lin category:stat.ML stat.ME  published:2016-06-08 summary:Community detection, which focuses on clustering nodes or detecting communities in (mostly) a single network, is a problem of considerable practical interest and has received a great deal of attention in the research community. While being able to cluster within a network is important, there are emerging needs to be able to cluster multiple networks. This is largely motivated by the routine collection of network data that are generated from potentially different populations, such as brain networks of subjects from different disease groups, genders, or biological networks generated under different experimental conditions, etc. We propose a simple and general framework for clustering multiple networks based on a mixture model on graphons. Our clustering method employs graphon estimation as a first step and performs spectral clustering on the matrix of distances between estimated graphons. This is illustrated through both simulated and real data sets, and theoretical justification of the algorithm is given in terms of consistency. version:1
arxiv-1601-04811 | Modeling Coverage for Neural Machine Translation | http://arxiv.org/abs/1601.04811 | id:1601.04811 author:Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li category:cs.CL  published:2016-01-19 summary:Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT. version:5
arxiv-1606-02396 | Deep Successor Reinforcement Learning | http://arxiv.org/abs/1606.02396 | id:1606.02396 author:Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, Samuel J. Gershman category:stat.ML cs.AI cs.LG cs.NE  published:2016-06-08 summary:Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine. version:1
arxiv-1606-02393 | Hierarchical Attention Networks | http://arxiv.org/abs/1606.02393 | id:1606.02393 author:Paul Hongsuck Seo, Zhe Lin, Scott Cohen, Xiaohui Shen, Bohyung Han category:cs.CV  published:2016-06-08 summary:We propose a novel attention network, which accurately attends to target objects of various scales and shapes in images through multiple stages. The proposed network enables multiple layers to estimate attention in a convolutional neural network (CNN). The hierarchical attention model gradually suppresses irrelevant regions in an input image using a progressive attentive process over multiple CNN layers. The attentive process in each layer determines whether to pass or suppress feature maps for use in the next convolution. We employ local contexts to estimate attention probability at each location since it is difficult to infer accurate attention by observing a feature vector from a single location only. The experiments on synthetic and real datasets show that the proposed attention network outperforms traditional attention methods in various attribute prediction tasks. version:1
arxiv-1512-07587 | A Latent-Variable Lattice Model | http://arxiv.org/abs/1512.07587 | id:1512.07587 author:Rajasekaran Masatran category:cs.LG cs.CV stat.ML  published:2015-12-23 summary:Markov random field (MRF) learning is intractable, and its approximation algorithms are computationally expensive. We target a small subset of MRF that is used frequently in computer vision. We characterize this subset with three concepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as an alternative. Our goal is robust learning from small datasets. Our learning algorithm uses vector quantization and, at time complexity O(U log U) for a dataset of U pixels, is much faster than that of general-purpose MRF. version:7
arxiv-1604-00425 | Cross-lingual Models of Word Embeddings: An Empirical Comparison | http://arxiv.org/abs/1604.00425 | id:1604.00425 author:Shyam Upadhyay, Manaal Faruqui, Chris Dyer, Dan Roth category:cs.CL  published:2016-04-01 summary:Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. version:2
arxiv-1606-02382 | Deep Learning Convolutional Networks for Multiphoton Microscopy Vasculature Segmentation | http://arxiv.org/abs/1606.02382 | id:1606.02382 author:Petteri Teikari, Marc Santos, Charissa Poon, Kullervo Hynynen category:cs.CV cs.AI  published:2016-06-08 summary:Recently there has been an increasing trend to use deep learning frameworks for both 2D consumer images and for 3D medical images. However, there has been little effort to use deep frameworks for volumetric vascular segmentation. We wanted to address this by providing a freely available dataset of 12 annotated two-photon vasculature microscopy stacks. We demonstrated the use of deep learning framework consisting both 2D and 3D convolutional filters (ConvNet). Our hybrid 2D-3D architecture produced promising segmentation result. We derived the architectures from Lee et al. who used the ZNN framework initially designed for electron microscope image segmentation. We hope that by sharing our volumetric vasculature datasets, we will inspire other researchers to experiment with vasculature dataset and improve the used network architectures. version:1
arxiv-1606-02378 | SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks | http://arxiv.org/abs/1606.02378 | id:1606.02378 author:Arunkumar Byravan, Dieter Fox category:cs.LG cs.AI cs.CV cs.RO  published:2016-06-08 summary:We introduce SE3-Nets, which are deep networks designed to model rigid body motion from raw point cloud data. Based only on pairs of depth images along with an action vector and point wise data associations, SE3-Nets learn to segment effected object parts and predict their motion resulting from the applied force. Rather than learning point wise flow vectors, SE3-Nets predict SE3 transformations for different parts of the scene. Using simulated depth data of a table top scene and a robot manipulator, we show that the structure underlying SE3-Nets enables them to generate a far more consistent prediction of object motion than traditional flow based networks. version:1
arxiv-1605-04278 | Universal Dependencies for Learner English | http://arxiv.org/abs/1605.04278 | id:1605.04278 author:Yevgeni Berzak, Jessica Kenney, Carolyn Spadine, Jing Xian Wang, Lucia Lam, Keiko Sophie Mori, Sebastian Garza, Boris Katz category:cs.CL  published:2016-05-13 summary:We introduce the Treebank of Learner English (TLE), the first publicly available syntactic treebank for English as a Second Language (ESL). The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English. Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language. The treebank is available at universaldependencies.org. The annotation manual used in this project and a graphical query engine are available at esltreebank.org. version:2
arxiv-1603-06598 | Stack-propagation: Improved Representation Learning for Syntax | http://arxiv.org/abs/1603.06598 | id:1603.06598 author:Yuan Zhang, David Weiss category:cs.CL  published:2016-03-21 summary:Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call "stack-propagation". We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model. version:2
arxiv-1602-08761 | Resource Constrained Structured Prediction | http://arxiv.org/abs/1602.08761 | id:1602.08761 author:Tolga Bolukbasi, Kai-Wei Chang, Joseph Wang, Venkatesh Saligrama category:stat.ML cs.CL cs.CV cs.LG  published:2016-02-28 summary:We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation system can be reduced to a series of structured learning problems, resulting in efficient training using existing structured learning algorithms. This framework provides theoretical justification for several existing heuristic approaches found in literature. We evaluate our proposed adaptive system on two structured prediction tasks, optical character recognition (OCR) and dependency parsing and show strong performance in reduction of the feature costs without degrading accuracy. version:2
arxiv-1601-00770 | End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures | http://arxiv.org/abs/1601.00770 | id:1601.00770 author:Makoto Miwa, Mohit Bansal category:cs.CL cs.LG  published:2016-01-05 summary:We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components. version:3
arxiv-1606-02359 | Structure Learning in Graphical Modeling | http://arxiv.org/abs/1606.02359 | id:1606.02359 author:Mathias Drton, Marloes H. Maathuis category:stat.ME stat.ML  published:2016-06-07 summary:A graphical model is a statistical model that is associated to a graph whose nodes correspond to variables of interest. The edges of the graph reflect allowed conditional dependencies among the variables. Graphical models admit computationally convenient factorization properties and have long been a valuable tool for tractable modeling of multivariate distributions. More recently, applications such as reconstructing gene regulatory networks from gene expression data have driven major advances in structure learning, that is, estimating the graph underlying a model. We review some of these advances and discuss methods such as the graphical lasso and neighborhood selection for undirected graphical models (or Markov random fields), and the PC algorithm and score-based search methods for directed graphical models (or Bayesian networks). We further review extensions that account for effects of latent variables and heterogeneous data sources. version:1
arxiv-1606-02355 | Active Long Term Memory Networks | http://arxiv.org/abs/1606.02355 | id:1606.02355 author:Tommaso Furlanello, Jiaping Zhao, Andrew M. Saxe, Laurent Itti, Bosco S. Tjan category:cs.LG cs.AI stat.ML  published:2016-06-07 summary:Continual Learning in artificial neural networks suffers from interference and forgetting when different tasks are learned sequentially. This paper introduces the Active Long Term Memory Networks (A-LTM), a model of sequential multi-task deep learning that is able to maintain previously learned association between sensory input and behavioral output while acquiring knew knowledge. A-LTM exploits the non-convex nature of deep neural networks and actively maintains knowledge of previously learned, inactive tasks using a distillation loss. Distortions of the learned input-output map are penalized but hidden layers are free to transverse towards new local optima that are more favorable for the multi-task objective. We re-frame the McClelland's seminal Hippocampal theory with respect to Catastrophic Inference (CI) behavior exhibited by modern deep architectures trained with back-propagation and inhomogeneous sampling of latent factors across epochs. We present empirical results of non-trivial CI during continual learning in Deep Linear Networks trained on the same task, in Convolutional Neural Networks when the task shifts from predicting semantic to graphical factors and during domain adaptation from simple to complex environments. We present results of the A-LTM model's ability to maintain viewpoint recognition learned in the highly controlled iLab-20M dataset with 10 object categories and 88 camera viewpoints, while adapting to the unstructured domain of Imagenet with 1,000 object categories. version:1
arxiv-1509-05113 | Revealed Preference at Scale: Learning Personalized Preferences from Assortment Choices | http://arxiv.org/abs/1509.05113 | id:1509.05113 author:Nathan Kallus, Madeleine Udell category:stat.ML cs.LG math.OC  published:2015-09-17 summary:We consider the problem of learning the preferences of a heterogeneous population by observing choices from an assortment of products, ads, or other offerings. Our observation model takes a form common in assortment planning applications: each arriving customer is offered an assortment consisting of a subset of all possible offerings; we observe only the assortment and the customer's single choice. In this paper we propose a mixture choice model with a natural underlying low-dimensional structure, and show how to estimate its parameters. In our model, the preferences of each customer or segment follow a separate parametric choice model, but the underlying structure of these parameters over all the models has low dimension. We show that a nuclear-norm regularized maximum likelihood estimator can learn the preferences of all customers using a number of observations much smaller than the number of item-customer combinations. This result shows the potential for structural assumptions to speed up learning and improve revenues in assortment planning and customization. We provide a specialized factored gradient descent algorithm and study the success of the approach empirically. version:2
arxiv-1601-01705 | Learning to Compose Neural Networks for Question Answering | http://arxiv.org/abs/1601.01705 | id:1601.01705 author:Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein category:cs.CL cs.CV cs.NE  published:2016-01-07 summary:We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains. version:4
arxiv-1606-02349 | Locally-Optimized Inter-Subject Alignment of Functional Cortical Regions | http://arxiv.org/abs/1606.02349 | id:1606.02349 author:Marius Cătălin Iordan, Armand Joulin, Diane M. Beck, Li Fei-Fei category:q-bio.NC stat.ML  published:2016-06-07 summary:Inter-subject registration of cortical areas is necessary in functional imaging (fMRI) studies for making inferences about equivalent brain function across a population. However, many high-level visual brain areas are defined as peaks of functional contrasts whose cortical position is highly variable. As such, most alignment methods fail to accurately map functional regions of interest (ROIs) across participants. To address this problem, we propose a locally optimized registration method that directly predicts the location of a seed ROI on a separate target cortical sheet by maximizing the functional correlation between their time courses, while simultaneously allowing for non-smooth local deformations in region topology. Our method outperforms the two most commonly used alternatives (anatomical landmark-based AFNI alignment and cortical convexity-based FreeSurfer alignment) in overlap between predicted region and functionally-defined LOC. Furthermore, the maps obtained using our method are more consistent across subjects than both baseline measures. Critically, our method represents an important step forward towards predicting brain regions without explicit localizer scans and deciphering the poorly understood relationship between the location of functional regions, their anatomical extent, and the consistency of computations those regions perform across people. version:1
arxiv-1606-02346 | How is a data-driven approach better than random choice in label space division for multi-label classification? | http://arxiv.org/abs/1606.02346 | id:1606.02346 author:Piotr Szymański, Tomasz Kajdanowicz, Kristian Kersting category:cs.LG cs.PF cs.SI stat.ML  published:2016-06-07 summary:We propose using five data-driven community detection approaches from social networks to partition the label space for the task of multi-label classification as an alternative to random partitioning into equal subsets as performed by RAkELd: modularity-maximizing fastgreedy and leading eigenvector, infomap, walktrap and label propagation algorithms. We construct a label co-occurence graph (both weighted an unweighted versions) based on training data and perform community detection to partition the label set. We include Binary Relevance and Label Powerset classification methods for comparison. We use gini-index based Decision Trees as the base classifier. We compare educated approaches to label space divisions against random baselines on 12 benchmark data sets over five evaluation measures. We show that in almost all cases seven educated guess approaches are more likely to outperform RAkELd than otherwise in all measures, but Hamming Loss. We show that fastgreedy and walktrap community detection methods on weighted label co-occurence graphs are 85-92% more likely to yield better F1 scores than random partitioning. Infomap on the unweighted label co-occurence graphs is on average 90% of the times better than random paritioning in terms of Subset Accuracy and 89% when it comes to Jaccard similarity. Weighted fastgreedy is better on average than RAkELd when it comes to Hamming Loss. version:1
arxiv-1510-07482 | Edge-Linear First-Order Dependency Parsing with Undirected Minimum Spanning Tree Inference | http://arxiv.org/abs/1510.07482 | id:1510.07482 author:Effi Levi, Roi Reichart, Ari Rappoport category:cs.CL  published:2015-10-26 summary:The run time complexity of state-of-the-art inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). Recently, pruning algorithms for these models have shown to cut a large portion of the graph edges, with minimal damage to the resulting parse trees. Solving the inference problem in run time complexity determined solely by the number of edges (m) is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running in O(n) time. In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very similarly to the original parser that runs an O(n^2) directed MST inference. version:4
arxiv-1606-02344 | Resting state brain networks from EEG: Hidden Markov states vs. classical microstates | http://arxiv.org/abs/1606.02344 | id:1606.02344 author:Tammo Rukat, Adam Baker, Andrew Quinn, Mark Woolrich category:q-bio.NC stat.ML  published:2016-06-07 summary:Functional brain networks exhibit dynamics on the sub-second temporal scale and are often assumed to embody the physiological substrate of cognitive processes. Here we analyse the temporal and spatial dynamics of these states, as measured by EEG, with a hidden Markov model and compare this approach to classical EEG microstate analysis. We find dominating state lifetimes of 100--150\,ms for both approaches. The state topographies show obvious similarities. However, they also feature distinct spatial and especially temporal properties. These differences may carry physiological meaningful information originating from patterns in the data that the HMM is able to integrate while the microstate analysis is not. This hypothesis is supported by a consistently high pairwise correlation of the temporal evolution of EEG microstates which is not observed for the HMM states and which seems unlikely to be a good description of the underlying physiology. However, further investigation is required to determine the robustness and the functional and clinical relevance of EEG HMM states in comparison to EEG microstates. version:1
arxiv-1605-09096 | Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change | http://arxiv.org/abs/1605.09096 | id:1605.09096 author:William L. Hamilton, Jure Leskovec, Dan Jurafsky category:cs.CL  published:2016-05-30 summary:Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change. version:2
arxiv-1606-02321 | Better Conditional Density Estimation for Neural Networks | http://arxiv.org/abs/1606.02321 | id:1606.02321 author:Wesley Tansey, Karl Pichotta, James G. Scott category:stat.ML  published:2016-06-07 summary:The vast majority of the neural network literature focuses on predicting point values for a given set of response variables, conditioned on a feature vector. In many cases we need to model the full joint conditional distribution over the response variables rather than simply making point predictions. In this paper, we present two novel approaches to such conditional density estimation (CDE): Multiscale Nets (MSNs) and CDE Trend Filtering. Multiscale nets transform the CDE regression task into a hierarchical classification task by decomposing the density into a series of half-spaces and learning boolean probabilities of each split. CDE Trend Filtering applies a k-th order graph trend filtering penalty to the unnormalized logits of a multinomial classifier network, with each edge in the graph corresponding to a neighboring point on a discretized version of the density. We compare both methods against plain multinomial classifier networks and mixture density networks (MDNs) on a simulated dataset and three real-world datasets. The results suggest the two methods are complementary: MSNs work well in a high-data-per-feature regime and CDE-TF is well suited for few-samples-per-feature scenarios where overfitting is a primary concern. version:1
arxiv-1512-05702 | Synthesis of recurrent neural networks for dynamical system simulation | http://arxiv.org/abs/1512.05702 | id:1512.05702 author:Adam Trischler, Gabriele MT D'Eleuterio category:cs.NE 68T01  published:2015-12-17 summary:We review several of the most widely used techniques for training recurrent neural networks to approximate dynamical systems, then describe a novel algorithm for this task. The algorithm is based on an earlier theoretical result that guarantees the quality of the network approximation. We show that a feedforward neural network can be trained on the vector field representation of a given dynamical system using backpropagation, then recast, using matrix manipulations, as a recurrent network that replicates the original system's dynamics. After detailing this algorithm and its relation to earlier approaches, we present numerical examples that demonstrate its capabilities. One of the distinguishing features of our approach is that both the original dynamical systems and the recurrent networks that simulate them operate in continuous time. version:2
arxiv-1512-04829 | Feature-Level Domain Adaptation | http://arxiv.org/abs/1512.04829 | id:1512.04829 author:Wouter M. Kouw, Jesse H. Krijthe, Marco Loog, Laurens J. P. van der Maaten category:stat.ML cs.LG  published:2015-12-15 summary:Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real-world problems show that FLDA performs on par with state-of-the-art domain-adaptation techniques. version:2
arxiv-1602-03348 | Iterative Hierarchical Optimization for Misspecified Problems (IHOMP) | http://arxiv.org/abs/1602.03348 | id:1602.03348 author:Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor category:cs.LG cs.AI  published:2016-02-10 summary:For complex, high-dimensional Markov Decision Processes (MDPs), it may be necessary to represent the policy with function approximation. A problem is misspecified whenever, the representation cannot express any policy with acceptable performance. We introduce IHOMP : an approach for solving misspecified problems. IHOMP iteratively learns a set of context specialized options and combines these options to solve an otherwise misspecified problem. Our main contribution is proving that IHOMP enjoys theoretical convergence guarantees. In addition, we extend IHOMP to exploit Option Interruption (OI) enabling it to decide where the learned options can be reused. Our experiments demonstrate that IHOMP can find near-optimal solutions to otherwise misspecified problems and that OI can further improve the solutions. version:2
arxiv-1606-02280 | Semi-Supervised Domain Adaptation for Weakly Labeled Semantic Video Object Segmentation | http://arxiv.org/abs/1606.02280 | id:1606.02280 author:Huiling Wang, Tapani Raiko, Lasse Lensu, Tinghuai Wang, Juha Karhunen category:cs.CV  published:2016-06-07 summary:Deep convolutional neural networks (CNNs) have been immensely successful in many high-level computer vision tasks given large labeled datasets. However, for video semantic object segmentation, a domain where labels are scarce, effectively exploiting the representation power of CNN with limited training data remains a challenge. Simply borrowing the existing pretrained CNN image recognition model for video segmentation task can severely hurt performance. We propose a semi-supervised approach to adapting CNN image recognition model trained from labeled image data to the target domain exploiting both semantic evidence learned from CNN, and the intrinsic structures of video data. By explicitly modeling and compensating for the domain shift from the source domain to the target domain, this proposed approach underpins a robust semantic object segmentation method against the changes in appearance, shape and occlusion in natural videos. We present extensive experiments on challenging datasets that demonstrate the superior performance of our approach compared with the state-of-the-art methods. version:1
arxiv-1606-02279 | Semi-supervised structured output prediction by local linear regression and sub-gradient descent | http://arxiv.org/abs/1606.02279 | id:1606.02279 author:Yihua Zhou, Jingbin Wang, Lihui Shi, Haoxiang Wang, Xin Du, Guilherme Silva category:cs.LG  published:2016-06-07 summary:We propose a novel semi-supervised structured out- put prediction method based on local linear regression in this paper. The existing semi-supervise structured output prediction methods learn a global predictor for all the data points in a data set, which ignores the differences of local distributions of the data set, and the effects to the structured output prediction. To solve this problem, we propose to learn the missing structured outputs and local predictors for neighborhoods of different data points jointly. Using the local linear regression strategy, in the neighborhood of each data point, we propose to learn a local linear predictor by minimizing both the complexity of the predictor and the upper bound of the structured prediction loss. The minimization problem is solved by sub-gradient descent algorithms. We conduct experiments over two benchmark data sets, and the results show the advantages of the proposed method. version:1
arxiv-1602-03351 | Adaptive Skills, Adaptive Partitions (ASAP) | http://arxiv.org/abs/1602.03351 | id:1602.03351 author:Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor category:cs.LG cs.AI stat.ML  published:2016-02-10 summary:We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that (1) learns skills (i.e., temporally extended actions or options) as well as (2) where to apply them. We believe that both (1) and (2) are necessary for a truly general skill learning framework, which is a key building block needed to scale up to lifelong learning agents. The ASAP framework can also solve related new tasks simply by adapting where it applies its existing learned skills. We prove that ASAP converges to a local optimum under natural conditions. Finally, our experimental results, which include a RoboCup domain, demonstrate the ability of ASAP to learn where to reuse skills as well as solve multiple tasks with considerably less experience than solving each task from scratch. version:2
arxiv-1606-02276 | Multilingual Visual Sentiment Concept Matching | http://arxiv.org/abs/1606.02276 | id:1606.02276 author:Nikolaos Pappas, Miriam Redi, Mercan Topkara, Brendan Jou, Hongyi Liu, Tao Chen, Shih-Fu Chang category:cs.CL cs.CV cs.IR cs.MM  published:2016-06-07 summary:The impact of culture in visual emotion perception has recently captured the attention of multimedia research. In this study, we pro- vide powerful computational linguistics tools to explore, retrieve and browse a dataset of 16K multilingual affective visual concepts and 7.3M Flickr images. First, we design an effective crowdsourc- ing experiment to collect human judgements of sentiment connected to the visual concepts. We then use word embeddings to repre- sent these concepts in a low dimensional vector space, allowing us to expand the meaning around concepts, and thus enabling insight about commonalities and differences among different languages. We compare a variety of concept representations through a novel evaluation task based on the notion of visual semantic relatedness. Based on these representations, we design clustering schemes to group multilingual visual concepts, and evaluate them with novel metrics based on the crowdsourced sentiment annotations as well as visual semantic relatedness. The proposed clustering framework enables us to analyze the full multilingual dataset in-depth and also show an application on a facial data subset, exploring cultural in- sights of portrait-related affective visual concepts. version:1
arxiv-1606-02275 | Measuring the reliability of MCMC inference with bidirectional Monte Carlo | http://arxiv.org/abs/1606.02275 | id:1606.02275 author:Roger B. Grosse, Siddharth Ancha, Daniel M. Roy category:cs.LG stat.CO stat.ML  published:2016-06-07 summary:Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We present Bounding Divergences with REverse Annealing (BREAD), a protocol for validating the relevance of simulated data experiments to real datasets, and integrate it into two probabilistic programming languages: WebPPL and Stan. As an example of how BREAD can be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in both WebPPL and Stan. version:1
arxiv-1511-01865 | Convolutional Neural Network for Stereotypical Motor Movement Detection in Autism | http://arxiv.org/abs/1511.01865 | id:1511.01865 author:Nastaran Mohammadian Rad, Andrea Bizzego, Seyed Mostafa Kia, Giuseppe Jurman, Paola Venuti, Cesare Furlanello category:cs.NE cs.CV cs.LG stat.ML  published:2015-11-05 summary:Autism Spectrum Disorders (ASDs) are often associated with specific atypical postural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have a specific visibility. While the identification and the quantification of SMM patterns remain complex, its automation would provide support to accurate tuning of the intervention in the therapy of autism. Therefore, it is essential to develop automatic SMM detection systems in a real world setting, taking care of strong inter-subject and intra-subject variability. Wireless accelerometer sensing technology can provide a valid infrastructure for real-time SMM detection, however such variability remains a problem also for machine learning methods, in particular whenever handcrafted features extracted from accelerometer signal are considered. Here, we propose to employ the deep learning paradigm in order to learn discriminating features from multi-sensor accelerometer signals. Our results provide preliminary evidence that feature learning and transfer learning embedded in the deep architecture achieve higher accurate SMM detectors in longitudinal scenarios. version:3
arxiv-1606-02261 | Reducing the error of Monte Carlo Algorithms by Learning Control Variates | http://arxiv.org/abs/1606.02261 | id:1606.02261 author:Brendan D. Tracey, David H. Wolpert category:stat.ML  published:2016-06-07 summary:Monte Carlo (MC) sampling algorithms are an extremely widely-used technique to estimate expectations of functions f(x), especially in high dimensions. Control variates are a very powerful technique to reduce the error of such estimates, but in their conventional form rely on having an accurate approximation of f, a priori. Stacked Monte Carlo (StackMC) is a recently introduced technique designed to overcome this limitation by fitting a control variate to the data samples themselves. Done naively, forming a control variate to the data would result in overfitting, typically worsening the MC algorithm's performance. StackMC uses in-sample / out-sample techniques to remove this overfitting. Crucially, it is a post-processing technique, requiring no additional samples, and can be applied to data generated by any MC estimator. Our preliminary experiments demonstrated that StackMC improved the estimates of expectations when it was used to post-process samples produces by a "simple sampling" MC estimator. Here we substantially extend this earlier work. We provide an in-depth analysis of the StackMC algorithm, which we use to construct an improved version of the original algorithm, with lower estimation error. We then perform experiments of StackMC on several additional kinds of MC estimators, demonstrating improved performance when the samples are generated via importance sampling, Latin-hypercube sampling and quasi-Monte Carlo sampling. We also show how to extend StackMC to combine multiple fitting functions, and how to apply it to discrete input spaces x. version:1
arxiv-1604-04112 | Deep Residual Networks with Exponential Linear Unit | http://arxiv.org/abs/1604.04112 | id:1604.04112 author:Anish Shah, Eashan Kadam, Hena Shah, Sameer Shinde category:cs.CV  published:2016-04-14 summary:Very deep convolutional neural networks introduced new problems like vanishing gradient and degradation. The recent successful contributions towards solving these problems are Residual and Highway Networks. These networks introduce skip connections that allow the information (from the input or those learned in earlier layers) to flow more into the deeper layers. These very deep models have lead to a considerable decrease in test errors, on benchmarks like ImageNet and COCO. In this paper, we propose the use of exponential linear unit instead of the combination of ReLU and Batch Normalization in Residual Networks. We show that this not only speeds up learning in Residual Networks but also improves the accuracy as the depth increases. It improves the test error on almost all data sets, like CIFAR-10 and CIFAR-100 version:2
arxiv-1606-02254 | Longitudinal Face Modeling via Temporal Deep Restricted Boltzmann Machines | http://arxiv.org/abs/1606.02254 | id:1606.02254 author:Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui category:cs.CV  published:2016-06-07 summary:Modeling the face aging process is a challenging task due to large and non-linear variations present in different stages of face development. This paper presents a deep model approach for face age progression that can efficiently capture the non-linear aging process and automatically synthesize a series of age-progressed faces in various age ranges. In this approach, we first decompose the long-term age progress into a sequence of short-term changes and model it as a face sequence. The Temporal Deep Restricted Boltzmann Machines based age progression model together with the prototype faces are then constructed to learn the aging transformation between faces in the sequence. In addition, to enhance the wrinkles of faces in the later age ranges, the wrinkle models are further constructed using Restricted Boltzmann Machines to capture their variations in different facial regions. The geometry constraints are also taken into account in the last step for more consistent age-progressed results. The proposed approach is evaluated using various face aging databases, i.e. FG-NET, Cross-Age Celebrity Dataset (CACD) and MORPH, and our collected large-scale aging database named AginG Faces in the Wild (AGFW). In addition, when ground-truth age is not available for input image, our proposed system is able to automatically estimate the age of the input face before aging process is employed. version:1
arxiv-1512-05610 | Classification of weak multi-view signals by sharing factors in a mixture of Bayesian group factor analyzers | http://arxiv.org/abs/1512.05610 | id:1512.05610 author:Sami Remes, Tommi Mononen, Samuel Kaski category:stat.ML  published:2015-12-17 summary:We propose a novel classification model for weak signal data, building upon a recent model for Bayesian multi-view learning, Group Factor Analysis (GFA). Instead of assuming all data to come from a single GFA model, we allow latent clusters, each having a different GFA model and producing a different class distribution. We show that sharing information across the clusters, by sharing factors, increases the classification accuracy considerably; the shared factors essentially form a flexible noise model that explains away the part of data not related to classification. Motivation for the setting comes from single-trial functional brain imaging data, having a very low signal-to-noise ratio and a natural multi-view setting, with the different sensors, measurement modalities (EEG, MEG, fMRI) and possible auxiliary information as views. We demonstrate our model on a MEG dataset. version:2
arxiv-1601-01345 | A Sharp Oracle Inequality for Bayesian Non-Negative Matrix Factorization | http://arxiv.org/abs/1601.01345 | id:1601.01345 author:Pierre Alquier, Benjamin Guedj category:stat.ML math.ST stat.TH  published:2016-01-06 summary:The aim of this paper is to provide some theoretical understanding of Bayesian non-negative matrix factorization methods. We prove a sharp oracle inequality for a quasi-Bayesian estimator. This result holds for a very general class of prior distributions and shows how the prior affects the rate of convergence. We illustrate our theoretical results with a short numerical study along with a discussion on existing implementations. version:2
arxiv-1606-02210 | Selective Unsupervised Feature Learning with Convolutional Neural Network (S-CNN) | http://arxiv.org/abs/1606.02210 | id:1606.02210 author:Amir Ghaderi, Vassilis Athitsos category:cs.CV  published:2016-06-07 summary:Supervised learning of convolutional neural networks (CNNs) can require very large amounts of labeled data. Labeling thousands or millions of training examples can be extremely time consuming and costly. One direction towards addressing this problem is to create features from unlabeled data. In this paper we propose a new method for training a CNN, with no need for labeled instances. This method for unsupervised feature learning is then successfully applied to a challenging object recognition task. The proposed algorithm is relatively simple, but attains accuracy comparable to that of more sophisticated methods. The proposed method is significantly easier to train, compared to existing CNN methods, making fewer requirements on manually labeled training data. It is also shown to be resistant to overfitting. We provide results on some well-known datasets, namely STL-10, CIFAR-10, and CIFAR-100. The results show that our method provides competitive performance compared with existing alternative methods. Selective Convolutional Neural Network (S-CNN) is a simple and fast algorithm, it introduces a new way to do unsupervised feature learning, and it provides discriminative features which generalize well. version:1
arxiv-1504-07968 | Learning Contextualized Music Semantics from Tags via a Siamese Network | http://arxiv.org/abs/1504.07968 | id:1504.07968 author:Ubai Sandouk, Ke Chen category:cs.LG I.2.6  published:2015-04-29 summary:Music information retrieval faces a challenge in modeling contextualized musical concepts formulated by a set of co-occurring tags. In this paper, we investigate the suitability of our recently proposed approach based on a Siamese neural network in fighting off this challenge. By means of tag features and probabilistic topic models, the network captures contextualized semantics from tags via unsupervised learning. This leads to a distributed semantics space and a potential solution to the out of vocabulary problem which has yet to be sufficiently addressed. We explore the nature of the resultant music-based semantics and address computational needs. We conduct experiments on three public music tag collections -namely, CAL500, MagTag5K and Million Song Dataset- and compare our approach to a number of state-of-the-art semantics learning approaches. Comparative results suggest that this approach outperforms previous approaches in terms of semantic priming and music tag completion. version:2
arxiv-1606-02206 | A Minimax Approach to Supervised Learning | http://arxiv.org/abs/1606.02206 | id:1606.02206 author:Farzan Farnia, David Tse category:stat.ML cs.IT cs.LG math.IT  published:2016-06-07 summary:Given a task of predicting $Y$ from $X$, a loss function $L$, and a set of probability distributions $\Gamma$, what is the optimal decision rule minimizing the worst-case expected loss over $\Gamma$? In this paper, we address this question by introducing a generalization of the principle of maximum entropy. Applying this principle to sets of distributions with a proposed structure, we develop a general minimax approach for supervised learning problems, that reduces to the maximum likelihood problem over generalized linear models. Through this framework, we develop two classification algorithms called the minimax SVM and the minimax Brier classifier. The minimax SVM, which is a relaxed version of the standard SVM, minimizes the worst-case 0-1 loss over the structured set of distribution, and by our numerical experiments can outperform the SVM. We also explore the application of the developed framework in robust feature selection. version:1
arxiv-1606-02193 | Adapting Sampling Interval of Sensor Networks Using On-Line Reinforcement Learning | http://arxiv.org/abs/1606.02193 | id:1606.02193 author:Gabriel Martins Dias, Maddalena Nurchis, Boris Bellalta category:cs.NI cs.LG cs.SY C.2.4; I.2.1  published:2016-06-07 summary:Monitoring Wireless Sensor Networks (WSNs) are composed by sensor nodes that report temperature, relative humidity and other environmental parameters. The time between two successive measurements is a critical parameter to set during the WSN configuration, because it can impact the WSN's lifetime, the wireless medium contention and the quality of the reported data. As trends in monitored parameters can significantly vary between scenarios and within time, identifying a sampling interval suitable for several cases is also challenging. In this work, we propose a dynamic sampling rate adaptation scheme based on reinforcement learning, able to tune sensors' sampling interval on-the-fly, according to environmental conditions and application requirements. The main goal is to set the sampling interval to the best value possible so as to avoid oversampling and save energy, while not missing environmental changes that can be relevant for the application. In simulations, our mechanism could reduce up to 73% the total number of transmissions compared to a fixed strategy and, simultaneously, keep the average quality of information provided by the WSN. The inherent flexibility of the reinforcement learning algorithm facilitates its use in several scenarios, so as to exploit the broad scope of the Internet of Things. version:1
arxiv-1603-07323 | Learning Mixtures of Plackett-Luce Models | http://arxiv.org/abs/1603.07323 | id:1603.07323 author:Zhibing Zhao, Peter Piech, Lirong Xia category:cs.LG  published:2016-03-23 summary:In this paper we address the identifiability and efficient learning problems of finite mixtures of Plackett-Luce models for rank data. We prove that for any $k\geq 2$, the mixture of $k$ Plackett-Luce models for no more than $2k-1$ alternatives is non-identifiable and this bound is tight for $k=2$. For generic identifiability, we prove that the mixture of $k$ Plackett-Luce models over $m$ alternatives is generically identifiable if $k\leq\lfloor\frac {m-2} 2\rfloor!$. We also propose an efficient generalized method of moments (GMM) algorithm to learn the mixture of two Plackett-Luce models and show that the algorithm is consistent. Our experiments show that our GMM algorithm is significantly faster than the EMM algorithm by Gormley and Murphy (2008), while achieving competitive statistical efficiency. version:3
arxiv-1606-02185 | Towards a Neural Statistician | http://arxiv.org/abs/1606.02185 | id:1606.02185 author:Harrison Edwards, Amos Storkey category:stat.ML cs.LG  published:2016-06-07 summary:An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. version:1
arxiv-1510-05484 | DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection | http://arxiv.org/abs/1510.05484 | id:1510.05484 author:Xi Li, Liming Zhao, Lina Wei, Ming-Hsuan Yang, Fei Wu, Yueting Zhuang, Haibin Ling, Jingdong Wang category:cs.CV  published:2015-10-19 summary:A key problem in salient object detection is how to effectively model the semantic properties of salient objects in a data-driven manner. In this paper, we propose a multi-task deep saliency model based on a fully convolutional neural network (FCNN) with global input (whole raw images) and global output (whole saliency maps). In principle, the proposed saliency model takes a data-driven strategy for encoding the underlying saliency prior information, and then sets up a multi-task learning scheme for exploring the intrinsic correlations between saliency detection and semantic image segmentation. Through collaborative feature learning from such two correlated tasks, the shared fully convolutional layers produce effective features for object perception. Moreover, it is capable of capturing the semantic information on salient objects across different levels using the fully convolutional layers, which investigate the feature-sharing properties of salient object detection with great feature redundancy reduction. Finally, we present a graph Laplacian regularized nonlinear regression model for saliency refinement. Experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches. version:2
arxiv-1606-02170 | Latent Constrained Correlation Filters for Object Localization | http://arxiv.org/abs/1606.02170 | id:1606.02170 author:Shangzhen Luan, Baochang Zhang, Jungong Han, Chen Chen, Ling Shao, Alessandro Perina, Linlin Shen category:cs.CV  published:2016-06-07 summary:There is a neglected fact in the traditional machine learning methods that the data sampling can actually lead to the solution sampling. We consider this observation to be important because having the solution sampling available makes the variable distribution estimation, which is a problem in many learning-related applications, more tractable. In this paper, we implement this idea on correlation filter, which has attracted much attention in the past few years due to its high performance with a low computational cost. More specifically, we propose a new method, named latent constrained correlation filters (LCCF) by mapping the correlation filters to a given latent subspace, in which we establish a new learning framework that embeds distribution-related constraints into the original problem. We further introduce a subspace based alternating direction method of multipliers (SADMM) to efficiently solve the optimization problem, which is proved to converge at the saddle point. Our approach is successfully applied to two different tasks inclduing eye localization and car detection. Extensive experiments demonstrate that LCCF outperforms the state-of-the-art methods when samples are suffered from noise and occlusion. version:1
arxiv-1511-06063 | A Novel Approach for Phase Identification in Smart Grids Using Graph Theory and Principal Component Analysis | http://arxiv.org/abs/1511.06063 | id:1511.06063 author:P Satya Jayadev, Aravind Rajeswaran, Nirav P Bhatt, Ramkrishna Pasumarthy category:cs.LG stat.AP stat.ML  published:2015-11-19 summary:Consumers with low demand, like households, are generally supplied single-phase power by connecting their service mains to one of the phases of a distribution transformer. The distribution companies face the problem of keeping a record of consumer connectivity to a phase due to uninformed changes that happen. The exact phase connectivity information is important for the efficient operation and control of distribution system. We propose a new data driven approach to the problem based on Principal Component Analysis (PCA) and its Graph Theoretic interpretations, using energy measurements in equally timed short intervals, generated from smart meters. We propose an algorithm for inferring phase connectivity from noisy measurements. The algorithm is demonstrated using simulated data for phase connectivities in distribution networks. version:2
arxiv-1606-02147 | ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation | http://arxiv.org/abs/1606.02147 | id:1606.02147 author:Adam Paszke, Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello category:cs.CV  published:2016-06-07 summary:The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18$\times$ faster, requires 75$\times$ less FLOPs, has 79$\times$ less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster. version:1
arxiv-1603-07252 | Neural Summarization by Extracting Sentences and Words | http://arxiv.org/abs/1603.07252 | id:1603.07252 author:Jianpeng Cheng, Mirella Lapata category:cs.CL  published:2016-03-23 summary:Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation. version:2
arxiv-1606-02126 | Supervised Syntax-based Alignment between English Sentences and Abstract Meaning Representation Graphs | http://arxiv.org/abs/1606.02126 | id:1606.02126 author:Chenhui Chu, Sadao Kurohashi category:cs.CL  published:2016-06-07 summary:As alignment links are not given between English sentences and Abstract Meaning Representation (AMR) graphs in the AMR annotation, automatic alignment becomes indispensable for training an AMR parser. Previous studies formalize it as a string-to-string problem, and solve it in an unsupervised way. In this paper, we formalize it as a syntax-based alignment problem, and solve it in a supervised manner based on the syntax trees. Experiments verify the effectiveness of the proposed method. version:1
arxiv-1606-02109 | Efficient differentially private learning improves drug sensitivity prediction | http://arxiv.org/abs/1606.02109 | id:1606.02109 author:Antti Honkela, Mrinal Das, Onur Dikmen, Samuel Kaski category:stat.ML cs.CR cs.LG stat.ME  published:2016-06-07 summary:Users of a personalised recommendation system face a dilemma: recommendations can be improved by learning from data, but only if the other users are willing to share their private information. Good personalised predictions are vitally important in precision medicine, but genomic information on which the predictions are based is also particularly sensitive, as it directly identifies the patients and hence cannot easily be anonymised. Differential privacy has emerged as a potentially promising solution: privacy is considered sufficient if presence of individual patients cannot be distinguished. However, differentially private learning with current methods does not improve predictions with feasible data sizes and dimensionalities. Here we show that useful predictors can be learned under powerful differential privacy guarantees, and even from moderately-sized data sets, by demonstrating significant improvements with a new robust private regression method in the accuracy of private drug sensitivity prediction. The method combines two key properties not present even in recent proposals, which can be generalised to other predictors: we prove it is asymptotically consistently and efficiently private, and demonstrate that it performs well on finite data. Good finite data performance is achieved by limiting the sharing of private information by decreasing the dimensionality and by projecting outliers to fit tighter bounds, therefore needing to add less noise for equal privacy. As already the simple-to-implement method shows promise on the challenging genomic data, we anticipate rapid progress towards practical applications in many fields, such as mobile sensing and social media, in addition to the badly needed precision medicine solutions. version:1
arxiv-1606-02092 | Joint Recursive Monocular Filtering of Camera Motion and Disparity Map | http://arxiv.org/abs/1606.02092 | id:1606.02092 author:Johannes Berger, Christoph Schnörr category:cs.CV math.OC 49J15  published:2016-06-07 summary:Monocular scene reconstruction is essential for modern applications such as robotics or autonomous driving. Although stereo methods usually result in better accuracy than monocular methods, they are more expensive and more difficult to calibrate. In this work, we present a novel second order optimal minimum energy filter that jointly estimates the camera motion, the disparity map and also higher order kinematics recursively on a product Lie group containing a novel disparity group. This mathematical framework enables to cope with non-Euclidean state spaces, non-linear observations and high dimensions which is infeasible for most classical filters. To be robust against outliers, we use a generalized Charbonnier energy function in this framework rather than a quadratic energy function as proposed in related work. Experiments confirm that our method enables accurate reconstructions on-par with state-of-the-art. version:1
arxiv-1603-06076 | Improving Hypernymy Detection with an Integrated Path-based and Distributional Method | http://arxiv.org/abs/1603.06076 | id:1603.06076 author:Vered Shwartz, Yoav Goldberg, Ido Dagan category:cs.CL  published:2016-03-19 summary:Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods, which received less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, that achieves results comparable to distributional methods. We then extend the approach to integrate both path-based and distributional signals, significantly improving upon the state-of-the-art on this task. version:3
arxiv-1606-02077 | Regret Bounds for Non-decomposable Metrics with Missing Labels | http://arxiv.org/abs/1606.02077 | id:1606.02077 author:Prateek Jain, Nagarajan Natarajan category:cs.LG stat.ML  published:2016-06-07 summary:We consider the problem of recommending relevant labels (items) for a given data point (user). In particular, we are interested in the practically important setting where the evaluation is with respect to non-decomposable (over labels) performance metrics like the $F_1$ measure, and the training data has missing labels. To this end, we propose a generic framework that given a performance metric $\Psi$, can devise a regularized objective function and a threshold such that all the values in the predicted score vector above and only above the threshold are selected to be positive. We show that the regret or generalization error in the given metric $\Psi$ is bounded ultimately by estimation error of certain underlying parameters. In particular, we derive regret bounds under three popular settings: a) collaborative filtering, b) multilabel classification, and c) PU (positive-unlabeled) learning. For each of the above problems, we can obtain precise non-asymptotic regret bound which is small even when a large fraction of labels is missing. Our empirical results on synthetic and benchmark datasets demonstrate that by explicitly modeling for missing labels and optimizing the desired performance metric, our algorithm indeed achieves significantly better performance (like $F_1$ score) when compared to methods that do not model missing label information carefully. version:1
arxiv-1603-00173 | Pattern recognition on the quantum Bloch sphere | http://arxiv.org/abs/1603.00173 | id:1603.00173 author:Giuseppe Sergioli, Enrica Santucci, Luca Didaci, Jaroslaw A. Miszczak, Roberto Giuntini category:quant-ph cs.CV  published:2016-03-01 summary:We introduce a framework suitable for describing pattern recognition task using the mathematical language of density matrices. In particular, we provide a one-to-one correspondence between patterns and pure density operators. This correspondence enables us to: i) represent the Nearest Mean Classifier (NMC) in terms of quantum objects, ii) introduce a Quantum Classifier (QC). By comparing the QC with the NMC on different 2D datasets, we show the first classifier can provide additional information that are particularly beneficial on a classical computer with respect to the second classi?er. version:2
arxiv-1606-02074 | Application of the Signature Method to Pattern Recognition in the CEQUEL Clinical Trial | http://arxiv.org/abs/1606.02074 | id:1606.02074 author:A. B. Kormilitzin, K. E. A. Saunders, P. J. Harrison, J. R. Geddes, T. J. Lyons category:stat.AP stat.ML  published:2016-06-07 summary:The classification procedure of streaming data usually requires various ad hoc methods or particular heuristic models. We explore a novel non-parametric and systematic approach to analysis of heterogeneous sequential data. We demonstrate an application of this method to classification of the delays in responding to the prompts, from subjects with bipolar disorder collected during a clinical trial, using both synthetic and real examples. We show how this method can provide a natural and systematic way to extract characteristic features from sequential data. version:1
arxiv-1604-07706 | Distributed Clustering of Linear Bandits in Peer to Peer Networks | http://arxiv.org/abs/1604.07706 | id:1604.07706 author:Nathan Korda, Balazs Szorenyi, Shuai Li category:cs.LG cs.AI stat.ML  published:2016-04-26 summary:We provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, we assume that all the peers are solving the same linear bandit problem, and prove that our algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, we assume that there are clusters of peers solving the same bandit problem within each cluster, and we prove that our algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, we demonstrate the performance of proposed algorithms compared to the state-of-the-art. version:3
arxiv-1605-00405 | Gradient Descent Only Converges to Minimizers: Non-Isolated Critical Points and Invariant Regions | http://arxiv.org/abs/1605.00405 | id:1605.00405 author:Ioannis Panageas, Georgios Piliouras category:math.DS cs.LG  published:2016-05-02 summary:Given a non-convex twice differentiable cost function f, we prove that the set of initial conditions so that gradient descent converges to saddle points where \nabla^2 f has at least one strictly negative eigenvalue has (Lebesgue) measure zero, even for cost functions f with non-isolated critical points, answering an open question in [Lee, Simchowitz, Jordan, Recht, COLT2016]. Moreover, this result extends to forward-invariant convex subspaces, allowing for weak (non-globally Lipschitz) smoothness assumptions. Finally, we produce an upper bound on the allowable step-size. version:2
arxiv-1606-02288 | Enhanced high dynamic range 3D shape measurement based on generalized phase-shifting algorithm | http://arxiv.org/abs/1606.02288 | id:1606.02288 author:Minmin Wang, Guangliang Du, Canlin Zhou, Chaorui Zhang, Shuchun Si, Hui Li, Zhenkun Lei, YanJie Li category:cs.CV physics.optics  published:2016-06-07 summary:It is a challenge for Phase Measurement Profilometry (PMP) to measure objects with a large range of reflectivity variation across the surface. Saturated or dark pixels in the deformed fringe patterns captured by the camera will lead to phase fluctuations and errors. Jiang et al. proposed a high dynamic range real-time 3D shape measurement method without changing camera exposures. Three inverted phase-shifted fringe patterns are used to complement three regular phase-shifted fringe patterns for phase retrieval when any of the regular fringe patterns are saturated. But Jiang's method still has some drawbacks: (1) The phases in saturated pixels are respectively estimated by different formulas for different cases. It is shortage of an universal formula; (2) it cannot be extended to four-step phase-shifting algorithm because inverted fringe patterns are the repetition of regular fringe patterns; (3) only three unsaturated intensity values at every pixel of fringe patterns are chosen for phase demodulation, lying idle the other unsaturated ones. We proposed a method for enhanced high dynamic range 3D shape measurement based on generalized phase-shifting algorithm, which combines the complementary technique of inverted and regular fringe patterns with generalized phase-shifting algorithm. Firstly, two sets of complementary phase-shifted fringe patterns, namely regular and inverted fringe patterns are projected and collected. Then all unsaturated intensity values at the same camera pixel from two sets of fringe patterns are selected, and employed to retrieve the phase by generalized phase-shifting algorithm. Finally, simulations and experiments are conducted to prove the validity of the proposed method. The results are analyzed and compared with Jiang's method, which demonstrate that the proposed method not only expands the scope of Jiang's method, but also improves the measurement accuracy. version:1
arxiv-1602-04427 | Exploiting Lists of Names for Named Entity Identification of Financial Institutions from Unstructured Documents | http://arxiv.org/abs/1602.04427 | id:1602.04427 author:Zheng Xu, Douglas Burdick, Louiqa Raschid category:cs.CL  published:2016-02-14 summary:There is a wealth of information about financial systems that is embedded in document collections. In this paper, we focus on a specialized text extraction task for this domain. The objective is to extract mentions of names of financial institutions, or FI names, from financial prospectus documents, and to identify the corresponding real world entities, e.g., by matching against a corpus of such entities. The tasks are Named Entity Recognition (NER) and Entity Resolution (ER); both are well studied in the literature. Our contribution is to develop a rule-based approach that will exploit lists of FI names for both tasks; our solution is labeled Dict-based NER and Rank-based ER. Since the FI names are typically represented by a root, and a suffix that modifies the root, we use these lists of FI names to create specialized root and suffix dictionaries. To evaluate the effectiveness of our specialized solution for extracting FI names, we compare Dict-based NER with a general purpose rule-based NER solution, ORG NER. Our evaluation highlights the benefits and limitations of specialized versus general purpose approaches, and presents additional suggestions for tuning and customization for FI name extraction. To our knowledge, our proposed solutions, Dict-based NER and Rank-based ER, and the root and suffix dictionaries, are the first attempt to exploit specialized knowledge, i.e., lists of FI names, for rule-based NER and ER. version:2
arxiv-1606-02031 | Hand Action Detection from Ego-centric Depth Sequences with Error-correcting Hough Transform | http://arxiv.org/abs/1606.02031 | id:1606.02031 author:Chi Xu, Lakshmi Narasimhan Govindarajan, Li Cheng category:cs.CV  published:2016-06-07 summary:Detecting hand actions from ego-centric depth sequences is a practically challenging problem, owing mostly to the complex and dexterous nature of hand articulations as well as non-stationary camera motion. We address this problem via a Hough transform based approach coupled with a discriminatively learned error-correcting component to tackle the well known issue of incorrect votes from the Hough transform. In this framework, local parts vote collectively for the start $\&$ end positions of each action over time. We also construct an in-house annotated dataset of 300 long videos, containing 3,177 single-action subsequences over 16 action classes collected from 26 individuals. Our system is empirically evaluated on this real-life dataset for both the action recognition and detection tasks, and is shown to produce satisfactory results. To facilitate reproduction, the new dataset and our implementation are also provided online. version:1
arxiv-1505-02417 | Towards stability and optimality in stochastic gradient descent | http://arxiv.org/abs/1505.02417 | id:1505.02417 author:Panos Toulis, Dustin Tran, Edoardo M. Airoldi category:stat.ME cs.LG stat.CO stat.ML  published:2015-05-10 summary:Iterative procedures for parameter estimation based on stochastic gradient descent allow the estimation to scale to massive data sets. However, in both theory and practice, they suffer from numerical instability. Moreover, they are statistically inefficient as estimators of the true parameter value. To address these two issues, we propose a new iterative procedure termed averaged implicit SGD (AI-SGD). For statistical efficiency, AI-SGD employs averaging of the iterates, which achieves the optimal Cram\'{e}r-Rao bound under strong convexity, i.e., it is an optimal unbiased estimator of the true parameter value. For numerical stability, AI-SGD employs an implicit update at each iteration, which is related to proximal operators in optimization. In practice, AI-SGD achieves competitive performance with other state-of-the-art procedures. Furthermore, it is more stable than averaging procedures that do not employ proximal updates, and is simple to implement as it requires fewer tunable hyperparameters than procedures that do employ proximal updates. version:4
arxiv-1412-7062 | Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs | http://arxiv.org/abs/1412.7062 | id:1412.7062 author:Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille category:cs.CV cs.LG cs.NE  published:2014-12-22 summary:Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU. version:4
arxiv-1606-02012 | Can neural machine translation do simultaneous translation? | http://arxiv.org/abs/1606.02012 | id:1606.02012 author:Kyunghyun Cho, Masha Esipova category:cs.CL  published:2016-06-07 summary:We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation. version:1
arxiv-1606-02009 | Weakly Supervised Change Detection in a Pair of Images | http://arxiv.org/abs/1606.02009 | id:1606.02009 author:Salman H Khan, Xuming He, Mohammed Bennamoun, Fatih Porikli, Ferdous Sohel, Roberto Togneri category:cs.CV  published:2016-06-07 summary:Conventional change detection methods require a large number of images to learn background models. The few recent approaches that attempt change detection between two images either use handcrafted features or depend strongly on tedious pixel-level labeling by humans. In this paper, we present a weakly supervised approach that needs only image-level labels to simultaneously detect and localize changes in a pair of images. To this end, we employ a deep neural network with DAG topology to { learn patterns of change} from image-level labeled training data. On top of the initial CNN activations, we define a CRF model to incorporate the local differences and the dense connections between individual pixels. We apply a constrained mean-field algorithm to estimate the pixel-level labels, and use the estimated labels to update the parameters of the CNN in an iterative EM framework. This enables imposing global constraints on the observed foreground probability mass function. Our evaluations on four large benchmark datasets demonstrate superior detection and localization performance. version:1
arxiv-1606-02006 | Incorporating Discrete Translation Lexicons into Neural Machine Translation | http://arxiv.org/abs/1606.02006 | id:1606.02006 author:Philip Arthur, Graham Neubig, Satoshi Nakamura category:cs.CL  published:2016-06-07 summary:Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time. version:1
arxiv-1606-02003 | Memory-enhanced Decoder for Neural Machine Translation | http://arxiv.org/abs/1606.02003 | id:1606.02003 author:Mingxuan Wang, Zhengdong Lu, Hang Li, Qun Liu category:cs.CL  published:2016-06-07 summary:We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN decoder is called \textsc{MemDec}. At each time during decoding, \textsc{MemDec} will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work\cite{RNNsearch} to store the representation of source sentence, the memory in \textsc{MemDec} is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses, yielding the best performance achieved with the same training set. version:1
arxiv-1606-00819 | Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations | http://arxiv.org/abs/1606.00819 | id:1606.00819 author:Alexandre Salle, Marco Idiart, Aline Villavicencio category:cs.CL  published:2016-06-02 summary:In this paper, we propose LexVec, a new method for generating distributed word representations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent co-occurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks. version:2
arxiv-1605-07912 | Encode, Review, and Decode: Reviewer Module for Caption Generation | http://arxiv.org/abs/1605.07912 | id:1605.07912 author:Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, William W. Cohen category:cs.LG cs.CL cs.CV  published:2016-05-25 summary:We propose a novel module, the reviewer module, to improve the encoder-decoder learning framework. The reviewer module is generic, and can be plugged into an existing encoder-decoder model. The reviewer module performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a fact vector after each review step; the fact vectors are used as the input of the attention mechanism in the decoder. We show that the conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework can improve over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning. version:3
arxiv-1606-01994 | CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases | http://arxiv.org/abs/1606.01994 | id:1606.01994 author:Zihang Dai, Lei Li, Wei Xu category:cs.CL  published:2016-06-07 summary:How can we enable computers to automatically answer questions like "Who created the character Harry Potter"? Carefully built knowledge bases provide rich sources of facts. However, it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question. In particular, we focus on the most common questions --- ones that can be answered with a single fact in the knowledge base. We propose CFO, a Conditional Focused neural-network-based approach to answering factoid questions with knowledge bases. Our approach first zooms in a question to find more probable candidate subject mentions, and infers the final answers with a unified conditional probabilistic framework. Powered by deep recurrent neural networks and neural embeddings, our proposed CFO achieves an accuracy of 75.7% on a dataset of 108k questions - the largest public one to date. It outperforms the current state of the art by an absolute margin of 11.8%. version:1
arxiv-1606-02679 | Learning Power Spectrum Maps from Quantized Power Measurements | http://arxiv.org/abs/1606.02679 | id:1606.02679 author:Daniel Romero, Seung-Jun Kim, Georgios B. Giannakis, Roberto Lopez-Valcarce category:cs.IT cs.LG math.FA math.IT stat.ML  published:2016-06-07 summary:Using power measurements collected by a network of low-cost sensors, power spectral density (PSD) maps are constructed to capture the distribution of RF power across space and frequency. Linearly compressed and quantized power measurements enable wideband sensing at affordable implementation complexity using a small number of bits. Strengths of data- and model-driven approaches are combined to develop estimators capable of incorporating multiple forms of spectral and propagation prior information while fitting the rapid variations of shadow fading across space. To this end, novel nonparametric and semiparametric formulations are investigated. It is shown that the desired PSD maps can be obtained using support vector machine-type solvers. In addition to batch approaches, an online algorithm attuned to real-time operation is developed. Numerical tests assess the performance of the novel algorithms. version:1
arxiv-1606-01990 | Neural Network Models for Implicit Discourse Relation Classification in English and Chinese without Surface Features | http://arxiv.org/abs/1606.01990 | id:1606.01990 author:Attapol T. Rutherford, Vera Demberg, Nianwen Xue category:cs.CL  published:2016-06-07 summary:Inferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Surface features achieve good performance, but they are not readily applicable to other languages without semantic lexicons. Previous neural models require parses, surface features, or a small label set to work well. Here, we propose neural network models that are based on feedforward and long-short term memory architecture without any surface features. To our surprise, our best configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Under various fine-grained label sets and a cross-linguistic setting, our feedforward models perform consistently better or at least just as well as systems that require hand-crafted surface features. Our models present the first neural Chinese discourse parser in the style of Chinese Discourse Treebank, showing that our results hold cross-linguistically. version:1
arxiv-1606-01984 | Expectile and Quantile Matrix Factorization for Extreme Data Analysis | http://arxiv.org/abs/1606.01984 | id:1606.01984 author:Rui Zhu, Di Niu, Linglong Kong, Zongpeng Li category:stat.ML stat.ME  published:2016-06-07 summary:Matrix factorization is a popular approach to matrix estimation based on partial observations. Existing matrix factorization methods are mostly based on least squares and aim to yield a low-rank matrix to interpret conditional sample means. However, in many real applications with extreme data, least squares cannot explain their central tendency or tail distributions, incurring undesired estimates. In this paper, we formulate expectile and quantile matrix factorization problems by introducing expectile or quantile regression into the matrix factorization framework. We propose efficient algorithms based on alternating minimization and iterative reweighted least squares (IRLS) to effectively solve the new formulations. We prove that both algorithms converge to the global optima and exactly recover the true low rank matrices when noise is zero. For synthetic data with skewed noise and a real-world dataset containing web service latencies, our schemes can achieve lower recovery errors and better recommendation performance than the traditional least squares matrix factorization. version:1
arxiv-1606-01584 | Curie: A method for protecting SVM Classifier from Poisoning Attack | http://arxiv.org/abs/1606.01584 | id:1606.01584 author:Ricky Laishram, Vir Virander Phoha category:cs.CR cs.LG  published:2016-06-05 summary:Machine learning is used in a number of security related applications such as biometric user authentication, speaker identification etc. A type of causative integrity attack against machine learning called Poisoning attack works by injecting specially crafted data points in the training data so as to increase the false positive rate of the classifier. In the context of the biometric authentication, this means that more intruders will be classified as valid user, and in case of speaker identification system, user A will be classified user B. In this paper, we examine poisoning attack against SVM and introduce - Curie - a method to protect the SVM classifier from the poisoning attack. The basic idea of our method is to identify the poisoned data points injected by the adversary and filter them out. Our method is light weight and can be easily integrated into existing systems. Experimental results show that it works very well in filtering out the poisoned data. version:2
arxiv-1606-01981 | Deep neural networks are robust to weight binarization and other non-linear distortions | http://arxiv.org/abs/1606.01981 | id:1606.01981 author:Paul Merolla, Rathinakumar Appuswamy, John Arthur, Steve K. Esser, Dharmendra Modha category:cs.NE cs.CV cs.LG  published:2016-06-07 summary:Recent results show that deep neural networks achieve excellent performance even when, during training, weights are quantized and projected to a binary representation. Here, we show that this is just the tip of the iceberg: these same networks, during testing, also exhibit a remarkable robustness to distortions beyond quantization, including additive and multiplicative noise, and a class of non-linear projections where binarization is just a special case. To quantify this robustness, we show that one such network achieves 11% test error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore, we find that a common training heuristic--namely, projecting quantized weights during backpropagation--can be altered (or even removed) and networks still achieve a base level of robustness during testing. Specifically, training with weight projections other than quantization also works, as does simply clipping the weights, both of which have never been reported before. We confirm our results for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas, we propose a stochastic projection rule that leads to a new state of the art network with 7.64% test error on CIFAR-10 using no data augmentation. version:1
arxiv-1606-03063 | Neural computation from first principles: Using the maximum entropy method to obtain an optimal bits-per-joule neuron | http://arxiv.org/abs/1606.03063 | id:1606.03063 author:William B Levy, Toby Berger, Mustafa Sungkar category:q-bio.NC stat.ML  published:2016-06-06 summary:Optimization results are one method for understanding neural computation from Nature's perspective and for defining the physical limits on neuron-like engineering. Earlier work looks at individual properties or performance criteria and occasionally a combination of two, such as energy and information. Here we make use of Jaynes' maximum entropy method and combine a larger set of constraints, possibly dimensionally distinct, each expressible as an expectation. The method identifies a likelihood-function and a sufficient statistic arising from each such optimization. This likelihood is a first-hitting time distribution in the exponential class. Particular constraint sets are identified that, from an optimal inference perspective, justify earlier neurocomputational models. Interactions between constraints, mediated through the inferred likelihood, restrict constraint-set parameterizations, e.g., the energy-budget limits estimation performance which, in turn, matches an axonal communication constraint. Such linkages are, for biologists, experimental predictions of the method. In addition to the related likelihood, at least one type of constraint set implies marginal distributions, and in this case, a Shannon bits/joule statement arises. version:1
arxiv-1601-01280 | Language to Logical Form with Neural Attention | http://arxiv.org/abs/1601.01280 | id:1601.01280 author:Li Dong, Mirella Lapata category:cs.CL  published:2016-01-06 summary:Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations. version:2
arxiv-1606-01933 | A Decomposable Attention Model for Natural Language Inference | http://arxiv.org/abs/1606.01933 | id:1606.01933 author:Ankur P. Parikh, Oscar Täckström, Dipanjan Das, Jakob Uszkoreit category:cs.CL  published:2016-06-06 summary:We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements. version:1
arxiv-1606-01885 | Learning to Optimize | http://arxiv.org/abs/1606.01885 | id:1606.01885 author:Ke Li, Jitendra Malik category:cs.LG cs.AI math.OC stat.ML  published:2016-06-06 summary:Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value. version:1
arxiv-1606-01869 | On Robustness of Kernel Clustering | http://arxiv.org/abs/1606.01869 | id:1606.01869 author:Bowei Yan, Purnamrita Sarkar category:stat.ML  published:2016-06-06 summary:Clustering is one of the most important unsupervised problems in machine learning and statistics. Among many existing algorithms, kernel k-means has drawn much research attention due to its ability to find non-linear cluster boundaries and its inherent simplicity. There are two main approaches for kernel k-means: SVD of the kernel matrix and convex relaxations. Despite the attention kernel clustering has received both from theoretical and applied quarters, not much is known about robustness of the methods. In this paper we first introduce a semidefinite programming relaxation for the kernel clustering problem, then prove that under a suitable model specification, both the K-SVD and SDP approaches are consistent in the limit, albeit SDP is strongly consistent, i.e. achieves exact recovery, whereas K-SVD is weakly consistent, i.e. the fraction of misclassified nodes vanish. version:1
arxiv-1606-01865 | Recurrent Neural Networks for Multivariate Time Series with Missing Values | http://arxiv.org/abs/1606.01865 | id:1606.01865 author:Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, Yan Liu category:cs.LG cs.NE stat.ML  published:2016-06-06 summary:Many multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. It has been noted that the missing patterns and values are often correlated with the target labels, a.k.a., missingness is informative, and there is significant interest to explore methods which model them for time series prediction and other related tasks. In this paper, we develop novel deep learning models based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network, to handle missing observations. Our model takes two representations of missing patterns, i.e., masking and time duration, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to improve the prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-art performance on these tasks and provide useful insights for time series with missing values. version:1
arxiv-1606-01855 | Bayesian Poisson Tucker Decomposition for Learning the Structure of International Relations | http://arxiv.org/abs/1606.01855 | id:1606.01855 author:Aaron Schein, Mingyuan Zhou, David M. Blei, Hanna Wallach category:stat.ML cs.AI cs.LG cs.SI stat.AP  published:2016-06-06 summary:We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling country--country interaction event data. These data consist of interaction events of the form "country $i$ took action $a$ toward country $j$ at time $t$." BPTD discovers overlapping country--community memberships, including the number of latent communities. In addition, it discovers directed community--community interaction networks that are specific to "topics" of action types and temporal "regimes." We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations. version:1
arxiv-1408-0848 | Multilayer bootstrap networks | http://arxiv.org/abs/1408.0848 | id:1408.0848 author:Xiao-Lei Zhang category:cs.LG cs.NE stat.ML  published:2014-08-05 summary:Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from bottom up for unsupervised nonlinear dimensionality reduction. Each layer of the network is a group of k-centers clusterings. Each clustering uses randomly sampled data points with randomly selected features as its centers, and learns a one-of-k encoding by one-nearest-neighbor optimization. Thanks to the binarized encoding, the similarity of two data points is measured by the number of the nearest centers they share in common, which is an adaptive similarity metric in the discrete space that needs no model assumption and parameter tuning. Thanks to the network structure, larger and larger local variations of data are gradually reduced from bottom up. The information loss caused by the binarized encoding is proportional to the correlation of the clusterings, both of which are reduced by the randomization steps. version:7
arxiv-1606-01847 | Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding | http://arxiv.org/abs/1606.01847 | id:1606.01847 author:Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach category:cs.CV cs.AI cs.CL  published:2016-06-06 summary:Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise multiplication or addition, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge. version:1
arxiv-1605-09304 | Synthesizing the preferred inputs for neurons in neural networks via deep generator networks | http://arxiv.org/abs/1605.09304 | id:1605.09304 author:Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff Clune category:cs.NE cs.AI cs.CV cs.LG  published:2016-05-30 summary:Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images). version:3
arxiv-1512-09328 | Homology Computation of Large Point Clouds using Quantum Annealing | http://arxiv.org/abs/1512.09328 | id:1512.09328 author:Raouf Dridi, Hedayat Alghassi category:quant-ph cs.LG  published:2015-12-23 summary:Homology is a tool in topological data analysis which measures the shape of the data. In many cases, these measurements translate into new insights which are not available by other means. To compute homology, we rely on mathematical constructions which scale exponentially with the size of the data. Therefore, for large point clouds, the computation is infeasible using classical computers. In this paper, we present a quantum annealing pipeline for computation of homology of large point clouds. The pipeline takes as input a graph approximating the given point cloud. It uses quantum annealing to compute a clique covering of the graph and then uses this cover to construct a Mayer-Vietoris complex. The pipeline terminates by performing a simplified homology computation of the Mayer-Vietoris complex. We have introduced three different clique coverings and their quantum annealing formulation. Our pipeline scales polynomially in the size of the data, once the covering step is solved. To prove correctness of our algorithm, we have also included tests using D-Wave 2X quantum processor. version:3
arxiv-1606-01822 | ROCS-Derived Features for Virtual Screening | http://arxiv.org/abs/1606.01822 | id:1606.01822 author:Steven Kearnes, Vijay Pande category:stat.ML  published:2016-06-06 summary:Rapid overlay of chemical structures (ROCS) is a standard tool for the calculation of 3D shape and chemical ("color") similarity. ROCS uses unweighted sums to combine many aspects of similarity, yielding parameter-free models for virtual screening. In this report, we decompose the ROCS color force field into \emph{color components} and \emph{color atom overlaps}, novel color similarity features that can be weighted in a system-specific manner by machine learning algorithms. In cross-validation experiments, these additional features significantly improve virtual screening performance relative to standard ROCS. version:1
arxiv-1310-1826 | Learning Non-Parametric Basis Independent Models from Point Queries via Low-Rank Methods | http://arxiv.org/abs/1310.1826 | id:1310.1826 author:Hemant Tyagi, Volkan Cevher category:stat.ML math.NA  published:2013-10-07 summary:We consider the problem of learning multi-ridge functions of the form f(x) = g(Ax) from point evaluations of f. We assume that the function f is defined on an l_2-ball in R^d, g is twice continuously differentiable almost everywhere, and A \in R^{k \times d} is a rank k matrix, where k << d. We propose a randomized, polynomial-complexity sampling scheme for estimating such functions. Our theoretical developments leverage recent techniques from low rank matrix recovery, which enables us to derive a polynomial time estimator of the function f along with uniform approximation guarantees. We prove that our scheme can also be applied for learning functions of the form: f(x) = \sum_{i=1}^{k} g_i(a_i^T x), provided f satisfies certain smoothness conditions in a neighborhood around the origin. We also characterize the noise robustness of the scheme. Finally, we present numerical examples to illustrate the theoretical bounds in action. version:2
arxiv-1603-07421 | On the Powerball Method | http://arxiv.org/abs/1603.07421 | id:1603.07421 author:Ye Yuan, Mu Li, Claire J. Tomlin category:cs.SY cs.LG math.OC  published:2016-03-24 summary:We propose a new method to accelerate the convergence of optimization algorithms. This method simply adds a power coefficient $\gamma\in[0,1)$ to the gradient during optimization. We call this the Powerball method after the well-known Heavy-ball method by Polyak. We analyze the convergence rate for the Powerball method for strongly convex functions and show that it has a faster convergence rate than gradient descent and Newton's method in the initial iterations. We also demonstrate that the Powerball method provides a $10$-fold speed up of the convergence of both gradient descent and L-BFGS on multiple real datasets as well as accelerates the computation for Pagerank vector. version:2
arxiv-1510-05893 | Online Unmixing of Multitemporal Hyperspectral Images accounting for Spectral Variability | http://arxiv.org/abs/1510.05893 | id:1510.05893 author:Pierre-Antoine Thouvenin, Nicolas Dobigeon, Jean-Yves Tourneret category:physics.data-an cs.CV stat.ME  published:2015-10-20 summary:Hyperspectral unmixing is aimed at identifying the reference spectral signatures composing an hyperspectral image and their relative abundance fractions in each pixel. In practice, the identified signatures may vary spectrally from an image to another due to varying acquisition conditions, thus inducing possibly significant estimation errors. Against this background, hyperspectral unmixing of several images acquired over the same area is of considerable interest. Indeed, such an analysis enables the endmembers of the scene to be tracked and the corresponding endmember variability to be characterized. Sequential endmember estimation from a set of hyperspectral images is expected to provide improved performance when compared to methods analyzing the images independently. However, the significant size of hyperspectral data precludes the use of batch procedures to jointly estimate the mixture parameters of a sequence of hyperspectral images. Provided that each elementary component is present in at least one image of the sequence, we propose to perform an online hyperspectral unmixing accounting for temporal endmember variability. The online hyperspectral unmixing is formulated as a two-stage stochastic program, which can be solved using a stochastic approximation. The performance of the proposed method is evaluated on synthetic and real data. A comparison with independent unmixing algorithms finally illustrates the interest of the proposed strategy. version:3
arxiv-1606-01800 | Finite Sample Analysis of Approximate Message Passing | http://arxiv.org/abs/1606.01800 | id:1606.01800 author:Cynthia Rush, Ramji Venkataramanan category:cs.IT math.IT math.ST stat.ML stat.TH  published:2016-06-06 summary:This paper analyzes the performance of Approximate Message Passing (AMP) in the regime where the problem dimension is large but finite. We consider the setting of high-dimensional regression, where the goal is to estimate a high-dimensional vector $\beta_0$ from a noisy measurement $y=A \beta_0 + w$. AMP is a low-complexity, scalable algorithm for this problem. Under suitable assumptions on the measurement matrix $A$, AMP has the attractive feature that its performance can be accurately characterized in the asymptotic large system limit by a simple scalar iteration called state evolution. Previous proofs of the validity of state evolution have all been asymptotic convergence results. In this paper, we derive a concentration result for AMP with i.i.d. Gaussian measurement matrices with finite dimension $n \times N$. The result shows that the probability of deviation from the state evolution prediction falls exponentially in $n$. Our result provides theoretical support for empirical findings that have demonstrated excellent agreement of AMP performance with state evolution predictions for moderately large dimensions. version:1
arxiv-1606-01793 | Low-rank optimization with convex constraints | http://arxiv.org/abs/1606.01793 | id:1606.01793 author:Christian Grussler, Anders Rantzer, Pontus Giselsson category:math.OC cs.LG stat.ML  published:2016-06-06 summary:The problem of low-rank approximation with convex constraints, which often appears in data analysis, image compression and model order reduction, is considered. Given a data matrix, the objective is to find an approximation of desired lower rank that fulfills the convex constraints and minimizes the distance to the data matrix in the Frobenius-norm. The problem of matrix completion can be seen as a special case of this. Today, one of the most widely used techniques is to approximate this non-convex problem using convex nuclear-norm regularization. In many situations, this technique does not give solutions with desirable properties. We instead propose to use the optimal convex minorizer -- the closed convex hull -- of the Frobenius-norm and the rank constraint as a convex proxy. This optimal convex proxy can be combined with other convex constraints to form an optimal convex minorizer of the original non-convex problem. With this approach, we get easily verifiable conditions under which the solutions to the convex relaxation and the original non-convex problem coincide. Several numerical examples are provided for which that is the case. We also see that our proposed convex relaxation consistently performs better than the nuclear norm heuristic, especially in the matrix completion case. The expressibility and computational tractability is of great importance for a convex relaxation. We provide a closed-form expression for the proposed convex approximation, and show how to represent it as a semi-definite program. We also show how to compute the proximal operator of the convex approximation. This allows us to use scalable first-order methods to solve convex approximation problems of large size. version:1
arxiv-1606-01792 | Neural Machine Translation with External Phrase Memory | http://arxiv.org/abs/1606.01792 | id:1606.01792 author:Yaohua Tang, Fandong Meng, Zhengdong Lu, Hang Li, Philip L. H. Yu category:cs.CL  published:2016-06-06 summary:In this paper, we propose phraseNet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. For any given source sentence, phraseNet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. The decoder utilizes a mixture of word-generating component and phrase-generating component, with a specifically designed strategy to generate a sequence of multiple words all at once. The phraseNet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the word-by-word generation mechanism of recurrent neural network. Our empirical study on Chinese-to-English translation shows that, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU improvement over the generic neural machine translator. version:1
arxiv-1605-09673 | Dynamic Filter Networks | http://arxiv.org/abs/1605.09673 | id:1605.09673 author:Bert De Brabandere, Xu Jia, Tinne Tuytelaars, Luc Van Gool category:cs.LG cs.CV  published:2016-05-31 summary:In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation. version:2
arxiv-1606-00318 | Discovering Phase Transitions with Unsupervised Learning | http://arxiv.org/abs/1606.00318 | id:1606.00318 author:Lei Wang category:cond-mat.stat-mech stat.ML  published:2016-06-01 summary:Unsupervised learning is a discipline of machine learning which aims at discovering patterns in big data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many body systems. Starting with raw spin configurations of a prototypical Ising model, we use principal component analysis to extract relevant low dimensional representations the original data and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds out physical concepts such as order parameter and structure factor to be indicators of the phase transition. We discuss future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques. version:2
arxiv-0705-4676 | Recursive n-gram hashing is pairwise independent, at best | http://arxiv.org/abs/0705.4676 | id:0705.4676 author:Daniel Lemire, Owen Kaser category:cs.DB cs.CL H.3.3  H.2.7  published:2007-05-31 summary:Many applications use sequences of n consecutive symbols (n-grams). Hashing these n-grams can be a performance bottleneck. For more speed, recursive hash families compute hash values by updating previous values. We prove that recursive hash families cannot be more than pairwise independent. While hashing by irreducible polynomials is pairwise independent, our implementations either run in time O(n) or use an exponential amount of memory. As a more scalable alternative, we make hashing by cyclic polynomials pairwise independent by ignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials is is twice as fast as hashing by irreducible polynomials. We also show that randomized Karp-Rabin hash families are not pairwise independent. version:8
arxiv-1606-01781 | Very Deep Convolutional Networks for Natural Language Processing | http://arxiv.org/abs/1606.01781 | id:1606.01781 author:Alexis Conneau, Holger Schwenk, Loïc Barrault, Yann Lecun category:cs.CL cs.LG cs.NE  published:2016-06-06 summary:The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP. version:1
arxiv-1605-09046 | TripleSpin - a generic compact paradigm for fast machine learning computations | http://arxiv.org/abs/1605.09046 | id:1605.09046 author:Krzysztof Choromanski, Francois Fagan, Cedric Gouy-Pailler, Anne Morvan, Tamas Sarlos, Jamal Atif category:cs.LG stat.ML  published:2016-05-29 summary:We present a generic compact computational framework relying on structured random matrices that can be applied to speed up several machine learning algorithms with almost no loss of accuracy. The applications include new fast LSH-based algorithms, efficient kernel computations via random feature maps, convex optimization algorithms, quantization techniques and many more. Certain models of the presented paradigm are even more compressible since they apply only bit matrices. This makes them suitable for deploying on mobile devices. All our findings come with strong theoretical guarantees. In particular, as a byproduct of the presented techniques and by using relatively new Berry-Esseen-type CLT for random vectors, we give the first theoretical guarantees for one of the most efficient existing LSH algorithms based on the $\textbf{HD}_{3}\textbf{HD}_{2}\textbf{HD}_{1}$ structured matrix ("Practical and Optimal LSH for Angular Distance"). These guarantees as well as theoretical results for other aforementioned applications follow from the same general theoretical principle that we present in the paper. Our structured family contains as special cases all previously considered structured schemes, including the recently introduced $P$-model. Experimental evaluation confirms the accuracy and efficiency of TripleSpin matrices. version:2
arxiv-1606-01746 | Unsupervised classification of children's bodies using currents | http://arxiv.org/abs/1606.01746 | id:1606.01746 author:Sonia Barahona, Ximo Gual-Arnau, Maria Victoria Ibáñez, Amelia Simó category:stat.ME cs.CV stat.AP  published:2016-06-06 summary:Object classification according to their shape and size is of key importance in many scientific fields. This work focuses on the case where the size and shape of an object is characterized by a current}. A current is a mathematical object which has been proved relevant to the modeling of geometrical data, like submanifolds, through integration of vector fields along them. As a consequence of the choice of a vector-valued Reproducing Kernel Hilbert Space (RKHS) as a test space for integrating manifolds, it is possible to consider that shapes are embedded in this Hilbert Space. A vector-valued RKHS is a Hilbert space of vector fields; therefore, it is possible to compute a mean of shapes, or to calculate a distance between two manifolds. This embedding enables us to consider size-and-shape classification algorithms. These algorithms are applied to a 3D database obtained from an anthropometric survey of the Spanish child population with a potential application to online sales of children's wear. version:1
arxiv-1601-06602 | Expected Similarity Estimation for Large-Scale Batch and Streaming Anomaly Detection | http://arxiv.org/abs/1601.06602 | id:1601.06602 author:Markus Schneider, Wolfgang Ertel, Fabio Ramos category:cs.LG cs.AI  published:2016-01-25 summary:We present a novel algorithm for anomaly detection on very large datasets and data streams. The method, named EXPected Similarity Estimation (EXPoSE), is kernel-based and able to efficiently compute the similarity between new data points and the distribution of regular data. The estimator is formulated as an inner product with a reproducing kernel Hilbert space embedding and makes no assumption about the type or shape of the underlying data distribution. We show that offline (batch) learning with EXPoSE can be done in linear time and online (incremental) learning takes constant time per instance and model update. Furthermore, EXPoSE can make predictions in constant time, while it requires only constant memory. In addition, we propose different methodologies for concept drift adaptation on evolving data streams. On several real datasets we demonstrate that our approach can compete with state of the art algorithms for anomaly detection while being an order of magnitude faster than most other approaches. version:3
arxiv-1606-01735 | Integrated perception with recurrent multi-task neural networks | http://arxiv.org/abs/1606.01735 | id:1606.01735 author:Hakan Bilen, Andrea Vedaldi category:stat.ML cs.CV cs.LG  published:2016-06-06 summary:Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for \emph{all} perceptual problems together, solving them efficiently and coherently in an \emph{integrated manner}. In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call \emph{multinet}, in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation. version:1
arxiv-1606-01721 | Less is More: Micro-expression Recognition from Video using Apex Frame | http://arxiv.org/abs/1606.01721 | id:1606.01721 author:Sze-Teng Liong, John See, Raphael Chung-Wei Phan, KokSheik Wong category:cs.CV  published:2016-06-06 summary:Despite recent interest and advances in facial micro-expression research, there is still plenty room for improvement in terms of micro-expression recognition. Conventional feature extraction approaches for micro-expression video consider either the whole video sequence or a part of it, for representation. However, with the high-speed video capture of micro-expressions (100-200 fps), are all frames necessary to provide a sufficiently meaningful representation? Is the luxury of data a bane to accurate recognition? A novel proposition is presented in this paper, whereby we utilize only two images per video: the apex frame and the onset frame. The apex frame of a video contains the highest intensity of expression changes among all frames, while the onset is the perfect choice of a reference frame with neutral expression. A new feature extractor, Bi-Weighted Oriented Optical Flow (Bi-WOOF) is proposed to encode essential expressiveness of the apex frame. We evaluated the proposed method on four micro-expression databases: CASME II, SMIC-HS, SMIC-NIR and SMIC-VIS. Our experiments lend credence to our hypothesis, with our proposed technique achieving a state-of-the-art F1-score recognition performance of 61% and 62% in the high frame rate CASME II and SMIC-HS databases respectively. version:1
arxiv-1602-03779 | Network of Bandits | http://arxiv.org/abs/1602.03779 | id:1602.03779 author:Raphaël Féraud category:cs.AI cs.DC cs.LG  published:2016-02-11 summary:The distribution of machine learning tasks on the user's devices offers several advantages for application purposes: scalability, reduction of deployment costs and privacy. We propose a basic brick, Distributed Median Elimination, which can be used to distribute the best arm identification task in various schemes. In comparison to Median Elimination run on a single player, we showed a near optimal speed-up factor. This speed-up factor is reached with a near optimal communication cost. Experiments illustrate and complete the analysis: in comparison to Median Elimination with unconstrained communication cost, the distributed version shows practical improvements. version:7
arxiv-1606-01720 | Proof nets for the Displacement calculus | http://arxiv.org/abs/1606.01720 | id:1606.01720 author:Richard Moot category:cs.LO cs.CL  published:2016-06-06 summary:We present a proof net calculus for the Displacement calculus and show its correctness. This is the first proof net calculus which models the Displacement calculus directly and not by some sort of translation into another formalism. The proof net calculus opens up new possibilities for parsing and proof search with the Displacement calculus. version:1
arxiv-1604-07759 | F-measure Maximization in Multi-Label Classification with Conditionally Independent Label Subsets | http://arxiv.org/abs/1604.07759 | id:1604.07759 author:Maxime Gasse, Alex Aussem category:cs.LG  published:2016-04-26 summary:We discuss a method to improve the exact F-measure maximization algorithm called GFM, proposed in (Dembczynski et al. 2011) for multi-label classification, assuming the label set can be can partitioned into conditionally independent subsets given the input features. If the labels were all independent, the estimation of only $m$ parameters ($m$ denoting the number of labels) would suffice to derive Bayes-optimal predictions in $O(m^2)$ operations. In the general case, $m^2+1$ parameters are required by GFM, to solve the problem in $O(m^3)$ operations. In this work, we show that the number of parameters can be reduced further to $m^2/n$, in the best case, assuming the label set can be partitioned into $n$ conditionally independent subsets. As this label partition needs to be estimated from the data beforehand, we use first the procedure proposed in (Gasse et al. 2015) that finds such partition and then infer the required parameters locally in each label subset. The latter are aggregated and serve as input to GFM to form the Bayes-optimal prediction. We show on a synthetic experiment that the reduction in the number of parameters brings about significant benefits in terms of performance. version:2
arxiv-1606-01700 | Gated Word-Character Recurrent Language Model | http://arxiv.org/abs/1606.01700 | id:1606.01700 author:Yasumasa Miyamoto, Kyunghyun Cho category:cs.CL  published:2016-06-06 summary:We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and word-level inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora. version:1
arxiv-1606-01651 | Feedforward Initialization for Fast Inference of Deep Generative Networks is biologically plausible | http://arxiv.org/abs/1606.01651 | id:1606.01651 author:Yoshua Bengio, Benjamin Scellier, Olexa Bilaniuk, Joao Sacramento, Walter Senn category:cs.LG cs.NE q-bio.NC  published:2016-06-06 summary:We consider deep multi-layered generative models such as Boltzmann machines or Hopfield nets in which computation (which implements inference) is both recurrent and stochastic, but where the recurrence is not to model sequential structure, only to perform computation. We find conditions under which a simple feedforward computation is a very good initialization for inference, after the input units are clamped to observed values. It means that after the feedforward initialization, the recurrent network is very close to a fixed point of the network dynamics, where the energy gradient is 0. The main condition is that consecutive layers form a good auto-encoder, or more generally that different groups of inputs into the unit (in particular, bottom-up inputs on one hand, top-down inputs on the other hand) are consistent with each other, producing the same contribution into the total weighted sum of inputs. In biological terms, this would correspond to having each dendritic branch correctly predicting the aggregate input from all the dendritic branches, i.e., the soma potential. This is consistent with the prediction that the synaptic weights into dendritic branches such as those of the apical and basal dendrites of pyramidal cells are trained to minimize the prediction error made by the dendritic branch when the target is the somatic activity. Whereas previous work has shown how to achieve fast negative phase inference (when the model is unclamped) in a predictive recurrent model, this contribution helps to achieve fast positive phase inference (when the target output is clamped) in such recurrent neural models. version:1
arxiv-1602-03040 | The Structured Weighted Violations Perceptron Algorithm | http://arxiv.org/abs/1602.03040 | id:1602.03040 author:Rotem Dror, Roi Reichart category:cs.LG  published:2016-02-09 summary:We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a new structured prediction algorithm that generalizes the Collins Structured Perceptron (CSP). Unlike CSP, the update rule of SWVP explicitly exploits the internal structure of the predicted labels. We prove the convergence of SWVP for linearly separable training sets, provide mistake and generalization bounds, and show that in the general case these bounds are tighter than those of the CSP special case. In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results. version:2
arxiv-1603-04525 | Pushing the Limits of Deep CNNs for Pedestrian Detection | http://arxiv.org/abs/1603.04525 | id:1603.04525 author:Qichang Hu, Peng Wang, Chunhua Shen, Anton van den Hengel, Fatih Porikli category:cs.CV  published:2016-03-15 summary:Compared to other applications in computer vision, convolutional neural networks have under-performed on pedestrian detection. A breakthrough was made very recently by using sophisticated deep CNN models, with a number of hand-crafted features, or explicit occlusion handling mechanism. In this work, we show that by re-using the convolutional feature maps (CFMs) of a deep convolutional neural network (DCNN) model as image features to train an ensemble of boosted decision models, we are able to achieve the best reported accuracy without using specially designed learning algorithms. We empirically identify and disclose important implementation details. We also show that pixel labelling may be simply combined with a detector to boost the detection performance. By adding complementary hand-crafted features such as optical flow, the DCNN based detector can be further improved. We set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from $11.7\%$ to $8.9\%$, a relative improvement of $24\%$. We also achieve a comparable result to the state-of-the-art approaches on the KITTI dataset. version:2
arxiv-1606-01621 | Photo Aesthetics Ranking Network with Attributes and Content Adaptation | http://arxiv.org/abs/1606.01621 | id:1606.01621 author:Shu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, Charless Fowlkes category:cs.CV cs.IR cs.MM  published:2016-06-06 summary:Real-world applications could benefit from the ability to automatically generate a fine-grained ranking of photo aesthetics. However, previous methods for image aesthetics analysis have primarily focused on the coarse, binary categorization of images into high- or low-aesthetic categories. In this work, we propose to learn a deep convolutional neural network to rank photo aesthetics in which the relative ranking of photo aesthetics are directly modeled in the loss function. Our model incorporates joint learning of meaningful photographic attributes and image content information which can help regularize the complicated photo aesthetics rating problem. To train and analyze this model, we have assembled a new aesthetics and attributes database (AADB) which contains aesthetic scores and meaningful attributes assigned to each image by multiple human raters. Anonymized rater identities are recorded across images allowing us to exploit intra-rater consistency using a novel sampling strategy when computing the ranking loss of training image pairs. We show the proposed sampling strategy is very effective and robust in face of subjective judgement of image aesthetics by individuals with different aesthetic tastes. Experiments demonstrate that our unified model can generate aesthetic rankings that are more consistent with human ratings. To further validate our model, we show that by simply thresholding the estimated aesthetic scores, we are able to achieve state-or-the-art classification performance on the existing AVA dataset benchmark. version:1
arxiv-1606-01614 | Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification | http://arxiv.org/abs/1606.01614 | id:1606.01614 author:Xilun Chen, Ben Athiwaratkun, Yu Sun, Kilian Weinberger, Claire Cardie category:cs.CL  published:2016-06-06 summary:In recent years deep neural networks have achieved great success in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most other languages do not enjoy such an abundance of annotated data for sentiment analysis. To combat this problem, we propose the Adversarial Deep Averaging Network (ADAN) to transfer sentiment knowledge learned from labeled English data to low-resource languages where only unlabeled data exists. ADAN is a "Y-shaped" network with two discriminative branches: a sentiment classifier and an adversarial language predictor. Both branches take input from a feature extractor that aims to learn hidden representations that capture the underlying sentiment of the text and are invariant across languages. Experiments on Chinese sentiment classification demonstrate that ADAN significantly outperforms several baselines, including a strong pipeline approach that relies on Google Translate, the state-of-the-art commercial machine translation system. version:1
arxiv-1511-08407 | The Mechanism of Additive Composition | http://arxiv.org/abs/1511.08407 | id:1511.08407 author:Ran Tian, Naoaki Okazaki, Kentaro Inui category:cs.CL cs.LG  published:2015-11-26 summary:Additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010) is a widely used method for computing meanings of phrases, which takes the average of vector representations of the constituent words. In this article, we prove an upper bound for the bias of additive composition, which is the first theoretical analysis on compositional frameworks from a machine learning point of view. Our proof relies on properties of natural language data that are empirically verified, and can be theoretically derived from an assumption that the data is generated from a Hierarchical Pitman-Yor Process. The theory endorses additive composition as a reasonable operation for calculating meanings of phrases, and suggests ways to improve additive compositionality, including: transforming entries of distributional word vectors by a function that meets a specific condition, constructing a novel type of vector representations to make additive composition sensitive to word order, and utilizing singular value decomposition to train word vectors. version:3
arxiv-1606-01603 | Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution | http://arxiv.org/abs/1606.01603 | id:1606.01603 author:Ting Liu, Yiming Cui, Qingyu Yin, Shijin Wang, Weinan Zhang, Guoping Hu category:cs.CL  published:2016-06-06 summary:Most existing approaches for zero pronoun resolution are supervised approaches, where annotated data are released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in zero pronoun resolution task. The existing approaches mainly face the challenge of costing manpower on labeling the extended data for better training performance and domain adaption. To alleviate the problem above, in this paper we propose a simple but novel approach to automatically produce large-scale pseudo training data for zero pronoun resolution. Furthermore, to avoid the drawbacks of the feature engineering based approaches, we proposed an attention-based LSTM model for this task. Experimental results show that our proposed approach outperforms the state-of-the-art methods significantly with an absolute improvement of 5.1% F-score in OntoNotes 5.0 corpus. version:1
arxiv-1606-01601 | shapeDTW: shape Dynamic Time Warping | http://arxiv.org/abs/1606.01601 | id:1606.01601 author:Jiaping Zhao, Laurent Itti category:cs.CV  published:2016-06-06 summary:Dynamic Time Warping (DTW) is an algorithm to align temporal sequences with possible local non-linear distortions, and has been widely applied to audio, video and graphics data alignments. DTW is essentially a point-to-point matching method under some boundary and temporal consistency constraints. Although DTW obtains a global optimal solution, it does not necessarily achieve locally sensible matchings. Concretely, two temporal points with entirely dissimilar local structures may be matched by DTW. To address this problem, we propose an improved alignment algorithm, named shape Dynamic Time Warping (shapeDTW), which enhances DTW by taking point-wise local structural information into consideration. shapeDTW is inherently a DTW algorithm, but additionally attempts to pair locally similar structures and to avoid matching points with distinct neighborhood structures. We apply shapeDTW to align audio signal pairs having ground-truth alignments, as well as artificially simulated pairs of aligned sequences, and obtain quantitatively much lower alignment errors than DTW and its two variants. When shapeDTW is used as a distance measure in a nearest neighbor classifier (NN-shapeDTW) to classify time series, it beats DTW on 64 out of 84 UCR time series datasets, with significantly improved classification accuracies. By using a properly designed local structure descriptor, shapeDTW improves accuracies by more than 10% on 18 datasets. To the best of our knowledge, shapeDTW is the first distance measure under the nearest neighbor classifier scheme to significantly outperform DTW, which had been widely recognized as the best distance measure to date. Our code is publicly accessible at: https://github.com/jiapingz/shapeDTW. version:1
arxiv-1606-01595 | Deep Linear Discriminant Analysis on Fisher Networks: A Hybrid Architecture for Person Re-identification | http://arxiv.org/abs/1606.01595 | id:1606.01595 author:Lin Wu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2016-06-06 summary:Person re-identification is to seek a correct match for a person of interest across views among a large number of imposters. It typically involves two procedures of non-linear feature extractions against dramatic appearance changes, and subsequent discriminative analysis in order to reduce intra- personal variations while enlarging inter-personal differences. In this paper, we introduce a hybrid architecture which combines Fisher vectors and deep neural networks to learn non-linear representations of person images to a space where data can be linearly separable. We reinforce a Linear Discriminant Analysis (LDA) on top of the deep neural network such that linearly separable latent representations can be learnt in an end-to-end fashion. By optimizing an objective function modified from LDA, the network is enforced to produce feature distributions which have a low variance within the same class and high variance between classes. The objective is essentially derived from the general LDA eigenvalue problem and allows to train the network with stochastic gradient descent and back-propagate LDA gradients to compute the gradients involved in Fisher vector encoding. For evaluation we test our approach on four benchmark data sets in person re-identification (VIPeR [1], CUHK03 [2], CUHK01 [3], and Market1501 [4]). Extensive experiments on these benchmarks show that our model can achieve state-of-the-art results. version:1
arxiv-1502-05477 | Trust Region Policy Optimization | http://arxiv.org/abs/1502.05477 | id:1502.05477 author:John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel category:cs.LG  published:2015-02-19 summary:We describe a iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters. version:4
arxiv-1504-01013 | Efficient piecewise training of deep structured models for semantic segmentation | http://arxiv.org/abs/1504.01013 | id:1504.01013 author:Guosheng Lin, Chunhua Shen, Anton van dan Hengel, Ian Reid category:cs.CV  published:2015-04-04 summary:Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information; specifically, we explore `patch-patch' context between image regions, and `patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset. version:4
arxiv-1606-01587 | A Deep-Learning Approach for Operation of an Automated Realtime Flare Forecast | http://arxiv.org/abs/1606.01587 | id:1606.01587 author:Yuko Hada-Muranushi, Takayuki Muranushi, Ayumi Asai, Daisuke Okanohara, Rudy Raymond, Gentaro Watanabe, Shigeru Nemoto, Kazunari Shibata category:astro-ph.SR cs.LG  published:2016-06-06 summary:Automated forecasts serve important role in space weather science, by providing statistical insights to flare-trigger mechanisms, and by enabling tailor-made forecasts and high-frequency forecasts. Only by realtime forecast we can experimentally measure the performance of flare-forecasting methods while confidently avoiding overlearning. We have been operating unmanned flare forecast service since August, 2015 that provides 24-hour-ahead forecast of solar flares, every 12 minutes. We report the method and prediction results of the system. version:1
arxiv-1606-01583 | Semi-Supervised Learning with Generative Adversarial Networks | http://arxiv.org/abs/1606.01583 | id:1606.01583 author:Augustus Odena category:stat.ML cs.LG  published:2016-06-05 summary:We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN. version:1
arxiv-1605-07866 | DeepCut: Object Segmentation from Bounding Box Annotations using Convolutional Neural Networks | http://arxiv.org/abs/1605.07866 | id:1605.07866 author:Martin Rajchl, Matthew C. H. Lee, Ozan Oktay, Konstantinos Kamnitsas, Jonathan Passerat-Palmbach, Wenjia Bai, Mellisa Damodaram, Mary A. Rutherford, Joseph V. Hajnal, Bernhard Kainz, Daniel Rueckert category:cs.CV  published:2016-05-25 summary:In this paper, we propose DeepCut, a method to obtain pixelwise object segmentations given an image dataset labelled with bounding box annotations. It extends the approach of the well-known GrabCut method to include machine learning by training a neural network classifier from bounding box annotations. We formulate the problem as an energy minimisation problem over a densely-connected conditional random field and iteratively update the training targets to obtain pixelwise object segmentations. Additionally, we propose variants of the DeepCut method and compare those to a naive approach to CNN training under weak supervision. We test its applicability to solve brain and lung segmentation problems on a challenging fetal magnetic resonance dataset and obtain encouraging results in terms of accuracy. version:2
arxiv-1606-01568 | Active-Labelling by Adaptive Huber Loss Regression | http://arxiv.org/abs/1606.01568 | id:1606.01568 author:Jacopo Cavazza, Vittorio Murino category:cs.LG cs.CV  published:2016-06-05 summary:This paper addresses the scalar regression problem presenting a solution for optimizing the Huber loss in a general semi-supervised setting, which combines multi-view learning and manifold regularization. To this aim, we propose a principled algorithm to 1) avoid computationally expensive iterative solutions while 2) adapting the Huber loss threshold in a data-driven fashion and 3) actively balancing the use of labelled data to remove noisy or inconsistent annotations from the training stage. In a wide experimental evaluation, dealing with diverse applications, we assess the superiority of our paradigm which is able to combine strong performance and robustness to noise at a low computational cost. version:1
arxiv-1606-01561 | Shallow Networks for High-Accuracy Road Object-Detection | http://arxiv.org/abs/1606.01561 | id:1606.01561 author:Khalid Ashraf, Bichen Wu, Forrest N. Iandola, Mattthew W. Moskewicz, Kurt Keutzer category:cs.CV  published:2016-06-05 summary:The ability to automatically detect other vehicles on the road is vital to the safety of partially-autonomous and fully-autonomous vehicles. Most of the high-accuracy techniques for this task are based on R-CNN or one of its faster variants. In the research community, much emphasis has been applied to using 3D vision or complex R-CNN variants to achieve higher accuracy. However, are there more straightforward modifications that could deliver higher accuracy? Yes. We show that increasing input image resolution (i.e. upsampling) offers up to 12 percentage-points higher accuracy compared to an off-the-shelf baseline. We also find situations where earlier/shallower layers of CNN provide higher accuracy than later/deeper layers. We further show that shallow models and upsampled images yield competitive accuracy. Our findings contrast with the current trend towards deeper and larger models to achieve high accuracy in domain specific detection tasks. version:1
arxiv-1606-01552 | View-tolerant face recognition and Hebbian learning imply mirror-symmetric neural tuning to head orientation | http://arxiv.org/abs/1606.01552 | id:1606.01552 author:Joel Z. Leibo, Qianli Liao, Winrich Freiwald, Fabio Anselmi, Tomaso Poggio category:cs.NE q-bio.NC  published:2016-06-05 summary:The primate brain contains a hierarchy of visual areas, dubbed the ventral stream, which rapidly computes object representations that are both specific for object identity and relatively robust against identity-preserving transformations like depth-rotations. Current computational models of object recognition, including recent deep learning networks, generate these properties through a hierarchy of alternating selectivity-increasing filtering and tolerance-increasing pooling operations, similar to simple-complex cells operations. While simulations of these models recapitulate the ventral stream's progression from early view-specific to late view-tolerant representations, they fail to generate the most salient property of the intermediate representation for faces found in the brain: mirror-symmetric tuning of the neural population to head orientation. Here we prove that a class of hierarchical architectures and a broad set of biologically plausible learning rules can provide approximate invariance at the top level of the network. While most of the learning rules do not yield mirror-symmetry in the mid-level representations, we characterize a specific biologically-plausible Hebb-type learning rule that is guaranteed to generate mirror-symmetric tuning to faces tuning at intermediate levels of the architecture. version:1
arxiv-1606-01550 | Pairwise Quantization | http://arxiv.org/abs/1606.01550 | id:1606.01550 author:Artem Babenko, Relja Arandjelović, Victor Lempitsky category:cs.CV cs.IR  published:2016-06-05 summary:We consider the task of lossy compression of high-dimensional vectors through quantization. We propose the approach that learns quantization parameters by minimizing the distortion of scalar products and squared distances between pairs of points. This is in contrast to previous works that obtain these parameters through the minimization of the reconstruction error of individual points. The proposed approach proceeds by finding a linear transformation of the data that effectively reduces the minimization of the pairwise distortions to the minimization of individual reconstruction errors. After such transformation, any of the previously-proposed quantization approaches can be used. Despite the simplicity of this transformation, the experiments demonstrate that it achieves considerable reduction of the pairwise distortions compared to applying quantization directly to the untransformed data. version:1
arxiv-1606-01549 | Gated-Attention Readers for Text Comprehension | http://arxiv.org/abs/1606.01549 | id:1606.01549 author:Bhuwan Dhingra, Hanxiao Liu, William W. Cohen, Ruslan Salakhutdinov category:cs.CL cs.LG  published:2016-06-05 summary:In this paper we study the problem of answering cloze-style questions over short documents. We introduce a new attention mechanism which uses multiplicative interactions between the query embedding and intermediate states of a recurrent neural network reader. This enables the reader to build query-specific representations of tokens in the document which are further used for answer selection. Our model, the Gated-Attention Reader, outperforms all state-of-the-art models on several large-scale benchmark datasets for this task---the CNN \& Dailymail news stories and Children's Book Test. We also provide a detailed analysis of the performance of our model and several baselines over a subset of questions manually annotated with certain linguistic features. The analysis sheds light on the strengths and weaknesses of several existing models. version:1
arxiv-1606-01545 | Neural Net Models for Open-Domain Discourse Coherence | http://arxiv.org/abs/1606.01545 | id:1606.01545 author:Jiwei Li, Dan Jurafsky category:cs.CL  published:2016-06-05 summary:Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) and are trained on narrow domains. We introduce algorithms that capture diverse kinds of coherence by learning to distinguish coherent from incoherent discourse from vast amounts of open-domain training data. We propose two models, one discriminative and one generative, both using LSTMs as the backbone. The discriminative model treats windows of sentences from original human-generated articles as coherent examples and windows generated by randomly replacing sentences as incoherent examples. The generative model is a \sts model that estimates the probability of generating a sentence given its contexts. Our models achieve state-of-the-art performance on multiple coherence evaluations. Qualitative analysis suggests that our generative model captures many aspects of coherence including lexical, temporal, causal, and entity-based coherence. version:1
arxiv-1606-01540 | OpenAI Gym | http://arxiv.org/abs/1606.01540 | id:1606.01540 author:Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba category:cs.LG cs.AI  published:2016-06-05 summary:OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software. version:1
arxiv-1503-01868 | Total Variation Regularized Tensor RPCA for Background Subtraction from Compressive Measurements | http://arxiv.org/abs/1503.01868 | id:1503.01868 author:Wenfei Cao, Yao Wang, Jian Sun, Deyu Meng, Can Yang, Andrzej Cichocki, Zongben Xu category:cs.CV  published:2015-03-06 summary:Background subtraction has been a fundamental and widely studied task in video analysis, with a wide range of applications in video surveillance, teleconferencing and 3D modeling. Recently, motivated by compressive imaging, background subtraction from compressive measurements (BSCM) is becoming an active research task in video surveillance. In this paper, we propose a novel tensor-based robust PCA (TenRPCA) approach for BSCM by decomposing video frames into backgrounds with spatial-temporal correlations and foregrounds with spatio-temporal continuity in a tensor framework. In this approach, we use 3D total variation (TV) to enhance the spatio-temporal continuity of foregrounds, and Tucker decomposition to model the spatio-temporal correlations of video background. Based on this idea, we design a basic tensor RPCA model over the video frames, dubbed as the holistic TenRPCA model (H-TenRPCA). To characterize the correlations among the groups of similar 3D patches of video background, we further design a patch-group-based tensor RPCA model (PG-TenRPCA) by joint tensor Tucker decompositions of 3D patch groups for modeling the video background. Efficient algorithms using alternating direction method of multipliers (ADMM) are developed to solve the proposed models. Extensive experiments on simulated and real-world videos demonstrate the superiority of the proposed approaches over the existing state-of-the-art approaches. version:4
arxiv-1606-01535 | What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures? | http://arxiv.org/abs/1606.01535 | id:1606.01535 author:Kevin Jarrett, Koray Kvukcuoglu, Karol Gregor, Yann LeCun category:cs.CV  published:2016-06-05 summary:(This paper was written in November 2011 and never published. It is posted on arXiv.org in its original form in June 2016). Many recent object recognition systems have proposed using a two phase training procedure to learn sparse convolutional feature hierarchies: unsupervised pre-training followed by supervised fine-tuning. Recent results suggest that these methods provide little improvement over purely supervised systems when the appropriate nonlinearities are included. This paper presents an empirical exploration of the space of learning procedures for sparse convolutional networks to assess which method produces the best performance. In our study, we introduce an augmentation of the Predictive Sparse Decomposition method that includes a discriminative term (DPSD). We also introduce a new single phase supervised learning procedure that places an L1 penalty on the output state of each layer of the network. This forces the network to produce sparse codes without the expensive pre-training phase. Using DPSD with a new, complex predictor that incorporates lateral inhibition, combined with multi-scale feature pooling, and supervised refinement, the system achieves a 70.6\% recognition rate on Caltech-101. With the addition of convolutional training, a 77\% recognition was obtained on the CIfAR-10 dataset. version:1
arxiv-1603-02194 | Gaussian Process Regression for Out-of-Sample Extension | http://arxiv.org/abs/1603.02194 | id:1603.02194 author:Oren Barkan, Jonathan Weill, Amir Averbuch category:cs.LG cs.CV  published:2016-03-07 summary:Manifold learning methods are useful for high dimensional data analysis. Many of the existing methods produce a low dimensional representation that attempts to describe the intrinsic geometric structure of the original data. Typically, this process is computationally expensive and the produced embedding is limited to the training data. In many real life scenarios, the ability to produce embedding of unseen samples is essential. In this paper we propose a Bayesian non-parametric approach for out-of-sample extension. The method is based on Gaussian Process Regression and independent of the manifold learning algorithm. Additionally, the method naturally provides a measure for the degree of abnormality for a newly arrived data point that did not participate in the training process. We derive the mathematical connection between the proposed method and the Nystrom extension and show that the latter is a special case of the former. We present extensive experimental results that demonstrate the performance of the proposed method and compare it to other existing out-of-sample extension methods. version:2
arxiv-1603-06571 | Bayesian Neural Word Embedding | http://arxiv.org/abs/1603.06571 | id:1603.06571 author:Oren Barkan category:cs.CL cs.LG  published:2016-03-21 summary:Recently, several works in the domain of natural language processing presented successful methods for word embedding. Among them, the Skip-Gram (SG) with negative sampling, known also as word2vec, advanced the state-of-the-art of various linguistics tasks. In this paper, we propose a scalable Bayesian neural word embedding algorithm that can be beneficial to general item similarity tasks as well. The algorithm relies on a Variational Bayes solution for the SG objective and a detailed step by step description of the algorithm is provided. We present experimental results that demonstrate the performance of the proposed algorithm and show it is competitive with the original SG method. version:2
arxiv-1606-01530 | Adaptive Submodular Ranking | http://arxiv.org/abs/1606.01530 | id:1606.01530 author:Fatemeh Navidi, Prabhanjan Kambadur, Viswanath Nagarajan category:cs.DS cs.LG  published:2016-06-05 summary:We study a general adaptive ranking problem where an algorithm needs to perform a sequence of actions on a random user, drawn from a known distribution, so as to "satisfy" the user as early as possible. The satisfaction of each user is captured by an individual submodular function, where the user is said to be satisfied when the function value goes above some threshold. We obtain a logarithmic factor approximation algorithm for this adaptive ranking problem, which is the best possible. The adaptive ranking problem has many applications in active learning and ranking: it significantly generalizes previously-studied problems such as optimal decision trees, equivalence class determination, decision region determination and submodular cover. We also present some preliminary experimental results based on our algorithm. version:1
arxiv-1606-01519 | A Deep Learning Approach to Block-based Compressed Sensing of Images | http://arxiv.org/abs/1606.01519 | id:1606.01519 author:Amir Adler, David Boublil, Michael Elad, Michael Zibulevsky category:cs.CV  published:2016-06-05 summary:Compressed sensing (CS) is a signal processing framework for efficiently reconstructing a signal from a small number of measurements, obtained by linear projections of the signal. Block-based CS is a lightweight CS approach that is mostly suitable for processing very high-dimensional images and videos: it operates on local patches, employs a low-complexity reconstruction operator and requires significantly less memory to store the sensing matrix. In this paper we present a deep learning approach for block-based CS, in which a fully-connected network performs both the block-based linear sensing and non-linear reconstruction stages. During the training phase, the sensing matrix and the non-linear reconstruction operator are \emph{jointly} optimized, and the proposed approach outperforms state-of-the-art both in terms of reconstruction quality and computation time. For example, at a 25% sensing rate the average PSNR advantage is 0.77dB and computation time is over 200-times faster. version:1
arxiv-1606-01515 | Coordination in Categorical Compositional Distributional Semantics | http://arxiv.org/abs/1606.01515 | id:1606.01515 author:Dimitri Kartsaklis category:cs.CL cs.AI math.CT  published:2016-06-05 summary:An open problem with categorical compositional distributional semantics is the representation of words that are considered semantically vacuous from a distributional perspective, such as determiners, prepositions, relative pronouns or coordinators. This paper deals with the topic of coordination between identical syntactic types, which accounts for the majority of coordination cases in language. By exploiting the compact closed structure of the underlying category and Frobenius operators canonically induced over the fixed basis of finite-dimensional vector spaces, we provide a morphism as representation of a coordinator tensor, and we show how it lifts from atomic types to compound types. Linguistic intuitions are provided, and the importance of the Frobenius operators as an addition to the compact closed setting with regard to language is discussed. version:1
arxiv-1602-00985 | Mental State Recognition via Wearable EEG | http://arxiv.org/abs/1602.00985 | id:1602.00985 author:Pouya Bashivan, Irina Rish, Steve Heisig category:cs.CV cs.HC  published:2016-02-02 summary:The increasing quality and affordability of consumer electroencephalogram (EEG) headsets make them attractive for situations where medical grade devices are impractical. Predicting and tracking cognitive states is possible for tasks that were previously not conducive to EEG monitoring. For instance, monitoring operators for states inappropriate to the task (e.g. drowsy drivers), tracking mental health (e.g. anxiety) and productivity (e.g. tiredness) are among possible applications for the technology. Consumer grade EEG headsets are affordable and relatively easy to use, but they lack the resolution and quality of signal that can be achieved using medical grade EEG devices. Thus, the key questions remain: to what extent are wearable EEG devices capable of mental state recognition, and what kind of mental states can be accurately recognized with these devices? In this work, we examined responses to two different types of input: instructional (logical) versus recreational (emotional) videos, using a range of machine-learning methods. We tried SVMs, sparse logistic regression, and Deep Belief Networks, to discriminate between the states of mind induced by different types of video input, that can be roughly labeled as logical vs. emotional. Our results demonstrate a significant potential of wearable EEG devices in differentiating cognitive states between situations with large contextual but subtle apparent differences. version:2
arxiv-1605-05396 | Generative Adversarial Text to Image Synthesis | http://arxiv.org/abs/1605.05396 | id:1605.05396 author:Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee category:cs.NE cs.CV  published:2016-05-17 summary:Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions. version:2
arxiv-1606-01487 | Bounds for Vector-Valued Function Estimation | http://arxiv.org/abs/1606.01487 | id:1606.01487 author:Andreas Maurer, Massimiliano Pontil category:stat.ML cs.LG  published:2016-06-05 summary:We present a framework to derive risk bounds for vector-valued learning with a broad class of feature maps and loss functions. Multi-task learning and one-vs-all multi-category learning are treated as examples. We discuss in detail vector-valued functions with one hidden layer, and demonstrate that the conditions under which shared representations are beneficial for multi- task learning are equally applicable to multi-category learning. version:1
arxiv-1606-01484 | Relaxation of the EM Algorithm via Quantum Annealing | http://arxiv.org/abs/1606.01484 | id:1606.01484 author:Hideyuki Miyahara, Koji Tsumura category:stat.ML cond-mat.stat-mech math.ST physics.comp-ph quant-ph stat.TH  published:2016-06-05 summary:The EM algorithm is a novel numerical method to obtain maximum likelihood estimates and is often used for practical calculations. However, many of maximum likelihood estimation problems are nonconvex, and it is known that the EM algorithm fails to give the optimal estimate by being trapped by local optima. In order to deal with this difficulty, we propose a deterministic quantum annealing EM algorithm by introducing the mathematical mechanism of quantum fluctuations into the conventional EM algorithm because quantum fluctuations induce the tunnel effect and are expected to relax the difficulty of nonconvex optimization problems in the maximum likelihood estimation problems. We show a theorem that guarantees its convergence and give numerical experiments to verify its efficiency. version:1
arxiv-1606-01481 | Better Image Segmentation by Exploiting Dense Semantic Predictions | http://arxiv.org/abs/1606.01481 | id:1606.01481 author:Qiyang Zhao, Lewis D Griffin category:cs.CV  published:2016-06-05 summary:It is well accepted that image segmentation can benefit from utilizing multilevel cues. The paper focuses on utilizing the FCNN-based dense semantic predictions in the bottom-up image segmentation, arguing to take semantic cues into account from the very beginning. By this we can avoid merging regions of similar appearance but distinct semantic categories as possible. The semantic inefficiency problem is handled. We also propose a straightforward way to use the contour cues to suppress the noise in multilevel cues, thus to improve the segmentation robustness. The evaluation on the BSDS500 shows that we obtain the competitive region and boundary performance. Furthermore, since all individual regions can be assigned with appropriate semantic labels during the computation, we are capable of extracting the adjusted semantic segmentations. The experiment on Pascal VOC 2012 shows our improvement to the original semantic segmentations which derives directly from the dense predictions. version:1
arxiv-1605-09080 | Beyond LDA: A Unified Framework for Learning Latent Normalized Infinitely Divisible Topic Models through Spectral Methods | http://arxiv.org/abs/1605.09080 | id:1605.09080 author:Forough Arabshahi, Animashree Anandkumar category:cs.LG stat.ML  published:2016-05-30 summary:In this paper we propose guaranteed spectral methods for learning a broad range of topic models, which generalize the popular Latent Dirichlet Allocation (LDA). We overcome the limitation of LDA to incorporate arbitrary topic correlations, by assuming that the hidden topic proportions are drawn from a flexible class of Normalized Infinitely Divisible (NID) distributions. NID distributions are generated through the process of normalizing a family of independent Infinitely Divisible (ID) random variables. The Dirichlet distribution is a special case obtained by normalizing a set of Gamma random variables. We prove that this flexible topic model class can be learnt via spectral methods using only moments up to the third order, with (low order) polynomial sample and computational complexity. The proof is based on a key new technique derived here that allows us to diagonalize the moments of the NID distribution through an efficient procedure that requires evaluating only univariate integrals, despite the fact that we are handling high dimensional multivariate moments. version:3
arxiv-1606-01473 | Statistical Inference for Algorithmic Leveraging | http://arxiv.org/abs/1606.01473 | id:1606.01473 author:Katelyn Gao category:stat.AP stat.ML  published:2016-06-05 summary:The age of big data has produced data sets that are computationally expensive to analyze. To deal with such large-scale data sets, the method of algorithmic leveraging proposes that we sample according to some special distribution, rescale the data, and then perform analysis on the smaller sample. Ma, Mahoney, and Yu (2015) provides a framework to determine the statistical properties of algorithmic leveraging in the context of estimating the regression coefficients in a linear model with a fixed number of predictors. In this paper, we discuss how to perform statistical inference on regression coefficients estimated using algorithmic leveraging. In particular, we show how to construct confidence intervals for each estimated coefficient and present an efficient algorithm for doing so when the error variance is known. Through simulations, we confirm that our procedure controls the type I errors of significance tests for the regression coefficients and show that it has good power for those tests. version:1
arxiv-1411-2738 | word2vec Parameter Learning Explained | http://arxiv.org/abs/1411.2738 | id:1411.2738 author:Xin Rong category:cs.CL  published:2014-11-11 summary:The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models. This note provides detailed derivations and explanations of the parameter update equations of the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization techniques, including hierarchical softmax and negative sampling. Intuitive interpretations of the gradient equations are also provided alongside mathematical derivations. In the appendix, a review on the basics of neuron networks and backpropagation is provided. I also created an interactive demo, wevi, to facilitate the intuitive understanding of the model. version:4
arxiv-1606-00119 | Latent Contextual Bandits: A Non-Negative Matrix Factorization Approach | http://arxiv.org/abs/1606.00119 | id:1606.00119 author:Rajat Sen, Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G. Dimakis, Sanjay Shakkottai category:cs.LG cs.SY stat.ML  published:2016-06-01 summary:We consider the stochastic contextual bandit problem with a large number of observed contexts and arms, but with a latent low-dimensional structure across contexts. This low dimensional(latent) structure encodes the fact that both the observed contexts and the mean rewards from the arms are convex mixtures of a small number of underlying latent contexts. At each time, we are presented with an observed context; the bandit problem is to determine the corresponding arm to pull in order to minimize regret. Assuming a separable and low rank latent context vs. mean-reward} matrix, we employ non-negative matrix factorization(NMF) techniques on sub-sampled estimates of matrix entries (estimates constructed from careful arm sampling) to efficiently discover the underlying factors. This estimation lies at the core of our proposed $\epsilon$-greedy NMF-Bandit algorithm that switches between arm exploration to reconstruct the reward matrix, and exploitation of arms using the reconstructed matrix in order to minimize regret. We identify singular value conditions on the non-negative factors under which the NMF-Bandit algorithm has $\mathcal{O}(L\text{poly}(m,\log K)\log{T})$ regret where $L$ is the number of observed contexts, $K$ is the number of arms, and $m$ is the number of latent contexts. We further propose a class of generative models that satisfy our sufficient conditions, and derive a lower bound that matches our achievable bounds up to a $\mathrm{poly}(m,\log K)$ factor. Finally, we validate the NMF-bandit algorithm on synthetic data-sets. version:2
arxiv-1606-01460 | Nighttime Haze Removal with Illumination Correction | http://arxiv.org/abs/1606.01460 | id:1606.01460 author:Jing Zhang, Yang Cao, Zengfu Wang category:cs.CV  published:2016-06-05 summary:Haze removal is important for computational photography and computer vision applications. However, most of the existing methods for dehazing are designed for daytime images, and cannot always work well in the nighttime. Different from the imaging conditions in the daytime, images captured in nighttime haze condition may suffer from non-uniform illumination due to artificial light sources, which exhibit low brightness/contrast and color distortion. In this paper, we present a new nighttime hazy imaging model that takes into account both the non-uniform illumination from artificial light sources and the scattering and attenuation effects of haze. Accordingly, we propose an efficient dehazing algorithm for nighttime hazy images. The proposed algorithm includes three sequential steps. i) It enhances the overall brightness by performing a gamma correction step after estimating the illumination from the original image. ii) Then it achieves a color-balance result by performing a color correction step after estimating the color characteristics of the incident light. iii) Finally, it remove the haze effect by applying the dark channel prior and estimating the point-wise environmental light based on the previous illumination-balance result. Experimental results show that the proposed algorithm can achieve illumination-balance and haze-free results with good color rendition ability. version:1
arxiv-1602-01168 | Learning Discriminative Features via Label Consistent Neural Network | http://arxiv.org/abs/1602.01168 | id:1602.01168 author:Zhuolin Jiang, Yaming Wang, Larry Davis, Walt Andrews, Viktor Rozgic category:cs.CV cs.LG cs.MM cs.NE stat.ML  published:2016-02-03 summary:Deep Convolutional Neural Networks (CNN) enforces supervised information only at the output layer, and hidden layers are trained by back propagating the prediction error from the output layer without explicit supervision. We propose a supervised feature learning approach, Label Consistent Neural Network, which enforces direct supervision in late hidden layers. We associate each neuron in a hidden layer with a particular class label and encourage it to be activated for input signals from the same class. More specifically, we introduce a label consistency regularization called "discriminative representation error" loss for late hidden layers and combine it with classification error loss to build our overall objective function. This label consistency constraint alleviates the common problem of gradient vanishing and tends to faster convergence; it also makes the features derived from late hidden layers discriminative enough for classification even using a simple $k$-NN classifier, since input signals from the same class will have very similar representations. Experimental results demonstrate that our approach achieves state-of-the-art performances on several public benchmarks for action and object category recognition. version:2
arxiv-1606-01455 | Multimodal Residual Learning for Visual QA | http://arxiv.org/abs/1606.01455 | id:1606.01455 author:Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang category:cs.CV  published:2016-06-05 summary:Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods. However, applications of these methods to multimodality remain limited. We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning. Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information. The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies. Various alternative models introduced by multimodality are explored based on our study. We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information. version:1
arxiv-1606-01453 | An Interactive Medical Image Segmentation Framework Using Iterative Refinement | http://arxiv.org/abs/1606.01453 | id:1606.01453 author:Pratik Kalshetti, Manas Bundele, Parag Rahangdale, Dinesh Jangra, Chiranjoy Chattopadhyay, Gaurav Harit, Abhay Elhence category:cs.CV cs.HC  published:2016-06-05 summary:Image segmentation is often performed on medical images for identifying diseases in clinical evaluation. Hence it has become one of the major research areas. Conventional image segmentation techniques are unable to provide satisfactory segmentation results for medical images as they contain irregularities. They need to be pre-processed before segmentation. In order to obtain the most suitable method for medical image segmentation, we propose a two stage algorithm. The first stage automatically generates a binary marker image of the region of interest using mathematical morphology. This marker serves as the mask image for the second stage which uses GrabCut on the input image thus resulting in an efficient segmented result. The obtained result can be further refined by user interaction which can be done using the Graphical User Interface (GUI). Experimental results show that the proposed method is accurate and provides satisfactory segmentation results with minimum user interaction on medical as well as natural images. version:1
arxiv-1604-00727 | Character-Level Question Answering with Attention | http://arxiv.org/abs/1604.00727 | id:1604.00727 author:David Golub, Xiaodong He category:cs.CL cs.AI cs.LG  published:2016-04-04 summary:We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing. version:4
arxiv-1606-01433 | Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint Inference for Clinical Temporal Information Extraction | http://arxiv.org/abs/1606.01433 | id:1606.01433 author:Jason Alan Fries category:cs.CL  published:2016-06-04 summary:We submitted two systems to the SemEval-2016 Task 12: Clinical TempEval challenge, participating in Phase 1, where we identified text spans of time and event expressions in clinical notes and Phase 2, where we predicted a relation between an event and its parent document creation time. For temporal entity extraction, we find that a joint inference-based approach using structured prediction outperforms a vanilla recurrent neural network that incorporates word embeddings trained on a variety of large clinical document sets. For document creation time relations, we find that a combination of date canonicalization and distant supervision rules for predicting relations on both events and time expressions improves classification, though gains are limited, likely due to the small scale of training data. version:1
arxiv-1606-01412 | Distance Metric Ensemble Learning and the Andrews-Curtis Conjecture | http://arxiv.org/abs/1606.01412 | id:1606.01412 author:Krzysztof Krawiec, Jerry Swan category:cs.AI cs.DM cs.LG I.2.8; I.2.6  published:2016-06-04 summary:Motivated by the search for a counterexample to the Poincar\'e conjecture in three and four dimensions, the Andrews-Curtis conjecture was proposed in 1965. It is now generally suspected that the Andrews-Curtis conjecture is false, but small potential counterexamples are not so numerous, and previous work has attempted to eliminate some via combinatorial search. Progress has however been limited, with the most successful approach (breadth-first-search using secondary storage) being neither scalable nor heuristically-informed. A previous empirical analysis of problem structure examined several heuristic measures of search progress and determined that none of them provided any useful guidance for search. In this article, we induce new quality measures directly from the problem structure and combine them to produce a more effective search driver via ensemble machine learning. By this means, we eliminate 19 potential counterexamples, the status of which had been unknown for some years. version:1
arxiv-1603-09326 | Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index | http://arxiv.org/abs/1603.09326 | id:1603.09326 author:Susan Athey, Raj Chetty, Guido Imbens, Hyunseung Kang category:stat.ME stat.ML  published:2016-03-30 summary:Estimating the long-term effects of treatments is of interest in many fields. A common challenge in estimating such treatment effects is that long-term outcomes are unobserved in the time frame needed to make policy decisions. One approach to overcome this missing data problem is to analyze treatments effects on an intermediate outcome, often called a statistical surrogate, if it satisfies the condition that treatment and outcome are independent conditional on the statistical surrogate. The validity of the surrogacy condition is often controversial. Here we exploit that fact that in modern datasets, researchers often observe a large number, possibly hundreds or thousands, of intermediate outcomes, thought to lie on or close to the causal chain between the treatment and the long-term outcome of interest. Even if none of the individual proxies satisfies the statistical surrogacy criterion by itself, using multiple proxies can be useful in causal inference. We focus primarily on a setting with two samples, an experimental sample containing data about the treatment indicator and the surrogates and an observational sample containing information about the surrogates and the primary outcome. We state assumptions under which the average treatment effect be identified and estimated with a high-dimensional vector of proxies that collectively satisfy the surrogacy assumption, and derive the bias from violations of the surrogacy assumption, and show that even if the primary outcome is also observed in the experimental sample, there is still information to be gained from using surrogates. version:2
arxiv-1606-01404 | Generating Natural Language Inference Chains | http://arxiv.org/abs/1606.01404 | id:1606.01404 author:Vladyslav Kolesnyk, Tim Rocktäschel, Sebastian Riedel category:cs.CL cs.AI cs.NE  published:2016-06-04 summary:The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82% of generated sentences are correct, an improvement of 10.3% over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence. version:1
arxiv-1410-7690 | Trend Filtering on Graphs | http://arxiv.org/abs/1410.7690 | id:1410.7690 author:Yu-Xiang Wang, James Sharpnack, Alex Smola, Ryan J. Tibshirani category:stat.ML cs.AI cs.LG stat.ME 62G05  published:2014-10-28 summary:We introduce a family of adaptive estimators on graphs, based on penalizing the $\ell_1$ norm of discrete graph differences. This generalizes the idea of trend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual $\ell_2$-based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through examples and theory. version:5
arxiv-1512-01818 | SentiBench - A Benchmark Comparison of State-of-the-Practice Sentiment Analysis Methods | http://arxiv.org/abs/1512.01818 | id:1512.01818 author:Filipe Nunes Ribeiro, Matheus Araújo, Pollyanna Gonçalves, Fabrício Benevenuto, Marcos André Gonçalves category:cs.CL cs.SI  published:2015-12-06 summary:In the last few years thousands of scientific papers have investigated sentiment analysis, several startups that measure opinions on real data have emerged and a number of innovative products related to this theme have been developed. There are multiple methods for measuring sentiments, including lexical-based and supervised machine learning methods. Despite the vast interest on the theme and wide popularity of some methods, it is unclear which one is better for identifying the polarity (i.e., positive or negative) of a message. Accordingly, there is a strong need to conduct a thorough apple-to-apple comparison of sentiment analysis methods, \textit{as they are used in practice}, across multiple datasets originated from different data sources. Such a comparison is key for understanding the potential limitations, advantages, and disadvantages of popular methods. This article aims at filling this gap by presenting a benchmark comparison of twenty-four popular sentiment analysis methods (which we call the state-of-the-practice methods). Our evaluation is based on a benchmark of eighteen labeled datasets, covering messages posted on social networks, movie and product reviews, as well as opinions and comments in news articles. Our results highlight the extent to which the prediction performance of these methods varies considerably across datasets. Aiming at boosting the development of this research area, we open the methods' codes and datasets used in this article, deploying them in a benchmark system, which provides an open API for accessing and comparing sentence-level sentiment analysis methods. version:4
arxiv-1606-01393 | Automated Image Captioning for Rapid Prototyping and Resource Constrained Environments | http://arxiv.org/abs/1606.01393 | id:1606.01393 author:Karan Sharma, Arun CS Kumar, Suchendra Bhandarkar category:cs.CV  published:2016-06-04 summary:Significant performance gains in deep learning coupled with the exponential growth of image and video data on the Internet have resulted in the recent emergence of automated image captioning systems. Ensuring scalability of automated image captioning systems with respect to the ever increasing volume of image and video data is a significant challenge. This paper provides a valuable insight in that the detection of a few significant (top) objects in an image allows one to extract other relevant information such as actions (verbs) in the image. We expect this insight to be useful in the design of scalable image captioning systems. We address two parameters by which the scalability of image captioning systems could be quantified, i.e., the traditional algorithmic time complexity which is important given the resource limitations of the user device and the system development time since the programmers' time is a critical resource constraint in many real-world scenarios. Additionally, we address the issue of how word embeddings could be used to infer the verb (action) from the nouns (objects) in a given image in a zero-shot manner. Our results show that it is possible to attain reasonably good performance on predicting actions and captioning images using our approaches with the added advantage of simplicity of implementation. version:1
arxiv-1606-01377 | Regularization on Image Patches: a linear reconstruction from manifold embedding | http://arxiv.org/abs/1606.01377 | id:1606.01377 author:Rujie Yin, Tingran Gao, Yue Lu, Ingrid Daubechies category:cs.CV I.4.4; I.4.10  published:2016-06-04 summary:Inspired by the recent low dimension manifold model (LDMM), which achieves high performance in imaging inpainting,we consider regularization on image patches from their low dimensional embedding. We provide a framework for patch-based signal decomposition by frames generated from the embedding coordinates and from patch bases in a convolution form. For any given embedding of patches (or more generally a set of data points in high dimensional space), the energy concentration of the coefficients of such frames characterizes the optimal patch basis, with respect to which the linear reconstruction from the embedding of the image has minimum error. This leads to a weighted $\ell_2$-regularization on coefficients in the decomposition of an image with respect to this optimal frame; we use this to extend the original LDMM to a re-weighted LDMM that achieves better inpainting results. Our framework can be generalized to the setting of a union of local embedding, and we show that the state-of-the-art denoising algorithm BM3D fits in our framework. version:1
arxiv-1603-06765 | Fully Convolutional Attention Localization Networks: Efficient Attention Localization for Fine-Grained Recognition | http://arxiv.org/abs/1603.06765 | id:1603.06765 author:Xiao Liu, Tian Xia, Jiang Wang, Yuanqing Lin category:cs.CV  published:2016-03-22 summary:Fine-grained recognition is challenging mainly because the inter-class differences between fine-grained classes are usually local and subtle while intra-class differences could be large due to pose variations. In order to distinguish them from intra-class variations, it is essential to zoom in on highly discriminative local regions. In this work, we introduce a reinforcement learning-based fully convolutional attention localization network to adaptively select multiple task-driven visual attention regions. We show that zooming in on the selected attention regions significantly improves the performance of fine-grained recognition. Compared to previous reinforcement learning-based models, the proposed approach is noticeably more computationally efficient during both training and testing because of its fully-convolutional architecture, and it is capable of simultaneous focusing its glimpse on multiple visual attention regions. The experiments demonstrate that the proposed method achieves notably higher classification accuracy on three benchmark fine-grained recognition datasets: Stanford Dogs, Stanford Cars, and CUB-200-2011. version:2
arxiv-1506-03365 | LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop | http://arxiv.org/abs/1506.03365 | id:1506.03365 author:Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, Jianxiong Xiao category:cs.CV  published:2015-06-10 summary:While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset. version:3
arxiv-1606-01341 | Neural Architectures for Fine-grained Entity Type Classification | http://arxiv.org/abs/1606.01341 | id:1606.01341 author:Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, Sebastian Riedel category:cs.CL  published:2016-06-04 summary:In this work, we investigate several neural network architectures for fine-grained entity type classification. Particularly, we consider extensions to a recently proposed attentive neural architecture and make three key contributions. Previous work on attentive neural architectures do not consider hand-crafted features, we combine learnt and hand-crafted features and observe that they complement each other. Additionally, through quantitative analysis we establish that the attention mechanism is capable of learning to attend over syntactic heads and the phrase containing the mention, where both are known strong hand-crafted features for our task. We enable parameter sharing through a hierarchical label encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data. We establish that the choice of training data has a drastic impact on performance, with decreases by as much as 9.85% loose micro F1 score for a previously proposed method. Despite this, our best model achieves state-of-the-art results with 75.36% loose micro F1 score on the well- established FIGER (GOLD) dataset. version:1
arxiv-1406-5614 | PAC-Bayes Analysis of Multi-view Learning | http://arxiv.org/abs/1406.5614 | id:1406.5614 author:Shiliang Sun, John Shawe-Taylor, Liang Mao category:cs.LG cs.AI stat.ML  published:2014-06-21 summary:This paper presents eight PAC-Bayes bounds to analyze the generalization performance of multi-view classifiers. These bounds adopt data dependent Gaussian priors which emphasize classifiers with high view agreements. The center of the prior for the first two bounds is the origin, while the center of the prior for the third and fourth bounds is given by a data dependent vector. An important technique to obtain these bounds is two derived logarithmic determinant inequalities whose difference lies in whether the dimensionality of data is involved. The centers of the fifth and sixth bounds are calculated on a separate subset of the training set. The last two bounds use unlabeled data to represent view agreements and are thus applicable to semi-supervised multi-view learning. We evaluate all the presented multi-view PAC-Bayes bounds on benchmark data and compare them with previous single-view PAC-Bayes bounds. The usefulness and performance of the multi-view bounds are discussed. version:2
arxiv-1511-06038 | Neural Variational Inference for Text Processing | http://arxiv.org/abs/1511.06038 | id:1511.06038 author:Yishu Miao, Lei Yu, Phil Blunsom category:cs.CL cs.LG stat.ML  published:2015-11-19 summary:Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks. version:4
arxiv-1606-01245 | Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization | http://arxiv.org/abs/1606.01245 | id:1606.01245 author:Fanhua Shang, Yuanyuan Liu, James Cheng category:cs.NA cs.AI math.OC stat.ML  published:2016-06-04 summary:The Schatten-p quasi-norm $(0<p<1)$ is usually used to replace the standard nuclear norm in order to approximate the rank function more accurately. However, existing Schatten-p quasi-norm minimization algorithms involve singular value decomposition (SVD) or eigenvalue decomposition (EVD) in each iteration, and thus may become very slow and impractical for large-scale problems. In this paper, we first define two tractable Schatten quasi-norms, i.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove that they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively, which lead to the design of very efficient algorithms that only need to update two much smaller factor matrices. We also design two efficient proximal alternating linearized minimization algorithms for solving representative matrix completion problems. Finally, we provide the global convergence and performance guarantees for our algorithms, which have better convergence properties than existing algorithms. Experimental results on synthetic and real-world data show that our algorithms are more accurate than the state-of-the-art methods, and are orders of magnitude faster. version:1
arxiv-1602-07868 | Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks | http://arxiv.org/abs/1602.07868 | id:1602.07868 author:Tim Salimans, Diederik P. Kingma category:cs.LG cs.AI cs.NE  published:2016-02-25 summary:We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning. version:3
arxiv-1508-00945 | Structured Prediction: From Gaussian Perturbations to Linear-Time Principled Algorithms | http://arxiv.org/abs/1508.00945 | id:1508.00945 author:Jean Honorio, Tommi Jaakkola category:stat.ML cs.LG  published:2015-08-05 summary:Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \cite{Altun03,Collins04b,Taskar03}. In natural language processing, recent work \cite{Zhang14,Zhang15} has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \cite{McAllester07}. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \cite{Zhang14,Zhang15}, our theoretical results show that more general techniques are possible. version:4
arxiv-1606-01307 | Scene Grammars, Factor Graphs, and Belief Propagation | http://arxiv.org/abs/1606.01307 | id:1606.01307 author:Jeroen Chua, Pedro F. Felzenszwalb category:cs.CV cs.AI  published:2016-06-03 summary:We consider a class of probabilistic grammars for generating scenes with multiple objects. Probabilistic scene grammars capture relationships between objects using compositional rules that provide important contextual cues for inference with ambiguous data. We show how to represent the distribution defined by a probabilistic scene grammar using a factor graph. We also show how to efficiently perform message passing in this factor graph. This leads to an efficient approach for inference with a grammar model using belief propagation as the underlying computational engine. Inference with belief propagation naturally combines bottom-up and top-down contextual information and leads to a robust algorithm for aggregating evidence. We show experiments on two different applications to demonstrate the generality of the framework. The first application involves detecting curves in noisy images, and we address this problem using a grammar that generates a collection of curves using a first-order Markov process. The second application involves localizing faces and parts of faces in images. In this case, we use a grammar that captures spatial relationships between the parts of a face. In both applications the same framework leads to robust inference algorithms that can effectively combine weak local information to reason about a scene. version:1
arxiv-1606-01299 | RAISR: Rapid and Accurate Image Super Resolution | http://arxiv.org/abs/1606.01299 | id:1606.01299 author:Yaniv Romano, John Isidoro, Peyman Milanfar category:cs.CV  published:2016-06-03 summary:Given an image, we wish to produce an image of larger size with significantly more pixels and higher image quality. This is generally known as the Single Image Super-Resolution (SISR) problem. The idea is that with sufficient training data (corresponding pairs of low and high resolution images) we can learn set of filters (i.e. a mapping) that when applied to given image that is not in the training set, will produce a higher resolution version of it, where the learning is preferably low complexity. In our proposed approach, the run-time is more than one to two orders of magnitude faster than the best competing methods currently available, while producing results comparable or better than state-of-the-art. A closely related topic is image sharpening and contrast enhancement, i.e., improving the visual quality of a blurry image by amplifying the underlying details (a wide range of frequencies). Our approach additionally includes an extremely efficient way to produce an image that is significantly sharper than the input blurry one, without introducing artifacts such as halos and noise amplification. We illustrate how this effective sharpening algorithm, in addition to being of independent interest, can be used as a pre-processing step to induce the learning of more effective upscaling filters with built-in sharpening and contrast enhancement effect. version:1
arxiv-1606-01292 | An Attentional Neural Conversation Model with Improved Specificity | http://arxiv.org/abs/1606.01292 | id:1606.01292 author:Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Kam-Fai Wong category:cs.CL cs.HC  published:2016-06-03 summary:In this paper we propose a neural conversation model for conducting dialogues. We demonstrate the use of this model to generate help desk responses, where users are asking questions about PC applications. Our model is distinguished by two characteristics. First, it models intention across turns with a recurrent network, and incorporates an attention model that is conditioned on the representation of intention. Secondly, it avoids generating non-specific responses by incorporating an IDF term in the objective function. The model is evaluated both as a pure generation model in which a help-desk response is generated from scratch, and as a retrieval model with performance measured using recall rates of the correct response. Experimental results indicate that the model outperforms previously proposed neural conversation architectures, and that using specificity in the objective function significantly improves performances for both generation and retrieval. version:1
arxiv-1606-01286 | Incorporating long-range consistency in CNN-based texture generation | http://arxiv.org/abs/1606.01286 | id:1606.01286 author:G. Berger, R. Memisevic category:cs.CV  published:2016-06-03 summary:Gatys et al. (2015) showed that pair-wise products of features in a convolutional network are a very effective representation of image textures. We propose a simple modification to that representation which makes it possible to incorporate long-range structure into image generation, and to render images that satisfy various symmetry constraints. We show how this can greatly improve rendering of regular textures and of images that contain other kinds of symmetric structure. We also present applications to inpainting and season transfer. version:1
arxiv-1606-01284 | Statistical Pattern Recognition for Driving Styles Based on Bayesian Probability and Kernel Density Estimation | http://arxiv.org/abs/1606.01284 | id:1606.01284 author:Wenshuo Wang, Junqiang Xi, Xiaohan Li category:stat.ML cs.CV  published:2016-06-03 summary:Driving styles have a great influence on vehicle fuel economy, active safety, and drivability. To recognize driving styles of path-tracking behaviors for different divers, a statistical pattern-recognition method is developed to deal with the uncertainty of driving styles or characteristics based on probability density estimation. First, to describe driver path-tracking styles, vehicle speed and throttle opening are selected as the discriminative parameters, and a conditional kernel density function of vehicle speed and throttle opening is built, respectively, to describe the uncertainty and probability of two representative driving styles, e.g., aggressive and normal. Meanwhile, a posterior probability of each element in feature vector is obtained using full Bayesian theory. Second, a Euclidean distance method is involved to decide to which class the driver should be subject instead of calculating the complex covariance between every two elements of feature vectors. By comparing the Euclidean distance between every elements in feature vector, driving styles are classified into seven levels ranging from low normal to high aggressive. Subsequently, to show benefits of the proposed pattern-recognition method, a cross-validated method is used, compared with a fuzzy logic-based pattern-recognition method. The experiment results show that the proposed statistical pattern-recognition method for driving styles based on kernel density estimation is more efficient and stable than the fuzzy logic-based method. version:1
arxiv-1606-01283 | Enhancing the LexVec Distributed Word Representation Model Using Positional Contexts and External Memory | http://arxiv.org/abs/1606.01283 | id:1606.01283 author:Alexandre Salle, Marco Idiart, Aline Villavicencio category:cs.CL  published:2016-06-03 summary:In this paper we take a state-of-the-art model for distributed word representation that explicitly factorizes the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling and address two of its shortcomings. We improve syntactic performance by using positional contexts, and solve the need to store the PPMI matrix in memory by working on aggregate data in external memory. The effectiveness of both modifications is shown using word similarity and analogy tasks. version:1
arxiv-1606-01280 | Dependency Parsing as Head Selection | http://arxiv.org/abs/1606.01280 | id:1606.01280 author:Xingxing Zhang, Jianpeng Cheng, Mirella Lapata category:cs.CL cs.LG  published:2016-06-03 summary:Conventional dependency parsers rely on a statistical model and a transition system or graph algorithm to enforce tree-structured outputs during training and inference. In this work we formalize dependency parsing as the problem of selecting the head (a.k.a. parent) of each word in a sentence. Our model which we call DeNSe (as shorthand for Dependency Neural Selection) employs bidirectional recurrent neural networks for the head selection task. Without enforcing any structural constraints during training, DeNSe generates (at inference time) trees for the overwhelming majority of sentences (95% on an English dataset), while remaining non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate DeNSe on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with or outperform the state of the art. version:1
arxiv-1606-01275 | Predicting with Distributions | http://arxiv.org/abs/1606.01275 | id:1606.01275 author:Michael Kearns, Zhiwei Steven Wu category:cs.DS cs.LG  published:2016-06-03 summary:We consider a new PAC-style learning model in which a joint distribution over vector pairs (x,y) is determined by an unknown function c(x) that maps input vectors x not to individual outputs, but to entire distributions over output vectors y. Our main results take the form of rather general reductions from our model to algorithms for PAC learning the function class and the distribution class separately, and show that virtually every such combination yields an efficient algorithm in our model. Our methods include a randomized reduction to classification noise that partially resolves an open problem in [RDM06], and an application of the Neyman-Pearson Lemma to obtain robust learning algorithms. version:1
arxiv-1603-00106 | Characterizing Diseases from Unstructured Text: A Vocabulary Driven Word2vec Approach | http://arxiv.org/abs/1603.00106 | id:1603.00106 author:Saurav Ghosh, Prithwish Chakraborty, Emily Cohn, John S. Brownstein, Naren Ramakrishnan category:cs.LG cs.CL stat.ML  published:2016-03-01 summary:Traditional disease surveillance can be augmented with a wide variety of real-time sources such as, news and social media. However, these sources are in general unstructured and, construction of surveillance tools such as taxonomical correlations and trace mapping involves considerable human supervision. In this paper, we motivate a disease vocabulary driven word2vec model (Dis2Vec) to model diseases and constituent attributes as word embeddings from the HealthMap news corpus. We use these word embeddings to automatically create disease taxonomies and evaluate our model against corresponding human annotated taxonomies. We compare our model accuracies against several state-of-the art word2vec methods. Our results demonstrate that Dis2Vec outperforms traditional distributed vector representations in its ability to faithfully capture taxonomical attributes across different class of diseases such as endemic, emerging and rare. version:2
arxiv-1606-01269 | End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning | http://arxiv.org/abs/1606.01269 | id:1606.01269 author:Jason D. Williams, Geoffrey Zweig category:cs.CL cs.AI cs.LG  published:2016-06-03 summary:This paper presents a model for end-to-end learning of task-oriented dialog systems. The main component of the model is a recurrent neural network (an LSTM), which maps from raw dialog history directly to a distribution over system actions. The LSTM automatically infers a representation of dialog history, which relieves the system developer of much of the manual feature engineering of dialog state. In addition, the developer can provide software that expresses business rules and provides access to programmatic APIs, enabling the LSTM to take actions in the real world on behalf of the user. The LSTM can be optimized using supervised learning (SL), where a domain expert provides example dialogs which the LSTM should imitate; or using reinforcement learning (RL), where the system improves by interacting directly with end users. Experiments show that SL and RL are complementary: SL alone can derive a reasonable initial policy from a small number of training dialogs; and starting RL optimization with a policy trained with SL substantially accelerates the learning rate of RL. version:1
arxiv-1606-01261 | Minimizing Regret on Reflexive Banach Spaces and Learning Nash Equilibria in Continuous Zero-Sum Games | http://arxiv.org/abs/1606.01261 | id:1606.01261 author:Maximilian Balandat, Walid Krichene, Claire Tomlin, Alexandre Bayen category:cs.LG  published:2016-06-03 summary:We study a general version of the adversarial online learning problem. We are given a decision set $\mathcal{X}$ in a reflexive Banach space $X$ and a sequence of reward vectors in the dual space of $X$. At each iteration, we choose an action from $\mathcal{X}$, based on the observed sequence of previous rewards. Our goal is to minimize regret, defined as the gap between the realized reward and the reward of the best fixed action in hindsight. Using results from infinite dimensional convex analysis, we generalize the method of Dual Averaging (or Follow the Regularized Leader) to our setting and obtain general upper bounds on the worst-case regret that subsume a wide range of results from the literature. Under the assumption of uniformly continuous rewards, we obtain explicit anytime regret bounds in a setting where the decision set is the set of probability distributions on a compact metric space $S$ whose Radon-Nikodym derivatives are elements of $L^p(S)$ for some $p > 1$. Importantly, we make no convexity assumptions on either the set $S$ or the reward functions. We also prove a general lower bound on the worst-case regret for any online algorithm. We then apply these results to the problem of learning in repeated continuous two-player zero-sum games, in which players' strategy sets are compact metric spaces. In doing so, we first prove that if both players play a Hannan-consistent strategy, then with probability 1 the empirical distributions of play weakly converge to the set of Nash equilibria of the game. We then show that, under mild assumptions, Dual Averaging on the (infinite-dimensional) space of probability distributions indeed achieves Hannan-consistency. Finally, we illustrate our results through numerical examples. version:1
arxiv-1606-01239 | Grid-like structure is optimal for path integration | http://arxiv.org/abs/1606.01239 | id:1606.01239 author:Reza Moazzezi category:cs.NE q-bio.NC  published:2016-06-03 summary:Grid cells in medial entorhinal cortex are believed to play a key role in path integration. However, the relation between path integration and the grid-like arrangement of their firing field remains unclear. We provide theoretical evidence that grid-like structure and path integration are closely related. In one dimension, the grid-like structure provides the optimal solution for path integration assuming that the noise correlation structure is Gaussian. In two dimensions, assuming that the noise is Gaussian, rectangular grid-like structure is the optimal solution provided that 1- both noise correlation and receptive field structures of the neurons can be multiplicatively decomposed into orthogonal components and 2- the eigenvalues of the decomposed correlation matrices decrease faster than the square of the frequency of the corresponding eigenvectors. We will also address the decoding mechanism and show that the problem of decoding reduces to the problem of extracting task relevant information in the presence of task irrelevant information. Change-based Population Coding provides the optimal solution for this problem. version:1
arxiv-1606-01219 | Learning Stylometric Representations for Authorship Analysis | http://arxiv.org/abs/1606.01219 | id:1606.01219 author:Steven H. H. Ding, Benjamin C. M. Fung, Farkhund Iqbal, William K. Cheung category:cs.CL cs.CY cs.SI K.4.1; I.7.5; I.2.7  published:2016-06-03 summary:Authorship analysis (AA) is the study of unveiling the hidden properties of authors from a body of exponentially exploding textual data. It extracts an author's identity and sociolinguistic characteristics based on the reflected writing styles in the text. It is an essential process for various areas, such as cybercrime investigation, psycholinguistics, political socialization, etc. However, most of the previous techniques critically depend on the manual feature engineering process. Consequently, the choice of feature set has been shown to be scenario- or dataset-dependent. In this paper, to mimic the human sentence composition process using a neural network approach, we propose to incorporate different categories of linguistic features into distributed representation of words in order to learn simultaneously the writing style representations based on unlabeled texts for authorship analysis. In particular, the proposed models allow topical, lexical, syntactical, and character-level feature vectors of each document to be extracted as stylometrics. We evaluate the performance of our approach on the problems of authorship characterization and authorship verification with the Twitter, novel, and essay datasets. The experiments suggest that our proposed text representation outperforms the bag-of-lexical-n-grams, Latent Dirichlet Allocation, Latent Semantic Analysis, PVDM, PVDBOW, and word2vec representations. version:1
arxiv-1511-05933 | Towards O(1) Seeding of K-Means | http://arxiv.org/abs/1511.05933 | id:1511.05933 author:Sayantan Dasgupta category:cs.LG  published:2015-11-18 summary:K-means is one of the most widely used algorithms for clustering in Data Mining applications, which attempts to minimize the sum of square of Euclidean distance of the points in the clusters from the respective means of the clusters. The simplicity and scalability of K-means makes it very appealing. However, K-means suffers from local minima problem, and comes with no guarantee to converge to the optimal cost. K-means++ tries to address the problem by seeding the means using a distance based sampling scheme. However, seeding the means in K-means++ needs $O\left(K\right)$ passes through the entire dataset. This could be very costly in large amount of dataset. Here we propose a method of seeding initial means based on factorizations of higher order moments for bounded data. Our method takes O(1) passes through the entire dataset to extract the initial set of means, and its final cost can be proven to be within $O(\sqrt{K})$ of the optimal cost. We demonstrate the performance of our algorithm in comparison with the existing algorithms on various benchmark datasets. version:6
arxiv-1606-01190 | Distributed stochastic optimization via matrix exponential learning | http://arxiv.org/abs/1606.01190 | id:1606.01190 author:Panayotis Mertikopoulos, E. Veronica Belmega, Romain Negrel, Luca Sanguinetti category:cs.IT cs.LG math.IT math.OC  published:2016-06-03 summary:In this paper, we investigate a distributed learning scheme for a broad class of stochastic optimization problems and games that arise in signal processing and wireless communications. The proposed algorithm relies on the method of matrix exponential learning (MXL) and only requires locally computable gradient observations that are possibly imperfect and/or obsolete. To analyze it, we introduce the notion of a stable Nash equilibrium and we show that the algorithm is globally convergent to such equilibria - or locally convergent when an equilibrium is only locally stable. We also derive an explicit linear bound for the algorithm's convergence speed, which remains valid under measurement errors and uncertainty of arbitrarily high variance. To validate our theoretical analysis, we test the algorithm in realistic multi-carrier/multiple-antenna wireless scenarios where several users seek to maximize their energy efficiency. Our results show that learning allows users to attain a net increase between 100% and 500% in energy efficiency, even under very high uncertainty. version:1
arxiv-1605-04481 | Bias and Agreement in Syntactic Annotations | http://arxiv.org/abs/1605.04481 | id:1605.04481 author:Yevgeni Berzak, Yan Huang, Andrei Barbu, Anna Korhonen, Boris Katz category:cs.CL  published:2016-05-15 summary:We present a study on two key characteristics of human syntactic annotations: anchoring and agreement. Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards pre-existing values. We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output. Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with human-based annotations. Using sentences from the Penn Treebank WSJ, we also report the first systematically obtained inter-annotator agreement estimates for English dependency parsing. Our agreement results control for anchoring bias, and are consequential in that they are on par with state of the art parsing performance for English. We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations. version:2
arxiv-1606-01178 | Reinforcement Learning for Semantic Segmentation in Indoor Scenes | http://arxiv.org/abs/1606.01178 | id:1606.01178 author:Md. Alimoor Reza, Jana Kosecka category:cs.CV cs.RO  published:2016-06-03 summary:Future advancements in robot autonomy and sophistication of robotics tasks rest on robust, efficient, and task-dependent semantic understanding of the environment. Semantic segmentation is the problem of simultaneous segmentation and categorization of a partition of sensory data. The majority of current approaches tackle this using multi-class segmentation and labeling in a Conditional Random Field (CRF) framework or by generating multiple object hypotheses and combining them sequentially. In practical settings, the subset of semantic labels that are needed depend on the task and particular scene and labelling every single pixel is not always necessary. We pursue these observations in developing a more modular and flexible approach to multi-class parsing of RGBD data based on learning strategies for combining independent binary object-vs-background segmentations in place of the usual monolithic multi-label CRF approach. Parameters for the independent binary segmentation models can be learned very efficiently, and the combination strategy---learned using reinforcement learning---can be set independently and can vary over different tasks and environments. Accuracy is comparable to state-of-art methods on a subset of the NYU-V2 dataset of indoor scenes, while providing additional flexibility and modularity. version:1
arxiv-1606-01167 | How Deep is the Feature Analysis underlying Rapid Visual Categorization? | http://arxiv.org/abs/1606.01167 | id:1606.01167 author:Sven Eberhardt, Jonah Cader, Thomas Serre category:cs.CV  published:2016-06-03 summary:Rapid categorization paradigms have a long history in experimental psychology: Characterized by short presentation times and speedy behavioral responses, these tasks highlight the efficiency with which our visual system processes natural object categories. Previous studies have shown that feed-forward hierarchical models of the visual cortex provide a good fit to human visual decisions. At the same time, recent work in computer vision has demonstrated significant gains in object recognition accuracy with increasingly deep hierarchical architectures. But it is unclear how well these models account for human visual decisions and what they may reveal about the underlying brain processes. We have conducted a large-scale psychophysics study to assess the correlation between computational models and human participants on a rapid animal vs. non-animal categorization task. We considered visual representations of varying complexity by analyzing the output of different stages of processing in three state-of-the-art deep networks. We found that recognition accuracy increases with higher stages of visual processing (higher level stages indeed outperforming human participants on the same task) but that human decisions agree best with predictions from intermediate stages. Overall, these results suggest that human participants may rely on visual features of intermediate complexity and that the complexity of visual representations afforded by modern deep network models may exceed those used by human participants during rapid categorization. version:1
arxiv-1509-03281 | Density Evolution in the Degree-correlated Stochastic Block Model | http://arxiv.org/abs/1509.03281 | id:1509.03281 author:Elchanan Mossel, Jiaming Xu category:stat.ML cs.IT math.IT math.PR  published:2015-09-10 summary:There is a recent surge of interest in identifying the sharp recovery thresholds for cluster recovery under the stochastic block model. In this paper, we address the more refined question of how many vertices that will be misclassified on average. We consider the binary form of the stochastic block model, where $n$ vertices are partitioned into two clusters with edge probability $a/n$ within the first cluster, $c/n$ within the second cluster, and $b/n$ across clusters. Suppose that as $n \to \infty$, $a= b+ \mu \sqrt{ b} $, $c=b+ \nu \sqrt{ b} $ for two fixed constants $\mu, \nu$, and $b \to \infty$ with $b=n^{o(1)}$. When the cluster sizes are balanced and $\mu \neq \nu$, we show that the minimum fraction of misclassified vertices on average is given by $Q(\sqrt{v^*})$, where $Q(x)$ is the Q-function for standard normal, $v^*$ is the unique fixed point of $v= \frac{(\mu-\nu)^2}{16} + \frac{ (\mu+\nu)^2 }{16} \mathbb{E}[ \tanh(v+ \sqrt{v} Z)],$ and $Z$ is standard normal. Moreover, the minimum misclassified fraction on average is attained by a local algorithm, namely belief propagation, in time linear in the number of edges. Our proof techniques are based on connecting the cluster recovery problem to tree reconstruction problems, and analyzing the density evolution of belief propagation on trees with Gaussian approximations. version:2
arxiv-1606-01166 | Generalizing the Convolution Operator to Extend CNNs to Irregular Domains | http://arxiv.org/abs/1606.01166 | id:1606.01166 author:Jean-Charles Vialatte, Vincent Gripon, Grégoire Mercier category:cs.LG cs.CV cs.NE  published:2016-06-03 summary:Convolutional Neural Networks (CNNs) have become the state-of-the-art in supervised learning vision tasks. Their convolutional filters are of paramount importance for they allow to learn patterns while disregarding their locations in input images. When facing highly irregular domains, generalized convolutional operators based on an underlying graph structure have been proposed. However, these operators do not exactly match standard ones on grid graphs, and introduce unwanted additional invariance (e.g. with regards to rotations). We propose a novel approach to generalize CNNs to irregular domains using weight sharing and graph-based operators. Using experiments, we show that these models resemble CNNs on regular domains and offer better performance than multilayer perceptrons on distorded ones. version:1
arxiv-1606-01164 | Dense Associative Memory for Pattern Recognition | http://arxiv.org/abs/1606.01164 | id:1606.01164 author:Dmitry Krotov, John J Hopfield category:cs.NE cond-mat.dis-nn cs.LG q-bio.NC stat.ML  published:2016-06-03 summary:A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used for training neural networks. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set. version:1
arxiv-1405-1297 | Combining Multiple Clusterings via Crowd Agreement Estimation and Multi-Granularity Link Analysis | http://arxiv.org/abs/1405.1297 | id:1405.1297 author:Dong Huang, Jian-Huang Lai, Chang-Dong Wang category:stat.ML cs.LG  published:2014-05-06 summary:The clustering ensemble technique aims to combine multiple clusterings into a probably better and more robust clustering and has been receiving an increasing attention in recent years. There are mainly two aspects of limitations in the existing clustering ensemble approaches. Firstly, many approaches lack the ability to weight the base clusterings without access to the original data and can be affected significantly by the low-quality, or even ill clusterings. Secondly, they generally focus on the instance level or cluster level in the ensemble system and fail to integrate multi-granularity cues into a unified model. To address these two limitations, this paper proposes to solve the clustering ensemble problem via crowd agreement estimation and multi-granularity link analysis. We present the normalized crowd agreement index (NCAI) to evaluate the quality of base clusterings in an unsupervised manner and thus weight the base clusterings in accordance with their clustering validity. To explore the relationship between clusters, the source aware connected triple (SACT) similarity is introduced with regard to their common neighbors and the source reliability. Based on NCAI and multi-granularity information collected among base clusterings, clusters, and data instances, we further propose two novel consensus functions, termed weighted evidence accumulation clustering (WEAC) and graph partitioning with multi-granularity link analysis (GP-MGLA) respectively. The experiments are conducted on eight real-world datasets. The experimental results demonstrate the effectiveness and robustness of the proposed methods. version:2
