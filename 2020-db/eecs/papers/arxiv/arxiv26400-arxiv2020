arxiv-1703-04819 | RECOD Titans at ISIC Challenge 2017 | http://arxiv.org/abs/1703.04819 | id:1703.04819 author:Afonso Menegola, Julia Tavares, Michel Fornaciali, Lin Tzy Li, Sandra Avila, Eduardo Valle category:cs.CV  published:2017-03-14 summary:This extended abstract describes the participation of RECOD Titans in parts 1 and 3 of the ISIC Challenge 2017 "Skin Lesion Analysis Towards Melanoma Detection" (ISBI 2017). Although our team has a long experience with melanoma classification, the ISIC Challenge 2017 was the very first time we worked on skin-lesion segmentation. For part 1 (segmentation), our final submission used four of our models: two trained with all 2000 samples, without a validation split, for 250 and for 500 epochs respectively; and other two trained and validated with two different 1600/400 splits, for 220 epochs. Those four models, individually, achieved between 0.780 and 0.783 official validation scores. Our final submission averaged the output of those four models achieved a score of 0.793. For part 3 (classification), the submitted test run as well as our last official validation run were the result from a meta-model that assembled seven base deep-learning models: three based on Inception-V4 trained on our largest dataset; three based on Inception trained on our smallest dataset; and one based on ResNet-101 trained on our smaller dataset. The results of those component models were stacked in a meta-learning layer based on an SVM trained on the validation set of our largest dataset. version:1
arxiv-1703-04818 | Neural Graph Machines: Learning Neural Networks Using Graphs | http://arxiv.org/abs/1703.04818 | id:1703.04818 author:Thang D. Bui, Sujith Ravi, Vivek Ramavajjala category:cs.LG cs.NE  published:2017-03-14 summary:Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural networks, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training framework with a graph-regularised objective, namely "Neural Graph Machines", that can combine the power of neural networks and label propagation. This work generalises previous literature on graph-augmented training of neural networks, enabling it to be applied to multiple neural architectures (Feed-forward NNs, CNNs and LSTM RNNs) and a wide range of graphs. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs, with a runtime that is linear in the number of edges. The proposed joint training approach convincingly outperforms many existing methods on a wide range of tasks (multi-label classification on social graphs, news categorization, document classification and semantic intent classification), with multiple forms of graph inputs (including graphs with and without node-level features) and using different types of neural networks. version:1
arxiv-1703-04816 | FastQA: A Simple and Efficient Neural Architecture for Question Answering | http://arxiv.org/abs/1703.04816 | id:1703.04816 author:Dirk Weissenborn, Georg Wiese, Laura Seiffe category:cs.CL cs.AI cs.NE  published:2017-03-14 summary:Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to a simpler neural baseline system that would justify their complexity. In this work, we propose a simple heuristic that guided the development of FastQA, an efficient end-to-end neural model for question answering that is very competitive with existing models. We further demonstrate, that an extended version (FastQAExt) achieves state-of-the-art results on recent benchmark datasets, namely SQuAD, NewsQA and MsMARCO, outperforming most existing models. However, we show that increasing the complexity of FastQA to FastQAExt does not yield any systematic improvements. We argue that the same holds true for most existing systems that are similar to FastQAExt. A manual analysis reveals that our proposed heuristic explains most predictions of our model, which indicates that modeling a simple heuristic is enough to achieve strong performance on extractive QA datasets. The overall strong performance of FastQA puts results of existing, more complex models into perspective. version:1
arxiv-1703-04813 | Learned Optimizers that Scale and Generalize | http://arxiv.org/abs/1703.04813 | id:1703.04813 author:Olga Wichrowska, Niru Maheswaranathan, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Nando de Freitas, Jascha Sohl-Dickstein category:cs.LG cs.NE stat.ML  published:2017-03-14 summary:Learning to learn has emerged as an important direction for achieving artificial intelligence. Two of the primary barriers to its adoption are an inability to scale to larger problems and a limited ability to generalize to new tasks. We introduce a learned gradient descent optimizer that generalizes well to new tasks, and which has significantly reduced memory and computation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal per-parameter overhead, augmented with additional architectural features that mirror the known structure of optimization tasks. We also develop a meta-training ensemble of small, diverse, optimization tasks capturing common properties of loss landscapes. The optimizer learns to out-perform RMSProp/ADAM on problems in this corpus. More importantly, it performs comparably or better when applied to small convolutional neural networks, despite seeing no neural networks in its meta-training set. Finally, it generalizes to train Inception V3 and ResNet V2 architectures on the ImageNet dataset, optimization problems that are of a vastly different scale than those it was trained on. version:1
arxiv-1703-04785 | Network Constrained Distributed Dual Coordinate Ascent for Machine Learning | http://arxiv.org/abs/1703.04785 | id:1703.04785 author:Myung Cho, Lifeng Lai, Weiyu Xu category:cs.DC cs.LG  published:2017-03-14 summary:With explosion of data size and limited storage space at a single location, data are often distributed at different locations. We thus face the challenge of performing large-scale machine learning from these distributed data through communication networks. In this paper, we study how the network communication constraints will impact the convergence speed of distributed machine learning optimization algorithms. In particular, we give the convergence rate analysis of the distributed dual coordinate ascent in a general tree structured network. Furthermore, by considering network communication delays, we optimize the network-constrained dual coordinate ascent algorithms to maximize its convergence speed. Our results show that under different network communication delays, to achieve maximum convergence speed, one needs to adopt delay-dependent numbers of local and global iterations for distributed dual coordinate ascent. version:1
arxiv-1703-04783 | Multichannel End-to-end Speech Recognition | http://arxiv.org/abs/1703.04783 | id:1703.04783 author:Tsubasa Ochiai, Shinji Watanabe, Takaaki Hori, John R. Hershey category:cs.SD cs.CL  published:2017-03-14 summary:The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer. version:1
arxiv-1703-04782 | Online Learning Rate Adaptation with Hypergradient Descent | http://arxiv.org/abs/1703.04782 | id:1703.04782 author:Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, Frank Wood category:cs.LG stat.ML 68T05 G.1.6; I.2.6  published:2017-03-14 summary:We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice. We analyze the effectiveness of the method by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it improves upon these commonly used algorithms on a range of optimization problems; in particular the kinds of objective functions that arise frequently in deep neural network training. Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself. Computing this "hypergradient" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation. version:1
arxiv-1703-04778 | A statistical model for aggregating judgments by incorporating peer predictions | http://arxiv.org/abs/1703.04778 | id:1703.04778 author:John McCoy, Drazen Prelec category:stat.ML  published:2017-03-14 summary:We propose a probabilistic model to aggregate the answers of respondents answering multiple-choice questions. The model does not assume that everyone has access to the same information, and so does not assume that the consensus answer is correct. Instead, it infers the most probable world state, even if only a minority vote for it. Each respondent is modeled as receiving a signal contingent on the actual world state, and as using this signal to both determine their own answer and predict the answers given by others. By incorporating respondent's predictions of others' answers, the model infers latent parameters corresponding to the prior over world states and the probability of different signals being received in all possible world states, including counterfactual ones. Unlike other probabilistic models for aggregation, our model applies to both single and multiple questions, in which case it estimates each respondent's expertise. The model shows good performance, compared to a number of other probabilistic models, on data from seven studies covering different types of expertise. version:1
arxiv-1703-04775 | Discriminate-and-Rectify Encoders: Learning from Image Transformation Sets | http://arxiv.org/abs/1703.04775 | id:1703.04775 author:Andrea Tacchetti, Stephen Voinea, Georgios Evangelopoulos category:cs.CV cs.LG stat.ML  published:2017-03-14 summary:The complexity of a learning task is increased by transformations in the input space that preserve class identity. Visual object recognition for example is affected by changes in viewpoint, scale, illumination or planar transformations. While drastically altering the visual appearance, these changes are orthogonal to recognition and should not be reflected in the representation or feature encoding used for learning. We introduce a framework for weakly supervised learning of image embeddings that are robust to transformations and selective to the class distribution, using sets of transforming examples (orbit sets), deep parametrizations and a novel orbit-based loss. The proposed loss combines a discriminative, contrastive part for orbits with a reconstruction error that learns to rectify orbit transformations. The learned embeddings are evaluated in distance metric-based tasks, such as one-shot classification under geometric transformations, as well as face verification and retrieval under more realistic visual variability. Our results suggest that orbit sets, suitably computed or observed, can be used for efficient, weakly-supervised learning of semantically relevant image embeddings. version:1
arxiv-1703-04770 | Audio Scene Classification with Deep Recurrent Neural Networks | http://arxiv.org/abs/1703.04770 | id:1703.04770 author:Huy Phan, Philipp Koch, Fabrice Katzberg, Marco Maass, Radoslaw Mazur, Alfred Mertins category:cs.SD cs.LG  published:2017-03-14 summary:We introduce in this work an efficient approach for audio scene classification using deep recurrent neural networks. A scene audio signal is firstly transformed into a sequence of high-level label tree embedding feature vectors. The vector sequence is then divided into multiple subsequences on which a deep GRU-based recurrent neural network is trained for sequence-to-label classification. The global predicted label for the entire sequence is finally obtained via aggregation of subsequence classification outputs. We will show that our approach obtain an F1-score of 97.7% on the LITIS Rouen dataset, which is the largest dataset publicly available for the task. Compared to the best previously reported result on the dataset, our approach is able to reduce the relative classification error by 35.3%. version:1
arxiv-1703-04757 | Convergence of Deep Neural Networks to a Hierarchical Covariance Matrix Decomposition | http://arxiv.org/abs/1703.04757 | id:1703.04757 author:Nima Dehmamy, Neda Rohani, Aggelos Katsaggelos category:cs.LG physics.data-an stat.ML  published:2017-03-14 summary:We show that in a deep neural network trained with ReLU, the low-lying layers should be replaceable with truncated linearly activated layers. We derive the gradient descent equations in this truncated linear model and demonstrate that --if the distribution of the training data is stationary during training-- the optimal choice for weights in these low-lying layers is the eigenvectors of the covariance matrix of the data. If the training data is random and uniform enough, these eigenvectors can be found using a small fraction of the training data, thus reducing the computational complexity of training. We show how this can be done recursively to form successive, trained layers. At least in the first layer, our tests show that this approach improves classification of images while reducing network size. version:1
arxiv-1703-04756 | Weighted Voting Via No-Regret Learning | http://arxiv.org/abs/1703.04756 | id:1703.04756 author:Nika Haghtalab, Ritesh Noothigattu, Ariel D. Procaccia category:cs.GT cs.AI cs.LG cs.MA  published:2017-03-14 summary:Voting systems typically treat all voters equally. We argue that perhaps they should not: Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones. To develop a formal framework for desirable weighting schemes, we draw on no-regret learning. Specifically, given a voting rule, we wish to design a weighting scheme such that applying the voting rule, with voters weighted by the scheme, leads to choices that are almost as good as those endorsed by the best voter in hindsight. We derive possibility and impossibility results for the existence of such weighting schemes, depending on whether the voting rule and the weighting scheme are deterministic or randomized, as well as on the social choice axioms satisfied by the voting rule. version:1
arxiv-1703-04730 | Understanding Black-box Predictions via Influence Functions | http://arxiv.org/abs/1703.04730 | id:1703.04730 author:Pang Wei Koh, Percy Liang category:stat.ML cs.AI cs.LG  published:2017-03-14 summary:How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, identifying the points most responsible for a given prediction. Applying ideas from second-order optimization, we scale up influence functions to modern machine learning settings and show that they can be applied to high-dimensional black-box models, even in non-convex and non-differentiable settings. We give a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for many different purposes: to understand model behavior, debug models and detect dataset errors, and even identify and exploit vulnerabilities to adversarial training-set attacks. version:1
arxiv-1703-04727 | Tracking Gaze and Visual Focus of Attention of People Involved in Social Interaction | http://arxiv.org/abs/1703.04727 | id:1703.04727 author:Benoît Massé, Silèye Ba, Radu Horaud category:cs.CV  published:2017-03-14 summary:The visual focus of attention (VFOA) has been recognized as a prominent conversational cue. We are interested in the VFOA tracking of a group of people involved in social interaction. We note that in this case the participants look either at each other or at an object of interest; therefore they don't always face a camera and, consequently, their gazes (and their VFOAs) cannot be based on eye detection and tracking. We propose a method that exploits the correlation between gaze direction and head orientation. Both VFOA and gaze are modeled as latent variables in a Bayesian switching linear dynamic model. The proposed formulation leads to a tractable learning procedure and to an efficient gaze-and-VFOA tracking algorithm. The method is tested and benchmarked using a publicly available dataset that contains typical multi-party human-robot interaction scenarios, and that was recorded with both a motion capture system, and with a camera mounted onto a robot head. version:1
arxiv-1703-04213 | MetaPAD: Meta Pattern Discovery from Massive Text Corpora | http://arxiv.org/abs/1703.04213 | id:1703.04213 author:Meng Jiang, Jingbo Shang, Taylor Cassidy, Xiang Ren, Lance M. Kaplan, Timothy P. Hanratty, Jiawei Han category:cs.CL  published:2017-03-13 summary:Mining textual patterns in news, tweets, papers, and many other kinds of text corpora has been an active theme in text mining and NLP research. Previous studies adopt a dependency parsing-based pattern discovery approach. However, the parsing results lose rich context around entities in the patterns, and the process is costly for a corpus of large scale. In this study, we propose a novel typed textual pattern structure, called meta pattern, which is extended to a frequent, informative, and precise subsequence pattern in certain context. We propose an efficient framework, called MetaPAD, which discovers meta patterns from massive corpora with three techniques: (1) it develops a context-aware segmentation method to carefully determine the boundaries of patterns with a learnt pattern quality assessment function, which avoids costly dependency parsing and generates high-quality patterns; (2) it identifies and groups synonymous meta patterns from multiple facets---their types, contexts, and extractions; and (3) it examines type distributions of entities in the instances extracted by each group of patterns, and looks for appropriate type levels to make discovered patterns precise. Experiments demonstrate that our proposed framework discovers high-quality typed textual patterns efficiently from different genres of massive corpora and facilitates information extraction. version:2
arxiv-1703-04699 | A fully end-to-end deep learning approach for real-time simultaneous 3D reconstruction and material recognition | http://arxiv.org/abs/1703.04699 | id:1703.04699 author:Cheng Zhao, Li Sun, Rustam Stolkin category:cs.CV  published:2017-03-14 summary:This paper addresses the problem of simultaneous 3D reconstruction and material recognition and segmentation. Enabling robots to recognise different materials (concrete, metal etc.) in a scene is important for many tasks, e.g. robotic interventions in nuclear decommissioning. Previous work on 3D semantic reconstruction has predominantly focused on recognition of everyday domestic objects (tables, chairs etc.), whereas previous work on material recognition has largely been confined to single 2D images without any 3D reconstruction. Meanwhile, most 3D semantic reconstruction methods rely on computationally expensive post-processing, using Fully-Connected Conditional Random Fields (CRFs), to achieve consistent segmentations. In contrast, we propose a deep learning method which performs 3D reconstruction while simultaneously recognising different types of materials and labelling them at the pixel level. Unlike previous methods, we propose a fully end-to-end approach, which does not require hand-crafted features or CRF post-processing. Instead, we use only learned features, and the CRF segmentation constraints are incorporated inside the fully end-to-end learned system. We present the results of experiments, in which we trained our system to perform real-time 3D semantic reconstruction for 23 different materials in a real-world application. The run-time performance of the system can be boosted to around 10Hz, using a conventional GPU, which is enough to achieve real-time semantic reconstruction using a 30fps RGB-D camera. To the best of our knowledge, this work is the first real-time end-to-end system for simultaneous 3D reconstruction and material recognition. version:1
arxiv-1703-04697 | On the benefits of output sparsity for multi-label classification | http://arxiv.org/abs/1703.04697 | id:1703.04697 author:Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Joseph Salmon category:math.ST cs.LG stat.ML stat.TH  published:2017-03-14 summary:The multi-label classification framework, where each observation can be associated with a set of labels, has generated a tremendous amount of attention over recent years. The modern multi-label problems are typically large-scale in terms of number of observations, features and labels, and the amount of labels can even be comparable with the amount of observations. In this context, different remedies have been proposed to overcome the curse of dimensionality. In this work, we aim at exploiting the output sparsity by introducing a new loss, called the sparse weighted Hamming loss. This proposed loss can be seen as a weighted version of classical ones, where active and inactive labels are weighted separately. Leveraging the influence of sparsity in the loss function, we provide improved generalization bounds for the empirical risk minimizer, a suitable property for large-scale problems. For this new loss, we derive rates of convergence linear in the underlying output-sparsity rather than linear in the number of labels. In practice, minimizing the associated risk can be performed efficiently by using convex surrogates and modern convex optimization algorithms. We provide experiments on various real-world datasets demonstrating the pertinence of our approach when compared to non-weighted techniques. version:1
arxiv-1703-04677 | A computational investigation of sources of variability in sentence comprehension difficulty in aphasia | http://arxiv.org/abs/1703.04677 | id:1703.04677 author:Paul Mätzig, Shravan Vasishth, Felix Engelmann, David Caplan category:cs.CL cs.AI  published:2017-03-14 summary:We present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis & Vasishth 2005 model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object relatives presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show differential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia. version:1
arxiv-1703-04670 | 6-DoF Object Pose from Semantic Keypoints | http://arxiv.org/abs/1703.04670 | id:1703.04670 author:Georgios Pavlakos, Xiaowei Zhou, Aaron Chan, Konstantinos G. Derpanis, Kostas Daniilidis category:cs.CV cs.RO  published:2017-03-14 summary:This paper presents a novel approach to estimating the continuous six degree of freedom (6-DoF) pose (3D translation and rotation) of an object from a single RGB image. The approach combines semantic keypoints predicted by a convolutional network (convnet) with a deformable shape model. Unlike prior work, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available training image data. Furthermore, the approach can be applied to instance- and class-based pose recovery. Empirically, we show that the proposed approach can accurately recover the 6-DoF object pose for both instance- and class-based scenarios with a cluttered background. For class-based object pose estimation, state-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset. version:1
arxiv-1703-02236 | Propensity score prediction for electronic healthcare databases using Super Learner and High-dimensional Propensity Score Methods | http://arxiv.org/abs/1703.02236 | id:1703.02236 author:Cheng Ju, Mary Combs, Samuel D Lendle, Jessica M Franklin, Richard Wyss, Sebastian Schneeweiss, Mark J. van der Laan category:stat.AP stat.ML  published:2017-03-07 summary:The optimal learner for prediction modeling varies depending on the underlying data-generating distribution. Super Learner (SL) is a generic ensemble learning algorithm that uses cross-validation to select among a "library" of candidate prediction models. The SL is not restricted to a single prediction model, but uses the strengths of a variety of learning algorithms to adapt to different databases. While the SL has been shown to perform well in a number of settings, it has not been thoroughly evaluated in large electronic healthcare databases that are common in pharmacoepidemiology and comparative effectiveness research. In this study, we applied and evaluated the performance of the SL in its ability to predict treatment assignment using three electronic healthcare databases. We considered a library of algorithms that consisted of both nonparametric and parametric models. We also considered a novel strategy for prediction modeling that combines the SL with the high-dimensional propensity score (hdPS) variable selection algorithm. Predictive performance was assessed using three metrics: the negative log-likelihood, area under the curve (AUC), and time complexity. Results showed that the best individual algorithm, in terms of predictive performance, varied across datasets. The SL was able to adapt to the given dataset and optimize predictive performance relative to any individual learner. Combining the SL with the hdPS was the most consistent prediction method and may be promising for PS estimation and prediction modeling in electronic healthcare databases. version:2
arxiv-1703-04665 | Geometry-Based Region Proposals for Real-Time Robot Detection of Tabletop Objects | http://arxiv.org/abs/1703.04665 | id:1703.04665 author:Alexander Broad, Brenna Argall category:cs.RO cs.CV  published:2017-03-14 summary:We present a novel object detection pipeline for localization and recognition in three dimensional environments. Our approach makes use of an RGB-D sensor and combines state-of-the-art techniques from the robotics and computer vision communities to create a robust, real-time detection system. We focus specifically on solving the object detection problem for tabletop scenes, a common environment for assistive manipulators. Our detection pipeline locates objects in a point cloud representation of the scene. These clusters are subsequently used to compute a bounding box around each object in the RGB space. Each defined patch is then fed into a Convolutional Neural Network (CNN) for object recognition. We also demonstrate that our region proposal method can be used to develop novel datasets that are both large and diverse enough to train deep learning models, and easy enough to collect that end-users can develop their own datasets. Lastly, we validate the resulting system through an extensive analysis of the accuracy and run-time of the full pipeline. version:1
arxiv-1703-04664 | Optimal Densification for Fast and Accurate Minwise Hashing | http://arxiv.org/abs/1703.04664 | id:1703.04664 author:Anshumali Shrivastava category:cs.DS cs.LG  published:2017-03-14 summary:Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown that it is possible to compute $k$ minwise hashes, of a vector with $d$ nonzeros, in mere $(d + k)$ computations, a significant improvement over the classical $O(dk)$. These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice. version:1
arxiv-1703-04653 | A Framework for Dynamic Image Sampling Based on Supervised Learning (SLADS) | http://arxiv.org/abs/1703.04653 | id:1703.04653 author:G. M. Dilshan P. Godaliyadda, Dong Hye Ye, Michael D. Uchic, Michael A. Groeber, Gregery T. Buzzard, Charles A. Bouman category:cs.CV  published:2017-03-14 summary:Sparse sampling schemes have the potential to dramatically reduce image acquisition time while simultaneously reducing radiation damage to samples. However, for a sparse sampling scheme to be useful it is important that we are able to reconstruct the underlying object with sufficient clarity using the sparse measurements. In dynamic sampling, each new measurement location is selected based on information obtained from previous measurements. Therefore, dynamic sampling schemes have the potential to dramatically reduce the number of measurements needed for high fidelity reconstructions. However, most existing dynamic sampling methods for point-wise measurement acquisition tend to be computationally expensive and are therefore too slow for practical applications. In this paper, we present a framework for dynamic sampling based on machine learning techniques, which we call a supervised learning approach for dynamic sampling (SLADS). In each step of SLADS, the objective is to find the pixel that maximizes the expected reduction in distortion (ERD) given previous measurements. SLADS is fast because we use a simple regression function to compute the ERD, and it is accurate because the regression function is trained using data sets that are representative of the specific application. In addition, we introduce a method to terminate dynamic sampling at a desired level of distortion, and we extended the SLADS methodology to sample groups of pixels at each step. Finally, we present results on computationally-generated synthetic data and experimentally-collected data to demonstrate a dramatic improvement over state-of-the-art static sampling methods. version:1
arxiv-1703-04650 | Joint Learning of Correlated Sequence Labelling Tasks Using Bidirectional Recurrent Neural Networks | http://arxiv.org/abs/1703.04650 | id:1703.04650 author:Vardaan Pahuja, Anirban Laha, Shachar Mirkin, Vikas Raykar, Lili Kotlerman, Guy Lev category:cs.CL  published:2017-03-14 summary:The stream of words produced by Automatic Speech Recognition (ASR) systems is devoid of any punctuations and formatting. Most natural language processing applications usually expect segmented and well-formatted texts as input, which is not available in ASR output. This paper proposes a novel technique of jointly modelling multiple correlated tasks such as punctuation and capitalization using bidirectional recurrent neural networks, which leads to improved performance for each of these tasks. This method can be extended for joint modelling of any other correlated multiple sequence labelling tasks. version:1
arxiv-1703-04636 | A PatchMatch-based Dense-field Algorithm for Video Copy-Move Detection and Localization | http://arxiv.org/abs/1703.04636 | id:1703.04636 author:Luca D'Amiano, Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva category:cs.CV  published:2017-03-14 summary:We propose a new algorithm for the reliable detection and localization of video copy-move forgeries. Discovering well crafted video copy-moves may be very difficult, especially when some uniform background is copied to occlude foreground objects. To reliably detect both additive and occlusive copy-moves we use a dense-field approach, with invariant features that guarantee robustness to several post-processing operations. To limit complexity, a suitable video-oriented version of PatchMatch is used, with a multiresolution search strategy, and a focus on volumes of interest. Performance assessment relies on a new dataset, designed ad hoc, with realistic copy-moves and a wide variety of challenging situations. Experimental results show the proposed method to detect and localize video copy-moves with good accuracy even in adverse conditions. version:1
arxiv-1703-04617 | Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering | http://arxiv.org/abs/1703.04617 | id:1703.04617 author:Junbei Zhang, Xiaodan Zhu, Qian Chen, Lirong Dai, Hui Jiang category:cs.CL  published:2017-03-14 summary:The last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA). In this paper, we approach the problems by closely modelling questions in a neural network framework. We first introduce syntactic information to help encode questions. We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline. version:1
arxiv-1703-04615 | Recasting Residual-based Local Descriptors as Convolutional Neural Networks: an Application to Image Forgery Detection | http://arxiv.org/abs/1703.04615 | id:1703.04615 author:Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva category:cs.CV  published:2017-03-14 summary:Local descriptors based on the image noise residual have proven extremely effective for a number of forensic applications, like forgery detection and localization. Nonetheless, motivated by promising results in computer vision, the focus of the research community is now shifting on deep learning. In this paper we show that a class of residual-based descriptors can be actually regarded as a simple constrained convolutional neural network (CNN). Then, by relaxing the constraints, and fine-tuning the net on a relatively small training set, we obtain a significant performance improvement with respect to the conventional detector. version:1
arxiv-1703-04611 | Subspace Learning in The Presence of Sparse Structured Outliers and Noise | http://arxiv.org/abs/1703.04611 | id:1703.04611 author:Shervin Minaee, Yao Wang category:cs.CV  published:2017-03-14 summary:Subspace learning is an important problem, which has many applications in image and video processing. It can be used to find a low-dimensional representation of signals and images. But in many applications, the desired signal is heavily distorted by outliers and noise, which negatively affect the learned subspace. In this work, we present a novel algorithm for learning a subspace for signal representation, in the presence of structured outliers and noise. The proposed algorithm tries to jointly detect the outliers and learn the subspace for images. We present an alternating optimization algorithm for solving this problem, which iterates between learning the subspace and finding the outliers. This algorithm has been trained on a large number of image patches, and the learned subspace is used for image segmentation, and is shown to achieve better segmentation results than prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm. version:1
arxiv-1703-04599 | Generalized Self-Concordant Functions: A Recipe for Newton-Type Methods | http://arxiv.org/abs/1703.04599 | id:1703.04599 author:Tianxiao Sun, Quoc Tran-Dinh category:math.OC stat.ML  published:2017-03-14 summary:We study the smooth structure of convex functions by generalizing a powerful concept so-called self-concordance introduced by Nesterov and Nemirovskii in the early 1990s to a broader class of convex functions, which we call generalized self-concordant functions. This notion allows us to develop a unified framework for designing Newton-type methods to solve convex optimiza- tion problems. The proposed theory provides a mathematical tool to analyze both local and global convergence of Newton-type methods without imposing unverifiable assumptions as long as the un- derlying functionals fall into our generalized self-concordant function class. First, we introduce the class of generalized self-concordant functions, which covers standard self-concordant functions as a special case. Next, we establish several properties and key estimates of this function class, which can be used to design numerical methods. Then, we apply this theory to develop several Newton-type methods for solving a class of smooth convex optimization problems involving the generalized self- concordant functions. We provide an explicit step-size for the damped-step Newton-type scheme which can guarantee a global convergence without performing any globalization strategy. We also prove a local quadratic convergence of this method and its full-step variant without requiring the Lipschitz continuity of the objective Hessian. Then, we extend our result to develop proximal Newton-type methods for a class of composite convex minimization problems involving generalized self-concordant functions. We also achieve both global and local convergence without additional assumption. Finally, we verify our theoretical results via several numerical examples, and compare them with existing methods. version:1
arxiv-1703-04590 | Learning Background-Aware Correlation Filters for Visual Tracking | http://arxiv.org/abs/1703.04590 | id:1703.04590 author:Hamed Kiani Galoogahi, Ashton Fagg, Simon Lucey category:cs.CV  published:2017-03-14 summary:Correlation Filters (CFs) have recently demonstrated excellent performance in terms of rapidly tracking objects under challenging photometric and geometric variations. The strength of the approach comes from its ability to efficiently learn - "on the fly" - how the object is changing over time. A fundamental drawback to CFs, however, is that the background of the object is not be modelled over time which can result in suboptimal results. In this paper we propose a Background-Aware CF that can model how both the foreground and background of the object varies over time. Our approach, like conventional CFs, is extremely computationally efficient - and extensive experiments over multiple tracking benchmarks demonstrate the superior accuracy and real-time performance of our method compared to the state-of-the-art trackers including those based on a deep learning paradigm. version:1
arxiv-1702-08699 | II-FCN for skin lesion analysis towards melanoma detection | http://arxiv.org/abs/1702.08699 | id:1702.08699 author:Hongdiao Wen category:cs.CV  published:2017-02-28 summary:Dermoscopy image detection stays a tough task due to the weak distinguishable property of the object.Although the deep convolution neural network signifigantly boosted the performance on prevelance computer vision tasks in recent years,there remains a room to explore more robust and precise models to the problem of low contrast image segmentation.Towards the challenge of Lesion Segmentation in ISBI 2017,we built a symmetrical identity inception fully convolution network which is based on only 10 reversible inception blocks,every block composed of four convolution branches with combination of different layer depth and kernel size to extract sundry semantic features.Then we proposed an approximate loss function for jaccard index metrics to train our model.To overcome the drawbacks of traditional convolution,we adopted the dilation convolution and conditional random field method to rectify our segmentation.We also introduced multiple ways to prevent the problem of overfitting.The experimental results shows that our model achived jaccard index of 0.82 and kept learning from epoch to epoch. version:2
arxiv-1702-06740 | Improving Chinese SRL with Heterogeneous Annotations | http://arxiv.org/abs/1702.06740 | id:1702.06740 author:Qiaolin Xia, Baobao Chang, Zhifang Sui category:cs.CL  published:2017-02-22 summary:Previous studies on Chinese semantic role labeling (SRL) have concentrated on single semantically annotated corpus. But the training data of single corpus is often limited. Meanwhile, there usually exists other semantically annotated corpora for Chinese SRL scattered across different annotation frameworks. Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In these papers, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that ours model outperforms state-of-the-art methods. version:3
arxiv-1703-04561 | Drone Squadron Optimization: a Self-adaptive Algorithm for Global Numerical Optimization | http://arxiv.org/abs/1703.04561 | id:1703.04561 author:Vinícius Veloso de Melo, Wolfgang Banzhaf category:math.OC cs.NE  published:2017-03-14 summary:This paper proposes Drone Squadron Optimization, a new self-adaptive metaheuristic for global numerical optimization which is updated online by a hyper-heuristic. DSO is an artifact-inspired technique, as opposed to many algorithms used nowadays, which are nature-inspired. DSO is very flexible because it is not related to behaviors or natural phenomena. DSO has two core parts: the semi-autonomous drones that fly over a landscape to explore, and the Command Center that processes the retrieved data and updates the drones' firmware whenever necessary. The self-adaptive aspect of DSO in this work is the perturbation/movement scheme, which is the procedure used to generate target coordinates. This procedure is evolved by the Command Center during the global optimization process in order to adapt DSO to the search landscape. DSO was evaluated on a set of widely employed benchmark functions. The statistical analysis of the results shows that the proposed method is competitive with the other methods in the comparison, the performance is promising, but several future improvements are planned. version:1
arxiv-1703-04559 | Fully Convolutional Networks to Detect Clinical Dermoscopic Features | http://arxiv.org/abs/1703.04559 | id:1703.04559 author:Jeremy Kawahara, Ghassan Hamarneh category:cs.CV  published:2017-03-14 summary:We use a pretrained fully convolutional neural network to detect clinical dermoscopic features from dermoscopy skin lesion images. We reformulate the superpixel classification task as an image segmentation problem, and extend a neural network architecture originally designed for image classification to detect dermoscopic features. Specifically, we interpolate the feature maps from several layers in the network to match the size of the input, concatenate the resized feature maps, and train the network to minimize a smoothed negative F1 score. Over the public validation leaderboard of the 2017 ISIC/ISBI Lesion Dermoscopic Feature Extraction Challenge, our approach achieves 89.3% AUROC, the highest averaged score when compared to the other two entries. Results over the private test leaderboard are still to be announced. version:1
arxiv-1702-07021 | One Size Fits Many: Column Bundle for Multi-X Learning | http://arxiv.org/abs/1702.07021 | id:1702.07021 author:Trang Pham, Truyen Tran, Svetha Venkatesh category:stat.ML cs.LG  published:2017-02-22 summary:Much recent machine learning research has been directed towards leveraging shared statistics among labels, instances and data views, commonly referred to as multi-label, multi-instance and multi-view learning. The underlying premises are that there exist correlations among input parts and among output targets, and the predictive performance would increase when the correlations are incorporated. In this paper, we propose Column Bundle (CLB), a novel deep neural network for capturing the shared statistics in data. CLB is generic that the same architecture can be applied for various types of shared statistics by changing only input and output handling. CLB is capable of scaling to thousands of input parts and output labels by avoiding explicit modeling of pairwise relations. We evaluate CLB on different types of data: (a) multi-label, (b) multi-view, (c) multi-view/multi-label and (d) multi-instance. CLB demonstrates a comparable and competitive performance in all datasets against state-of-the-art methods designed specifically for each type. version:2
arxiv-1703-04550 | Sensor Fusion for Robot Control through Deep Reinforcement Learning | http://arxiv.org/abs/1703.04550 | id:1703.04550 author:Steven Bohez, Tim Verbelen, Elias De Coninck, Bert Vankeirsbilck, Pieter Simoens, Bart Dhoedt category:cs.RO cs.LG cs.NE cs.SY  published:2017-03-13 summary:Deep reinforcement learning is becoming increasingly popular for robot control algorithms, with the aim for a robot to self-learn useful feature representations from unstructured sensory input leading to the optimal actuation policy. In addition to sensors mounted on the robot, sensors might also be deployed in the environment, although these might need to be accessed via an unreliable wireless connection. In this paper, we demonstrate deep neural network architectures that are able to fuse information coming from multiple sensors and are robust to sensor failures at runtime. We evaluate our method on a search and pick task for a robot both in simulation and the real world. version:1
arxiv-1703-04529 | Task-based End-to-end Model Learning | http://arxiv.org/abs/1703.04529 | id:1703.04529 author:Priya L. Donti, Brandon Amos, J. Zico Kolter category:cs.LG cs.AI  published:2017-03-13 summary:As machine learning techniques have become more ubiquitous, it has become common to see machine learning prediction algorithms operating within some larger process. However, the criteria by which we train machine learning algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models within the context of stochastic programming, in a manner that directly captures the ultimate task-based objective for which they will be used. We then present two experimental evaluations of the proposed approach, one as applied to a generic inventory stock problem and the second to a real-world electrical grid scheduling task. In both cases, we show that the proposed approach can outperform both a traditional modeling approach and a purely black-box policy optimization approach. version:1
arxiv-1703-04498 | High-Throughput and Language-Agnostic Entity Disambiguation and Linking on User Generated Data | http://arxiv.org/abs/1703.04498 | id:1703.04498 author:Preeti Bhargava, Nemanja Spasojevic, Guoning Hu category:cs.IR cs.AI cs.CL  published:2017-03-13 summary:The Entity Disambiguation and Linking (EDL) task matches entity mentions in text to a unique Knowledge Base (KB) identifier such as a Wikipedia or Freebase id. It plays a critical role in the construction of a high quality information network, and can be further leveraged for a variety of information retrieval and NLP tasks such as text categorization and document tagging. EDL is a complex and challenging problem due to ambiguity of the mentions and real world text being multi-lingual. Moreover, EDL systems need to have high throughput and should be lightweight in order to scale to large datasets and run on off-the-shelf machines. More importantly, these systems need to be able to extract and disambiguate dense annotations from the data in order to enable an Information Retrieval or Extraction task running on the data to be more efficient and accurate. In order to address all these challenges, we present the Lithium EDL system and algorithm - a high-throughput, lightweight, language-agnostic EDL system that extracts and correctly disambiguates 75% more entities than state-of-the-art EDL systems and is significantly faster than them. version:1
arxiv-1702-08896 | Deep and Hierarchical Implicit Models | http://arxiv.org/abs/1702.08896 | id:1702.08896 author:Dustin Tran, Rajesh Ranganath, David M. Blei category:stat.ML cs.LG stat.CO stat.ME  published:2017-02-28 summary:Implicit probabilistic models are a flexible class for modeling data. They define a process to simulate observations, and unlike traditional models, they do not require a tractable likelihood function. In this paper, we develop two families of models: hierarchical implicit models and deep implicit models. They combine the idea of implicit densities with hierarchical Bayesian modeling and deep neural networks. The use of implicit models with Bayesian analysis has been limited by our ability to perform accurate and scalable inference. We develop likelihood-free variational inference (LFVI). Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. Our work scales up implicit models to sizes previously not possible and advances their modeling design. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation. version:2
arxiv-1703-04496 | Comparison of echo state network output layer classification methods on noisy data | http://arxiv.org/abs/1703.04496 | id:1703.04496 author:Ashley Prater category:cs.NE  published:2017-03-13 summary:Echo state networks are a recently developed type of recurrent neural network where the internal layer is fixed with random weights, and only the output layer is trained on specific data. Echo state networks are increasingly being used to process spatiotemporal data in real-world settings, including speech recognition, event detection, and robot control. A strength of echo state networks is the simple method used to train the output layer - typically a collection of linear readout weights found using a least squares approach. Although straightforward to train and having a low computational cost to use, this method may not yield acceptable accuracy performance on noisy data. This study compares the performance of three echo state network output layer methods to perform classification on noisy data: using trained linear weights, using sparse trained linear weights, and using trained low-rank approximations of reservoir states. The methods are investigated experimentally on both synthetic and natural datasets. The experiments suggest that using regularized least squares to train linear output weights is superior on data with low noise, but using the low-rank approximations may significantly improve accuracy on datasets contaminated with higher noise levels. version:1
arxiv-1703-04489 | Reinforcement Learning for Transition-Based Mention Detection | http://arxiv.org/abs/1703.04489 | id:1703.04489 author:Georgiana Dinu, Wael Hamza, Radu Florian category:cs.CL cs.AI  published:2017-03-13 summary:This paper describes an application of reinforcement learning to the mention detection task. We define a novel action-based formulation for the mention detection task, in which a model can flexibly revise past labeling decisions by grouping together tokens and assigning partial mention labels. We devise a method to create mention-level episodes and we train a model by rewarding correctly labeled complete mentions, irrespective of the inner structure created. The model yields results which are on par with a competitive supervised counterpart while being more flexible in terms of achieving targeted behavior through reward modeling and generating internal mention structure, especially on longer mentions. version:1
arxiv-1703-04482 | Social Fingerprinting: detection of spambot groups through DNA-inspired behavioral modeling | http://arxiv.org/abs/1703.04482 | id:1703.04482 author:Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, Maurizio Tesconi category:cs.SI cs.CR cs.LG  published:2017-03-13 summary:Spambot detection in online social networks is a long-lasting challenge involving the study and design of detection techniques capable of efficiently identifying ever-evolving spammers. Recently, a new wave of social spambots has emerged, with advanced human-like characteristics that allow them to go undetected even by current state-of-the-art algorithms. In this paper, we show that efficient spambots detection can be achieved via an in-depth analysis of their collective behaviors exploiting the digital DNA technique for modeling the behaviors of social network users. Inspired by its biological counterpart, in the digital DNA representation the behavioral lifetime of a digital account is encoded in a sequence of characters. Then, we define a similarity measure for such digital DNA sequences. We build upon digital DNA and the similarity between groups of users to characterize both genuine accounts and spambots. Leveraging such characterization, we design the Social Fingerprinting technique, which is able to discriminate among spambots and genuine accounts in both a supervised and an unsupervised fashion. We finally evaluate the effectiveness of Social Fingerprinting and we compare it with three state-of-the-art detection algorithms. Among the peculiarities of our approach is the possibility to apply off-the-shelf DNA analysis techniques to study online users behaviors and to efficiently rely on a limited number of lightweight account characteristics. version:1
arxiv-1703-04481 | Geometrical morphology | http://arxiv.org/abs/1703.04481 | id:1703.04481 author:John Goldsmith, Eric Rosen category:cs.CL  published:2017-03-13 summary:We explore inflectional morphology as an example of the relationship of the discrete and the continuous in linguistics. The grammar requests a form of a lexeme by specifying a set of feature values, which corresponds to a corner M of a hypercube in feature value space. The morphology responds to that request by providing a morpheme, or a set of morphemes, whose vector sum is geometrically closest to the corner M. In short, the chosen morpheme $\mu$ is the morpheme (or set of morphemes) that maximizes the inner product of $\mu$ and M. version:1
arxiv-1703-02379 | Global Weisfeiler-Lehman Graph Kernels | http://arxiv.org/abs/1703.02379 | id:1703.02379 author:Christopher Morris, Kristian Kersting, Petra Mutzel category:cs.LG stat.ML  published:2017-03-07 summary:Most state-of-the-art graph kernels only take local graph properties into account, i.e., the kernel is computed with regard to properties of the neighborhood of vertices or other small substructures only. On the other hand, kernels that do take global graph properties into account may not scale well to large graph databases. Here we propose to start exploring the space between local and global graph kernels, striking the balance between both worlds. Specifically, we introduce a novel graph kernel based on the $k$-dimensional Weisfeiler-Lehman algorithm, and show that it takes local as well as global properties into account. Unfortunately, the $k$-dimensional Weisfeiler-Lehman scales exponentially in $k$. Consequently, we devise a stochastic version of the kernel with provable approximation guarantees using conditional Rademacher averages. On bounded-degree graphs, it can even be computed in constant time. We support our theoretical results with experiments on several graph classification benchmarks, showing that our kernels often outperform the state-of-the-art in terms of classification accuracies. version:2
arxiv-1703-04474 | DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks | http://arxiv.org/abs/1703.04474 | id:1703.04474 author:Lingpeng Kong, Chris Alberti, Daniel Andor, Ivan Bogatyy, David Weiss category:cs.CL  published:2017-03-13 summary:In this work, we present a compact, modular framework for constructing novel recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and re-cursive tree-structured models. A TBRU can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks. version:1
arxiv-1703-04455 | Multivariate Gaussian and Student$-t$ Process Regression for Multi-output Prediction | http://arxiv.org/abs/1703.04455 | id:1703.04455 author:Zexun Chen, Bo Wang, Alexander N. Gorban category:stat.ML  published:2017-03-13 summary:Gaussian process for vector-valued function model has been shown to be a useful method for multi-output prediction. The existing method for this model is to re-formulate the matrix-variate Gaussian distribution as a multivariate normal distribution. Although it is effective and convenient in many cases, re-formulation is not always workable and difficult to extend because not all matrix-variate distributions can be transformed to related multivariate distributions, such as the case for matrix-variate Student$-t$ distribution. In this paper, we propose a multivariate Gaussian process regression (MV-GPR) as well as multivariate Student$-t$ process regression (MV-TPR) for multi-output prediction, where the model settings, derivations, and computations are all performed in matrix form directly, rather than vectorizing the matrices involved. Compared with independent Gaussian process regression and Student$-t$ process regression models, both MV-GPR and MV-TPR significantly have the outstanding performances in the simulated examples and the corresponding Buy\&Sell strategies can be also shown profitable in stock market investment. In particular, MV-TPR has a distinct investment in NetEase among three Chinese stocks from NASDAQ. From the view of industrial sector, MV-GPR has considerable performances in Industrials, Consumer Goods, Consumer Services, and Financials sectors while MV-TPR can gain maximum profit in Health Care sector. version:1
arxiv-1703-04454 | Detailed, accurate, human shape estimation from clothed 3D scan sequences | http://arxiv.org/abs/1703.04454 | id:1703.04454 author:Chao Zhang, Sergi Pujades, Michael Black, Gerard Pons-Moll category:cs.CV  published:2017-03-13 summary:We address the problem of estimating human body shape from 3D scans over time. Reliable estimation of 3D body shape is necessary for many applications including virtual try-on, health monitoring, and avatar creation for virtual reality. Scanning bodies in minimal clothing, however, presents a practical barrier to these applications. We address this problem by estimating body shape under clothing from a sequence of 3D scans. Previous methods that have exploited statistical models of body shape produce overly smooth shapes lacking personalized details. In this paper we contribute a new approach to recover not only an approximate shape of the person, but also their detailed shape. Our approach allows the estimated shape to deviate from a parametric model to fit the 3D scans. We demonstrate the method using high quality 4D data as well as sequences of visual hulls extracted from multi-view images. We also make available a new high quality 4D dataset that enables quantitative evaluation. Our method outperforms the previous state of the art, both qualitatively and quantitatively. version:1
arxiv-1703-04446 | A Lagrangian Gauss-Newton-Krylov Solver for Mass- and Intensity-Preserving Diffeomorphic Image Registration | http://arxiv.org/abs/1703.04446 | id:1703.04446 author:Andreas Mang, Lars Ruthotto category:cs.CV cs.CE  published:2017-03-13 summary:We present an efficient solver for diffeomorphic image registration problems in the framework of Large Deformations Diffeomorphic Metric Mappings (LDDMM). We use an optimal control formulation, in which the velocity field of a hyperbolic PDE needs to be found such that the distance between the final state of the system (the transformed/transported template image) and the observation (the reference image) is minimized. Our solver supports both stationary and non-stationary (i.e., transient or time-dependent) velocity fields. As transformation models, we consider both the transport equation (assuming intensities are preserved during the deformation) and the continuity equation (assuming mass-preservation). We consider the reduced form of the optimal control problem and solve the resulting unconstrained optimization problem using a discretize-then-optimize approach. A key contribution is the elimination of the PDE constraint using a Lagrangian hyperbolic PDE solver. Lagrangian methods rely on the concept of characteristic curves that we approximate here using a fourth-order Runge-Kutta method. We also present an efficient algorithm for computing the derivatives of final state of the system with respect to the velocity field. This allows us to use fast Gauss-Newton based methods. We present quickly converging iterative linear solvers using spectral preconditioners that render the overall optimization efficient and scalable. Our method is embedded into the image registration framework FAIR and, thus, supports the most commonly used similarity measures and regularization functionals. We demonstrate the potential of our new approach using several synthetic and real world test problems with up to 14.7 million degrees of freedom. version:1
arxiv-1703-04421 | Guetzli: Perceptually Guided JPEG Encoder | http://arxiv.org/abs/1703.04421 | id:1703.04421 author:Jyrki Alakuijala, Robert Obryk, Ostap Stoliarchuk, Zoltan Szabadka, Lode Vandevenne, Jan Wassenberg category:cs.CV cs.HC  published:2017-03-13 summary:Guetzli is a new JPEG encoder that aims to produce visually indistinguishable images at a lower bit-rate than other common JPEG encoders. It optimizes both the JPEG global quantization tables and the DCT coefficient values in each JPEG block using a closed-loop optimizer. Guetzli uses Butteraugli, our perceptual distance metric, as the source of feedback in its optimization process. We reach a 29-45% reduction in data size for a given perceptual distance, according to Butteraugli, in comparison to other compressors we tried. Guetzli's computation is currently extremely slow, which limits its applicability to compressing static content and serving as a proof- of-concept that we can achieve significant reductions in size by combining advanced psychovisual models with lossy compression techniques. version:1
arxiv-1703-04418 | Improving LBP and its variants using anisotropic diffusion | http://arxiv.org/abs/1703.04418 | id:1703.04418 author:Mariane B. Neiva, Patrick Guidotti, Odemir M. Bruno category:cs.CV  published:2017-03-13 summary:The main purpose of this paper is to propose a new preprocessing step in order to improve local feature descriptors and texture classification. Preprocessing is implemented by using transformations which help highlight salient features that play a significant role in texture recognition. We evaluate and compare four different competing methods: three different anisotropic diffusion methods including the classical anisotropic Perona-Malik diffusion and two subsequent regularizations of it and the application of a Gaussian kernel, which is the classical multiscale approach in texture analysis. The combination of the transformed images and the original ones are analyzed. The results show that the use of the preprocessing step does lead to improved texture recognition. version:1
arxiv-1703-04417 | El Lenguaje Natural como Lenguaje Formal | http://arxiv.org/abs/1703.04417 | id:1703.04417 author:Franco M. Luque category:cs.CL cs.FL  published:2017-03-13 summary:Formal languages theory is useful for the study of natural language. In particular, it is of interest to study the adequacy of the grammatical formalisms to express syntactic phenomena present in natural language. First, it helps to draw hypothesis about the nature and complexity of the speaker-hearer linguistic competence, a fundamental question in linguistics and other cognitive sciences. Moreover, from an engineering point of view, it allows the knowledge of practical limitations of applications based on those formalisms. In this article I introduce the adequacy problem of grammatical formalisms for natural language, also introducing some formal language theory concepts required for this discussion. Then, I review the formalisms that have been proposed in history, and the arguments that have been given to support or reject their adequacy. ----- La teor\'ia de lenguajes formales es \'util para el estudio de los lenguajes naturales. En particular, resulta de inter\'es estudiar la adecuaci\'on de los formalismos gramaticales para expresar los fen\'omenos sint\'acticos presentes en el lenguaje natural. Primero, ayuda a trazar hip\'otesis acerca de la naturaleza y complejidad de las competencias ling\"u\'isticas de los hablantes-oyentes del lenguaje, un interrogante fundamental de la ling\"u\'istica y otras ciencias cognitivas. Adem\'as, desde el punto de vista de la ingenier\'ia, permite conocer limitaciones pr\'acticas de las aplicaciones basadas en dichos formalismos. En este art\'iculo hago una introducci\'on al problema de la adecuaci\'on de los formalismos gramaticales para el lenguaje natural, introduciendo tambi\'en algunos conceptos de la teor\'ia de lenguajes formales necesarios para esta discusi\'on. Luego, hago un repaso de los formalismos que han sido propuestos a lo largo de la historia, y de los argumentos que se han dado para sostener o refutar su adecuaci\'on. version:1
arxiv-1703-04416 | Users prefer Guetzli JPEG over same-sized libjpeg | http://arxiv.org/abs/1703.04416 | id:1703.04416 author:Jyrki Alakuijala, Robert Obryk, Zoltan Szabadka, Jan Wassenberg category:cs.CV cs.HC  published:2017-03-13 summary:We report on pairwise comparisons by human raters of JPEG images from libjpeg and our new Guetzli encoder. Although both files are size-matched, 75% of ratings are in favor of Guetzli. This implies the Butteraugli psychovisual image similarity metric which guides Guetzli is reasonably close to human perception at high quality levels. We provide access to the raw ratings and source images for further analysis and study. version:1
arxiv-1703-04394 | Zero-Shot Learning - The Good, the Bad and the Ugly | http://arxiv.org/abs/1703.04394 | id:1703.04394 author:Yongqin Xian, Bernt Schiele, Zeynep Akata category:cs.CV  published:2017-03-13 summary:Due to the importance of zero-shot learning, the number of proposed approaches has increased steadily recently. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss limitations of the current status of the area which can be taken as a basis for advancing it. version:1
arxiv-1703-04389 | Bayesian Optimization with Gradients | http://arxiv.org/abs/1703.04389 | id:1703.04389 author:Jian Wu, Matthias Poloczek, Andrew Gordon Wilson, Peter I. Frazier category:stat.ML cs.AI cs.LG math.OC  published:2017-03-13 summary:In recent years, Bayesian optimization has proven successful for global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, and comes in both sequential and batch forms. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, kernel learning, and k-nearest neighbors. version:1
arxiv-1703-04379 | Langevin Dynamics with Continuous Tempering for High-dimensional Non-convex Optimization | http://arxiv.org/abs/1703.04379 | id:1703.04379 author:Nanyang Ye, Zhanxing Zhu, Rafal K. Mantiuk category:cs.LG stat.ML  published:2017-03-13 summary:Minimizing non-convex and high-dimensional objective functions are challenging, especially when training modern deep neural networks. In this paper, a novel approach is proposed which divides the training process into two consecutive phases to obtain better generalization performance: Bayesian sampling and stochastic optimization. The first phase is to explore the energy landscape and to capture the "fat" modes; and the second one is to fine-tune the parameter learned from the first phase. In the Bayesian learning phase, we apply continuous tempering and stochastic approximation into the Langevin dynamics to create an efficient and effective sampler, in which the temperature is adjusted automatically according to the designed "temperature dynamics". These strategies can overcome the challenge of early trapping into bad local minima and have achieved remarkable improvements in various types of neural networks as shown in our theoretical analysis and empirical experiments. version:1
arxiv-1703-04364 | Deep Learning for Skin Lesion Classification | http://arxiv.org/abs/1703.04364 | id:1703.04364 author:P. Mirunalini, Aravindan Chandrabose, Vignesh Gokul, S. M. Jaisakthi category:cs.CV  published:2017-03-13 summary:Melanoma, a malignant form of skin cancer is very threatening to life. Diagnosis of melanoma at an earlier stage is highly needed as it has a very high cure rate. Benign and malignant forms of skin cancer can be detected by analyzing the lesions present on the surface of the skin using dermoscopic images. In this work, an automated skin lesion detection system has been developed which learns the representation of the image using Google's pretrained CNN model known as Inception-v3 \cite{cnn}. After obtaining the representation vector for our input dermoscopic images we have trained two layer feed forward neural network to classify the images as malignant or benign. The system also classifies the images based on the cause of the cancer either due to melanocytic or non-melanocytic cells using a different neural network. These classification tasks are part of the challenge organized by International Skin Imaging Collaboration (ISIC) 2017. Our system learns to classify the images based on the model built using the training images given in the challenge and the experimental results were evaluated using validation and test sets. Our system has achieved an overall accuracy of 65.8\% for the validation set. version:1
arxiv-1703-00410 | Detecting Adversarial Samples from Artifacts | http://arxiv.org/abs/1703.00410 | id:1703.00410 author:Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, Andrew B. Gardner category:stat.ML cs.LG  published:2017-03-01 summary:Deep neural networks (DNNs) are powerful nonlinear architectures that are known to be robust to random perturbations of the input. However, these models are vulnerable to adversarial perturbations--small input changes crafted explicitly to fool the model. In this paper, we ask whether a DNN can distinguish adversarial samples from their normal and noisy counterparts. We investigate model confidence on adversarial samples by looking at Bayesian uncertainty estimates, available in dropout neural networks, and by performing density estimation in the subspace of deep features learned by the model. The result is a method for implicit adversarial detection that is oblivious to the attack algorithm. We evaluate this method on a variety of standard datasets including MNIST and CIFAR-10 and show that it generalizes well across different architectures and attacks. Our findings report that 85-93% ROC-AUC can be achieved on a number of standard classification tasks with a negative class that consists of both normal and noisy samples. version:2
arxiv-1703-04363 | Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs | http://arxiv.org/abs/1703.04363 | id:1703.04363 author:Michael Gygli, Mohammad Norouzi, Anelia Angelova category:cs.LG cs.AI cs.CV  published:2017-03-13 summary:We approach structured output prediction by learning a deep value network (DVN) that evaluates different output structures for a given input. For example, when applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar score evaluating the mask quality and its correspondence with the image. Once the value network is optimized, at inference, it finds output structures that maximize the score of the value net via gradient descent on continuous relaxations of structured outputs. Thus DVN takes advantage of the joint modeling of the inputs and outputs. Our framework applies to a wide range of structured output prediction problems. We conduct experiments on multi-label classification based on text data and on image segmentation problems. DVN outperforms several strong baselines and the state-of-the-art results on these benchmarks. In addition, on image segmentation, the proposed deep value network learns complex shape priors and effectively combines image information with the prior to obtain competitive segmentation results. version:1
arxiv-1703-04357 | Nematus: a Toolkit for Neural Machine Translation | http://arxiv.org/abs/1703.04357 | id:1703.04357 author:Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel Läubli, Antonio Valerio Miceli Barone, Jozef Mokry, Maria Nădejde category:cs.CL  published:2017-03-13 summary:We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments. version:1
arxiv-1703-04347 | A Localisation-Segmentation Approach for Multi-label Annotation of Lumbar Vertebrae using Deep Nets | http://arxiv.org/abs/1703.04347 | id:1703.04347 author:Anjany Sekuboyina, Alexander Valentinitsch, Jan S. Kirschke, Bjoern H. Menze category:cs.CV  published:2017-03-13 summary:Multi-class segmentation of vertebrae is a non-trivial task mainly due to the high correlation in the appearance of adjacent vertebrae. Hence, such a task calls for the consideration of both global and local context. Based on this motivation, we propose a two-staged approach that, given a computed tomography dataset of the spine, segments the five lumbar vertebrae and simultaneously labels them. The first stage employs a multi-layered perceptron performing non-linear regression for locating the lumbar region using the global context. The second stage, comprised of a fully-convolutional deep network, exploits the local context in the localised lumbar region to segment and label the lumbar vertebrae in one go. Aided with practical data augmentation for training, our approach is highly generalisable, capable of successfully segmenting both healthy and abnormal vertebrae (fractured and scoliotic spines). We consistently achieve an average Dice coefficient of over 90 percent on a publicly available dataset of the xVertSeg segmentation challenge of MICCAI 2016. This is particularly noteworthy because the xVertSeg dataset is beset with severe deformities in the form of vertebral fractures and scoliosis. version:1
arxiv-1703-04336 | A Visual Representation of Wittgenstein's Tractatus Logico-Philosophicus | http://arxiv.org/abs/1703.04336 | id:1703.04336 author:Anca Bucur, Sergiu Nisioi category:cs.IR cs.CL  published:2017-03-13 summary:In this paper we present a data visualization method together with its potential usefulness in digital humanities and philosophy of language. We compile a multilingual parallel corpus from different versions of Wittgenstein's Tractatus Logico-Philosophicus, including the original in German and translations into English, Spanish, French, and Russian. Using this corpus, we compute a similarity measure between propositions and render a visual network of relations for different languages. version:1
arxiv-1703-04335 | Practical Bayesian Optimization for Variable Cost Objectives | http://arxiv.org/abs/1703.04335 | id:1703.04335 author:Mark McLeod, Michael A. Osborne, Stephen J. Roberts category:stat.ML  published:2017-03-13 summary:We propose a novel Bayesian Optimization approach for black-box functions with an environmental variable whose value determines the tradeoff between evaluation cost and the fidelity of the evaluations. Further, we use a novel approach to sampling support points, allowing faster construction of the acquisition function. This allows us to achieve optimization with lower overheads than previous approaches and is implemented for a more general class of problem. We show this approach to be effective on synthetic and real world benchmark problems. version:1
arxiv-1703-04334 | Probabilistic Matching: Causal Inference under Measurement Errors | http://arxiv.org/abs/1703.04334 | id:1703.04334 author:Fani Tsapeli, Peter Tino, Mirco Musolesi category:stat.ME stat.CO stat.ML  published:2017-03-13 summary:The abundance of data produced daily from large variety of sources has boosted the need of novel approaches on causal inference analysis from observational data. Observational data often contain noisy or missing entries. Moreover, causal inference studies may require unobserved high-level information which needs to be inferred from other observed attributes. In such cases, inaccuracies of the applied inference methods will result in noisy outputs. In this study, we propose a novel approach for causal inference when one or more key variables are noisy. Our method utilizes the knowledge about the uncertainty of the real values of key variables in order to reduce the bias induced by noisy measurements. We evaluate our approach in comparison with existing methods both on simulated and real scenarios and we demonstrate that our method reduces the bias and avoids false causal inference conclusions in most cases. version:1
arxiv-1703-04332 | What You Expect is NOT What You Get! Questioning Reconstruction/Classification Correlation of Stacked Convolutional Auto-Encoder Features | http://arxiv.org/abs/1703.04332 | id:1703.04332 author:Michele Alberti, Mathias Seuret, Rolf Ingold, Marcus Liwicki category:cs.CV I.2.6; I.5.2; I.7.5  published:2017-03-13 summary:In this paper, we thoroughly investigate the quality of features produced by deep neural network architectures obtained by stacking and convolving Auto-Encoders. In particular, we are interested into the relation of their reconstruction score with their performance on document layout analysis. When using Auto-Encoders, intuitively one could assume that features which are good for reconstruction will also lead to high classification accuracies. However, we prove that this is not always the case. We examine the reconstruction score, training error and the results obtained if we were to use the same features for both input reconstruction and a classification task. We show that the reconstruction score is not a good metric because it is biased by the decoder quality. Furthermore, experimental results suggest that there is no correlation between the reconstruction score and the quality of features for a classification task and that given the network size and configuration it is not possible to make assumptions on its training error magnitude. Therefore we conclude that both, reconstruction score and training error should not be used jointly to evaluate the quality of the features produced by a Stacked Convolutional Auto-Encoders for a classification task. Consequently one should independently investigate the network classification abilities directly. version:1
arxiv-1703-04330 | Story Cloze Ending Selection Baselines and Data Examination | http://arxiv.org/abs/1703.04330 | id:1703.04330 author:Todor Mihaylov, Anette Frank category:cs.CL  published:2017-03-13 summary:This paper describes two supervised baseline systems for the Story Cloze Test Shared Task (Mostafazadeh et al., 2016a). We first build a classifier using features based on word embeddings and semantic similarity computation. We further implement a neural LSTM system with different encoding strategies that try to model the relation between the story and the provided endings. Our experiments show that a model using representation features based on average word embedding vectors over the given story words and the candidate ending sentences words, joint with similarity features between the story and candidate ending representations performed better than the neural models. Our best model achieves an accuracy of 72.42, ranking 3rd in the official evaluation. version:1
arxiv-1703-04318 | Blocking Transferability of Adversarial Examples in Black-Box Learning Systems | http://arxiv.org/abs/1703.04318 | id:1703.04318 author:Hossein Hosseini, Yize Chen, Sreeram Kannan, Baosen Zhang, Radha Poovendran category:cs.LG  published:2017-03-13 summary:Advances in Machine Learning (ML) have led to its adoption as an integral component in many applications, including banking, medical diagnosis, and driverless cars. To further broaden the use of ML models, cloud-based services offered by Microsoft, Amazon, Google, and others have developed ML-as-a-service tools as black-box systems. However, ML classifiers are vulnerable to adversarial examples: inputs that are maliciously modified can cause the classifier to provide adversary-desired outputs. Moreover, it is known that adversarial examples generated on one classifier are likely to cause another classifier to make the same mistake, even if the classifiers have different architectures or are trained on disjoint datasets. This property, which is known as transferability, opens up the possibility of attacking black-box systems by generating adversarial examples on a substitute classifier and transferring the examples to the target classifier. Therefore, the key to protect black-box learning systems against the adversarial examples is to block their transferability. To this end, we propose a training method that, as the input is more perturbed, the classifier smoothly outputs lower confidence on the original label and instead predicts that the input is "invalid". In essence, we augment the output class set with a NULL label and train the classifier to reject the adversarial examples by classifying them as NULL. In experiments, we apply a wide range of attacks based on adversarial examples on the black-box systems. We show that a classifier trained with the proposed method effectively resists against the adversarial examples, while maintaining the accuracy on clean data. version:1
arxiv-1703-04309 | End-to-End Learning of Geometry and Context for Deep Stereo Regression | http://arxiv.org/abs/1703.04309 | id:1703.04309 author:Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, Adam Bry category:cs.CV cs.NE  published:2017-03-13 summary:We propose a novel deep learning architecture for regressing disparity from a rectified pair of stereo images. We leverage knowledge of the problem's geometry to form a cost volume using deep feature representations. We learn to incorporate contextual information using 3-D convolutions over this volume. Disparity values are regressed from the cost volume using a proposed differentiable soft argmin operation, which allows us to train our method end-to-end to sub-pixel accuracy without any additional post-processing or regularization. We evaluate our method on the Scene Flow and KITTI datasets and on KITTI we set a new state-of-the-art benchmark, while being significantly faster than competing approaches. version:1
arxiv-1703-04301 | Automatic Skin Lesion Segmentation using Semi-supervised Learning Technique | http://arxiv.org/abs/1703.04301 | id:1703.04301 author:S. M. Jaisakthi, Aravindan Chandrabose, P. Mirunalini category:cs.CV  published:2017-03-13 summary:Skin cancer is the most common of all cancers and each year million cases of skin cancer are treated. Treating and curing skin cancer is easy, if it is diagnosed and treated at an early stage. In this work we propose an automatic technique for skin lesion segmentation in dermoscopic images which helps in classifying the skin cancer types. The proposed method comprises of two major phases (1) preprocessing and (2) segmentation using semi-supervised learning algorithm. In the preprocessing phase noise are removed using filtering technique and in the segmentation phase skin lesions are segmented based on clustering technique. K-means clustering algorithm is used to cluster the preprocessed images and skin lesions are filtered from these clusters based on the color feature. Color of the skin lesions are learned from the training images using histograms calculations in RGB color space. The training images were downloaded from the ISIC 2017 challenge website and the experimental results were evaluated using validation and test sets. version:1
arxiv-1703-04274 | Online Learning with Local Permutations and Delayed Feedback | http://arxiv.org/abs/1703.04274 | id:1703.04274 author:Ohad Shamir, Liran Szlak category:cs.LG  published:2017-03-13 summary:We propose an Online Learning with Local Permutations (OLLP) setting, in which the learner is allowed to slightly permute the \emph{order} of the loss functions generated by an adversary. On one hand, this models natural situations where the exact order of the learner's responses is not crucial, and on the other hand, might allow better learning and regret performance, by mitigating highly adversarial loss sequences. Also, with random permutations, this can be seen as a setting interpolating between adversarial and stochastic losses. In this paper, we consider the applicability of this setting to convex online learning with delayed feedback, in which the feedback on the prediction made in round $t$ arrives with some delay $\tau$. With such delayed feedback, the best possible regret bound is well-known to be $O(\sqrt{\tau T})$. We prove that by being able to permute losses by a distance of at most $M$ (for $M\geq \tau$), the regret can be improved to $O(\sqrt{T}(1+\sqrt{\tau^2/M}))$, using a Mirror-Descent based algorithm which can be applied for both Euclidean and non-Euclidean geometries. We also prove a lower bound, showing that for $M<\tau/3$, it is impossible to improve the standard $O(\sqrt{\tau T})$ regret bound by more than constant factors. Finally, we provide some experiments validating the performance of our algorithm. version:1
arxiv-1703-04265 | Conjugate-Computation Variational Inference : Converting Variational Inference in Non-Conjugate Models to Inferences in Conjugate Models | http://arxiv.org/abs/1703.04265 | id:1703.04265 author:Mohammad Emtiyaz Khan, Wu Lin category:cs.LG  published:2017-03-13 summary:Variational inference is computationally challenging in models that contain both conjugate and non-conjugate terms. Methods specifically designed for conjugate models, even though computationally efficient, find it difficult to deal with non-conjugate terms. On the other hand, stochastic-gradient methods can handle the non-conjugate terms but they usually ignore the conjugate structure of the model which might result in slow convergence. In this paper, we propose a new algorithm called Conjugate-computation Variational Inference (CVI) which brings the best of the two worlds together -- it uses conjugate computations for the conjugate terms and employs stochastic gradients for the rest. We derive this algorithm by using a stochastic mirror-descent method in the mean-parameter space, and then expressing each gradient step as a variational inference in a conjugate model. We demonstrate our algorithm's applicability to a large class of models and establish its convergence. Our experimental results show that our method converges much faster than the methods that ignore the conjugate structure of the model. version:1
arxiv-1703-04264 | Poisson multi-Bernoulli mixture filter: direct derivation and implementation | http://arxiv.org/abs/1703.04264 | id:1703.04264 author:Ángel F. García-Fernández, Jason L. Williams, Karl Granström, Lennart Svensson category:cs.CV stat.ME  published:2017-03-13 summary:We provide a derivation of the Poisson multi-Bernoulli mixture (PMBM) filter for multi-target tracking with the standard point target measurements without using probability generating functionals or functional derivatives. We also establish the connection with the \delta-generalised labelled multi-Bernoulli (\delta-GLMB) filter, showing that a \delta-GLMB density represents a multi-Bernoulli mixture with labelled targets so it can be seen as a special case of PMBM. In addition, we propose an implementation for linear/Gaussian dynamic and measurement models and how to efficiently obtain typical estimators in the literature from the PMBM. The PMBM filter is shown to outperform other filters in the literature in a challenging scenario version:1
arxiv-1703-04247 | DeepFM: A Factorization-Machine based Neural Network for CTR Prediction | http://arxiv.org/abs/1703.04247 | id:1703.04247 author:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He category:cs.IR cs.CL  published:2017-03-13 summary:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its "wide" and "deep" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data. version:1
arxiv-1703-04244 | GUN: Gradual Upsampling Network for single image super-resolution | http://arxiv.org/abs/1703.04244 | id:1703.04244 author:Yang Zhao, Ronggang Wang, Weisheng Dong, Wei Jia, Jianchao Yang, Xiaoping Liu, Wen Gao category:cs.CV  published:2017-03-13 summary:In this paper, we propose an efficient super-resolution (SR) method based on deep convolutional neural network (CNN), namely gradual upsampling network (GUN). Recent CNN based SR methods either preliminarily magnify the low resolution (LR) input to high resolution (HR) and then reconstruct the HR input, or directly reconstruct the LR input and then recover the HR result at the last layer. The proposed GUN utilizes a gradual process instead of these two kinds of frameworks. The GUN consists of an input layer, multistep upsampling and convolutional layers, and an output layer. By means of the gradual process, the proposed network can simplify the difficult direct SR problem to multistep easier upsampling tasks with very small magnification factor in each step. Furthermore, a gradual training strategy is presented for the GUN. In the proposed training process, an initial network can be easily trained with edge-like samples, and then the weights are gradually tuned with more complex samples. The GUN can recover fine and vivid results, and is easy to be trained. The experimental results on several image sets demonstrate the effectiveness of the proposed network. version:1
arxiv-1703-03633 | Learning Gradient Descent: Better Generalization and Longer Horizons | http://arxiv.org/abs/1703.03633 | id:1703.03633 author:Kaifeng Lv, Shunhua Jiang, Jian Li category:cs.LG cs.AI  published:2017-03-10 summary:Training deep neural networks is a highly nontrivial task, involving carefully selecting appropriate training algorithms, scheduling step sizes and tuning other hyperparameters. Trying different combinations can be quite labor-intensive and time consuming. Recently, researchers have tried to use deep learning algorithms to exploit the landscape of the loss function of the training problem of interest, and learn how to optimize over it in an automatic way. In this paper, we propose a new learning-to-learn model and some useful and practical tricks. Our optimizer outperforms generic, hand-crafted optimization algorithms and state-of-the-art learning-to-learn optimizers by DeepMind in many tasks. We demonstrate the effectiveness of our algorithms on a number of tasks, including deep MLPs, CNNs, and simple LSTMs. version:2
arxiv-1703-02102 | Revisiting stochastic off-policy action-value gradients | http://arxiv.org/abs/1703.02102 | id:1703.02102 author:Yemi Okesanjo, Victor Kofia category:stat.ML cs.LG  published:2017-03-06 summary:Off-policy stochastic actor-critic methods rely on approximating the stochastic policy gradient in order to derive an optimal policy. One may also derive the optimal policy by approximating the action-value gradient. The use of action-value gradients is desirable as policy improvement occurs along the direction of steepest ascent. This has been studied extensively within the context of natural gradient actor-critic algorithms and more recently within the context of deterministic policy gradients. In this paper we briefly discuss the off-policy stochastic counterpart to deterministic action-value gradients, as well as an incremental approach for following the policy gradient in lieu of the natural gradient. version:2
arxiv-1703-04219 | SPARTan: Scalable PARAFAC2 for Large & Sparse Data | http://arxiv.org/abs/1703.04219 | id:1703.04219 author:Ioakeim Perros, Evangelos E. Papalexakis, Fei Wang, Richard Vuduc, Elizabeth Searles, Michael Thompson, Jimeng Sun category:cs.LG cs.NA  published:2017-03-13 summary:In exploratory tensor mining, a common problem is how to analyze a set of variables across a set of subjects whose observations do not align naturally. For example, when modeling medical features across a set of patients, the number and duration of treatments may vary widely in time, meaning there is no meaningful way to align their clinical records across time points for analysis purposes. To handle such data, the state-of-the-art tensor model is the so-called PARAFAC2, which yields interpretable and robust output and can naturally handle sparse data. However, its main limitation up to now has been the lack of efficient algorithms that can handle large-scale datasets. In this work, we fill this gap by developing a scalable method to compute the PARAFAC2 decomposition of large and sparse datasets, called SPARTan. Our method exploits special structure within PARAFAC2, leading to a novel algorithmic reformulation that is both fast (in absolute time) and more memory-efficient than prior work. We evaluate SPARTan on both synthetic and real datasets, showing 22X performance gains over the best previous implementation and also handling larger problem instances for which the baseline fails. Furthermore, we are able to apply SPARTan to the mining of temporally-evolving phenotypes on data taken from real and medically complex pediatric patients. The clinical meaningfulness of the phenotypes identified in this process, as well as their temporal evolution over time for several patients, have been endorsed by clinical experts. version:1
arxiv-1703-01056 | Gauge Optimization via ADMM for Approximate Inference | http://arxiv.org/abs/1703.01056 | id:1703.01056 author:Sungsoo Ahn, Michael Chertkov, Jinwoo Shin category:stat.ML  published:2017-03-03 summary:Computing partition function is the most important inference task arising in applications of Graphical Models (GM). Since it is computationally intractable, approximate algorithms are used to tackle the problem. In this paper, by relying on the technique, coined gauge transformation, modifying GM factors such that the partition function stay the same (invariant), we propose two optimization formulations which generalize the Bethe Free Energy, Belief Propagation approach. Then, we show that the optimizations can be solved efficiently by Alternating Direction Method of Multipliers (ADMM) algorithms. Our first algorithm provides deterministic lower bounds of the partition function. The algorithm is exact for GMs over a single loop with a special structure, even though the popular Belief Propagation algorithm performs badly in this case. Our second algorithm is of a randomized, Monte Carlo, type. It lowers sample variance, which can be further reduced with the help of annealed/sequential/adaptive importance sampling. The experiments show that the newly proposed Gauge-ADMM algorithms outperform other known algorithms for the approximate inference task. version:3
arxiv-1703-04200 | Improved multitask learning through synaptic intelligence | http://arxiv.org/abs/1703.04200 | id:1703.04200 author:Friedemann Zenke, Ben Poole, Surya Ganguli category:cs.LG q-bio.NC stat.ML  published:2017-03-13 summary:Deep learning has led to remarkable advances when applied to problems where the data distribution does not change over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, and solve a diversity of tasks simultaneously. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery enabling non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce a model of intelligent synapses that accumulate task relevant information over time, and exploit this information to efficiently consolidate memories of old tasks to protect them from being overwritten as new tasks are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency. version:1
arxiv-1703-02435 | Unsupervised learning of phase transitions: from principal component analysis to variational autoencoders | http://arxiv.org/abs/1703.02435 | id:1703.02435 author:Sebastian Johann Wetzel category:cond-mat.stat-mech cs.LG stat.ML  published:2017-03-07 summary:We employ unsupervised machine learning techniques to learn latent parameters which best describe states of the two-dimensional Ising model and the three-dimensional XY model. These methods range from principal component analysis to artificial neural network based variational autoencoders. The states are sampled using a Monte-Carlo simulation above and below the critical temperature. We find that the predicted latent parameters correspond to the known order parameters. The latent representation of the states of the models in question are clustered, which makes it possible to identify phases without prior knowledge of their existence or the underlying Hamiltonian. Furthermore, we find that the reconstruction loss function can be used as a universal identifier for phase transitions. version:2
arxiv-1703-04197 | Automatic Skin Lesion Analysis using Large-scale Dermoscopy Images and Deep Residual Networks | http://arxiv.org/abs/1703.04197 | id:1703.04197 author:Lei Bi, Jinman Kim, Euijoon Ahn, Dagan Feng category:cs.CV  published:2017-03-12 summary:Malignant melanoma has one of the most rapidly increasing incidences in the world and has a considerable mortality rate. Early diagnosis is particularly important since melanoma can be cured with prompt excision. Dermoscopy images play an important role in the non-invasive early detection of melanoma [1]. However, melanoma detection using human vision alone can be subjective, inaccurate and poorly reproducible even among experienced dermatologists. This is attributed to the challenges in interpreting images with diverse characteristics including lesions of varying sizes and shapes, lesions that may have fuzzy boundaries, different skin colors and the presence of hair [2]. Therefore, the automatic analysis of dermoscopy images is a valuable aid for clinical decision making and for image-based diagnosis to identify diseases such as melanoma [1-4]. Deep residual networks (ResNets) has achieved state-of-the-art results in image classification and detection related problems [5-8]. In this ISIC 2017 skin lesion analysis challenge [9], we propose to exploit the deep ResNets for robust visual features learning and representations. version:1
arxiv-1702-08882 | Deep Semi-Random Features for Nonlinear Function Approximation | http://arxiv.org/abs/1702.08882 | id:1702.08882 author:Kenji Kawaguchi, Bo Xie, Le Song category:cs.LG cs.NE stat.ML  published:2017-02-28 summary:We propose semi-random features for nonlinear function approximation. Semi-random features are defined as the product of a random nonlinear switching unit and a linear adjustable unit. The flexibility of semi-random feature lies between the fully adjustable units in deep learning and the random features used in kernel methods. We show that semi-random features possess a collection of nice theoretical properties despite the non-convex nature of its learning problem. In experiments, we show that semi-random features can match the performance of neural networks by using slightly more units, and it outperforms random features by using significantly fewer units. Semi-random features provide an interesting data point in between kernel methods and neural networks to advance our understanding of the challenge of nonlinear function approximation, and it opens up new avenues to tackle the challenge further. version:2
arxiv-1703-04706 | Tree Memory Networks for Modelling Long-term Temporal Dependencies | http://arxiv.org/abs/1703.04706 | id:1703.04706 author:Tharindu Fernando, Simon Denman, Aaron McFadyen, Sridha Sridharan, Clinton Fookes category:cs.LG cs.CV cs.NE  published:2017-03-12 summary:In the domain of sequence modelling, Recurrent Neural Networks (RNN) have been capable of achieving impressive results in a variety of application areas including visual question answering, part-of-speech tagging and machine translation. However this success in modelling short term dependencies has not successfully transitioned to application areas such as trajectory prediction, which require capturing both short term and long term relationships. In this paper, we propose a Tree Memory Network (TMN) for modelling long term and short term relationships in sequence-to-sequence mapping problems. The proposed network architecture is composed of an input module, controller and a memory module. In contrast to related literature, which models the memory as a sequence of historical states, we model the memory as a recursive tree structure. This structure more effectively captures temporal dependencies across both short term and long term sequences using its hierarchical structure. We demonstrate the effectiveness and flexibility of the proposed TMN in two practical problems, aircraft trajectory modelling and pedestrian trajectory modelling in a surveillance setting, and in both cases we outperform the current state-of-the-art. Furthermore, we perform an in depth analysis on the evolution of the memory module content over time and provide visual evidence on how the proposed TMN is able to map both long term and short term relationships efficiently via a hierarchical structure. version:1
arxiv-1703-04178 | Why we have switched from building full-fledged taxonomies to simply detecting hypernymy relations | http://arxiv.org/abs/1703.04178 | id:1703.04178 author:Jose Camacho-Collados category:cs.CL  published:2017-03-12 summary:The study of taxonomies and hypernymy relations has been extensive on the Natural Language Processing (NLP) literature. However, the evaluation of taxonomy learning approaches has traditionally troublesome, as it mainly relies on ad-hoc experiments, which are hardly reproducible and manually expensive. Partly because of this, current research has been lately focusing on the hypernymy detection task. In this paper we reflect on this trend, analyzing issues related to current evaluation procedures. Finally, we propose three potential avenues for future work so that is-a relations and resources based on them play a more important role in downstream NLP applications. version:1
arxiv-1703-04170 | Leak Event Identification in Water Systems Using High Order CRF | http://arxiv.org/abs/1703.04170 | id:1703.04170 author:Qing Han, Wentao Zhu, Yang Shi category:cs.LG  published:2017-03-12 summary:Today, detection of anomalous events in civil infrastructures (e.g. water pipe breaks and leaks) is time consuming and often takes hours or days. Pipe breakage as one of the most frequent types of failure of water networks often causes community disruptions ranging from temporary interruptions in services to extended loss of business and relocation of residents. In this project, we design and implement a two-phase approach for leak event identification, which leverages dynamic data from multiple information sources including IoT sensing data (pressure values and/or flow rates), geophysical data (water systems), and human inputs (tweets posted on Twitter). In the approach, a high order Conditional Random Field (CRF) is constructed that enforces predictions based on IoT observations consistent with human inputs to improve the performance of event identifications. Considering the physical water network as a graph, a CRF model is built and learned by the Structured Support Vector Machine (SSVM) using node features such as water pressure and flow rate. After that, we built the high order CRF system by enforcing twitter leakage detection information. An optimal inference algorithm is proposed for the adapted high order CRF model. Experimental results show the effectiveness of our system. version:1
arxiv-1703-04145 | Robustness from structure: Inference with hierarchical spiking networks on analog neuromorphic hardware | http://arxiv.org/abs/1703.04145 | id:1703.04145 author:Mihai A. Petrovici, Anna Schroeder, Oliver Breitwieser, Andreas Grübl, Johannes Schemmel, Karlheinz Meier category:q-bio.NC cs.NE stat.ML  published:2017-03-12 summary:How spiking networks are able to perform probabilistic inference is an intriguing question, not only for understanding information processing in the brain, but also for transferring these computational principles to neuromorphic silicon circuits. A number of computationally powerful spiking network models have been proposed, but most of them have only been tested, under ideal conditions, in software simulations. Any implementation in an analog, physical system, be it in vivo or in silico, will generally lead to distorted dynamics due to the physical properties of the underlying substrate. In this paper, we discuss several such distortive effects that are difficult or impossible to remove by classical calibration routines or parameter training. We then argue that hierarchical networks of leaky integrate-and-fire neurons can offer the required robustness for physical implementation and demonstrate this with both software simulations and emulation on an accelerated analog neuromorphic device. version:1
arxiv-1703-04140 | Multiscale Hierarchical Convolutional Networks | http://arxiv.org/abs/1703.04140 | id:1703.04140 author:Jörn-Henrik Jacobsen, Edouard Oyallon, Stéphane Mallat, Arnold W. M. Smeulders category:cs.LG stat.ML  published:2017-03-12 summary:Deep neural network algorithms are difficult to analyze because they lack structure allowing to understand the properties of underlying transforms and invariants. Multiscale hierarchical convolutional networks are structured deep convolutional networks where layers are indexed by progressively higher dimensional attributes, which are learned from training data. Each new layer is computed with multidimensional convolutions along spatial and attribute variables. We introduce an efficient implementation of such networks where the dimensionality is progressively reduced by averaging intermediate layers along attribute indices. Hierarchical networks are tested on CIFAR image data bases where they obtain comparable precisions to state of the art networks, with much fewer parameters. We study some properties of the attributes learned from these databases. version:1
arxiv-1703-04135 | Hardware-Driven Nonlinear Activation for Stochastic Computing Based Deep Convolutional Neural Networks | http://arxiv.org/abs/1703.04135 | id:1703.04135 author:Ji Li, Zihao Yuan, Zhe Li, Caiwen Ding, Ao Ren, Qinru Qiu, Jeffrey Draper, Yanzhi Wang category:cs.CV  published:2017-03-12 summary:Recently, Deep Convolutional Neural Networks (DCNNs) have made unprecedented progress, achieving the accuracy close to, or even better than human-level perception in various tasks. There is a timely need to map the latest software DCNNs to application-specific hardware, in order to achieve orders of magnitude improvement in performance, energy efficiency and compactness. Stochastic Computing (SC), as a low-cost alternative to the conventional binary computing paradigm, has the potential to enable massively parallel and highly scalable hardware implementation of DCNNs. One major challenge in SC based DCNNs is designing accurate nonlinear activation functions, which have a significant impact on the network-level accuracy but cannot be implemented accurately by existing SC computing blocks. In this paper, we design and optimize SC based neurons, and we propose highly accurate activation designs for the three most frequently used activation functions in software DCNNs, i.e, hyperbolic tangent, logistic, and rectified linear units. Experimental results on LeNet-5 using MNIST dataset demonstrate that compared with a binary ASIC hardware DCNN, the DCNN with the proposed SC neurons can achieve up to 61X, 151X, and 2X improvement in terms of area, power, and energy, respectively, at the cost of small precision degradation.In addition, the SC approach achieves up to 21X and 41X of the area, 41X and 72X of the power, and 198200X and 96443X of the energy, compared with CPU and GPU approaches, respectively, while the error is increased by less than 3.07%. ReLU activation is suggested for future SC based DCNNs considering its superior performance under a small bit stream length. version:1
arxiv-1703-04122 | Autoregressive Convolutional Neural Networks for Asynchronous Time Series | http://arxiv.org/abs/1703.04122 | id:1703.04122 author:Mikolaj Binkowski, Gautier Marti, Philippe Donnat category:cs.LG  published:2017-03-12 summary:We propose 'Significance-Offset Convolutional Neural Network', a deep convolutional network architecture for multivariate time series regression. The model is inspired by standard autoregressive (AR) models and gating mechanisms used in recurrent neural networks. It involves an AR-like weighting system, where the final predictor is obtained as a weighted sum of sub-predictors while the weights are data-dependent functions learnt through a convolutional network.The architecture was designed for applications on asynchronous time series with low signal-to-noise ratio and hence is evaluated on such datasets: a hedge fund proprietary dataset of over2 million quotes for a credit derivative index andan artificially generated noisy autoregressive series. The proposed architecture achieves promising results compared to convolutional and recur-rent neural networks. The code for the numerical experiments and the architecture implementation will be shared online to make the research reproducible. version:1
arxiv-1703-04111 | Co-occurrence Filter | http://arxiv.org/abs/1703.04111 | id:1703.04111 author:Roy J Jevnisek, Shai Avidan category:cs.CV  published:2017-03-12 summary:Co-occurrence Filter (CoF) is a boundary preserving filter. It is based on the Bilateral Filter (BF) but instead of using a Gaussian on the range values to preserve edges it relies on a co-occurrence matrix. Pixel values that co-occur frequently in the image (i.e., inside textured regions) will have a high weight in the co-occurrence matrix. This, in turn, means that such pixel pairs will be averaged and hence smoothed, regardless of their intensity differences. On the other hand, pixel values that rarely co-occur (i.e., across texture boundaries) will have a low weight in the co-occurrence matrix. As a result, they will not be averaged and the boundary between them will be preserved. The CoF therefore extends the BF to deal with boundaries, not just edges. It learns co-occurrences directly from the image. We can achieve various filtering results by directing it to learn the co-occurrence matrix from a part of the image, or a different image. We give the definition of the filter, discuss how to use it with color images and show several use cases. version:1
arxiv-1703-04096 | Improving Interpretability of Deep Neural Networks with Semantic Information | http://arxiv.org/abs/1703.04096 | id:1703.04096 author:Yinpeng Dong, Hang Su, Jun Zhu, Bo Zhang category:cs.CV  published:2017-03-12 summary:Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure. version:1
arxiv-1703-04088 | Local Patch Classification Based Framework for Single Image Super-Resolution | http://arxiv.org/abs/1703.04088 | id:1703.04088 author:Yang Zhao, Ronggang Wang, Wei Jia, Jianchao Yang, Wenmin Wang, Wen Gao category:cs.CV  published:2017-03-12 summary:Recent learning-based super-resolution (SR) methods often focus on the dictionary learning or network training. In this paper, we detailedly discuss a new SR framework based on local classification instead of traditional dictionary learning. The proposed efficient and extendible SR framework is named as local patch classification (LPC) based framework. The LPC framework consists of a learning stage and a reconstructing stage. In the learning stage, image patches are classified into different classes by means of the proposed local patch encoding (LPE), and then a projection matrix is computed for each class by utilizing a simple constraint. In the reconstructing stage, an input LR patch can be simply reconstructed by computing its LPE code and then multiplying corresponding projection matrix. Furthermore, we establish the relationship between the proposed method and the anchored neighborhood regression based methods; and we also analyze the extendibility of the proposed framework. The experimental results on several image sets demonstrate the effectiveness of the proposed framework. version:1
arxiv-1703-04081 | Feature overwriting as a finite mixture process: Evidence from comprehension data | http://arxiv.org/abs/1703.04081 | id:1703.04081 author:Shravan Vasishth, Lena A. Jaeger, Bruno Nicenboim category:stat.ML cs.CL stat.AP  published:2017-03-12 summary:The ungrammatical sentence "The key to the cabinets are on the table" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence "The key to the cabinet are on the table". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account is in terms of cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the auxiliary initiates a memory access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts. version:1
arxiv-1703-04079 | SurfNet: Generating 3D shape surfaces using deep residual networks | http://arxiv.org/abs/1703.04079 | id:1703.04079 author:Ayan Sinha, Asim Unmesh, Qixing Huang, Karthik Ramani category:cs.CV cs.CG  published:2017-03-12 summary:3D shape models are naturally parameterized using vertices and faces, \ie, composed of polygons forming a surface. However, current 3D learning paradigms for predictive and generative tasks using convolutional neural networks focus on a voxelized representation of the object. Lifting convolution operators from the traditional 2D to 3D results in high computational overhead with little additional benefit as most of the geometry information is contained on the surface boundary. Here we study the problem of directly generating the 3D shape surface of rigid and non-rigid shapes using deep convolutional neural networks. We develop a procedure to create consistent `geometry images' representing the shape surface of a category of 3D objects. We then use this consistent representation for category-specific shape surface generation from a parametric representation or an image by developing novel extensions of deep residual networks for the task of geometry image generation. Our experiments indicate that our network learns a meaningful representation of shape surfaces allowing it to interpolate between shape orientations and poses, invent new shape surfaces and reconstruct 3D shape surfaces from previously unseen images. version:1
arxiv-1703-04078 | Prostate Cancer Diagnosis using Deep Learning with 3D Multiparametric MRI | http://arxiv.org/abs/1703.04078 | id:1703.04078 author:Saifeng Liu, Huaixiu Zheng, Yesu Feng, Wei Li category:cs.CV stat.ML  published:2017-03-12 summary:A novel deep learning architecture (XmasNet) based on convolutional neural networks was developed for the classification of prostate cancer lesions, using the 3D multiparametric MRI data provided by the PROSTATEx challenge. End-to-end training was performed for XmasNet, with data augmentation done through 3D rotation and slicing, in order to incorporate the 3D information of the lesion. XmasNet outperformed traditional machine learning models based on engineered features, for both train and test data. For the test data, XmasNet outperformed 69 methods from 33 participating groups and achieved the second highest AUC (0.84) in the PROSTATEx challenge. This study shows the great potential of deep learning for cancer imaging. version:1
arxiv-1703-04071 | A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation | http://arxiv.org/abs/1703.04071 | id:1703.04071 author:Chunpeng Wu, Wei Wen, Tariq Afzal, Yongmei Zhang, Yiran Chen, Hai Li category:cs.CV cs.AI cs.NE  published:2017-03-12 summary:Recently, DNN model compression based on network architecture design, e.g., SqueezeNet, attracted a lot attention. No accuracy drop on image classification is observed on these extremely compact networks, compared to well-known models. An emerging question, however, is whether these model compression techniques hurt DNN's learning ability other than classifying images on a single dataset. Our preliminary experiment shows that these compression methods could degrade domain adaptation (DA) ability, though the classification performance is preserved. Therefore, we propose a new compact network architecture and unsupervised DA method in this paper. The DNN is built on a new basic module Conv-M which provides more diverse feature extractors without significantly increasing parameters. The unified framework of our DA method will simultaneously learn invariance across domains, reduce divergence of feature representations, and adapt label prediction. Our DNN has 4.1M parameters, which is only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN obtains GoogLeNet-level accuracy both on classification and DA, and our DA method slightly outperforms previous competitive ones. Put all together, our DA strategy based on our DNN achieves state-of-the-art on sixteen of total eighteen DA tasks on popular Office-31 and Office-Caltech datasets. version:1
arxiv-1703-04070 | Prediction and Control with Temporal Segment Models | http://arxiv.org/abs/1703.04070 | id:1703.04070 author:Nikhil Mishra, Pieter Abbeel, Igor Mordatch category:cs.LG cs.RO  published:2017-03-12 summary:We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method. version:1
arxiv-1703-04062 | Multi-Pose Face Recognition Using Hybrid Face Features Descriptor | http://arxiv.org/abs/1703.04062 | id:1703.04062 author:I Gede Pasek Suta Wijaya, Keiichi Uchimura, Gou Koutaki category:cs.CV  published:2017-03-12 summary:This paper presents a multi-pose face recognition approach using hybrid face features descriptors (HFFD). The HFFD is a face descriptor containing of rich discriminant information that is created by fusing some frequency-based features extracted using both wavelet and DCT analysis of several different poses of 2D face images. The main aim of this method is to represent the multi-pose face images using a dominant frequency component with still having reasonable achievement compared to the recent multi-pose face recognition methods. The HFFD based face recognition tends to achieve better performance than that of the recent 2D-based face recognition method. In addition, the HFFD-based face recognition also is sufficiently to handle large face variability due to face pose variations . version:1
arxiv-1703-04046 | DeepSleepNet: a Model for Automatic Sleep Stage Scoring based on Raw Single-Channel EEG | http://arxiv.org/abs/1703.04046 | id:1703.04046 author:Akara Supratak, Hao Dong, Chao Wu, Yike Guo category:stat.ML  published:2017-03-12 summary:Objective: The present study proposes a deep learn- ing model, named DeepSleepNet, for automatic sleep stage scoring based on raw single-channel EEG, and a two-step training algorithm used to effectively train such model. Methods: Most of the existing methods rely on hand-engineered features which require prior knowledge about sleep stage scoring. Only a few of them encode the temporal information such as stage transition rules, which is important to correctly identify the next possible sleep stages, into the extracted features. In the proposed model, we utilize Convolutional Neural Networks (CNNs) to extract time-invariant features, and bidirectional Long Short- Term Memory (bidirectional-LSTM) to learn transition rules among sleep stages from EEG epochs. We implement a two-step training algorithm that pre-trains the model with oversampled dataset to alleviate class-imbalance problems and fine-tunes the model with sequences of EEG epochs to encode the temporal information into the model. Results: We applied our model to the F4-EOG (Left) channel from a set of 62 subjects in an open- access database, containing 58600 EEG epochs (~488 hours). The results demonstrated that our model scored the EEG epochs with the accuracy of 86.2% and the macro F1-score of 81.7. Conclusion: Without utilizing any hand-engineered features, our model can achieve a similar sleep stage scoring performance with the highest macro F1-score compared to the state-of-the- art methods. Significance: This study proposes a deep learning model that can automatically learn features from raw single- channel EEG, and accurately score EEG epochs as good as the state-of-the-art hand-engineering methods. version:1
arxiv-1703-04044 | Colorization as a Proxy Task for Visual Understanding | http://arxiv.org/abs/1703.04044 | id:1703.04044 author:Gustav Larsson, Michael Maire, Gregory Shakhnarovich category:cs.CV  published:2017-03-11 summary:We investigate and improve self-supervision as a drop-in replacement for ImageNet pretraining, focusing on automatic colorization as the proxy task. Self-supervised training has been shown to be more promising for utilizing unlabeled data than other, traditional unsupervised learning methods. We show the ability of our self-supervised network in several contexts. On VOC segmentation and classification tasks, we present results that are state-of-the-art among methods not using ImageNet labels for pretraining representations. Moreover, we present the first in-depth analysis of self-supervision via colorization, concluding that formulation of the loss, training details and network architecture play important roles in its effectiveness. This investigation is further expanded by revisiting the ImageNet pretraining paradigm, asking questions such as: How much training data is needed? How many labels are needed? How much do features change when fine-tuned? We relate these questions back to self-supervision by showing that self-supervised colorization provides a similarly powerful supervision as various flavors of ImageNet pretraining. version:1
arxiv-1703-04025 | Learning Large-Scale Bayesian Networks with the sparsebn Package | http://arxiv.org/abs/1703.04025 | id:1703.04025 author:Bryon Aragam, Jiaying Gu, Qing Zhou category:stat.ML cs.LG stat.CO stat.ME  published:2017-03-11 summary:Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets typically have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we develop a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing packages for this task within the R ecosystem, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. The sparsebn package is open-source and available on CRAN. version:1
arxiv-1702-04457 | Automated Phrase Mining from Massive Text Corpora | http://arxiv.org/abs/1702.04457 | id:1702.04457 author:Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han category:cs.CL  published:2017-02-15 summary:As one of the fundamental tasks in text analysis, phrase mining aims at extracting quality phrases from a text corpus. Phrase mining is important in various tasks such as information extraction/retrieval, taxonomy construction, and topic modeling. Most existing methods rely on complex, trained linguistic analyzers, and thus likely have unsatisfactory performance on text corpora of new domains and genres without extra but expensive adaption. Recently, a few data-driven methods have been developed successfully for extraction of phrases from massive domain-specific text. However, none of the state-of-the-art models is fully automated because they require human experts for designing rules or labeling phrases. Since one can easily obtain many quality phrases from public knowledge bases to a scale that is much larger than that produced by human experts, in this paper, we propose a novel framework for automated phrase mining, AutoPhrase, which leverages this large amount of high-quality phrases in an effective way and achieves better performance compared to limited human labeled phrases. In addition, we develop a POS-guided phrasal segmentation model, which incorporates the shallow syntactic information in part-of-speech (POS) tags to further enhance the performance, when a POS tagger is available. Note that, AutoPhrase can support any language as long as a general knowledge base (e.g., Wikipedia) in that language is available, while benefiting from, but not requiring, a POS tagger. Compared to the state-of-the-art methods, the new method has shown significant improvements in effectiveness on five real-world datasets across different domains and languages. version:2
arxiv-1703-04019 | Negentropic Planar Symmetry Detector | http://arxiv.org/abs/1703.04019 | id:1703.04019 author:Agata Migalska, JP Lewis category:cs.CV cs.IT math.IT  published:2017-03-11 summary:In this paper we observe that information theoretical concepts are valuable tools for extracting information from images and, in particular, information on image symmetries. It is shown that the problem of detecting reflectional and rotational symmetries in a two-dimensional image can be reduced to the problem of detecting point-symmetry and periodicity in one-dimensional negentropy functions. Based on these findings a detector of reflectional and rotational global symmetries in greyscale images is constructed. We discuss the importance of high precision in symmetry detection in applications arising from quality control and illustrate how the proposed method satisfies this requirement. Finally, a superior performance of our method to other existing methods, demonstrated by the results of a rigorous experimental verification, is an indication that our approach rooted in information theory is a promising direction in a development of a robust and widely applicable symmetry detector. version:1
arxiv-1702-08435 | Statistical Anomaly Detection via Composite Hypothesis Testing for Markov Models | http://arxiv.org/abs/1702.08435 | id:1702.08435 author:Jing Zhang, Ioannis Ch. Paschalidis category:cs.SY math.OC stat.ML  published:2017-02-27 summary:Under Markovian assumptions we leverage a Central Limit Theorem (CLT) related to the test statistic in the composite hypothesis Hoeffding test so as to derive a new estimator for the threshold needed by the test. We first show the advantages of our estimator over an existing estimator by conducting extensive numerical experiments. We find that our estimator controls better for false alarms while maintaining satisfactory detection probabilities. We then apply the Hoeffding test with our threshold estimator to detecting anomalies in both communication and transportation networks. The former application seeks to enhance cyber security and the latter aims at building smarter transportation systems in cities. version:2
arxiv-1703-04009 | Automated Hate Speech Detection and the Problem of Offensive Language | http://arxiv.org/abs/1703.04009 | id:1703.04009 author:Thomas Davidson, Dana Warmsley, Michael Macy, Ingmar Weber category:cs.CL  published:2017-03-11 summary:A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify. version:1
arxiv-1703-04001 | Language Use Matters: Analysis of the Linguistic Structure of Question Texts Can Characterize Answerability in Quora | http://arxiv.org/abs/1703.04001 | id:1703.04001 author:Suman Kalyan Maity, Aman Kharb, Animesh Mukherjee category:cs.CL cs.SI  published:2017-03-11 summary:Quora is one of the most popular community Q&A sites of recent times. However, many question posts on this Q&A site often do not get answered. In this paper, we quantify various linguistic activities that discriminates an answered question from an unanswered one. Our central finding is that the way users use language while writing the question text can be a very effective means to characterize answerability. This characterization helps us to predict early if a question remaining unanswered for a specific time period t will eventually be answered or not and achieve an accuracy of 76.26% (t = 1 month) and 68.33% (t = 3 months). Notably, features representing the language use patterns of the users are most discriminative and alone account for an accuracy of 74.18%. We also compare our method with some of the similar works (Dror et al., Yang et al.) achieving a maximum improvement of ~39% in terms of accuracy. version:1
arxiv-1702-05563 | On the Equivalence of Holographic and Complex Embeddings for Link Prediction | http://arxiv.org/abs/1702.05563 | id:1702.05563 author:Katsuhiko Hayashi, Masashi Shimbo category:cs.LG  published:2017-02-18 summary:We show the equivalence of two state-of-the-art link prediction/knowledge graph completion methods: Nickel et al's holographic embedding and Trouillon et~al.'s complex embedding. We first consider a spectral version of the holographic embedding, exploiting the frequency domain in the Fourier transform for efficient computation. The analysis of the resulting method reveals that it can be viewed as an instance of the complex embedding with certain constraints cast on the initial vectors upon training. Conversely, any complex embedding can be converted to an equivalent holographic embedding. version:2
arxiv-1703-03957 | Neural method for Explicit Mapping of Quasi-curvature Locally Linear Embedding in image retrieval | http://arxiv.org/abs/1703.03957 | id:1703.03957 author:Shenglan Liu, Jun Wu, Lin Feng, Feilong Wang category:cs.CV  published:2017-03-11 summary:This paper proposed a new explicit nonlinear dimensionality reduction using neural networks for image retrieval tasks. We first proposed a Quasi-curvature Locally Linear Embedding (QLLE) for training set. QLLE guarantees the linear criterion in neighborhood of each sample. Then, a neural method (NM) is proposed for out-of-sample problem. Combining QLLE and NM, we provide a explicit nonlinear dimensionality reduction approach for efficient image retrieval. The experimental results in three benchmark datasets illustrate that our method can get better performance than other state-of-the-art out-of-sample methods. version:1
arxiv-1703-03943 | A norm knockout method on indirect reciprocity to reveal indispensable norms | http://arxiv.org/abs/1703.03943 | id:1703.03943 author:Hitoshi Yamamoto, Isamu Okada, Satoshi Uchida, Tatsuya Sasaki category:physics.soc-ph cs.MA cs.NE q-bio.PE  published:2017-03-11 summary:Although various norms for reciprocity-based cooperation have been suggested that are evolutionarily stable against invasion from free riders, the process of alternation of norms and the role of diversified norms remain unclear in the evolution of cooperation. We clarify the co-evolutionary dynamics of norms and cooperation in indirect reciprocity and also identify the indispensable norms for the evolution of cooperation. Inspired by the gene knockout method, a genetic engineering technique, we developed the norm knockout method and clarified the norms necessary for the establishment of cooperation. The results of numerical investigations revealed that the majority of norms gradually transitioned to tolerant norms after defectors are eliminated by strict norms. Furthermore, no cooperation emerges when specific norms that are intolerant to defectors are knocked out. version:1
arxiv-1703-03940 | A 3D Object Detection and Pose Estimation Pipeline Using RGB-D Images | http://arxiv.org/abs/1703.03940 | id:1703.03940 author:Ruotao He, Juan Rojas, Yisheng Guan category:cs.RO cs.CV  published:2017-03-11 summary:3D object detection and pose estimation has been studied extensively in recent decades for its potential applications in robotics. However, there still remains challenges when we aim at detecting multiple objects while retaining low false positive rate in cluttered environments. This paper proposes a robust 3D object detection and pose estimation pipeline based on RGB-D images, which can detect multiple objects simultaneously while reducing false positives. Detection begins with template matching and yields a set of template matches. A clustering algorithm then groups templates of similar spatial location and produces multiple-object hypotheses. A scoring function evaluates the hypotheses using their associated templates and non-maximum suppression is adopted to remove duplicate results based on the scores. Finally, a combination of point cloud processing algorithms are used to compute objects' 3D poses. Existing object hypotheses are verified by computing the overlap between model and scene points. Experiments demonstrate that our approach provides competitive results comparable to the state-of-the-arts and can be applied to robot random bin-picking. version:1
arxiv-1703-03939 | Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model) | http://arxiv.org/abs/1703.03939 | id:1703.03939 author:Govardana Sachithana, Ajay Sohmshetty category:cs.CL cs.LG cs.NE  published:2017-03-11 summary:We examine Memory Networks for the task of question answering (QA), under common real world scenario where training examples are scarce and under weakly supervised scenario, that is only extrinsic labels are available for training. We propose extensions for the Dynamic Memory Network (DMN), specifically within the attention mechanism, we call the resulting Neural Architecture as Dynamic Memory Tensor Network (DMTN). Ultimately, we see that our proposed extensions results in over 80% improvement in the number of task passed against the baselined standard DMN and 20% more task passed compared to state-of-the-art End-to-End Memory Network for Facebook's single task weakly trained 1K bAbi dataset. version:1
arxiv-1703-03928 | Recruiting from the network: discovering Twitter users who can help combat Zika epidemics | http://arxiv.org/abs/1703.03928 | id:1703.03928 author:Paolo Missier, Callum McClean, Jonathan Carlton, Diego Cedrim, Leonardo Silva, Alessandro Garcia, Alexandre Plastino, Alexander Romanovsky category:cs.LG cs.SI  published:2017-03-11 summary:Tropical diseases like \textit{Chikungunya} and \textit{Zika} have come to prominence in recent years as the cause of serious, long-lasting, population-wide health problems. In large countries like Brasil, traditional disease prevention programs led by health authorities have not been particularly effective. We explore the hypothesis that monitoring and analysis of social media content streams may effectively complement such efforts. Specifically, we aim to identify selected members of the public who are likely to be sensitive to virus combat initiatives that are organised in local communities. Focusing on Twitter and on the topic of Zika, our approach involves (i) training a classifier to select topic-relevant tweets from the Twitter feed, and (ii) discovering the top users who are actively posting relevant content about the topic. We may then recommend these users as the prime candidates for direct engagement within their community. In this short paper we describe our analytical approach and prototype architecture, discuss the challenges of dealing with noisy and sparse signal, and present encouraging preliminary results. version:1
arxiv-1703-03924 | Real-Time Machine Learning: The Missing Pieces | http://arxiv.org/abs/1703.03924 | id:1703.03924 author:Robert Nishihara, Philipp Moritz, Stephanie Wang, Alexey Tumanov, William Paul, Johann Schleier-Smith, Richard Liaw, Michael I. Jordan, Ion Stoica category:cs.DC cs.AI cs.LG  published:2017-03-11 summary:Machine learning applications are increasingly deployed not only to serve predictions using static models, but also as tightly-integrated components of feedback loops involving dynamic, real-time decision making. These applications pose a new set of requirements, none of which are difficult to achieve in isolation, but the combination of which creates a challenge for existing distributed execution frameworks: computation with millisecond latency at high throughput, adaptive construction of arbitrary task graphs, and execution of heterogeneous kernels over diverse sets of resources. We assert that a new distributed execution framework is needed for such ML applications and propose a candidate approach with a proof-of-concept architecture that achieves a 63x performance improvement over a state-of-the-art execution framework for a representative application. version:1
arxiv-1703-04718 | Extending Automatic Discourse Segmentation for Texts in Spanish to Catalan | http://arxiv.org/abs/1703.04718 | id:1703.04718 author:Iria da Cunha, Eric SanJuan, Juan-Manuel Torres-Moreno, Irene Castellón category:cs.CL  published:2017-03-11 summary:At present, automatic discourse analysis is a relevant research topic in the field of NLP. However, discourse is one of the phenomena most difficult to process. Although discourse parsers have been already developed for several languages, this tool does not exist for Catalan. In order to implement this kind of parser, the first step is to develop a discourse segmenter. In this article we present the first discourse segmenter for texts in Catalan. This segmenter is based on Rhetorical Structure Theory (RST) for Spanish, and uses lexical and syntactic information to translate rules valid for Spanish into rules for Catalan. We have evaluated the system by using a gold standard corpus including manually segmented texts and results are promising. version:1
arxiv-1703-03923 | A German Corpus for Text Similarity Detection Tasks | http://arxiv.org/abs/1703.03923 | id:1703.03923 author:Juan-Manuel Torres-Moreno, Gerardo Sierra, Peter Peinl category:cs.IR cs.CL  published:2017-03-11 summary:Text similarity detection aims at measuring the degree of similarity between a pair of texts. Corpora available for text similarity detection are designed to evaluate the algorithms to assess the paraphrase level among documents. In this paper we present a textual German corpus for similarity detection. The purpose of this corpus is to automatically assess the similarity between a pair of texts and to evaluate different similarity measures, both for whole documents or for individual sentences. Therefore we have calculated several simple measures on our corpus based on a library of similarity functions. version:1
arxiv-1703-03921 | Gait Pattern Recognition Using Accelerometers | http://arxiv.org/abs/1703.03921 | id:1703.03921 author:Vahid Alizadeh category:cs.CV  published:2017-03-11 summary:Motion ability is one of the most important human properties, including gait as a basis of human transitional movement. Gait, as a biometric for recognizing human identities, can be non-intrusively captured signals using wearable or portable smart devices. In this study gait patterns is collected using a wireless platform of two sensors located at chest and right ankle of the subjects. Then the raw data has undergone some preprocessing methods and segmented into 5 seconds windows. Some time and frequency domain features is extracted and the performance evaluated by 5 different classifiers. Decision Tree (with all features) and K-Nearest Neighbors (with 10 selected features) classifiers reached 99.4% and 100% respectively. version:1
arxiv-1703-03906 | Massive Exploration of Neural Machine Translation Architectures | http://arxiv.org/abs/1703.03906 | id:1703.03906 author:Denny Britz, Anna Goldie, Thang Luong, Quoc Le category:cs.CL  published:2017-03-11 summary:Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results. version:1
arxiv-1703-03888 | Segmentation of skin lesions based on fuzzy classification of pixels and histogram thresholding | http://arxiv.org/abs/1703.03888 | id:1703.03888 author:Jose Luis Garcia-Arroyo, Begonya Garcia-Zapirain category:cs.CV cs.AI stat.ML  published:2017-03-11 summary:This paper proposes an innovative method for segmentation of skin lesions in dermoscopy images developed by the authors, based on fuzzy classification of pixels and histogram thresholding. version:1
arxiv-1703-03872 | Deep Image Matting | http://arxiv.org/abs/1703.03872 | id:1703.03872 author:Ning Xu, Brian Price, Scott Cohen, Thomas Huang category:cs.CV  published:2017-03-10 summary:Image matting is a fundamental computer vision problem and has many applications. Previous algorithms have poor performance when an image has similar foreground and background colors or complicated textures. The main reasons are prior methods 1) only use low-level features and 2) lack high-level context. In this paper, we propose a novel deep learning based algorithm that can tackle both these problems. Our deep model has two parts. The first part is a deep convolutional encoder-decoder network that takes an image and the corresponding trimap as inputs and predict the alpha matte of the image. The second part is a small convolutional network that refines the alpha matte predictions of the first network to have more accurate alpha values and sharper edges. In addition, we also create a large-scale image matting dataset including 49300 training images and 1000 testing images. We evaluate our algorithm on the image matting benchmark, our testing set, and a wide variety of real images. Experimental results clearly demonstrate the superiority of our algorithm over previous methods. version:1
arxiv-1703-03869 | Deep Learning in Customer Churn Prediction: Unsupervised Feature Learning on Abstract Company Independent Feature Vectors | http://arxiv.org/abs/1703.03869 | id:1703.03869 author:Philip Spanoudes, Thomson Nguyen category:cs.LG stat.ML  published:2017-03-10 summary:As companies increase their efforts in retaining customers, being able to predict accurately ahead of time, whether a customer will churn in the foreseeable future is an extremely powerful tool for any marketing team. The paper describes in depth the application of Deep Learning in the problem of churn prediction. Using abstract feature vectors, that can generated on any subscription based company's user event logs, the paper proves that through the use of the intrinsic property of Deep Neural Networks (learning secondary features in an unsupervised manner), the complete pipeline can be applied to any subscription based company with extremely good churn predictive performance. Furthermore the research documented in the paper was performed for Framed Data (a company that sells churn prediction as a service for other companies) in conjunction with the Data Science Institute at Lancaster University, UK. This paper is the intellectual property of Framed Data. version:1
arxiv-1703-03867 | Depth from Monocular Images using a Semi-Parallel Deep Neural Network (SPDNN) Hybrid Architecture | http://arxiv.org/abs/1703.03867 | id:1703.03867 author:S. Bazrafkan, H. Javidnia, J. Lemley, P. Corcoran category:cs.CV  published:2017-03-10 summary:Computing pixel depth values provide a basis for understanding the 3D geometrical structure of an image. As it has been presented in recent research, using stereo images provides an accurate depth due to the advantage of having local correspondences; however, the processing time of these methods are still an open issue. To solve this problem, it has been suggested to use single images to compute the depth values but extracting depth from monocular images requires extracting a large number of cues from the global and local information in the image. This challenge has been studied for a decade and it is still an open problem. Recently the idea of using neural networks to solve this problem has attracted attention. In this paper, we tackle this challenge by employing a Deep Neural Network (DNN) equipped with semantic pixel-wise segmentation utilizing our recently published disparity post-processing method. Four models are trained in this study and they have been evaluated at 2 stages on KITTI dataset. The ground truth images in the first part of the experiment come from the benchmark and for the second part, the ground truth images are considered to be the disparity results from applying a state-of-art stereo matching method. The results of this evaluation demonstrate that using post-processing techniques to refine the target of the network increases the accuracy of depth estimation on individual mono images. The second evaluation shows that using segmentation data as the input can improve the depth estimation results to a point where performance is comparable with stereo depth matching. version:1
arxiv-1703-03864 | Evolution Strategies as a Scalable Alternative to Reinforcement Learning | http://arxiv.org/abs/1703.03864 | id:1703.03864 author:Tim Salimans, Jonathan Ho, Xi Chen, Ilya Sutskever category:stat.ML cs.AI cs.LG cs.NE  published:2017-03-10 summary:We explore the use of Evolution Strategies, a class of black box optimization algorithms, as an alternative to popular RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using hundreds to thousands of parallel workers, ES can solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training time. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation. version:1
arxiv-1703-03863 | Tuning Over-Relaxed ADMM | http://arxiv.org/abs/1703.03863 | id:1703.03863 author:Guilherme França, José Bento category:stat.ML math.DS math.OC  published:2017-03-10 summary:The framework of Integral Quadratic Constraints (IQC) reduces the computation of upper bounds on the convergence rate of several optimization algorithms to a semi-definite program (SDP). In the case of over-relaxed Alternating Direction Method of Multipliers (ADMM), an explicit and closed form solution to this SDP was derived in our recent work [1]. The purpose of this paper is twofold. First, we summarize these results. Second, we explore one of its consequences which allows us to obtain general and simple formulas for optimal parameter selection. These results are valid for arbitrary strongly convex objective functions. version:1
arxiv-1703-03862 | Joint Embedding of Graphs | http://arxiv.org/abs/1703.03862 | id:1703.03862 author:Shangsi Wang, Joshua T. Vogelstein, Carey E. Priebe category:stat.AP cs.LG stat.ML  published:2017-03-10 summary:Feature extraction and dimension reduction for networks is critical in a wide variety of domains. Efficiently and accurately learning features for multiple graphs has important applications in statistical inference on graphs. We propose a method to jointly embed multiple undirected graphs. Given a set of graphs, the joint embedding method identifies a linear subspace spanned by rank one symmetric matrices and projects adjacency matrices of graphs into this subspace. The projection coefficients can be treated as features of the graphs. We also propose a random graph model which generalizes classical random graph model and can be used to model multiple graphs. We show through theory and numerical experiments that under the model, the joint embedding method produces estimates of parameters with small errors. Via simulation experiments, we demonstrate that the joint embedding method produces features which lead to state of the art performance in classifying graphs. Applying the joint embedding method to human brain graphs, we find it extract interpretable features that can be used to predict individual composite creativity index. version:1
arxiv-1703-03859 | Markov Chain Lifting and Distributed ADMM | http://arxiv.org/abs/1703.03859 | id:1703.03859 author:Guilherme França, José Bento category:stat.ML cs.DS cs.IT cs.LG math.IT math.OC  published:2017-03-10 summary:The time to converge to the steady state of a finite Markov chain can be greatly reduced by a lifting operation, which creates a new Markov chain on an expanded state space. For a class of quadratic objectives, we show an analogous behavior where a distributed ADMM algorithm can be seen as a lifting of Gradient Descent algorithm. This provides a deep insight for its faster convergence rate under optimal parameter tuning. We conjecture that this gain is always present, as opposed to the lifting of a Markov chain which sometimes only provides a marginal speedup. version:1
arxiv-1703-03854 | Convolutional Spike Timing Dependent Plasticity based Feature Learning in Spiking Neural Networks | http://arxiv.org/abs/1703.03854 | id:1703.03854 author:Priyadarshini Panda, Gopalakrishnan Srinivasan, Kaushik Roy category:cs.NE cs.AI cs.CV  published:2017-03-10 summary:Brain-inspired learning models attempt to mimic the cortical architecture and computations performed in the neurons and synapses constituting the human brain to achieve its efficiency in cognitive tasks. In this work, we present convolutional spike timing dependent plasticity based feature learning with biologically plausible leaky-integrate-and-fire neurons in Spiking Neural Networks (SNNs). We use shared weight kernels that are trained to encode representative features underlying the input patterns thereby improving the sparsity as well as the robustness of the learning model. We demonstrate that the proposed unsupervised learning methodology learns several visual categories for object recognition with fewer number of examples and outperforms traditional fully-connected SNN architectures while yielding competitive accuracy. Additionally, we observe that the learning model performs out-of-set generalization further making the proposed biologically plausible framework a viable and efficient architecture for future neuromorphic applications. version:1
arxiv-1703-03848 | Development of An Android Application for Object Detection Based on Color, Shape, or Local Features | http://arxiv.org/abs/1703.03848 | id:1703.03848 author:Lamiaa A. Elrefaei, Mona Omar Al-musawa, Norah Abdullah Al-gohany category:cs.CV  published:2017-03-10 summary:Object detection and recognition is an important task in many computer vision applications. In this paper an Android application was developed using Eclipse IDE and OpenCV3 Library. This application is able to detect objects in an image that is loaded from the mobile gallery, based on its color, shape, or local features. The image is processed in the HSV color domain for better color detection. Circular shapes are detected using Circular Hough Transform and other shapes are detected using Douglas-Peucker algorithm. BRISK (binary robust invariant scalable keypoints) local features were applied in the developed Android application for matching an object image in another scene image. The steps of the proposed detection algorithms are described, and the interfaces of the application are illustrated. The application is ported and tested on Galaxy S3, S6, and Note1 Smartphones. Based on the experimental results, the application is capable of detecting eleven different colors, detecting two dimensional geometrical shapes including circles, rectangles, triangles, and squares, and correctly match local features of object and scene images for different conditions. The application could be used as a standalone application, or as a part of another application such as Robot systems, traffic systems, e-learning applications, information retrieval and many others. version:1
arxiv-1703-03842 | Effects of Limiting Memory Capacity on the Behaviour of Exemplar Dynamics | http://arxiv.org/abs/1703.03842 | id:1703.03842 author:B. Goodman, P. F. Tupper category:cs.CL 91F20  published:2017-03-10 summary:Exemplar models are a popular class of models used to describe language change. Here we study how limiting the memory capacity of an individual in these models affects the system's behaviour. In particular we demonstrate the effect this change has on the extinction of categories. Previous work in exemplar dynamics has not addressed this question. In order to investigate this, we will inspect a simplified exemplar model. We will prove for the simplified model that all the sound categories but one will always become extinct, whether memory storage is limited or not. However, computer simulations show that changing the number of stored memories alters how fast categories become extinct. version:1
arxiv-1702-06914 | Training a Subsampling Mechanism in Expectation | http://arxiv.org/abs/1702.06914 | id:1702.06914 author:Colin Raffel, Dieterich Lawson category:cs.LG  published:2017-02-22 summary:We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation. We test this approach on a simple toy problem and discuss its shortcomings. version:2
arxiv-1703-03773 | Evolutionary Image Composition Using Feature Covariance Matrices | http://arxiv.org/abs/1703.03773 | id:1703.03773 author:Aneta Neumann, Zygmunt L. Szpak, Wojciech Chojnacki, Frank Neumann category:cs.NE  published:2017-03-10 summary:Evolutionary algorithms have recently been used to create a wide range of artistic work. In this paper, we propose a new approach for the composition of new images from existing ones, that retain some salient features of the original images. We introduce evolutionary algorithms that create new images based on a fitness function that incorporates feature covariance matrices associated with different parts of the images. This approach is very flexible in that it can work with a wide range of features and enables targeting specific regions in the images. For the creation of the new images, we propose a population-based evolutionary algorithm with mutation and crossover operators based on random walks. Our experimental results reveal a spectrum of aesthetically pleasing images that can be obtained with the aid of our evolutionary process. version:1
arxiv-1703-03771 | Coping with Construals in Broad-Coverage Semantic Annotation of Adpositions | http://arxiv.org/abs/1703.03771 | id:1703.03771 author:Jena D. Hwang, Archna Bhatia, Na-Rae Han, Tim O'Gorman, Vivek Srikumar, Nathan Schneider category:cs.CL  published:2017-03-10 summary:We consider the semantics of prepositions, revisiting a broad-coverage annotation scheme used for annotating all 4,250 preposition tokens in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and case markers in other languages, as well as some problematic cases in English, have led us to reconsider the assumption that a preposition's lexical contribution is equivalent to the role/relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition's lexical function so they can be annotated at scale---supporting automatic, statistical processing of domain-general language---and sketch how this representation would inform a constructional analysis. version:1
arxiv-1703-03768 | Integer Factorization with a Neuromorphic Sieve | http://arxiv.org/abs/1703.03768 | id:1703.03768 author:John V. Monaco, Manuel M. Vindiola category:cs.NE cs.CR  published:2017-03-10 summary:The bound to factor large integers is dominated by the computational effort to discover numbers that are smooth, typically performed by sieving a polynomial sequence. On a von Neumann architecture, sieving has log-log amortized time complexity to check each value for smoothness. This work presents a neuromorphic sieve that achieves a constant time check for smoothness by exploiting two characteristic properties of neuromorphic architectures: constant time synaptic integration and massively parallel computation. The approach is validated by modifying msieve, one of the fastest publicly available integer factorization implementations, to use the IBM Neurosynaptic System (NS1e) as a coprocessor for the sieving stage. version:1
arxiv-1703-03717 | Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations | http://arxiv.org/abs/1703.03717 | id:1703.03717 author:Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez category:cs.LG stat.ML  published:2017-03-10 summary:Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test. version:1
arxiv-1703-03714 | Applying the Wizard-of-Oz Technique to Multimodal Human-Robot Dialogue | http://arxiv.org/abs/1703.03714 | id:1703.03714 author:Matthew Marge, Claire Bonial, Brendan Byrne, Taylor Cassidy, A. William Evans, Susan G. Hill, Clare Voss category:cs.CL cs.AI cs.HC cs.RO  published:2017-03-10 summary:Our overall program objective is to provide more natural ways for soldiers to interact and communicate with robots, much like how soldiers communicate with other soldiers today. We describe how the Wizard-of-Oz (WOz) method can be applied to multimodal human-robot dialogue in a collaborative exploration task. While the WOz method can help design robot behaviors, traditional approaches place the burden of decisions on a single wizard. In this work, we consider two wizards to stand in for robot navigation and dialogue management software components. The scenario used to elicit data is one in which a human-robot team is tasked with exploring an unknown environment: a human gives verbal instructions from a remote location and the robot follows them, clarifying possible misunderstandings as needed via dialogue. We found the division of labor between wizards to be workable, which holds promise for future software development. version:1
arxiv-1703-03702 | Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis | http://arxiv.org/abs/1703.03702 | id:1703.03702 author:Adrian Galdran, Aitor Alvarez-Gila, Maria Ines Meyer, Cristina L. Saratxaga, Teresa Araújo, Estibaliz Garrote, Guilherme Aresta, Pedro Costa, A. M. Mendonça, Aurélio Campilho category:cs.CV  published:2017-03-10 summary:Dermoscopic skin images are often obtained with different imaging devices, under varying acquisition conditions. In this work, instead of attempting to perform intensity and color normalization, we propose to leverage computational color constancy techniques to build an artificial data augmentation technique suitable for this kind of images. Specifically, we apply the \emph{shades of gray} color constancy technique to color-normalize the entire training set of images, while retaining the estimated illuminants. We then draw one sample from the distribution of training set illuminants and apply it on the normalized image. We employ this technique for training two deep convolutional neural networks for the tasks of skin lesion segmentation and skin lesion classification, in the context of the ISIC 2017 challenge and without using any external dermatologic image set. Our results on the validation set are promising, and will be supplemented with extended results on the hidden test set when available. version:1
arxiv-1703-01127 | On the Behavior of Convolutional Nets for Feature Extraction | http://arxiv.org/abs/1703.01127 | id:1703.01127 author:Dario Garcia-Gasulla, Ferran Parés, Armand Vilalta, Jonatan Moreno, Eduard Ayguadé, Jesús Labarta, Ulises Cortés, Toyotaro Suzumura category:cs.NE cs.AI cs.LG stat.ML  published:2017-03-03 summary:Convolutional neural networks (CNN) are representation learning techniques that achieve state-of-the-art performance on almost every image-related, machine learning task. Applying the representation languages build by these models to tasks beyond the one they were originally trained for is a field of interest known as transfer learning for feature extraction. Through this approach, one can apply the image descriptors learnt by a CNN after processing millions of images to any dataset, without an expensive training phase. Contributions to this field have so far focused on extracting CNN features from layers close to the output (e.g., fully connected layers), particularly because they work better when used out-of-the-box to feed a classifier. Nevertheless, the rest of CNN features is known to encode a wide variety of visual information, which could be potentially exploited on knowledge representation and reasoning tasks. In this paper we analyze the behavior of each feature individually, exploring their intra/inter class activations for all classes of three different datasets. From this study we learn that low and middle level features behave very differently to high level features, the former being more descriptive and the latter being more discriminant. We show how low and middle level features can be used for knowledge representation purposes both by their presence or by their absence. We also study how much noise these features may encode, and propose a thresholding approach to discard most of it. Finally, we discuss the potential implications of these results in the context of knowledge representation using features extracted from a CNN. version:2
arxiv-1703-03666 | Comparison of SMT and RBMT; The Requirement of Hybridization for Marathi-Hindi MT | http://arxiv.org/abs/1703.03666 | id:1703.03666 author:Sreelekha. S, Pushpak Bhattacharyya category:cs.CL  published:2017-03-10 summary:We present in this paper our work on comparison between Statistical Machine Translation (SMT) and Rule-based machine translation for translation from Marathi to Hindi. Rule Based systems although robust take lots of time to build. On the other hand statistical machine translation systems are easier to create, maintain and improve upon. We describe the development of a basic Marathi-Hindi SMT system and evaluate its performance. Through a detailed error analysis, we, point out the relative strengths and weaknesses of both systems. Effectively, we shall see that even with a small amount of training corpus a statistical machine translation system has many advantages for high quality domain specific machine translation over that of a rule-based counterpart. version:1
arxiv-1703-03664 | Parallel Multiscale Autoregressive Density Estimation | http://arxiv.org/abs/1703.03664 | id:1703.03664 author:Scott Reed, Aäron van den Oord, Nal Kalchbrenner, Sergio Gómez Colmenarejo, Ziyu Wang, Dan Belov, Nando de Freitas category:cs.CV cs.NE  published:2017-03-10 summary:PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels. This can be sped up by caching activations, but still involves generating each pixel sequentially. In this work, we propose a parallelized PixelCNN that allows more efficient inference by modeling certain pixel groups as conditionally independent. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup - O(log N) sampling instead of O(N) - enabling the practical generation of 512x512 images. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling. version:1
arxiv-1703-03640 | A Study of Metrics of Distance and Correlation Between Ranked Lists for Compositionality Detection | http://arxiv.org/abs/1703.03640 | id:1703.03640 author:Christina Lioma, Niels Dalum Hansen category:cs.CL  published:2017-03-10 summary:Compositionality in language refers to how much the meaning of some phrase can be decomposed into the meaning of its constituents and the way these constituents are combined. Based on the premise that substitution by synonyms is meaning-preserving, compositionality can be approximated as the semantic similarity between a phrase and a version of that phrase where words have been replaced by their synonyms. Different ways of representing such phrases exist (e.g., vectors [1] or language models [2]), and the choice of representation affects the measurement of semantic similarity. We propose a new compositionality detection method that represents phrases as ranked lists of term weights. Our method approximates the semantic similarity between two ranked list representations using a range of well-known distance and correlation metrics. In contrast to most state-of-the-art approaches in compositionality detection, our method is completely unsupervised. Experiments with a publicly available dataset of 1048 human-annotated phrases shows that, compared to strong supervised baselines, our approach provides superior measurement of compositionality using any of the distance and correlation metrics considered. version:1
arxiv-1703-03020 | Spectral Graph Convolutions for Population-based Disease Prediction | http://arxiv.org/abs/1703.03020 | id:1703.03020 author:Sarah Parisot, Sofia Ira Ktena, Enzo Ferrante, Matthew Lee, Ricardo Guerrerro Moreno, Ben Glocker, Daniel Rueckert category:stat.ML cs.LG  published:2017-03-08 summary:Exploiting the wealth of imaging and non-imaging information for disease prediction tasks requires models capable of representing, at the same time, individual features as well as data associations between subjects from potentially large populations. Graphs provide a natural framework for such tasks, yet previous graph-based approaches focus on pairwise similarities without modelling the subjects' individual characteristics and features. On the other hand, relying solely on subject-specific imaging feature vectors fails to model the interaction and similarity between subjects, which can reduce performance. In this paper, we introduce the novel concept of Graph Convolutional Networks (GCN) for brain analysis in populations, combining imaging and non-imaging data. We represent populations as a sparse graph where its vertices are associated with image-based feature vectors and the edges encode phenotypic information. This structure was used to train a GCN model on partially labelled graphs, aiming to infer the classes of unlabelled nodes from the node features and pairwise associations between subjects. We demonstrate the potential of the method on the challenging ADNI and ABIDE databases, as a proof of concept of the benefit from integrating contextual information in classification tasks. This has a clear impact on the quality of the predictions, leading to 69.5% accuracy for ABIDE (outperforming the current state of the art of 66.8%) and 77% for ADNI for prediction of MCI conversion, significantly outperforming standard linear classifiers where only individual features are considered. version:2
arxiv-1703-03624 | From Depth Data to Head Pose Estimation: a Siamese approach | http://arxiv.org/abs/1703.03624 | id:1703.03624 author:Marco Venturelli, Guido Borghi, Roberto Vezzani, Rita Cucchiara category:cs.CV  published:2017-03-10 summary:The correct estimation of the head pose is a problem of the great importance for many applications. For instance, it is an enabling technology in automotive for driver attention monitoring. In this paper, we tackle the pose estimation problem through a deep learning network working in regression manner. Traditional methods usually rely on visual facial features, such as facial landmarks or nose tip position. In contrast, we exploit a Convolutional Neural Network (CNN) to perform head pose estimation directly from depth data. We exploit a Siamese architecture and we propose a novel loss function to improve the learning of the regression network layer. The system has been tested on two public datasets, Biwi Kinect Head Pose and ICT-3DHP database. The reported results demonstrate the improvement in accuracy with respect to current state-of-the-art approaches and the real time capabilities of the overall framework. version:1
arxiv-1703-01622 | Automatic Classification of Cancerous Tissue in Laserendomicroscopy Images of the Oral Cavity using Deep Learning | http://arxiv.org/abs/1703.01622 | id:1703.01622 author:Marc Aubreville, Christian Knipfer, Nicolai Oetter, Christian Jaremenko, Erik Rodner, Joachim Denzler, Christopher Bohr, Helmut Neumann, Florian Stelzle, Andreas Maier category:cs.CV  published:2017-03-05 summary:Oral Squamous Cell Carcinoma (OSCC) is a common type of cancer of the oral epithelium. Despite their high impact on mortality, sufficient screening methods for early diagnosis of OSCC often lack accuracy and thus OSCCs are mostly diagnosed at a late stage. Early detection and accurate outline estimation of OSCCs would lead to a better curative outcome and an reduction in recurrence rates after surgical treatment. Confocal Laser Endomicroscopy (CLE) records sub-surface micro-anatomical images for in vivo cell structure analysis. Recent CLE studies showed great prospects for a reliable, real-time ultrastructural imaging of OSCC in situ. We present and evaluate a novel automatic approach for a highly accurate OSCC diagnosis using deep learning technologies on CLE images. The method is compared against textural feature-based machine learning approaches that represent the current state of the art. For this work, CLE image sequences (7894 images) from patients diagnosed with OSCC were obtained from 4 specific locations in the oral cavity, including the OSCC lesion. The present approach is found to outperform the state of the art in CLE image recognition with an area under the curve (AUC) of 0.96 and a mean accuracy of 88.3% (sensitivity 86.6%, specificity 90%). version:2
arxiv-1703-03613 | Fast LIDAR-based Road Detection Using Convolutional Neural Networks | http://arxiv.org/abs/1703.03613 | id:1703.03613 author:Luca Caltagirone, Samuel Scheidegger, Lennart Svensson, Mattias Wahde category:cs.CV  published:2017-03-10 summary:In this work, a deep learning approach has been developed to carry out road detection using only LIDAR data. Starting from an unstructured point cloud, top-view images encoding several basic statistics such as mean height and density are generated. By considering a top-view representation, road detection is reduced to a single-scale problem that can be addressed with a simple and fast convolutional neural network (CNN). The CNN is specifically designed for the task of pixel-wise semantic segmentation by combining a large receptive field with high-resolution feature maps. The proposed system achieves state-of-the-art results on the KITTI road benchmark. It is currently the top-performing algorithm among the published methods in the overall category urban road and outperforms the second best LIDAR-only approach by 7.4 percentage points. Its fast inference makes it particularly suitable for real-time applications. version:1
arxiv-1703-00676 | A Unifying View of Explicit and Implicit Feature Maps for Structured Data: Systematic Studies of Graph Kernels | http://arxiv.org/abs/1703.00676 | id:1703.00676 author:Nils M. Kriege, Marion Neumann, Christopher Morris, Kristian Kersting, Petra Mutzel category:cs.LG stat.ML  published:2017-03-02 summary:Non-linear kernel methods can be approximated by fast linear ones using suitable explicit feature maps allowing their application to large scale problems. To this end, explicit feature maps of kernels for vectorial data have been extensively studied. As many real-world data is structured, various kernels for complex data like graphs have been proposed. Indeed, many of them directly compute feature maps. However, the kernel trick is employed when the number of features is very large or the individual vertices of graphs are annotated by real-valued attributes. Can we still compute explicit feature maps efficiently under these circumstances? Triggered by this question, we investigate how general convolution kernels are composed from base kernels and construct corresponding feature maps. We apply our results to widely used graph kernels and analyze for which kernels and graph properties computation by explicit feature maps is feasible and actually more efficient. In particular, we derive feature maps for random walk and subgraph matching kernels and apply them to real-world graphs with discrete labels. Thereby, our theoretical results are confirmed experimentally by observing a phase transition when comparing running time with respect to label diversity, walk lengths and subgraph size, respectively. Moreover, we derive approximative, explicit feature maps for state-of-the-art kernels supporting real-valued attributes including the GraphHopper and Graph Invariant kernels. In extensive experiments we show that our approaches often achieve a classification accuracy close to the exact methods based on the kernel trick, but require only a fraction of their running time. version:2
arxiv-1703-03609 | NetSpam: a Network-based Spam Detection Framework for Reviews in Online Social Media | http://arxiv.org/abs/1703.03609 | id:1703.03609 author:Saeedreza Shehnepoor, Mostafa Salehi, Reza Farahbakhsh, Noel Crespi category:cs.SI cs.CL cs.IR physics.soc-ph  published:2017-03-10 summary:Nowadays, a big part of people rely on available content in social media in their decisions (e.g. reviews and feedback on a topic or product). The possibility that anybody can leave a review provide a golden opportunity for spammers to write spam reviews about products and services for different interests. Identifying these spammers and the spam content is a hot topic of research and although a considerable number of studies have been done recently toward this end, but so far the methodologies put forth still barely detect spam reviews, and none of them show the importance of each extracted feature type. In this study, we propose a novel framework, named NetSpam, which utilizes spam features for modeling review datasets as heterogeneous information networks to map spam detection procedure into a classification problem in such networks. Using the importance of spam features help us to obtain better results in terms of different metrics experimented on real-world review datasets from Yelp and Amazon websites. The results show that NetSpam outperforms the existing methods and among four categories of features; including review-behavioral, user-behavioral, reviewlinguistic, user-linguistic, the first type of features performs better than the other categories. version:1
arxiv-1703-03608 | Multi-frequency image reconstruction for radio-interferometry with self-tuned regularization parameters | http://arxiv.org/abs/1703.03608 | id:1703.03608 author:Rita Ammanouil, André Ferrari, Rémi Flamary, Chiara Ferrari, David Mary category:cs.CV  published:2017-03-10 summary:As the world's largest radio telescope, the Square Kilometer Array (SKA) will provide radio interferometric data with unprecedented detail. Image reconstruction algorithms for radio interferometry are challenged to scale well with TeraByte image sizes never seen before. In this work, we investigate one such 3D image reconstruction algorithm known as MUFFIN (MUlti-Frequency image reconstruction For radio INterferometry). In particular, we focus on the challenging task of automatically finding the optimal regularization parameter values. In practice, finding the regularization parameters using classical grid search is computationally intensive and nontrivial due to the lack of ground- truth. We adopt a greedy strategy where, at each iteration, the optimal parameters are found by minimizing the predicted Stein unbiased risk estimate (PSURE). The proposed self-tuned version of MUFFIN involves parallel and computationally efficient steps, and scales well with large- scale data. Finally, numerical results on a 3D image are presented to showcase the performance of the proposed approach. version:1
arxiv-1703-03596 | High SNR Consistent Compressive Sensing | http://arxiv.org/abs/1703.03596 | id:1703.03596 author:Sreejith Kallummil, Sheetal Kalyani category:stat.ML cs.IT math.IT  published:2017-03-10 summary:High signal to noise ratio (SNR) consistency of model selection criteria in linear regression models has attracted a lot of attention recently. However, most of the existing literature on high SNR consistency deals with model order selection. Further, the limited literature available on the high SNR consistency of subset selection procedures (SSPs) is applicable to linear regression with full rank measurement matrices only. Hence, the performance of SSPs used in underdetermined linear models (a.k.a compressive sensing (CS) algorithms) at high SNR is largely unknown. This paper fills this gap by deriving necessary and sufficient conditions for the high SNR consistency of popular CS algorithms like $l_0$-minimization, basis pursuit de-noising or LASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions analytically establish the high SNR inconsistency of CS algorithms when used with the tuning parameters discussed in literature. Novel tuning parameters with SNR adaptations are developed using the sufficient conditions and the choice of SNR adaptations are discussed analytically using convergence rate analysis. CS algorithms with the proposed tuning parameters are numerically shown to be high SNR consistent and outperform existing tuning parameters in the moderate to high SNR regime. version:1
arxiv-1703-02182 | Using Deep Learning Method for Classification: A Proposed Algorithm for the ISIC 2017 Skin Lesion Classification Challenge | http://arxiv.org/abs/1703.02182 | id:1703.02182 author:Wenhao Zhang, Liangcai Gao, Runtao Liu category:cs.CV  published:2017-03-07 summary:Skin cancer, the most common human malignancy, is primarily diagnosed visually by physicians [1]. Classification with an automated method like CNN [2, 3] shows potential for challenging tasks [1]. By now, the deep convolutional neural networks are on par with human dermatologist [1]. This abstract is dedicated on developing a Deep Learning method for ISIC [5] 2017 Skin Lesion Detection Competition hosted at [6] to classify the dermatology pictures, which is aimed at improving the diagnostic accuracy rate and general level of the human health. The challenge falls into three sub-challenges, including Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion Classification. This project only participates in the Lesion Classification part. This algorithm is comprised of three steps: (1) original images preprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4] framework, (3) predicting the test images and calculating the scores that represent the likelihood of corresponding classification. The models are built on the source images are using the Caffe [4] framework. The scores in prediction step are obtained by two different models from the source images. version:2
arxiv-1703-03200 | Turkish PoS Tagging by Reducing Sparsity with Morpheme Tags in Small Datasets | http://arxiv.org/abs/1703.03200 | id:1703.03200 author:Burcu Can, Ahmet Üstün, Murathan Kurfalı category:cs.CL  published:2017-03-09 summary:Sparsity is one of the major problems in natural language processing. The problem becomes even more severe in agglutinating languages that are highly prone to be inflected. We deal with sparsity in Turkish by adopting morphological features for part-of-speech tagging. We learn inflectional and derivational morpheme tags in Turkish by using conditional random fields (CRF) and we employ the morpheme tags in part-of-speech (PoS) tagging by using hidden Markov models (HMMs) to mitigate sparsity. Results show that using morpheme tags in PoS tagging helps alleviate the sparsity in emission probabilities. Our model outperforms other hidden Markov model based PoS tagging models for small training datasets in Turkish. We obtain an accuracy of 94.1% in morpheme tagging and 89.2% in PoS tagging on a 5K training dataset. version:2
arxiv-1703-03567 | A New Evaluation Protocol and Benchmarking Results for Extendable Cross-media Retrieval | http://arxiv.org/abs/1703.03567 | id:1703.03567 author:Ruoyu Liu, Yao Zhao, Liang Zheng, Shikui Wei, Yi Yang category:cs.CV  published:2017-03-10 summary:This paper proposes a new evaluation protocol for cross-media retrieval which better fits the real-word applications. Both image-text and text-image retrieval modes are considered. Traditionally, class labels in the training and testing sets are identical. That is, it is usually assumed that the query falls into some pre-defined classes. However, in practice, the content of a query image/text may vary extensively, and the retrieval system does not necessarily know in advance the class label of a query. Considering the inconsistency between the real-world applications and laboratory assumptions, we think that the existing protocol that works under identical train/test classes can be modified and improved. This work is dedicated to addressing this problem by considering the protocol under an extendable scenario, \ie, the training and testing classes do not overlap. We provide extensive benchmarking results obtained by the existing protocol and the proposed new protocol on several commonly used datasets. We demonstrate a noticeable performance drop when the testing classes are unseen during training. Additionally, a trivial solution, \ie, directly using the predicted class label for cross-media retrieval, is tested. We show that the trivial solution is very competitive in traditional non-extendable retrieval, but becomes less so under the new settings. The train/test split, evaluation code, and benchmarking results are publicly available on our website. version:1
arxiv-1702-06856 | Robustness to Adversarial Examples through an Ensemble of Specialists | http://arxiv.org/abs/1702.06856 | id:1702.06856 author:Mahdieh Abbasi, Christian Gagné category:cs.NE cs.LG  published:2017-02-22 summary:We are proposing to use an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. Indeed, we observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm that interpretation, opening a way to make the system more robust to adversarial examples through a rejection mechanism, rather than trying to classify them properly at any cost. version:3
arxiv-1703-03507 | Decorrelated Jet Substructure Tagging using Adversarial Neural Networks | http://arxiv.org/abs/1703.03507 | id:1703.03507 author:Chase Shimmin, Peter Sadowski, Pierre Baldi, Edison Weik, Daniel Whiteson, Edward Goul, Andreas Søgaard category:hep-ex physics.data-an stat.ML  published:2017-03-10 summary:We describe a strategy for constructing a neural network jet substructure tagger which powerfully discriminates boosted decay signals while remaining largely uncorrelated with the jet mass. This reduces the impact of systematic uncertainties in background modeling while enhancing signal purity, resulting in improved discovery significance relative to existing taggers. The network is trained using an adversarial strategy, resulting in a tagger that learns to balance classification accuracy with decorrelation. As a benchmark scenario, we consider the case where large-radius jets originating from a boosted resonance decay are discriminated from a background of nonresonant quark and gluon jets. We show that in the presence of systematic uncertainties on the background rate, our adversarially-trained, decorrelated tagger considerably outperforms a conventionally trained neural network, despite having a slightly worse signal-background separation power. We generalize the adversarial training technique to include a parametric dependence on the signal hypothesis, training a single network that provides optimized, interpolatable decorrelated jet tagging across a continuous range of hypothetical resonance masses, after training on discrete choices of the signal mass. version:1
arxiv-1703-03503 | Density Level Set Estimation on Manifolds with DBSCAN | http://arxiv.org/abs/1703.03503 | id:1703.03503 author:Heinrich Jiang category:stat.ML  published:2017-03-10 summary:DBSCAN is one of the most popular clustering algorithms amongst practitioners, but it has received comparatively less theoretical treatment. We show that given $\lambda > 0$ and its parameters set under appropriate ranges, DBSCAN estimates the connected components of the $\lambda$-density level set (i.e. $\{ x : f(x) \ge \lambda \}$ where $f$ is the density). We characterize the regularity of the level set boundaries using parameter $\beta > 0$ and analyze the estimation error under the Hausdorff metric. When the data lies in $\mathbb{R}^D$ we obtain an estimation rate of $O(n^{-1/(2\beta + D)})$, which matches known lower bounds up to logarithmic factors. When the data lies on an embedded unknown $d$-dimensional manifold in $\mathbb{R}^D$, then we obtain an estimation rate of $O(n^{-1/(2\beta + d\cdot \max\{1, \beta \})})$. Finally, we provide adaptive parameter tuning in order to attain these rates with no a priori knowledge of the intrinsic dimension, density, or $\beta$. version:1
arxiv-1702-06763 | DeepCloak: Masking DNN Models for robustness against adversarial samples | http://arxiv.org/abs/1702.06763 | id:1702.06763 author:Ji Gao, Beilun Wang, Zeming Lin, Weilin Xu, Yanjun Qi category:cs.LG  published:2017-02-22 summary:Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task. To overcome this problem, we introduce a defensive mechanism called DeepCloak. By identifying and removing unnecessary features in a DNN model, DeepCloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepCloak is easy to implement and computationally efficient. Experimental results show that DeepCloak can increase the performance of state-of-the-art DNN models against adversarial samples. version:5
arxiv-1703-03492 | A New Representation of Skeleton Sequences for 3D Action Recognition | http://arxiv.org/abs/1703.03492 | id:1703.03492 author:Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, Farid Boussaid category:cs.CV  published:2017-03-09 summary:Skeleton sequences provide 3D trajectories of human skeleton joints. The spatial temporal information is very important for action recognition. Considering that deep convolutional neural network (CNN) is very powerful for feature learning in images, in this paper, we propose to transform a skeleton sequence into an image-based representation for spatial temporal information learning with CNN. Specifically, for each channel of the 3D coordinates, we represent the sequence into a clip with several gray images, which represent multiple spatial structural information of the joints. Those images are fed to a deep CNN to learn high-level features. The CNN features of all the three clips at the same time-step are concatenated in a feature vector. Each feature vector represents the temporal information of the entire skeleton sequence and one particular spatial relationship of the joints. We then propose a Multi-Task Learning Network (MTLN) to jointly process the feature vectors of all time-steps in parallel for action recognition. Experimental results clearly show the effectiveness of the proposed new representation and feature learning method for 3D action recognition. version:1
arxiv-1703-03478 | On-line Learning with Abstention | http://arxiv.org/abs/1703.03478 | id:1703.03478 author:Corinna Cortes, Giulia DeSalvo, Mehryar Mohri, Scott Yang category:cs.LG  published:2017-03-09 summary:We introduce and analyze an on-line learning setting where the learner has the added option of abstaining from making a prediction at the price of a fixed cost. When the learner abstains, no feedback is provided, and she does not receive the label associated with the example. We design several algorithms and derive regret guarantees in both the adversarial and stochastic loss setting. In the process, we derive a new bound for on-line learning with feedback graphs that generalizes and extends existing work. We also design a new algorithm for on-line learning with sleeping experts that takes advantage of time-varying feedback graphs. We present natural extensions of existing algorithms as a baseline, and we then design more sophisticated algorithms that explicitly exploit the structure of our problem. We empirically validate the improvement of these more sophisticated algorithms on several datasets. version:1
arxiv-1703-03470 | Deep Radial Kernel Networks: Approximating Radially Symmetric Functions with Deep Networks | http://arxiv.org/abs/1703.03470 | id:1703.03470 author:Brendan McCane, Lech Szymanski category:cs.LG 68T05  68Q32 I.2.6; I.5.1; I.5.2  published:2017-03-09 summary:We prove that a particular deep network architecture is more efficient at approximating radially symmetric functions than the best known 2 or 3 layer networks. We use this architecture to approximate Gaussian kernel SVMs, and subsequently improve upon them with further training. The architecture and initial weights of the Deep Radial Kernel Network are completely specified by the SVM and therefore sidesteps the problem of empirically choosing an appropriate deep network architecture. version:1
arxiv-1703-03468 | Position Tracking for Virtual Reality Using Commodity WiFi | http://arxiv.org/abs/1703.03468 | id:1703.03468 author:Manikanta Kotaru, Sachin Katti category:cs.CV cs.NI  published:2017-03-09 summary:Today, experiencing virtual reality (VR) is a cumbersome experience which either requires dedicated infrastructure like infrared cameras to track the headset and hand-motion controllers (e.g., Oculus Rift, HTC Vive), or provides only 3-DoF (Degrees of Freedom) tracking which severely limits the user experience (e.g., Samsung Gear). To truly enable VR everywhere, we need position tracking to be available as a ubiquitous service. This paper presents WiCapture, a novel approach which leverages commodity WiFi infrastructure, which is ubiquitous today, for tracking purposes. We prototype WiCapture using off-the-shelf WiFi radios and show that it achieves an accuracy of 0.88 cm compared to sophisticated infrared based tracking systems like the Oculus, while providing much higher range, resistance to occlusion, ubiquity and ease of deployment. version:1
arxiv-1703-03457 | Parallel Markov Chain Monte Carlo for the Indian Buffet Process | http://arxiv.org/abs/1703.03457 | id:1703.03457 author:Michael M. Zhang, Avinava Dubey, Sinead A. Williamson category:stat.ML  published:2017-03-09 summary:Indian Buffet Process based models are an elegant way for discovering underlying features within a data set, but inference in such models can be slow. Inferring underlying features using Markov chain Monte Carlo either relies on an uncollapsed representation, which leads to poor mixing, or on a collapsed representation, which leads to a quadratic increase in computational complexity. Existing attempts at distributing inference have introduced additional approximation within the inference procedure. In this paper we present a novel algorithm to perform asymptotically exact parallel Markov chain Monte Carlo inference for Indian Buffet Process models. We take advantage of the fact that the features are conditionally independent under the beta-Bernoulli process. Because of this conditional independence, we can partition the features into two parts: one part containing only the finitely many instantiated features and the other part containing the infinite tail of uninstantiated features. For the finite partition, parallel inference is simple given the instantiation of features. But for the infinite tail, performing uncollapsed MCMC leads to poor mixing and hence we collapse out the features. The resulting hybrid sampler, while being parallel, produces samples asymptotically from the true posterior. version:1
arxiv-1703-03454 | Sample Efficient Feature Selection for Factored MDPs | http://arxiv.org/abs/1703.03454 | id:1703.03454 author:Zhaohan Daniel Guo, Emma Brunskill category:cs.LG stat.ML  published:2017-03-09 summary:In reinforcement learning, the state of the real world is often represented by feature vectors. However, not all of the features may be pertinent for solving the current task. We propose Feature Selection Explore and Exploit (FS-EE), an algorithm that automatically selects the necessary features while learning a Factored Markov Decision Process, and prove that under mild assumptions, its sample complexity scales with the in-degree of the dynamics of just the necessary features, rather than the in-degree of all features. This can result in a much better sample complexity when the in-degree of the necessary features is smaller than the in-degree of all features. version:1
arxiv-1703-03442 | The cognitive roots of regularization in language | http://arxiv.org/abs/1703.03442 | id:1703.03442 author:Vanessa Ferdinand, Simon Kirby, Kenny Smith category:cs.CL q-bio.NC  published:2017-03-09 summary:Regularization occurs when the output a learner produces is less variable than the linguistic data they observed. In an artificial language learning experiment, we show that there exist at least two independent sources of regularization bias in cognition: a domain-general source based on cognitive load and a domain-specific source triggered by linguistic stimuli. Both of these factors modulate how frequency information is encoded and produced, but only the production-side modulations result in regularization (i.e. cause learners to eliminate variation from the observed input). We formalize the definition of regularization as the reduction of entropy and find that entropy measures are better at identifying regularization behavior than frequency-based analyses. We also use a model of cultural transmission to extrapolate from our experimental data in order to predict the amount of regularization which would develop in each experimental condition if the artificial language was transmitted over several generations of learners. Here we find an interaction between cognitive load and linguistic domain, suggesting that the effect of cognitive constraints can become more complex when put into the context of cultural evolution: although learning biases certainly carry information about the course of language evolution, we should not expect a one-to-one correspondence between the micro-level processes that regularize linguistic datasets and the macro-level evolution of linguistic regularity. version:1
arxiv-1703-01680 | Multi-Objective Non-parametric Sequential Prediction | http://arxiv.org/abs/1703.01680 | id:1703.01680 author:Guy Uziel, Ran El-Yaniv category:cs.LG  published:2017-03-05 summary:Online-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented. In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while fulfilling any continuous and convex constraining criterion. version:2
arxiv-1703-03429 | What can you do with a rock? Affordance extraction via word embeddings | http://arxiv.org/abs/1703.03429 | id:1703.03429 author:Nancy Fulda, Daniel Ricks, Ben Murdoch, David Wingate category:cs.AI cs.CL  published:2017-03-09 summary:Autonomous agents must often detect affordances: the set of behaviors enabled by a situation. Affordance detection is particularly helpful in domains with large action spaces, allowing the agent to prune its search space by avoiding futile behaviors. This paper presents a method for affordance extraction via word embeddings trained on a Wikipedia corpus. The resulting word vectors are treated as a common knowledge database which can be queried using linear algebra. We apply this method to a reinforcement learning agent in a text-only environment and show that affordance-based action selection improves performance most of the time. Our method increases the computational complexity of each learning step but significantly reduces the total number of steps needed. In addition, the agent's action selections begin to resemble those a human would choose. version:1
arxiv-1703-03400 | Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks | http://arxiv.org/abs/1703.03400 | id:1703.03400 author:Chelsea Finn, Pieter Abbeel, Sergey Levine category:cs.LG cs.AI cs.CV cs.NE  published:2017-03-09 summary:We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on a few-shot image classification benchmark, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies. version:1
arxiv-1703-03389 | Faster Greedy MAP Inference for Determinantal Point Processes | http://arxiv.org/abs/1703.03389 | id:1703.03389 author:Insu Han, Prabhanjan Kambadur, Kyoungsoo Park, Jinwoo Shin category:cs.DM cs.LG  published:2017-03-09 summary:Determinantal point processes (DPPs) are popular probabilistic models that arise in many machine learning tasks, where distributions of diverse sets are characterized by matrix determinants. In this paper, we develop fast algorithms to find the most likely configuration (MAP) of large-scale DPPs, which is NP-hard in general. Due to the submodular nature of the MAP objective, greedy algorithms have been used with empirical success. Greedy implementations require computation of log-determinants, matrix inverses or solving linear systems at each iteration. We present faster implementations of the greedy algorithms by utilizing the complementary benefits of two log-determinant approximation schemes: (a) first-order expansions to the matrix log-determinant function and (b) high-order expansions to the scalar log function with stochastic trace estimators. In our experiments, our algorithms are orders of magnitude faster than their competitors, while sacrificing marginal accuracy. version:1
arxiv-1703-03386 | Loyalty in Online Communities | http://arxiv.org/abs/1703.03386 | id:1703.03386 author:William L. Hamilton, Justine Zhang, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky, Jure Leskovec category:cs.SI cs.CL  published:2017-03-09 summary:Loyalty is an essential component of multi-community engagement. When users have the choice to engage with a variety of different communities, they often become loyal to just one, focusing on that community at the expense of others. However, it is unclear how loyalty is manifested in user behavior, or whether loyalty is encouraged by certain community characteristics. In this paper we operationalize loyalty as a user-community relation: users loyal to a community consistently prefer it over all others; loyal communities retain their loyal users over time. By exploring this relation using a large dataset of discussion communities from Reddit, we reveal that loyalty is manifested in remarkably consistent behaviors across a wide spectrum of communities. Loyal users employ language that signals collective identity and engage with more esoteric, less popular content, indicating they may play a curational role in surfacing new material. Loyal communities have denser user-user interaction networks and lower rates of triadic closure, suggesting that community-level loyalty is associated with more cohesive interactions and less fragmentation into subgroups. We exploit these general patterns to predict future rates of loyalty. Our results show that a user's propensity to become loyal is apparent from their first interactions with a community, suggesting that some users are intrinsically loyal from the very beginning. version:1
arxiv-1703-03385 | Visual-Interactive Similarity Search for Complex Objects by Example of Soccer Player Analysis | http://arxiv.org/abs/1703.03385 | id:1703.03385 author:Jürgen Bernard, Christian Ritter, David Sessler, Matthias Zeppelzauer, Jörn Kohlhammer, Dieter Fellner category:cs.LG cs.IR  published:2017-03-09 summary:The definition of similarity is a key prerequisite when analyzing complex data types in data mining, information retrieval, or machine learning. However, the meaningful definition is often hampered by the complexity of data objects and particularly by different notions of subjective similarity latent in targeted user groups. Taking the example of soccer players, we present a visual-interactive system that learns users' mental models of similarity. In a visual-interactive interface, users are able to label pairs of soccer players with respect to their subjective notion of similarity. Our proposed similarity model automatically learns the respective concept of similarity using an active learning strategy. A visual-interactive retrieval technique is provided to validate the model and to execute downstream retrieval tasks for soccer player analysis. The applicability of the approach is demonstrated in different evaluation strategies, including usage scenarions and cross-validation tests. version:1
arxiv-1703-03365 | Learning Active Learning from Real and Synthetic Data | http://arxiv.org/abs/1703.03365 | id:1703.03365 author:Ksenia Konyushkova, Raphael Sznitman, Pascal Fua category:cs.LG  published:2017-03-09 summary:In this paper, we suggest a novel data-driven approach to active learning: Learning Active Learning (LAL). The key idea behind LAL is to train a regressor that predicts the expected error reduction for a potential sample in a particular learning state. By treating the query selection procedure as a regression problem we are not restricted to dealing with existing AL heuristics; instead, we learn strategies based on experience from previous active learning experiments. We show that LAL can be learnt from a simple artificial 2D dataset and yields strategies that work well on real data from a wide range of domains. Moreover, if some domain-specific samples are available to bootstrap active learning, the LAL strategy can be tailored for a particular problem. version:1
arxiv-1703-03352 | A log-linear time algorithm for constrained changepoint detection | http://arxiv.org/abs/1703.03352 | id:1703.03352 author:Toby Dylan Hocking, Guillem Rigaill, Paul Fearnhead, Guillaume Bourque category:stat.CO q-bio.GN stat.ML  published:2017-03-09 summary:Changepoint detection is a central problem in time series and genomic data. For some applications, it is natural to impose constraints on the directions of changes. One example is ChIP-seq data, for which adding an up-down constraint improves peak detection accuracy, but makes the optimization problem more complicated. We show how a recently proposed functional pruning technique can be adapted to solve such constrained changepoint detection problems. This leads to a new algorithm which can solve problems with arbitrary affine constraints on adjacent segment means, and which has empirical time complexity that is log-linear in the amount of data. This algorithm achieves state-of-the-art accuracy in a benchmark of several genomic data sets, and is orders of magnitude faster than existing algorithms that have similar accuracy. Our implementation is available as the PeakSegPDPA function in the coseg R package, https://github.com/tdhock/coseg version:1
arxiv-1703-03349 | Fast and Robust Detection of Fallen People from a Mobile Robot | http://arxiv.org/abs/1703.03349 | id:1703.03349 author:Morris Antonello, Marco Carraro, Marco Pierobon, Emanuele Menegatti category:cs.RO cs.CV  published:2017-03-09 summary:This paper deals with the problem of detecting fallen people lying on the floor by means of a mobile robot equipped with a 3D depth sensor. In the proposed algorithm, inspired by semantic segmentation techniques, the 3D scene is over-segmented into small patches. Fallen people are then detected by means of two SVM classifiers: the first one labels each patch, while the second one captures the spatial relations between them. This novel approach showed to be robust and fast. Indeed, thanks to the use of small patches, fallen people in real cluttered scenes with objects side by side are correctly detected. Moreover, the algorithm can be executed on a mobile robot fitted with a standard laptop making it possible to exploit the 2D environmental map built by the robot and the multiple points of view obtained during the robot navigation. Additionally, this algorithm is robust to illumination changes since it does not rely on RGB data but on depth data. All the methods have been thoroughly validated on the IASLAB-RGBD Fallen Person Dataset, which is published online as a further contribution. It consists of several static and dynamic sequences with 15 different people and 2 different environments. version:1
arxiv-1703-03347 | A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation | http://arxiv.org/abs/1703.03347 | id:1703.03347 author:Chaitanya Mitash, Kostas E. Bekris, Abdeslam Boularias category:cs.RO cs.CV  published:2017-03-09 summary:Impressive progress has been achieved in object detection with the use of deep learning. Nevertheless, such tools typically require a large amount of training data and significant manual effort for labeling objects. This limits their applicability in robotics, where it is necessary to scale solutions to a large number of objects and a variety of conditions. This work proposes a fully autonomous process to train a Convolutional Neural Network (CNN) for object detection and pose estimation in robotic setups. The application involves detection of objects placed in a clutter and in tight environments, such as a shelf. In particular, given access to 3D object models, several aspects of the environment are simulated and the models are placed in physically realistic poses with respect to their environment to generate a labeled synthetic dataset. To further improve object detection, the network self-trains over real images that are labeled using a robust multi-view pose estimation process. The proposed training process is evaluated on several existing datasets and on a dataset that we collected with a Motoman robotic manipulator. Results show that the proposed process outperforms popular training processes relying on synthetic data generation and manual annotation. version:1
arxiv-1703-03722 | Recovery of Sparse and Low Rank Components of Matrices Using Iterative Method with Adaptive Thresholding | http://arxiv.org/abs/1703.03722 | id:1703.03722 author:Nematollah Zarmehi, Farokh Marvasti category:cs.NA stat.ML  published:2017-03-09 summary:In this letter, we propose an algorithm for recovery of sparse and low rank components of matrices using an iterative method with adaptive thresholding. In each iteration, the low rank and sparse components are obtained using a thresholding operator. This algorithm is fast and can be implemented easily. We compare it with one of the most common fast methods in which the rank and sparsity are approximated by $\ell_1$ norm. We also apply it to some real applications where the noise is not so sparse. The simulation results show that it has a suitable performance with low run-time. version:1
arxiv-1703-03329 | UntrimmedNets for Weakly Supervised Action Recognition and Detection | http://arxiv.org/abs/1703.03329 | id:1703.03329 author:Limin Wang, Yuanjun Xiong, Dahua Lin, Luc Van Gool category:cs.CV  published:2017-03-09 summary:Current action recognition methods heavily rely on trimmed videos for model training. However, it is very expensive and time-consuming to acquire a large-scale trimmed video dataset. This paper presents a new weakly supervised architecture, called UntrimmedNet, which is able to directly learn from untrimmed videos without the need of temporal annotations of action instances. Our UntrimmedNet couples two important components, the classification module and the selection module, to learn the action models and reason about the temporal duration of action instances, respectively. These two modules are implemented with feed-forward networks. UntrimmedNet is essentially an end-to-end trainable architecture, which allows for the joint optimization of model parameters of both components. We exploit the learned models for the problems of action recognition (WSR) and detection (WSD) on the untrimmed video datasets of THUMOS14 and ActivityNet. Although our UntrimmedNet only employs weak supervision, our method achieves performance superior or comparable to that of strongly supervised approaches on these two datasets. version:1
arxiv-1703-03305 | End-to-end semantic face segmentation with conditional random fields as convolutional, recurrent and adversarial networks | http://arxiv.org/abs/1703.03305 | id:1703.03305 author:Umut Güçlü, Yağmur Güçlütürk, Meysam Madadi, Sergio Escalera, Xavier Baró, Jordi González, Rob van Lier, Marcel A. J. van Gerven category:cs.CV  published:2017-03-09 summary:Recent years have seen a sharp increase in the number of related yet distinct advances in semantic segmentation. Here, we tackle this problem by leveraging the respective strengths of these advances. That is, we formulate a conditional random field over a four-connected graph as end-to-end trainable convolutional and recurrent networks, and estimate them via an adversarial process. Importantly, our model learns not only unary potentials but also pairwise potentials, while aggregating multi-scale contexts and controlling higher-order inconsistencies. We evaluate our model on two standard benchmark datasets for semantic face segmentation, achieving state-of-the-art results on both of them. version:1
arxiv-1703-02628 | Global optimization of Lipschitz functions | http://arxiv.org/abs/1703.02628 | id:1703.02628 author:Cédric Malherbe, Nicolas Vayatis category:stat.ML  published:2017-03-07 summary:The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional H\"older like condition. An adaptive version of LIPO is also introduced for the more realistic setup where the Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive LIPO algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization. version:2
arxiv-1703-03216 | Robust Density Ratio Estimation: Trimming the Likelihood Ratio | http://arxiv.org/abs/1703.03216 | id:1703.03216 author:Song Liu, Akiko Takeda, Taiji Suzuki, Kenji Fukumizu category:stat.ML  published:2017-03-09 summary:Density ratio estimation has become a versatile tool in machine learning community recently. However, due to its unbounded nature, density ratio estimation is vulnerable to corrupted data points, which misleads the estimated ratio toward infinity. In this paper, we present a robust estimator which automatically identifies and trims outliers according to the log likelihood ratio values. Such an estimator has a convex formulation and can be efficiently solved. We analyze the \ell_{2} parameter estimation error of such an estimator under two scenarios motivated by real-world problems. Numerical analysis was conducted to verify the effectiveness of such an estimator. version:1
arxiv-1703-03208 | Compressed Sensing using Generative Models | http://arxiv.org/abs/1703.03208 | id:1703.03208 author:Ashish Bora, Ajil Jalal, Eric Price, Alexandros G. Dimakis category:stat.ML cs.IT cs.LG math.IT  published:2017-03-09 summary:The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range of a generative model $G: \mathbb{R}^k \to \mathbb{R}^n$. Our main theorem is that, if $G$ is $L$-Lipschitz, then roughly $O(k \log L)$ random Gaussian measurements suffice for an $\ell_2/\ell_2$ recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use $5$-$10$x fewer measurements than Lasso for the same accuracy. version:1
arxiv-1703-00087 | Segmentation of Lesions in Dermoscopy Images Using Saliency Map And Contour Propagation | http://arxiv.org/abs/1703.00087 | id:1703.00087 author:Mostafa Jahanifar, Neda Zamani Tajeddin, Ali Gooya, Babak Mohammadzadeh Asl category:cs.CV  published:2017-02-28 summary:Melanoma is one of the most dangerous types of skin cancer and causes thousands of deaths worldwide each year. Recently dermoscopic imaging systems have been widely used as a diagnostic tool for melanoma detection. The first step in the automatic analysis of dermoscopy images is the lesion segmentation. In this article, a novel method for skin lesion segmentation that could be applied to a variety of images with different properties and deficiencies is proposed. After a multi-step preprocessing phase (hair removal and illumination correction), a supervised saliency map construction method is used to obtain an initial guess of lesion location. The construction of the saliency map is based on a random forest regressor that takes a vector of regional image features and return a saliency score based on them. This regressor is trained in a multi-level manner based on 2000 training data provided in ISIC2017 melanoma recognition challenge. In addition to obtaining an initial contour of lesion, the output saliency map can be used as a speed function alongside with image gradient to derive the initial contour toward the lesion boundary using a propagation model. The proposed algorithm has been tested on the ISIC2017 training, validation and test datasets, and gained high values for evaluation metrics. version:2
arxiv-1703-00810 | Opening the Black Box of Deep Neural Networks via Information | http://arxiv.org/abs/1703.00810 | id:1703.00810 author:Ravid Shwartz-Ziv, Naftali Tishby category:cs.LG  published:2017-03-02 summary:Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work [Tishby & Zaslavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. We first show that the stochastic gradient descent (SGD) epochs have two distinct phases: fast empirical error minimization followed by slow representation compression, for each layer. We then argue that the DNN layers end up very close to the IB theoretical bound, and present a new theoretical argument for the computational benefit of the hidden layers. version:2
arxiv-1703-03196 | Prior-based Hierarchical Segmentation Highlighting Structures of Interest | http://arxiv.org/abs/1703.03196 | id:1703.03196 author:Amin Fehri, Santiago Velasco-Forero, Fernand Meyer category:cs.CV  published:2017-03-09 summary:Image segmentation is the process of partitioning an image into a set of meaningful regions according to some criteria. Hierarchical segmentation has emerged as a major trend in this regard as it favors the emergence of important regions at different scales. On the other hand, many methods allow us to have prior information on the position of structures of interest in the images. In this paper, we present a versatile hierarchical segmentation method that takes into account any prior spatial information and outputs a hierarchical segmentation that emphasizes the contours or regions of interest while preserving the important structures in the image. Several applications are presented that illustrate the method versatility and efficiency. version:1
arxiv-1703-03186 | Segmenting Dermoscopic Images | http://arxiv.org/abs/1703.03186 | id:1703.03186 author:Mario Rosario Guarracino, Lucia Maddalena category:cs.CV  published:2017-03-09 summary:We propose an automatic algorithm, named SDI, for the segmentation of skin lesions in dermoscopic images, articulated into three main steps: selection of the image ROI, selection of the segmentation band, and segmentation. We present extensive experimental results achieved by the SDI algorithm on the lesion segmentation dataset made available for the ISIC 2017 challenge on Skin Lesion Analysis Towards Melanoma Detection, highlighting its advantages and disadvantages. version:1
arxiv-1703-03167 | Cross-validation | http://arxiv.org/abs/1703.03167 | id:1703.03167 author:Sylvain Arlot category:math.ST stat.ML stat.TH  published:2017-03-09 summary:This text is a survey on cross-validation. We define all classical cross-validation procedures, and we study their properties for two different goals: estimating the risk of a given estimator, and selecting the best estimator among a given family. For the risk estimation problem, we compute the bias (which can also be corrected) and the variance of cross-validation methods. For estimator selection, we first provide a first-order analysis (based on expectations). Then, we explain how to take into account second-order terms (from variance computations, and by taking into account the usefulness of overpenalization). This allows, in the end, to provide some guidelines for choosing the best cross-validation method for a given learning problem. version:1
arxiv-1703-03156 | Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social Media | http://arxiv.org/abs/1703.03156 | id:1703.03156 author:Enes Kocabey, Mustafa Camurcu, Ferda Ofli, Yusuf Aytar, Javier Marin, Antonio Torralba, Ingmar Weber category:cs.HC cs.CV cs.CY  published:2017-03-09 summary:A person's weight status can have profound implications on their life, ranging from mental health, to longevity, to financial income. At the societal level, "fat shaming" and other forms of "sizeism" are a growing concern, while increasing obesity rates are linked to ever raising healthcare costs. For these reasons, researchers from a variety of backgrounds are interested in studying obesity from all angles. To obtain data, traditionally, a person would have to accurately self-report their body-mass index (BMI) or would have to see a doctor to have it measured. In this paper, we show how computer vision can be used to infer a person's BMI from social media images. We hope that our tool, which we release, helps to advance the study of social aspects related to body weight. version:1
arxiv-1702-06676 | Counterfactual Control for Free from Generative Models | http://arxiv.org/abs/1702.06676 | id:1702.06676 author:Nicholas Guttenberg, Yen Yu, Ryota Kanai category:cs.LG stat.ML 68T05  published:2017-02-22 summary:We introduce a method by which a generative model learning the joint distribution between actions and future states can be used to automatically infer a control scheme for any desired reward function, which may be altered on the fly without retraining the model. In this method, the problem of action selection is reduced to one of gradient descent on the latent space of the generative model, with the model itself providing the means of evaluating outcomes and finding the gradient, much like how the reward network in Deep Q-Networks (DQN) provides gradient information for the action generator. Unlike DQN or Actor-Critic, which are conditional models for a specific reward, using a generative model of the full joint distribution permits the reward to be changed on the fly. In addition, the generated futures can be inspected to gain insight in to what the network 'thinks' will happen, and to what went wrong when the outcomes deviate from prediction. version:2
arxiv-1703-03149 | Detecting Sockpuppets in Deceptive Opinion Spam | http://arxiv.org/abs/1703.03149 | id:1703.03149 author:Marjan Hosseinia, Arjun Mukherjee category:cs.CL  published:2017-03-09 summary:This paper explores the problem of sockpuppet detection in deceptive opinion spam using authorship attribution and verification approaches. Two methods are explored. The first is a feature subsampling scheme that uses the KL-Divergence on stylistic language models of an author to find discriminative features. The second is a transduction scheme, spy induction that leverages the diversity of authors in the unlabeled test set by sending a set of spies (positive samples) from the training set to retrieve hidden samples in the unlabeled test set using nearest and farthest neighbors. Experiments using ground truth sockpuppet data show the effectiveness of the proposed schemes. version:1
arxiv-1703-01008 | End-to-End Task-Completion Neural Dialogue Systems | http://arxiv.org/abs/1703.01008 | id:1703.01008 author:Xiujun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao category:cs.CL cs.AI  published:2017-03-03 summary:This paper presents an end-to-end learning framework for task-completion neural dialogue systems, which leverages supervised and reinforcement learning with various deep-learning models. The system is able to interface with a structured database, and interact with users for assisting them to access information and complete tasks such as booking movie tickets. Our experiments in a movie-ticket booking domain show the proposed system outperforms a modular-based dialogue system and is more robust to noise produced by other components in the system. version:2
arxiv-1703-03130 | A Structured Self-attentive Sentence Embedding | http://arxiv.org/abs/1703.03130 | id:1703.03130 author:Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, Yoshua Bengio category:cs.CL cs.AI cs.LG cs.NE  published:2017-03-09 summary:This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks. version:1
arxiv-1703-03129 | Learning to Remember Rare Events | http://arxiv.org/abs/1703.03129 | id:1703.03129 author:Łukasz Kaiser, Ofir Nachum, Aurko Roy, Samy Bengio category:cs.LG  published:2017-03-09 summary:Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training. Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task. version:1
arxiv-1703-03126 | DeepSD: Generating High Resolution Climate Change Projections through Single Image Super-Resolution | http://arxiv.org/abs/1703.03126 | id:1703.03126 author:Thomas Vandal, Evan Kodra, Sangram Ganguly, Andrew Michaelis, Ramakrishna Nemani, Auroop R Ganguly category:cs.CV  published:2017-03-09 summary:The impacts of climate change are felt by most critical systems, such as infrastructure, ecological systems, and power-plants. However, contemporary Earth System Models (ESM) are run at spatial resolutions too coarse for assessing effects this localized. Local scale projections can be obtained using statistical downscaling, a technique which uses historical climate observations to learn a low-resolution to high-resolution mapping. Depending on statistical modeling choices, downscaled projections have been shown to vary significantly terms of accuracy and reliability. The spatio-temporal nature of the climate system motivates the adaptation of super-resolution image processing techniques to statistical downscaling. In our work, we present DeepSD, a generalized stacked super resolution convolutional neural network (SRCNN) framework for statistical downscaling of climate variables. DeepSD augments SRCNN with multi-scale input channels to maximize predictability in statistical downscaling. We provide a comparison with Bias Correction Spatial Disaggregation as well as three Automated-Statistical Downscaling approaches in downscaling daily precipitation from 1 degree (~100km) to 1/8 degrees (~12.5km) over the Continental United States. Furthermore, a framework using the NASA Earth Exchange (NEX) platform is discussed for downscaling more than 20 ESM models with multiple emission scenarios. version:1
arxiv-1703-03121 | Coordinated Multi-Agent Imitation Learning | http://arxiv.org/abs/1703.03121 | id:1703.03121 author:Hoang M. Le, Yisong Yue, Peter Carr category:cs.LG  published:2017-03-09 summary:We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often implicit in the demonstrations and must be inferred as a latent variable. We propose a joint approach that simultaneously learns a latent coordination model along with the individual policies. In particular, our method integrates unsupervised structure learning with conventional imitation learning. We illustrate the power of our approach on a difficult problem of learning multiple policies for fine-grained behavior modeling in team sports, where different players occupy different roles in the coordinated team strategy. We show that having a coordination model to infer the roles of players yields substantially improved imitation loss compared to conventional baselines. version:1
arxiv-1703-03111 | Statistical Cost Sharing | http://arxiv.org/abs/1703.03111 | id:1703.03111 author:Eric Balkanski, Umar Syed, Sergei Vassilvitskii category:cs.GT cs.LG  published:2017-03-09 summary:We study the cost sharing problem for cooperative games in situations where the cost function $C$ is not available via oracle queries, but must instead be derived from data, represented as tuples $(S, C(S))$, for different subsets $S$ of players. We formalize this approach, which we call statistical cost sharing, and consider the computation of the core and the Shapley value, when the tuples are drawn from some distribution $\mathcal{D}$. Previous work by Balcan et al. in this setting showed how to compute cost shares that satisfy the core property with high probability for limited classes of functions. We expand on their work and give an algorithm that computes such cost shares for any function with a non-empty core. We complement these results by proving an inapproximability lower bound for a weaker relaxation. We then turn our attention to the Shapley value. We first show that when cost functions come from the family of submodular functions with bounded curvature, $\kappa$, the Shapley value can be approximated from samples up to a $\sqrt{1 - \kappa}$ factor, and that the bound is tight. We then define statistical analogues of the Shapley axioms, and derive a notion of statistical Shapley value. We show that these can always be approximated arbitrarily well for general functions over any distribution $\mathcal{D}$. version:1
arxiv-1703-03108 | Image Classification of Melanoma, Nevus and Seborrheic Keratosis by Deep Neural Network Ensemble | http://arxiv.org/abs/1703.03108 | id:1703.03108 author:Kazuhisa Matsunaga, Akira Hamada, Akane Minagawa, Hiroshi Koga category:cs.CV  published:2017-03-09 summary:This short paper reports the method and the evaluation results of Casio and Shinshu University joint team for the ISBI Challenge 2017 - Skin Lesion Analysis Towards Melanoma Detection - Part 3: Lesion Classification hosted by ISIC. Our online validation score was 0.958 with melanoma classifier AUC 0.924 and seborrheic keratosis classifier AUC 0.993. version:1
arxiv-1702-03584 | Similarity Preserving Representation Learning for Time Series Analysis | http://arxiv.org/abs/1702.03584 | id:1702.03584 author:Qi Lei, Jinfeng Yi, Roman Vaculin, Lingfei Wu, Inderjit S. Dhillon category:cs.AI cs.LG  published:2017-02-12 summary:A considerable amount of machine learning algorithms take instance-feature matrices as their inputs. As such, they cannot directly analyze time series data due to its temporal nature, usually unequal lengths, and complex properties. This is a great pity since many of these algorithms are effective, robust, efficient, and easy to use. In this paper, we bridge this gap by proposing an efficient representation learning framework that is able to convert a set of time series with equal or unequal lengths to a matrix format. In particular, we guarantee that the pairwise similarities between time series are well preserved after the transformation. The learned feature representation is particularly suitable to the class of learning problems that are sensitive to data similarities. Given a set of $n$ time series, we first construct an $n\times n$ partially observed similarity matrix by randomly sampling $O(n \log n)$ pairs of time series and computing their pairwise similarities. We then propose an extremely efficient algorithm that solves a highly non-convex and NP-hard problem to learn new features based on the partially observed similarity matrix. We use the learned features to conduct experiments on both data classification and clustering tasks. Our extensive experimental results demonstrate that the proposed framework is both effective and efficient. version:2
arxiv-1702-06362 | Negative-Unlabeled Tensor Factorization for Location Category Inference from Inaccurate Mobility Data | http://arxiv.org/abs/1702.06362 | id:1702.06362 author:Jinfeng Yi, Qi Lei, Wesley Gifford, Ji Liu category:cs.LG  published:2017-02-21 summary:Identifying significant location categories visited by mobile phone users is the key to a variety of applications. This is an extremely challenging task due to the possible deviation between the estimated location coordinate and the actual location, which could be on the order of kilometers. Using the collected location coordinate as the center and its associated location error as the radius, we can draw a location uncertainty circle that may cover multiple location categories, especially in densely populated areas. To estimate the actual location category more precisely, we propose a novel tensor factorization framework, through several key observations including the intrinsic correlations between users, to infer the most likely location categories within the location uncertainty circle. In addition, the proposed algorithm can also predict where users are even when there is no location update. In order to efficiently solve the proposed framework, we propose a parameter-free and scalable optimization algorithm by effectively exploring the sparse and low-rank structure of the tensor. Our empirical studies show that the proposed algorithm is both efficient and effective: it can solve problems with millions of users and billions of location updates, and also provides superior prediction accuracies on real-world location updates and check-in data sets. version:2
arxiv-1703-03098 | DA-RNN: Semantic Mapping with Data Associated Recurrent Neural Networks | http://arxiv.org/abs/1703.03098 | id:1703.03098 author:Yu Xiang, Dieter Fox category:cs.CV cs.RO  published:2017-03-09 summary:3D scene understanding is important for robots to interact with the 3D world in a meaningful way. Most previous works on 3D scene understanding focus on recognizing geometrical or semantic properties of the scene independently. In this work, we introduce Data Associated Recurrent Neural Networks (DA-RNNs), a novel framework for joint 3D scene mapping and semantic labeling. DA-RNNs use a new recurrent neural network architecture for semantic labeling on RGB-D videos. The output of the network is integrated with mapping techniques such as KinectFusion in order to inject semantic information into the reconstructed 3D scene. Experiments conducted on a real world dataset and a synthetic dataset with RGB-D videos demonstrate the ability of our method in semantic 3D scene mapping. version:1
arxiv-1703-03097 | Information Extraction in Illicit Domains | http://arxiv.org/abs/1703.03097 | id:1703.03097 author:Mayank Kejriwal, Pedro Szekely category:cs.CL cs.AI  published:2017-03-09 summary:Extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact. Such domains employ atypical language models, have `long tails' and suffer from the problem of concept drift. In this paper, we propose a lightweight, feature-agnostic Information Extraction (IE) paradigm specifically designed for such domains. Our approach uses raw, unlabeled text from an initial corpus, and a few (12-120) seed annotations per domain-specific attribute, to learn robust IE models for unobserved pages and websites. Empirically, we demonstrate that our approach can outperform feature-centric Conditional Random Field baselines by over 18\% F-Measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings. We also show that our approach is demonstrably robust to concept drift, and can be efficiently bootstrapped even in a serial computing environment. version:1
arxiv-1703-03091 | Deep Learning applied to NLP | http://arxiv.org/abs/1703.03091 | id:1703.03091 author:Marc Moreno Lopez, Jugal Kalita category:cs.CL  published:2017-03-09 summary:Convolutional Neural Network (CNNs) are typically associated with Computer Vision. CNNs are responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today. More recently CNNs have been applied to problems in Natural Language Processing and gotten some interesting results. In this paper, we will try to explain the basics of CNNs, its different variations and how they have been applied to NLP. version:1
arxiv-1703-03076 | Efficient Simulation of Financial Stress Testing Scenarios with Suppes-Bayes Causal Networks | http://arxiv.org/abs/1703.03076 | id:1703.03076 author:Gelin Gao, Bud Mishra, Daniele Ramazzotti category:cs.LG cs.AI cs.CE  published:2017-03-08 summary:The most recent financial upheavals have cast doubt on the adequacy of some of the conventional quantitative risk management strategies, such as VaR (Value at Risk), in many common situations. Consequently, there has been an increasing need for verisimilar financial stress testings, namely simulating and analyzing financial portfolios in extreme, albeit rare scenarios. Unlike conventional risk management which exploits statistical correlations among financial instruments, here we focus our analysis on the notion of probabilistic causation, which is embodied by Suppes-Bayes Causal Networks (SBCNs), SBCNs are probabilistic graphical models that have many attractive features in terms of more accurate causal analysis for generating financial stress scenarios. In this paper, we present a novel approach for conducting stress testing of financial portfolios based on SBCNs in combination with classical machine learning classification tools. The resulting method is shown to be capable of correctly discovering the causal relationships among financial factors that affect the portfolios and thus, simulating stress testing scenarios with a higher accuracy and lower computational complexity than conventional Monte Carlo Simulations. version:1
arxiv-1703-03074 | Learning the Probabilistic Structure of Cumulative Phenomena with Suppes-Bayes Causal Networks | http://arxiv.org/abs/1703.03074 | id:1703.03074 author:Daniele Ramazzotti, Marco S. Nobile, Marco Antoniotti, Alex Graudenzi category:cs.LG cs.AI  published:2017-03-08 summary:One of the critical issues when adopting Bayesian networks (BNs) to model dependencies among random variables is to "learn" their structure, given the huge search space of possible solutions, i.e., all the possible direct acyclic graphs. This is a well-known NP-hard problem, which is also complicated by known pitfalls such as the issue of I-equivalence among different structures. In this work we restrict the investigations on BN structure learning to a specific class of networks, i.e., those representing the dynamics of phenomena characterized by the monotonic accumulation of events. Such phenomena allow to set specific structural constraints based on Suppes' theory of probabilistic causation and, accordingly, to define constrained BNs, named Suppes-Bayes Causal Networks (SBCNs). We here investigate the structure learning of SBCNs via extensive simulations with various state-of-the-art search strategies, such as canonical local search techniques and Genetic Algorithms. Among the main results we show that Suppes' constraints deeply simplify the learning task, by reducing the solution search space and providing a temporal ordering on the variables. version:1
arxiv-1703-03073 | Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations | http://arxiv.org/abs/1703.03073 | id:1703.03073 author:Liangzhen Lai, Naveen Suda, Vikas Chandra category:cs.LG cs.CV  published:2017-03-08 summary:Deep convolutional neural network (CNN) inference requires significant amount of memory and computation, which limits its deployment on embedded devices. To alleviate these problems to some extent, prior research utilize low precision fixed-point numbers to represent the CNN weights and activations. However, the minimum required data precision of fixed-point weights varies across different networks and also across different layers of the same network. In this work, we propose using floating-point numbers for representing the weights and fixed-point numbers for representing the activations. We show that using floating-point representation for weights is more efficient than fixed-point representation for the same bit-width and demonstrate it on popular large-scale CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16. We also show that such a representation scheme enables compact hardware multiply-and-accumulate (MAC) unit design. Experimental results show that the proposed scheme reduces the weight storage by up to 36% and power consumption of the hardware multiplier by up to 50%. version:1
arxiv-1703-03055 | Interpretable Structure-Evolving LSTM | http://arxiv.org/abs/1703.03055 | id:1703.03055 author:Xiaodan Liang, Liang Lin, Xiaohui Shen, Jiashi Feng, Shuicheng Yan, Eric P. Xing category:cs.CV cs.AI cs.LG  published:2017-03-08 summary:This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks. version:1
arxiv-1703-03054 | Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection | http://arxiv.org/abs/1703.03054 | id:1703.03054 author:Xiaodan Liang, Lisa Lee, Eric P. Xing category:cs.CV cs.AI cs.LG  published:2017-03-08 summary:Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes. version:1
arxiv-1703-03044 | A GAMP Based Low Complexity Sparse Bayesian Learning Algorithm | http://arxiv.org/abs/1703.03044 | id:1703.03044 author:Maher Al-Shoukairi, Philip Schniter, Bhaskar D. Rao category:cs.LG stat.ML  published:2017-03-08 summary:In this paper, we present an algorithm for the sparse signal recovery problem that incorporates damped Gaussian generalized approximate message passing (GGAMP) into Expectation-Maximization (EM)-based sparse Bayesian learning (SBL). In particular, GGAMP is used to implement the E-step in SBL in place of matrix inversion, leveraging the fact that GGAMP is guaranteed to converge with appropriate damping. The resulting GGAMP-SBL algorithm is much more robust to arbitrary measurement matrix $\boldsymbol{A}$ than the standard damped GAMP algorithm while being much lower complexity than the standard SBL algorithm. We then extend the approach from the single measurement vector (SMV) case to the temporally correlated multiple measurement vector (MMV) case, leading to the GGAMP-TSBL algorithm. We verify the robustness and computational advantages of the proposed algorithms through numerical experiments. version:1
arxiv-1703-03041 | Combining Bayesian Approaches and Evolutionary Techniques for the Inference of Breast Cancer Networks | http://arxiv.org/abs/1703.03041 | id:1703.03041 author:Stefano Beretta, Mauro Castelli, Ivo Goncalves, Ivan Merelli, Daniele Ramazzotti category:cs.LG cs.AI  published:2017-03-08 summary:Gene and protein networks are very important to model complex large-scale systems in molecular biology. Inferring or reverseengineering such networks can be defined as the process of identifying gene/protein interactions from experimental data through computational analysis. However, this task is typically complicated by the enormously large scale of the unknowns in a rather small sample size. Furthermore, when the goal is to study causal relationships within the network, tools capable of overcoming the limitations of correlation networks are required. In this work, we make use of Bayesian Graphical Models to attach this problem and, specifically, we perform a comparative study of different state-of-the-art heuristics, analyzing their performance in inferring the structure of the Bayesian Network from breast cancer data. version:1
arxiv-1703-03038 | Parallel Implementation of Efficient Search Schemes for the Inference of Cancer Progression Models | http://arxiv.org/abs/1703.03038 | id:1703.03038 author:Daniele Ramazzotti, Marco S. Nobile, Paolo Cazzaniga, Giancarlo Mauri, Marco Antoniotti category:cs.LG stat.ML  published:2017-03-08 summary:The emergence and development of cancer is a consequence of the accumulation over time of genomic mutations involving a specific set of genes, which provides the cancer clones with a functional selective advantage. In this work, we model the order of accumulation of such mutations during the progression, which eventually leads to the disease, by means of probabilistic graphic models, i.e., Bayesian Networks (BNs). We investigate how to perform the task of learning the structure of such BNs, according to experimental evidence, adopting a global optimization meta-heuristics. In particular, in this work we rely on Genetic Algorithms, and to strongly reduce the execution time of the inference -- which can also involve multiple repetitions to collect statistically significant assessments of the data -- we distribute the calculations using both multi-threading and a multi-node architecture. The results show that our approach is characterized by good accuracy and specificity; we also demonstrate its feasibility, thanks to a 84x reduction of the overall execution time with respect to a traditional sequential implementation. version:1
arxiv-1703-00297 | Group Sparsity Residual Constraint for Image Denoising | http://arxiv.org/abs/1703.00297 | id:1703.00297 author:Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Yechao Bai, Lan Tang category:cs.CV  published:2017-03-01 summary:Group sparsity or nonlocal image representation has shown great potential in image denoising. However, most existing methods only consider the nonlocal self-similarity (NSS) prior of noisy input image, that is, the similar patches collected only from degraded input, which makes the quality of image denoising largely depend on the input itself. In this paper we propose a new prior model for image denoising, called group sparsity residual constraint (GSRC). Different from the most existing NSS prior-based denoising methods, two kinds of NSS prior (i.e., NSS priors of noisy input image and pre-filtered image) are simultaneously used for image denoising. In particular, to boost the performance of group sparse-based image denoising, the group sparsity residual is proposed, and thus the problem of image denoising is transformed into one that reduces the group sparsity residual. To reduce the residual, we first obtain a good estimation of the group sparse coefficients of the original image by pre-filtering and then the group sparse coefficients of noisy input image are used to approximate the estimation. To improve the accuracy of the nonlocal similar patches selection, an adaptive patch search scheme is proposed. Moreover, to fuse these two NSS priors better, an effective iterative shrinkage algorithm is developed to solve the proposed GSRC model. Experimental results have demonstrated that the proposed GSRC modeling outperforms many state-of-the-art denoising methods in terms of the objective and the perceptual qualities. version:2
arxiv-1703-02992 | A Manifold Approach to Learning Mutually Orthogonal Subspaces | http://arxiv.org/abs/1703.02992 | id:1703.02992 author:Stephen Giguere, Francisco Garcia, Sridhar Mahadevan category:cs.LG G.1.6; I.2.6  published:2017-03-08 summary:Although many machine learning algorithms involve learning subspaces with particular characteristics, optimizing a parameter matrix that is constrained to represent a subspace can be challenging. One solution is to use Riemannian optimization methods that enforce such constraints implicitly, leveraging the fact that the feasible parameter values form a manifold. While Riemannian methods exist for some specific problems, such as learning a single subspace, there are more general subspace constraints that offer additional flexibility when setting up an optimization problem, but have not been formulated as a manifold. We propose the partitioned subspace (PS) manifold for optimizing matrices that are constrained to represent one or more subspaces. Each point on the manifold defines a partitioning of the input space into mutually orthogonal subspaces, where the number of partitions and their sizes are defined by the user. As a result, distinct groups of features can be learned by defining different objective functions for each partition. We illustrate the properties of the manifold through experiments on multiple dataset analysis and domain adaptation. version:1
arxiv-1703-02965 | Unsupervised Ensemble Regression | http://arxiv.org/abs/1703.02965 | id:1703.02965 author:Omer Dror, Boaz Nadler, Erhan Bilal, Yuval Kluger category:stat.ML cs.LG  published:2017-03-08 summary:Consider a regression problem where there is no labeled data and the only observations are the predictions $f_i(x_j)$ of $m$ experts $f_{i}$ over many samples $x_j$. With no knowledge on the accuracy of the experts, is it still possible to accurately estimate the unknown responses $y_{j}$? Can one still detect the least or most accurate experts? In this work we propose a framework to study these questions, based on the assumption that the $m$ experts have uncorrelated deviations from the optimal predictor. Assuming the first two moments of the response are known, we develop methods to detect the best and worst regressors, and derive U-PCR, a novel principal components approach for unsupervised ensemble regression. We provide theoretical support for U-PCR and illustrate its improved accuracy over the ensemble mean and median on a variety of regression problems. version:1
arxiv-1702-05960 | A Statistical Learning Approach to Modal Regression | http://arxiv.org/abs/1702.05960 | id:1702.05960 author:Yunlong Feng, Jun Fan, Johan A. K. Suykens category:stat.ML math.ST stat.ME stat.TH  published:2017-02-20 summary:This paper studies the nonparametric modal regression problem systematically from a statistical learning view. Originally motivated by pursuing a theoretical understanding of the maximum correntropy criterion based regression (MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is essentially modal regression. We show that nonparametric modal regression problem can be approached via the classical empirical risk minimization. Some efforts are then made to develop a framework for analyzing and implementing modal regression. For instance, the modal regression function is described, the modal regression risk is defined explicitly and its \textit{Bayes} rule is characterized; for the sake of computational tractability, the surrogate modal regression risk, which is termed as the generalization risk in our study, is introduced. On the theoretical side, the excess modal regression risk, the excess generalization risk, the function estimation error, and the relations among the above three quantities are studied rigorously. It turns out that under mild conditions, function estimation consistency and convergence may be pursued in modal regression as in vanilla regression protocols, such as mean regression, median regression, and quantile regression. However, it outperforms these regression models in terms of robustness as shown in our study from a re-descending M-estimation view. This coincides with and in return explains the merits of MCCR on robustness. On the practical side, the implementation issues of modal regression including the computational algorithm and the tuning parameters selection are discussed. Numerical assessments on modal regression are also conducted to verify our findings empirically. version:2
arxiv-1703-02952 | A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile Analytics | http://arxiv.org/abs/1703.02952 | id:1703.02952 author:Seyed Ali Ossia, Ali Shahin Shamsabadi, Ali Taheri, Hamid R. Rabiee, Nic Lane, Hamed Haddadi category:cs.LG cs.CV  published:2017-03-08 summary:The increasing quality of smartphone cameras and variety of photo editing applications, in addition to the rise in popularity of image-centric social media, have all led to a phenomenal growth in mobile-based photography. Advances in computer vision and machine learning techniques provide a large number of cloud-based services with the ability to provide content analysis, face recognition, and object detection facilities to third parties. These inferences and analytics might come with undesired privacy risks to the individuals. In this paper, we address a fundamental challenge: Can we utilize the local processing capabilities of modern smartphones efficiently to provide desired features to approved analytics services, while protecting against undesired inference attacks and preserving privacy on the cloud? We propose a hybrid architecture for a distributed deep learning model between the smartphone and the cloud. We rely on the Siamese network and machine learning approaches for providing privacy based on defined privacy constraints. We also use transfer learning techniques to evaluate the proposed method. Using the latest deep learning models for Face Recognition, Emotion Detection, and Gender Classification techniques, we demonstrate the effectiveness of our technique in providing highly accurate classification results for the desired analytics, while proving strong privacy guarantees. version:1
arxiv-1703-02942 | QuaSI: Quantile Sparse Image Prior for Spatio-Temporal Denoising of Retinal OCT Data | http://arxiv.org/abs/1703.02942 | id:1703.02942 author:Franziska Schirrmacher, Thomas Köhler, Lennart Husvogt, James G. Fujimoto, Joachim Hornegger, Andreas K. Maier category:cs.CV  published:2017-03-08 summary:Optical coherence tomography (OCT) enables high-resolution and non-invasive 3D imaging of the human retina but is inherently impaired by speckle noise. This paper introduces a spatio-temporal denoising algorithm for OCT data on a B-scan level using a novel quantile sparse image (QuaSI) prior. To remove speckle noise while preserving image structures of diagnostic relevance, we implement our QuaSI prior via median filter regularization coupled with a Huber data fidelity model in a variational approach. For efficient energy minimization, we develop an alternating direction method of multipliers (ADMM) scheme using a linearization of median filtering. Our spatio-temporal method can handle both, denoising of single B-scans and temporally consecutive B-scans, to gain volumetric OCT data with enhanced signal-to-noise ratio. Our algorithm based on 4 B-scans only achieved comparable performance to averaging 13 B-scans and outperformed other current denoising methods. version:1
arxiv-1703-02931 | Fast Gesture Recognition with Multiple Stream Discrete HMMs on 3D Skeletons | http://arxiv.org/abs/1703.02931 | id:1703.02931 author:Guido Borghi, Roberto Vezzani, Rita Cucchiara category:cs.CV  published:2017-03-08 summary:HMMs are widely used in action and gesture recognition due to their implementation simplicity, low computational requirement, scalability and high parallelism. They have worth performance even with a limited training set. All these characteristics are hard to find together in other even more accurate methods. In this paper, we propose a novel double-stage classification approach, based on Multiple Stream Discrete Hidden Markov Models (MSD-HMM) and 3D skeleton joint data, able to reach high performances maintaining all advantages listed above. The approach allows both to quickly classify pre-segmented gestures (offline classification), and to perform temporal segmentation on streams of gestures (online classification) faster than real time. We test our system on three public datasets, MSRAction3D, UTKinect-Action and MSRDailyAction, and on a new dataset, Kinteract Dataset, explicitly created for Human Computer Interaction (HCI). We obtain state of the art performances on all of them. version:1
arxiv-1703-02930 | Nearly-tight VC-dimension bounds for piecewise linear neural networks | http://arxiv.org/abs/1703.02930 | id:1703.02930 author:Nick Harvey, Chris Liaw, Abbas Mehrabian category:cs.LG  published:2017-03-08 summary:We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting $W$ be the number of weights and $L$ be the number of layers, we prove that the VC-dimension is $O(W L \log(W))$ and $\Omega( W L \log(W/L) )$. This improves both the previously known upper bounds and lower bounds. In terms of the number $U$ of non-linear units, we prove a tight bound $\Theta(W U)$ on the VC-dimension. All of these results generalize to arbitrary piecewise linear activation functions. version:1
arxiv-1703-02921 | Transformation-Grounded Image Generation Network for Novel 3D View Synthesis | http://arxiv.org/abs/1703.02921 | id:1703.02921 author:Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, Alexander C. Berg category:cs.CV  published:2017-03-08 summary:We present a transformation-grounded image generation network for novel 3D view synthesis from a single image. Instead of taking a 'blank slate' approach, we first explicitly infer the parts of the geometry visible both in the input and novel views and then re-cast the remaining synthesis problem as image completion. Specifically, we both predict a flow to move the pixels from the input to the novel view along with a novel visibility map that helps deal with occulsion/disocculsion. Next, conditioned on those intermediate results, we hallucinate (infer) parts of the object invisible in the input image. In addition to the new network structure, training with a combination of adversarial and perceptual loss results in a reduction in common artifacts of novel view synthesis such as distortions and holes, while successfully generating high frequency details and preserving visual aspects of the input image. We evaluate our approach on a wide range of synthetic and real examples. Both qualitative and quantitative results show our method achieves significantly better results compared to existing methods. version:1
arxiv-1703-02914 | Dropout Inference in Bayesian Neural Networks with Alpha-divergences | http://arxiv.org/abs/1703.02914 | id:1703.02914 author:Yingzhen Li, Yarin Gal category:cs.LG stat.ML  published:2017-03-08 summary:To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI's KL objective, which are able to avoid VI's uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model's epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's uncertainty. version:1
arxiv-1703-02910 | Deep Bayesian Active Learning with Image Data | http://arxiv.org/abs/1703.02910 | id:1703.02910 author:Yarin Gal, Riashat Islam, Zoubin Ghahramani category:cs.LG cs.CV stat.ML  published:2017-03-08 summary:Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task). version:1
arxiv-1703-02905 | Learning a Unified Control Policy for Safe Falling | http://arxiv.org/abs/1703.02905 | id:1703.02905 author:Visak CV Kumar, Sehoon Ha, C Karen Liu category:cs.RO cs.AI cs.LG  published:2017-03-08 summary:Being able to fall safely is a necessary motor skill for humanoids performing highly dynamic tasks, such as running and jumping. We propose a new method to learn a policy that minimizes the maximal impulse during the fall. The optimization solves for both a discrete contact planning problem and a continuous optimal control problem. Once trained, the policy can compute the optimal next contacting body part (e.g. left foot, right foot, or hands), contact location and timing, and the required joint actuation. We represent the policy as a mixture of actor-critic neural network, which consists of n control policies and the corresponding value functions. Each pair of actor-critic is associated with one of the n possible contacting body parts. During execution, the policy corresponding to the highest value function will be executed while the associated body part will be the next contact with the ground. With this mixture of actor-critic architecture, the discrete contact sequence planning is solved through the selection of the best critics while the continuous control problem is solved by the optimization of actors. We show that our policy can achieve comparable, sometimes even higher, rewards than a recursive search of the action space using dynamic programming, while enjoying 50 to 400 times of speed gain during online execution. version:1
arxiv-1703-02899 | Model-Based Policy Search for Automatic Tuning of Multivariate PID Controllers | http://arxiv.org/abs/1703.02899 | id:1703.02899 author:Andreas Doerr, Duy Nguyen-Tuong, Alonso Marco, Stefan Schaal, Sebastian Trimpe category:cs.LG cs.RO cs.SY stat.ML  published:2017-03-08 summary:PID control architectures are widely used in industrial applications. Despite their low number of open parameters, tuning multiple, coupled PID controllers can become tedious in practice. In this paper, we extend PILCO, a model-based policy search framework, to automatically tune multivariate PID controllers purely based on data observed on an otherwise unknown system. The system's state is extended appropriately to frame the PID policy as a static state feedback policy. This renders PID tuning possible as the solution of a finite horizon optimal control problem without further a priori knowledge. The framework is applied to the task of balancing an inverted pendulum on a seven degree-of-freedom robotic arm, thereby demonstrating its capabilities of fast and data-efficient policy learning, even on complex real world problems. version:1
arxiv-1703-02883 | Memory Enriched Big Bang Big Crunch Optimization Algorithm for Data Clustering | http://arxiv.org/abs/1703.02883 | id:1703.02883 author:Kayvan Bijari, Hadi Zare, Hadi Veisi, Hossein Bobarshad category:cs.AI cs.LG  published:2017-03-08 summary:Cluster analysis plays an important role in decision making process for many knowledge-based systems. There exist a wide variety of different approaches for clustering applications including the heuristic techniques, probabilistic models, and traditional hierarchical algorithms. In this paper, a novel heuristic approach based on big bang-big crunch algorithm is proposed for clustering problems. The proposed method not only takes advantage of heuristic nature to alleviate typical clustering algorithms such as k-means, but it also benefits from the memory based scheme as compared to its similar heuristic techniques. Furthermore, the performance of the proposed algorithm is investigated based on several benchmark test functions as well as on the well-known datasets. The experimental results show the significant superiority of the proposed method over the similar algorithms. version:1
arxiv-1703-02245 | Design of the Artificial: lessons from the biological roots of general intelligence | http://arxiv.org/abs/1703.02245 | id:1703.02245 author:Nima Dehghani category:cs.AI cs.NE nlin.AO q-bio.NC  published:2017-03-07 summary:Our desire and fascination with intelligent machines dates back to the antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought (syllogism) and Heron of Alexandria's mechanical machines and automata. However, the quest for Artificial General Intelligence (AGI) is troubled with repeated failures of strategies and approaches throughout the history. This decade has seen a shift in interest towards bio-inspired software and hardware, with the assumption that such mimicry entails intelligence. Though these steps are fruitful in certain directions and have advanced automation, their singular design focus renders them highly inefficient in achieving AGI. Which set of requirements have to be met in the design of AGI? What are the limits in the design of the artificial? Here, a careful examination of computation in biological systems hints that evolutionary tinkering of contextual processing of information enabled by a hierarchical architecture is the key to build AGI. version:2
arxiv-1703-02860 | Spice up Your Chat: The Intentions and Sentiment Effects of Using Emoji | http://arxiv.org/abs/1703.02860 | id:1703.02860 author:Tianran Hu, Han Guo, Hao Sun, Thuy-vy Thi Nguyen, Jiebo Luo category:cs.CL cs.HC  published:2017-03-08 summary:Emojis, as a new way of conveying nonverbal cues, are widely adopted in computer-mediated communications. In this paper, first from a message sender perspective, we focus on people's motives in using four types of emojis -- positive, neutral, negative, and non-facial. We compare the willingness levels of using these emoji types for seven typical intentions that people usually apply nonverbal cues for in communication. The results of extensive statistical hypothesis tests not only report the popularities of the intentions, but also uncover the subtle differences between emoji types in terms of intended uses. Second, from a perspective of message recipients, we further study the sentiment effects of emojis, as well as their duplications, on verbal messages. Different from previous studies in emoji sentiment, we study the sentiments of emojis and their contexts as a whole. The experiment results indicate that the powers of conveying sentiment are different between four emoji types, and the sentiment effects of emojis vary in the contexts of different valences. version:1
arxiv-1703-02859 | A World of Difference: Divergent Word Interpretations among People | http://arxiv.org/abs/1703.02859 | id:1703.02859 author:Tianran Hu, Ruihua Song, Philip Ding, Xing Xie, Jiebo Luo category:cs.CL  published:2017-03-08 summary:Divergent word usages reflect differences among people. In this paper, we present a novel angle for studying word usage divergence -- word interpretations. We propose an approach that quantifies semantic differences in interpretations among different groups of people. The effectiveness of our approach is validated by quantitative evaluations. Experiment results indicate that divergences in word interpretations exist. We further apply the approach to two well studied types of differences between people -- gender and region. The detected words with divergent interpretations reveal the unique features of specific groups of people. For gender, we discover that certain different interests, social attitudes, and characters between males and females are reflected in their divergent interpretations of many words. For region, we find that specific interpretations of certain words reveal the geographical and cultural features of different regions. version:1
arxiv-1703-02834 | Exact Dimensionality Selection for Bayesian PCA | http://arxiv.org/abs/1703.02834 | id:1703.02834 author:Charles Bouveyron, Pierre Latouche, Pierre-Alexandre Mattei category:stat.ME math.ST stat.ML stat.TH  published:2017-03-08 summary:We present a Bayesian model selection approach to estimate the intrinsic dimensionality of a high-dimensional dataset. To this end, we introduce a novel formulation of the probabilisitic principal component analysis model based on a normal-gamma prior distribution. In this context, we exhibit a closed-form expression of the marginal likelihood which allows to infer an optimal number of components. We also propose a heuristic based on the expected shape of the marginal likelihood curve in order to choose the hyperparameters. In non-asymptotic frameworks, we show on simulated data that this exact dimensionality selection approach is competitive with both Bayesian and frequentist state-of-the-art methods. version:1
arxiv-1703-02826 | A Linear Extrinsic Calibration of Kaleidoscopic Imaging System from Single 3D Point | http://arxiv.org/abs/1703.02826 | id:1703.02826 author:Kosuke Takahashi, Akihiro Miyata, Shohei Nobuhara, Takashi Matsuyama category:cs.CV  published:2017-03-08 summary:This paper proposes a new extrinsic calibration of kaleidoscopic imaging system by estimating normals and distances of the mirrors. The problem to be solved in this paper is a simultaneous estimation of all mirror parameters consistent throughout multiple reflections. Unlike conventional methods utilizing a pair of direct and mirrored images of a reference 3D object to estimate the parameters on a per-mirror basis, our method renders the simultaneous estimation problem into solving a linear set of equations. The key contribution of this paper is to introduce a linear estimation of multiple mirror parameters from kaleidoscopic 2D projections of a single 3D point of unknown geometry. Evaluations with synthesized and real images demonstrate the performance of the proposed algorithm in comparison with conventional methods. version:1
arxiv-1412-4869 | Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data | http://arxiv.org/abs/1412.4869 | id:1412.4869 author:Andrew Gelman, Aki Vehtari, Pasi Jylänki, Tuomas Sivula, Dustin Tran, Swupnil Sahai, Paul Blomstedt, John P. Cunningham, David Schiminovich, Christian Robert category:stat.CO stat.ME stat.ML  published:2014-12-16 summary:A common approach for Bayesian computation with big data is to partition the data into smaller pieces, perform local inference for each piece separately, and finally combine the results to obtain an approximation to the global posterior. Looking at this from the bottom up, one can perform separate analyses on individual sources of data and then want to combine these in a larger Bayesian model. In either case, the idea of distributed modeling and inference has both conceptual and computational appeal, but from the Bayesian perspective there is no general way of handling the prior distribution: if the prior is included in each separate inference, it will be multiply-counted when the inferences are combined; but if the prior is itself divided into pieces, it may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma, expectation propagation (EP) has been proposed as a prototype for distributed Bayesian inference. The central idea is to factor the likelihood according to the data partitions, and to iteratively combine each factor with an approximate model of the prior and all other parts of the data, thus producing an overall approximation to the global posterior at convergence. In this paper, we give an introduction to EP and an overview of some recent developments of the method, with particular emphasis on its use in combining inferences from partitioned data. In addition to distributed modeling of large datasets, our unified treatment also includes hierarchical modeling of data with a naturally partitioned structure. version:2
arxiv-1703-02819 | Introduction to Formal Concept Analysis and Its Applications in Information Retrieval and Related Fields | http://arxiv.org/abs/1703.02819 | id:1703.02819 author:Dmitry I. Ignatov category:cs.IR cs.AI cs.CL cs.DM stat.ML 68P20  06B99  68T30 H.3.3; G.2; I.2  published:2017-03-08 summary:This paper is a tutorial on Formal Concept Analysis (FCA) and its applications. FCA is an applied branch of Lattice Theory, a mathematical discipline which enables formalisation of concepts as basic units of human thinking and analysing data in the object-attribute form. Originated in early 80s, during the last three decades, it became a popular human-centred tool for knowledge representation and data analysis with numerous applications. Since the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics include Information Retrieval with a focus on visualisation aspects, Machine Learning, Data Mining and Knowledge Discovery, Text Mining and several others. version:1
arxiv-1703-02810 | An Integrated and Scalable Platform for Proactive Event-Driven Traffic Management | http://arxiv.org/abs/1703.02810 | id:1703.02810 author:Alain Kibangou, Alexander Artikis, Evangelos Michelioudakis, Georgios Paliouras, Marius Schmitt, John Lygeros, Chris Baber, Natan Morar, Fabiana Fournier, Inna Skarbovsky category:cs.AI cs.LG cs.SY  published:2017-03-08 summary:Traffic on freeways can be managed by means of ramp meters from Road Traffic Control rooms. Human operators cannot efficiently manage a network of ramp meters. To support them, we present an intelligent platform for traffic management which includes a new ramp metering coordination scheme in the decision making module, an efficient dashboard for interacting with human operators, machine learning tools for learning event definitions and Complex Event Processing tools able to deal with uncertainties inherent to the traffic use case. Unlike the usual approach, the devised event-driven platform is able to predict a congestion up to 4 minutes before it really happens. Proactive decision making can then be established leading to significant improvement of traffic conditions. version:1
arxiv-1703-02806 | Deep Reservoir Computing Using Cellular Automata | http://arxiv.org/abs/1703.02806 | id:1703.02806 author:Stefano Nichele, Andreas Molund category:cs.NE cs.ET  published:2017-03-08 summary:Recurrent Neural Networks (RNNs) have been a prominent concept within artificial intelligence. They are inspired by Biological Neural Networks (BNNs) and provide an intuitive and abstract representation of how BNNs work. Derived from the more generic Artificial Neural Networks (ANNs), the recurrent ones are meant to be used for temporal tasks, such as speech recognition, because they are capable of memorizing historic input. However, such networks are very time consuming to train as a result of their inherent nature. Recently, Echo State Networks and Liquid State Machines have been proposed as possible RNN alternatives, under the name of Reservoir Computing (RC). RCs are far more easy to train. In this paper, Cellular Automata are used as reservoir, and are tested on the 5-bit memory task (a well known benchmark within the RC community). The work herein provides a method of mapping binary inputs from the task onto the automata, and a recurrent architecture for handling the sequential aspects of it. Furthermore, a layered (deep) reservoir architecture is proposed. Performances are compared towards earlier work, in addition to its single-layer version. Results show that the single CA reservoir system yields similar results to state-of-the-art work. The system comprised of two layered reservoirs do show a noticeable improvement compared to a single CA reservoir. This indicates potential for further research and provides valuable insight on how to design CA reservoir systems. version:1
arxiv-1703-02757 | Byzantine-Tolerant Machine Learning | http://arxiv.org/abs/1703.02757 | id:1703.02757 author:Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, Julien Stainer category:cs.DC cs.LG cs.NE math.OC stat.ML  published:2017-03-08 summary:The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \cdot (d + \log n))$. version:1
arxiv-1702-08000 | Kiefer Wolfowitz Algorithm is Asymptotically Optimal for a Class of Non-Stationary Bandit Problems | http://arxiv.org/abs/1702.08000 | id:1702.08000 author:Rahul Singh, Taposh Banerjee category:stat.ML cs.LG  published:2017-02-26 summary:We consider the problem of designing an allocation rule or an "online learning algorithm" for a class of bandit problems in which the set of control actions available at each time $s$ is a convex, compact subset of $\mathbb{R}^d$. Upon choosing an action $x$ at time $s$, the algorithm obtains a noisy value of the unknown and time-varying function $f_s$ evaluated at $x$. The "regret" of an algorithm is the gap between its expected reward, and the reward earned by a strategy which has the knowledge of the function $f_s$ at each time $s$ and hence chooses the action $x_s$ that maximizes $f_s$. For this non-stationary bandit problem set-up, we consider two variants of the Kiefer Wolfowitz (KW) algorithm i) KW with fixed step-size $\beta$, and ii) KW with sliding window of length $L$. We show that if the number of times that the function $f_s$ varies during time $T$ is $o(T)$, and if the learning rates of the proposed algorithms are chosen "optimally", then the regret of the proposed algorithms is $o(T)$, and hence the algorithms are asymptotically efficient. version:2
arxiv-1703-02529 | Optimizing Deep CNN-Based Queries over Video Streams at Scale | http://arxiv.org/abs/1703.02529 | id:1703.02529 author:Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis, Matei Zaharia category:cs.DB cs.CV  published:2017-03-07 summary:Video is one of the fastest-growing sources of data and is rich with interesting semantic information. Furthermore, recent advances in computer vision, in the form of deep convolutional neural networks (CNNs), have made it possible to query this semantic information with near-human accuracy (in the form of image tagging). However, performing inference with state-of-the-art CNNs is computationally expensive: analyzing videos in real time (at 30 frames/sec) requires a $1200 GPU per video stream, posing a serious computational barrier to CNN adoption in large-scale video data management systems. In response, we present NOSCOPE, a system that uses cost-based optimization to assemble a specialized video processing pipeline for each input video stream, greatly accelerating subsequent CNNbased queries on the video. As NOSCOPE observes a video, it trains two types of pipeline components (which we call filters) to exploit the locality in the video stream: difference detectors that exploit temporal locality between frames, and specialized models that are tailored to a specific scene and query (i.e., exploit environmental and query-specific locality). We show that the optimal set of filters and their parameters depends significantly on the video stream and query in question, so NOSCOPE introduces an efficient cost-based optimizer for this problem to select them. With this approach, our NOSCOPE prototype achieves up to 120-3,200x speed-ups (318- 8,500x real-time) on binary classification tasks over real-world webcam and surveillance video while maintaining accuracy within 1-5% of a state-of-the-art CNN. version:2
arxiv-1703-02728 | Inference in Sparse Graphs with Pairwise Measurements and Side Information | http://arxiv.org/abs/1703.02728 | id:1703.02728 author:Dylan J. Foster, Daniel Reichman, Karthik Sridharan category:cs.LG  published:2017-03-08 summary:We consider the statistical problem of recovering a hidden "ground truth" binary labeling for the vertices of a graph G up to low Hamming error from noisy edge and vertex measurements. We present new algorithms and a sharp finite-sample analysis for this problem on trees and sparse graphs with poor expansion properties such as hypergrids and ring lattices. Our method generalizes and improves over Globerson et al. (2015), who introduced the problem for two-dimensional grid lattices. For trees we provide a simple, efficient, algorithm that infers the ground truth with optimal Hamming error and implies recovery results for all connected graphs. Here, the presence of side information is critical to obtain a non-trivial recovery rate. We then show how to adapt this algorithm to tree decompositions of edge-subgraphs of certain graph families such as lattices, resulting in optimal recovery error rates that can be obtained in a highly efficient manner. The thrust of our analysis is to 1) use the tree decomposition along with edge measurements to produce a small class of viable vertex labelings and 2) apply fast rate results from statistical learn-ing theory to show that we can infer the ground truth from this class using vertex measurements. We show the power of our method by providing several examples including hypergrids, ring lattices, and the Newman-Watts model for small world graphs. For two-dimensional grids, our results improve over Globerson et al. (2015) by obtaining optimal recovery in the constant-height regime. version:1
arxiv-1703-02724 | Guaranteed Tensor PCA with Optimality in Statistics and Computation | http://arxiv.org/abs/1703.02724 | id:1703.02724 author:Anru Zhang, Dong Xia category:math.ST cs.LG stat.ME stat.ML stat.TH  published:2017-03-08 summary:Tensors, or high-order arrays, attract much attention in recent research. In this paper, we propose a general framework for tensor principal component analysis (tensor PCA), which focuses on the methodology and theory for extracting the hidden low-rank structure from the high-dimensional tensor data. A unified solution is provided for tensor PCA with considerations in both statistical limits and computational costs. The problem exhibits three different phases according to the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast spectral power iteration method that achieves the minimax optimal rate of convergence in estimation; with weak SNR, the information-theoretical lower bound shows that it is impossible to have consistent estimation in general; with moderate SNR, we show that the non-convex maximum likelihood estimation provides optimal solution, but with NP-hard computational cost; moreover, under the hardness hypothesis of hypergraphic planted clique detection, there are no polynomial-time algorithms performing consistently in general. Simulation studies show that the proposed spectral power iteration method have good performance under a variety of settings. version:1
arxiv-1703-02723 | Scalable Greedy Feature Selection via Weak Submodularity | http://arxiv.org/abs/1703.02723 | id:1703.02723 author:Rajiv Khanna, Ethan Elenberg, Alexandros G. Dimakis, Sahand Negahban, Joydeep Ghosh category:stat.ML cs.IT cs.LG math.IT  published:2017-03-08 summary:Greedy algorithms are widely used for problems in machine learning such as feature selection and set function optimization. Unfortunately, for large datasets, the running time of even greedy algorithms can be quite high. This is because for each greedy step we need to refit a model or calculate a function using the previously selected choices and the new candidate. Two algorithms that are faster approximations to the greedy forward selection were introduced recently ([Mirzasoleiman et al. 2013, 2015]). They achieve better performance by exploiting distributed computation and stochastic evaluation respectively. Both algorithms have provable performance guarantees for submodular functions. In this paper we show that divergent from previously held opinion, submodularity is not required to obtain approximation guarantees for these two algorithms. Specifically, we show that a generalized concept of weak submodularity suffices to give multiplicative approximation guarantees. Our result extends the applicability of these algorithms to a larger class of functions. Furthermore, we show that a bounded submodularity ratio can be used to provide data dependent bounds that can sometimes be tighter also for submodular functions. We empirically validate our work by showing superior performance of fast greedy approximations versus several established baselines on artificial and real datasets. version:1
arxiv-1703-02721 | On Approximation Guarantees for Greedy Low Rank Optimization | http://arxiv.org/abs/1703.02721 | id:1703.02721 author:Rajiv Khanna, Ethan Elenberg, Alexandros G. Dimakis, Sahand Negahban category:stat.ML cs.IT cs.LG math.IT  published:2017-03-08 summary:We provide new approximation guarantees for greedy low rank matrix estimation under standard assumptions of restricted strong convexity and smoothness. Our novel analysis also uncovers previously unknown connections between the low rank estimation and combinatorial optimization, so much so that our bounds are reminiscent of corresponding approximation bounds in submodular maximization. Additionally, we also provide statistical recovery guarantees. Finally, we present empirical comparison of greedy estimation with established baselines on two important real-world problems. version:1
arxiv-1703-02719 | Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network | http://arxiv.org/abs/1703.02719 | id:1703.02719 author:Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, Jian Sun category:cs.CV  published:2017-03-08 summary:One of recent trends [30, 31, 14] in network architec- ture design is stacking small filters (e.g., 1x1 or 3x3) in the entire network because the stacked small filters is more ef- ficient than a large kernel, given the same computational complexity. However, in the field of semantic segmenta- tion, where we need to perform dense per-pixel prediction, we find that the large kernel (and effective receptive field) plays an important role when we have to perform the clas- sification and localization tasks simultaneously. Following our design principle, we propose a Global Convolutional Network to address both the classification and localization issues for the semantic segmentation. We also suggest a residual-based boundary refinement to further refine the ob- ject boundaries. Our approach achieves state-of-art perfor- mance on two public benchmarks and significantly outper- forms previous results, 82.2% (vs 80.2%) on PASCAL VOC 2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset. version:1
arxiv-1703-02716 | A Pursuit of Temporal Accuracy in General Activity Detection | http://arxiv.org/abs/1703.02716 | id:1703.02716 author:Yuanjun Xiong, Yue Zhao, Limin Wang, Dahua Lin, Xiaoou Tang category:cs.CV  published:2017-03-08 summary:Detecting activities in untrimmed videos is an important but challenging task. The performance of existing methods remains unsatisfactory, e.g., they often meet difficulties in locating the beginning and end of a long complex action. In this paper, we propose a generic framework that can accurately detect a wide variety of activities from untrimmed videos. Our first contribution is a novel proposal scheme that can efficiently generate candidates with accurate temporal boundaries. The other contribution is a cascaded classification pipeline that explicitly distinguishes between relevance and completeness of a candidate instance. On two challenging temporal activity detection datasets, THUMOS14 and ActivityNet, the proposed framework significantly outperforms the existing state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling activities with various temporal structures. version:1
arxiv-1703-02710 | Tree-Structured Reinforcement Learning for Sequential Object Localization | http://arxiv.org/abs/1703.02710 | id:1703.02710 author:Zequn Jie, Xiaodan Liang, Jiashi Feng, Xiaojie Jin, Wen Feng Lu, Shuicheng Yan category:cs.CV  published:2017-03-08 summary:Existing object proposal algorithms usually search for possible object regions over multiple locations and scales separately, which ignore the interdependency among different objects and deviate from the human perception procedure. To incorporate global interdependency between objects into object localization, we propose an effective Tree-structured Reinforcement Learning (Tree-RL) approach to sequentially search for objects by fully exploiting both the current observation and historical search paths. The Tree-RL approach learns multiple searching policies through maximizing the long-term reward that reflects localization accuracies over all the objects. Starting with taking the entire image as a proposal, the Tree-RL approach allows the agent to sequentially discover multiple objects via a tree-structured traversing scheme. Allowing multiple near-optimal policies, Tree-RL offers more diversity in search paths and is able to find multiple objects with a single feed-forward pass. Therefore, Tree-RL can better cover different objects with various scales which is quite appealing in the context of object proposal. Experiments on PASCAL VOC 2007 and 2012 validate the effectiveness of the Tree-RL, which can achieve comparable recalls with current object proposal algorithms via much fewer candidate windows. version:1
arxiv-1703-02702 | Robust Adversarial Reinforcement Learning | http://arxiv.org/abs/1703.02702 | id:1703.02702 author:Lerrel Pinto, James Davidson, Rahul Sukthankar, Abhinav Gupta category:cs.LG cs.AI cs.MA cs.RO  published:2017-03-08 summary:Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced -- that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary. version:1
arxiv-1703-02442 | Detecting Cancer Metastases on Gigapixel Pathology Images | http://arxiv.org/abs/1703.02442 | id:1703.02442 author:Yun Liu, Krishna Gadepalli, Mohammad Norouzi, George E. Dahl, Timo Kohlberger, Aleksey Boyko, Subhashini Venugopalan, Aleksei Timofeev, Philip Q. Nelson, Greg S. Corrado, Jason D. Hipp, Lily Peng, Martin C. Stumpe category:cs.CV  published:2017-03-03 summary:Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection. version:2
arxiv-1703-02690 | Leveraging Sparsity for Efficient Submodular Data Summarization | http://arxiv.org/abs/1703.02690 | id:1703.02690 author:Erik M. Lindgren, Shanshan Wu, Alexandros G. Dimakis category:stat.ML cs.DS cs.IT cs.LG math.IT  published:2017-03-08 summary:The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary---solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries. version:1
arxiv-1703-02689 | Exact MAP Inference by Avoiding Fractional Vertices | http://arxiv.org/abs/1703.02689 | id:1703.02689 author:Erik M. Lindgren, Alexandros G. Dimakis, Adam Klivans category:stat.ML cs.DS cs.IT cs.LG math.IT  published:2017-03-08 summary:Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice. A major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time. We require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions. version:1
arxiv-1703-02091 | Optimized Cost per Click in Taobao Display Advertising | http://arxiv.org/abs/1703.02091 | id:1703.02091 author:Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, Kun Gai category:cs.GT stat.ML  published:2017-02-27 summary:Taobao, as the largest online retail platform in the world, provides billions of online display advertising impressions for millions of advertisers every day. For commercial purposes, the advertisers bid for specific spots and target crowds to compete for business traffic. The platform chooses the most suitable ads to display in tens of milliseconds. Common pricing methods include cost per mille (CPM) and cost per click (CPC). Traditional advertising systems target certain traits of users and ad placements with fixed bids, essentially regarded as coarse-grained matching of bid and traffic quality. However, the fixed bids set by the advertisers competing for different quality requests cannot fully optimize the advertisers' key requirements. Moreover, the platform has to be responsible for the business revenue and user experience. Thus, we proposed a bid optimizing strategy called optimized cost per click (OCPC) which automatically adjusts the bid to achieve finer matching of bid and traffic quality of page view (PV) request granularity. Our approach optimizes advertisers' demands, platform business revenue and user experience and as a whole improves traffic allocation efficiency. We have validated our approach in Taobao display advertising system in production. The online A/B test shows our algorithm yields substantially better results than previous fixed bid manner. version:2
arxiv-1703-02682 | Sparse Quadratic Logistic Regression in Sub-quadratic Time | http://arxiv.org/abs/1703.02682 | id:1703.02682 author:Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G. Dimakis, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT  published:2017-03-08 summary:We consider support recovery in the quadratic logistic regression setting - where the target depends on both p linear terms $x_i$ and up to $p^2$ quadratic terms $x_i x_j$. Quadratic terms enable prediction/modeling of higher-order effects between features and the target, but when incorporated naively may involve solving a very large regression problem. We consider the sparse case, where at most $s$ terms (linear or quadratic) are non-zero, and provide a new faster algorithm. It involves (a) identifying the weak support (i.e. all relevant variables) and (b) standard logistic regression optimization only on these chosen variables. The first step relies on a novel insight about correlation tests in the presence of non-linearity, and takes $O(pn)$ time for $n$ samples - giving potentially huge computational gains over the naive approach. Motivated by insights from the boolean case, we propose a non-linear correlation test for non-binary finite support case that involves hashing a variable and then correlating with the output variable. We also provide experimental results to demonstrate the effectiveness of our methods. version:1
arxiv-1703-02679 | Performance Bounds for Graphical Record Linkage | http://arxiv.org/abs/1703.02679 | id:1703.02679 author:Rebecca C. Steorts, Matt Barnes, Willie Neiswanger category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH  published:2017-03-08 summary:Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat record linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance. version:1
arxiv-1702-03505 | A Novel Weight-Shared Multi-Stage Network Architecture of CNNs for Scale Invariance | http://arxiv.org/abs/1702.03505 | id:1702.03505 author:Ryo Takahashi, Takashi Matsubara, Kuniaki Uehara category:cs.CV  published:2017-02-12 summary:Convolutional neural networks (CNNs) have demonstrated remarkable results in image classification tasks for benchmark and practical uses. The CNNs with deeper architectures have achieved higher performances recently thanks to their robustness to parallel shift of objects in images aw well as their numerous parameters and resulting high expression ability. However, the CNNs have a limited robustness to other geometric transformations such as scaling and rotation. This problem is considered to limit performance improvement of the deep CNNs but there is no established solution. This study focuses on scale transformation and proposes a novel network architecture called weight-shared multi-stage network (WSMS-Net), consisting of multiple stages of CNNs. The WSMS-Net is easily combined with existing deep CNNs, such as ResNet and DenseNet, and enables them to acquire a robustness to scaling of objects. The experimental results demonstrate that existing deep CNNs combined with the proposed WSMS-Net achieve higher accuracy for image classification tasks only with a little increase in the number of parameters. version:2
arxiv-1703-02674 | Column Subset Selection via Polynomial Time Dual Volume Sampling | http://arxiv.org/abs/1703.02674 | id:1703.02674 author:Chengtao Li, Stefanie Jegelka, Suvrit Sra category:stat.ML  published:2017-03-08 summary:We study dual volume sampling, a method for selecting k columns from an n*m short and wide matrix (n <= k <= m) such that the probability of selection is proportional to the volume of the parallelepiped spanned by the rows of the induced submatrix. This method was studied in [3], who motivated it as a promising method for column subset selection. However, the development of polynomial time sampling algorithms -- exact or approximate -- has been since open. We close this open problem by presenting (i) an exact (randomized) polynomial time sampling algorithm; (ii) its derandomization that samples subsets satisfying the desired properties deterministically; and (iii) an efficient approximate sampling procedure using Markov chains that are provably fast mixing. Our algorithms can thus benefit downstream applications of dual volume sampling, such as column subset selection and experimental design. version:1
arxiv-1703-02662 | Structural Data Recognition with Graph Model Boosting | http://arxiv.org/abs/1703.02662 | id:1703.02662 author:Tomo Miyazaki, Shinichiro Omachi category:cs.LG stat.ML  published:2017-03-08 summary:This paper presents a novel method for structural data recognition using a large number of graph models. In general, prevalent methods for structural data recognition have two shortcomings: 1) Only a single model is used to capture structural variation. 2) Naive recognition methods are used, such as the nearest neighbor method. In this paper, we propose strengthening the recognition performance of these models as well as their ability to capture structural variation. The proposed method constructs a large number of graph models and trains decision trees using the models. This paper makes two main contributions. The first is a novel graph model that can quickly perform calculations, which allows us to construct several models in a feasible amount of time. The second contribution is a novel approach to structural data recognition: graph model boosting. Comprehensive structural variations can be captured with a large number of graph models constructed in a boosting framework, and a sophisticated classifier can be formed by aggregating the decision trees. Consequently, we can carry out structural data recognition with powerful recognition capability in the face of comprehensive structural variation. The experiments shows that the proposed method achieves impressive results and outperforms existing methods on datasets of IAM graph database repository. version:1
arxiv-1703-02660 | Towards Generalization and Simplicity in Continuous Control | http://arxiv.org/abs/1703.02660 | id:1703.02660 author:Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, Sham Kakade category:cs.LG cs.AI cs.RO cs.SY  published:2017-03-08 summary:This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of continuous control tasks, including the OpenAI gym benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, existing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory-centric policies. Training with a diverse initial state distribution is shown to produce more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the supplementary video. version:1
arxiv-1703-02647 | Streaming Weak Submodularity: Interpreting Neural Networks on the Fly | http://arxiv.org/abs/1703.02647 | id:1703.02647 author:Ethan R. Elenberg, Alexandros G. Dimakis, Moran Feldman, Amin Karbasi category:stat.ML cs.IT cs.LG math.IT  published:2017-03-08 summary:In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions, and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions $10$ times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016]. version:1
arxiv-1703-02645 | Cost-Optimal Learning of Causal Graphs | http://arxiv.org/abs/1703.02645 | id:1703.02645 author:Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath category:cs.AI cs.IT math.IT stat.ML  published:2017-03-08 summary:We consider the problem of learning a causal graph over a set of variables with interventions. We study the cost-optimal causal graph learning problem: For a given skeleton (undirected version of the causal graph), design the set of interventions with minimum total cost, that can uniquely identify any causal graph with the given skeleton. We show that this problem is solvable in polynomial time. Later, we consider the case when the number of interventions is limited. For this case, we provide polynomial time algorithms when the skeleton is a tree or a clique tree. For a general chordal skeleton, we develop an efficient greedy algorithm, which can be improved when the causal graph skeleton is an interval graph. version:1
arxiv-1703-02641 | Don't Fear the Bit Flips: Optimized Coding Strategies for Binary Classification | http://arxiv.org/abs/1703.02641 | id:1703.02641 author:Frederic Sala, Shahroze Kabir, Guy Van den Broeck, Lara Dolecek category:stat.ML cs.LG  published:2017-03-08 summary:After being trained, classifiers must often operate on data that has been corrupted by noise. In this paper, we consider the impact of such noise on the features of binary classifiers. Inspired by tools for classifier robustness, we introduce the same classification probability (SCP) to measure the resulting distortion on the classifier outputs. We introduce a low-complexity estimate of the SCP based on quantization and polynomial multiplication. We also study channel coding techniques based on replication error-correcting codes. In contrast to the traditional channel coding approach, where error-correction is meant to preserve the data and is agnostic to the application, our schemes specifically aim to maximize the SCP (equivalently minimizing the distortion of the classifier output) for the same redundancy overhead. version:1
arxiv-1702-07825 | Deep Voice: Real-time Neural Text-to-Speech | http://arxiv.org/abs/1702.07825 | id:1702.07825 author:Sercan O. Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yongguo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, Shubho Sengupta, Mohammad Shoeybi category:cs.CL cs.LG cs.NE cs.SD  published:2017-02-25 summary:We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations. version:2
arxiv-1703-02629 | Online Learning Without Prior Information | http://arxiv.org/abs/1703.02629 | id:1703.02629 author:Ashok Cutkosky, Kwabena Boahen category:cs.LG stat.ML  published:2017-03-07 summary:The vast majority of optimization and online learning algorithms today require some prior information about the data (often in the form of bounds on gradients or on the optimal parameter value). When this information is not available, these algorithms require laborious manual tuning of various hyperparameters, motivating the search for algorithms that can adapt to the data with no prior information. We describe a frontier of new lower bounds on the performance of such algorithms, reflecting a tradeoff between a term that depends on the optimal parameter value and a term that depends on the gradients' rate of growth. Further, we construct a family of algorithms whose performance matches any desired point on this frontier, which no previous algorithm reaches. version:1
arxiv-1703-02626 | Horde of Bandits using Gaussian Markov Random Fields | http://arxiv.org/abs/1703.02626 | id:1703.02626 author:Sharan Vaswani, Mark Schmidt, Laks V. S. Lakshmanan category:cs.LG  published:2017-03-07 summary:The gang of bandits (GOB) model \cite{cesa2013gang} is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph. This model is useful in problems like recommender systems where the large number of users makes it vital to transfer information between users. Despite its effectiveness, the existing GOB model can only be applied to small problems due to its quadratic time-dependence on the number of nodes. Existing solutions to combat the scalability issue require an often-unrealistic clustering assumption. By exploiting a connection to Gaussian Markov random fields (GMRFs), we show that the GOB model can be made to scale to much larger graphs without additional assumptions. In addition, we propose a Thompson sampling algorithm which uses the recent GMRF sampling-by-perturbation technique, allowing it to scale to even larger problems (leading to a "horde" of bandits). We give regret bounds and experimental results for GOB with Thompson sampling and epoch-greedy algorithms, indicating that these methods are as good as or significantly better than ignoring the graph or adopting a clustering-based approach. Finally, when an existing graph is not available, we propose a heuristic for learning it on the fly and show promising results. version:1
arxiv-1703-02624 | Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms | http://arxiv.org/abs/1703.02624 | id:1703.02624 author:Jialei Wang, Lin Xiao category:math.OC stat.ML  published:2017-03-07 summary:We consider empirical risk minimization of linear predictors with convex loss functions. Such problems can be reformulated as convex-concave saddle point problems, and thus are well suitable for primal-dual first-order algorithms. However, primal-dual algorithms often require explicit strongly convex regularization in order to obtain fast linear convergence, and the required dual proximal mapping may not admit closed-form or efficient solution. In this paper, we develop both batch and randomized primal-dual algorithms that can exploit strong convexity from data adaptively and are capable of achieving linear convergence even without regularization. We also present dual-free variants of the adaptive primal-dual algorithms that do not require computing the dual proximal mapping, which are especially suitable for logistic regression. version:1
arxiv-1703-02622 | Online Convex Optimization with Unconstrained Domains and Losses | http://arxiv.org/abs/1703.02622 | id:1703.02622 author:Ashok Cutkosky, Kwabena Boahen category:cs.LG stat.ML  published:2017-03-07 summary:We propose an online convex optimization algorithm (RescaledExp) that achieves optimal regret in the unconstrained setting without prior knowledge of any bounds on the loss functions. We prove a lower bound showing an exponential separation between the regret of existing algorithms that require a known bound on the loss functions and any algorithm that does not require such knowledge. RescaledExp matches this lower bound asymptotically in the number of iterations. RescaledExp is naturally hyperparameter-free and we demonstrate empirically that it matches prior optimization algorithms that require hyperparameter optimization. version:1
arxiv-1703-02620 | Linguistic Knowledge as Memory for Recurrent Neural Networks | http://arxiv.org/abs/1703.02620 | id:1703.02620 author:Bhuwan Dhingra, Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov category:cs.CL  published:2017-03-07 summary:Training recurrent neural networks to model long term dependencies is difficult. Hence, we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize. Specifically, external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements, and the resulting graph is decomposed into directed acyclic subgraphs. We introduce a model that encodes such graphs as explicit memory in recurrent neural networks, and use it to model coreference relations in text. We apply our model to several text comprehension tasks and achieve new state-of-the-art results on all considered benchmarks, including CNN, bAbi, and LAMBADA. On the bAbi QA tasks, our model solves 15 out of the 20 tasks with only 1000 training examples per task. Analysis of the learned representations further demonstrates the ability of our model to encode fine-grained entity information across a document. version:1
arxiv-1703-02596 | Customer Life Time Value Prediction Using Embeddings | http://arxiv.org/abs/1703.02596 | id:1703.02596 author:Benjamin Paul Chamberlain, Angelo Cardoso, C. H. Bryan Liu, Roberto Pagliari, Marc Peter Deisenroth category:cs.LG cs.CY cs.IR cs.NE stat.ML  published:2017-03-07 summary:We describe the Customer Life Time Value (CLTV) prediction system deployed at ASOS.com, a global online fashion retailer. CLTV prediction is an important problem in e-commerce where an accurate estimate of future value allows retailers to effectively allocate marketing spend, identify and nurture high value customers and mitigate exposure to losses. The system at ASOS provides daily estimates of the future value of every customer and is one of the cornerstones of the personalised shopping experience. The state of the art in this domain uses large numbers of handcrafted features and ensemble regressors to forecast value, predict churn and evaluate customer loyalty. We describe our system, which adopts this approach, and our ongoing efforts to further improve it. Recently, domains including language, vision and speech have shown dramatic advances by replacing handcrafted features with features that are learned automatically from data. We show that learning feature representations is a promising extension to the state of the art in CLTV modelling. We propose a novel way to generate embeddings of customers, which addresses the issue of the ever changing product catalogue and obtain a significant improvement over an exhaustive set of handcrafted features. version:1
arxiv-1703-02589 | Texture Classification of MR Images of the Brain in ALS using CoHOG | http://arxiv.org/abs/1703.02589 | id:1703.02589 author:G M Mashrur E Elahi, Sanjay Kalra, Yee-Hong Yang category:cs.CV  published:2017-03-07 summary:Texture analysis is a well-known research topic in computer vision and image processing and has many applications. Gradient-based texture methods have become popular in classification problems. For the first time we extend a well-known gradient-based method, Co-occurrence Histograms of Oriented Gradients (CoHOG) to extract texture features from 2D Magnetic Resonance Images (MRI). Unlike the original CoHOG method, we use the whole image instead of sub-regions for feature calculation. Also, we use a larger neighborhood size. Gradient orientations of the image pixels are calculated using Sobel, Gaussian Derivative (GD) and Local Frequency Descriptor Gradient (LFDG) operators. The extracted feature vector size is very large and classification using a large number of similar features does not provide the best results. In our proposed method, for the first time to our best knowledge, only a minimum number of significant features are selected using area under the receiver operator characteristic (ROC) curve (AUC) thresholds with <= 0.01. In this paper, we apply the proposed method to classify Amyotrophic Lateral Sclerosis (ALS) patients from the controls. It is observed that selected texture features from downsampled images are significantly different between patients and controls. These features are used in a linear support vector machine (SVM) classifier to determine the classification accuracy. Optimal sensitivity and specificity are also calculated. Three different cohort datasets are used in the experiments. The performance of the proposed method using three gradient operators and two different neighborhood sizes is analyzed. Region based analysis is performed to demonstrate that significant changes between patients and controls are limited to the motor cortex. version:1
arxiv-1703-02573 | Data Noising as Smoothing in Neural Network Language Models | http://arxiv.org/abs/1703.02573 | id:1703.02573 author:Ziang Xie, Sida I. Wang, Jiwei Li, Daniel Lévy, Aiming Nie, Dan Jurafsky, Andrew Y. Ng category:cs.LG cs.CL  published:2017-03-07 summary:Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in $n$-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing. version:1
arxiv-1703-02570 | Regularising Non-linear Models Using Feature Side-information | http://arxiv.org/abs/1703.02570 | id:1703.02570 author:Amina Mollaysa, Pablo Strasser, Alexandros Kalousis category:cs.LG stat.ML  published:2017-03-07 summary:Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature side-information is most often ignored or used only for feature selection prior to model fitting. We believe that feature side-information which carries information about features intrinsic property will help improve model prediction if used in a proper way during learning process. In this paper, we propose a framework that allows for the incorporation of the feature side-information during the learning of very general model families to improve the prediction performance. We control the structures of the learned models so that they reflect features similarities as these are defined on the basis of the side-information. We perform experiments on a number of benchmark datasets which show significant predictive performance gains, over a number of baselines, as a result of the exploitation of the side-information. version:1
arxiv-1703-02567 | Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions | http://arxiv.org/abs/1703.02567 | id:1703.02567 author:Sevi Baltaoglu, Lang Tong, Qing Zhao category:cs.GT cs.LG  published:2017-03-07 summary:We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his total T-period payoff, the bidder wants to determine the optimal allocation of his fixed budget among his bids for $K$ different goods at each period. As a bidding strategy, we propose a polynomial time algorithm, referred to as dynamic programming on discrete set (DPDS), which is inspired by the dynamic programming approach to Knapsack problems. We show that DPDS achieves the regret order of $O(\sqrt{T\log{T}})$. Also, by showing that the regret growth rate is lower bounded by $\Omega(\sqrt{T})$ for any bidding strategy, we conclude that DPDS algorithm is order optimal up to a $\sqrt{\log{T}}$ term. We also evaluate the performance of DPDS empirically in the context of virtual bidding in wholesale electricity markets by using historical data from the New York energy market. version:1
arxiv-1703-02563 | Optical Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation | http://arxiv.org/abs/1703.02563 | id:1703.02563 author:Christian Bailer, Bertram Taetz, Didier Stricker category:cs.CV I.4.8  published:2017-03-07 summary:Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier-prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this article we present a dense correspondence field approach that is much less outlier-prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach does not require explicit regularization, smoothing (like median filtering) or a new data term. Instead we solely rely on patch matching techniques and a novel multi-scale matching strategy. We also present enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than modern descriptor matching techniques. We do so by initializing EpicFlow with our approach instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. In this extended article of our former conference publication we further improve our approach in matching accuracy as well as runtime and present more experiments and insights. version:1
arxiv-1703-02528 | Stopping GAN Violence: Generative Unadversarial Networks | http://arxiv.org/abs/1703.02528 | id:1703.02528 author:Samuel Albanie, Sébastien Ehrhardt, João F. Henriques category:stat.ML cs.LG  published:2017-03-07 summary:While the costs of human violence have attracted a great deal of attention from the research community, the effects of the network-on-network (NoN) violence popularised by Generative Adversarial Networks have yet to be addressed. In this work, we quantify the financial, social, spiritual, cultural, grammatical and dermatological impact of this aggression and address the issue by proposing a more peaceful approach which we term Generative Unadversarial Networks (GUNs). Under this framework, we simultaneously train two models: a generator G that does its best to capture whichever data distribution it feels it can manage, and a motivator M that helps G to achieve its dream. Fighting is strictly verboten and both models evolve by learning to respect their differences. The framework is both theoretically and electrically grounded in game theory, and can be viewed as a winner-shares-all two-player game in which both players work as a team to achieve the best score. Experiments show that by working in harmony, the proposed model is able to claim both the moral and log-likelihood high ground. Our work builds on a rich history of carefully argued position-papers, published as anonymous YouTube comments, which prove that the optimal solution to NoN violence is more GUNs. version:1
arxiv-1703-02527 | Online Learning to Rank in Stochastic Click Models | http://arxiv.org/abs/1703.02527 | id:1703.02527 author:Mohammad Ghavamzadeh, Branislav Kveton, Csaba Szepesvari, Tomas Tunys, Zheng Wen, Masrour Zoghi category:cs.LG stat.ML  published:2017-03-07 summary:Online learning to rank is an important problem in machine learning, information retrieval, recommender systems, and display advertising. Many provably efficient algorithms have been developed for this problem recently, under specific click models. A click model is a model of how users click on a list of documents. Though these results are significant, the proposed algorithms have limited application because they are designed for specific click models, and do not have guarantees beyond them. To overcome this challenge, we propose a novel algorithm, which we call MergeRank, for learning to rank in a class of click models that satisfy mild technical assumptions. This class encompasses two most fundamental click models, the cascade and position-based models. We derive a gap-dependent upper bound on the expected $n$-step regret of MergeRank and evaluate it on web search queries. We observe that MergeRank performs better than ranked bandits and is more robust than CascadeKL-UCB, an existing algorithm for learning to rank in the cascade model. version:1
arxiv-1702-06457 | A Unified Optimization View on Generalized Matching Pursuit and Frank-Wolfe | http://arxiv.org/abs/1702.06457 | id:1702.06457 author:Francesco Locatello, Rajiv Khanna, Michael Tschannen, Martin Jaggi category:cs.LG stat.ML  published:2017-02-21 summary:Two of the most fundamental prototypes of greedy optimization are the matching pursuit and Frank-Wolfe algorithms. In this paper, we take a unified view on both classes of methods, leading to the first explicit convergence rates of matching pursuit methods in an optimization sense, for general sets of atoms. We derive sublinear ($1/t$) convergence for both classes on general smooth objectives, and linear convergence on strongly convex objectives, as well as a clear correspondence of algorithm variants. Our presented algorithms and rates are affine invariant, and do not need any incoherence or sparsity assumptions. version:2
arxiv-1703-01253 | Machine Learning on Sequential Data Using a Recurrent Weighted Average | http://arxiv.org/abs/1703.01253 | id:1703.01253 author:Jared Ostmeyer, Lindsay Cowell category:stat.ML cs.LG  published:2017-03-03 summary:Recurrent Neural Networks (RNN) are a type of statistical model designed to handle sequential data. The model reads a sequence one symbol at a time. Each symbol is processed based on information collected from the previous symbols. With existing RNN architectures, each symbol is processed using only information from the previous processing step. To overcome this limitation, we propose a new kind of RNN model that computes a recurrent weighted average (RWA) over every past processing step. Because the RWA can be computed as a running average, the computational overhead scales like that of any other RNN. The approach essentially reformulates the attention mechanism into a stand-alone model. When assessing a RWA model, it is found to train faster and generalize better than a standard LSTM model when performing the variable copy problem, the adding problem, classification of artificial grammar, classification of sequences by length, and classification of MNIST handwritten digits (where the pixels are read sequentially one at a time). version:2
arxiv-1703-02521 | Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos | http://arxiv.org/abs/1703.02521 | id:1703.02521 author:De-An Huang, Joseph J. Lim, Li Fei-Fei, Juan Carlos Niebles category:cs.CV  published:2017-03-07 summary:We propose an unsupervised method for reference resolution in instructional videos, where the goal is to temporally link an entity (e.g., "dressing") to the action (e.g., "mix yogurt") that produced it. The key challenge is the inevitable visual-linguistic ambiguities arising from the changes in both visual appearance and referring expression of an entity in the video. This challenge is amplified by the fact that we aim to resolve references with no supervision. We address these challenges by learning a joint visual-linguistic model, where linguistic cues can help resolve visual ambiguities and vice versa. We verify our approach by learning our model unsupervisedly using more than two thousand unstructured cooking videos from YouTube, and show that our visual-linguistic model can substantially improve upon state-of-the-art linguistic only model on reference resolution in instructional videos. version:1
arxiv-1703-02518 | Faster Coordinate Descent via Adaptive Importance Sampling | http://arxiv.org/abs/1703.02518 | id:1703.02518 author:Dmytro Perekrestenko, Volkan Cevher, Martin Jaggi category:cs.LG cs.CV math.OC stat.CO stat.ML G.1.6  published:2017-03-07 summary:Coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems. In this work, we introduce new adaptive rules for the random selection of their updates. By adaptive, we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration. We theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art, and extend our theory and algorithms to general convex objectives. Numerical evidence with hinge-loss support vector machines and Lasso confirm that the practice follows the theory. version:1
arxiv-1703-02517 | Learning opacity in Stratal Maximum Entropy Grammar | http://arxiv.org/abs/1703.02517 | id:1703.02517 author:Aleksei Nazarov, Joe Pater category:cs.CL  published:2017-03-07 summary:Opaque phonological patterns are sometimes claimed to be difficult to learn; specific hypotheses have been advanced about the relative difficulty of particular kinds of opaque processes (Kiparsky 1971, 1973), and the kind of data that will be helpful in learning an opaque pattern (Kiparsky 2000). In this paper, we present a computationally implemented learning theory for one grammatical theory of opacity: a Maximum Entropy version of Stratal OT (Berm\'udez-Otero 1999, Kiparsky 2000), and test it on simplified versions of opaque French tense-lax vowel alternations and the opaque interaction of diphthong raising and flapping in Canadian English. We find that the difficulty of opacity can be influenced by evidence for stratal affiliation: the Canadian English case is easier if the learner encounters application of raising outside the flapping context, or non-application of raising between words (i.e., <life> with a raised vowel; <lie for> with a non-raised vowel). version:1
arxiv-1703-02511 | Deep Learning for Automated Quality Assessment of Color Fundus Images in Diabetic Retinopathy Screening | http://arxiv.org/abs/1703.02511 | id:1703.02511 author:Sajib Kumar Saha, Basura Fernando, Jorge Cuadros, Di Xiao, Yogesan Kanagasingam category:cs.CV  published:2017-03-07 summary:Purpose To develop a computer based method for the automated assessment of image quality in the context of diabetic retinopathy (DR) to guide the photographer. Methods A deep learning framework was trained to grade the images automatically. A large representative set of 7000 color fundus images were used for the experiment which were obtained from the EyePACS that were made available by the California Healthcare Foundation. Three retinal image analysis experts were employed to categorize these images into Accept and Reject classes based on the precise definition of image quality in the context of DR. A deep learning framework was trained using 3428 images. Results A total of 3572 images were used for the evaluation of the proposed method. The method shows an accuracy of 100% to successfully categorise Accept and Reject images. Conclusion Image quality is an essential prerequisite for the grading of DR. In this paper we have proposed a deep learning based automated image quality assessment method in the context of DR. The method can be easily incorporated with the fundus image capturing system and thus can guide the photographer whether a recapture is necessary or not. version:1
arxiv-1703-02507 | Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features | http://arxiv.org/abs/1703.02507 | id:1703.02507 author:Matteo Pagliardini, Prakhar Gupta, Martin Jaggi category:cs.CL cs.AI cs.IR I.2.7  published:2017-03-07 summary:The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, and on many tasks even beats supervised models, highlighting the robustness of the produced sentence embeddings. version:1
arxiv-1703-02504 | Leveraging Large Amounts of Weakly Supervised Data for Multi-Language Sentiment Classification | http://arxiv.org/abs/1703.02504 | id:1703.02504 author:Jan Deriu, Aurelien Lucchi, Valeria De Luca, Aliaksei Severyn, Simon Müller, Mark Cieliebak, Thomas Hofmann, Martin Jaggi category:cs.CL cs.IR cs.LG I.2.7  published:2017-03-07 summary:This paper presents a novel approach for multi-lingual sentiment classification in short texts. This is a challenging task as the amount of training data in languages other than English is very limited. Previously proposed multi-lingual approaches typically require to establish a correspondence to English for which powerful classifiers are already available. In contrast, our method does not require such supervision. We leverage large amounts of weakly-supervised data in various languages to train a multi-layer convolutional network and demonstrate the importance of using pre-training of such networks. We thoroughly evaluate our approach on various multi-lingual datasets, including the recent SemEval-2016 sentiment prediction benchmark (Task 4), where we achieved state-of-the-art performance. We also compare the performance of our model trained individually for each language to a variant trained for all languages at once. We show that the latter model reaches slightly worse - but still acceptable - performance when compared to the single language model, while benefiting from better generalization properties across languages. version:1
arxiv-1703-02492 | Online Multilinear Dictionary Learning for Sequential Compressive Sensing | http://arxiv.org/abs/1703.02492 | id:1703.02492 author:Thiernithi Variddhisaï, Danilo Mandic category:cs.LG stat.ML  published:2017-03-07 summary:A method for online tensor dictionary learning is proposed. With the assumption of separable dictionaries, tensor contraction is used to diminish a $N$-way model of $\mathcal{O}\left(L^N\right)$ into a simple matrix equation of $\mathcal{O}\left(NL^2\right)$ with a real-time capability. To avoid numerical instability due to inversion of sparse matrix, a class of stochastic gradient with memory is formulated via a least-square solution to guarantee convergence and robustness. Both gradient descent with exact line search and Newton's method are discussed and realized. Extensions onto how to deal with bad initialization and outliers are also explained in detail. Experiments on two synthetic signals confirms an impressive performance of our proposed method. version:1
arxiv-1703-02458 | End-to-End Prediction of Buffer Overruns from Raw Source Code via Neural Memory Networks | http://arxiv.org/abs/1703.02458 | id:1703.02458 author:Min-je Choi, Sehun Jeong, Hakjoo Oh, Jaegul Choo category:cs.SE cs.NE  published:2017-03-07 summary:Detecting buffer overruns from a source code is one of the most common and yet challenging tasks in program analysis. Current approaches have mainly relied on rigid rules and handcrafted features devised by a few experts, limiting themselves in terms of flexible applicability and robustness due to diverse bug patterns and characteristics existing in sophisticated real-world software programs. In this paper, we propose a novel, data-driven approach that is completely end-to-end without requiring any hand-crafted features, thus free from any program language-specific structural limitations. In particular, our approach leverages a recently proposed neural network model called memory networks that have shown the state-of-the-art performances mainly in question-answering tasks. Our experimental results using source codes demonstrate that our proposed model is capable of accurately detecting simple buffer overruns. We also present in-depth analyses on how a memory network can learn to understand the semantics in programming languages solely from raw source codes, such as tracing variables of interest, identifying numerical values, and performing their quantitative comparisons. version:1
arxiv-1703-02445 | Object classification in images of Neoclassical furniture using Deep Learning | http://arxiv.org/abs/1703.02445 | id:1703.02445 author:Bernhard Bermeitinger, André Freitas, Simon Donig, Siegfried Handschuh category:cs.CV  published:2017-03-07 summary:This short paper outlines research results on object classification in images of Neoclassical furniture. The motivation was to provide an object recognition framework which is able to support the alignment of furniture images with a symbolic level model. A data-driven bottom-up research routine in the Neoclassica research framework is the main use-case. It strives to deliver tools for analyzing the spread of aesthetic forms which are considered as a cultural transfer process. version:1
arxiv-1703-02437 | PathTrack: Fast Trajectory Annotation with Path Supervision | http://arxiv.org/abs/1703.02437 | id:1703.02437 author:Santiago Manen, Michael Gygli, Dengxin Dai, Luc Van Gool category:cs.CV cs.LG cs.MM  published:2017-03-07 summary:Progress in Multiple Object Tracking (MOT) has been limited by the size of the available datasets. We present an efficient framework to annotate trajectories and use it to produce a MOT dataset of unprecedented size. A novel path supervision paradigm lets the annotator loosely track the object with a cursor while watching the video. This results in a path annotation for each object in the sequence. These path annotations, together with object detections, are fed into a two-step optimization to produce full bounding-box trajectories. Our experiments on existing datasets prove that our framework produces more accurate annotations than the state of the art and this in a fraction of the time. We further validate our approach by generating the PathTrack dataset, with more than 15,000 person trajectories in 720 sequences. We believe tracking approaches can benefit from a larger dataset like this one, just as was the case in object recognition. We show its potential by using it to retrain an off-the-shelf person matching network, originally trained on the MOT15 dataset, almost halving the misclassification rate. Additionally, training on our data consistently improves tracking results, both on our dataset and on MOT15. In the latter, where we improve the top-performing tracker (NOMT) dropping the number of ID Switches by 18% and fragments by 5%. version:1
arxiv-1703-02433 | An investigation into machine learning approaches for forecasting spatio-temporal demand in ride-hailing service | http://arxiv.org/abs/1703.02433 | id:1703.02433 author:Ismaïl Saadi, Melvin Wong, Bilal Farooq, Jacques Teller, Mario Cools category:cs.LG stat.ML  published:2017-03-07 summary:In this paper, we present machine learning approaches for characterizing and forecasting the short-term demand for on-demand ride-hailing services. We propose the spatio-temporal estimation of the demand that is a function of variable effects related to traffic, pricing and weather conditions. With respect to the methodology, a single decision tree, bootstrap-aggregated (bagged) decision trees, random forest, boosted decision trees, and artificial neural network for regression have been adapted and systematically compared using various statistics, e.g. R-square, Root Mean Square Error (RMSE), and slope. To better assess the quality of the models, they have been tested on a real case study using the data of DiDi Chuxing, the main on-demand ride hailing service provider in China. In the current study, 199,584 time-slots describing the spatio-temporal ride-hailing demand has been extracted with an aggregated-time interval of 10 mins. All the methods are trained and validated on the basis of two independent samples from this dataset. The results revealed that boosted decision trees provide the best prediction accuracy (RMSE=16.41), while avoiding the risk of over-fitting, followed by artificial neural network (20.09), random forest (23.50), bagged decision trees (24.29) and single decision tree (33.55). version:1
arxiv-1703-02419 | Probabilistic learning of nonlinear dynamical systems using sequential Monte Carlo | http://arxiv.org/abs/1703.02419 | id:1703.02419 author:Thomas B. Schön, Andreas Svensson, Lawrence Murray, Fredrik Lindsten category:stat.CO cs.LG cs.SY  published:2017-03-07 summary:Probabilistic modeling provides the capability to represent and manipulate uncertainty in data, models, decisions and predictions. We are concerned with the problem of learning probabilistic models of dynamical systems from measured data. Specifically, we consider learning of probabilistic nonlinear state space models. There is no closed-form solution available for this problem, implying that we are forced to use approximations. In this tutorial we will provide a self-contained introduction to one of the state-of-the-art methods---the particle Metropolis-Hastings algorithm---which has proven to offer very practical approximations. This is a Monte Carlo based method, where the so-called particle filter is used to guide a Markov chain Monte Carlo method through the parameter space. One of the key merits of the particle Metropolis-Hastings method is that it is guaranteed to converge to the "true solution" under mild assumptions, despite being based on a practical implementation of a particle filter (i.e., using a finite number of particles). We will also provide a motivating numerical example illustrating the method which we have implemented in an in-house developed modeling language, serving the purpose of abstracting away the underlying mathematics of the Monte Carlo approximations from the user. This modeling language will open up the power of sophisticated Monte Carlo methods, including particle Metropolis-Hastings, to a large group of users without requiring them to know all the underlying mathematical details. version:1
arxiv-1703-02403 | On Structured Prediction Theory with Calibrated Convex Surrogate Losses | http://arxiv.org/abs/1703.02403 | id:1703.02403 author:Anton Osokin, Francis Bach, Simon Lacoste-Julien category:cs.LG stat.ML  published:2017-03-07 summary:We provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees. For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called "calibration function" relating the excess surrogate risk to the actual risk. In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for general structured prediction. version:1
arxiv-1703-02391 | Learning from Noisy Labels with Distillation | http://arxiv.org/abs/1703.02391 | id:1703.02391 author:Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, Jia Li category:cs.CV  published:2017-03-07 summary:The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, the label noises have been treated as statistical outliers, and approaches such as importance re-weighting and bootstrap have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multi-mode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use side information, including a small clean dataset and label relations in knowledge graph, to "hedge the risk" of learning from noisy labels. Furthermore, unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains. version:1
arxiv-1703-02850 | Pretata: predicting TATA binding proteins with novel features and dimensionality reduction strategy | http://arxiv.org/abs/1703.02850 | id:1703.02850 author:Quan Zou, Shixiang Wan, Ying Ju, Jijun Tang, Xiangxiang Zeng category:q-bio.QM cs.LG q-bio.BM  published:2017-03-07 summary:Background: It is necessary and essential to discovery protein function from the novel primary sequences. Wet lab experimental procedures are not only time-consuming, but also costly, so predicting protein structure and function reliably based only on amino acid sequence has significant value. TATA-binding protein (TBP) is a kind of DNA binding protein, which plays a key role in the transcription regulation. Our study proposed an automatic approach for identifying TATA-binding proteins efficiently, accurately, and conveniently. This method would guide for the special protein identification with computational intelligence strategies. Results: Firstly, we proposed novel fingerprint features for TBP based on pseudo amino acid composition, physicochemical properties, and secondary structure. Secondly, hierarchical features dimensionality reduction strategies were employed to improve the performance furthermore. Currently, Pretata achieves 92.92% TATA- binding protein prediction accuracy, which is better than all other existing methods. Conclusions: The experiments demonstrate that our method could greatly improve the prediction accuracy and speed, thus allowing large-scale NGS data prediction to be practical. A web server is developed to facilitate the other researchers, which can be accessed at http://server.malab.cn/preTata/. version:1
arxiv-1703-02375 | Graph sketching-based Massive Data Clustering | http://arxiv.org/abs/1703.02375 | id:1703.02375 author:Anne Morvan, Krzysztof Choromanski, Cédric Gouy-Pailler, Jamal Atif category:cs.LG cs.DS  published:2017-03-07 summary:In this paper, we address the problem of recovering arbitrary-shaped data clusters from massive datasets. We present DBMSTClu a new density-based non-parametric method working on a limited number of linear measurements i.e. a sketched version of the similarity graph $G$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular structures. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. DBMSTClu as a graph-based technique relies on the similarity graph $G$ which costs theoretically $O(N^2)$ in memory. However, our algorithm follows the dynamic semi-streaming model by handling $G$ as a stream of edge weight updates and sketches it in one pass over the data into a compact structure requiring $O(\operatorname{poly} \operatorname{log} (N))$ space. Thanks to the property of the Minimum Spanning Tree (MST) for expressing the underlying structure of a graph, our algorithm successfully detects the right number of non-convex clusters by recovering an approximate MST from the graph sketch of $G$. We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets. version:1
arxiv-1703-02363 | Qualitative Assessment of Recurrent Human Motion | http://arxiv.org/abs/1703.02363 | id:1703.02363 author:Andre Ebert, Michael Till Beck, Andy Mattausch, Lenz Belzner, Claudia Linnhoff Popien category:cs.LG cs.CV  published:2017-03-07 summary:Smartphone applications designed to track human motion in combination with wearable sensors, e.g., during physical exercising, raised huge attention recently. Commonly, they provide quantitative services, such as personalized training instructions or the counting of distances. But qualitative monitoring and assessment is still missing, e.g., to detect malpositions, to prevent injuries, or to optimize training success. We address this issue by presenting a concept for qualitative as well as generic assessment of recurrent human motion by processing multi-dimensional, continuous time series tracked with motion sensors. Therefore, our segmentation procedure extracts individual events of specific length and we propose expressive features to accomplish a qualitative motion assessment by supervised classification. We verified our approach within a comprehensive study encompassing 27 athletes undertaking different body weight exercises. We are able to recognize six different exercise types with a success rate of 100% and to assess them qualitatively with an average success rate of 99.3%. version:1
arxiv-1703-01946 | Metric Learning for Generalizing Spatial Relations to New Objects | http://arxiv.org/abs/1703.01946 | id:1703.01946 author:Oier Mees, Nichola Abdo, Mladen Mazuran, Wolfram Burgard category:cs.RO cs.AI cs.LG  published:2017-03-06 summary:Human-centered environments are rich with a wide variety of spatial relations between everyday objects. For autonomous robots to operate effectively in such environments, they should be able to reason about these relations and generalize them to objects with different shapes and sizes. For example, having learned to place a toy inside a basket, a robot should be able to generalize this concept using a spoon and a cup. This requires a robot to have the flexibility to learn arbitrary relations in a lifelong manner, making it challenging for an expert to pre-program it with sufficient knowledge to do so beforehand. In this paper, we address the problem of learning spatial relations by introducing a novel method from the perspective of distance metric learning. Our approach enables a robot to reason about the similarity between pairwise spatial relations, thereby enabling it to use its previous knowledge when presented with a new relation to imitate. We show how this makes it possible to learn arbitrary spatial relations from non-expert users using a small number of examples and in an interactive manner. Our extensive evaluation with real-world data demonstrates the effectiveness of our method in reasoning about a continuous spectrum of spatial relations and generalizing them to new objects. version:2
arxiv-1703-02344 | Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce | http://arxiv.org/abs/1703.02344 | id:1703.02344 author:Devashish Shankar, Sujay Narumanchi, H A Ananya, Pramod Kompalli, Krishnendu Chaudhury category:cs.CV  published:2017-03-07 summary:In this paper, we present a unified end-to-end approach to build a large scale Visual Search and Recommendation system for e-commerce. Previous works have targeted these problems in isolation. We believe a more effective and elegant solution could be obtained by tackling them together. We propose a unified Deep Convolutional Neural Network architecture, called VisNet, to learn embeddings to capture the notion of visual similarity, across several semantic granularities. We demonstrate the superiority of our approach for the task of image retrieval, by comparing against the state-of-the-art on the Exact Street2Shop dataset. We then share the design decisions and trade-offs made while deploying the model to power Visual Recommendations across a catalog of 50M products, supporting 2K queries a second at Flipkart, India's largest e-commerce company. The deployment of our solution has yielded a significant business impact, as measured by the conversion-rate. version:1
arxiv-1703-02317 | Convolutional Recurrent Neural Networks for Bird Audio Detection | http://arxiv.org/abs/1703.02317 | id:1703.02317 author:EmreÇakır, Sharath Adavanne, Giambattista Parascandolo, Konstantinos Drossos, Tuomas Virtanen category:cs.SD cs.LG stat.ML  published:2017-03-07 summary:Bird sounds possess distinctive spectral structure which may exhibit small shifts in spectrum depending on the bird species and environmental conditions. In this paper, we propose using convolutional recurrent neural networks on the task of automated bird audio detection in real-life environments. In the proposed method, convolutional layers extract high dimensional, local frequency shift invariant features, while recurrent layers capture longer term dependencies between the features extracted from short time frames. This method achieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data and obtains the second place in the Bird Audio Detection challenge. version:1
arxiv-1703-02310 | Deep Robust Kalman Filter | http://arxiv.org/abs/1703.02310 | id:1703.02310 author:Shirli Di-Castro Shashua, Shie Mannor category:cs.AI cs.LG stat.ML  published:2017-03-07 summary:A Robust Markov Decision Process (RMDP) is a sequential decision making model that accounts for uncertainty in the parameters of dynamic systems. This uncertainty introduces difficulties in learning an optimal policy, especially for environments with large state spaces. We propose two algorithms, RTD-DQN and Deep-RoK, for solving large-scale RMDPs using nonlinear approximation schemes such as deep neural networks. The RTD-DQN algorithm incorporates the robust Bellman temporal difference error into a robust loss function, yielding robust policies for the agent. The Deep-RoK algorithm is a robust Bayesian method, based on the Extended Kalman Filter (EKF), that accounts for both the uncertainty in the weights of the approximated value function and the uncertainty in the transition probabilities, improving the robustness of the agent. We provide theoretical results for our approach and test the proposed algorithms on a continuous state domain. version:1
arxiv-1703-02898 | Large-scale image analysis using docker sandboxing | http://arxiv.org/abs/1703.02898 | id:1703.02898 author:B Sengupta, E Vazquez, M Sasdelli, Y Qian, M Peniak, L Netherton, G Delfino category:cs.CV cs.DC cs.IR cs.NE  published:2017-03-07 summary:With the advent of specialized hardware such as Graphics Processing Units (GPUs), large scale image localization, classification and retrieval have seen increased prevalence. Designing scalable software architecture that co-evolves with such specialized hardware is a challenge in the commercial setting. In this paper, we describe one such architecture (\textit{Cortexica}) that leverages scalability of GPUs and sandboxing offered by docker containers. This allows for the flexibility of mixing different computer architectures as well as computational algorithms with the security of a trusted environment. We illustrate the utility of this framework in a commercial setting i.e., searching for multiple products in an image by combining image localisation and retrieval. version:1
arxiv-1703-02291 | Triple Generative Adversarial Nets | http://arxiv.org/abs/1703.02291 | id:1703.02291 author:Chongxuan Li, Kun Xu, Jun Zhu, Bo Zhang category:cs.LG cs.CV  published:2017-03-07 summary:Generative adversarial nets (GANs) are good at generating realistic images and have been extended for semi-supervised classification. However, under a two-player formulation, existing work shares competing roles of identifying fake samples and predicting labels via a single discriminator network, which can lead to undesirable incompatibility. We present triple generative adversarial net (Triple-GAN), a flexible game-theoretical framework for classification and class-conditional generation in semi-supervised learning. Triple-GAN consists of three players - a generator, a discriminator and a classifier, where the generator and classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. With designed utilities, the distributions characterized by the classifier and generator both concentrate to the data distribution under nonparametric assumptions. We further propose unbiased regularization terms to make the classifier and generator strongly coupled and some biased techniques to boost the performance of Triple-GAN in practice. Our results on several datasets demonstrate the promise in semi-supervised learning, where Triple-GAN achieves comparable or superior performance than state-of-the-art classification results among DGMs; it is also able to disentangle the classes and styles and transfer smoothly on the data level via interpolation on the latent space class-conditionally. version:1
arxiv-1703-00832 | Face Image Reconstruction from Deep Templates | http://arxiv.org/abs/1703.00832 | id:1703.00832 author:Guangcan Mai, Kai Cao, Pong C. Yuen, Anil K. Jain category:cs.CV  published:2017-03-02 summary:State-of-the-art face recognition systems are based on deep (convolutional) neural networks. Therefore, it is imperative to determine to what extent face templates derived from deep networks can be inverted to obtain the original face image. In this paper, we discuss the vulnerabilities of a face recognition system based on deep templates, extracted by deep networks under image reconstruction attack. We propose a de-convolutional neural network (D-CNN) to reconstruct images of faces from their deep templates. In our experiments, we did not assume any knowledge about the target subject nor the deep network. To train the D-CNN reconstruction models, we augmented existing face datasets with a large collection of images synthesized using a face generator. The proposed reconstruction method was evaluated using type-I (comparing the reconstructed images against the original face images used to generate the deep template) and type-II (comparing the reconstructed images against a different face image of the same subject) attacks. We conducted a three-trial attack for each target face image using three face images reconstructed from three different D-CNNs. Each D-CNN was trained on a different dataset (VGG-Face, CASIA-Webface, or Multi-PIE). The type-I attack achieved a true accept rate (TAR) of 85.48% at a false accept rate (FAR) of 0.1% on the LFW dataset. The corresponding TAR for the type-II attack is 14.71%. Our experimental results demonstrate the need to secure deep templates in face recognition systems. version:2
arxiv-1703-02271 | X-ray Astronomical Point Sources Recognition Using Granular Binary-tree SVM | http://arxiv.org/abs/1703.02271 | id:1703.02271 author:Zhixian Ma, Weitian Li, Lei Wang, Haiguang Xu, Jie Zhu category:cs.CV  published:2017-03-07 summary:The study on point sources in astronomical images is of special importance, since most energetic celestial objects in the Universe exhibit a point-like appearance. An approach to recognize the point sources (PS) in the X-ray astronomical images using our newly designed granular binary-tree support vector machine (GBT-SVM) classifier is proposed. First, all potential point sources are located by peak detection on the image. The image and spectral features of these potential point sources are then extracted. Finally, a classifier to recognize the true point sources is build through the extracted features. Experiments and applications of our approach on real X-ray astronomical images are demonstrated. comparisons between our approach and other SVM-based classifiers are also carried out by evaluating the precision and recall rates, which prove that our approach is better and achieves a higher accuracy of around 89%. version:1
arxiv-1702-06135 | Enabling Multi-Source Neural Machine Translation By Concatenating Source Sentences In Multiple Languages | http://arxiv.org/abs/1702.06135 | id:1702.06135 author:Raj Dabre, Fabien Cromieres, Sadao Kurohashi category:cs.CL  published:2017-02-20 summary:In this paper, we propose a novel and elegant solution to "Multi-Source Neural Machine Translation" (MSNMT) which only relies on preprocessing a N-way multilingual corpus without modifying the Neural Machine Translation (NMT) architecture or training procedure. We simply concatenate the source sentences to form a single long multi-source input sentence while keeping the target side sentence as it is and train an NMT system using this augmented corpus. We evaluate our method in a low resource, general domain setting and show its effectiveness (+2 BLEU using 2 source languages and +6 BLEU using 5 source languages) along with some insights on how the NMT system leverages multilingual information in such a scenario by visualizing attention. version:2
arxiv-1703-02243 | SRN: Side-output Residual Network for Object Symmetry Detection in the Wild | http://arxiv.org/abs/1703.02243 | id:1703.02243 author:Wei Ke, Jie Chen, Jianbin Jiao, Guoying Zhao, Qixiang Ye category:cs.CV  published:2017-03-07 summary:In this paper, we establish a baseline for object symmetry detection in complex backgrounds by presenting a new benchmark and an end-to-end deep learning approach, opening up a promising direction for symmetry detection in the wild. The new benchmark, named Sym-PASCAL, spans challenges including object diversity, multi-objects, part-invisibility, and various complex backgrounds that are far beyond those in existing datasets. The proposed symmetry detection approach, named Side-output Residual Network (SRN), leverages output Residual Units (RUs) to fit the errors between the object symmetry groundtruth and the outputs of RUs. By stacking RUs in a deep-to-shallow manner, SRN exploits the 'flow' of errors among multiple scales to ease the problems of fitting complex outputs with limited layers, suppressing the complex backgrounds, and effectively matching object symmetry of different scales. Experimental results validate both the benchmark and its challenging aspects related to realworld images, and the state-of-the-art performance of our symmetry detection approach. The benchmark and the code for SRN are publicly available at https://github.com/KevinKecc/SRN. version:1
arxiv-1703-02242 | Shape DNA: Basic Generating Functions for Geometric Moment Invariants | http://arxiv.org/abs/1703.02242 | id:1703.02242 author:Erbo Li, Yazhou Huang, Dong Xu, Hua Li category:cs.CV  published:2017-03-07 summary:Geometric moment invariants (GMIs) have been widely used as basic tool in shape analysis and information retrieval. Their structure and characteristics determine efficiency and effectiveness. Two fundamental building blocks or generating functions (GFs) for invariants are discovered, which are dot product and vector product of point vectors in Euclidean space. The primitive invariants (PIs) can be derived by carefully selecting different products of GFs and calculating the corresponding multiple integrals, which translates polynomials of coordinates of point vectors into geometric moments. Then the invariants themselves are expressed in the form of product of moments. This procedure is just like DNA encoding proteins. All GMIs available in the literature can be decomposed into linear combinations of PIs. This paper shows that Hu's seven well known GMIs in computer vision have a more deep structure, which can be further divided into combination of simpler PIs. In practical uses, low order independent GMIs are of particular interest. In this paper, a set of PIs for similarity transformation and affine transformation in 2D are presented, which are simpler to use, and some of which are newly reported. The discovery of the two generating functions provides a new perspective of better understanding shapes in 2D and 3D Euclidean spaces, and the method proposed can be further extended to higher dimensional spaces and different manifolds, such as curves, surfaces and so on. version:1
arxiv-1703-02868 | Discriminative models for multi-instance problems with tree-structure | http://arxiv.org/abs/1703.02868 | id:1703.02868 author:Tomas Pevny, Petr Somol category:cs.CR cs.LG  published:2017-03-07 summary:Modeling network traffic is gaining importance in order to counter modern threats of ever increasing sophistication. It is though surprisingly difficult and costly to construct reliable classifiers on top of telemetry data due to the variety and complexity of signals that no human can manage to interpret in full. Obtaining training data with sufficiently large and variable body of labels can thus be seen as prohibitive problem. The goal of this work is to detect infected computers by observing their HTTP(S) traffic collected from network sensors, which are typically proxy servers or network firewalls, while relying on only minimal human input in model training phase. We propose a discriminative model that makes decisions based on all computer's traffic observed during predefined time window (5 minutes in our case). The model is trained on collected traffic samples over equally sized time window per large number of computers, where the only labels needed are human verdicts about the computer as a whole (presumed infected vs. presumed clean). As part of training the model itself recognizes discriminative patterns in traffic targeted to individual servers and constructs the final high-level classifier on top of them. We show the classifier to perform with very high precision, while the learned traffic patterns can be interpreted as Indicators of Compromise. In the following we implement the discriminative model as a neural network with special structure reflecting two stacked multi-instance problems. The main advantages of the proposed configuration include not only improved accuracy and ability to learn from gross labels, but also automatic learning of server types (together with their detectors) which are typically visited by infected computers. version:1
arxiv-1703-02217 | Removal of Salt and Pepper noise from Gray-Scale and Color Images: An Adaptive Approach | http://arxiv.org/abs/1703.02217 | id:1703.02217 author:Sujaya Kumar Sathua, Arabinda Dash, Aishwaryarani Behera category:cs.CV  published:2017-03-07 summary:An efficient adaptive algorithm for the removal of Salt and Pepper noise from gray scale and color image is presented in this paper. In this proposed method first a 3X3 window is taken and the central pixel of the window is considered as the processing pixel. If the processing pixel is found as uncorrupted, then it is left unchanged. And if the processing pixel is found corrupted one, then the window size is increased according to the conditions given in the proposed algorithm. Finally the processing pixel or the central pixel is replaced by either the mean, median or trimmed value of the elements in the current window depending upon different conditions of the algorithm. The proposed algorithm efficiently removes noise at all densities with better Peak Signal to Noise Ratio (PSNR) and Image Enhancement Factor (IEF). The proposed algorithm is compared with different existing algorithms like MF, AMF, MDBUTMF, MDBPTGMF and AWMF. version:1
arxiv-1702-08640 | Selective Video Cutout using Global Pyramid Models and Local Uncertainty Propagation | http://arxiv.org/abs/1702.08640 | id:1702.08640 author:Wenguan Wang, Jianbing Shen, Fatih Porikli category:cs.CV  published:2017-02-28 summary:Conventional video segmentation approaches rely heavily on appearance models. Such methods often use appearance descriptors that have limited discriminative power under complex scenarios. To improve the segmentation performance, this paper presents a pyramid histogram based confidence map that incorporates structure information into appearance statistics. It also combines geodesic distance based dynamic models. Then, it employs an efficient measure of uncertainty propagation using local classifiers to determine the image regions where the object labels might be ambiguous. The final foreground cutout is obtained by refining on the uncertain regions. Additionally, to reduce manual labeling, our method determines the frames to be labeled by the human operator in a principled manner, which further boosts the segmentation performance and minimizes the labeling effort. Our extensive experimental analyses on two big benchmarks demonstrate that our solution achieves superior performance, favorable computational efficiency, and reduced manual labeling in comparison to the state-of-the-art. version:2
arxiv-1703-01398 | Sparse Depth Sensing for Resource-Constrained Robots | http://arxiv.org/abs/1703.01398 | id:1703.01398 author:Fangchang Ma, Luca Carlone, Ulas Ayaz, Sertac Karaman category:cs.RO cs.CV  published:2017-03-04 summary:We consider the case in which a robot has to navigate in an unknown environment, but does not have enough on-board power or payload to carry a traditional depth sensor (e.g., a 3D lidar) and thus can only acquire a few (point- wise) depth measurements. Therefore, we address the following question: is it possible to reconstruct the geometry of an unknown environment using sparse and incomplete depth measurements? Reconstruction from incomplete data is not possible in general, but when the robot operates in man-made environments, the depth exhibits some regularity (e.g., many planar surfaces with few edges); we leverage this regularity to infer depth from a small number of measurements. Our first contribution is a formulation of the depth reconstruction problem that bridges robot perception with the compressive sensing literature in signal processing. The second contribution includes a set of formal results that ascertain the exactness and stability of the depth reconstruction in 2D and 3D problems, and completely characterize the geometry of the signals that we can reconstruct. Our third contribution is a set of practical algorithms for depth reconstruction: our formulation directly translates into algorithms for depth estimation based on convex programming. In real-world problems, these convex programs are very large and general-purpose solvers are relatively slow. For this reason, we discuss ad-hoc solvers that enable fast depth reconstruction in real problems. The last contribution is an extensive experimental evaluation in 2D and 3D problems, involving Monte Carlo runs on simulated instances and testing on multiple real datasets. Empirical results confirm that the proposed approach ensures accurate depth reconstruction, outperforms interpolation-based strategies, and performs well even when the assumption of structured environment is violated. version:2
arxiv-1703-02205 | Raw Waveform-based Speech Enhancement by Fully Convolutional Networks | http://arxiv.org/abs/1703.02205 | id:1703.02205 author:Szu-Wei Fu, Yu Tsao, Xugang Lu, Hisashi Kawai category:stat.ML cs.LG cs.SD  published:2017-03-07 summary:This paper proposes a fully convolutional network (FCN) model for raw waveform-based speech enhancement. The proposed system performs speech enhancement in an end-to-end (i.e. waveform-in and waveform-out) manner, which differs from most existing denoising methods that process the magnitude spectrum (e.g. log power spectrum (LPS)) only. Because the fully connected layers, which are involved in deep neural networks (DNN) and convolutional neural networks (CNN), may not accurately characterize local information of speech signals, especially for high-frequency components, we employed fully convolutional layers to model the waveform. More specifically, FCN only consists convolutional layers and hence the local temporal structures of speech signals can be efficiently and effectively preserved with a relatively small number of weights. Experimental results show that DNN and CNN based models have limited capability to restore high-frequency components of waveforms, thus leading to imperfect intelligibility of enhanced speech. On the other hand, the proposed FCN model can not only well recover the waveforms but also outperform the LPS-based DNN baseline in terms of STOI and PESQ. In addition, the number of model parameters in FCN is roughly only 0.2% compared with that in DNN and CNN. version:1
