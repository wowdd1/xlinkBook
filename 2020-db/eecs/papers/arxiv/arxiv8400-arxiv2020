arxiv-1501-04717 | Robust Face Recognition by Constrained Part-based Alignment | http://arxiv.org/abs/1501.04717 | id:1501.04717 author:Yuting Zhang, Kui Jia, Yueming Wang, Gang Pan, Tsung-Han Chan, Yi Ma category:cs.CV cs.LG  published:2015-01-20 summary:Developing a reliable and practical face recognition system is a long-standing goal in computer vision research. Existing literature suggests that pixel-wise face alignment is the key to achieve high-accuracy face recognition. By assuming a human face as piece-wise planar surfaces, where each surface corresponds to a facial part, we develop in this paper a Constrained Part-based Alignment (CPA) algorithm for face recognition across pose and/or expression. Our proposed algorithm is based on a trainable CPA model, which learns appearance evidence of individual parts and a tree-structured shape configuration among different parts. Given a probe face, CPA simultaneously aligns all its parts by fitting them to the appearance evidence with consideration of the constraint from the tree-structured shape configuration. This objective is formulated as a norm minimization problem regularized by graph likelihoods. CPA can be easily integrated with many existing classifiers to perform part-based face recognition. Extensive experiments on benchmark face datasets show that CPA outperforms or is on par with existing methods for robust face recognition across pose, expression, and/or illumination changes. version:1
arxiv-1501-04711 | DeepHash: Getting Regularization, Depth and Fine-Tuning Right | http://arxiv.org/abs/1501.04711 | id:1501.04711 author:Jie Lin, Olivier Morere, Vijay Chandrasekhar, Antoine Veillard, Hanlin Goh category:cs.CV cs.IR  published:2015-01-20 summary:This work focuses on representing very high-dimensional global image descriptors using very compact 64-1024 bit binary hashes for instance retrieval. We propose DeepHash: a hashing scheme based on deep networks. Key to making DeepHash work at extremely low bitrates are three important considerations -- regularization, depth and fine-tuning -- each requiring solutions specific to the hashing problem. In-depth evaluation shows that our scheme consistently outperforms state-of-the-art methods across all data sets for both Fisher Vectors and Deep Convolutional Neural Network features, by up to 20 percent over other schemes. The retrieval performance with 256-bit hashes is close to that of the uncompressed floating point features -- a remarkable 512 times compression. version:1
arxiv-1501-04267 | Comment on "Clustering by fast search and find of density peaks" | http://arxiv.org/abs/1501.04267 | id:1501.04267 author:Shuliang Wang, Dakui Wang, Caoyuan Li, Yan Li category:cs.LG  published:2015-01-18 summary:In [1], a clustering algorithm was given to find the centers of clusters quickly. However, the accuracy of this algorithm heavily depend on the threshold value of d-c. Furthermore, [1] has not provided any efficient way to select the threshold value of d-c, that is, one can have to estimate the value of d_c depend on one's subjective experience. In this paper, based on the data field [2], we propose a new way to automatically extract the threshold value of d_c from the original data set by using the potential entropy of data field. For any data set to be clustered, the most reasonable value of d_c can be objectively calculated from the data set by using our proposed method. The same experiments in [1] are redone with our proposed method on the same experimental data set used in [1], the results of which shows that the problem to calculate the threshold value of d_c in [1] has been solved by using our method. version:2
arxiv-1305-0319 | Learning Mixtures of Bernoulli Templates by Two-Round EM with Performance Guarantee | http://arxiv.org/abs/1305.0319 | id:1305.0319 author:Adrian Barbu, Tianfu Wu, Ying Nian Wu category:stat.ML  published:2013-05-02 summary:Dasgupta and Shulman showed that a two-round variant of the EM algorithm can learn mixture of Gaussian distributions with near optimal precision with high probability if the Gaussian distributions are well separated and if the dimension is sufficiently high. In this paper, we generalize their theory to learning mixture of high-dimensional Bernoulli templates. Each template is a binary vector, and a template generates examples by randomly switching its binary components independently with a certain probability. In computer vision applications, a binary vector is a feature map of an image, where each binary component indicates whether a local feature or structure is present or absent within a certain cell of the image domain. A Bernoulli template can be considered as a statistical model for images of objects (or parts of objects) from the same category. We show that the two-round EM algorithm can learn mixture of Bernoulli templates with near optimal precision with high probability, if the Bernoulli templates are sufficiently different and if the number of features is sufficiently high. We illustrate the theoretical results by synthetic and real examples. version:6
arxiv-1501-04690 | Naive-Deep Face Recognition: Touching the Limit of LFW Benchmark or Not? | http://arxiv.org/abs/1501.04690 | id:1501.04690 author:Erjin Zhou, Zhimin Cao, Qi Yin category:cs.CV  published:2015-01-20 summary:Face recognition performance improves rapidly with the recent deep learning technique developing and underlying large training dataset accumulating. In this paper, we report our observations on how big data impacts the recognition performance. According to these observations, we build our Megvii Face Recognition System, which achieves 99.50% accuracy on the LFW benchmark, outperforming the previous state-of-the-art. Furthermore, we report the performance in a real-world security certification scenario. There still exists a clear gap between machine recognition and human performance. We summarize our experiments and present three challenges lying ahead in recent face recognition. And we indicate several possible solutions towards these challenges. We hope our work will stimulate the community's discussion of the difference between research benchmark and real-world applications. version:1
arxiv-1501-04686 | Deep Convolutional Neural Networks for Action Recognition Using Depth Map Sequences | http://arxiv.org/abs/1501.04686 | id:1501.04686 author:Pichao Wang, Wanqing Li, Zhimin Gao, Jing Zhang, Chang Tang, Philip Ogunbona category:cs.CV  published:2015-01-20 summary:Recently, deep learning approach has achieved promising results in various fields of computer vision. In this paper, a new framework called Hierarchical Depth Motion Maps (HDMM) + 3 Channel Deep Convolutional Neural Networks (3ConvNets) is proposed for human action recognition using depth map sequences. Firstly, we rotate the original depth data in 3D pointclouds to mimic the rotation of cameras, so that our algorithms can handle view variant cases. Secondly, in order to effectively extract the body shape and motion information, we generate weighted depth motion maps (DMM) at several temporal scales, referred to as Hierarchical Depth Motion Maps (HDMM). Then, three channels of ConvNets are trained on the HDMMs from three projected orthogonal planes separately. The proposed algorithms are evaluated on MSRAction3D, MSRAction3DExt, UTKinect-Action and MSRDailyActivity3D datasets respectively. We also combine the last three datasets into a larger one (called Combined Dataset) and test the proposed method on it. The results show that our approach can achieve state-of-the-art results on the individual datasets and without dramatical performance degradation on the Combined Dataset. version:1
arxiv-1501-04621 | Sparse Bayesian Learning for EEG Source Localization | http://arxiv.org/abs/1501.04621 | id:1501.04621 author:Sajib Saha, Frank de Hoog, Ya. I. Nesterets, Rajib Rana, M. Tahtali, T. E. Gureyev category:q-bio.QM cs.LG q-bio.NC  published:2015-01-19 summary:Purpose: Localizing the sources of electrical activity from electroencephalographic (EEG) data has gained considerable attention over the last few years. In this paper, we propose an innovative source localization method for EEG, based on Sparse Bayesian Learning (SBL). Methods: To better specify the sparsity profile and to ensure efficient source localization, the proposed approach considers grouping of the electrical current dipoles inside human brain. SBL is used to solve the localization problem in addition with imposed constraint that the electric current dipoles associated with the brain activity are isotropic. Results: Numerical experiments are conducted on a realistic head model that is obtained by segmentation of MRI images of the head and includes four major components, namely the scalp, the skull, the cerebrospinal fluid (CSF) and the brain, with appropriate relative conductivity values. The results demonstrate that the isotropy constraint significantly improves the performance of SBL. In a noiseless environment, the proposed method was 1 found to accurately (with accuracy of >75%) locate up to 6 simultaneously active sources, whereas for SBL without the isotropy constraint, the accuracy of finding just 3 simultaneously active sources was <75%. Conclusions: Compared to the state-of-the-art algorithms, the proposed method is potentially more consistent in specifying the sparsity profile of human brain activity and is able to produce better source localization for EEG. version:1
arxiv-1501-04467 | Implementable confidence sets in high dimensional regression | http://arxiv.org/abs/1501.04467 | id:1501.04467 author:Alexandra Carpentier category:stat.ML  published:2015-01-19 summary:We consider the setting of linear regression in high dimension. We focus on the problem of constructing adaptive and honest confidence sets for the sparse parameter \theta, i.e. we want to construct a confidence set for theta that contains theta with high probability, and that is as small as possible. The l_2 diameter of a such confidence set should depend on the sparsity S of \theta - the larger S, the wider the confidence set. However, in practice, S is unknown. This paper focuses on constructing a confidence set for \theta which contains \theta with high probability, whose diameter is adaptive to the unknown sparsity S, and which is implementable in practice. version:1
arxiv-1411-7494 | An Evolutionary Optimization Approach to Risk Parity Portfolio Selection | http://arxiv.org/abs/1411.7494 | id:1411.7494 author:Ronald Hochreiter category:q-fin.PM cs.NE  published:2014-11-27 summary:In this paper we present an evolutionary optimization approach to solve the risk parity portfolio selection problem. While there exist convex optimization approaches to solve this problem when long-only portfolios are considered, the optimization problem becomes non-trivial in the long-short case. To solve this problem, we propose a genetic algorithm as well as a local search heuristic. This algorithmic framework is able to compute solutions successfully. Numerical results using real-world data substantiate the practicability of the approach presented in this paper. version:2
arxiv-1501-04413 | Statistical-mechanical analysis of pre-training and fine tuning in deep learning | http://arxiv.org/abs/1501.04413 | id:1501.04413 author:Masayuki Ohzeki category:stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG  published:2015-01-19 summary:In this paper, we present a statistical-mechanical analysis of deep learning. We elucidate some of the essential components of deep learning---pre-training by unsupervised learning and fine tuning by supervised learning. We formulate the extraction of features from the training data as a margin criterion in a high-dimensional feature-vector space. The self-organized classifier is then supplied with small amounts of labelled data, as in deep learning. Although we employ a simple single-layer perceptron model, rather than directly analyzing a multi-layer neural network, we find a nontrivial phase transition that is dependent on the number of unlabelled data in the generalization error of the resultant classifier. In this sense, we evaluate the efficacy of the unsupervised learning component of deep learning. The analysis is performed by the replica method, which is a sophisticated tool in statistical mechanics. We validate our result in the manner of deep learning, using a simple iterative algorithm to learn the weight vector on the basis of belief propagation. version:1
arxiv-1501-04370 | Structure Learning in Bayesian Networks of Moderate Size by Efficient Sampling | http://arxiv.org/abs/1501.04370 | id:1501.04370 author:Ru He, Jin Tian, Huaiqing Wu category:cs.AI cs.LG stat.ML  published:2015-01-19 summary:We study the Bayesian model averaging approach to learning Bayesian network structures (DAGs) from data. We develop new algorithms including the first algorithm that is able to efficiently sample DAGs according to the exact structure posterior. The DAG samples can then be used to construct estimators for the posterior of any feature. We theoretically prove good properties of our estimators and empirically show that our estimators considerably outperform the estimators from the previous state-of-the-art methods. version:1
arxiv-1501-04367 | Reconstruction-free action inference from compressive imagers | http://arxiv.org/abs/1501.04367 | id:1501.04367 author:Kuldeep Kulkarni, Pavan Turaga category:cs.CV  published:2015-01-18 summary:Persistent surveillance from camera networks, such as at parking lots, UAVs, etc., often results in large amounts of video data, resulting in significant challenges for inference in terms of storage, communication and computation. Compressive cameras have emerged as a potential solution to deal with the data deluge issues in such applications. However, inference tasks such as action recognition require high quality features which implies reconstructing the original video data. Much work in compressive sensing (CS) theory is geared towards solving the reconstruction problem, where state-of-the-art methods are computationally intensive and provide low-quality results at high compression rates. Thus, reconstruction-free methods for inference are much desired. In this paper, we propose reconstruction-free methods for action recognition from compressive cameras at high compression ratios of 100 and above. Recognizing actions directly from CS measurements requires features which are mostly nonlinear and thus not easily applicable. This leads us to search for such properties that are preserved in compressive measurements. To this end, we propose the use of spatio-temporal smashed filters, which are compressive domain versions of pixel-domain matched filters. We conduct experiments on publicly available databases and show that one can obtain recognition rates that are comparable to the oracle method in uncompressed setup, even for high compression ratios. version:1
arxiv-1501-04346 | Mathematical Language Processing: Automatic Grading and Feedback for Open Response Mathematical Questions | http://arxiv.org/abs/1501.04346 | id:1501.04346 author:Andrew S. Lan, Divyanshu Vats, Andrew E. Waters, Richard G. Baraniuk category:stat.ML cs.AI cs.CL cs.LG  published:2015-01-18 summary:While computer and communication technologies have provided effective means to scale up many aspects of education, the submission and grading of assessments such as homework assignments and tests remains a weak link. In this paper, we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in STEM (science, technology, engineering, and mathematics) courses. Our data-driven framework for mathematical language processing (MLP) leverages solution data from a large number of learners to evaluate the correctness of their solutions, assign partial-credit scores, and provide feedback to each learner on the likely locations of any errors. MLP takes inspiration from the success of natural language processing for text data and comprises three main steps. First, we convert each solution to an open response mathematical question into a series of numerical features. Second, we cluster the features from several solutions to uncover the structures of correct, partially correct, and incorrect solutions. We develop two different clustering approaches, one that leverages generic clustering algorithms and one based on Bayesian nonparametrics. Third, we automatically grade the remaining (potentially large number of) solutions based on their assigned cluster and one instructor-provided grade per cluster. As a bonus, we can track the cluster assignment of each step of a multistep solution and determine when it departs from a cluster of correct solutions, which enables us to indicate the likely locations of errors to learners. We test and validate MLP on real-world MOOC data to demonstrate how it can substantially reduce the human effort required in large-scale educational platforms. version:1
arxiv-1501-04325 | Deep Belief Nets for Topic Modeling | http://arxiv.org/abs/1501.04325 | id:1501.04325 author:Lars Maaloe, Morten Arngren, Ole Winther category:cs.CL cs.LG stat.ML  published:2015-01-18 summary:Applying traditional collaborative filtering to digital publishing is challenging because user data is very sparse due to the high volume of documents relative to the number of users. Content based approaches, on the other hand, is attractive because textual content is often very informative. In this paper we describe large-scale content based collaborative filtering for digital publishing. To solve the digital publishing recommender problem we compare two approaches: latent Dirichlet allocation (LDA) and deep belief nets (DBN) that both find low-dimensional latent representations for documents. Efficient retrieval can be carried out in the latent representation. We work both on public benchmarks and digital media content provided by Issuu, an online publishing platform. This article also comes with a newly developed deep belief nets toolbox for topic modeling tailored towards performance evaluation of the DBN model and comparisons to the LDA model. version:1
arxiv-1501-04324 | Phrase Based Language Model For Statistical Machine Translation | http://arxiv.org/abs/1501.04324 | id:1501.04324 author:Jia Xu, Geliang Chen category:cs.CL  published:2015-01-18 summary:We consider phrase based Language Models (LM), which generalize the commonly used word level models. Similar concept on phrase based LMs appears in speech recognition, which is rather specialized and thus less suitable for machine translation (MT). In contrast to the dependency LM, we first introduce the exhaustive phrase-based LMs tailored for MT use. Preliminary experimental results show that our approach outperform word based LMs with the respect to perplexity and translation quality. version:1
arxiv-1501-04318 | A Generalized Affinity Propagation Clustering Algorithm for Nonspherical Cluster Discovery | http://arxiv.org/abs/1501.04318 | id:1501.04318 author:Teng Qiu, Yongjie Li category:cs.LG cs.CV stat.ML  published:2015-01-18 summary:Clustering analysis aims to discover the underlying clusters in the data points according to their similarities. It has wide applications ranging from bioinformatics to astronomy. Here, we proposed a Generalized Affinity Propagation (G-AP) clustering algorithm. Data points are first organized in a sparsely connected in-tree (IT) structure by a physically inspired strategy. Then, additional edges are added to the IT structure for those reachable nodes. This expanded structure is subsequently trimmed by affinity propagation method. Consequently, the underlying cluster structure, with separate clusters, emerges. In contrast to other IT-based methods, G-AP is fully automatic and takes as input the pairs of similarities between data points only. Unlike affinity propagation, G-AP is capable of discovering nonspherical clusters. version:1
arxiv-1501-04309 | Information Theory and its Relation to Machine Learning | http://arxiv.org/abs/1501.04309 | id:1501.04309 author:Bao-Gang Hu category:cs.IT cs.LG math.IT  published:2015-01-18 summary:In this position paper, I first describe a new perspective on machine learning (ML) by four basic problems (or levels), namely, "What to learn?", "How to learn?", "What to evaluate?", and "What to adjust?". The paper stresses more on the first level of "What to learn?", or "Learning Target Selection". Towards this primary problem within the four levels, I briefly review the existing studies about the connection between information theoretical learning (ITL [1]) and machine learning. A theorem is given on the relation between the empirically-defined similarity measure and information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection. version:1
arxiv-1501-04292 | Image classification by visual bag-of-words refinement and reduction | http://arxiv.org/abs/1501.04292 | id:1501.04292 author:Zhiwu Lu, Liwei Wang, Ji-Rong Wen category:cs.CV  published:2015-01-18 summary:This paper presents a new framework for visual bag-of-words (BOW) refinement and reduction to overcome the drawbacks associated with the visual BOW model which has been widely used for image classification. Although very influential in the literature, the traditional visual BOW model has two distinct drawbacks. Firstly, for efficiency purposes, the visual vocabulary is commonly constructed by directly clustering the low-level visual feature vectors extracted from local keypoints, without considering the high-level semantics of images. That is, the visual BOW model still suffers from the semantic gap, and thus may lead to significant performance degradation in more challenging tasks (e.g. social image classification). Secondly, typically thousands of visual words are generated to obtain better performance on a relatively large image dataset. Due to such large vocabulary size, the subsequent image classification may take sheer amount of time. To overcome the first drawback, we develop a graph-based method for visual BOW refinement by exploiting the tags (easy to access although noisy) of social images. More notably, for efficient image classification, we further reduce the refined visual BOW model to a much smaller size through semantic spectral clustering. Extensive experimental results show the promising performance of the proposed framework for visual BOW refinement and reduction. version:1
arxiv-1501-04284 | Pairwise Constraint Propagation on Multi-View Data | http://arxiv.org/abs/1501.04284 | id:1501.04284 author:Zhiwu Lu, Liwei Wang category:cs.CV cs.LG  published:2015-01-18 summary:This paper presents a graph-based learning approach to pairwise constraint propagation on multi-view data. Although pairwise constraint propagation has been studied extensively, pairwise constraints are usually defined over pairs of data points from a single view, i.e., only intra-view constraint propagation is considered for multi-view tasks. In fact, very little attention has been paid to inter-view constraint propagation, which is more challenging since pairwise constraints are now defined over pairs of data points from different views. In this paper, we propose to decompose the challenging inter-view constraint propagation problem into semi-supervised learning subproblems so that they can be efficiently solved based on graph-based label propagation. To the best of our knowledge, this is the first attempt to give an efficient solution to inter-view constraint propagation from a semi-supervised learning viewpoint. Moreover, since graph-based label propagation has been adopted for basic optimization, we develop two constrained graph construction methods for interview constraint propagation, which only differ in how the intra-view pairwise constraints are exploited. The experimental results in cross-view retrieval have shown the promising performance of our inter-view constraint propagation. version:1
arxiv-1501-04282 | Regularized maximum correntropy machine | http://arxiv.org/abs/1501.04282 | id:1501.04282 author:Jim Jing-Yan Wang, Yunji Wang, Bing-Yi Jing, Xin Gao category:cs.LG  published:2015-01-18 summary:In this paper we investigate the usage of regularized correntropy framework for learning of classifiers from noisy labels. The class label predictors learned by minimizing transitional loss functions are sensitive to the noisy and outlying labels of training samples, because the transitional loss functions are equally applied to all the samples. To solve this problem, we propose to learn the class label predictors by maximizing the correntropy between the predicted labels and the true labels of the training samples, under the regularized Maximum Correntropy Criteria (MCC) framework. Moreover, we regularize the predictor parameter to control the complexity of the predictor. The learning problem is formulated by an objective function considering the parameter regularization and MCC simultaneously. By optimizing the objective function alternately, we develop a novel predictor learning algorithm. The experiments on two chal- lenging pattern classification tasks show that it significantly outperforms the machines with transitional loss functions. version:1
arxiv-1501-04277 | Correntropy Induced L2 Graph for Robust Subspace Clustering | http://arxiv.org/abs/1501.04277 | id:1501.04277 author:Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, Zhouchen Lin category:cs.CV  published:2015-01-18 summary:In this paper, we study the robust subspace clustering problem, which aims to cluster the given possibly noisy data points into their underlying subspaces. A large pool of previous subspace clustering methods focus on the graph construction by different regularization of the representation coefficient. We instead focus on the robustness of the model to non-Gaussian noises. We propose a new robust clustering method by using the correntropy induced metric, which is robust for handling the non-Gaussian and impulsive noises. Also we further extend the method for handling the data with outlier rows/features. The multiplicative form of half-quadratic optimization is used to optimize the non-convex correntropy objective function of the proposed models. Extensive experiments on face datasets well demonstrate that the proposed methods are more robust to corruptions and occlusions. version:1
arxiv-1501-04276 | Correlation Adaptive Subspace Segmentation by Trace Lasso | http://arxiv.org/abs/1501.04276 | id:1501.04276 author:Canyi Lu, Jiashi Feng, Zhouchen Lin, Shuicheng Yan category:cs.CV  published:2015-01-18 summary:This paper studies the subspace segmentation problem. Given a set of data points drawn from a union of subspaces, the goal is to partition them into their underlying subspaces they were drawn from. The spectral clustering method is used as the framework. It requires to find an affinity matrix which is close to block diagonal, with nonzero entries corresponding to the data point pairs from the same subspace. In this work, we argue that both sparsity and the grouping effect are important for subspace segmentation. A sparse affinity matrix tends to be block diagonal, with less connections between data points from different subspaces. The grouping effect ensures that the highly corrected data which are usually from the same subspace can be grouped together. Sparse Subspace Clustering (SSC), by using $\ell^1$-minimization, encourages sparsity for data selection, but it lacks of the grouping effect. On the contrary, Low-Rank Representation (LRR), by rank minimization, and Least Squares Regression (LSR), by $\ell^2$-regularization, exhibit strong grouping effect, but they are short in subset selection. Thus the obtained affinity matrix is usually very sparse by SSC, yet very dense by LRR and LSR. In this work, we propose the Correlation Adaptive Subspace Segmentation (CASS) method by using trace Lasso. CASS is a data correlation dependent method which simultaneously performs automatic data selection and groups correlated data together. It can be regarded as a method which adaptively balances SSC and LSR. Both theoretical and experimental results show the effectiveness of CASS. version:1
arxiv-1501-01548 | Implementation of Auto Monitoring and Short-Message-Service System via GSM Modem | http://arxiv.org/abs/1501.01548 | id:1501.01548 author:Akilan Thangarajah, Buddhapala Wongkaew, Mongkol Ekpanyapong category:cs.CV  published:2015-01-07 summary:Auto-Monitoring and Short-Messaging-Service System is a real-time monitoring system for any critical operational environments. It detects an undesired event occurring in the environment, generates an alert with detailed message and sends it to the user to prevent hazards. This system employs a Friendly ARM as main controller while, sensors and terminals to interact with the real world. A GSM network is utilized to bridge the communication between monitoring system and user. This paper presents details of prototyping the system. version:2
arxiv-1501-04244 | Generalised Random Forest Space Overview | http://arxiv.org/abs/1501.04244 | id:1501.04244 author:Miron B. Kursa category:cs.LG  published:2015-01-17 summary:Assuming a view of the Random Forest as a special case of a nested ensemble of interchangeable modules, we construct a generalisation space allowing one to easily develop novel methods based on this algorithm. We discuss the role and required properties of modules at each level, especially in context of some already proposed RF generalisations. version:1
arxiv-1209-0841 | Constructing the L2-Graph for Robust Subspace Learning and Subspace Clustering | http://arxiv.org/abs/1209.0841 | id:1209.0841 author:Xi Peng, Zhiding Yu, Huajin Tang, Zhang Yi category:cs.CV cs.MM  published:2012-09-05 summary:Under the framework of graph-based learning, the key to robust subspace clustering and subspace learning is to obtain a good similarity graph that eliminates the effects of errors and retains only connections between the data points from the same subspace (i.e., intra-subspace data points). Recent works achieve good performance by modeling errors into their objective functions to remove the errors from the inputs. However, these approaches face the limitations that the structure of errors should be known prior and a complex convex problem must be solved. In this paper, we present a novel method to eliminate the effects of the errors from the projection space (representation) rather than from the input space. We first prove that $\ell_1$-, $\ell_2$-, $\ell_{\infty}$-, and nuclear-norm based linear projection spaces share the property of Intra-subspace Projection Dominance (IPD), i.e., the coefficients over intra-subspace data points are larger than those over inter-subspace data points. Based on this property, we introduce a method to construct a sparse similarity graph, called L2-Graph. The subspace clustering and subspace learning algorithms are developed upon L2-Graph. Experiments show that L2-Graph algorithms outperform the state-of-the-art methods for feature extraction, image clustering, and motion segmentation in terms of accuracy, robustness, and time efficiency. version:7
arxiv-1501-04163 | Meaningful Objects Segmentation from SAR Images via A Multi-Scale Non-Local Active Contour Model | http://arxiv.org/abs/1501.04163 | id:1501.04163 author:Gui-Song Xia, Gang Liu, Wen Yang category:cs.CV  published:2015-01-17 summary:The segmentation of synthetic aperture radar (SAR) images is a longstanding yet challenging task, not only because of the presence of speckle, but also due to the variations of surface backscattering properties in the images. Tremendous investigations have been made to eliminate the speckle effects for the segmentation of SAR images, while few work devotes to dealing with the variations of backscattering coefficients in the images. In order to overcome both the two difficulties, this paper presents a novel SAR image segmentation method by exploiting a multi-scale active contour model based on the non-local processing principle. More precisely, we first formulize the SAR segmentation problem with an active contour model by integrating the non-local interactions between pairs of patches inside and outside the segmented regions. Secondly, a multi-scale strategy is proposed to speed up the non-local active contour segmentation procedure and to avoid falling into local minimum for achieving more accurate segmentation results. Experimental results on simulated and real SAR images demonstrate the efficiency and feasibility of the proposed method: it can not only achieve precise segmentations for images with heavy speckles and non-local intensity variations, but also can be used for SAR images from different types of sensors. version:1
arxiv-1501-04140 | A Fast Fractal Image Compression Algorithm Using Predefined Values for Contrast Scaling | http://arxiv.org/abs/1501.04140 | id:1501.04140 author:H. Miar Naimi, M. Salarian category:cs.CV  published:2015-01-17 summary:In this paper a new fractal image compression algorithm is proposed in which the time of encoding process is considerably reduced. The algorithm exploits a domain pool reduction approach, along with using innovative predefined values for contrast scaling factor, S, instead of scanning the parameter space [0,1]. Within this approach only domain blocks with entropies greater than a threshold are considered. As a novel point, it is assumed that in each step of the encoding process, the domain block with small enough distance shall be found only for the range blocks with low activity (equivalently low entropy). This novel point is used to find reasonable estimations of S, and use them in the encoding process as predefined values, mentioned above. The algorithm has been examined for some well-known images. This result shows that our proposed algorithm considerably reduces the encoding time producing images that are approximately the same in quality. version:1
arxiv-1408-2590 | Multidimensional Digital Filters for Point-Target Detection in Cluttered Infrared Scenes | http://arxiv.org/abs/1408.2590 | id:1408.2590 author:Hugh L. Kennedy category:cs.CV  published:2014-08-12 summary:A 3-D spatiotemporal prediction-error filter (PEF), is used to enhance foreground/background contrast in (real and simulated) sensor image sequences. Relative velocity is utilized to extract point-targets that would otherwise be indistinguishable on spatial frequency alone. An optical-flow field is generated using local estimates of the 3-D autocorrelation function via the application of the fast Fourier transform (FFT) and inverse FFT. Velocity estimates are then used to tune in a background-whitening PEF that is matched to the motion and texture of the local background. Finite-impulse-response (FIR) filters are designed and implemented in the frequency domain. An analytical expression for the frequency response of velocity-tuned FIR filters, of odd or even dimension, with an arbitrary delay in each dimension, is derived. version:3
arxiv-1311-6834 | Semi-Supervised Sparse Coding | http://arxiv.org/abs/1311.6834 | id:1311.6834 author:Jim Jing-Yan Wang, Xin Gao category:stat.ML cs.LG  published:2013-11-26 summary:Sparse coding approximates the data sample as a sparse linear combination of some basic codewords and uses the sparse codes as new presentations. In this paper, we investigate learning discriminative sparse codes by sparse coding in a semi-supervised manner, where only a few training samples are labeled. By using the manifold structure spanned by the data set of both labeled and unlabeled samples and the constraints provided by the labels of the labeled samples, we learn the variable class labels for all the samples. Furthermore, to improve the discriminative ability of the learned sparse codes, we assume that the class labels could be predicted from the sparse codes directly using a linear classifier. By solving the codebook, sparse codes, class labels and classifier parameters simultaneously in a unified objective function, we develop a semi-supervised sparse coding algorithm. Experiments on two real-world pattern recognition problems demonstrate the advantage of the proposed methods over supervised sparse coding methods on partially labeled data sets. version:2
arxiv-1310-5715 | Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm | http://arxiv.org/abs/1310.5715 | id:1310.5715 author:Deanna Needell, Nathan Srebro, Rachel Ward category:math.NA cs.CV cs.LG math.OC stat.ML  published:2013-10-21 summary:We obtain an improved finite-sample guarantee on the linear convergence of stochastic gradient descent for smooth and strongly convex objectives, improving from a quadratic dependence on the conditioning $(L/\mu)^2$ (where $L$ is a bound on the smoothness and $\mu$ on the strong convexity) to a linear dependence on $L/\mu$. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence in the average smoothness, dominating previous results. We also discuss importance sampling for SGD more broadly and show how it can improve convergence also in other scenarios. Our results are based on a connection we make between SGD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods. In particular, we recast the randomized Kaczmarz algorithm as an instance of SGD, and apply our results to prove its exponential convergence, but to the solution of a weighted least squares problem rather than the original least squares problem. We then present a modified Kaczmarz algorithm with partially biased sampling which does converge to the original least squares solution with the same exponential convergence rate. version:5
arxiv-1501-04010 | Coevolutionary intransitivity in games: A landscape analysis | http://arxiv.org/abs/1501.04010 | id:1501.04010 author:Hendrik Richter category:cs.NE q-bio.PE  published:2015-01-16 summary:Intransitivity is supposed to be a main reason for deficits in coevolutionary progress and inheritable superiority. Besides, coevolutionary dynamics is characterized by interactions yielding subjective fitness, but aiming at solutions that are superior with respect to an objective measurement. Such an approximation of objective fitness may be, for instance, generalization performance. In the paper a link between rating-- and ranking--based measures of intransitivity and fitness landscapes that can address the dichotomy between subjective and objective fitness is explored. The approach is illustrated by numerical experiments involving a simple random game with continuously tunable degree of randomness. version:1
arxiv-1501-03975 | Stochastic Gradient Based Extreme Learning Machines For Online Learning of Advanced Combustion Engines | http://arxiv.org/abs/1501.03975 | id:1501.03975 author:Vijay Manikandan Janakiraman, XuanLong Nguyen, Dennis Assanis category:cs.NE cs.LG cs.SY  published:2015-01-16 summary:In this article, a stochastic gradient based online learning algorithm for Extreme Learning Machines (ELM) is developed (SG-ELM). A stability criterion based on Lyapunov approach is used to prove both asymptotic stability of estimation error and stability in the estimated parameters suitable for identification of nonlinear dynamic systems. The developed algorithm not only guarantees stability, but also reduces the computational demand compared to the OS-ELM approach based on recursive least squares. In order to demonstrate the effectiveness of the algorithm on a real-world scenario, an advanced combustion engine identification problem is considered. The algorithm is applied to two case studies: An online regression learning for system identification of a Homogeneous Charge Compression Ignition (HCCI) Engine and an online classification learning (with class imbalance) for identifying the dynamic operating envelope of the HCCI Engine. The results indicate that the accuracy of the proposed SG-ELM is comparable to that of the state-of-the-art but adds stability and a reduction in computational effort. version:1
arxiv-1501-03969 | Nonlinear Model Predictive Control of A Gasoline HCCI Engine Using Extreme Learning Machines | http://arxiv.org/abs/1501.03969 | id:1501.03969 author:Vijay Manikandan Janakiraman, XuanLong Nguyen, Dennis Assanis category:cs.SY cs.NE  published:2015-01-16 summary:Homogeneous charge compression ignition (HCCI) is a futuristic combustion technology that operates with a high fuel efficiency and reduced emissions. HCCI combustion is characterized by complex nonlinear dynamics which necessitates a model based control approach for automotive application. HCCI engine control is a nonlinear, multi-input multi-output problem with state and actuator constraints which makes controller design a challenging task. Typical HCCI controllers make use of a first principles based model which involves a long development time and cost associated with expert labor and calibration. In this paper, an alternative approach based on machine learning is presented using extreme learning machines (ELM) and nonlinear model predictive control (MPC). A recurrent ELM is used to learn the nonlinear dynamics of HCCI engine using experimental data and is shown to accurately predict the engine behavior several steps ahead in time, suitable for predictive control. Using the ELM engine models, an MPC based control algorithm with a simplified quadratic program update is derived for real time implementation. The working and effectiveness of the MPC approach has been analyzed on a nonlinear HCCI engine model for tracking multiple reference quantities along with constraints defined by HCCI states, actuators and operational limits. version:1
arxiv-1501-03959 | Value Iteration with Options and State Aggregation | http://arxiv.org/abs/1501.03959 | id:1501.03959 author:Kamil Ciosek, David Silver category:cs.AI cs.LG stat.ML  published:2015-01-16 summary:This paper presents a way of solving Markov Decision Processes that combines state abstraction and temporal abstraction. Specifically, we combine state aggregation with the options framework and demonstrate that they work well together and indeed it is only after one combines the two that the full benefit of each is realized. We introduce a hierarchical value iteration algorithm where we first coarsely solve subgoals and then use these approximate solutions to exactly solve the MDP. This algorithm solved several problems faster than vanilla value iteration. version:1
arxiv-1501-03952 | Mind the Gap: Subspace based Hierarchical Domain Adaptation | http://arxiv.org/abs/1501.03952 | id:1501.03952 author:Anant Raj, Vinay P. Namboodiri, Tinne Tuytelaars category:cs.CV  published:2015-01-16 summary:Domain adaptation techniques aim at adapting a classifier learnt on a source domain to work on the target domain. Exploiting the subspaces spanned by features of the source and target domains respectively is one approach that has been investigated towards solving this problem. These techniques normally assume the existence of a single subspace for the entire source / target domain. In this work, we consider the hierarchical organization of the data and consider multiple subspaces for the source and target domain based on the hierarchy. We evaluate different subspace based domain adaptation techniques under this setting and observe that using different subspaces based on the hierarchy yields consistent improvement over a non-hierarchical baseline version:1
arxiv-1501-03786 | Multi-view learning for multivariate performance measures optimization | http://arxiv.org/abs/1501.03786 | id:1501.03786 author:Jim Jing-Yan Wang category:cs.LG  published:2015-01-15 summary:In this paper, we propose the problem of optimizing multivariate performance measures from multi-view data, and an effective method to solve it. This problem has two features: the data points are presented by multiple views, and the target of learning is to optimize complex multivariate performance measures. We propose to learn a linear discriminant functions for each view, and combine them to construct a overall multivariate mapping function for mult-view data. To learn the parameters of the linear dis- criminant functions of different views to optimize multivariate performance measures, we formulate a optimization problem. In this problem, we propose to minimize the complexity of the linear discriminant functions of each view, encourage the consistences of the responses of different views over the same data points, and minimize the upper boundary of a given multivariate performance measure. To optimize this problem, we employ the cutting-plane method in an iterative algorithm. In each iteration, we update a set of constrains, and optimize the mapping function parameter of each view one by one. version:2
arxiv-1501-03915 | Feature Selection based on Machine Learning in MRIs for Hippocampal Segmentation | http://arxiv.org/abs/1501.03915 | id:1501.03915 author:Sabina Tangaro, Nicola Amoroso, Massimo Brescia, Stefano Cavuoti, Andrea Chincarini, Rosangela Errico, Paolo Inglese, Giuseppe Longo, Rosalia Maglietta, Andrea Tateo, Giuseppe Riccio, Roberto Bellotti category:physics.med-ph cs.CV cs.LG  published:2015-01-16 summary:Neurodegenerative diseases are frequently associated with structural changes in the brain. Magnetic Resonance Imaging (MRI) scans can show these variations and therefore be used as a supportive feature for a number of neurodegenerative diseases. The hippocampus has been known to be a biomarker for Alzheimer disease and other neurological and psychiatric diseases. However, it requires accurate, robust and reproducible delineation of hippocampal structures. Fully automatic methods are usually the voxel based approach, for each voxel a number of local features were calculated. In this paper we compared four different techniques for feature selection from a set of 315 features extracted for each voxel: (i) filter method based on the Kolmogorov-Smirnov test; two wrapper methods, respectively, (ii) Sequential Forward Selection and (iii) Sequential Backward Elimination; and (iv) embedded method based on the Random Forest Classifier on a set of 10 T1-weighted brain MRIs and tested on an independent set of 25 subjects. The resulting segmentations were compared with manual reference labelling. By using only 23 features for each voxel (sequential backward elimination) we obtained comparable state of-the-art performances with respect to the standard tool FreeSurfer. version:1
arxiv-1106-3703 | Prediction and Modularity in Dynamical Systems | http://arxiv.org/abs/1106.3703 | id:1106.3703 author:Artemy Kolchinsky, Luis M. Rocha category:nlin.AO cs.AI cs.IT cs.LG cs.SY math.IT q-bio.QM stat.ME G.3  published:2011-06-19 summary:Identifying and understanding modular organizations is centrally important in the study of complex systems. Several approaches to this problem have been advanced, many framed in information-theoretic terms. Our treatment starts from the complementary point of view of statistical modeling and prediction of dynamical systems. It is known that for finite amounts of training data, simpler models can have greater predictive power than more complex ones. We use the trade-off between model simplicity and predictive accuracy to generate optimal multiscale decompositions of dynamical networks into weakly-coupled, simple modules. State-dependent and causal versions of our method are also proposed. version:2
arxiv-1501-03879 | A new ADMM algorithm for the Euclidean median and its application to robust patch regression | http://arxiv.org/abs/1501.03879 | id:1501.03879 author:Kunal N. Chaudhury, K. R. Ramakrishnan category:cs.CV  published:2015-01-16 summary:The Euclidean Median (EM) of a set of points $\Omega$ in an Euclidean space is the point x minimizing the (weighted) sum of the Euclidean distances of x to the points in $\Omega$. While there exits no closed-form expression for the EM, it can nevertheless be computed using iterative methods such as the Wieszfeld algorithm. The EM has classically been used as a robust estimator of centrality for multivariate data. It was recently demonstrated that the EM can be used to perform robust patch-based denoising of images by generalizing the popular Non-Local Means algorithm. In this paper, we propose a novel algorithm for computing the EM (and its box-constrained counterpart) using variable splitting and the method of augmented Lagrangian. The attractive feature of this approach is that the subproblems involved in the ADMM-based optimization of the augmented Lagrangian can be resolved using simple closed-form projections. The proposed ADMM solver is used for robust patch-based image denoising and is shown to exhibit faster convergence compared to an existing solver. version:1
arxiv-1501-03861 | Bayesian Nonparametrics in Topic Modeling: A Brief Tutorial | http://arxiv.org/abs/1501.03861 | id:1501.03861 author:Alexander Spangher category:stat.ML  published:2015-01-16 summary:Using nonparametric methods has been increasingly explored in Bayesian hierarchical modeling as a way to increase model flexibility. Although the field shows a lot of promise, inference in many models, including Hierachical Dirichlet Processes (HDP), remain prohibitively slow. One promising path forward is to exploit the submodularity inherent in Indian Buffet Process (IBP) to derive near-optimal solutions in polynomial time. In this work, I will present a brief tutorial on Bayesian nonparametric methods, especially as they are applied to topic modeling. I will show a comparison between different non-parametric models and the current state-of-the-art parametric model, Latent Dirichlet Allocation (LDA). version:1
arxiv-1312-5047 | Stable Camera Motion Estimation Using Convex Programming | http://arxiv.org/abs/1312.5047 | id:1312.5047 author:Onur Ozyesil, Amit Singer, Ronen Basri category:cs.CV  published:2013-12-18 summary:We study the inverse problem of estimating n locations $t_1, ..., t_n$ (up to global scale, translation and negation) in $R^d$ from noisy measurements of a subset of the (unsigned) pairwise lines that connect them, that is, from noisy measurements of $\pm (t_i - t_j)/\ t_i - t_j\ $ for some pairs (i,j) (where the signs are unknown). This problem is at the core of the structure from motion (SfM) problem in computer vision, where the $t_i$'s represent camera locations in $R^3$. The noiseless version of the problem, with exact line measurements, has been considered previously under the general title of parallel rigidity theory, mainly in order to characterize the conditions for unique realization of locations. For noisy pairwise line measurements, current methods tend to produce spurious solutions that are clustered around a few locations. This sensitivity of the location estimates is a well-known problem in SfM, especially for large, irregular collections of images. In this paper we introduce a semidefinite programming (SDP) formulation, specially tailored to overcome the clustering phenomenon. We further identify the implications of parallel rigidity theory for the location estimation problem to be well-posed, and prove exact (in the noiseless case) and stable location recovery results. We also formulate an alternating direction method to solve the resulting semidefinite program, and provide a distributed version of our formulation for large numbers of locations. Specifically for the camera location estimation problem, we formulate a pairwise line estimation method based on robust camera orientation and subspace estimation. Lastly, we demonstrate the utility of our algorithm through experiments on real images. version:3
arxiv-1501-03838 | PAC-Bayes with Minimax for Confidence-Rated Transduction | http://arxiv.org/abs/1501.03838 | id:1501.03838 author:Akshay Balsubramani, Yoav Freund category:cs.LG stat.ML  published:2015-01-15 summary:We consider using an ensemble of binary classifiers for transductive prediction, when unlabeled test data are known in advance. We derive minimax optimal rules for confidence-rated prediction in this setting. By using PAC-Bayes analysis on these rules, we obtain data-dependent performance guarantees without distributional assumptions on the data. Our analysis techniques are readily extended to a setting in which the predictor is allowed to abstain. version:1
arxiv-1310-0532 | Perfect Clustering for Stochastic Blockmodel Graphs via Adjacency Spectral Embedding | http://arxiv.org/abs/1310.0532 | id:1310.0532 author:Vince Lyzinski, Daniel Sussman, Minh Tang, Avanti Athreya, Carey Priebe category:stat.ML  published:2013-10-02 summary:Vertex clustering in a stochastic blockmodel graph has wide applicability and has been the subject of extensive research. In thispaper, we provide a short proof that the adjacency spectral embedding can be used to obtain perfect clustering for the stochastic blockmodel and the degree-corrected stochastic blockmodel. We also show an analogous result for the more general random dot product graph model. version:4
arxiv-1501-03796 | The Fast Convergence of Incremental PCA | http://arxiv.org/abs/1501.03796 | id:1501.03796 author:Akshay Balsubramani, Sanjoy Dasgupta, Yoav Freund category:cs.LG stat.ML  published:2015-01-15 summary:We consider a situation in which we see samples in $\mathbb{R}^d$ drawn i.i.d. from some distribution with mean zero and unknown covariance A. We wish to compute the top eigenvector of A in an incremental fashion - with an algorithm that maintains an estimate of the top eigenvector in O(d) space, and incrementally adjusts the estimate with each new data point that arrives. Two classical such schemes are due to Krasulina (1969) and Oja (1983). We give finite-sample convergence rates for both. version:1
arxiv-1501-03779 | Computer-assisted polyp matching between optical colonoscopy and CT colonography: a phantom study | http://arxiv.org/abs/1501.03779 | id:1501.03779 author:Holger R. Roth, Thomas E. Hampshire, Emma Helbren, Mingxing Hu, Roser Vega, Steve Halligan, David J. Hawkes category:cs.CV  published:2015-01-15 summary:Potentially precancerous polyps detected with CT colonography (CTC) need to be removed subsequently, using an optical colonoscope (OC). Due to large colonic deformations induced by the colonoscope, even very experienced colonoscopists find it difficult to pinpoint the exact location of the colonoscope tip in relation to polyps reported on CTC. This can cause unduly prolonged OC examinations that are stressful for the patient, colonoscopist and supporting staff. We developed a method, based on monocular 3D reconstruction from OC images, that automatically matches polyps observed in OC with polyps reported on prior CTC. A matching cost is computed, using rigid point-based registration between surface point clouds extracted from both modalities. A 3D printed and painted phantom of a 25 cm long transverse colon segment was used to validate the method on two medium sized polyps. Results indicate that the matching cost is smaller at the correct corresponding polyp between OC and CTC: the value is 3.9 times higher at the incorrect polyp, comparing the correct match between polyps to the incorrect match. Furthermore, we evaluate the matching of the reconstructed polyp from OC with other colonic endoluminal surface structures such as haustral folds and show that there is a minimum at the correct polyp from CTC. Automated matching between polyps observed at OC and prior CTC would facilitate the biopsy or removal of true-positive pathology or exclusion of false-positive CTC findings, and would reduce colonoscopy false-negative (missed) polyps. Ultimately, such a method might reduce healthcare costs, patient inconvenience and discomfort. version:1
arxiv-1501-03771 | Submodular relaxation for inference in Markov random fields | http://arxiv.org/abs/1501.03771 | id:1501.03771 author:Anton Osokin, Dmitry Vetrov category:cs.CV math.OC stat.ML  published:2015-01-15 summary:In this paper we address the problem of finding the most probable state of a discrete Markov random field (MRF), also known as the MRF energy minimization problem. The task is known to be NP-hard in general and its practical importance motivates numerous approximate algorithms. We propose a submodular relaxation approach (SMR) based on a Lagrangian relaxation of the initial problem. Unlike the dual decomposition approach of Komodakis et al., 2011 SMR does not decompose the graph structure of the initial problem but constructs a submodular energy that is minimized within the Lagrangian relaxation. Our approach is applicable to both pairwise and high-order MRFs and allows to take into account global potentials of certain types. We study theoretical properties of the proposed approach and evaluate it experimentally. version:1
arxiv-1501-04009 | Visual Analytics of Image-Centric Cohort Studies in Epidemiology | http://arxiv.org/abs/1501.04009 | id:1501.04009 author:Bernhard Preim, Paul Klemm, Helwig Hauser, Katrin Hegenscheid, Steffen Oeltze, Klaus Toennies, Henry Völzke category:cs.CV cs.CY  published:2015-01-15 summary:Epidemiology characterizes the influence of causes to disease and health conditions of defined populations. Cohort studies are population-based studies involving usually large numbers of randomly selected individuals and comprising numerous attributes, ranging from self-reported interview data to results from various medical examinations, e.g., blood and urine samples. Since recently, medical imaging has been used as an additional instrument to assess risk factors and potential prognostic information. In this chapter, we discuss such studies and how the evaluation may benefit from visual analytics. Cluster analysis to define groups, reliable image analysis of organs in medical imaging data and shape space exploration to characterize anatomical shapes are among the visual analytics tools that may enable epidemiologists to fully exploit the potential of their huge and complex data. To gain acceptance, visual analytics tools need to complement more classical epidemiologic tools, primarily hypothesis-driven statistical analysis. version:1
arxiv-1308-3136 | Toward the Coevolution of Novel Vertical-Axis Wind Turbines | http://arxiv.org/abs/1308.3136 | id:1308.3136 author:Richard J. Preen, Larry Bull category:cs.NE cs.AI cs.CE  published:2013-08-13 summary:The production of renewable and sustainable energy is one of the most important challenges currently facing mankind. Wind has made an increasing contribution to the world's energy supply mix, but still remains a long way from reaching its full potential. In this paper, we investigate the use of artificial evolution to design vertical-axis wind turbine prototypes that are physically instantiated and evaluated under fan generated wind conditions. Initially a conventional evolutionary algorithm is used to explore the design space of a single wind turbine and later a cooperative coevolutionary algorithm is used to explore the design space of an array of wind turbines. Artificial neural networks are used throughout as surrogate models to assist learning and found to reduce the number of fabrications required to reach a higher aerodynamic efficiency. Unlike in other approaches, such as computational fluid dynamics simulations, no mathematical formulations are used and no model assumptions are made. version:2
arxiv-1501-03719 | LATCH: Learned Arrangements of Three Patch Codes | http://arxiv.org/abs/1501.03719 | id:1501.03719 author:Gil Levi, Tal Hassner category:cs.CV  published:2015-01-15 summary:We present a novel means of describing local image appearances using binary strings. Binary descriptors have drawn increasing interest in recent years due to their speed and low memory footprint. A known shortcoming of these representations is their inferior performance compared to larger, histogram based descriptors such as the SIFT. Our goal is to close this performance gap while maintaining the benefits attributed to binary representations. To this end we propose the Learned Arrangements of Three Patch Codes descriptors, or LATCH. Our key observation is that existing binary descriptors are at an increased risk from noise and local appearance variations. This, as they compare the values of pixel pairs; changes to either of the pixels can easily lead to changes in descriptor values, hence damaging its performance. In order to provide more robustness, we instead propose a novel means of comparing pixel patches. This ostensibly small change, requires a substantial redesign of the descriptors themselves and how they are produced. Our resulting LATCH representation is rigorously compared to state-of-the-art binary descriptors and shown to provide far better performance for similar computation and space requirements. version:1
arxiv-1410-0547 | Design Mining Interacting Wind Turbines | http://arxiv.org/abs/1410.0547 | id:1410.0547 author:Richard J. Preen, Larry Bull category:cs.NE cs.AI cs.CE  published:2014-10-02 summary:An initial study of surrogate-assisted evolutionary algorithms used to design vertical-axis wind turbines wherein candidate prototypes are evaluated under fan generated wind conditions after being physically instantiated by a 3D printer has recently been presented. Unlike other approaches, such as computational fluid dynamics simulations, no mathematical formulations were used and no model assumptions were made. This paper extends that work by exploring alternative surrogate modelling and evolutionary techniques. The accuracy of various modelling algorithms used to estimate the fitness of evaluated individuals from the initial experiments is compared. The effect of temporally windowing surrogate model training samples is explored. A surrogate-assisted approach based on an enhanced local search is introduced; and alternative coevolution collaboration schemes are examined. version:2
arxiv-1501-03302 | Hard to Cheat: A Turing Test based on Answering Questions about Images | http://arxiv.org/abs/1501.03302 | id:1501.03302 author:Mateusz Malinowski, Mario Fritz category:cs.AI cs.CL cs.CV cs.LG  published:2015-01-14 summary:Progress in language and image understanding by machines has sparkled the interest of the research community in more open-ended, holistic tasks, and refueled an old AI dream of building intelligent machines. We discuss a few prominent challenges that characterize such holistic tasks and argue for "question answering about images" as a particular appealing instance of such a holistic task. In particular, we point out that it is a version of a Turing Test that is likely to be more robust to over-interpretations and contrast it with tasks like grounding and generation of descriptions. Finally, we discuss tools to measure progress in this field. version:2
arxiv-1501-02894 | A Modified No Search Algorithm for Fractal Image Compression | http://arxiv.org/abs/1501.02894 | id:1501.02894 author:Mehdi. Salarian, Babak. Mohamadinia, Jalil Rasekhi category:cs.CV cs.MM  published:2015-01-13 summary:Fractal image compression has some desirable properties like high quality at high compression ratio, fast decoding, and resolution independence. Therefore it can be used for many applications such as texture mapping and pattern recognition and image watermarking. But it suffers from long encoding time due to its need to find the best match between sub blocks. This time is related to the approach that is used. In this paper we present a fast encoding Algorithm based on no search method. Our goal is that more blocks are covered in initial step of quad tree algorithm. Experimental result has been compared with other new fast fractal coding methods, showing it is better in term of bit rate in same condition while the other parameters are fixed. version:2
arxiv-1408-3873 | Classifying sequences by the optimized dissimilarity space embedding approach: a case study on the solubility analysis of the E. coli proteome | http://arxiv.org/abs/1408.3873 | id:1408.3873 author:Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.CV cs.AI physics.bio-ph q-bio.BM I.5  published:2014-08-17 summary:We evaluate a version of the recently-proposed classification system named Optimized Dissimilarity Space Embedding (ODSE) that operates in the input space of sequences of generic objects. The ODSE system has been originally presented as a classification system for patterns represented as labeled graphs. However, since ODSE is founded on the dissimilarity space representation of the input data, the classifier can be easily adapted to any input domain where it is possible to define a meaningful dissimilarity measure. Here we demonstrate the effectiveness of the ODSE classifier for sequences by considering an application dealing with the recognition of the solubility degree of the Escherichia coli proteome. Solubility, or analogously aggregation propensity, is an important property of protein molecules, which is intimately related to the mechanisms underlying the chemico-physical process of folding. Each protein of our dataset is initially associated with a solubility degree and it is represented as a sequence of symbols, denoting the 20 amino acid residues. The herein obtained computational results, which we stress that have been achieved with no context-dependent tuning of the ODSE system, confirm the validity and generality of the ODSE-based approach for structured data classification. version:2
arxiv-1501-03347 | Dirichlet Process Parsimonious Mixtures for clustering | http://arxiv.org/abs/1501.03347 | id:1501.03347 author:Faicel Chamroukhi, Marius Bartcus, Hervé Glotin category:stat.ML cs.LG stat.ME  published:2015-01-14 summary:The parsimonious Gaussian mixture models, which exploit an eigenvalue decomposition of the group covariance matrices of the Gaussian mixture, have shown their success in particular in cluster analysis. Their estimation is in general performed by maximum likelihood estimation and has also been considered from a parametric Bayesian prospective. We propose new Dirichlet Process Parsimonious mixtures (DPPM) which represent a Bayesian nonparametric formulation of these parsimonious Gaussian mixture models. The proposed DPPM models are Bayesian nonparametric parsimonious mixture models that allow to simultaneously infer the model parameters, the optimal number of mixture components and the optimal parsimonious mixture structure from the data. We develop a Gibbs sampling technique for maximum a posteriori (MAP) estimation of the developed DPMM models and provide a Bayesian model selection framework by using Bayes factors. We apply them to cluster simulated data and real data sets, and compare them to the standard parsimonious mixture models. The obtained results highlight the effectiveness of the proposed nonparametric parsimonious mixture models as a good nonparametric alternative for the parametric parsimonious models. version:1
arxiv-1501-03320 | Image enhancement in intensity projected multichannel MRI using spatially adaptive directional anisotropic diffusion | http://arxiv.org/abs/1501.03320 | id:1501.03320 author:P. K. Akshara, J. S. Paul category:cs.CV  published:2015-01-14 summary:Anisotropic Diffusion is widely used for noise reduction with simultaneous preservation of vascular structures in maximum intensity projected (MIP) angiograms. However, extension to minimum intensity projected (mIP) venograms in Susceptibility Weighted Imaging (SWI) poses difficulties due to spatially varying baseline. Here, we introduce a modified version of the directional anisotropic diffusion which allows us to simultaneously reduce the noise and enhance vascular structures reconstructed using both M/mIP angiograms. This method is based on spatial adaptation of the diffusion function, separately in the directions of the gradient, and along those of the minimum and maximum curvatures. The existing approach of directional anisotropic diffusion uses binary switched diffusion function to ensure diffusion along the direction of maximum curvature stopped near the vessel borders. Here, the choice of a threshold for detecting the upper limit of diffusion becomes difficult in the presence of spatially varying baseline. Also, the approach of using vesselness measure to steer the diffusion process results in structural discontinuities due to junction suppression in mIP. The merits of the proposed method include elimination of the need for an apriori choice of a threshold to detect the vessel, and problems due to junction suppression. The proposed method is also extended to multi-channel phase contrast angiogram. version:1
arxiv-1501-03273 | Classification with Low Rank and Missing Data | http://arxiv.org/abs/1501.03273 | id:1501.03273 author:Elad Hazan, Roi Livni, Yishay Mansour category:cs.LG  published:2015-01-14 summary:We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data. version:1
arxiv-1501-03271 | Higher dimensional homodyne filtering for suppression of incidental phase artifacts in multichannel MRI | http://arxiv.org/abs/1501.03271 | id:1501.03271 author:Joseph Suresh Paul, Uma Krishna Swamy Pillai category:cs.CV physics.med-ph  published:2015-01-14 summary:The aim of this paper is to introduce procedural steps for extension of the 1D homodyne phase correction for k-space truncation in all gradient encoding directions. Compared to the existing method applied to 2D partial k-space, signal losses introduced by the phase correction filter is observed to be minimal for the extended approach. In addition, the modified form of phase correction mitigates Incidental Phase Artifacts (IPA) due to truncation. For parallel imaging with undersampling along phase encode direction, the extended homodyne filtering is shown to be effective for minimizing these artifacts when each of the channel k-spaces are truncated along both phase and frequency encode directions. This is illustrated with 2D partial k-space for flow compensated multichannel Susceptibility Weighted Imaging (SWI). Extension of our method to 3D partial k-space shows improved reconstruction of flow information in phase contrast angiography. version:1
arxiv-1501-03214 | Quantifying Prosodic Variability in Middle English Alliterative Poetry | http://arxiv.org/abs/1501.03214 | id:1501.03214 author:Roger Bilisoly category:stat.AP cs.CL  published:2015-01-14 summary:Interest in the mathematical structure of poetry dates back to at least the 19th century: after retiring from his mathematics position, J. J. Sylvester wrote a book on prosody called $\textit{The Laws of Verse}$. Today there is interest in the computer analysis of poems, and this paper discusses how a statistical approach can be applied to this task. Starting with the definition of what Middle English alliteration is, $\textit{Sir Gawain and the Green Knight}$ and William Langland's $\textit{Piers Plowman}$ are used to illustrate the methodology. Theory first developed for analyzing data from a Riemannian manifold turns out to be applicable to strings allowing one to compute a generalized mean and variance for textual data, which is applied to the poems above. The ratio of these two variances produces the analogue of the F test, and resampling allows p-values to be estimated. Consequently, this methodology provides a way to compare prosodic variability between two texts. version:1
arxiv-1501-03210 | Towards Deep Semantic Analysis Of Hashtags | http://arxiv.org/abs/1501.03210 | id:1501.03210 author:Piyush Bansal, Romil Bansal, Vasudeva Varma category:cs.IR cs.CL  published:2015-01-13 summary:Hashtags are semantico-syntactic constructs used across various social networking and microblogging platforms to enable users to start a topic specific discussion or classify a post into a desired category. Segmenting and linking the entities present within the hashtags could therefore help in better understanding and extraction of information shared across the social media. However, due to lack of space delimiters in the hashtags (e.g #nsavssnowden), the segmentation of hashtags into constituent entities ("NSA" and "Edward Snowden" in this case) is not a trivial task. Most of the current state-of-the-art social media analytics systems like Sentiment Analysis and Entity Linking tend to either ignore hashtags, or treat them as a single word. In this paper, we present a context aware approach to segment and link entities in the hashtags to a knowledge base (KB) entry, based on the context within the tweet. Our approach segments and links the entities in hashtags such that the coherence between hashtag semantics and the tweet is maximized. To the best of our knowledge, no existing study addresses the issue of linking entities in hashtags for extracting semantic information. We evaluate our method on two different datasets, and demonstrate the effectiveness of our technique in improving the overall entity linking in tweets via additional semantic information provided by segmenting and linking entities in a hashtag. version:1
arxiv-1501-03191 | Annotating Cognates and Etymological Origin in Turkic Languages | http://arxiv.org/abs/1501.03191 | id:1501.03191 author:Benjamin S. Mericli, Michael Bloodgood category:cs.CL I.2.7  published:2015-01-13 summary:Turkic languages exhibit extensive and diverse etymological relationships among lexical items. These relationships make the Turkic languages promising for exploring automated translation lexicon induction by leveraging cognate and other etymological information. However, due to the extent and diversity of the types of relationships between words, it is not clear how to annotate such information. In this paper, we present a methodology for annotating cognates and etymological origin in Turkic languages. Our method strives to balance the amount of research effort the annotator expends with the utility of the annotations for supporting research on improving automated translation lexicon induction. version:1
arxiv-1501-03124 | Robust and Real Time Detection of Curvy Lanes (Curves) with Desired Slopes for Driving Assistance and Autonomous Vehicles | http://arxiv.org/abs/1501.03124 | id:1501.03124 author:Amartansh Dubey, K. M. Bhurchandi category:cs.CV  published:2015-01-13 summary:One of the biggest reasons for road accidents is curvy lanes and blind turns. Even one of the biggest hurdles for new autonomous vehicles is to detect curvy lanes, multiple lanes and lanes with a lot of discontinuity and noise. This paper presents very efficient and advanced algorithm for detecting curves having desired slopes (especially for detecting curvy lanes in real time) and detection of curves (lanes) with a lot of noise, discontinuity and disturbances. Overall aim is to develop robust method for this task which is applicable even in adverse conditions. Even in some of most famous and useful libraries like OpenCV and Matlab, there is no function available for detecting curves having desired slopes , shapes, discontinuities. Only few predefined shapes like circle, ellipse, etc, can be detected using presently available functions. Proposed algorithm can not only detect curves with discontinuity, noise, desired slope but also it can perform shadow and illumination correction and detect/ differentiate between different curves. version:1
arxiv-1501-03084 | Deep Learning with Nonparametric Clustering | http://arxiv.org/abs/1501.03084 | id:1501.03084 author:Gang Chen category:cs.LG 68T10 I.2.6  published:2015-01-13 summary:Clustering is an essential problem in machine learning and data mining. One vital factor that impacts clustering performance is how to learn or design the data representation (or features). Fortunately, recent advances in deep learning can learn unsupervised features effectively, and have yielded state of the art performance in many classification problems, such as character recognition, object recognition and document categorization. However, little attention has been paid to the potential of deep learning for unsupervised clustering problems. In this paper, we propose a deep belief network with nonparametric clustering. As an unsupervised method, our model first leverages the advantages of deep learning for feature representation and dimension reduction. Then, it performs nonparametric clustering under a maximum margin framework -- a discriminative clustering model and can be trained online efficiently in the code space. Lastly model parameters are refined in the deep belief network. Thus, this model can learn features for clustering and infer model complexity in an unified framework. The experimental results show the advantage of our approach over competitive baselines. version:1
arxiv-1410-5473 | Feature Selection Based on Confidence Machine | http://arxiv.org/abs/1410.5473 | id:1410.5473 author:Chang Liu, Yi Xu category:cs.LG  published:2014-10-20 summary:In machine learning and pattern recognition, feature selection has been a hot topic in the literature. Unsupervised feature selection is challenging due to the loss of labels which would supply the related information.How to define an appropriate metric is the key for feature selection. We propose a filter method for unsupervised feature selection which is based on the Confidence Machine. Confidence Machine offers an estimation of confidence on a feature'reliability. In this paper, we provide the math model of Confidence Machine in the context of feature selection, which maximizes the relevance and minimizes the redundancy of the selected feature. We compare our method against classic feature selection methods Laplacian Score, Pearson Correlation and Principal Component Analysis on benchmark data sets. The experimental results demonstrate the efficiency and effectiveness of our method. version:2
arxiv-1501-03058 | An Adaptive Neuro-Fuzzy Inference System Modeling for Grid-Adaptive Interpolation over Depth Images | http://arxiv.org/abs/1501.03058 | id:1501.03058 author:Arbaaz Singh Sidhu category:cs.CV  published:2015-01-13 summary:A suitable interpolation method is essential to keep the noise level minimum along with the time-delay. In recent years, many different interpolation filters have been developed for instance H.264-6 tap filter, and AVS- 4 tap filter. The present work uses Adaptive Neuro-Fuzzy Inference System (ANFIS) technique to model and investigate the effects of a four-tap low-pass tap filter (Grid-adaptive filter) on a hole-filled depth image. The work demonstrates the general form of uniform interpolations for both integer and sub-pixel locations in terms of the sampling interval and filter length of depth-images via diverse finite impulse response filtering schemes. The demonstrated model combined modelling function of fuzzy inference with the learning ability of artificial neural network. version:1
arxiv-1411-3229 | Multi-modal Image Registration for Correlative Microscopy | http://arxiv.org/abs/1411.3229 | id:1411.3229 author:Tian Cao, Christopher Zach, Shannon Modla, Debbie Powell, Kirk Czymmek, Marc Niethammer category:cs.CV  published:2014-11-12 summary:Correlative microscopy is a methodology combining the functionality of light microscopy with the high resolution of electron microscopy and other microscopy technologies. Image registration for correlative microscopy is quite challenging because it is a multi-modal, multi-scale and multi-dimensional registration problem. In this report, I introduce two methods of image registration for correlative microscopy. The first method is based on fiducials (beads). I generate landmarks from the fiducials and compute the similarity transformation matrix based on three pairs of nearest corresponding landmarks. A least-squares matching process is applied afterwards to further refine the registration. The second method is inspired by the image analogies approach. I introduce the sparse representation model into image analogies. I first train representative image patches (dictionaries) for pre-registered datasets from two different modalities, and then I use the sparse coding technique to transfer a given image to a predicted image from one modality to another based on the learned dictionaries. The final image registration is between the predicted image and the original image corresponding to the given image in the different modality. The method transforms a multi-modal registration problem to a mono-modal one. I test my approaches on Transmission Electron Microscopy (TEM) and confocal microscopy images. Experimental results of the methods are also shown in this report. version:2
arxiv-1501-03015 | Exploring the efficacy of molecular fragments of different complexity in computational SAR modeling | http://arxiv.org/abs/1501.03015 | id:1501.03015 author:Albrecht Zimmermann, Björn Bringmann, Luc De Raedt category:cs.CE cs.LG  published:2015-01-13 summary:An important first step in computational SAR modeling is to transform the compounds into a representation that can be processed by predictive modeling techniques. This is typically a feature vector where each feature indicates the presence or absence of a molecular fragment. While the traditional approach to SAR modeling employed size restricted fingerprints derived from path fragments, much research in recent years focussed on mining more complex graph based fragments. Today, there seems to be a growing consensus in the data mining community that these more expressive fragments should be more useful. We question this consensus and show experimentally that fragments of low complexity, i.e. sequences, perform better than equally large sets of more complex ones, an effect we explain by pairwise correlation among fragments and the ability of a fragment set to encode compounds from different classes distinctly. The size restriction on these sets is based on ordering the fragments by class-correlation scores. In addition, we also evaluate the effects of using a significance value instead of a length restriction for path fragments and find a significant reduction in the number of features with little loss in performance. version:1
arxiv-1501-03002 | An Improvement to the Domain Adaptation Bound in a PAC-Bayesian context | http://arxiv.org/abs/1501.03002 | id:1501.03002 author:Pascal Germain, Amaury Habrard, Francois Laviolette, Emilie Morvant category:stat.ML cs.LG  published:2015-01-13 summary:This paper provides a theoretical analysis of domain adaptation based on the PAC-Bayesian theory. We propose an improvement of the previous domain adaptation bound obtained by Germain et al. in two ways. We first give another generalization bound tighter and easier to interpret. Moreover, we provide a new analysis of the constant term appearing in the bound that can be of high interest for developing new algorithmic solutions. version:1
arxiv-1501-03001 | On Generalizing the C-Bound to the Multiclass and Multi-label Settings | http://arxiv.org/abs/1501.03001 | id:1501.03001 author:Francois Laviolette, Emilie Morvant, Liva Ralaivola, Jean-Francis Roy category:stat.ML cs.LG  published:2015-01-13 summary:The C-bound, introduced in Lacasse et al., gives a tight upper bound on the risk of a binary majority vote classifier. In this work, we present a first step towards extending this work to more complex outputs, by providing generalizations of the C-bound to the multiclass and multi-label settings. version:1
arxiv-1501-02995 | Improved 8-point Approximate DCT for Image and Video Compression Requiring Only 14 Additions | http://arxiv.org/abs/1501.02995 | id:1501.02995 author:U. S. Potluri, A. Madanayake, R. J. Cintra, F. M. Bayer, S. Kulasekera, A. Edirisuriya category:cs.MM cs.CV cs.NA stat.ME  published:2015-01-13 summary:Video processing systems such as HEVC requiring low energy consumption needed for the multimedia market has lead to extensive development in fast algorithms for the efficient approximation of 2-D DCT transforms. The DCT is employed in a multitude of compression standards due to its remarkable energy compaction properties. Multiplier-free approximate DCT transforms have been proposed that offer superior compression performance at very low circuit complexity. Such approximations can be realized in digital VLSI hardware using additions and subtractions only, leading to significant reductions in chip area and power consumption compared to conventional DCTs and integer transforms. In this paper, we introduce a novel 8-point DCT approximation that requires only 14 addition operations and no multiplications. The proposed transform possesses low computational complexity and is compared to state-of-the-art DCT approximations in terms of both algorithm complexity and peak signal-to-noise ratio. The proposed DCT approximation is a candidate for reconfigurable video standards such as HEVC. The proposed transform and several other DCT approximations are mapped to systolic-array digital architectures and physically realized as digital prototype circuits using FPGA technology and mapped to 45 nm CMOS technology. version:1
arxiv-1501-02990 | Random Bits Regression: a Strong General Predictor for Big Data | http://arxiv.org/abs/1501.02990 | id:1501.02990 author:Yi Wang, Yi Li, Momiao Xiong, Li Jin category:stat.ML cs.LG  published:2015-01-13 summary:To improve accuracy and speed of regressions and classifications, we present a data-based prediction method, Random Bits Regression (RBR). This method first generates a large number of random binary intermediate/derived features based on the original input matrix, and then performs regularized linear/logistic regression on those intermediate/derived features to predict the outcome. Benchmark analyses on a simulated dataset, UCI machine learning repository datasets and a GWAS dataset showed that RBR outperforms other popular methods in accuracy and robustness. RBR (available on https://sourceforge.net/projects/rbr/) is very fast and requires reasonable memories, therefore, provides a strong, robust and fast predictor in the big data era. version:1
arxiv-1407-0439 | Geometric Tight Frame based Stylometry for Art Authentication of van Gogh Paintings | http://arxiv.org/abs/1407.0439 | id:1407.0439 author:Haixia Liu, Raymond H. Chan, Yuan Yao category:cs.LG cs.CV  published:2014-07-02 summary:This paper is about authenticating genuine van Gogh paintings from forgeries. The authentication process depends on two key steps: feature extraction and outlier detection. In this paper, a geometric tight frame and some simple statistics of the tight frame coefficients are used to extract features from the paintings. Then a forward stage-wise rank boosting is used to select a small set of features for more accurate classification so that van Gogh paintings are highly concentrated towards some center point while forgeries are spread out as outliers. Numerical results show that our method can achieve 86.08% classification accuracy under the leave-one-out cross-validation procedure. Our method also identifies five features that are much more predominant than other features. Using just these five features for classification, our method can give 88.61% classification accuracy which is the highest so far reported in literature. Evaluation of the five features is also performed on two hundred datasets generated by bootstrap sampling with replacement. The median and the mean are 88.61% and 87.77% respectively. Our results show that a small set of statistics of the tight frame coefficients along certain orientations can serve as discriminative features for van Gogh paintings. It is more important to look at the tail distributions of such directional coefficients than mean values and standard deviations. It reflects a highly consistent style in van Gogh's brushstroke movements, where many forgeries demonstrate a more diverse spread in these features. version:3
arxiv-1501-02859 | $\ell_0$ Sparsifying Transform Learning with Efficient Optimal Updates and Convergence Guarantees | http://arxiv.org/abs/1501.02859 | id:1501.02859 author:Saiprasad Ravishankar, Yoram Bresler category:stat.ML cs.LG  published:2015-01-13 summary:Many applications in signal processing benefit from the sparsity of signals in a certain transform domain or dictionary. Synthesis sparsifying dictionaries that are directly adapted to data have been popular in applications such as image denoising, inpainting, and medical image reconstruction. In this work, we focus instead on the sparsifying transform model, and study the learning of well-conditioned square sparsifying transforms. The proposed algorithms alternate between a $\ell_0$ "norm"-based sparse coding step, and a non-convex transform update step. We derive the exact analytical solution for each of these steps. The proposed solution for the transform update step achieves the global minimum in that step, and also provides speedups over iterative solutions involving conjugate gradients. We establish that our alternating algorithms are globally convergent to the set of local minimizers of the non-convex transform learning problems. In practice, the algorithms are insensitive to initialization. We present results illustrating the promising performance and significant speed-ups of transform learning over synthesis K-SVD in image denoising. version:1
arxiv-1501-02844 | SPRITE: A Response Model For Multiple Choice Testing | http://arxiv.org/abs/1501.02844 | id:1501.02844 author:Ryan Ning, Andrew E. Waters, Christoph Studer, Richard G. Baraniuk category:stat.ML  published:2015-01-12 summary:Item response theory (IRT) models for categorical response data are widely used in the analysis of educational data, computerized adaptive testing, and psychological surveys. However, most IRT models rely on both the assumption that categories are strictly ordered and the assumption that this ordering is known a priori. These assumptions are impractical in many real-world scenarios, such as multiple-choice exams where the levels of incorrectness for the distractor categories are often unknown. While a number of results exist on IRT models for unordered categorical data, they tend to have restrictive modeling assumptions that lead to poor data fitting performance in practice. Furthermore, existing unordered categorical models have parameters that are difficult to interpret. In this work, we propose a novel methodology for unordered categorical IRT that we call SPRITE (short for stochastic polytomous response item model) that: (i) analyzes both ordered and unordered categories, (ii) offers interpretable outputs, and (iii) provides improved data fitting compared to existing models. We compare SPRITE to existing item response models and demonstrate its efficacy on both synthetic and real-world educational datasets. version:1
arxiv-1308-0810 | Risk-consistency of cross-validation with lasso-type procedures | http://arxiv.org/abs/1308.0810 | id:1308.0810 author:Darren Homrighausen, Daniel J. McDonald category:math.ST stat.ML stat.TH  published:2013-08-04 summary:The lasso and related sparsity inducing algorithms have been the target of substantial theoretical and applied research. Correspondingly, many results are known about their behavior for a fixed or optimally chosen tuning parameter specified up to unknown constants. In practice, however, this oracle tuning parameter is inaccessible, so one must instead use the data to choose a tuning parameter. Common statistical practice is to use one of a few variants of cross-validation for this task. However, very little is known about the theoretical properties of the resulting predictions using data-dependent methods. We consider the high-dimensional setting with random design wherein the number of predictors $p$ grows with the number of observations $n$. We show that the lasso remains risk consistent relative to its linear oracle even when the tuning parameter is chosen via cross-validation and the true model is not necessarily linear. We generalize these results to the group lasso and $\sqrt{\mbox{lasso}}$ and compare the performance of cross-validation to other tuning parameter selection methods via simulations. version:2
arxiv-1501-02825 | A Survey on Recent Advances of Computer Vision Algorithms for Egocentric Video | http://arxiv.org/abs/1501.02825 | id:1501.02825 author:Sven Bambach category:cs.CV  published:2015-01-12 summary:Recent technological advances have made lightweight, head mounted cameras both practical and affordable and products like Google Glass show first approaches to introduce the idea of egocentric (first-person) video to the mainstream. Interestingly, the computer vision community has only recently started to explore this new domain of egocentric vision, where research can roughly be categorized into three areas: Object recognition, activity detection/recognition, video summarization. In this paper, we try to give a broad overview about the different problems that have been addressed and collect and compare evaluation results. Moreover, along with the emergence of this new domain came the introduction of numerous new and versatile benchmark datasets, which we summarize and compare as well. version:1
arxiv-1501-02702 | Max-Cost Discrete Function Evaluation Problem under a Budget | http://arxiv.org/abs/1501.02702 | id:1501.02702 author:Feng Nan, Joseph Wang, Venkatesh Saligrama category:cs.LG  published:2015-01-12 summary:We propose novel methods for max-cost Discrete Function Evaluation Problem (DFEP) under budget constraints. We are motivated by applications such as clinical diagnosis where a patient is subjected to a sequence of (possibly expensive) tests before a decision is made. Our goal is to develop strategies for minimizing max-costs. The problem is known to be NP hard and greedy methods based on specialized impurity functions have been proposed. We develop a broad class of \emph{admissible} impurity functions that admit monomials, classes of polynomials, and hinge-loss functions that allow for flexible impurity design with provably optimal approximation bounds. This flexibility is important for datasets when max-cost can be overly sensitive to "outliers." Outliers bias max-cost to a few examples that require a large number of tests for classification. We design admissible functions that allow for accuracy-cost trade-off and result in $O(\log n)$ guarantees of the optimal cost among trees with corresponding classification accuracy levels. version:1
arxiv-1404-2644 | A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse Learning | http://arxiv.org/abs/1404.2644 | id:1404.2644 author:Aurélien Bellet, Yingyu Liang, Alireza Bagheri Garakani, Maria-Florina Balcan, Fei Sha category:cs.DC cs.LG stat.ML  published:2014-04-09 summary:Learning sparse combinations is a frequent theme in machine learning. In this paper, we study its associated optimization problem in the distributed setting where the elements to be combined are not centrally located but spread over a network. We address the key challenges of balancing communication costs and optimization errors. To this end, we propose a distributed Frank-Wolfe (dFW) algorithm. We obtain theoretical guarantees on the optimization error $\epsilon$ and communication cost that do not depend on the total number of combining elements. We further show that the communication cost of dFW is optimal by deriving a lower-bound on the communication cost required to construct an $\epsilon$-approximate solution. We validate our theoretical analysis with empirical studies on synthetic and real-world data, which demonstrate that dFW outperforms both baselines and competing methods. We also study the performance of dFW when the conditions of our analysis are relaxed, and show that dFW is fairly robust. version:3
arxiv-1501-02670 | Navigating the Semantic Horizon using Relative Neighborhood Graphs | http://arxiv.org/abs/1501.02670 | id:1501.02670 author:Amaru Cuba Gyllensten, Magnus Sahlgren category:cs.CL  published:2015-01-12 summary:This paper is concerned with nearest neighbor search in distributional semantic models. A normal nearest neighbor search only returns a ranked list of neighbors, with no information about the structure or topology of the local neighborhood. This is a potentially serious shortcoming of the mode of querying a distributional semantic model, since a ranked list of neighbors may conflate several different senses. We argue that the topology of neighborhoods in semantic space provides important information about the different senses of terms, and that such topological structures can be used for word-sense induction. We also argue that the topology of the neighborhoods in semantic space can be used to determine the semantic horizon of a point, which we define as the set of neighbors that have a direct connection to the point. We introduce relative neighborhood graphs as method to uncover the topological properties of neighborhoods in semantic models. We also provide examples of relative neighborhood graphs for three well-known semantic models; the PMI model, the GloVe model, and the skipgram model. version:1
arxiv-1501-01242 | Efficient Online Relative Comparison Kernel Learning | http://arxiv.org/abs/1501.01242 | id:1501.01242 author:Eric Heim, Matthew Berger, Lee M. Seversky, Milos Hauskrecht category:cs.LG  published:2015-01-06 summary:Learning a kernel matrix from relative comparison human feedback is an important problem with applications in collaborative filtering, object retrieval, and search. For learning a kernel over a large number of objects, existing methods face significant scalability issues inhibiting the application of these methods to settings where a kernel is learned in an online and timely fashion. In this paper we propose a novel framework called Efficient online Relative comparison Kernel LEarning (ERKLE), for efficiently learning the similarity of a large set of objects in an online manner. We learn a kernel from relative comparisons via stochastic gradient descent, one query response at a time, by taking advantage of the sparse and low-rank properties of the gradient to efficiently restrict the kernel to lie in the space of positive semidefinite matrices. In addition, we derive a passive-aggressive online update for minimally satisfying new relative comparisons as to not disrupt the influence of previously obtained comparisons. Experimentally, we demonstrate a considerable improvement in speed while obtaining improved or comparable accuracy compared to current methods in the online learning setting. version:2
arxiv-1501-02592 | Photonic Delay Systems as Machine Learning Implementations | http://arxiv.org/abs/1501.02592 | id:1501.02592 author:Michiel Hermans, Miguel Soriano, Joni Dambre, Peter Bienstman, Ingo Fischer category:cs.NE cs.LG  published:2015-01-12 summary:Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers. version:1
arxiv-1501-02579 | Combined modeling of sparse and dense noise for improvement of Relevance Vector Machine | http://arxiv.org/abs/1501.02579 | id:1501.02579 author:Martin Sundin, Saikat Chatterjee, Magnus Jansson category:stat.ML  published:2015-01-12 summary:Using a Bayesian approach, we consider the problem of recovering sparse signals under additive sparse and dense noise. Typically, sparse noise models outliers, impulse bursts or data loss. To handle sparse noise, existing methods simultaneously estimate the sparse signal of interest and the sparse noise of no interest. For estimating the sparse signal, without the need of estimating the sparse noise, we construct a robust Relevance Vector Machine (RVM). In the RVM, sparse noise and ever present dense noise are treated through a combined noise model. The precision of combined noise is modeled by a diagonal matrix. We show that the new RVM update equations correspond to a non-symmetric sparsity inducing cost function. Further, the combined modeling is found to be computationally more efficient. We also extend the method to block-sparse signals and noise with known and unknown block structures. Through simulations, we show the performance and computation efficiency of the new RVM in several applications: recovery of sparse and block sparse signals, housing price prediction and image denoising. version:1
arxiv-1412-7522 | Learning Deep Temporal Representations for Brain Decoding | http://arxiv.org/abs/1412.7522 | id:1412.7522 author:Orhan Firat, Emre Aksan, Ilke Oztekin, Fatos T. Yarman Vural category:cs.LG cs.NE  published:2014-12-23 summary:Functional magnetic resonance imaging produces high dimensional data, with a less then ideal number of labelled samples for brain decoding tasks (predicting brain states). In this study, we propose a new deep temporal convolutional neural network architecture with spatial pooling for brain decoding which aims to reduce dimensionality of feature space along with improved classification performance. Temporal representations (filters) for each layer of the convolutional model are learned by leveraging unlabelled fMRI data in an unsupervised fashion with regularized autoencoders. Learned temporal representations in multiple levels capture the regularities in the temporal domain and are observed to be a rich bank of activation patterns which also exhibit similarities to the actual hemodynamic responses. Further, spatial pooling layers in the convolutional architecture reduce the dimensionality without losing excessive information. By employing the proposed temporal convolutional architecture with spatial pooling, raw input fMRI data is mapped to a non-linear, highly-expressive and low-dimensional feature space where the final classification is conducted. In addition, we propose a simple heuristic approach for hyper-parameter tuning when no validation data is available. Proposed method is tested on a ten class recognition memory experiment with nine subjects. The results support the efficiency and potential of the proposed model, compared to the baseline multi-voxel pattern analysis techniques. version:4
arxiv-1501-02530 | A Dataset for Movie Description | http://arxiv.org/abs/1501.02530 | id:1501.02530 author:Anna Rohrbach, Marcus Rohrbach, Niket Tandon, Bernt Schiele category:cs.CV cs.CL cs.IR  published:2015-01-12 summary:Descriptive video service (DVS) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed DVS, which is temporally aligned to full length HD movies. In addition we also collected the aligned movie scripts which have been used in prior work and compare the two different sources of descriptions. In total the Movie Description dataset contains a parallel corpus of over 54,000 sentences and video snippets from 72 HD movies. We characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing DVS to scripts, we find that DVS is far more visual and describes precisely what is shown rather than what should happen according to the scripts created prior to movie production. version:1
arxiv-1501-02527 | Autodetection and Classification of Hidden Cultural City Districts from Yelp Reviews | http://arxiv.org/abs/1501.02527 | id:1501.02527 author:Harini Suresh, Nicholas Locascio category:cs.CL cs.AI cs.IR  published:2015-01-12 summary:Topic models are a way to discover underlying themes in an otherwise unstructured collection of documents. In this study, we specifically used the Latent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to classify restaurants based off of their reviews. Furthermore, we hypothesize that within a city, restaurants can be grouped into similar "clusters" based on both location and similarity. We used several different clustering methods, including K-means Clustering and a Probabilistic Mixture Model, in order to uncover and classify districts, both well-known and hidden (i.e. cultural areas like Chinatown or hearsay like "the best street for Italian restaurants") within a city. We use these models to display and label different clusters on a map. We also introduce a topic similarity heatmap that displays the similarity distribution in a city to a new restaurant. version:1
arxiv-1404-1361 | Learning the Conditional Independence Structure of Stationary Time Series: A Multitask Learning Approach | http://arxiv.org/abs/1404.1361 | id:1404.1361 author:Alexander Jung category:stat.ML  published:2014-04-04 summary:We propose a method for inferring the conditional independence graph (CIG) of a high-dimensional Gaussian vector time series (discrete-time process) from a finite-length observation. By contrast to existing approaches, we do not rely on a parametric process model (such as, e.g., an autoregressive model) for the observed random process. Instead, we only require certain smoothness properties (in the Fourier domain) of the process. The proposed inference scheme works even for sample sizes much smaller than the number of scalar process components if the true underlying CIG is sufficiently sparse. A theoretical performance analysis provides conditions which guarantee that the probability of the proposed inference method to deliver a wrong CIG is below a prescribed value. These conditions imply lower bounds on the sample size such that the new method is consistent asymptotically. Some numerical experiments validate our theoretical performance analysis and demonstrate superior performance of our scheme compared to an existing (parametric) approach in case of model mismatch. version:3
arxiv-1501-02497 | Identifiability and optimal rates of convergence for parameters of multiple types in finite mixtures | http://arxiv.org/abs/1501.02497 | id:1501.02497 author:Nhat Ho, XuanLong Nguyen category:math.ST stat.ML stat.TH  published:2015-01-11 summary:This paper studies identifiability and convergence behaviors for parameters of multiple types in finite mixtures, and the effects of model fitting with extra mixing components. First, we present a general theory for strong identifiability, which extends from the previous work of Nguyen [2013] and Chen [1995] to address a broad range of mixture models and to handle matrix-variate parameters. These models are shown to share the same Wasserstein distance based optimal rates of convergence for the space of mixing distributions --- $n^{-1/2}$ under $W_1$ for the exact-fitted and $n^{-1/4}$ under $W_2$ for the over-fitted setting, where $n$ is the sample size. This theory, however, is not applicable to several important model classes, including location-scale multivariate Gaussian mixtures, shape-scale Gamma mixtures and location-scale-shape skew-normal mixtures. The second part of this work is devoted to demonstrating that for these "weakly identifiable" classes, algebraic structures of the density family play a fundamental role in determining convergence rates of the model parameters, which display a very rich spectrum of behaviors. For instance, the optimal rate of parameter estimation in an over-fitted location-covariance Gaussian mixture is precisely determined by the order of a solvable system of polynomial equations --- these rates deteriorate rapidly as more extra components are added to the model. The established rates for a variety of settings are illustrated by a simulation study. version:1
arxiv-1501-02484 | Crowd-ML: A Privacy-Preserving Learning Framework for a Crowd of Smart Devices | http://arxiv.org/abs/1501.02484 | id:1501.02484 author:Jihun Hamm, Adam Champion, Guoxing Chen, Mikhail Belkin, Dong Xuan category:cs.LG cs.CR cs.DC cs.NI  published:2015-01-11 summary:Smart devices with built-in sensors, computational capabilities, and network connectivity have become increasingly pervasive. The crowds of smart devices offer opportunities to collectively sense and perform computing tasks in an unprecedented scale. This paper presents Crowd-ML, a privacy-preserving machine learning framework for a crowd of smart devices, which can solve a wide range of learning problems for crowdsensing data with differential privacy guarantees. Crowd-ML endows a crowdsensing system with an ability to learn classifiers or predictors online from crowdsensing data privately with minimal computational overheads on devices and servers, suitable for a practical and large-scale employment of the framework. We analyze the performance and the scalability of Crowd-ML, and implement the system with off-the-shelf smartphones as a proof of concept. We demonstrate the advantages of Crowd-ML with real and simulated experiments under various conditions. version:1
arxiv-1411-6232 | Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks | http://arxiv.org/abs/1411.6232 | id:1411.6232 author:Xiaojun Chang, Yi Yang category:cs.LG  published:2014-11-23 summary:In this paper, we propose a novel semi-supervised feature selection framework by mining correlations among multiple tasks and apply it to different multimedia applications. Instead of independently computing the importance of features for each task, our algorithm leverages shared knowledge from multiple related tasks, thus, improving the performance of feature selection. Note that we build our algorithm on assumption that different tasks share common structures. The proposed algorithm selects features in a batch mode, by which the correlations between different features are taken into consideration. Besides, considering the fact that labeling a large amount of training data in real world is both time-consuming and tedious, we adopt manifold learning which exploits both labeled and unlabeled training data for feature space analysis. Since the objective function is non-smooth and difficult to solve, we propose an iterative algorithm with fast convergence. Extensive experiments on different applications demonstrate that our algorithm outperforms other state-of-the-art feature selection algorithms. version:2
arxiv-1501-02887 | Online Handwritten Devanagari Stroke Recognition Using Extended Directional Features | http://arxiv.org/abs/1501.02887 | id:1501.02887 author:Lajish VL, Sunil Kumar Kopparapu category:cs.CV  published:2015-01-11 summary:This paper describes a new feature set, called the extended directional features (EDF) for use in the recognition of online handwritten strokes. We use EDF specifically to recognize strokes that form a basis for producing Devanagari script, which is the most widely used Indian language script. It should be noted that stroke recognition in handwritten script is equivalent to phoneme recognition in speech signals and is generally very poor and of the order of 20% for singing voice. Experiments are conducted for the automatic recognition of isolated handwritten strokes. Initially we describe the proposed feature set, namely EDF and then show how this feature can be effectively utilized for writer independent script recognition through stroke recognition. Experimental results show that the extended directional feature set performs well with about 65+% stroke level recognition accuracy for writer independent data set. version:1
arxiv-1407-7556 | Entropic one-class classifiers | http://arxiv.org/abs/1407.7556 | id:1407.7556 author:Lorenzo Livi, Alireza Sadeghian, Witold Pedrycz category:cs.CV cs.LG stat.ML I.2.6; K.2.3  published:2014-07-28 summary:The one-class classification problem is a well-known research endeavor in pattern recognition. The problem is also known under different names, such as outlier and novelty/anomaly detection. The core of the problem consists in modeling and recognizing patterns belonging only to a so-called target class. All other patterns are termed non-target, and therefore they should be recognized as such. In this paper, we propose a novel one-class classification system that is based on an interplay of different techniques. Primarily, we follow a dissimilarity representation based approach; we embed the input data into the dissimilarity space by means of an appropriate parametric dissimilarity measure. This step allows us to process virtually any type of data. The dissimilarity vectors are then represented through a weighted Euclidean graphs, which we use to (i) determine the entropy of the data distribution in the dissimilarity space, and at the same time (ii) derive effective decision regions that are modeled as clusters of vertices. Since the dissimilarity measure for the input data is parametric, we optimize its parameters by means of a global optimization scheme, which considers both mesoscopic and structural characteristics of the data represented through the graphs. The proposed one-class classifier is designed to provide both hard (Boolean) and soft decisions about the recognition of test patterns, allowing an accurate description of the classification process. We evaluate the performance of the system on different benchmarking datasets, containing either feature-based or structured patterns. Experimental results demonstrate the effectiveness of the proposed technique. version:3
arxiv-1501-02467 | Fast and optimal nonparametric sequential design for astronomical observations | http://arxiv.org/abs/1501.02467 | id:1501.02467 author:Justin J. Yang, Xufei Wang, Pavlos Protopapas, Luke Bornn category:stat.ME stat.AP stat.ML  published:2015-01-11 summary:The spectral energy distribution (SED) is a relatively easy way for astronomers to distinguish between different astronomical objects such as galaxies, black holes, and stellar objects. By comparing the observations from a source at different frequencies with template models, astronomers are able to infer the type of this observed object. In this paper, we take a Bayesian model averaging perspective to learn astronomical objects, employing a Bayesian nonparametric approach to accommodate the deviation from convex combinations of known log-SEDs. To effectively use telescope time for observations, we then study Bayesian nonparametric sequential experimental design without conjugacy, in which we use sequential Monte Carlo as an efficient tool to maximize the volume of information stored in the posterior distribution of the parameters of interest. A new technique for performing inferences in log-Gaussian Cox processes called the Poisson log-normal approximation is also proposed. Simulations show the speed, accuracy, and usefulness of our method. While the strategy we propose in this paper is brand new in the astronomy literature, the inferential techniques developed apply to more general nonparametric sequential experimental design problems. version:1
arxiv-1501-02432 | Learning a Fuzzy Hyperplane Fat Margin Classifier with Minimum VC dimension | http://arxiv.org/abs/1501.02432 | id:1501.02432 author:Jayadeva, Sanjit Singh Batra, Siddarth Sabharwal category:cs.LG 68T05  68T10  68Q32 I.5.1; I.5.2  published:2015-01-11 summary:The Vapnik-Chervonenkis (VC) dimension measures the complexity of a learning machine, and a low VC dimension leads to good generalization. The recently proposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by minimizing an exact bound on the VC dimension. This paper extends the MCM classifier to the fuzzy domain. The use of a fuzzy membership is known to reduce the effect of outliers, and to reduce the effect of noise on learning. Experimental results show, that on a number of benchmark datasets, the the fuzzy MCM classifier outperforms SVMs and the conventional MCM in terms of generalization, and that the fuzzy MCM uses fewer support vectors. On several benchmark datasets, the fuzzy MCM classifier yields excellent test set accuracies while using one-tenth the number of support vectors used by SVMs. version:1
arxiv-1501-02411 | A Gaussian Particle Filter Approach for Sensors to Track Multiple Moving Targets | http://arxiv.org/abs/1501.02411 | id:1501.02411 author:Haojun Li category:cs.LG  published:2015-01-11 summary:In a variety of problems, the number and state of multiple moving targets are unknown and are subject to be inferred from their measurements obtained by a sensor with limited sensing ability. This type of problems is raised in a variety of applications, including monitoring of endangered species, cleaning, and surveillance. Particle filters are widely used to estimate target state from its prior information and its measurements that recently become available, especially for the cases when the measurement model and the prior distribution of state of interest are non-Gaussian. However, the problem of estimating number of total targets and their state becomes intractable when the number of total targets and the measurement-target association are unknown. This paper presents a novel Gaussian particle filter technique that combines Kalman filter and particle filter for estimating the number and state of total targets based on the measurement obtained online. The estimation is represented by a set of weighted particles, different from classical particle filter, where each particle is a Gaussian distribution instead of a point mass. version:1
arxiv-1406-6319 | Techniques for clustering interaction data as a collection of graphs | http://arxiv.org/abs/1406.6319 | id:1406.6319 author:Nam H. Lee, Carey Priebe, Youngser Park, I-Jeng Wang, Michael Rosen category:stat.ML  published:2014-06-24 summary:A natural approach to analyze interaction data of form "what-connects-to-what-when" is to create a time-series (or rather a sequence) of graphs through temporal discretization (bandwidth selection) and spatial discretization (vertex contraction). Such discretization together with non-negative factorization techniques can be useful for obtaining clustering of graphs. Motivating application of performing clustering of graphs (as opposed to vertex clustering) can be found in neuroscience and in social network analysis, and it can also be used to enhance community detection (i.e., vertex clustering) by way of conditioning on the cluster labels. In this paper, we formulate a problem of clustering of graphs as a model selection problem. Our approach involves information criteria, non-negative matrix factorization and singular value thresholding, and we illustrate our techniques using real and simulated data. version:3
arxiv-1501-02393 | Riemannian Metric Learning for Symmetric Positive Definite Matrices | http://arxiv.org/abs/1501.02393 | id:1501.02393 author:Raviteja Vemulapalli, David W. Jacobs category:cs.CV cs.LG  published:2015-01-10 summary:Over the past few years, symmetric positive definite (SPD) matrices have been receiving considerable attention from computer vision community. Though various distance measures have been proposed in the past for comparing SPD matrices, the two most widely-used measures are affine-invariant distance and log-Euclidean distance. This is because these two measures are true geodesic distances induced by Riemannian geometry. In this work, we focus on the log-Euclidean Riemannian geometry and propose a data-driven approach for learning Riemannian metrics/geodesic distances for SPD matrices. We show that the geodesic distance learned using the proposed approach performs better than various existing distance measures when evaluated on face matching and clustering tasks. version:1
arxiv-1501-02379 | Autonomous Farm Vehicles: Prototype of Power Reaper | http://arxiv.org/abs/1501.02379 | id:1501.02379 author:Abdul Qadeer Khan, Ayyaz Akhtar, Muhammad Zubair Ahmad category:cs.RO cs.CV cs.CY  published:2015-01-10 summary:Chapter 2 will begin with introduction of Agricultural Robotics. There will be a literature review of the mechanical structure, vision and control algorithms. In chapter 3 we will discuss the methodology in detail using block diagrams and flowcharts. The results of the tested and the proposed algorithms will also be displayed. In chapter 4 we will discuss the results in detail and how they are of significance in our work. In chapter 5 we will conclude our work and discuss some future perspectives. In appendices we will provide some background information necessary regarding this project. version:1
arxiv-1501-02378 | Low Cost Semi-Autonomous Agricultural Robots In Pakistan-Vision Based Navigation Scalable methodology for wheat harvesting | http://arxiv.org/abs/1501.02378 | id:1501.02378 author:Muhammad Zubair Ahmad, Ayyaz Akhtar, Abdul Qadeer Khan, Amir Ali Khan, Muhammad Murtaza Khan category:cs.RO cs.CV cs.CY  published:2015-01-10 summary:Robots have revolutionized our way of life in recent years.One of the domains that has not yet completely benefited from the robotic automation is the agricultural sector. Agricultural Robotics should complement humans in the arduous tasks during different sub-domains of this sector. Extensive research in Agricultural Robotics has been carried out in Japan, USA, Australia and Germany focusing mainly on the heavy agricultural machinery. Pakistan is an agricultural rich country and its economy and food security are closely tied with agriculture in general and wheat in particular. However, agricultural research in Pakistan is still carried out using the conventional methodologies. This paper is an attempt to trigger the research in this modern domain so that we can benefit from cost effective and resource efficient autonomous agricultural methodologies. This paper focuses on a scalable low cost semi-autonomous technique for wheat harvest which primarily focuses on the farmers with small land holdings. The main focus will be on the vision part of the navigation system deployed by the proposed robot. version:1
arxiv-1501-02376 | Simplified vision based automatic navigation for wheat harvesting in low income economies | http://arxiv.org/abs/1501.02376 | id:1501.02376 author:Muhammad Zubair Ahmad, Ayyaz Akhtar, Abdul Qadeer Khan, Amir A. Khan category:cs.RO cs.CV cs.CY  published:2015-01-10 summary:Recent developments in the domain of agricultural robotics have resulted in development of complex and efficient systems. Most of the land owners in the South Asian region are low income farmers. The agricultural experience for them is still a completely manual process. However, the extreme weather conditions, heat and flooding, often combine to put a lot of stress on these small land owners and the associated labor. In this paper, we propose a prototype for an automated power reaper for the wheat crop. This automated vehicle is navigated using a simple vision based approach employing the low-cost camera and assisted GPS. The mechanical platform is driven by three motors controlled through an interface between the proposed vision algorithm and the electrical drive. The proposed methodology is applied on some real field scenarios to demonstrate the efficiency of the vision based algorithm. version:1
arxiv-1501-03383 | On the Distribution of Salient Objects in Web Images and its Influence on Salient Object Detection | http://arxiv.org/abs/1501.03383 | id:1501.03383 author:Boris Schauerte, Rainer Stiefelhagen category:cs.CV  published:2015-01-10 summary:It has become apparent that a Gaussian center bias can serve as an important prior for visual saliency detection, which has been demonstrated for predicting human eye fixations and salient object detection. Tseng et al. have shown that the photographer's tendency to place interesting objects in the center is a likely cause for the center bias of eye fixations. We investigate the influence of the photographer's center bias on salient object detection, extending our previous work. We show that the centroid locations of salient objects in photographs of Achanta and Liu's data set in fact correlate strongly with a Gaussian model. This is an important insight, because it provides an empirical motivation and justification for the integration of such a center bias in salient object detection algorithms and helps to understand why Gaussian models are so effective. To assess the influence of the center bias on salient object detection, we integrate an explicit Gaussian center bias model into two state-of-the-art salient object detection algorithms. This way, first, we quantify the influence of the Gaussian center bias on pixel- and segment-based salient object detection. Second, we improve the performance in terms of F1 score, Fb score, area under the recall-precision curve, area under the receiver operating characteristic curve, and hit-rate on the well-known data set by Achanta and Liu. Third, by debiasing Cheng et al.'s region contrast model, we exemplarily demonstrate that implicit center biases are partially responsible for the outstanding performance of state-of-the-art algorithms. Last but not least, as a result of debiasing Cheng et al.'s algorithm, we introduce a non-biased salient object detection method, which is of interest for applications in which the image data is not likely to have a photographer's center bias (e.g., image data of surveillance cameras or autonomous robots). version:1
arxiv-1405-3133 | Graph Matching: Relax at Your Own Risk | http://arxiv.org/abs/1405.3133 | id:1405.3133 author:Vince Lyzinski, Donniell Fishkind, Marcelo Fiori, Joshua T. Vogelstein, Carey E. Priebe, Guillermo Sapiro category:stat.ML math.OC  published:2014-05-13 summary:Graph matching---aligning a pair of graphs to minimize their edge disagreements---has received wide-spread attention from both theoretical and applied communities over the past several decades, including combinatorics, computer vision, and connectomics. Its attention can be partially attributed to its computational difficulty. Although many heuristics have previously been proposed in the literature to approximately solve graph matching, very few have any theoretical support for their performance. A common technique is to relax the discrete problem to a continuous problem, therefore enabling practitioners to bring gradient-descent-type algorithms to bear. We prove that an indefinite relaxation (when solved exactly) almost always discovers the optimal permutation, while a common convex relaxation almost always fails to discover the optimal permutation. These theoretical results suggest that initializing the indefinite algorithm with the convex optimum might yield improved practical performance. Indeed, experimental results illuminate and corroborate these theoretical findings, demonstrating that excellent results are achieved in both benchmark and real data problems by amalgamating the two approaches. version:3
arxiv-1412-7983 | Exploring Sparsity in Multi-class Linear Discriminant Analysis | http://arxiv.org/abs/1412.7983 | id:1412.7983 author:Dong Xia category:stat.ML I.5.2  published:2014-12-26 summary:Recent studies in the literature have paid much attention to the sparsity in linear classification tasks. One motivation of imposing sparsity assumption on the linear discriminant direction is to rule out the noninformative features, making hardly contribution to the classification problem. Most of those work were focused on the scenarios of binary classification. In the presence of multi-class data, preceding researches recommended individually pairwise sparse linear discriminant analysis(LDA). However, further sparsity should be explored. In this paper, an estimator of grouped LASSO type is proposed to take advantage of sparsity for multi-class data. It enjoys appealing non-asymptotic properties which allows insignificant correlations among features. This estimator exhibits superior capability on both simulated and real data. version:2
arxiv-1501-02218 | Survey schemes for stochastic gradient descent with applications to M-estimation | http://arxiv.org/abs/1501.02218 | id:1501.02218 author:Stéphan Clémençon, Patrice Bertail, Emilie Chautru, Guillaume Papa category:stat.ML  published:2015-01-09 summary:In certain situations that shall be undoubtedly more and more common in the Big Data era, the datasets available are so massive that computing statistics over the full sample is hardly feasible, if not unfeasible. A natural approach in this context consists in using survey schemes and substituting the "full data" statistics with their counterparts based on the resulting random samples, of manageable size. It is the main purpose of this paper to investigate the impact of survey sampling with unequal inclusion probabilities on stochastic gradient descent-based M-estimation methods in large-scale statistical and machine-learning problems. Precisely, we prove that, in presence of some a priori information, one may significantly increase asymptotic accuracy when choosing appropriate first order inclusion probabilities, without affecting complexity. These striking results are described here by limit theorems and are also illustrated by numerical experiments. version:1
arxiv-1501-02192 | Investigation of a chaotic spiking neuron model | http://arxiv.org/abs/1501.02192 | id:1501.02192 author:M. Alhawarat, T. Olde Scheper, N. T. Crook category:cs.NE cs.AI  published:2015-01-09 summary:Chaos provides many interesting properties that can be used to achieve computational tasks. Such properties are sensitivity to initial conditions, space filling, control and synchronization. Chaotic neural models have been devised to exploit such properties. In this paper, a chaotic spiking neuron model is investigated experimentally. This investigation is performed to understand the dynamic behaviours of the model. The aim of this research is to investigate the dynamics of the nonlinear dynamic state neuron (NDS) experimentally. The experimental approach has revealed some quantitative and qualitative properties of the NDS model such as the control mechanism, the reset mechanism, and the way the model may exhibit dynamic behaviours in phase space. It is shown experimentally in this paper that both the reset mechanism and the self-feed back control mechanism are important for the NDS model to work and to stabilise to one of the large number of available unstable periodic orbits (UPOs) that are embedded in its attractor. The experimental investigation suggests that the internal dynamics of the NDS neuron provide a rich set of dynamic behaviours that can be controlled and stabilised. These wide range of dynamic behaviours may be exploited to carry out information processing tasks. version:1
arxiv-1501-02128 | Introduction and Ranking Results of the ICSI 2014 Competition on Single Objective Optimization | http://arxiv.org/abs/1501.02128 | id:1501.02128 author:Ying Tan, Junzhi Li, Zhongyang Zheng category:cs.NE  published:2015-01-09 summary:This technical report includes the introduction and ranking results of the ICSI 2014 Competition on Single Objective Optimization. version:1
arxiv-1501-02113 | Filter Design and Performance Evaluation for Fingerprint Image Segmentation | http://arxiv.org/abs/1501.02113 | id:1501.02113 author:Duy Hoang Thai, Stephan Huckemann, Carsten Gottschlich category:cs.CV  published:2015-01-09 summary:Fingerprint recognition plays an important role in many commercial applications and is used by millions of people every day, e.g. for unlocking mobile phones. Fingerprint image segmentation is typically the first processing step of most fingerprint algorithms and it divides an image into foreground, the region of interest, and background. Two types of error can occur during this step which both have a negative impact on the recognition performance: 'true' foreground can be labeled as background and features like minutiae can be lost, or conversely 'true' background can be misclassified as foreground and spurious features can be introduced. The contribution of this paper is threefold: firstly, we propose a novel factorized directional bandpass (FDB) segmentation method for texture extraction based on the directional Hilbert transform of a Butterworth bandpass (DHBB) filter interwoven with soft-thresholding. Secondly, we provide a manually marked ground truth segmentation for 10560 images as an evaluation benchmark. Thirdly, we conduct a systematic performance comparison between the FDB method and four of the most often cited fingerprint segmentation algorithms showing that the FDB segmentation method clearly outperforms these four widely used methods. The benchmark and the implementation of the FDB method are made publicly available. version:1
arxiv-1501-02103 | Margins of discrete Bayesian networks | http://arxiv.org/abs/1501.02103 | id:1501.02103 author:Robin J. Evans category:math.ST stat.ML stat.TH  published:2015-01-09 summary:In this paper we provide a complete algebraic characterization of the model implied by a Bayesian network with latent variables when the observed variables are discrete. We show that it is algebraically equivalent to the so-called nested Markov model, meaning that the two are the same up to inequality constraints on the joint probabilities. The nested Markov model is therefore the best possible approximation to the latent variable model whilst avoiding inequalities, which are extremely complicated in general. Latent variable models also suffer from difficulties of unidentifiable parameters and non-regular asymptotics; in contrast the nested Markov model is fully identifiable, represents a curved exponential family of known dimension, and can easily be fitted using an explicit parameterization. version:1
arxiv-1501-02058 | HOG based Fast Human Detection | http://arxiv.org/abs/1501.02058 | id:1501.02058 author:M. Kachouane, S. Sahki, M. Lakrouf, N. Ouadah category:cs.RO cs.CV cs.LG  published:2015-01-09 summary:Objects recognition in image is one of the most difficult problems in computer vision. It is also an important step for the implementation of several existing applications that require high-level image interpretation. Therefore, there is a growing interest in this research area during the last years. In this paper, we present an algorithm for human detection and recognition in real-time, from images taken by a CCD camera mounted on a car-like mobile robot. The proposed technique is based on Histograms of Oriented Gradient (HOG) and SVM classifier. The implementation of our detector has provided good results, and can be used in robotics tasks. version:1
arxiv-1412-6612 | The Vapnik-Chervonenkis Dimension of Norms on $\mathbb{R}^d$ | http://arxiv.org/abs/1412.6612 | id:1412.6612 author:Christian J. J. Despres category:math.CO math.MG stat.ML  published:2014-12-20 summary:The Vapnik-Chervonenkis dimension of a collection of subsets of a set is an important combinatorial parameter in machine learning. In this paper we show that the VC dimension of the family of d-dimensional cubes in $\mathbb{R}^d$ (that is, the closed balls according to the $\ell^\infty$ norm) is $\lfloor (3d+1)/2 \rfloor$. We also prove that the VC dimension of certain families of convex sets in $\mathbb{R}^2$ (including the balls of all norms) is at most 3, and that there is a norm in $\mathbb{R}^3$ the collection of whose balls has infinite VC dimension. version:2
arxiv-1204-2296 | Co-clustering for directed graphs: the Stochastic co-Blockmodel and spectral algorithm Di-Sim | http://arxiv.org/abs/1204.2296 | id:1204.2296 author:Karl Rohe, Tai Qin, Bin Yu category:stat.ML math.ST stat.TH  published:2012-04-10 summary:Directed graphs have asymmetric connections, yet the current graph clustering methodologies cannot identify the potentially global structure of these asymmetries. We give a spectral algorithm called di-sim that builds on a dual measure of similarity that correspond to how a node (i) sends and (ii) receives edges. Using di-sim, we analyze the global asymmetries in the networks of Enron emails, political blogs, and the c elegans neural connectome. In each example, a small subset of nodes have persistent asymmetries; these nodes send edges with one cluster, but receive edges with another cluster. Previous approaches would have assigned these asymmetric nodes to only one cluster, failing to identify their sending/receiving asymmetries. Regularization and "projection" are two steps of di-sim that are essential for spectral clustering algorithms to work in practice. The theoretical results show that these steps make the algorithm weakly consistent under the degree corrected Stochastic co-Blockmodel, a model that generalizes the Stochastic Blockmodel to allow for both (i) degree heterogeneity and (ii) the global asymmetries that we intend to detect. The theoretical results make no assumptions on the smallest degree nodes. Instead, the theorem requires that the average degree grows sufficiently fast and that the weak consistency only applies to the subset of the nodes with sufficiently large leverage scores. The results results also apply to bipartite graphs. version:2
arxiv-1501-01617 | A Conditional Dependence Measure with Applications to Undirected Graphical Models | http://arxiv.org/abs/1501.01617 | id:1501.01617 author:Jianqing Fan, Yang Feng, Lucy Xia category:stat.ME math.ST stat.AP stat.ML stat.TH  published:2015-01-07 summary:Measuring conditional dependence is an important topic in statistics with broad applications including graphical models. Under a factor model setting, a new conditional dependence measure is proposed. The measure is derived by using distance covariance after adjusting the common observable factors or covariates. The corresponding conditional independence test is given with the asymptotic null distribution unveiled. The latter gives a somewhat surprising result: the estimating errors in factor loading matrices, while of root$-n$ order, do not have material impact on the asymptotic null distribution of the test statistic, which is also in the root$-n$ domain. It is also shown that the new test has strict control over the asymptotic significance level and can be calculated efficiently. A generic method for building dependency graphs using the new test is elaborated. Numerical results and real data analysis show the superiority of the new method. version:2
arxiv-1501-01924 | Less is More: Building Selective Anomaly Ensembles | http://arxiv.org/abs/1501.01924 | id:1501.01924 author:Shebuti Rayana, Leman Akoglu category:cs.DB cs.LG  published:2015-01-08 summary:Ensemble techniques for classification and clustering have long proven effective, yet anomaly ensembles have been barely studied. In this work, we tap into this gap and propose a new ensemble approach for anomaly mining, with application to event detection in temporal graphs. Our method aims to combine results from heterogeneous detectors with varying outputs, and leverage the evidence from multiple sources to yield better performance. However, trusting all the results may deteriorate the overall ensemble accuracy, as some detectors may fall short and provide inaccurate results depending on the nature of the data in hand. This suggests that being selective in which results to combine is vital in building effective ensembles---hence "less is more". In this paper we propose SELECT; an ensemble approach for anomaly mining that employs novel techniques to automatically and systematically select the results to assemble in a fully unsupervised fashion. We apply our method to event detection in temporal graphs, where SELECT successfully utilizes five base detectors and seven consensus methods under a unified ensemble framework. We provide extensive quantitative evaluation of our approach on five real-world datasets (four with ground truth), including Enron email communications, New York Times news corpus, and World Cup 2014 Twitter news feed. Thanks to its selection mechanism, SELECT yields superior performance compared to individual detectors alone, the full ensemble (naively combining all results), and an existing diversity-based ensemble. version:1
arxiv-1501-01894 | Quantifying Scripts: Defining metrics of characters for quantitative and descriptive analysis | http://arxiv.org/abs/1501.01894 | id:1501.01894 author:Vinodh Rajan category:cs.CL  published:2015-01-08 summary:Analysis of scripts plays an important role in paleography and in quantitative linguistics. Especially in the field of digital paleography quantitative features are much needed to differentiate glyphs. We describe an elaborate set of metrics that quantify qualitative information contained in characters and hence indirectly also quantify the scribal features. We broadly divide the metrics into several categories and describe each individual metric with its underlying qualitative significance. The metrics are largely derived from the related area of gesture design and recognition. We also propose several novel metrics. The proposed metrics are soundly grounded on the principles of handwriting production and handwriting analysis. These computed metrics could serve as descriptors for scripts and also be used for comparing and analyzing scripts. We illustrate some quantitative analysis based on the proposed metrics by applying it to the paleographic evolution of the medieval Tamil script from Brahmi. We also outline future work. version:1
arxiv-1501-01866 | The Hebrew Bible as Data: Laboratory - Sharing - Experiences | http://arxiv.org/abs/1501.01866 | id:1501.01866 author:Dirk Roorda category:cs.CL cs.DL  published:2015-01-08 summary:The systematic study of ancient texts including their production, transmission and interpretation is greatly aided by the digital methods that started taking off in the 1970s. But how is that research in turn transmitted to new generations of researchers? We tell a story of Bible and computer across the decades and then point out the current challenges: (1) finding a stable data representation for changing methods of computation; (2) sharing results in inter- and intra-disciplinary ways, for reproducibility and cross-fertilization. We report recent developments in meeting these challenges. The scene is the text database of the Hebrew Bible, constructed by the Eep Talstra Centre for Bible and Computer (ETCBC), which is still growing in detail and sophistication. We show how a subtle mix of computational ingredients enable scholars to research the transmission and interpretation of the Hebrew Bible in new ways: (1) a standard data format, Linguistic Annotation Framework (LAF); (2) the methods of scientific computing, made accessible by (interactive) Python and its associated ecosystem. Additionally, we show how these efforts have culminated in the construction of a new, publicly accessible search engine SHEBANQ, where the text of the Hebrew Bible and its underlying data can be queried in a simple, yet powerful query language MQL, and where those queries can be saved and shared. version:1
arxiv-1501-01744 | Optimal Radiometric Calibration for Camera-Display Communication | http://arxiv.org/abs/1501.01744 | id:1501.01744 author:Wenjia Yuan, Eric Wengrowski, Kristin J. Dana, Ashwin Ashok, Marco Gruteser, Narayan Mandayam category:cs.CV  published:2015-01-08 summary:We present a novel method for communicating between a camera and display by embedding and recovering hidden and dynamic information within a displayed image. A handheld camera pointed at the display can receive not only the display image, but also the underlying message. These active scenes are fundamentally different from traditional passive scenes like QR codes because image formation is based on display emittance, not surface reflectance. Detecting and decoding the message requires careful photometric modeling for computational message recovery. Unlike standard watermarking and steganography methods that lie outside the domain of computer vision, our message recovery algorithm uses illumination to optically communicate hidden messages in real world scenes. The key innovation of our approach is an algorithm that performs simultaneous radiometric calibration and message recovery in one convex optimization problem. By modeling the photometry of the system using a camera-display transfer function (CDTF), we derive a physics-based kernel function for support vector machine classification. We demonstrate that our method of optimal online radiometric calibration (OORC) leads to an efficient and robust algorithm for computational messaging between nine commercial cameras and displays. version:1
arxiv-1501-01723 | An Effective Image Feature Classiffication using an improved SOM | http://arxiv.org/abs/1501.01723 | id:1501.01723 author:M. Abdelsamea, Marghny H. Mohamed, Mohamed Bamatraf category:cs.CV  published:2015-01-08 summary:Image feature classification is a challenging problem in many computer vision applications, specifically, in the fields of remote sensing, image analysis and pattern recognition. In this paper, a novel Self Organizing Map, termed improved SOM (iSOM), is proposed with the aim of effectively classifying Mammographic images based on their texture feature representation. The main contribution of the iSOM is to introduce a new node structure for the map representation and adopting a learning technique based on Kohonen SOM accordingly. The main idea is to control, in an unsupervised fashion, the weight updating procedure depending on the class reliability of the node, during the weight update time. Experiments held on a real Mammographic images. Results showed high accuracy compared to classical SOM and other state-of-art classifiers. version:1
arxiv-1501-01689 | Sparse Solutions to Nonnegative Linear Systems and Applications | http://arxiv.org/abs/1501.01689 | id:1501.01689 author:Aditya Bhaskara, Ananda Theertha Suresh, Morteza Zadimoghaddam category:cs.DS cs.IT cs.LG math.IT  published:2015-01-07 summary:We give an efficient algorithm for finding sparse approximate solutions to linear systems of equations with nonnegative coefficients. Unlike most known results for sparse recovery, we do not require {\em any} assumption on the matrix other than non-negativity. Our algorithm is combinatorial in nature, inspired by techniques for the set cover problem, as well as the multiplicative weight update method. We then present a natural application to learning mixture models in the PAC framework. For learning a mixture of $k$ axis-aligned Gaussians in $d$ dimensions, we give an algorithm that outputs a mixture of $O(k/\epsilon^3)$ Gaussians that is $\epsilon$-close in statistical distance to the true distribution, without any separation assumptions. The time and sample complexity is roughly $O(kd/\epsilon^3)^{d}$. This is polynomial when $d$ is constant -- precisely the regime in which known methods fail to identify the components efficiently. Given that non-negativity is a natural assumption, we believe that our result may find use in other settings in which we wish to approximately explain data using a small number of a (large) candidate set of components. version:1
arxiv-1501-01571 | An Introduction to Matrix Concentration Inequalities | http://arxiv.org/abs/1501.01571 | id:1501.01571 author:Joel A. Tropp category:math.PR cs.DS cs.IT cs.NA math.IT stat.ML  published:2015-01-07 summary:In recent years, random matrices have come to play a major role in computational mathematics, but most of the classical areas of random matrix theory remain the province of experts. Over the last decade, with the advent of matrix concentration inequalities, research has advanced to the point where we can conquer many (formerly) challenging problems with a page or two of arithmetic. The aim of this monograph is to describe the most successful methods from this area along with some interesting examples that these techniques can illuminate. version:1
arxiv-1412-4846 | Scaling laws in human speech, decreasing emergence of new words and a generalized model | http://arxiv.org/abs/1412.4846 | id:1412.4846 author:Ruokuang Lin, Qianli D. Y. Ma, Chunhua Bian category:cs.CL physics.data-an  published:2014-12-16 summary:Human language, as a typical complex system, its organization and evolution is an attractive topic for both physical and cultural researchers. In this paper, we present the first exhaustive analysis of the text organization of human speech. Two important results are that: (i) the construction and organization of spoken language can be characterized as Zipf's law and Heaps' law, as observed in written texts; (ii) word frequency vs. rank distribution and the growth of distinct words with the increase of text length shows significant differences between book and speech. In speech word frequency distribution are more concentrated on higher frequency words, and the emergence of new words decreases much rapidly when the content length grows. Based on these observations, a new generalized model is proposed to explain these complex dynamical behaviors and the differences between speech and book. version:2
arxiv-1501-01457 | Comparison of Selection Methods in On-line Distributed Evolutionary Robotics | http://arxiv.org/abs/1501.01457 | id:1501.01457 author:Iñaki Fernández Pérez, Amine Boumaza, François Charpillet category:cs.AI cs.MA cs.NE cs.RO  published:2015-01-07 summary:In this paper, we study the impact of selection methods in the context of on-line on-board distributed evolutionary algorithms. We propose a variant of the mEDEA algorithm in which we add a selection operator, and we apply it in a taskdriven scenario. We evaluate four selection methods that induce different intensity of selection pressure in a multi-robot navigation with obstacle avoidance task and a collective foraging task. Experiments show that a small intensity of selection pressure is sufficient to rapidly obtain good performances on the tasks at hand. We introduce different measures to compare the selection methods, and show that the higher the selection pressure, the better the performances obtained, especially for the more challenging food foraging task. version:1
arxiv-1501-01386 | Roman Urdu Opinion Mining System (RUOMiS) | http://arxiv.org/abs/1501.01386 | id:1501.01386 author:Misbah Daud, Rafiullah Khan, Mohibullah, Aitazaz Daud category:cs.CL cs.IR  published:2015-01-07 summary:Convincing a customer is always considered as a challenging task in every business. But when it comes to online business, this task becomes even more difficult. Online retailers try everything possible to gain the trust of the customer. One of the solutions is to provide an area for existing users to leave their comments. This service can effectively develop the trust of the customer however normally the customer comments about the product in their native language using Roman script. If there are hundreds of comments this makes difficulty even for the native customers to make a buying decision. This research proposes a system which extracts the comments posted in Roman Urdu, translate them, find their polarity and then gives us the rating of the product. This rating will help the native and non-native customers to make buying decision efficiently from the comments posted in Roman Urdu. version:1
arxiv-1501-01364 | Leader Follower Formation Control of Ground Vehicles Using Camshift Based Guidance | http://arxiv.org/abs/1501.01364 | id:1501.01364 author:S. M. Vaitheeswaran, Bharath M. K., Gokul M category:cs.CV  published:2015-01-07 summary:Autonomous ground vehicles have been designed for the purpose of that relies on ranging and bearing information received from forward looking camera on the Formation control . A visual guidance control algorithm is designed where real time image processing is used to provide feedback signals. The vision subsystem and control subsystem work in parallel to accomplish formation control. A proportional navigation and line of sight guidance laws are used to estimate the range and bearing information from the leader vehicle using the vision subsystem. The algorithms for vision detection and localization used here are similar to approaches for many computer vision tasks such as face tracking and detection that are based color-and texture based features, and non-parametric Continuously Adaptive Mean-shift algorithms to keep track of the leader. This is being proposed for the first time in the leader follower framework. The algorithms are simple but effective for real time and provide an alternate approach to traditional based approaches like the Viola Jones algorithm. Further to stabilize the follower to the leader trajectory, the sliding mode controller is used to dynamically track the leader. The performance of the results is demonstrated in simulation and in practical experiments. version:1
arxiv-1501-01348 | Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data | http://arxiv.org/abs/1501.01348 | id:1501.01348 author:Lee Zamparo, Zhaolei Zhang category:cs.LG  published:2015-01-07 summary:High-content screening uses large collections of unlabeled cell image data to reason about genetics or cell biology. Two important tasks are to identify those cells which bear interesting phenotypes, and to identify sub-populations enriched for these phenotypes. This exploratory data analysis usually involves dimensionality reduction followed by clustering, in the hope that clusters represent a phenotype. We propose the use of stacked de-noising auto-encoders to perform dimensionality reduction for high-content screening. We demonstrate the superior performance of our approach over PCA, Local Linear Embedding, Kernel PCA and Isomap. version:1
arxiv-1406-7250 | Reconstructing subclonal composition and evolution from whole genome sequencing of tumors | http://arxiv.org/abs/1406.7250 | id:1406.7250 author:Amit G. Deshwar, Shankar Vembu, Christina K. Yung, Gun Ho Jang, Lincoln Stein, Quaid Morris category:q-bio.PE cs.LG stat.ML  published:2014-06-27 summary:Tumors often contain multiple subpopulations of cancerous cells defined by distinct somatic mutations. We describe a new method, PhyloWGS, that can be applied to WGS data from one or more tumor samples to reconstruct complete genotypes of these subpopulations based on variant allele frequencies (VAFs) of point mutations and population frequencies of structural variations. We introduce a principled phylogenic correction for VAFs in loci affected by copy number alterations and we show that this correction greatly improves subclonal reconstruction compared to existing methods. version:3
arxiv-1501-01321 | ITCM: A Real Time Internet Traffic Classifier Monitor | http://arxiv.org/abs/1501.01321 | id:1501.01321 author:Silas Santiago Lopes Pereira, José Everardo Bessa Maia, Jorge Luiz de Castro e Silva category:cs.NI cs.LG  published:2015-01-06 summary:The continual growth of high speed networks is a challenge for real-time network analysis systems. The real time traffic classification is an issue for corporations and ISPs (Internet Service Providers). This work presents the design and implementation of a real time flow-based network traffic classification system. The classifier monitor acts as a pipeline consisting of three modules: packet capture and pre-processing, flow reassembly, and classification with Machine Learning (ML). The modules are built as concurrent processes with well defined data interfaces between them so that any module can be improved and updated independently. In this pipeline, the flow reassembly function becomes the bottleneck of the performance. In this implementation, was used a efficient method of reassembly which results in a average delivery delay of 0.49 seconds, approximately. For the classification module, the performances of the K-Nearest Neighbor (KNN), C4.5 Decision Tree, Naive Bayes (NB), Flexible Naive Bayes (FNB) and AdaBoost Ensemble Learning Algorithm are compared in order to validate our approach. version:1
arxiv-1501-01318 | Arabic Text Categorization Algorithm using Vector Evaluation Method | http://arxiv.org/abs/1501.01318 | id:1501.01318 author:Ashraf Odeh, Aymen Abu-Errub, Qusai Shambour, Nidal Turab category:cs.IR cs.CL  published:2015-01-06 summary:Text categorization is the process of grouping documents into categories based on their contents. This process is important to make information retrieval easier, and it became more important due to the huge textual information available online. The main problem in text categorization is how to improve the classification accuracy. Although Arabic text categorization is a new promising field, there are a few researches in this field. This paper proposes a new method for Arabic text categorization using vector evaluation. The proposed method uses a categorized Arabic documents corpus, and then the weights of the tested document's words are calculated to determine the document keywords which will be compared with the keywords of the corpus categorizes to determine the tested document's best category. version:1
arxiv-1501-01254 | Unknown Words Analysis in POS tagging of Sinhala Language | http://arxiv.org/abs/1501.01254 | id:1501.01254 author:A. J. P. M. P. Jayaweera, N. G. J. Dias category:cs.CL I.2.7  published:2015-01-06 summary:Part of Speech (POS) is a very vital topic in Natural Language Processing (NLP) task in any language, which involves analysing the construction of the language, behaviours and the dynamics of the language, the knowledge that could be utilized in computational linguistics analysis and automation applications. In this context, dealing with unknown words (words do not appear in the lexicon referred as unknown words) is also an important task, since growing NLP systems are used in more and more new applications. One aid of predicting lexical categories of unknown words is the use of syntactical knowledge of the language. The distinction between open class words and closed class words together with syntactical features of the language used in this research to predict lexical categories of unknown words in the tagging process. An experiment is performed to investigate the ability of the approach to parse unknown words using syntactical knowledge without human intervention. This experiment shows that the performance of the tagging process is enhanced when word class distinction is used together with syntactic rules to parse sentences containing unknown words in Sinhala language. version:1
arxiv-1501-01252 | Optimisation using Natural Language Processing: Personalized Tour Recommendation for Museums | http://arxiv.org/abs/1501.01252 | id:1501.01252 author:Mayeul Mathias, Assema Moussa, Fen Zhou, Juan-Manuel Torres-Moreno, Marie-Sylvie Poli, Didier Josselin, Marc El-Bèze, Andréa Carneiro Linhares, Francoise Rigat category:cs.AI cs.CL  published:2015-01-06 summary:This paper proposes a new method to provide personalized tour recommendation for museum visits. It combines an optimization of preference criteria of visitors with an automatic extraction of artwork importance from museum information based on Natural Language Processing using textual energy. This project includes researchers from computer and social sciences. Some results are obtained with numerical experiments. They show that our model clearly improves the satisfaction of the visitor who follows the proposed tour. This work foreshadows some interesting outcomes and applications about on-demand personalized visit of museums in a very near future. version:1
arxiv-1501-01243 | Un résumeur à base de graphes, indépéndant de la langue | http://arxiv.org/abs/1501.01243 | id:1501.01243 author:Juan-Manuel Torres-Moreno, Javier Ramirez, Iria da Cunha category:cs.CL  published:2015-01-06 summary:In this paper we present REG, a graph-based approach for study a fundamental problem of Natural Language Processing (NLP): the automatic text summarization. The algorithm maps a document as a graph, then it computes the weight of their sentences. We have applied this approach to summarize documents in three languages. version:1
arxiv-1603-09712 | Analog Signal Processing Approach for Coarse and Fine Depth Estimation | http://arxiv.org/abs/1603.09712 | id:1603.09712 author:Nihar Athreyas, Zhiguo Lai, Jai Gupta, Dev Gupta category:cs.CV  published:2015-01-06 summary:Imaging and Image sensors is a field that is continuously evolving. There are new products coming into the market every day. Some of these have very severe Size, Weight and Power constraints whereas other devices have to handle very high computational loads. Some require both these conditions to be met simultaneously. Current imaging architectures and digital image processing solutions will not be able to meet these ever increasing demands. There is a need to develop novel imaging architectures and image processing solutions to address these requirements. In this work we propose analog signal processing as a solution to this problem. The analog processor is not suggested as a replacement to a digital processor but it will be used as an augmentation device which works in parallel with the digital processor, making the system faster and more efficient. In order to show the merits of analog processing two stereo correspondence algorithms are implemented. We propose novel modifications to the algorithms and new imaging architectures which, significantly reduces the computation time. version:1
arxiv-1407-2602 | Compressed sensing for longitudinal MRI: An adaptive-weighted approach | http://arxiv.org/abs/1407.2602 | id:1407.2602 author:Lior Weizman, Yonina C. Eldar, Dafna Ben Bashat category:physics.med-ph cs.CV  published:2014-07-10 summary:Purpose: Repeated brain MRI scans are performed in many clinical scenarios, such as follow up of patients with tumors and therapy response assessment. In this paper, the authors show an approach to utilize former scans of the patient for the acceleration of repeated MRI scans. Methods: The proposed approach utilizes the possible similarity of the repeated scans in longitudinal MRI studies. Since similarity is not guaranteed, sampling and reconstruction are adjusted during acquisition to match the actual similarity between the scans. The baseline MR scan is utilized both in the sampling stage, via adaptive sampling, and in the reconstruction stage, with weighted reconstruction. In adaptive sampling, k-space sampling locations are optimized during acquisition. Weighted reconstruction uses the locations of the nonzero coefficients in the sparse domains as a prior in the recovery process. The approach was tested on 2D and 3D MRI scans of patients with brain tumors. Results: The longitudinal adaptive CS MRI (LACS-MRI) scheme provides reconstruction quality which outperforms other CS-based approaches for rapid MRI. Examples are shown on patients with brain tumors and demonstrate improved spatial resolution. Compared with data sampled at Nyquist rate, LACS-MRI exhibits Signal-to-Error Ratio (SER) of 24.8dB with undersampling factor of 16.6 in 3D MRI. Conclusions: The authors have presented a novel method for image reconstruction utilizing similarity of scans in longitudinal MRI studies, where possible. The proposed approach can play a major part and significantly reduce scanning time in many applications that consist of disease follow-up and monitoring of longitudinal changes in brain MRI. version:3
arxiv-1412-7625 | An Effective Semi-supervised Divisive Clustering Algorithm | http://arxiv.org/abs/1412.7625 | id:1412.7625 author:Teng Qiu, Yongjie Li category:cs.LG cs.CV stat.ML  published:2014-12-24 summary:Nowadays, data are generated massively and rapidly from scientific fields as bioinformatics, neuroscience and astronomy to business and engineering fields. Cluster analysis, as one of the major data analysis tools, is therefore more significant than ever. We propose in this work an effective Semi-supervised Divisive Clustering algorithm (SDC). Data points are first organized by a minimal spanning tree. Next, this tree structure is transitioned to the in-tree structure, and then divided into sub-trees under the supervision of the labeled data, and in the end, all points in the sub-trees are directly associated with specific cluster centers. SDC is fully automatic, non-iterative, involving no free parameter, insensitive to noise, able to detect irregularly shaped cluster structures, applicable to the data sets of high dimensionality and different attributes. The power of SDC is demonstrated on several datasets. version:2
arxiv-1501-01106 | A Study on Clustering for Clustering Based Image De-Noising | http://arxiv.org/abs/1501.01106 | id:1501.01106 author:Hossein Bakhshi Golestani, Mohsen Joneidi, Mostafa Sadeghi category:cs.CV  published:2015-01-06 summary:In this paper, the problem of de-noising of an image contaminated with Additive White Gaussian Noise (AWGN) is studied. This subject is an open problem in signal processing for more than 50 years. Local methods suggested in recent years, have obtained better results than global methods. However by more intelligent training in such a way that first, important data is more effective for training, second, clustering in such way that training blocks lie in low-rank subspaces, we can design a dictionary applicable for image de-noising and obtain results near the state of the art local methods. In the present paper, we suggest a method based on global clustering of image constructing blocks. As the type of clustering plays an important role in clustering-based de-noising methods, we address two questions about the clustering. The first, which parts of the data should be considered for clustering? and the second, what data clustering method is suitable for de-noising.? Then clustering is exploited to learn an over complete dictionary. By obtaining sparse decomposition of the noisy image blocks in terms of the dictionary atoms, the de-noised version is achieved. In addition to our framework, 7 popular dictionary learning methods are simulated and compared. The results are compared based on two major factors: (1) de-noising performance and (2) execution time. Experimental results show that our dictionary learning framework outperforms its competitors in terms of both factors. version:1
arxiv-1501-01090 | A Novel Technique for Grading of Dates using Shape and Texture Features | http://arxiv.org/abs/1501.01090 | id:1501.01090 author:S. H. Mohana, C. J. Prabhakar category:cs.CV  published:2015-01-06 summary:This paper presents a novel method to grade the date fruits based on the combination of shape and texture features. The method begins with reducing the specular reflection and small noise using a bilateral filter. Threshold based segmentation is performed for background removal and fruit part selection from the given image. Shape features is extracted using the contour of the date fruit and texture features are extracted using Curvelet transform and Local Binary Pattern (LBP) from the selected date fruit region. Finally, combinations of shape and texture features are fused to grade the dates into six grades. k-Nearest Neighbour(k-NN) classifier yields the best grading rate compared to other two classifiers such as Support Vector Machine (SVM) and Linear Discriminant(LDA) classifiers. The experiment result shows that our technique achieves highest accuracy. version:1
arxiv-1501-01083 | Stem-Calyx Recognition of an Apple using Shape Descriptors | http://arxiv.org/abs/1501.01083 | id:1501.01083 author:S. H. Mohana, C. J. Prabhakar category:cs.CV  published:2015-01-06 summary:This paper presents a novel method to recognize stem - calyx of an apple using shape descriptors. The main drawback of existing apple grading techniques is that stem - calyx part of an apple is treated as defects, this leads to poor grading of apples. In order to overcome this drawback, we proposed an approach to recognize stem-calyx and differentiated from true defects based on shape features. Our method comprises of steps such as segmentation of apple using grow-cut method, candidate objects such as stem-calyx and small defects are detected using multi-threshold segmentation. The shape features are extracted from detected objects using Multifractal, Fourier and Radon descriptor and finally stem-calyx regions are recognized and differentiated from true defects using SVM classifier. The proposed algorithm is evaluated using experiments conducted on apple image dataset and results exhibit considerable improvement in recognition of stem-calyx region compared to other techniques. version:1
arxiv-1501-02246 | The Effect of Wedge Tip Angles on Stress Intensity Factors in the Contact Problem between Tilted Wedge and a Half Plane with an Edge Crack Using Digital Image Correlation | http://arxiv.org/abs/1501.02246 | id:1501.02246 author:Seyedmeysam Khaleghian, Anahita Emami, Mohammad Yadegari, Nasser Soltani category:cond-mat.mtrl-sci cs.CV physics.optics  published:2015-01-06 summary:The first and second mode stress intensity factors (SIFs) of a contact problem between a half-plane with an edge crack and an asymmetric tilted wedge were obtained using experimental method of Digital Image Correlation (DIC). In this technique, displacement and strain fields can be measured using two digital images of the same sample at different stages of loading. However, several images were taken consequently in each stage of this experiment to avoid the noise effect. A pair of images of each stage was compared to each other. Then, the correlation coefficients between them were studied using a computer code. The pairs with the correlation coefficient higher than 0.8 were selected as the acceptable match for displacement measurements near the crack tip. Subsequently, the SIFs of specimens were calculated using displacement fields obtained from DIC method. The effect of wedge tips angle on their SIFs was also studied. Moreover, the results of DIC method were compared with the results of photoelasticity method and a close agreement between them was observed. version:1
arxiv-1501-01075 | Skincure: An Innovative Smart Phone-Based Application To Assist In Melanoma Early Detection And Prevention | http://arxiv.org/abs/1501.01075 | id:1501.01075 author:Omar Abuzaghleh, Miad Faezipour, Buket D. Barkana category:cs.CV cs.CY  published:2015-01-06 summary:Melanoma spreads through metastasis, and therefore it has been proven to be very fatal. Statistical evidence has revealed that the majority of deaths resulting from skin cancer are as a result of melanoma. Further investigations have shown that the survival rates in patients depend on the stage of the infection; early detection and intervention of melanoma implicates higher chances of cure. Clinical diagnosis and prognosis of melanoma is challenging since the processes are prone to misdiagnosis and inaccuracies due to doctors subjectivity. This paper proposes an innovative and fully functional smart-phone based application to assist in melanoma early detection and prevention. The application has two major components; the first component is a real-time alert to help users prevent skin burn caused by sunlight; a novel equation to compute the time for skin to burn is thereby introduced. The second component is an automated image analysis module which contains image acquisition, hair detection and exclusion, lesion segmentation, feature extraction, and classification. The proposed system exploits PH2 Dermoscopy image database from Pedro Hispano Hospital for development and testing purposes. The image database contains a total of 200 dermoscopy images of lesions, including normal, atypical, and melanoma cases. The experimental results show that the proposed system is efficient, achieving classification of the normal, atypical and melanoma images with accuracy of 96.3%, 95.7% and 97.5%, respectively. version:1
arxiv-1210-3335 | Improved Graph Clustering | http://arxiv.org/abs/1210.3335 | id:1210.3335 author:Yudong Chen, Sujay Sanghavi, Huan Xu category:stat.ML  published:2012-10-11 summary:Graph clustering involves the task of dividing nodes into clusters, so that the edge density is higher within clusters as opposed to across clusters. A natural, classic and popular statistical setting for evaluating solutions to this problem is the stochastic block model, also referred to as the planted partition model. In this paper we present a new algorithm--a convexified version of Maximum Likelihood--for graph clustering. We show that, in the classic stochastic block model setting, it outperforms existing methods by polynomial factors when the cluster size is allowed to have general scalings. In fact, it is within logarithmic factors of known lower bounds for spectral methods, and there is evidence suggesting that no polynomial time algorithm would do significantly better. We then show that this guarantee carries over to a more general extension of the stochastic block model. Our method can handle the settings of semi-random graphs, heterogeneous degree distributions, unequal cluster sizes, unaffiliated nodes, partially observed graphs and planted clique/coloring etc. In particular, our results provide the best exact recovery guarantees to date for the planted partition, planted k-disjoint-cliques and planted noisy coloring models with general cluster sizes; in other settings, we match the best existing results up to logarithmic factors. version:3
arxiv-1501-02741 | Salient Object Detection: A Benchmark | http://arxiv.org/abs/1501.02741 | id:1501.02741 author:Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li category:cs.CV  published:2015-01-05 summary:We extensively compare, qualitatively and quantitatively, 40 state-of-the-art models (28 salient object detection, 10 fixation prediction, 1 objectness, and 1 baseline) over 6 challenging datasets for the purpose of benchmarking salient object detection and segmentation methods. From the results obtained so far, our evaluation shows a consistent rapid progress over the last few years in terms of both accuracy and running time. The top contenders in this benchmark significantly outperform the models identified as the best in the previous benchmark conducted just two years ago. We find that the models designed specifically for salient object detection generally work better than models in closely related areas, which in turn provides a precise definition and suggests an appropriate treatment of this problem that distinguishes it from other problems. In particular, we analyze the influences of center bias and scene complexity in model performance, which, along with the hard cases for state-of-the-art models, provide useful hints towards constructing more challenging large scale datasets and better saliency models. Finally, we propose probable solutions for tackling several open problems such as evaluation scores and dataset bias, which also suggest future research directions in the rapidly-growing field of salient object detection. version:1
arxiv-1501-00909 | Adaptive Objectness for Object Tracking | http://arxiv.org/abs/1501.00909 | id:1501.00909 author:Pengpeng Liang, Chunyuan Liao, Xue Mei, Haibin Ling category:cs.CV  published:2015-01-05 summary:Object tracking is a long standing problem in vision. While great efforts have been spent to improve tracking performance, a simple yet reliable prior knowledge is left unexploited: the target object in tracking must be an object other than non-object. The recently proposed and popularized objectness measure provides a natural way to model such prior in visual tracking. Thus motivated, in this paper we propose to adapt objectness for visual object tracking. Instead of directly applying an existing objectness measure that is generic and handles various objects and environments, we adapt it to be compatible to the specific tracking sequence and object. More specifically, we use the newly proposed BING objectness as the base, and then train an object-adaptive objectness for each tracking task. The training is implemented by using an adaptive support vector machine that integrates information from the specific tracking target into the BING measure. We emphasize that the benefit of the proposed adaptive objectness, named ADOBING, is generic. To show this, we combine ADOBING with seven top performed trackers in recent evaluations. We run the ADOBING-enhanced trackers with their base trackers on two popular benchmarks, the CVPR2013 benchmark (50 sequences) and the Princeton Tracking Benchmark (100 sequences). On both benchmarks, our methods not only consistently improve the base trackers, but also achieve the best known performances. Noting that the way we integrate objectness in visual tracking is generic and straightforward, we expect even more improvement by using tracker-specific objectness. version:1
arxiv-1209-2669 | Likelihood Estimation with Incomplete Array Variate Observations | http://arxiv.org/abs/1209.2669 | id:1209.2669 author:Deniz Akdemir category:stat.ME math.ST stat.ML stat.TH  published:2012-09-12 summary:Missing data is an important challenge when dealing with high dimensional data arranged in the form of an array. In this paper, we propose methods for estimation of the parameters of array variate normal probability model from partially observed multiway data. The methods developed here are useful for missing data imputation, estimation of mean and covariance parameters for multiway data. A multiway semi-parametric mixed effects model that allows separation of multiway covariance effects is also defined and an efficient algorithm for estimation is recommended. We provide simulation results along with real life data from genetics to demonstrate these methods. version:9
arxiv-1501-00857 | Fast forward feature selection for the nonlinear classification of hyperspectral images | http://arxiv.org/abs/1501.00857 | id:1501.00857 author:Mathieu Fauvel, Clement Dechesne, Anthony Zullo, Frédéric Ferraty category:cs.CV  published:2015-01-05 summary:A fast forward feature selection algorithm is presented in this paper. It is based on a Gaussian mixture model (GMM) classifier. GMM are used for classifying hyperspectral images. The algorithm selects iteratively spectral features that maximizes an estimation of the classification rate. The estimation is done using the k-fold cross validation. In order to perform fast in terms of computing time, an efficient implementation is proposed. First, the GMM can be updated when the estimation of the classification rate is computed, rather than re-estimate the full model. Secondly, using marginalization of the GMM, sub models can be directly obtained from the full model learned with all the spectral features. Experimental results for two real hyperspectral data sets show that the method performs very well in terms of classification accuracy and processing time. Furthermore, the extracted model contains very few spectral channels. version:1
arxiv-1412-6630 | Neural Network Regularization via Robust Weight Factorization | http://arxiv.org/abs/1412.6630 | id:1412.6630 author:Jan Rudy, Weiguang Ding, Daniel Jiwoong Im, Graham W. Taylor category:cs.LG cs.NE stat.ML  published:2014-12-20 summary:Regularization is essential when training large neural networks. As deep neural networks can be mathematically interpreted as universal function approximators, they are effective at memorizing sampling noise in the training data. This results in poor generalization to unseen data. Therefore, it is no surprise that a new regularization technique, Dropout, was partially responsible for the now-ubiquitous winning entry to ImageNet 2012 by the University of Toronto. Currently, Dropout (and related methods such as DropConnect) are the most effective means of regularizing large neural networks. These amount to efficiently visiting a large number of related models at training time, while aggregating them to a single predictor at test time. The proposed FaMe model aims to apply a similar strategy, yet learns a factorization of each weight matrix such that the factors are robust to noise. version:2
arxiv-1501-00841 | Chasing the Ghosts of Ibsen: A computational stylistic analysis of drama in translation | http://arxiv.org/abs/1501.00841 | id:1501.00841 author:Gerard Lynch, Carl Vogel category:cs.CL  published:2015-01-05 summary:Research into the stylistic properties of translations is an issue which has received some attention in computational stylistics. Previous work by Rybicki (2006) on the distinguishing of character idiolects in the work of Polish author Henryk Sienkiewicz and two corresponding English translations using Burrow's Delta method concluded that idiolectal differences could be observed in the source texts and this variation was preserved to a large degree in both translations. This study also found that the two translations were also highly distinguishable from one another. Burrows (2002) examined English translations of Juvenal also using the Delta method, results of this work suggest that some translators are more adept at concealing their own style when translating the works of another author whereas other authors tend to imprint their own style to a greater extent on the work they translate. Our work examines the writing of a single author, Norwegian playwright Henrik Ibsen, and these writings translated into both German and English from Norwegian, in an attempt to investigate the preservation of characterization, defined here as the distinctiveness of textual contributions of characters. version:1
arxiv-1501-00834 | Inverse Renormalization Group Transformation in Bayesian Image Segmentations | http://arxiv.org/abs/1501.00834 | id:1501.00834 author:Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Masayuki Ohzeki category:cs.CV cond-mat.stat-mech stat.ML  published:2015-01-05 summary:A new Bayesian image segmentation algorithm is proposed by combining a loopy belief propagation with an inverse real space renormalization group transformation to reduce the computational time. In results of our experiment, we observe that the proposed method can reduce the computational time to less than one-tenth of that taken by conventional Bayesian approaches. version:1
arxiv-1501-00825 | Group $K$-Means | http://arxiv.org/abs/1501.00825 | id:1501.00825 author:Jianfeng Wang, Shuicheng Yan, Yi Yang, Mohan S Kankanhalli, Shipeng Li, Jingdong Wang category:cs.CV  published:2015-01-05 summary:We study how to learn multiple dictionaries from a dataset, and approximate any data point by the sum of the codewords each chosen from the corresponding dictionary. Although theoretically low approximation errors can be achieved by the global solution, an effective solution has not been well studied in practice. To solve the problem, we propose a simple yet effective algorithm \textit{Group $K$-Means}. Specifically, we take each dictionary, or any two selected dictionaries, as a group of $K$-means cluster centers, and then deal with the approximation issue by minimizing the approximation errors. Besides, we propose a hierarchical initialization for such a non-convex problem. Experimental results well validate the effectiveness of the approach. version:1
arxiv-1501-00777 | Sparse Deep Stacking Network for Image Classification | http://arxiv.org/abs/1501.00777 | id:1501.00777 author:Jun Li, Heyou Chang, Jian Yang category:cs.CV cs.LG cs.NE  published:2015-01-05 summary:Sparse coding can learn good robust representation to noise and model more higher-order representation for image classification. However, the inference algorithm is computationally expensive even though the supervised signals are used to learn compact and discriminative dictionaries in sparse coding techniques. Luckily, a simplified neural network module (SNNM) has been proposed to directly learn the discriminative dictionaries for avoiding the expensive inference. But the SNNM module ignores the sparse representations. Therefore, we propose a sparse SNNM module by adding the mixed-norm regularization (l1/l2 norm). The sparse SNNM modules are further stacked to build a sparse deep stacking network (S-DSN). In the experiments, we evaluate S-DSN with four databases, including Extended YaleB, AR, 15 scene and Caltech101. Experimental results show that our model outperforms related classification methods with only a linear classifier. It is worth noting that we reach 98.8% recognition accuracy on 15 scene. version:1
arxiv-1501-00756 | Hashing with binary autoencoders | http://arxiv.org/abs/1501.00756 | id:1501.00756 author:Miguel Á. Carreira-Perpiñán, Ramin Raziperchikolaei category:cs.LG cs.CV math.OC stat.ML  published:2015-01-05 summary:An attractive approach for fast search in image databases is binary hashing, where each high-dimensional, real-valued image is mapped onto a low-dimensional, binary vector and the search is done in this binary space. Finding the optimal hash function is difficult because it involves binary constraints, and most approaches approximate the optimization by relaxing the constraints and then binarizing the result. Here, we focus on the binary autoencoder model, which seeks to reconstruct an image from the binary code produced by the hash function. We show that the optimization can be simplified with the method of auxiliary coordinates. This reformulates the optimization as alternating two easier steps: one that learns the encoder and decoder separately, and one that optimizes the code for each image. Image retrieval experiments, using precision/recall and a measure of code utilization, show the resulting hash function outperforms or is competitive with state-of-the-art methods for binary hashing. version:1
arxiv-1401-0852 | Concave Penalized Estimation of Sparse Gaussian Bayesian Networks | http://arxiv.org/abs/1401.0852 | id:1401.0852 author:Bryon Aragam, Qing Zhou category:stat.ME cs.LG stat.ML  published:2014-01-04 summary:We develop a penalized likelihood estimation framework to estimate the structure of Gaussian Bayesian networks from observational data. In contrast to recent methods which accelerate the learning problem by restricting the search space, our main contribution is a fast algorithm for score-based structure learning which does not restrict the search space in any way and works on high-dimensional datasets with thousands of variables. Our use of concave regularization, as opposed to the more popular $\ell_0$ (e.g. BIC) penalty, is new. Moreover, we provide theoretical guarantees which generalize existing asymptotic results when the underlying distribution is Gaussian. Most notably, our framework does not require the existence of a so-called faithful DAG representation, and as a result the theory must handle the inherent nonidentifiability of the estimation problem in a novel way. Finally, as a matter of independent interest, we provide a comprehensive comparison of our approach to several standard structure learning methods using open-source packages developed for the R language. Based on these experiments, we show that our algorithm is significantly faster than other competing methods while obtaining higher sensitivity with comparable false discovery rates for high-dimensional data. In particular, the total runtime for our method to generate a solution path of 20 estimates for DAGs with 8000 nodes is around one hour. version:2
arxiv-1501-00728 | Differential Search Algorithm-based Parametric Optimization of Fuzzy Generalized Eigenvalue Proximal Support Vector Machine | http://arxiv.org/abs/1501.00728 | id:1501.00728 author:M. H. Marghny, Rasha M. Abd ElAziz, Ahmed I. Taloba category:cs.LG  published:2015-01-04 summary:Support Vector Machine (SVM) is an effective model for many classification problems. However, SVM needs the solution of a quadratic program which require specialized code. In addition, SVM has many parameters, which affects the performance of SVM classifier. Recently, the Generalized Eigenvalue Proximal SVM (GEPSVM) has been presented to solve the SVM complexity. In real world applications data may affected by error or noise, working with this data is a challenging problem. In this paper, an approach has been proposed to overcome this problem. This method is called DSA-GEPSVM. The main improvements are carried out based on the following: 1) a novel fuzzy values in the linear case. 2) A new Kernel function in the nonlinear case. 3) Differential Search Algorithm (DSA) is reformulated to find near optimal values of the GEPSVM parameters and its kernel parameters. The experimental results show that the proposed approach is able to find the suitable parameter values, and has higher classification accuracy compared with some other algorithms. version:1
arxiv-1501-00725 | A generalization error bound for sparse and low-rank multivariate Hawkes processes | http://arxiv.org/abs/1501.00725 | id:1501.00725 author:Emmanuel Bacry, Stéphane Gaïffas, Jean-François Muzy category:stat.ML  published:2015-01-04 summary:We consider the problem of unveiling the implicit network structure of user interactions in a social network, based only on high-frequency timestamps. Our inference is based on the minimization of the least-squares loss associated with a multivariate Hawkes model, penalized by $\ell_1$ and trace norms. We provide a first theoretical analysis of the generalization error for this problem, that includes sparsity and low-rank inducing priors. This result involves a new data-driven concentration inequality for matrix martingales in continuous time with observable variance, which is a result of independent interest. A consequence of our analysis is the construction of sharply tuned $\ell_1$ and trace-norm penalizations, that leads to a data-driven scaling of the variability of information available for each users. Numerical experiments illustrate the strong improvements achieved by the use of such data-driven penalizations. version:1
arxiv-1412-7828 | Protein Secondary Structure Prediction with Long Short Term Memory Networks | http://arxiv.org/abs/1412.7828 | id:1412.7828 author:Søren Kaae Sønderby, Ole Winther category:q-bio.QM cs.LG cs.NE  published:2014-12-25 summary:Prediction of protein secondary structure from the amino acid sequence is a classical bioinformatics problem. Common methods use feed forward neural networks or SVMs combined with a sliding window, as these models does not naturally handle sequential data. Recurrent neural networks are an generalization of the feed forward neural network that naturally handle sequential data. We use a bidirectional recurrent neural network with long short term memory cells for prediction of secondary structure and evaluate using the CB513 dataset. On the secondary structure 8-class problem we report better performance (0.674) than state of the art (0.664). Our model includes feed forward networks between the long short term memory cells, a path that can be further explored. version:2
arxiv-1501-00687 | On Enhancing The Performance Of Nearest Neighbour Classifiers Using Hassanat Distance Metric | http://arxiv.org/abs/1501.00687 | id:1501.00687 author:Mouhammd Alkasassbeh, Ghada A. Altarawneh, Ahmad B. A. Hassanat category:cs.LG  published:2015-01-04 summary:We showed in this work how the Hassanat distance metric enhances the performance of the nearest neighbour classifiers. The results demonstrate the superiority of this distance metric over the traditional and most-used distances, such as Manhattan distance and Euclidian distance. Moreover, we proved that the Hassanat distance metric is invariant to data scale, noise and outliers. Throughout this work, it is clearly notable that both ENN and IINC performed very well with the distance investigated, as their accuracy increased significantly by 3.3% and 3.1% respectively, with no significant advantage of the ENN over the IINC in terms of accuracy. Correspondingly, it can be noted from our results that there is no optimal algorithm that can solve all real-life problems perfectly; this is supported by the no-free-lunch theorem version:1
arxiv-1501-00680 | A New Method for Signal and Image Analysis: The Square Wave Method | http://arxiv.org/abs/1501.00680 | id:1501.00680 author:Osvaldo Skliar, Ricardo E. Monge, Sherry Gapper category:cs.NA cs.CV math.NA 94A12  65F99  published:2015-01-04 summary:A brief review is provided of the use of the Square Wave Method (SWM) in the field of signal and image analysis and it is specified how results thus obtained are expressed using the Square Wave Transform (SWT), in the frequency domain. To illustrate the new approach introduced in this field, the results of two cases are analyzed: a) a sequence of samples (that is, measured values) of an electromyographic recording; and b) the classic image of Lenna. version:1
arxiv-1501-00653 | Hostile Intent Identification by Movement Pattern Analysis: Using Artificial Neural Networks | http://arxiv.org/abs/1501.00653 | id:1501.00653 author:Souham Biswas, Manisha J. Nene category:cs.AI cs.NE  published:2015-01-04 summary:In the recent years, the problem of identifying suspicious behavior has gained importance and identifying this behavior using computational systems and autonomous algorithms is highly desirable in a tactical scenario. So far, the solutions have been primarily manual which elicit human observation of entities to discern the hostility of the situation. To cater to this problem statement, a number of fully automated and partially automated solutions exist. But, these solutions lack the capability of learning from experiences and work in conjunction with human supervision which is extremely prone to error. In this paper, a generalized methodology to predict the hostility of a given object based on its movement patterns is proposed which has the ability to learn and is based upon the mechanism of humans of learning from experiences. The methodology so proposed has been implemented in a computer simulation. The results show that the posited methodology has the potential to be applied in real world tactical scenarios. version:1
arxiv-1412-7384 | Microbial community pattern detection in human body habitats via ensemble clustering framework | http://arxiv.org/abs/1412.7384 | id:1412.7384 author:Peng Yang, Xiaoquan Su, Le Ou-Yang, Hon-Nian Chua, Xiao-Li Li, Kang Ning category:q-bio.QM cs.CE cs.LG q-bio.GN  published:2014-12-21 summary:The human habitat is a host where microbial species evolve, function, and continue to evolve. Elucidating how microbial communities respond to human habitats is a fundamental and critical task, as establishing baselines of human microbiome is essential in understanding its role in human disease and health. However, current studies usually overlook a complex and interconnected landscape of human microbiome and limit the ability in particular body habitats with learning models of specific criterion. Therefore, these methods could not capture the real-world underlying microbial patterns effectively. To obtain a comprehensive view, we propose a novel ensemble clustering framework to mine the structure of microbial community pattern on large-scale metagenomic data. Particularly, we first build a microbial similarity network via integrating 1920 metagenomic samples from three body habitats of healthy adults. Then a novel symmetric Nonnegative Matrix Factorization (NMF) based ensemble model is proposed and applied onto the network to detect clustering pattern. Extensive experiments are conducted to evaluate the effectiveness of our model on deriving microbial community with respect to body habitat and host gender. From clustering results, we observed that body habitat exhibits a strong bound but non-unique microbial structural patterns. Meanwhile, human microbiome reveals different degree of structural variations over body habitat and host gender. In summary, our ensemble clustering framework could efficiently explore integrated clustering results to accurately identify microbial communities, and provide a comprehensive view for a set of microbial communities. Such trends depict an integrated biography of microbial communities, which offer a new insight towards uncovering pathogenic model of human microbiome. version:3
arxiv-1501-00614 | Understanding Trajectory Behavior: A Motion Pattern Approach | http://arxiv.org/abs/1501.00614 | id:1501.00614 author:Mahdi M. Kalayeh, Stephen Mussmann, Alla Petrakova, Niels da Vitoria Lobo, Mubarak Shah category:cs.CV  published:2015-01-04 summary:Mining the underlying patterns in gigantic and complex data is of great importance to data analysts. In this paper, we propose a motion pattern approach to mine frequent behaviors in trajectory data. Motion patterns, defined by a set of highly similar flow vector groups in a spatial locality, have been shown to be very effective in extracting dominant motion behaviors in video sequences. Inspired by applications and properties of motion patterns, we have designed a framework that successfully solves the general task of trajectory clustering. Our proposed algorithm consists of four phases: flow vector computation, motion component extraction, motion component's reachability set creation, and motion pattern formation. For the first phase, we break down trajectories into flow vectors that indicate instantaneous movements. In the second phase, via a Kmeans clustering approach, we create motion components by clustering the flow vectors with respect to their location and velocity. Next, we create motion components' reachability set in terms of spatial proximity and motion similarity. Finally, for the fourth phase, we cluster motion components using agglomerative clustering with the weighted Jaccard distance between the motion components' signatures, a set created using path reachability. We have evaluated the effectiveness of our proposed method in an extensive set of experiments on diverse datasets. Further, we have shown how our proposed method handles difficulties in the general task of trajectory clustering that challenge the existing state-of-the-art methods. version:1
arxiv-1501-00607 | Evaluation of Predictive Data Mining Algorithms in Erythemato-Squamous Disease Diagnosis | http://arxiv.org/abs/1501.00607 | id:1501.00607 author:Kwetishe Danjuma, Adenike O. Osofisan category:cs.LG cs.CE  published:2015-01-03 summary:A lot of time is spent searching for the most performing data mining algorithms applied in clinical diagnosis. The study set out to identify the most performing predictive data mining algorithms applied in the diagnosis of Erythemato-squamous diseases. The study used Naive Bayes, Multilayer Perceptron and J48 decision tree induction to build predictive data mining models on 366 instances of Erythemato-squamous diseases datasets. Also, 10-fold cross-validation and sets of performance metrics were used to evaluate the baseline predictive performance of the classifiers. The comparative analysis shows that the Naive Bayes performed best with accuracy of 97.4%, Multilayer Perceptron came out second with accuracy of 96.6%, and J48 came out the worst with accuracy of 93.5%. The evaluation of these classifiers on clinical datasets, gave an insight into the predictive ability of different data mining algorithms applicable in clinical diagnosis especially in the diagnosis of Erythemato-squamous diseases. version:1
arxiv-1501-00604 | A Taxonomy of Big Data for Optimal Predictive Machine Learning and Data Mining | http://arxiv.org/abs/1501.00604 | id:1501.00604 author:Ernest Fokoue category:stat.ML 60K35  published:2015-01-03 summary:Big data comes in various ways, types, shapes, forms and sizes. Indeed, almost all areas of science, technology, medicine, public health, economics, business, linguistics and social science are bombarded by ever increasing flows of data begging to analyzed efficiently and effectively. In this paper, we propose a rough idea of a possible taxonomy of big data, along with some of the most commonly used tools for handling each particular category of bigness. The dimensionality p of the input space and the sample size n are usually the main ingredients in the characterization of data bigness. The specific statistical machine learning technique used to handle a particular big data set will depend on which category it falls in within the bigness taxonomy. Large p small n data sets for instance require a different set of tools from the large n small p variety. Among other tools, we discuss Preprocessing, Standardization, Imputation, Projection, Regularization, Penalization, Compression, Reduction, Selection, Kernelization, Hybridization, Parallelization, Aggregation, Randomization, Replication, Sequentialization. Indeed, it is important to emphasize right away that the so-called no free lunch theorem applies here, in the sense that there is no universally superior method that outperforms all other methods on all categories of bigness. It is also important to stress the fact that simplicity in the sense of Ockham's razor non plurality principle of parsimony tends to reign supreme when it comes to massive data. We conclude with a comparison of the predictive performance of some of the most commonly used methods on a few data sets. version:1
arxiv-1501-00559 | The Learnability of Unknown Quantum Measurements | http://arxiv.org/abs/1501.00559 | id:1501.00559 author:Hao-Chung Cheng, Min-Hsiu Hsieh, Ping-Cheng Yeh category:quant-ph cs.LG stat.ML  published:2015-01-03 summary:Quantum machine learning has received significant attention in recent years, and promising progress has been made in the development of quantum algorithms to speed up traditional machine learning tasks. In this work, however, we focus on investigating the information-theoretic upper bounds of sample complexity - how many training samples are sufficient to predict the future behaviour of an unknown target function. This kind of problem is, arguably, one of the most fundamental problems in statistical learning theory and the bounds for practical settings can be completely characterised by a simple measure of complexity. Our main result in the paper is that, for learning an unknown quantum measurement, the upper bound, given by the fat-shattering dimension, is linearly proportional to the dimension of the underlying Hilbert space. Learning an unknown quantum state becomes a dual problem to ours, and as a byproduct, we can recover Aaronson's famous result [Proc. R. Soc. A 463:3089-3144 (2007)] solely using a classical machine learning technique. In addition, other famous complexity measures like covering numbers and Rademacher complexities are derived explicitly. We are able to connect measures of sample complexity with various areas in quantum information science, e.g. quantum state/measurement tomography, quantum state discrimination and quantum random access codes, which may be of independent interest. Lastly, with the assistance of general Bloch-sphere representation, we show that learning quantum measurements/states can be mathematically formulated as a neural network. Consequently, classical ML algorithms can be applied to efficiently accomplish the two quantum learning tasks. version:1
arxiv-1501-00503 | An Empirical Study of the L2-Boost technique with Echo State Networks | http://arxiv.org/abs/1501.00503 | id:1501.00503 author:Sebastián Basterrech category:cs.LG cs.NE  published:2015-01-02 summary:A particular case of Recurrent Neural Network (RNN) was introduced at the beginning of the 2000s under the name of Echo State Networks (ESNs). The ESN model overcomes the limitations during the training of the RNNs while introducing no significant disadvantages. Although the model presents some well-identified drawbacks when the parameters are not well initialised. The performance of an ESN is highly dependent on its internal parameters and pattern of connectivity of the hidden-hidden weights Often, the tuning of the network parameters can be hard and can impact in the accuracy of the models. In this work, we investigate the performance of a specific boosting technique (called L2-Boost) with ESNs as single predictors. The L2-Boost technique has been shown to be an effective tool to combine "weak" predictors in regression problems. In this study, we use an ensemble of random initialized ESNs (without control their parameters) as "weak" predictors of the boosting procedure. We evaluate our approach on five well-know time-series benchmark problems. Additionally, we compare this technique with a baseline approach that consists of averaging the prediction of an ensemble of ESNs. version:1
arxiv-1407-1537 | Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent | http://arxiv.org/abs/1407.1537 | id:1407.1537 author:Zeyuan Allen-Zhu, Lorenzo Orecchia category:cs.DS cs.LG cs.NA math.OC stat.ML  published:2014-07-06 summary:First-order methods play a central role in large-scale convex optimization. Even though many variations exist, each suited to a particular problem form, almost all such methods fundamentally rely on two types of algorithmic steps and two corresponding types of analysis: gradient-descent steps, which yield primal progress, and mirror-descent steps, which yield dual progress. In this paper, we observe that the performances of these two types of step are complementary, so that faster algorithms can be designed by linearly coupling the two steps. In particular, we obtain a simple accelerated gradient method for the class of smooth convex optimization problems. The first such method was proposed by Nesterov back to 1983, but to the best of our knowledge, the proof of the fast convergence of accelerated gradient methods has not found a clear interpretation and is still regarded by many as crucially relying on "algebraic tricks". We apply our novel insights to construct a new accelerated gradient method as a natural linear coupling of gradient descent and mirror descent and to write its proof of convergence as a simple combination of the convergence analyses of the two underlying descent steps. We believe that the complementary view and the linear coupling technique in this paper will prove very useful in the design of first-order methods as it allows us to design fast algorithms in a conceptually easier way. For instance, our technique greatly facilitates the recent breakthroughs in solving packing and covering linear programs [AO14, AO15]. version:4
arxiv-1401-0211 | Feature Augmentation via Nonparametrics and Selection (FANS) in High Dimensional Classification | http://arxiv.org/abs/1401.0211 | id:1401.0211 author:Jianqing Fan, Yang Feng, Jiancheng Jiang, Xin Tong category:stat.ME math.ST stat.AP stat.ML stat.TH  published:2013-12-31 summary:We propose a high dimensional classification method that involves nonparametric feature augmentation. Knowing that marginal density ratios are the most powerful univariate classifiers, we use the ratio estimates to transform the original feature measurements. Subsequently, penalized logistic regression is invoked, taking as input the newly transformed or augmented features. This procedure trains models equipped with local complexity and global simplicity, thereby avoiding the curse of dimensionality while creating a flexible nonlinear decision boundary. The resulting method is called Feature Augmentation via Nonparametrics and Selection (FANS). We motivate FANS by generalizing the Naive Bayes model, writing the log ratio of joint densities as a linear combination of those of marginal densities. It is related to generalized additive models, but has better interpretability and computability. Risk bounds are developed for FANS. In numerical analysis, FANS is compared with competing methods, so as to provide a guideline on its best application domain. Real data analysis demonstrates that FANS performs very competitively on benchmark email spam and gene expression data sets. Moreover, FANS is implemented by an extremely fast algorithm through parallel computing. version:2
arxiv-1501-00437 | Computational Feasibility of Clustering under Clusterability Assumptions | http://arxiv.org/abs/1501.00437 | id:1501.00437 author:Shai Ben-David category:cs.CC cs.LG 68Q25  68Q32  published:2015-01-02 summary:It is well known that most of the common clustering objectives are NP-hard to optimize. In practice, however, clustering is being routinely carried out. One approach for providing theoretical understanding of this seeming discrepancy is to come up with notions of clusterability that distinguish realistically interesting input data from worst-case data sets. The hope is that there will be clustering algorithms that are provably efficient on such 'clusterable' instances. In other words, hope that "Clustering is difficult only when it does not matter" (CDNM thesis, for short). We believe that to some extent this may indeed be the case. This paper provides a survey of recent papers along this line of research and a critical evaluation their results. Our bottom line conclusion is that that CDNM thesis is still far from being formally substantiated. We start by discussing which requirements should be met in order to provide formal support the validity of the CDNM thesis. In particular, we list some implied requirements for notions of clusterability. We then examine existing results in view of those requirements and outline some research challenges and open questions. version:1
arxiv-1501-00436 | An Experimental Analysis of the Echo State Network Initialization Using the Particle Swarm Optimization | http://arxiv.org/abs/1501.00436 | id:1501.00436 author:Sebastián Basterrech, Enrique Alba, Václav Snášel category:cs.NE  published:2015-01-02 summary:This article introduces a robust hybrid method for solving supervised learning tasks, which uses the Echo State Network (ESN) model and the Particle Swarm Optimization (PSO) algorithm. An ESN is a Recurrent Neural Network with the hidden-hidden weights fixed in the learning process. The recurrent part of the network stores the input information in internal states of the network. Another structure forms a free-memory method used as supervised learning tool. The setting procedure for initializing the recurrent structure of the ESN model can impact on the model performance. On the other hand, the PSO has been shown to be a successful technique for finding optimal points in complex spaces. Here, we present an approach to use the PSO for finding some initial hidden-hidden weights of the ESN model. We present empirical results that compare the canonical ESN model with this hybrid method on a wide range of benchmark problems. version:1
arxiv-1501-00405 | Efficiently Discovering Frequent Motifs in Large-scale Sensor Data | http://arxiv.org/abs/1501.00405 | id:1501.00405 author:Puneet Agarwal, Gautam Shroff, Sarmimala Saikia, Zaigham Khan category:cs.DB cs.LG  published:2015-01-02 summary:While analyzing vehicular sensor data, we found that frequently occurring waveforms could serve as features for further analysis, such as rule mining, classification, and anomaly detection. The discovery of waveform patterns, also known as time-series motifs, has been studied extensively; however, available techniques for discovering frequently occurring time-series motifs were found lacking in either efficiency or quality: Standard subsequence clustering results in poor quality, to the extent that it has even been termed 'meaningless'. Variants of hierarchical clustering using techniques for efficient discovery of 'exact pair motifs' find high-quality frequent motifs, but at the cost of high computational complexity, making such techniques unusable for our voluminous vehicular sensor data. We show that good quality frequent motifs can be discovered using bounded spherical clustering of time-series subsequences, which we refer to as COIN clustering, with near linear complexity in time-series size. COIN clustering addresses many of the challenges that previously led to subsequence clustering being viewed as meaningless. We describe an end-to-end motif-discovery procedure using a sequence of pre and post-processing techniques that remove trivial-matches and shifted-motifs, which also plagued previous subsequence-clustering approaches. We demonstrate that our technique efficiently discovers frequent motifs in voluminous vehicular sensor data as well as in publicly available data sets. version:1
arxiv-1501-00375 | Passing Expectation Propagation Messages with Kernel Methods | http://arxiv.org/abs/1501.00375 | id:1501.00375 author:Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess category:stat.ML cs.LG  published:2015-01-02 summary:We propose to learn a kernel-based message operator which takes as input all expectation propagation (EP) incoming messages to a factor node and produces an outgoing message. In ordinary EP, computing an outgoing message involves estimating a multivariate integral which may not have an analytic expression. Learning such an operator allows one to bypass the expensive computation of the integral during inference by directly mapping all incoming messages into an outgoing message. The operator can be learned from training data (examples of input and output messages) which allows automated inference to be made on any kind of factor that can be sampled. version:1
arxiv-1401-2753 | Stochastic Optimization with Importance Sampling | http://arxiv.org/abs/1401.2753 | id:1401.2753 author:Peilin Zhao, Tong Zhang category:stat.ML cs.LG  published:2014-01-13 summary:Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Gradient Descent (prox-SGD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization with importance sampling, which improves the convergence rate by reducing the stochastic variance. Specifically, we study prox-SGD (actually, stochastic mirror descent) with importance sampling and prox-SDCA with importance sampling. For prox-SGD, instead of adopting uniform sampling throughout the training process, the proposed algorithm employs importance sampling to minimize the variance of the stochastic gradient. For prox-SDCA, the proposed importance sampling scheme aims to achieve higher expected dual value at each dual coordinate ascent step. We provide extensive theoretical analysis to show that the convergence rates with the proposed importance sampling methods can be significantly improved under suitable conditions both for prox-SGD and for prox-SDCA. Experiments are provided to verify the theoretical analysis. version:2
arxiv-1501-00358 | Comprehend DeepWalk as Matrix Factorization | http://arxiv.org/abs/1501.00358 | id:1501.00358 author:Cheng Yang, Zhiyuan Liu category:cs.LG  published:2015-01-02 summary:Word2vec, as an efficient tool for learning vector representation of words has shown its effectiveness in many natural language processing tasks. Mikolov et al. issued Skip-Gram and Negative Sampling model for developing this toolbox. Perozzi et al. introduced the Skip-Gram model into the study of social network for the first time, and designed an algorithm named DeepWalk for learning node embedding on a graph. We prove that the DeepWalk algorithm is actually factoring a matrix M where each entry M_{ij} is logarithm of the average probability that node i randomly walks to node j in fix steps. version:1
arxiv-1501-00329 | Multi-Access Communications with Energy Harvesting: A Multi-Armed Bandit Model and the Optimality of the Myopic Policy | http://arxiv.org/abs/1501.00329 | id:1501.00329 author:Pol Blasco, Deniz Gunduz category:cs.IT cs.LG math.IT  published:2015-01-01 summary:A multi-access wireless network with N transmitting nodes, each equipped with an energy harvesting (EH) device and a rechargeable battery of finite capacity, is studied. At each time slot (TS) a node is operative with a certain probability, which may depend on the availability of data, or the state of its channel. The energy arrival process at each node is modelled as an independent two-state Markov process, such that, at each TS, a node either harvests one unit of energy, or none. At each TS a subset of the nodes is scheduled by the access point (AP). The scheduling policy that maximises the total throughput is studied assuming that the AP does not know the states of either the EH processes or the batteries. The problem is identified as a restless multiarmed bandit (RMAB) problem, and an upper bound on the optimal scheduling policy is found. Under certain assumptions regarding the EH processes and the battery sizes, the optimality of the myopic policy (MP) is proven. For the general case, the performance of MP is compared numerically to the upper bound. version:1
arxiv-1411-5065 | SIRF: Simultaneous Image Registration and Fusion in A Unified Framework | http://arxiv.org/abs/1411.5065 | id:1411.5065 author:Chen Chen, Yeqing Li, Wei Liu, Junzhou Huang category:cs.CV  published:2014-11-18 summary:In this paper, we propose a novel method for image fusion with a high-resolution panchromatic image and a low-resolution multispectral image at the same geographical location. The fusion is formulated as a convex optimization problem which minimizes a linear combination of a least-squares fitting term and a dynamic gradient sparsity regularizer. The former is to preserve accurate spectral information of the multispectral image, while the latter is to keep sharp edges of the high-resolution panchromatic image. We further propose to simultaneously register the two images during the fusing process, which is naturally achieved by virtue of the dynamic gradient sparsity property. An efficient algorithm is then devised to solve the optimization problem, accomplishing a linear computational complexity in the size of the output image in each iteration. We compare our method against seven state-of-the-art image fusion methods on multispectral image datasets from four satellites. Extensive experimental results demonstrate that the proposed method substantially outperforms the others in terms of both spatial and spectral qualities. We also show that our method can provide high-quality products from coarsely registered real-world datasets. Finally, a MATLAB implementation is provided to facilitate future research. version:2
arxiv-1501-00320 | A robust sub-linear time R-FFAST algorithm for computing a sparse DFT | http://arxiv.org/abs/1501.00320 | id:1501.00320 author:Sameer Pawar, Kannan Ramchandran category:cs.IT cs.LG math.IT  published:2015-01-01 summary:The Fast Fourier Transform (FFT) is the most efficiently known way to compute the Discrete Fourier Transform (DFT) of an arbitrary n-length signal, and has a computational complexity of O(n log n). If the DFT X of the signal x has only k non-zero coefficients (where k < n), can we do better? In [1], we addressed this question and presented a novel FFAST (Fast Fourier Aliasing-based Sparse Transform) algorithm that cleverly induces sparse graph alias codes in the DFT domain, via a Chinese-Remainder-Theorem (CRT)-guided sub-sampling operation of the time-domain samples. The resulting sparse graph alias codes are then exploited to devise a fast and iterative onion-peeling style decoder that computes an n length DFT of a signal using only O(k) time-domain samples and O(klog k) computations. The FFAST algorithm is applicable whenever k is sub-linear in n (i.e. k = o(n)), but is obviously most attractive when k is much smaller than n. In this paper, we adapt the FFAST framework of [1] to the case where the time-domain samples are corrupted by a white Gaussian noise. In particular, we show that the extended noise robust algorithm R-FFAST computes an n-length k-sparse DFT X using O(klog ^3 n) noise-corrupted time-domain samples, in O(klog^4n) computations, i.e., sub-linear time complexity. While our theoretical results are for signals with a uniformly random support of the non-zero DFT coefficients and additive white Gaussian noise, we provide simulation results which demonstrates that the R-FFAST algorithm performs well even for signals like MR images, that have an approximately sparse Fourier spectrum with a non-uniform support for the dominant DFT coefficients. version:1
arxiv-1305-2436 | Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima | http://arxiv.org/abs/1305.2436 | id:1305.2436 author:Po-Ling Loh, Martin J. Wainwright category:math.ST cs.IT math.IT stat.ML stat.TH 62F12  published:2013-05-10 summary:We provide novel theoretical results regarding local optima of regularized $M$-estimators, allowing for nonconvexity in both loss and penalty functions. Under restricted strong convexity on the loss and suitable regularity conditions on the penalty, we prove that \emph{any stationary point} of the composite objective function will lie within statistical precision of the underlying parameter vector. Our theory covers many nonconvex objective functions of interest, including the corrected Lasso for errors-in-variables linear models; regression for generalized linear models with nonconvex penalties such as SCAD, MCP, and capped-$\ell_1$; and high-dimensional graphical model estimation. We quantify statistical accuracy by providing bounds on the $\ell_1$-, $\ell_2$-, and prediction error between stationary points and the population-level optimum. We also propose a simple modification of composite gradient descent that may be used to obtain a near-global optimum within statistical precision $\epsilon$ in $\log(1/\epsilon)$ steps, which is the fastest possible rate of any first-order method. We provide simulation studies illustrating the sharpness of our theoretical results. version:2
arxiv-1501-00312 | Statistical consistency and asymptotic normality for high-dimensional robust M-estimators | http://arxiv.org/abs/1501.00312 | id:1501.00312 author:Po-Ling Loh category:math.ST cs.IT math.IT stat.ML stat.TH 62F12  published:2015-01-01 summary:We study theoretical properties of regularized robust M-estimators, applicable when data are drawn from a sparse high-dimensional linear model and contaminated by heavy-tailed distributions and/or outliers in the additive errors and covariates. We first establish a form of local statistical consistency for the penalized regression estimators under fairly mild conditions on the error distribution: When the derivative of the loss function is bounded and satisfies a local restricted curvature condition, all stationary points within a constant radius of the true regression vector converge at the minimax rate enjoyed by the Lasso with sub-Gaussian errors. When an appropriate nonconvex regularizer is used in place of an l_1-penalty, we show that such stationary points are in fact unique and equal to the local oracle solution with the correct support---hence, results on asymptotic normality in the low-dimensional case carry over immediately to the high-dimensional setting. This has important implications for the efficiency of regularized nonconvex M-estimators when the errors are heavy-tailed. Our analysis of the local curvature of the loss function also has useful consequences for optimization when the robust regression function and/or regularizer is nonconvex and the objective function possesses stationary points outside the local region. We show that as long as a composite gradient descent algorithm is initialized within a constant radius of the true regression vector, successive iterates will converge at a linear rate to a stationary point within the local region. Furthermore, the global optimum of a convex regularized robust regression function may be used to obtain a suitable initialization. The result is a novel two-step procedure that uses a convex M-estimator to achieve consistency and a nonconvex M-estimator to increase efficiency. version:1
arxiv-1501-00311 | QANUS: An Open-source Question-Answering Platform | http://arxiv.org/abs/1501.00311 | id:1501.00311 author:Jun-Ping Ng, Min-Yen Kan category:cs.IR cs.CL  published:2015-01-01 summary:In this paper, we motivate the need for a publicly available, generic software framework for question-answering (QA) systems. We present an open-source QA framework QANUS which researchers can leverage on to build new QA systems easily and rapidly. The framework implements much of the code that will otherwise have been repeated across different QA systems. To demonstrate the utility and practicality of the framework, we further present a fully functioning factoid QA system QA-SYS built on top of QANUS. version:1
arxiv-1501-00299 | Sequence Modeling using Gated Recurrent Neural Networks | http://arxiv.org/abs/1501.00299 | id:1501.00299 author:Mohammad Pezeshki category:cs.NE cs.LG  published:2015-01-01 summary:In this paper, we have used Recurrent Neural Networks to capture and model human motion data and generate motions by prediction of the next immediate data point at each time-step. Our RNN is armed with recently proposed Gated Recurrent Units which has shown promising results in some sequence modeling problems such as Machine Translation and Speech Synthesis. We demonstrate that this model is able to capture long-term dependencies in data and generate realistic motions. version:1
arxiv-1501-00287 | Consistent Classification Algorithms for Multi-class Non-Decomposable Performance Metrics | http://arxiv.org/abs/1501.00287 | id:1501.00287 author:Harish G. Ramaswamy, Harikrishna Narasimhan, Shivani Agarwal category:cs.LG stat.ML  published:2015-01-01 summary:We study consistency of learning algorithms for a multi-class performance metric that is a non-decomposable function of the confusion matrix of a classifier and cannot be expressed as a sum of losses on individual data points; examples of such performance metrics include the macro F-measure popular in information retrieval and the G-mean metric used in class-imbalanced problems. While there has been much work in recent years in understanding the consistency properties of learning algorithms for `binary' non-decomposable metrics, little is known either about the form of the optimal classifier for a general multi-class non-decomposable metric, or about how these learning algorithms generalize to the multi-class case. In this paper, we provide a unified framework for analysing a multi-class non-decomposable performance metric, where the problem of finding the optimal classifier for the performance metric is viewed as an optimization problem over the space of all confusion matrices achievable under the given distribution. Using this framework, we show that (under a continuous distribution) the optimal classifier for a multi-class performance metric can be obtained as the solution of a cost-sensitive classification problem, thus generalizing several previous results on specific binary non-decomposable metrics. We then design a consistent learning algorithm for concave multi-class performance metrics that proceeds via a sequence of cost-sensitive classification problems, and can be seen as applying the conditional gradient (CG) optimization method over the space of feasible confusion matrices. To our knowledge, this is the first efficient learning algorithm (whose running time is polynomial in the number of classes) that is consistent for a large family of multi-class non-decomposable metrics. Our consistency proof uses a novel technique based on the convergence analysis of the CG method. version:1
arxiv-1501-00263 | Communication-Efficient Distributed Optimization of Self-Concordant Empirical Loss | http://arxiv.org/abs/1501.00263 | id:1501.00263 author:Yuchen Zhang, Lin Xiao category:math.OC cs.LG stat.ML  published:2015-01-01 summary:We consider distributed convex optimization problems originated from sample average approximation of stochastic optimization, or empirical risk minimization in machine learning. We assume that each machine in the distributed computing system has access to a local empirical loss function, constructed with i.i.d. data sampled from a common distribution. We propose a communication-efficient distributed algorithm to minimize the overall empirical loss, which is the average of the local empirical losses. The algorithm is based on an inexact damped Newton method, where the inexact Newton steps are computed by a distributed preconditioned conjugate gradient method. We analyze its iteration complexity and communication efficiency for minimizing self-concordant empirical loss functions, and discuss the results for distributed ridge regression, logistic regression and binary classification with a smoothed hinge loss. In a standard setting for supervised learning, the required number of communication rounds of the algorithm does not increase with the sample size, and only grows slowly with the number of machines. version:1
arxiv-1410-7812 | Beta-Negative Binomial Process and Exchangeable Random Partitions for Mixed-Membership Modeling | http://arxiv.org/abs/1410.7812 | id:1410.7812 author:Mingyuan Zhou category:stat.ME stat.ML  published:2014-10-28 summary:The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance. version:2
arxiv-1501-00208 | The continuum-of-urns scheme, generalized beta and Indian buffet processes, and hierarchies thereof | http://arxiv.org/abs/1501.00208 | id:1501.00208 author:Daniel M. Roy category:math.PR math.ST stat.ML stat.TH  published:2014-12-31 summary:We describe the combinatorial stochastic process underlying a sequence of conditionally independent Bernoulli processes with a shared beta process hazard measure. As shown by Thibaux and Jordan [TJ07], in the special case when the underlying beta process has a constant concentration function and a finite and nonatomic mean, the combinatorial structure is that of the Indian buffet process (IBP) introduced by Griffiths and Ghahramani [GG05]. By reinterpreting the beta process introduced by Hjort [Hjo90] as a measurable family of Dirichlet processes, we obtain a simple predictive rule for the general case, which can be thought of as a continuum of Blackwell-MacQueen urn schemes (or equivalently, one-parameter Hoppe urn schemes). The corresponding measurable family of Perman-Pitman-Yor processes leads to a continuum of two-parameter Hoppe urn schemes, whose ordinary component is the three-parameter IBP introduced by Teh and G\"or\"ur [TG09], which exhibits power-law behavior, as further studied by Broderick, Jordan, and Pitman [BJP12]. The idea extends to arbitrary measurable families of exchangeable partition probability functions and gives rise to generalizations of the beta process with matching buffet processes. Finally, in the same way that hierarchies of Dirichlet processes were given Chinese restaurant franchise representations by Teh, Jordan, Beal, and Blei [Teh+06], one can construct representations of sequences of Bernoulli processes directed by hierarchies of beta processes (and their generalizations) using the stochastic process we uncover. version:1
arxiv-1501-00199 | ACCAMS: Additive Co-Clustering to Approximate Matrices Succinctly | http://arxiv.org/abs/1501.00199 | id:1501.00199 author:Alex Beutel, Amr Ahmed, Alexander J. Smola category:cs.LG stat.ML H.2.8; H.3.3; I.2.6  published:2014-12-31 summary:Matrix completion and approximation are popular tools to capture a user's preferences for recommendation and to approximate missing data. Instead of using low-rank factorization we take a drastically different approach, based on the simple insight that an additive model of co-clusterings allows one to approximate matrices efficiently. This allows us to build a concise model that, per bit of model learned, significantly beats all factorization approaches to matrix approximation. Even more surprisingly, we find that summing over small co-clusterings is more effective in modeling matrices than classic co-clustering, which uses just one large partitioning of the matrix. Following Occam's razor principle suggests that the simple structure induced by our model better captures the latent preferences and decision making processes present in the real world than classic co-clustering or matrix factorization. We provide an iterative minimization algorithm, a collapsed Gibbs sampler, theoretical guarantees for matrix approximation, and excellent empirical evidence for the efficacy of our approach. We achieve state-of-the-art results on the Netflix problem with a fraction of the model complexity. version:1
arxiv-1501-00125 | Maximum Margin Clustering for State Decomposition of Metastable Systems | http://arxiv.org/abs/1501.00125 | id:1501.00125 author:Hao Wu category:cs.LG cs.NA cs.SY math.NA physics.data-an  published:2014-12-31 summary:When studying a metastable dynamical system, a prime concern is how to decompose the phase space into a set of metastable states. Unfortunately, the metastable state decomposition based on simulation or experimental data is still a challenge. The most popular and simplest approach is geometric clustering which is developed based on the classical clustering technique. However, the prerequisites of this approach are: (1) data are obtained from simulations or experiments which are in global equilibrium and (2) the coordinate system is appropriately selected. Recently, the kinetic clustering approach based on phase space discretization and transition probability estimation has drawn much attention due to its applicability to more general cases, but the choice of discretization policy is a difficult task. In this paper, a new decomposition method designated as maximum margin metastable clustering is proposed, which converts the problem of metastable state decomposition to a semi-supervised learning problem so that the large margin technique can be utilized to search for the optimal decomposition without phase space discretization. Moreover, several simulation examples are given to illustrate the effectiveness of the proposed method. version:1
arxiv-1501-00108 | HSI based colour image equalization using iterative nth root and nth power | http://arxiv.org/abs/1501.00108 | id:1501.00108 author:Gholamreza Anbarjafari category:cs.CV cs.GR  published:2014-12-31 summary:In this paper an equalization technique for colour images is introduced. The method is based on nth root and nth power equalization approach but with optimization of the mean of the image in different colour channels such as RGB and HSI. The performance of the proposed method has been measured by the means of peak signal to noise ratio. The proposed algorithm has been compared with conventional histogram equalization and the visual and quantitative experimental results are showing that the proposed method over perform the histogram equalization. version:1
arxiv-1501-00105 | Face recognition using color local binary pattern from mutually independent color channels | http://arxiv.org/abs/1501.00105 | id:1501.00105 author:Gholamreza Anbarjafari category:cs.CV  published:2014-12-31 summary:In this paper, a high performance face recognition system based on local binary pattern (LBP) using the probability distribution functions (PDF) of pixels in different mutually independent color channels which are robust to frontal homogenous illumination and planer rotation is proposed. The illumination of faces is enhanced by using the state-of-the-art technique which is using discrete wavelet transform (DWT) and singular value decomposition (SVD). After equalization, face images are segmented by use of local Successive Mean Quantization Transform (SMQT) followed by skin color based face detection system. Kullback-Leibler Distance (KLD) between the concatenated PDFs of a given face obtained by LBP and the concatenated PDFs of each face in the database is used as a metric in the recognition process. Various decision fusion techniques have been used in order to improve the recognition rate. The proposed system has been tested on the FERET, HP, and Bosphorus face databases. The proposed system is compared with conventional and thestate-of-the-art techniques. The recognition rates obtained using FVF approach for FERET database is 99.78% compared with 79.60% and 68.80% for conventional gray scale LBP and Principle Component Analysis (PCA) based face recognition techniques respectively. version:1
arxiv-1501-00052 | Detailed Derivations of Small-Variance Asymptotics for some Hierarchical Bayesian Nonparametric Models | http://arxiv.org/abs/1501.00052 | id:1501.00052 author:Jonathan H. Huggins, Ardavan Saeedi, Matthew J. Johnson category:stat.ML cs.LG  published:2014-12-31 summary:In this note we provide detailed derivations of two versions of small-variance asymptotics for hierarchical Dirichlet process (HDP) mixture models and the HDP hidden Markov model (HDP-HMM, a.k.a. the infinite HMM). We include derivations for the probabilities of certain CRP and CRF partitions, which are of more general interest. version:1
arxiv-1501-00037 | Discriminative Clustering with Relative Constraints | http://arxiv.org/abs/1501.00037 | id:1501.00037 author:Yuanli Pei, Xiaoli Z. Fern, Rómer Rosales, Teresa Vania Tjahja category:cs.LG  published:2014-12-30 summary:We study the problem of clustering with relative constraints, where each constraint specifies relative similarities among instances. In particular, each constraint $(x_i, x_j, x_k)$ is acquired by posing a query: is instance $x_i$ more similar to $x_j$ than to $x_k$? We consider the scenario where answers to such queries are based on an underlying (but unknown) class concept, which we aim to discover via clustering. Different from most existing methods that only consider constraints derived from yes and no answers, we also incorporate don't know responses. We introduce a Discriminative Clustering method with Relative Constraints (DCRC) which assumes a natural probabilistic relationship between instances, their underlying cluster memberships, and the observed constraints. The objective is to maximize the model likelihood given the constraints, and in the meantime enforce cluster separation and cluster balance by also making use of the unlabeled instances. We evaluated the proposed method using constraints generated from ground-truth class labels, and from (noisy) human judgments from a user study. Experimental results demonstrate: 1) the usefulness of relative constraints, in particular when don't know answers are considered; 2) the improved performance of the proposed method over state-of-the-art methods that utilize either relative or pairwise constraints; and 3) the robustness of our method in the presence of noisy constraints, such as those provided by human judgement. version:1
arxiv-1412-8690 | Breaking the Curse of Dimensionality with Convex Neural Networks | http://arxiv.org/abs/1412.8690 | id:1412.8690 author:Francis Bach category:cs.LG math.OC math.ST stat.TH  published:2014-12-30 summary:We consider neural networks with a single hidden layer and non-decreasing homogeneous activa-tion functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of ob-servations. In addition, we provide a simple geometric interpretation to the non-convex problem of addition of a new unit, which is the core potentially hard computational element in the framework of learning from continuously many basis functions. We provide simple conditions for convex relaxations to achieve the same generalization error bounds, even when constant-factor approxi-mations cannot be found (e.g., because it is NP-hard such as for the zero-homogeneous activation function). We were not able to find strong enough convex relaxations and leave open the existence or non-existence of polynomial-time algorithms. version:1
arxiv-1407-4508 | Large scale canonical correlation analysis with iterative least squares | http://arxiv.org/abs/1407.4508 | id:1407.4508 author:Yichao Lu, Dean P. Foster category:stat.ML  published:2014-07-16 summary:Canonical Correlation Analysis (CCA) is a widely used statistical tool with both well established theory and favorable performance for a wide range of machine learning problems. However, computing CCA for huge datasets can be very slow since it involves implementing QR decomposition or singular value decomposition of huge matrices. In this paper we introduce L-CCA, a iterative algorithm which can compute CCA fast on huge sparse datasets. Theory on both the asymptotic convergence and finite time accuracy of L-CCA are established. The experiments also show that L-CCA outperform other fast CCA approximation schemes on two real datasets. version:2
arxiv-1412-8656 | A multistep segmentation algorithm for vessel extraction in medical imaging | http://arxiv.org/abs/1412.8656 | id:1412.8656 author:Nasser Aghazadeh, Ladan Sharafyan Cigaroudy category:cs.CV math.NA  published:2014-12-30 summary:The main contribution of this paper is to propose an iterative procedure for tubular structure segmentation of 2D images, which combines tight frame of Curvelet transforms with a SURE technique thresholding which is based on principle obtained by minimizing Stein Unbiased Risk Estimate for denoising. This proposed algorithm is mainly based on the TFA proposal presented in [1, 9], which we use eigenvectors of Hessian matrix of image for improving this iterative part in segmenting unclear and narrow vessels and filling the gap between separate pieces of detected vessels. The experimental results are presented to demonstrate the effectiveness of the proposed model. version:1
arxiv-1411-0589 | Modular proximal optimization for multidimensional total-variation regularization | http://arxiv.org/abs/1411.0589 | id:1411.0589 author:Álvaro Barbero, Suvrit Sra category:stat.ML math.OC  published:2014-11-03 summary:One of the most frequently used notions of "structured sparsity" is that of sparse (discrete) gradients, a structure typically elicited through \emph{Total-Variation (TV)} regularizers. This paper focuses on anisotropic TV-regularizers, in particular on $\ell_p$-norm \emph{weighted TV regularizers} for which it develops efficient algorithms to compute the corresponding proximity operators. Our algorithms enable one to scalably incorporate TV regularization of vector, matrix, or tensor data into a proximal convex optimization solvers. For the special case of vectors, we derive and implement a highly efficient weighted 1D-TV solver. This solver provides a backbone for subsequently handling the more complex task of higher-dimensional (two or more) TV by means of a modular proximal optimization approach. We present numerical experiments that demonstrate how our 1D-TV solver matches or exceeds the best known 1D-TV solvers. Thereafter, we illustrate the benefits of our modular design through extensive experiments on: (i) image denoising; (ii) image deconvolution; and (iii) four variants of fused-lasso. Our results show the flexibility and speed our TV solvers offer over competing approaches. To underscore our claims, we provide our TV solvers in an easy to use multi-threaded C++ library (which also aids reproducibility of our results). version:2
arxiv-1501-03997 | Holistic random encoding for imaging through multimode fibers | http://arxiv.org/abs/1501.03997 | id:1501.03997 author:Hwanchol Jang, Changhyeong Yoon, Euiheon Chung, Wonshik Choi, Heung-No Lee category:physics.optics cs.CV  published:2014-12-30 summary:The input numerical aperture (NA) of multimode fiber (MMF) can be effectively increased by placing turbid media at the input end of the MMF. This provides the potential for high-resolution imaging through the MMF. While the input NA is increased, the number of propagation modes in the MMF and hence the output NA remains the same. This makes the image reconstruction process underdetermined and may limit the quality of the image reconstruction. In this paper, we aim to improve the signal to noise ratio (SNR) of the image reconstruction in imaging through MMF. We notice that turbid media placed in the input of the MMF transforms the incoming waves into a better format for information transmission and information extraction. We call this transformation as holistic random (HR) encoding of turbid media. By exploiting the HR encoding, we make a considerable improvement on the SNR of the image reconstruction. For efficient utilization of the HR encoding, we employ sparse representation (SR), a relatively new signal reconstruction framework when it is provided with a HR encoded signal. This study shows for the first time to our knowledge the benefit of utilizing the HR encoding of turbid media for recovery in the optically underdetermined systems where the output NA of it is smaller than the input NA for imaging through MMF. version:1
arxiv-1312-2050 | Consistency of spectral clustering in stochastic block models | http://arxiv.org/abs/1312.2050 | id:1312.2050 author:Jing Lei, Alessandro Rinaldo category:math.ST stat.ML stat.TH  published:2013-12-07 summary:We analyze the performance of spectral clustering for community extraction in stochastic block models. We show that, under mild conditions, spectral clustering applied to the adjacency matrix of the network can consistently recover hidden communities even when the order of the maximum expected degree is as small as $\log n$, with $n$ the number of nodes. This result applies to some popular polynomial time spectral clustering algorithms and is further extended to degree corrected stochastic block models using a spherical $k$-median spectral clustering method. A key component of our analysis is a combinatorial bound on the spectrum of binary random matrices, which is sharper than the conventional matrix Bernstein inequality and may be of independent interest. version:3
arxiv-1412-7003 | A Bayesian encourages dropout | http://arxiv.org/abs/1412.7003 | id:1412.7003 author:Shin-ichi Maeda category:cs.LG cs.NE stat.ML  published:2014-12-22 summary:Dropout is one of the key techniques to prevent the learning from overfitting. It is explained that dropout works as a kind of modified L2 regularization. Here, we shed light on the dropout from Bayesian standpoint. Bayesian interpretation enables us to optimize the dropout rate, which is beneficial for learning of weight parameters and prediction after learning. The experiment result also encourages the optimization of the dropout. version:3
arxiv-1412-8566 | Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing | http://arxiv.org/abs/1412.8566 | id:1412.8566 author:Yuri Burda, Roger B. Grosse, Ruslan Salakhutdinov category:cs.LG stat.ML  published:2014-12-30 summary:Markov random fields (MRFs) are difficult to evaluate as generative models because computing the test log-probabilities requires the intractable partition function. Annealed importance sampling (AIS) is widely used to estimate MRF partition functions, and often yields quite accurate results. However, AIS is prone to overestimate the log-likelihood with little indication that anything is wrong. We present the Reverse AIS Estimator (RAISE), a stochastic lower bound on the log-likelihood of an approximation to the original MRF model. RAISE requires only the same MCMC transition operators as standard AIS. Experimental results indicate that RAISE agrees closely with AIS log-probability estimates for RBMs, DBMs, and DBNs, but typically errs on the side of underestimating, rather than overestimating, the log-likelihood. version:1
arxiv-1405-2874 | A Study of Entanglement in a Categorical Framework of Natural Language | http://arxiv.org/abs/1405.2874 | id:1405.2874 author:Dimitri Kartsaklis, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT quant-ph  published:2014-05-12 summary:In both quantum mechanics and corpus linguistics based on vector spaces, the notion of entanglement provides a means for the various subsystems to communicate with each other. In this paper we examine a number of implementations of the categorical framework of Coecke, Sadrzadeh and Clark (2010) for natural language, from an entanglement perspective. Specifically, our goal is to better understand in what way the level of entanglement of the relational tensors (or the lack of it) affects the compositional structures in practical situations. Our findings reveal that a number of proposals for verb construction lead to almost separable tensors, a fact that considerably simplifies the interactions between the words. We examine the ramifications of this fact, and we show that the use of Frobenius algebras mitigates the potential problems to a great extent. Finally, we briefly examine a machine learning method that creates verb tensors exhibiting a sufficient level of entanglement. version:2
arxiv-1404-3925 | Complexity of Grammar Induction for Quantum Types | http://arxiv.org/abs/1404.3925 | id:1404.3925 author:Antonin Delpeuch category:cs.CL math.CT  published:2014-04-13 summary:Most categorical models of meaning use a functor from the syntactic category to the semantic category. When semantic information is available, the problem of grammar induction can therefore be defined as finding preimages of the semantic types under this forgetful functor, lifting the information flow from the semantic level to a valid reduction at the syntactic level. We study the complexity of grammar induction, and show that for a variety of type systems, including pivotal and compact closed categories, the grammar induction problem is NP-complete. Our approach could be extended to linguistic type systems such as autonomous or bi-closed categories. version:2
arxiv-1412-8534 | Disjunctive Normal Networks | http://arxiv.org/abs/1412.8534 | id:1412.8534 author:Mehdi Sajjadi, Mojtaba Seyedhosseini, Tolga Tasdizen category:cs.LG cs.NE  published:2014-12-30 summary:Artificial neural networks are powerful pattern classifiers; however, they have been surpassed in accuracy by methods such as support vector machines and random forests that are also easier to use and faster to train. Backpropagation, which is used to train artificial neural networks, suffers from the herd effect problem which leads to long training times and limit classification accuracy. We use the disjunctive normal form and approximate the boolean conjunction operations with products to construct a novel network architecture. The proposed model can be trained by minimizing an error function and it allows an effective and intuitive initialization which solves the herd-effect problem associated with backpropagation. This leads to state-of-the art classification accuracy and fast training times. In addition, our model can be jointly optimized with convolutional features in an unified structure leading to state-of-the-art results on computer vision problems with fast convergence rates. A GPU implementation of LDNN with optional convolutional features is also available version:1
arxiv-1412-8527 | From Logical to Distributional Models | http://arxiv.org/abs/1412.8527 | id:1412.8527 author:Anne Preller category:cs.LO cs.CL quant-ph F.4.0; F.4.1  published:2014-12-30 summary:The paper relates two variants of semantic models for natural language, logical functional models and compositional distributional vector space models, by transferring the logic and reasoning from the logical to the distributional models. The geometrical operations of quantum logic are reformulated as algebraic operations on vectors. A map from functional models to vector space models makes it possible to compare the meaning of sentences word by word. version:1
arxiv-1412-8504 | Probing the topological properties of complex networks modeling short written texts | http://arxiv.org/abs/1412.8504 | id:1412.8504 author:Diego R. Amancio category:cs.CL physics.soc-ph  published:2014-12-29 summary:In recent years, graph theory has been widely employed to probe several language properties. More specifically, the so-called word adjacency model has been proven useful for tackling several practical problems, especially those relying on textual stylistic analysis. The most common approach to treat texts as networks has simply considered either large pieces of texts or entire books. This approach has certainly worked well -- many informative discoveries have been made this way -- but it raises an uncomfortable question: could there be important topological patterns in small pieces of texts? To address this problem, the topological properties of subtexts sampled from entire books was probed. Statistical analyzes performed on a dataset comprising 50 novels revealed that most of the traditional topological measurements are stable for short subtexts. When the performance of the authorship recognition task was analyzed, it was found that a proper sampling yields a discriminability similar to the one found with full texts. Surprisingly, the support vector machine classification based on the characterization of short texts outperformed the one performed with entire books. These findings suggest that a local topological analysis of large documents might improve its global characterization. Most importantly, it was verified, as a proof of principle, that short texts can be analyzed with the methods and concepts of complex networks. As a consequence, the techniques described here can be extended in a straightforward fashion to analyze texts as time-varying complex networks. version:1
arxiv-1412-8496 | Accurate Localization in Dense Urban Area Using Google Street View Image | http://arxiv.org/abs/1412.8496 | id:1412.8496 author:Mahdi Salarian category:cs.CV  published:2014-12-29 summary:Accurate information about the location and orientation of a camera in mobile devices is central to the utilization of location-based services (LBS). Most of such mobile devices rely on GPS data but this data is subject to inaccuracy due to imperfections in the quality of the signal provided by satellites. This shortcoming has spurred the research into improving the accuracy of localization. Since mobile devices have camera, a major thrust of this research has been seeks to acquire the local scene and apply image retrieval techniques by querying a GPS-tagged image database to find the best match for the acquired scene.. The techniques are however computationally demanding and unsuitable for real-time applications such as assistive technology for navigation by the blind and visually impaired which motivated out work. To overcome the high complexity of those techniques, we investigated the use of inertial sensors as an aid in image-retrieval-based approach. Armed with information of media other than images, such as data from the GPS module along with orientation sensors such as accelerometer and gyro, we sought to limit the size of the image set to c search for the best match. Specifically, data from the orientation sensors along with Dilution of precision (DOP) from GPS are used to find the angle of view and estimation of position. We present analysis of the reduction in the image set size for the search as well as simulations to demonstrate the effectiveness in a fast implementation with 98% Estimated Position Error. version:1
arxiv-1412-8493 | An ADMM algorithm for solving a proximal bound-constrained quadratic program | http://arxiv.org/abs/1412.8493 | id:1412.8493 author:Miguel Á. Carreira-Perpiñán category:math.OC cs.LG stat.ML  published:2014-12-29 summary:We consider a proximal operator given by a quadratic function subject to bound constraints and give an optimization algorithm using the alternating direction method of multipliers (ADMM). The algorithm is particularly efficient to solve a collection of proximal operators that share the same quadratic form, or if the quadratic program is the relaxation of a binary quadratic problem. version:1
arxiv-1412-8341 | Spectral classification using convolutional neural networks | http://arxiv.org/abs/1412.8341 | id:1412.8341 author:Pavel Hála category:cs.CV astro-ph.IM cs.NE  published:2014-12-29 summary:There is a great need for accurate and autonomous spectral classification methods in astrophysics. This thesis is about training a convolutional neural network (ConvNet) to recognize an object class (quasar, star or galaxy) from one-dimension spectra only. Author developed several scripts and C programs for datasets preparation, preprocessing and postprocessing of the data. EBLearn library (developed by Pierre Sermanet and Yann LeCun) was used to create ConvNets. Application on dataset of more than 60000 spectra yielded success rate of nearly 95%. This thesis conclusively proved great potential of convolutional neural networks and deep learning methods in astrophysics. version:1
arxiv-1407-3956 | Globally Optimal Joint Image Segmentation and Shape Matching Based on Wasserstein Modes | http://arxiv.org/abs/1407.3956 | id:1407.3956 author:Bernhard Schmitzer, Christoph Schnörr category:cs.CV 49Q10  62H35  published:2014-07-15 summary:A functional for joint variational object segmentation and shape matching is developed. The formulation is based on optimal transport w.r.t. geometric distance and local feature similarity. Geometric invariance and modelling of object-typical statistical variations is achieved by introducing degrees of freedom that describe transformations and deformations of the shape template. The shape model is mathematically equivalent to contour-based approaches but inference can be performed without conversion between the contour and region representations, allowing combination with other convex segmentation approaches and simplifying optimization. While the overall functional is non-convex, non-convexity is confined to a low-dimensional variable. We propose a locally optimal alternating optimization scheme and a globally optimal branch and bound scheme, based on adaptive convex relaxation. Combining both methods allows to eliminate the delicate initialization problem inherent to many contour based approaches while remaining computationally practical. The properties of the functional, its ability to adapt to a wide range of input data structures and the different optimization schemes are illustrated and compared by numerical experiments. version:2
arxiv-1412-8291 | Improving approximate RPCA with a k-sparsity prior | http://arxiv.org/abs/1412.8291 | id:1412.8291 author:Maximilian Karl, Christian Osendorfer category:cs.NE cs.LG  published:2014-12-29 summary:A process centric view of robust PCA (RPCA) allows its fast approximate implementation based on a special form o a deep neural network with weights shared across all layers. However, empirically this fast approximation to RPCA fails to find representations that are parsemonious. We resolve these bad local minima by relaxing the elementwise L1 and L2 priors and instead utilize a structure inducing k-sparsity prior. In a discriminative classification task the newly learned representations outperform these from the original approximate RPCA formulation significantly. version:1
arxiv-1412-8287 | Rigid and Non-rigid Shape Evolutions for Shape Alignment and Recovery in Images | http://arxiv.org/abs/1412.8287 | id:1412.8287 author:Junyan Wang, Kap-Luk Chan category:cs.CV  published:2014-12-29 summary:The same type of objects in different images may vary in their shapes because of rigid and non-rigid shape deformations, occluding foreground as well as cluttered background. The problem concerned in this work is the shape extraction in such challenging situations. We approach the shape extraction through shape alignment and recovery. This paper presents a novel and general method for shape alignment and recovery by using one example shapes based on deterministic energy minimization. Our idea is to use general model of shape deformation in minimizing active contour energies. Given \emph{a priori} form of the shape deformation, we show how the curve evolution equation corresponding to the shape deformation can be derived. The curve evolution is called the prior variation shape evolution (PVSE). We also derive the energy-minimizing PVSE for minimizing active contour energies. For shape recovery, we propose to use the PVSE that deforms the shape while preserving its shape characteristics. For choosing such shape-preserving PVSE, a theory of shape preservability of the PVSE is established. Experimental results validate the theory and the formulations, and they demonstrate the effectiveness of our method. version:1
arxiv-1412-8147 | Improving Persian Document Classification Using Semantic Relations between Words | http://arxiv.org/abs/1412.8147 | id:1412.8147 author:Saeed Parseh, Ahmad Baraani category:cs.IR cs.LG  published:2014-12-28 summary:With the increase of information, document classification as one of the methods of text mining, plays vital role in many management and organizing information. Document classification is the process of assigning a document to one or more predefined category labels. Document classification includes different parts such as text processing, term selection, term weighting and final classification. The accuracy of document classification is very important. Thus improvement in each part of classification should lead to better results and higher precision. Term weighting has a great impact on the accuracy of the classification. Most of the existing weighting methods exploit the statistical information of terms in documents and do not consider semantic relations between words. In this paper, an automated document classification system is presented that uses a novel term weighting method based on semantic relations between terms. To evaluate the proposed method, three standard Persian corpuses are used. Experiment results show 2 to 4 percent improvement in classification accuracy compared with the best previous designed system for Persian documents. version:1
arxiv-1412-8102 | Proceedings of the 11th workshop on Quantum Physics and Logic | http://arxiv.org/abs/1412.8102 | id:1412.8102 author:Bob Coecke, Ichiro Hasuo, Prakash Panangaden category:cs.LO cs.CL cs.PL quant-ph  published:2014-12-28 summary:This volume contains the proceedings of the 11th International Workshop on Quantum Physics and Logic (QPL 2014), which was held from the 4th to the 6th of June, 2014, at Kyoto University, Japan. The goal of the QPL workshop series is to bring together researchers working on mathematical foundations of quantum physics, quantum computing and spatio-temporal causal structures, and in particular those that use logical tools, ordered algebraic and category-theoretic structures, formal languages, semantic methods and other computer science methods for the study of physical behavior in general. Over the past few years, there has been growing activity in these foundational approaches, together with a renewed interest in the foundations of quantum theory, which complement the more mainstream research in quantum computation. Earlier workshops in this series, with the same acronym under the name "Quantum Programming Languages", were held in Ottawa (2003), Turku (2004), Chicago (2005), and Oxford (2006). The first QPL under the new name Quantum Physics and Logic was held in Reykjavik (2008), followed by Oxford (2009 and 2010), Nijmegen (2011), Brussels (2012) and Barcelona (2013). version:1
arxiv-1412-8079 | Persian Sentiment Analyzer: A Framework based on a Novel Feature Selection Method | http://arxiv.org/abs/1412.8079 | id:1412.8079 author:Ayoub Bagheri, Mohamad Saraee category:cs.CL cs.IR  published:2014-12-27 summary:In the recent decade, with the enormous growth of digital content in internet and databases, sentiment analysis has received more and more attention between information retrieval and natural language processing researchers. Sentiment analysis aims to use automated tools to detect subjective information from reviews. One of the main challenges in sentiment analysis is feature selection. Feature selection is widely used as the first stage of analysis and classification tasks to reduce the dimension of problem, and improve speed by the elimination of irrelevant and redundant features. Up to now as there are few researches conducted on feature selection in sentiment analysis, there are very rare works for Persian sentiment analysis. This paper considers the problem of sentiment classification using different feature selection methods for online customer reviews in Persian language. Three of the challenges of Persian text are using of a wide variety of declensional suffixes, different word spacing and many informal or colloquial words. In this paper we study these challenges by proposing a model for sentiment classification of Persian review documents. The proposed model is based on lemmatization and feature selection and is employed Naive Bayes algorithm for classification. We evaluate the performance of the model on a manually gathered collection of cellphone reviews, where the results show the effectiveness of the proposed approaches. version:1
arxiv-1412-8070 | Functional correspondence by matrix completion | http://arxiv.org/abs/1412.8070 | id:1412.8070 author:Artiom Kovnatsky, Michael M. Bronstein, Xavier Bresson, Pierre Vandergheynst category:cs.CV  published:2014-12-27 summary:In this paper, we consider the problem of finding dense intrinsic correspondence between manifolds using the recently introduced functional framework. We pose the functional correspondence problem as matrix completion with manifold geometric structure and inducing functional localization with the $L_1$ norm. We discuss efficient numerical procedures for the solution of our problem. Our method compares favorably to the accuracy of state-of-the-art correspondence algorithms on non-rigid shape matching benchmarks, and is especially advantageous in settings when only scarce data is available. version:1
arxiv-1403-5787 | Scalable detection of statistically significant communities and hierarchies, using message-passing for modularity | http://arxiv.org/abs/1403.5787 | id:1403.5787 author:Pan Zhang, Cristopher Moore category:physics.soc-ph cond-mat.stat-mech cs.SI stat.ML  published:2014-03-23 summary:Modularity is a popular measure of community structure. However, maximizing the modularity can lead to many competing partitions, with almost the same modularity, that are poorly correlated with each other. It can also produce illusory "communities" in random graphs where none exist. We address this problem by using the modularity as a Hamiltonian at finite temperature, and using an efficient Belief Propagation algorithm to obtain the consensus of many partitions with high modularity, rather than looking for a single partition that maximizes it. We show analytically and numerically that the proposed algorithm works all the way down to the detectability transition in networks generated by the stochastic block model. It also performs well on real-world networks, revealing large communities in some networks where previous work has claimed no communities exist. Finally we show that by applying our algorithm recursively, subdividing communities until no statistically-significant subcommunities can be found, we can detect hierarchical structure in real-world networks more efficiently than previous methods. version:3
arxiv-1412-7242 | Learning of Proto-object Representations via Fixations on Low Resolution | http://arxiv.org/abs/1412.7242 | id:1412.7242 author:Chengyao Shen, Xun Huang, Qi Zhao category:cs.CV  published:2014-12-23 summary:While previous researches in eye fixation prediction typically rely on integrating low-level features (e.g. color, edge) to form a saliency map, recently it has been found that the structural organization of these features into a proto-object representation can play a more significant role. In this work, we present a computational framework based on deep network to demonstrate that proto-object representations can be learned from low-resolution image patches from fixation regions. We advocate the use of low-resolution inputs in this work due to the following reasons: (1) Proto-objects are computed in parallel over an entire visual field (2) People can perceive or recognize objects well even it is in low resolution. (3) Fixations from lower resolution images can predict fixations on higher resolution images. In the proposed computational model, we extract multi-scale image patches on fixation regions from eye fixation datasets, resize them to low resolution and feed them into a hierarchical. With layer-wise unsupervised feature learning, we find that many proto-objects like features responsive to different shapes of object blobs are learned out. Visualizations also show that these features are selective to potential objects in the scene and the responses of these features work well in predicting eye fixations on the images when combined with learned weights. version:2
arxiv-1412-8010 | Construction of Vietnamese SentiWordNet by using Vietnamese Dictionary | http://arxiv.org/abs/1412.8010 | id:1412.8010 author:Xuan-Son Vu, Seong-Bae Park category:cs.CL  published:2014-12-27 summary:SentiWordNet is an important lexical resource supporting sentiment analysis in opinion mining applications. In this paper, we propose a novel approach to construct a Vietnamese SentiWordNet (VSWN). SentiWordNet is typically generated from WordNet in which each synset has numerical scores to indicate its opinion polarities. Many previous studies obtained these scores by applying a machine learning method to WordNet. However, Vietnamese WordNet is not available unfortunately by the time of this paper. Therefore, we propose a method to construct VSWN from a Vietnamese dictionary, not from WordNet. We show the effectiveness of the proposed method by generating a VSWN with 39,561 synsets automatically. The method is experimentally tested with 266 synsets with aspect of positivity and negativity. It attains a competitive result compared with English SentiWordNet that is 0.066 and 0.052 differences for positivity and negativity sets respectively. version:1
arxiv-1005-5603 | On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem | http://arxiv.org/abs/1005.5603 | id:1005.5603 author:Daniil Ryabko category:cs.LG cs.IT math.IT math.ST stat.TH  published:2010-05-31 summary:A sequence $x_1,\dots,x_n,\dots$ of discrete-valued observations is generated according to some unknown probabilistic law (measure) $\mu$. After observing each outcome, one is required to give conditional probabilities of the next observation. The realizable case is when the measure $\mu$ belongs to an arbitrary but known class $\mathcal C$ of process measures. The non-realizable case is when $\mu$ is completely arbitrary, but the prediction performance is measured with respect to a given set $\mathcal C$ of process measures. We are interested in the relations between these problems and between their solutions, as well as in characterizing the cases when a solution exists and finding these solutions. We show that if the quality of prediction is measured using the total variation distance, then these problems coincide, while if it is measured using the expected average KL divergence, then they are different. For some of the formalizations we also show that when a solution exists, it can be obtained as a Bayes mixture over a countable subset of $\mathcal C$. We also obtain several characterization of those sets $\mathcal C$ for which solutions to the considered problems exist. As an illustration to the general results obtained, we show that a solution to the non-realizable case of the sequence prediction problem exists for the set of all finite-memory processes, but does not exist for the set of all stationary processes. It should be emphasized that the framework is completely general: the processes measures considered are not required to be i.i.d., mixing, stationary, or to belong to any parametric family. version:3
arxiv-1412-7990 | Predicting User Engagement in Twitter with Collaborative Ranking | http://arxiv.org/abs/1412.7990 | id:1412.7990 author:Ernesto Diaz-Aviles, Hoang Thanh Lam, Fabio Pinelli, Stefano Braghin, Yiannis Gkoufas, Michele Berlingerio, Francesco Calabrese category:cs.IR cs.CY cs.LG H.3.3; I.2.6  published:2014-12-26 summary:Collaborative Filtering (CF) is a core component of popular web-based services such as Amazon, YouTube, Netflix, and Twitter. Most applications use CF to recommend a small set of items to the user. For instance, YouTube presents to a user a list of top-n videos she would likely watch next based on her rating and viewing history. Current methods of CF evaluation have been focused on assessing the quality of a predicted rating or the ranking performance for top-n recommended items. However, restricting the recommender system evaluation to these two aspects is rather limiting and neglects other dimensions that could better characterize a well-perceived recommendation. In this paper, instead of optimizing rating or top-n recommendation, we focus on the task of predicting which items generate the highest user engagement. In particular, we use Twitter as our testbed and cast the problem as a Collaborative Ranking task where the rich features extracted from the metadata of the tweets help to complement the transaction information limited to user ids, item ids, ratings and timestamps. We learn a scoring function that directly optimizes the user engagement in terms of nDCG@10 on the predicted ranking. Experiments conducted on an extended version of the MovieTweetings dataset, released as part of the RecSys Challenge 2014, show the effectiveness of our approach. version:1
arxiv-1412-7963 | Texture analysis by multi-resolution fractal descriptors | http://arxiv.org/abs/1412.7963 | id:1412.7963 author:João B. Florindo, Odemir M. Bruno category:cs.CV  published:2014-12-26 summary:This work proposes a texture descriptor based on fractal theory. The method is based on the Bouligand-Minkowski descriptors. We decompose the original image recursively into 4 equal parts. In each recursion step, we estimate the average and the deviation of the Bouligand-Minkowski descriptors computed over each part. Thus, we extract entropy features from both average and deviation. The proposed descriptors are provided by the concatenation of such measures. The method is tested in a classification experiment under well known datasets, that is, Brodatz and Vistex. The results demonstrate that the proposed technique achieves better results than classical and state-of-the-art texture descriptors, such as Gabor-wavelets and co-occurrence matrix. version:1
arxiv-1412-7957 | Detect2Rank : Combining Object Detectors Using Learning to Rank | http://arxiv.org/abs/1412.7957 | id:1412.7957 author:Sezer Karaoglu, Yang Liu, Theo Gevers category:cs.CV  published:2014-12-26 summary:Object detection is an important research area in the field of computer vision. Many detection algorithms have been proposed. However, each object detector relies on specific assumptions of the object appearance and imaging conditions. As a consequence, no algorithm can be considered as universal. With the large variety of object detectors, the subsequent question is how to select and combine them. In this paper, we propose a framework to learn how to combine object detectors. The proposed method uses (single) detectors like DPM, CN and EES, and exploits their correlation by high level contextual features to yield a combined detection list. Experiments on the PASCAL VOC07 and VOC10 datasets show that the proposed method significantly outperforms single object detectors, DPM (8.4%), CN (6.8%) and EES (17.0%) on VOC07 and DPM (6.5%), CN (5.5%) and EES (16.2%) on VOC10. version:1
arxiv-1412-7955 | Unsupervised Learning through Prediction in a Model of Cortex | http://arxiv.org/abs/1412.7955 | id:1412.7955 author:Christos H. Papadimitriou, Santosh S. Vempala category:cs.NE cs.DS q-bio.NC stat.ML  published:2014-12-26 summary:We propose a primitive called PJOIN, for "predictive join," which combines and extends the operations JOIN and LINK, which Valiant proposed as the basis of a computational theory of cortex. We show that PJOIN can be implemented in Valiant's model. We also show that, using PJOIN, certain reasonably complex learning and pattern matching tasks can be performed, in a way that involves phenomena which have been observed in cognition and the brain, namely memory-based prediction and downward traffic in the cortical hierarchy. version:1
arxiv-1412-7934 | A Novel Feature Selection and Extraction Technique for Classification | http://arxiv.org/abs/1412.7934 | id:1412.7934 author:Kratarth Goel, Raunaq Vohra, Ainesh Bakshi category:cs.LG cs.CV  published:2014-12-26 summary:This paper presents a versatile technique for the purpose of feature selection and extraction - Class Dependent Features (CDFs). We use CDFs to improve the accuracy of classification and at the same time control computational expense by tackling the curse of dimensionality. In order to demonstrate the generality of this technique, it is applied to handwritten digit recognition and text categorization. version:1
arxiv-1412-7927 | Polyphonic Music Generation by Modeling Temporal Dependencies Using a RNN-DBN | http://arxiv.org/abs/1412.7927 | id:1412.7927 author:Kratarth Goel, Raunaq Vohra, J. K. Sahoo category:cs.LG cs.AI cs.NE  published:2014-12-26 summary:In this paper, we propose a generic technique to model temporal dependencies and sequences using a combination of a recurrent neural network and a Deep Belief Network. Our technique, RNN-DBN, is an amalgamation of the memory state of the RNN that allows it to provide temporal information and a multi-layer DBN that helps in high level representation of the data. This makes RNN-DBNs ideal for sequence generation. Further, the use of a DBN in conjunction with the RNN makes this model capable of significantly more complex data representation than an RBM. We apply this technique to the task of polyphonic music generation. version:1
arxiv-1412-6631 | Visualizing and Comparing Convolutional Neural Networks | http://arxiv.org/abs/1412.6631 | id:1412.6631 author:Wei Yu, Kuiyuan Yang, Yalong Bai, Hongxun Yao, Yong Rui category:cs.CV  published:2014-12-20 summary:Convolutional Neural Networks (CNNs) have achieved comparable error rates to well-trained human on ILSVRC2014 image classification task. To achieve better performance, the complexity of CNNs is continually increasing with deeper and bigger architectures. Though CNNs achieved promising external classification behavior, understanding of their internal work mechanism is still limited. In this work, we attempt to understand the internal work mechanism of CNNs by probing the internal representations in two comprehensive aspects, i.e., visualizing patches in the representation spaces constructed by different layers, and visualizing visual information kept in each layer. We further compare CNNs with different depths and show the advantages brought by deeper architecture. version:2
arxiv-1402-5564 | Structure Tensor Based Image Interpolation Method | http://arxiv.org/abs/1402.5564 | id:1402.5564 author:Ahmadreza Baghaie, Zeyun Yu category:cs.CV  published:2014-02-22 summary:Feature preserving image interpolation is an active area in image processing field. In this paper a new direct edge directed image super-resolution algorithm based on structure tensors is proposed. Using an isotropic Gaussian filter, the structure tensor at each pixel of the input image is computed and the pixels are classified to three distinct classes; uniform region, corners and edges, according to the eigenvalues of the structure tensor. Due to application of the isotropic Gaussian filter, the classification is robust to noise presented in image. Based on the tangent eigenvector of the structure tensor, the edge direction is determined and used for interpolation along the edges. In comparison to some previous edge directed image interpolation methods, the proposed method achieves higher quality in both subjective and objective aspects. Also the proposed method outperforms previous methods in case of noisy and JPEG compressed images. Furthermore, without the need for optimization in the process, the algorithm can achieve higher speed. version:3
arxiv-1412-7889 | Improved texture image classification through the use of a corrosion-inspired cellular automaton | http://arxiv.org/abs/1412.7889 | id:1412.7889 author:Núbia Rosa da Silva, Pieter Van der Weeën, Bernard De Baets, Odemir Martinez Bruno category:cs.CV  published:2014-12-26 summary:In this paper, the problem of classifying synthetic and natural texture images is addressed. To tackle this problem, an innovative method is proposed that combines concepts from corrosion modeling and cellular automata to generate a texture descriptor. The core processes of metal (pitting) corrosion are identified and applied to texture images by incorporating the basic mechanisms of corrosion in the transition function of the cellular automaton. The surface morphology of the image is analyzed before and during the application of the transition function of the cellular automaton. In each iteration the cumulative mass of corroded product is obtained to construct each of the attributes of the texture descriptor. In a final step, this texture descriptor is used for image classification by applying Linear Discriminant Analysis. The method was tested on the well-known Brodatz and Vistex databases. In addition, in order to verify the robustness of the method, its invariance to noise and rotation were tested. To that end, different variants of the original two databases were obtained through addition of noise to and rotation of the images. The results showed that the method is effective for texture classification according to the high success rates obtained in all cases. This indicates the potential of employing methods inspired on natural phenomena in other fields. version:1
arxiv-1412-7884 | Sparkle Vision: Seeing the World through Random Specular Microfacets | http://arxiv.org/abs/1412.7884 | id:1412.7884 author:Zhengdong Zhang, Phillip Isola, Edward H. Adelson category:cs.CV  published:2014-12-26 summary:In this paper, we study the problem of reproducing the world lighting from a single image of an object covered with random specular microfacets on the surface. We show that such reflectors can be interpreted as a randomized mapping from the lighting to the image. Such specular objects have very different optical properties from both diffuse surfaces and smooth specular objects like metals, so we design special imaging system to robustly and effectively photograph them. We present simple yet reliable algorithms to calibrate the proposed system and do the inference. We conduct experiments to verify the correctness of our model assumptions and prove the effectiveness of our pipeline. version:1
arxiv-1412-7880 | Enhancing fractal descriptors on images by combining boundary and interior of Minkowski dilation | http://arxiv.org/abs/1412.7880 | id:1412.7880 author:Marcos W. S. Oliveira, Dalcimar Casanova, João B. Florindo, Odemir Martinez Bruno category:physics.data-an cs.CV  published:2014-12-26 summary:This work proposes to obtain novel fractal descriptors from gray-level texture images by combining information from interior and boundary measures of the Minkowski dilation applied to the texture surface. At first, the image is converted into a surface where the height of each point is the gray intensity of the respective pixel in that position in the image. Thus, this surface is morphologically dilated by spheres. The radius of such spheres is ranged within an interval and the volume and the external area of the dilated structure are computed for each radius. The final descriptors are given by such measures concatenated and subject to a canonical transform to reduce the dimensionality. The proposal is an enhancement to the classical Bouligand-Minkowski fractal descriptors, where only the volume (interior) information is considered. As different structures may have the same volume, but not the same area, the proposal yields to more rich descriptors as confirmed by results on the classification of benchmark databases. version:1
arxiv-1412-7868 | Gaussian Process Pseudo-Likelihood Models for Sequence Labeling | http://arxiv.org/abs/1412.7868 | id:1412.7868 author:P. K. Srijith, P. Balamurugan, Shirish Shevade category:cs.LG stat.ML  published:2014-12-25 summary:Several machine learning problems arising in natural language processing can be modeled as a sequence labeling problem. We provide Gaussian process models based on pseudo-likelihood approximation to perform sequence labeling. Gaussian processes (GPs) provide a Bayesian approach to learning in a kernel based framework. The pseudo-likelihood model enables one to capture long range dependencies among the output components of the sequence without becoming computationally intractable. We use an efficient variational Gaussian approximation method to perform inference in the proposed model. We also provide an iterative algorithm which can effectively make use of the information from the neighboring labels to perform prediction. The ability to capture long range dependencies makes the proposed approach useful for a wide range of sequence labeling problems. Numerical experiments on some sequence labeling data sets demonstrate the usefulness of the proposed approach. version:1
arxiv-1412-7856 | Gabor wavelets combined with volumetric fractal dimension applied to texture analysis | http://arxiv.org/abs/1412.7856 | id:1412.7856 author:Álvaro Gomez Z., João B. Florindo, Odemir M. Bruno category:cs.CV  published:2014-12-25 summary:Texture analysis and classification remain as one of the biggest challenges for the field of computer vision and pattern recognition. On this matter, Gabor wavelets has proven to be a useful technique to characterize distinctive texture patterns. However, most of the approaches used to extract descriptors of the Gabor magnitude space usually fail in representing adequately the richness of detail present into a unique feature vector. In this paper, we propose a new method to enhance the Gabor wavelets process extracting a fractal signature of the magnitude spaces. Each signature is reduced using a canonical analysis function and concatenated to form the final feature vector. Experiments were conducted on several texture image databases to prove the power and effectiveness of the proposed method. Results obtained shown that this method outperforms other early proposed method, creating a more reliable technique for texture feature extraction. version:1
arxiv-1412-7854 | Joint Deep Learning for Car Detection | http://arxiv.org/abs/1412.7854 | id:1412.7854 author:Seyedshams Feyzabadi category:cs.CV  published:2014-12-25 summary:Traditional object recognition approaches apply feature extraction, part deformation handling, occlusion handling and classification sequentially while they are independent from each other. Ouyang and Wang proposed a model for jointly learning of all of the mentioned processes using one deep neural network. We utilized, and manipulated their toolbox in order to apply it in car detection scenarios where it had not been tested. Creating a single deep architecture from these components, improves the interaction between them and can enhance the performance of the whole system. We believe that the approach can be used as a general purpose object detection toolbox. We tested the algorithm on UIUC car dataset, and achieved a reasonable result. The accuracy of our method was 86 % while there are better results of accuracy with up to 91 % and will be shown later. We strongly believe that having an experiment on a larger dataset can show the advantage of using deep models over shallow ones. version:1
arxiv-1412-7851 | Fractal descriptors based on the probability dimension: a texture analysis and classification approach | http://arxiv.org/abs/1412.7851 | id:1412.7851 author:João Batista Florindo, Odemir Martinez Bruno category:cs.CV  published:2014-12-25 summary:In this work, we propose a novel technique for obtaining descriptors of gray-level texture images. The descriptors are provided by applying a multiscale transform to the fractal dimension of the image estimated through the probability (Voss) method. The effectiveness of the descriptors is verified in a classification task using benchmark over texture datasets. The results obtained demonstrate the efficiency of the proposed method as a tool for the description and discrimination of texture images. version:1
arxiv-1412-7849 | Brachiaria species identification using imaging techniques based on fractal descriptors | http://arxiv.org/abs/1412.7849 | id:1412.7849 author:João Batista Florindo, Núbia Rosa da Silva, Liliane Maria Romualdo, Fernanda de Fátima da Silva, Pedro Henrique de Cerqueira Luz, Valdo Rodrigues Herling, Odemir Martinez Bruno category:cs.CV  published:2014-12-25 summary:The use of a rapid and accurate method in diagnosis and classification of species and/or cultivars of forage has practical relevance, scientific and trade in various areas of study. Thus, leaf samples of fodder plant species \textit{Brachiaria} were previously identified, collected and scanned to be treated by means of artificial vision to make the database and be used in subsequent classifications. Forage crops used were: \textit{Brachiaria decumbens} cv. IPEAN; \textit{Brachiaria ruziziensis} Germain \& Evrard; \textit{Brachiaria Brizantha} (Hochst. ex. A. Rich.) Stapf; \textit{Brachiaria arrecta} (Hack.) Stent. and \textit{Brachiaria spp}. The images were analyzed by the fractal descriptors method, where a set of measures are obtained from the values of the fractal dimension at different scales. Therefore such values are used as inputs for a state-of-the-art classifier, the Support Vector Machine, which finally discriminates the images according to the respective species. version:1
arxiv-1412-7844 | Texture analysis using volume-radius fractal dimension | http://arxiv.org/abs/1412.7844 | id:1412.7844 author:André R. Backes, Odemir M. Bruno category:cs.CV  published:2014-12-25 summary:Texture plays an important role in computer vision. It is one of the most important visual attributes used in image analysis, once it provides information about pixel organization at different regions of the image. This paper presents a novel approach for texture characterization, based on complexity analysis. The proposed approach expands the idea of the Mass-radius fractal dimension, a method originally developed for shape analysis, to a set of coordinates in 3D-space that represents the texture under analysis in a signature able to characterize efficiently different texture classes in terms of complexity. An experiment using images from the Brodatz album illustrates the method performance. version:1
arxiv-1407-6251 | FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory and Computation | http://arxiv.org/abs/1407.6251 | id:1407.6251 author:Philip Lenz, Andreas Geiger, Raquel Urtasun category:cs.CV  published:2014-07-23 summary:One of the most popular approaches to multi-target tracking is tracking-by-detection. Current min-cost flow algorithms which solve the data association problem optimally have three main drawbacks: they are computationally expensive, they assume that the whole video is given as a batch, and they scale badly in memory and computation with the length of the video sequence. In this paper, we address each of these issues, resulting in a computationally and memory-bounded solution. First, we introduce a dynamic version of the successive shortest-path algorithm which solves the data association problem optimally while reusing computation, resulting in significantly faster inference than standard solvers. Second, we address the optimal solution to the data association problem when dealing with an incoming stream of data (i.e., online setting). Finally, we present our main contribution which is an approximate online solution with bounded memory and computation which is capable of handling videos of arbitrarily length while performing tracking in real time. We demonstrate the effectiveness of our algorithms on the KITTI and PETS2009 benchmarks and show state-of-the-art performance, while being significantly faster than existing solvers. version:2
arxiv-1412-7782 | Plagiarism Detection on Electronic Text based Assignments using Vector Space Model (ICIAfS14) | http://arxiv.org/abs/1412.7782 | id:1412.7782 author:MAC Jiffriya, MAC Akmal Jahan, Roshan G. Ragel category:cs.IR cs.CL  published:2014-12-25 summary:Plagiarism is known as illegal use of others' part of work or whole work as one's own in any field such as art, poetry, literature, cinema, research and other creative forms of study. Plagiarism is one of the important issues in academic and research fields and giving more concern in academic systems. The situation is even worse with the availability of ample resources on the web. This paper focuses on an effective plagiarism detection tool on identifying suitable intra-corpal plagiarism detection for text based assignments by comparing unigram, bigram, trigram of vector space model with cosine similarity measure. Manually evaluated, labelled dataset was tested using unigram, bigram and trigram vector. Even though trigram vector consumes comparatively more time, it shows better results with the labelled data. In addition, the selected trigram vector space model with cosine similarity measure is compared with tri-gram sequence matching technique with Jaccard measure. In the results, cosine similarity score shows slightly higher values than the other. Because, it focuses on giving more weight for terms that do not frequently exist in the dataset and cosine similarity measure using trigram technique is more preferable than the other. Therefore, we present our new tool and it could be used as an effective tool to evaluate text based electronic assignments and minimize the plagiarism among students. version:1
arxiv-1412-7774 | Improved Parameter Identification Method Based on Moving Rate | http://arxiv.org/abs/1412.7774 | id:1412.7774 author:Chol Man Ho, Son Il Gwak, Song Ho Pak, Jong Won Ha category:cs.NE  published:2014-12-25 summary:To improve the problem that the parameter identification for fuzzy neural network has many time complexities in calculating, an improved T-S fuzzy inference method and an parameter identification method for fuzzy neural network are proposed. It mainly includes three parts. First, improved fuzzy inference method based on production term for T-S Fuzzy model is explained. Then, compared with existing Sugeno fuzzy inference based on Compositional rules and type-distance fuzzy inference method, the proposed fuzzy inference algorithm has a less amount of complexity in calculating and the calculating process is simple. Next, a parameter identification method for FNN based on production inference is proposed. Finally, the proposed method is applied for the precipitation forecast and security situation prediction. Test results showed that the proposed method significantly improved the effectiveness of identification, reduced the learning order, time complexity and learning error. version:1
arxiv-1412-7705 | Concentration for matrix martingales in continuous time and microscopic activity of social networks | http://arxiv.org/abs/1412.7705 | id:1412.7705 author:Emmanuel Bacry, Stéphane Gaïffas, Jean-François Muzy category:math.PR stat.ML  published:2014-12-24 summary:This paper gives new concentration inequalities for the spectral norm of matrix martingales in continuous time. Both cases of purely discountinuous and continuous martingales are considered. The analysis is based on a new supermartingale property of the trace exponential, based on tools from stochastic calculus. Matrix martingales in continuous time are probabilistic objects that naturally appear for statistical learning of time-dependent systems. We focus here on the the microscopic study of (social) networks, based on self-exciting counting processes, such as the Hawkes process, together with a low-rank prior assumption of the self-exciting component. A consequence of these new concentration inequalities is a push forward of the theoretical analysis of such models. version:1
arxiv-1412-7689 | Locating Tables in Scanned Documents for Reconstructing and Republishing (ICIAfS14) | http://arxiv.org/abs/1412.7689 | id:1412.7689 author:Akmal Jahan Mac, Roshan G Ragel category:cs.CV  published:2014-12-24 summary:Pool of knowledge available to the mankind depends on the source of learning resources, which can vary from ancient printed documents to present electronic material. The rapid conversion of material available in traditional libraries to digital form needs a significant amount of work if we are to maintain the format and the look of the electronic documents as same as their printed counterparts. Most of the printed documents contain not only characters and its formatting but also some associated non text objects such as tables, charts and graphical objects. It is challenging to detect them and to concentrate on the format preservation of the contents while reproducing them. To address this issue, we propose an algorithm using local thresholds for word space and line height to locate and extract all categories of tables from scanned document images. From the experiments performed on 298 documents, we conclude that our algorithm has an overall accuracy of about 75% in detecting tables from the scanned document images. Since the algorithm does not completely depend on rule lines, it can detect all categories of tables in a range of scanned documents with different font types, styles and sizes to extract their formatting features. Moreover, the algorithm can be applied to locate tables in multi column layouts with small modification in layout analysis. Treating tables with their existing formatting features will tremendously help the reproducing of printed documents for reprinting and updating purposes. version:1
arxiv-1412-7680 | A Fuzzy Based Model to Identify Printed Sinhala Characters (ICIAfS14) | http://arxiv.org/abs/1412.7680 | id:1412.7680 author:G. I. Gunarathna, M. A. P. Chamikara, R. G. Ragel category:cs.CV  published:2014-12-24 summary:Character recognition techniques for printed documents are widely used for English language. However, the systems that are implemented to recognize Asian languages struggle to increase the accuracy of recognition. Among other Asian languages (such as Arabic, Tamil, Chinese), Sinhala characters are unique, mainly because they are round in shape. This unique feature makes it a challenge to extend the prevailing techniques to improve recognition of Sinhala characters. Therefore, a little attention has been given to improve the accuracy of Sinhala character recognition. A novel method, which makes use of this unique feature, could be advantageous over other methods. This paper describes the use of a fuzzy inference system to recognize Sinhala characters. Feature extraction is mainly focused on distance and intersection measurements in different directions from the center of the letter making use of the round shape of characters. The results showed an overall accuracy of 90.7% for 140 instances of letters tested, much better than similar systems. version:1
arxiv-1412-7638 | Inference for Sparse Conditional Precision Matrices | http://arxiv.org/abs/1412.7638 | id:1412.7638 author:Jialei Wang, Mladen Kolar category:stat.ML  published:2014-12-24 summary:Given $n$ i.i.d. observations of a random vector $(X,Z)$, where $X$ is a high-dimensional vector and $Z$ is a low-dimensional index variable, we study the problem of estimating the conditional inverse covariance matrix $\Omega(z) = (E[(X-E[X \mid Z])(X-E[X \mid Z])^T \mid Z=z])^{-1}$ under the assumption that the set of non-zero elements is small and does not depend on the index variable. We develop a novel procedure that combines the ideas of the local constant smoothing and the group Lasso for estimating the conditional inverse covariance matrix. A proximal iterative smoothing algorithm is used to solve the corresponding convex optimization problems. We prove that our procedure recovers the conditional independence assumptions of the distribution $X \mid Z$ with high probability. This result is established by developing a uniform deviation bound for the high-dimensional conditional covariance matrix from its population counterpart, which may be of independent interest. Furthermore, we develop point-wise confidence intervals for individual elements of the conditional inverse covariance matrix. We perform extensive simulation studies, in which we demonstrate that our proposed procedure outperforms sensible competitors. We illustrate our proposal on a S&P 500 stock price data set. version:1
arxiv-1412-7513 | Symmetry in Image Registration and Deformation Modeling | http://arxiv.org/abs/1412.7513 | id:1412.7513 author:Stefan Sommer, Henry O. Jacobs category:cs.CV math.DG  published:2014-12-23 summary:We survey the role of symmetry in diffeomorphic registration of landmarks, curves, surfaces, images and higher-order data. The infinite dimensional problem of finding correspondences between objects can for a range of concrete data types be reduced resulting in compact representations of shape and spatial structure. This reduction is possible because the available data is incomplete in encoding the full deformation model. Using reduction by symmetry, we describe the reduced models in a common theoretical framework that draws on links between the registration problem and geometric mechanics. Symmetry also arises in reduction to the Lie algebra using particle relabeling symmetry allowing the equations of motion to be written purely in terms of Eulerian velocity field. Reduction by symmetry has recently been applied for jet-matching and higher-order discrete approximations of the image matching problem. We outline these constructions and further cases where reduction by symmetry promises new approaches to registration of complex data types. version:2
arxiv-1412-7626 | AltecOnDB: A Large-Vocabulary Arabic Online Handwriting Recognition Database | http://arxiv.org/abs/1412.7626 | id:1412.7626 author:Ibrahim Abdelaziz, Sherif Abdou category:cs.CV  published:2014-12-24 summary:Arabic is a semitic language characterized by a complex and rich morphology. The exceptional degree of ambiguity in the writing system, the rich morphology, and the highly complex word formation process of roots and patterns all contribute to making computational approaches to Arabic very challenging. As a result, a practical handwriting recognition system should support large vocabulary to provide a high coverage and use the context information for disambiguation. Several research efforts have been devoted for building online Arabic handwriting recognition systems. Most of these methods are either using their small private test data sets or a standard database with limited lexicon and coverage. A large scale handwriting database is an essential resource that can advance the research of online handwriting recognition. Currently, there is no online Arabic handwriting database with large lexicon, high coverage, large number of writers and training/testing data. In this paper, we introduce AltecOnDB, a large scale online Arabic handwriting database. AltecOnDB has 98% coverage of all the possible PAWS of the Arabic language. The collected samples are complete sentences that include digits and punctuation marks. The collected data is available on sentence, word and character levels, hence, high-level linguistic models can be used for performance improvements. Data is collected from more than 1000 writers with different backgrounds, genders and ages. Annotation and verification tools are developed to facilitate the annotation and verification phases. We built an elementary recognition system to test our database and show the existing difficulties when handling a large vocabulary and dealing with large amounts of styles variations in the collected data. version:1
arxiv-1402-0694 | Particle Metropolis adjusted Langevin algorithms for state space models | http://arxiv.org/abs/1402.0694 | id:1402.0694 author:Chris Nemeth, Paul Fearnhead category:stat.CO stat.ML  published:2014-02-04 summary:Particle MCMC is a class of algorithms that can be used to analyse state-space models. They use MCMC moves to update the parameters of the models, and particle filters to propose values for the path of the state-space model. Currently the default is to use random walk Metropolis to update the parameter values. We show that it is possible to use information from the output of the particle filter to obtain better proposal distributions for the parameters. In particular it is possible to obtain estimates of the gradient of the log posterior from each run of the particle filter, and use these estimates within a Langevin-type proposal. We propose using the recent computationally efficient approach of Nemeth et al. (2013) for obtaining such estimates. We show empirically that for a variety of state-space models this proposal is more efficient than the standard random walk Metropolis proposal in terms of: reducing autocorrelation of the posterior samples, reducing the burn-in time of the MCMC sampler and increasing the squared jump distance between posterior samples. version:2
arxiv-1412-7978 | The Computational Theory of Intelligence: Information Entropy | http://arxiv.org/abs/1412.7978 | id:1412.7978 author:Daniel Kovach category:cs.AI cs.LG 68T27 I.2.1  published:2014-12-24 summary:This paper presents an information theoretic approach to the concept of intelligence in the computational sense. We introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level. Using this new scheme, we develop a simple data driven clustering example and discuss its applications. version:1
arxiv-1412-6181 | Crypto-Nets: Neural Networks over Encrypted Data | http://arxiv.org/abs/1412.6181 | id:1412.6181 author:Pengtao Xie, Misha Bilenko, Tom Finley, Ran Gilad-Bachrach, Kristin Lauter, Michael Naehrig category:cs.LG cs.CR cs.NE  published:2014-12-18 summary:The problem we address is the following: how can a user employ a predictive model that is held by a third party, without compromising private information. For example, a hospital may wish to use a cloud service to predict the readmission risk of a patient. However, due to regulations, the patient's medical files cannot be revealed. The goal is to make an inference using the model, without jeopardizing the accuracy of the prediction or the privacy of the data. To achieve high accuracy, we use neural networks, which have been shown to outperform other learning models for many tasks. To achieve the privacy requirements, we use homomorphic encryption in the following protocol: the data owner encrypts the data and sends the ciphertexts to the third party to obtain a prediction from a trained model. The model operates on these ciphertexts and sends back the encrypted prediction. In this protocol, not only the data remains private, even the values predicted are available only to the data owner. Using homomorphic encryption and modifications to the activation functions and training algorithms of neural networks, we show that it is protocol is possible and may be feasible. This method paves the way to build a secure cloud-based neural network prediction services without invading users' privacy. version:2
arxiv-1412-7584 | Differential Privacy and Machine Learning: a Survey and Review | http://arxiv.org/abs/1412.7584 | id:1412.7584 author:Zhanglong Ji, Zachary C. Lipton, Charles Elkan category:cs.LG cs.CR cs.DB  published:2014-12-24 summary:The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals. In this paper, we consider differential privacy, one of the most popular and powerful definitions of privacy. We explore the interplay between machine learning and differential privacy, namely privacy-preserving machine learning algorithms and learning-based data release mechanisms. We also describe some theoretical results that address what can be learned differentially privately and upper bounds of loss functions for differentially private algorithms. Finally, we present some open questions, including how to incorporate public data, how to deal with missing data in private datasets, and whether, as the number of observed samples grows arbitrarily large, differentially private machine learning algorithms can be achieved at no cost to utility as compared to corresponding non-differentially private algorithms. version:1
arxiv-1302-5010 | Matching Pursuit LASSO Part II: Applications and Sparse Recovery over Batch Signals | http://arxiv.org/abs/1302.5010 | id:1302.5010 author:Mingkui Tan, Ivor W. Tsang, Li Wang category:cs.CV cs.LG stat.ML  published:2013-02-20 summary:Matching Pursuit LASSIn Part I \cite{TanPMLPart1}, a Matching Pursuit LASSO ({MPL}) algorithm has been presented for solving large-scale sparse recovery (SR) problems. In this paper, we present a subspace search to further improve the performance of MPL, and then continue to address another major challenge of SR -- batch SR with many signals, a consideration which is absent from most of previous $\ell_1$-norm methods. As a result, a batch-mode {MPL} is developed to vastly speed up sparse recovery of many signals simultaneously. Comprehensive numerical experiments on compressive sensing and face recognition tasks demonstrate the superior performance of MPL and BMPL over other methods considered in this paper, in terms of sparse recovery ability and efficiency. In particular, BMPL is up to 400 times faster than existing $\ell_1$-norm methods considered to be state-of-the-art.O Part II: Applications and Sparse Recovery over Batch Signals version:2
arxiv-1308-0187 | A Time and Space Efficient Junction Tree Architecture | http://arxiv.org/abs/1308.0187 | id:1308.0187 author:Stephen Pasteris category:cs.AI cs.LG  published:2013-07-31 summary:The junction tree algorithm is a way of computing marginals of boolean multivariate probability distributions that factorise over sets of random variables. The junction tree algorithm first constructs a tree called a junction tree who's vertices are sets of random variables. The algorithm then performs a generalised version of belief propagation on the junction tree. The Shafer-Shenoy and Hugin architectures are two ways to perform this belief propagation that tradeoff time and space complexities in different ways: Hugin propagation is at least as fast as Shafer-Shenoy propagation and in the cases that we have large vertices of high degree is significantly faster. However, this speed increase comes at the cost of an increased space complexity. This paper first introduces a simple novel architecture, ARCH-1, which has the best of both worlds: the speed of Hugin propagation and the low space requirements of Shafer-Shenoy propagation. A more complicated novel architecture, ARCH-2, is then introduced which has, up to a factor only linear in the maximum cardinality of any vertex, time and space complexities at least as good as ARCH-1 and in the cases that we have large vertices of high degree is significantly faster than ARCH-1. version:9
arxiv-1412-7468 | Model Selection in High-Dimensional Misspecified Models | http://arxiv.org/abs/1412.7468 | id:1412.7468 author:Pallavi Basu, Yang Feng, Jinchi Lv category:math.ST stat.ME stat.ML stat.TH  published:2014-12-23 summary:Model selection is indispensable to high-dimensional sparse modeling in selecting the best set of covariates among a sequence of candidate models. Most existing work assumes implicitly that the model is correctly specified or of fixed dimensions. Yet model misspecification and high dimensionality are common in real applications. In this paper, we investigate two classical Kullback-Leibler divergence and Bayesian principles of model selection in the setting of high-dimensional misspecified models. Asymptotic expansions of these principles reveal that the effect of model misspecification is crucial and should be taken into account, leading to the generalized AIC and generalized BIC in high dimensions. With a natural choice of prior probabilities, we suggest the generalized BIC with prior probability which involves a logarithmic factor of the dimensionality in penalizing model complexity. We further establish the consistency of the covariance contrast matrix estimator in a general setting. Our results and new method are supported by numerical studies. version:1
arxiv-1412-6093 | Learning Temporal Dependencies in Data Using a DBN-BLSTM | http://arxiv.org/abs/1412.6093 | id:1412.6093 author:Kratarth Goel, Raunaq Vohra category:cs.LG cs.NE  published:2014-12-18 summary:Since the advent of deep learning, it has been used to solve various problems using many different architectures. The application of such deep architectures to auditory data is also not uncommon. However, these architectures do not always adequately consider the temporal dependencies in data. We thus propose a new generic architecture called the Deep Belief Network - Bidirectional Long Short-Term Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data. We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results. version:2
arxiv-1506-01939 | Facial Expressions recognition Based on Principal Component Analysis (PCA) | http://arxiv.org/abs/1506.01939 | id:1506.01939 author:Abdelmajid Hassan Mansour, Gafar Zen Alabdeen Salh, Ali Shaif Alhalemi category:cs.CV  published:2014-12-23 summary:The facial expression recognition is an ocular task that can be performed without human discomfort, is really a speedily growing on the computer research field. There are many applications and programs uses facial expression to evaluate human character, judgment, feelings, and viewpoint. The process of recognizing facial expression is a hard task due to the several circumstances such as facial occlusions, face shape, illumination, face colors, and etc. This paper present a PCA methodology to distinguish expressions of faces under different circumstances and identifying it. Relies on Eigen faces technique using standard Data base images. So as to overcome the problem of difficulty to computers to identify the features and expressions of persons. version:1
arxiv-1306-5226 | Global registration of multiple point clouds using semidefinite programming | http://arxiv.org/abs/1306.5226 | id:1306.5226 author:Kunal N. Chaudhury, Yuehaw Khoo, Amit Singer category:cs.CV cs.NA math.NA math.OC 90C22  52C25  05C50  published:2013-06-21 summary:Consider $N$ points in $\mathbb{R}^d$ and $M$ local coordinate systems that are related through unknown rigid transforms. For each point we are given (possibly noisy) measurements of its local coordinates in some of the coordinate systems. Alternatively, for each coordinate system, we observe the coordinates of a subset of the points. The problem of estimating the global coordinates of the $N$ points (up to a rigid transform) from such measurements comes up in distributed approaches to molecular conformation and sensor network localization, and also in computer vision and graphics. The least-squares formulation of this problem, though non-convex, has a well known closed-form solution when $M=2$ (based on the singular value decomposition). However, no closed form solution is known for $M\geq 3$. In this paper, we demonstrate how the least-squares formulation can be relaxed into a convex program, namely a semidefinite program (SDP). By setting up connections between the uniqueness of this SDP and results from rigidity theory, we prove conditions for exact and stable recovery for the SDP relaxation. In particular, we prove that the SDP relaxation can guarantee recovery under more adversarial conditions compared to earlier proposed spectral relaxations, and derive error bounds for the registration error incurred by the SDP relaxation. We also present results of numerical experiments on simulated data to confirm the theoretical findings. We empirically demonstrate that (a) unlike the spectral relaxation, the relaxation gap is mostly zero for the semidefinite program (i.e., we are able to solve the original non-convex least-squares problem) up to a certain noise threshold, and (b) the semidefinite program performs significantly better than spectral and manifold-optimization methods, particularly at large noise levels. version:5
arxiv-1412-7277 | Fusing Color and Texture Cues to Categorize the Fruit Diseases from Images | http://arxiv.org/abs/1412.7277 | id:1412.7277 author:Shiv Ram Dubey, Anand Singh Jalal category:cs.CV  published:2014-12-23 summary:The economic and production losses in agricultural industry worldwide are due to the presence of diseases in the several kinds of fruits. In this paper, a method for the classification of fruit diseases is proposed and experimentally validated. The image processing based proposed approach is composed of the following main steps; in the first step K-Means clustering technique is used for the defect segmentation, in the second step color and textural cues are extracted and fused from the segmented image, and finally images are classified into one of the classes by using a Multi-class Support Vector Machine. We have considered diseases of apple as a test case and evaluated our approach for three types of apple diseases namely apple scab, apple blotch and apple rot and normal apples without diseases. Our experimentation points out that the proposed fusion scheme can significantly support accurate detection and automatic classification of fruit diseases. version:1
arxiv-1408-1031 | Text to Multi-level MindMaps: A Novel Method for Hierarchical Visual Abstraction of Natural Language Text | http://arxiv.org/abs/1408.1031 | id:1408.1031 author:Mohamed Elhoseiny, Ahmed Elgammal category:cs.CL cs.HC  published:2014-08-01 summary:MindMapping is a well-known technique used in note taking, which encourages learning and studying. MindMapping has been manually adopted to help present knowledge and concepts in a visual form. Unfortunately, there is no reliable automated approach to generate MindMaps from Natural Language text. This work firstly introduces MindMap Multilevel Visualization concept which is to jointly visualize and summarize textual information. The visualization is achieved pictorially across multiple levels using semantic information (i.e. ontology), while the summarization is achieved by the information in the highest levels as they represent abstract information in the text. This work also presents the first automated approach that takes a text input and generates a MindMap visualization out of it. The approach could visualize text documents in multilevel MindMaps, in which a high-level MindMap node could be expanded into child MindMaps. \ignore{ As far as we know, this is the first work that view MindMapping as a new approach to jointly summarize and visualize textual information.} The proposed method involves understanding of the input text and converting it into intermediate Detailed Meaning Representation (DMR). The DMR is then visualized with two modes; Single level or Multiple levels, which is convenient for larger text. The generated MindMaps from both approaches were evaluated based on Human Subject experiments performed on Amazon Mechanical Turk with various parameter settings. version:2
arxiv-1408-5823 | Improved Distributed Principal Component Analysis | http://arxiv.org/abs/1408.5823 | id:1408.5823 author:Maria-Florina Balcan, Vandana Kanchanapally, Yingyu Liang, David Woodruff category:cs.LG  published:2014-08-25 summary:We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve $\ell_2$-error fitting problems such as $k$-means clustering and subspace clustering. The essential properties of an approximate distributed PCA algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications. We give new algorithms and analyses for distributed PCA which lead to improved communication and computational costs for $k$-means clustering and related problems. Our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality. Some of these techniques we develop, such as a general transformation from a constant success probability subspace embedding to a high success probability subspace embedding with a dimension and sparsity independent of the success probability, may be of independent interest. version:5
arxiv-1412-7216 | An $\{l_1,l_2,l_{\infty}\}$-Regularization Approach to High-Dimensional Errors-in-variables Models | http://arxiv.org/abs/1412.7216 | id:1412.7216 author:Alexandre Belloni, Mathieu Rosenbaum, Alexandre B. Tsybakov category:math.ST stat.ML stat.TH  published:2014-12-22 summary:Several new estimation methods have been recently proposed for the linear regression model with observation error in the design. Different assumptions on the data generating process have motivated different estimators and analysis. In particular, the literature considered (1) observation errors in the design uniformly bounded by some $\bar \delta$, and (2) zero mean independent observation errors. Under the first assumption, the rates of convergence of the proposed estimators depend explicitly on $\bar \delta$, while the second assumption has been applied when an estimator for the second moment of the observational error is available. This work proposes and studies two new estimators which, compared to other procedures for regression models with errors in the design, exploit an additional $l_{\infty}$-norm regularization. The first estimator is applicable when both (1) and (2) hold but does not require an estimator for the second moment of the observational error. The second estimator is applicable under (2) and requires an estimator for the second moment of the observation error. Importantly, we impose no assumption on the accuracy of this pilot estimator, in contrast to the previously known procedures. As the recent proposals, we allow the number of covariates to be much larger than the sample size. We establish the rates of convergence of the estimators and compare them with the bounds obtained for related estimators in the literature. These comparisons show interesting insights on the interplay of the assumptions and the achievable rates of convergence. version:1
arxiv-1412-7215 | Online Distributed Optimization on Dynamic Networks | http://arxiv.org/abs/1412.7215 | id:1412.7215 author:Saghar Hosseini, Airlie Chapman, Mehran Mesbahi category:math.OC cs.DS cs.LG cs.MA cs.SY  published:2014-12-22 summary:This paper presents a distributed optimization scheme over a network of agents in the presence of cost uncertainties and over switching communication topologies. Inspired by recent advances in distributed convex optimization, we propose a distributed algorithm based on a dual sub-gradient averaging. The objective of this algorithm is to minimize a cost function cooperatively. Furthermore, the algorithm changes the weights on the communication links in the network to adapt to varying reliability of neighboring agents. A convergence rate analysis as a function of the underlying network topology is then presented, followed by simulation results for representative classes of sensor networks. version:1
arxiv-1307-5449 | Non-stationary Stochastic Optimization | http://arxiv.org/abs/1307.5449 | id:1307.5449 author:O. Besbes, Y. Gur, A. Zeevi category:math.PR cs.LG stat.ML  published:2013-07-20 summary:We consider a non-stationary variant of a sequential stochastic optimization problem, in which the underlying cost functions may change along the horizon. We propose a measure, termed variation budget, that controls the extent of said change, and study how restrictions on this budget impact achievable performance. We identify sharp conditions under which it is possible to achieve long-run-average optimality and more refined performance measures such as rate optimality that fully characterize the complexity of such problems. In doing so, we also establish a strong connection between two rather disparate strands of literature: adversarial online convex optimization; and the more traditional stochastic approximation paradigm (couched in a non-stationary setting). This connection is the key to deriving well performing policies in the latter, by leveraging structure of optimal policies in the former. Finally, tight bounds on the minimax regret allow us to quantify the "price of non-stationarity," which mathematically captures the added complexity embedded in a temporally changing environment versus a stationary one. version:2
arxiv-1412-7193 | Audio Source Separation Using a Deep Autoencoder | http://arxiv.org/abs/1412.7193 | id:1412.7193 author:Giljin Jang, Han-Gyu Kim, Yung-Hwan Oh category:cs.SD cs.LG cs.NE  published:2014-12-22 summary:This paper proposes a novel framework for unsupervised audio source separation using a deep autoencoder. The characteristics of unknown source signals mixed in the mixed input is automatically by properly configured autoencoders implemented by a network with many layers, and separated by clustering the coefficient vectors in the code layer. By investigating the weight vectors to the final target, representation layer, the primitive components of the audio signals in the frequency domain are observed. By clustering the activation coefficients in the code layer, the previously unknown source signals are segregated. The original source sounds are then separated and reconstructed by using code vectors which belong to different clusters. The restored sounds are not perfect but yield promising results for the possibility in the success of many practical applications. version:1
arxiv-1412-7180 | Bayesian Optimisation for Machine Translation | http://arxiv.org/abs/1412.7180 | id:1412.7180 author:Yishu Miao, Ziyu Wang, Phil Blunsom category:cs.CL cs.LG I.2.7  published:2014-12-22 summary:This paper presents novel Bayesian optimisation algorithms for minimum error rate training of statistical machine translation systems. We explore two classes of algorithms for efficiently exploring the translation space, with the first based on N-best lists and the second based on a hypergraph representation that compactly represents an exponential number of translation options. Our algorithms exhibit faster convergence and are capable of obtaining lower error rates than the existing translation model specific approaches, all within a generic Bayesian optimisation framework. Further more, we also introduce a random embedding algorithm to scale our approach to sparse high dimensional feature sets. version:1
arxiv-1412-4314 | Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus | http://arxiv.org/abs/1412.4314 | id:1412.4314 author:Joseph Chee Chang, Chu-Cheng Lin category:cs.NE cs.CL  published:2014-12-14 summary:Mixed language data is one of the difficult yet less explored domains of natural language processing. Most research in fields like machine translation or sentiment analysis assume monolingual input. However, people who are capable of using more than one language often communicate using multiple languages at the same time. Sociolinguists believe this "code-switching" phenomenon to be socially motivated. For example, to express solidarity or to establish authority. Most past work depend on external tools or resources, such as part-of-speech tagging, dictionary look-up, or named-entity recognizers to extract rich features for training machine learning models. In this paper, we train recurrent neural networks with only raw features, and use word embedding to automatically learn meaningful representations. Using the same mixed-language Twitter corpus, our system is able to outperform the best SVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in accuracy, or by 17% in error rate reduction. version:2
arxiv-1412-6885 | Half-CNN: A General Framework for Whole-Image Regression | http://arxiv.org/abs/1412.6885 | id:1412.6885 author:Jun Yuan, Bingbing Ni, Ashraf A. Kassim category:cs.CV cs.LG cs.NE  published:2014-12-22 summary:The Convolutional Neural Network (CNN) has achieved great success in image classification. The classification model can also be utilized at image or patch level for many other applications, such as object detection and segmentation. In this paper, we propose a whole-image CNN regression model, by removing the full connection layer and training the network with continuous feature maps. This is a generic regression framework that fits many applications. We demonstrate this method through two tasks: simultaneous face detection & segmentation, and scene saliency prediction. The result is comparable with other models in the respective fields, using only a small scale network. Since the regression model is trained on corresponding image / feature map pairs, there are no requirements on uniform input size as opposed to the classification model. Our framework avoids classifier design, a process that may introduce too much manual intervention in model development. Yet, it is highly correlated to the classification network and offers some in-deep review of CNN structures. version:1
arxiv-1412-6847 | A New Way to Factorize Linear Cameras | http://arxiv.org/abs/1412.6847 | id:1412.6847 author:Feng Lu, Ziqiang Chen category:cs.CV  published:2014-12-22 summary:The implementation details of factorizing the 3x4 projection matrices of linear cameras into their left matrix factors and the 4x4 homogeneous central(also parallel for infinite center cases) projection factors are presented in this work. Any full row rank 3x4 real matrix can be factorized into such basic matrices which will be called LC factors. A further extension to multiple view midpoint triangulation, for both pinhole and affine camera cases, is also presented based on such camera factorizations. version:1
arxiv-1412-6821 | A Stable Multi-Scale Kernel for Topological Machine Learning | http://arxiv.org/abs/1412.6821 | id:1412.6821 author:Jan Reininghaus, Stefan Huber, Ulrich Bauer, Roland Kwitt category:stat.ML cs.CV cs.LG math.AT  published:2014-12-21 summary:Topological data analysis offers a rich source of valuable information to study vision problems. Yet, so far we lack a theoretically sound connection to popular kernel-based learning techniques, such as kernel SVMs or kernel PCA. In this work, we establish such a connection by designing a multi-scale kernel for persistence diagrams, a stable summary representation of topological features in data. We show that this kernel is positive definite and prove its stability with respect to the 1-Wasserstein distance. Experiments on two benchmark datasets for 3D shape classification/retrieval and texture recognition show considerable performance gains of the proposed method compared to an alternative approach that is based on the recently introduced persistence landscapes. version:1
arxiv-1411-2540 | Parameter estimation in spherical symmetry groups | http://arxiv.org/abs/1411.2540 | id:1411.2540 author:Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Marc DeGraef, Jeffrey Simmons, Alfred Hero category:stat.ML  published:2014-11-10 summary:This paper considers statistical estimation problems where the probability distribution of the observed random variable is invariant with respect to actions of a finite topological group. It is shown that any such distribution must satisfy a restricted finite mixture representation. When specialized to the case of distributions over the sphere that are invariant to the actions of a finite spherical symmetry group $\mathcal G$, a group-invariant extension of the Von Mises Fisher (VMF) distribution is obtained. The $\mathcal G$-invariant VMF is parameterized by location and scale parameters that specify the distribution's mean orientation and its concentration about the mean, respectively. Using the restricted finite mixture representation these parameters can be estimated using an Expectation Maximization (EM) maximum likelihood (ML) estimation algorithm. This is illustrated for the problem of mean crystal orientation estimation under the spherically symmetric group associated with the crystal form, e.g., cubic or octahedral or hexahedral. Simulations and experiments establish the advantages of the extended VMF EM-ML estimator for data acquired by Electron Backscatter Diffraction (EBSD) microscopy of a polycrystalline Nickel alloy sample. version:2
arxiv-1412-6791 | Mixture of Parts Revisited: Expressive Part Interactions for Pose Estimation | http://arxiv.org/abs/1412.6791 | id:1412.6791 author:Anoop Katti, Anurag Mittal category:cs.CV  published:2014-12-21 summary:Part-based models with restrictive tree-structured interactions for the Human Pose Estimation problem, leaves many part interactions unhandled. Two of the most common and strong manifestations of such unhandled interactions are self-occlusion among the parts and the confusion in the localization of the non-adjacent symmetric parts. By handling the self-occlusion in a data efficient manner, we improve the performance of the basic Mixture of Parts model by a large margin, especially on uncommon poses. Through addressing the confusion in the symmetric limb localization using a combination of two complementing trees, we improve the performance on all the parts by atmost doubling the running time. Finally, we show that the combination of the two solutions improves the results. We report results that are equivalent to the state-of-the-art on two standard datasets. Because of maintaining the tree-structured interactions and only part-level modeling of the base Mixture of Parts model, this is achieved in time that is much less than the best performing part-based model. version:1
arxiv-1412-6759 | Bi-directional Shape Correspondences (BSC): A Novel Technique for 2-d Shape Warping in Quadratic Time? | http://arxiv.org/abs/1412.6759 | id:1412.6759 author:Abdulrahman Oladipupo Ibraheem category:cs.CV  published:2014-12-21 summary:We propose Bidirectional Shape Correspondence (BSC) as a possible improvement on the famous shape contexts (SC) framework. Our proposals derive from the observation that the SC framework enforces a one-to-one correspondence between sample points, and that this leads to two possible drawbacks. First, this denies the framework the opportunity to effect advantageous many-to-many matching between points on the two shapes being compared. Second, this calls for the Hungarian algorithm which unfortunately usurps cubic time. While the dynamic-space-warping dynamic programming algorithm has provided a standard solution to the first problem above, it demands quintic time for general multi-contour shapes, and w times quadratic time for the special case of single-contour shapes, even after an heuristic search window of width w has been chosen. Therefore, in this work, we propose a simple method for computing "many-to-many" correspondences for the class of all 2-d shapes in quadratic time. Our approach is to explicitly let each point on the first shape choose a best match on the second shape, and vice versa. Along the way, we also propose the use of data-clustering techniques for dealing with the outliers problem, and, from another viewpoint, it turns out that this clustering can be seen as an autonomous, rather than pre-computed, sampling of shape boundary. version:1
arxiv-1412-6752 | Correlation of Data Reconstruction Error and Shrinkages in Pair-wise Distances under Principal Component Analysis (PCA) | http://arxiv.org/abs/1412.6752 | id:1412.6752 author:Abdulrahman Oladipupo Ibraheem category:cs.LG stat.ML  published:2014-12-21 summary:In this on-going work, I explore certain theoretical and empirical implications of data transformations under the PCA. In particular, I state and prove three theorems about PCA, which I paraphrase as follows: 1). PCA without discarding eigenvector rows is injective, but looses this injectivity when eigenvector rows are discarded 2). PCA without discarding eigen- vector rows preserves pair-wise distances, but tends to cause pair-wise distances to shrink when eigenvector rows are discarded. 3). For any pair of points, the shrinkage in pair-wise distance is bounded above by an L1 norm reconstruction error associated with the points. Clearly, 3). suggests that there might exist some correlation between shrinkages in pair-wise distances and mean square reconstruction error which is defined as the sum of those eigenvalues associated with the discarded eigenvectors. I therefore decided to perform numerical experiments to obtain the corre- lation between the sum of those eigenvalues and shrinkages in pair-wise distances. In addition, I have also performed some experiments to check respectively the effect of the sum of those eigenvalues and the effect of the shrinkages on classification accuracies under the PCA map. So far, I have obtained the following results on some publicly available data from the UCI Machine Learning Repository: 1). There seems to be a strong cor- relation between the sum of those eigenvalues associated with discarded eigenvectors and shrinkages in pair-wise distances. 2). Neither the sum of those eigenvalues nor pair-wise distances have any strong correlations with classification accuracies. 1 version:1
arxiv-1412-6749 | SENNS: Sparse Extraction Neural NetworkS for Feature Extraction | http://arxiv.org/abs/1412.6749 | id:1412.6749 author:Abdulrahman Oladipupo Ibraheem category:cs.CV cs.AI cs.NE math.OC stat.ML 90-08  published:2014-12-21 summary:By drawing on ideas from optimisation theory, artificial neural networks (ANN), graph embeddings and sparse representations, I develop a novel technique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at addressing the feature extraction problem. The proposed method uses (preferably deep) ANNs for projecting input attribute vectors to an output space wherein pairwise distances are maximized for vectors belonging to different classes, but minimized for those belonging to the same class, while simultaneously enforcing sparsity on the ANN outputs. The vectors that result from the projection can then be used as features in any classifier of choice. Mathematically, I formulate the proposed method as the minimisation of an objective function which can be interpreted, in the ANN output space, as a negative factor of the sum of the squares of the pair-wise distances between output vectors belonging to different classes, added to a positive factor of the sum of squares of the pair-wise distances between output vectors belonging to the same classes, plus sparsity and weight decay terms. To derive an algorithm for minimizing the objective function via gradient descent, I use the multi-variate version of the chain rule to obtain the partial derivatives of the function with respect to ANN weights and biases, and find that each of the required partial derivatives can be expressed as a sum of six terms. As it turns out, four of those six terms can be computed using the standard back propagation algorithm; the fifth can be computed via a slight modification of the standard backpropagation algorithm; while the sixth one can be computed via simple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora of digits and letters, the CMU PIE database of faces, the MNIST digits database, and other standard machine learning databases. version:1
arxiv-1412-6741 | Locally Weighted Learning for Naive Bayes Classifier | http://arxiv.org/abs/1412.6741 | id:1412.6741 author:Kim-Hung Li, Cheuk Ting Li category:stat.ML cs.LG  published:2014-12-21 summary:As a consequence of the strong and usually violated conditional independence assumption (CIA) of naive Bayes (NB) classifier, the performance of NB becomes less and less favorable compared to sophisticated classifiers when the sample size increases. We learn from this phenomenon that when the size of the training data is large, we should either relax the assumption or apply NB to a "reduced" data set, say for example use NB as a local model. The latter approach trades the ignored information for the robustness to the model assumption. In this paper, we consider using NB as a model for locally weighted data. A special weighting function is designed so that if CIA holds for the unweighted data, it also holds for the weighted data. The new method is intuitive and capable of handling class imbalance. It is theoretically more sound than the locally weighted learners of naive Bayes that base classification only on the $k$ nearest neighbors. Empirical study shows that the new method with appropriate choice of parameter outperforms seven existing classifiers of similar nature. version:1
arxiv-1412-6734 | Implicit Temporal Differences | http://arxiv.org/abs/1412.6734 | id:1412.6734 author:Aviv Tamar, Panos Toulis, Shie Mannor, Edoardo M. Airoldi category:stat.ML cs.LG  published:2014-12-21 summary:In reinforcement learning, the TD($\lambda$) algorithm is a fundamental policy evaluation method with an efficient online implementation that is suitable for large-scale problems. One practical drawback of TD($\lambda$) is its sensitivity to the choice of the step-size. It is an empirically well-known fact that a large step-size leads to fast convergence, at the cost of higher variance and risk of instability. In this work, we introduce the implicit TD($\lambda$) algorithm which has the same function and computational cost as TD($\lambda$), but is significantly more stable. We provide a theoretical explanation of this stability and an empirical evaluation of implicit TD($\lambda$) on typical benchmark tasks. Our results show that implicit TD($\lambda$) outperforms standard TD($\lambda$) and a state-of-the-art method that automatically tunes the step-size, and thus shows promise for wide applicability. version:1
arxiv-1209-3686 | Active Learning for Crowd-Sourced Databases | http://arxiv.org/abs/1209.3686 | id:1209.3686 author:Barzan Mozafari, Purnamrita Sarkar, Michael J. Franklin, Michael I. Jordan, Samuel Madden category:cs.LG cs.DB  published:2012-09-17 summary:Crowd-sourcing has become a popular means of acquiring labeled data for a wide variety of tasks where humans are more accurate than computers, e.g., labeling images, matching objects, or analyzing sentiment. However, relying solely on the crowd is often impractical even for data sets with thousands of items, due to time and cost constraints of acquiring human input (which cost pennies and minutes per label). In this paper, we propose algorithms for integrating machine learning into crowd-sourced databases, with the goal of allowing crowd-sourcing applications to scale, i.e., to handle larger datasets at lower costs. The key observation is that, in many of the above tasks, humans and machine learning algorithms can be complementary, as humans are often more accurate but slow and expensive, while algorithms are usually less accurate, but faster and cheaper. Based on this observation, we present two new active learning algorithms to combine humans and algorithms together in a crowd-sourced database. Our algorithms are based on the theory of non-parametric bootstrap, which makes our results applicable to a broad class of machine learning models. Our results, on three real-life datasets collected with Amazon's Mechanical Turk, and on 15 well-known UCI data sets, show that our methods on average ask humans to label one to two orders of magnitude fewer items to achieve the same accuracy as a baseline that labels random images, and two to eight times fewer questions than previous active learning schemes. version:4
arxiv-1404-3840 | Surpassing Human-Level Face Verification Performance on LFW with GaussianFace | http://arxiv.org/abs/1404.3840 | id:1404.3840 author:Chaochao Lu, Xiaoou Tang category:cs.CV cs.LG stat.ML  published:2014-04-15 summary:Face verification remains a challenging problem in very complex conditions with large variations such as pose, illumination, expression, and occlusions. This problem is exacerbated when we rely unrealistically on a single training data source, which is often insufficient to cover the intrinsically complex face variations. This paper proposes a principled multi-task learning approach based on Discriminative Gaussian Process Latent Variable Model, named GaussianFace, to enrich the diversity of training data. In comparison to existing methods, our model exploits additional data from multiple source-domains to improve the generalization performance of face verification in an unknown target-domain. Importantly, our model can adapt automatically to complex data distributions, and therefore can well capture complex face variations inherent in multiple sources. Extensive experiments demonstrate the effectiveness of the proposed model in learning from diverse data sources and generalize to unseen domain. Specifically, the accuracy of our algorithm achieves an impressive accuracy rate of 98.52% on the well-known and challenging Labeled Faces in the Wild (LFW) benchmark. For the first time, the human-level performance in face verification (97.53%) on LFW is surpassed. version:3
arxiv-1404-1140 | Scalable Planning and Learning for Multiagent POMDPs: Extended Version | http://arxiv.org/abs/1404.1140 | id:1404.1140 author:Christopher Amato, Frans A. Oliehoek category:cs.AI cs.LG  published:2014-04-04 summary:Online, sample-based planning algorithms for POMDPs have shown great promise in scaling to problems with large state spaces, but they become intractable for large action and observation spaces. This is particularly problematic in multiagent POMDPs where the action and observation space grows exponentially with the number of agents. To combat this intractability, we propose a novel scalable approach based on sample-based planning and factored value functions that exploits structure present in many multiagent settings. This approach applies not only in the planning case, but also in the Bayesian reinforcement learning setting. Experimental results show that we are able to provide high quality solutions to large multiagent planning and learning problems. version:2
arxiv-1410-8498 | Training for Fast Sequential Prediction Using Dynamic Feature Selection | http://arxiv.org/abs/1410.8498 | id:1410.8498 author:Emma Strubell, Luke Vilnis, Andrew McCallum category:cs.CL cs.AI  published:2014-10-30 summary:We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. We present experiments in left-to-right part-of-speech tagging on WSJ, demonstrating that we can preserve accuracy above 97% with over a five-fold reduction in run-time. version:2
arxiv-1412-5567 | Deep Speech: Scaling up end-to-end speech recognition | http://arxiv.org/abs/1412.5567 | id:1412.5567 author:Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, Andrew Y. Ng category:cs.CL cs.LG cs.NE  published:2014-12-17 summary:We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems. version:2
arxiv-1412-6045 | A Simple and Efficient Method To Generate Word Sense Representations | http://arxiv.org/abs/1412.6045 | id:1412.6045 author:Luis Nieto Piña, Richard Johansson category:cs.CL  published:2014-12-18 summary:Distributed representations of words have boosted the performance of many Natural Language Processing tasks. However, usually only one representation per word is obtained, not acknowledging the fact that some words have multiple meanings. This has a negative effect on the individual word representations and the language model as a whole. In this paper we present a simple model that enables recent techniques for building word vectors to represent distinct senses of polysemic words. In our assessment of this model we show that it is able to effectively discriminate between words' senses and to do so in a computationally efficient manner. version:2
arxiv-1407-0726 | Fast Algorithm for Low-rank matrix recovery in Poisson noise | http://arxiv.org/abs/1407.0726 | id:1407.0726 author:Yang Cao, Yao Xie category:stat.ML cs.LG math.ST stat.TH  published:2014-07-02 summary:This paper describes a fast algorithm for recovering low-rank matrices from their linear measurements contaminated with Poisson noise: the Poisson noise Maximum Likelihood Singular Value thresholding (PMLSV) algorithm. We propose a convex optimization formulation with a cost function consisting of the sum of a likelihood function and a regularization function which the nuclear norm of the matrix. Instead of solving the optimization problem directly by semi-definite program (SDP), we derive an iterative singular value thresholding algorithm by expanding the likelihood function. We demonstrate the good performance of the proposed algorithm on recovery of solar flare images with Poisson noise: the algorithm is more efficient than solving SDP using the interior-point algorithm and it generates a good approximate solution compared to that solved from SDP. version:2
arxiv-1412-6506 | Cauchy Principal Component Analysis | http://arxiv.org/abs/1412.6506 | id:1412.6506 author:Pengtao Xie, Eric Xing category:cs.LG stat.ML  published:2014-12-19 summary:Principal Component Analysis (PCA) has wide applications in machine learning, text mining and computer vision. Classical PCA based on a Gaussian noise model is fragile to noise of large magnitude. Laplace noise assumption based PCA methods cannot deal with dense noise effectively. In this paper, we propose Cauchy Principal Component Analysis (Cauchy PCA), a very simple yet effective PCA method which is robust to various types of noise. We utilize Cauchy distribution to model noise and derive Cauchy PCA under the maximum likelihood estimation (MLE) framework with low rank constraint. Our method can robustly estimate the low rank matrix regardless of whether noise is large or small, dense or sparse. We analyze the robustness of Cauchy PCA from a robust statistics view and present an efficient singular value projection optimization method. Experimental results on both simulated data and real applications demonstrate the robustness of Cauchy PCA to various noise patterns. version:1
arxiv-1412-6493 | A la Carte - Learning Fast Kernels | http://arxiv.org/abs/1412.6493 | id:1412.6493 author:Zichao Yang, Alexander J. Smola, Le Song, Andrew Gordon Wilson category:cs.LG stat.ML  published:2014-12-19 summary:Kernel methods have great promise for learning rich statistical representations of large modern datasets. However, compared to neural networks, kernel methods have been perceived as lacking in scalability and flexibility. We introduce a family of fast, flexible, lightly parametrized and general purpose kernel learning methods, derived from Fastfood basis function expansions. We provide mechanisms to learn the properties of groups of spectral frequencies in these expansions, which require only O(mlogd) time and O(m) memory, for m basis functions and d input dimensions. We show that the proposed methods can learn a wide class of kernels, outperforming the alternatives in accuracy, speed, and memory consumption. version:1
arxiv-1412-6464 | Simplified firefly algorithm for 2D image key-points search | http://arxiv.org/abs/1412.6464 | id:1412.6464 author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana, Zbigniew Marszałek, Dawid Połap, Marcin Woźniak category:cs.NE cs.AI cs.CV  published:2014-12-19 summary:In order to identify an object, human eyes firstly search the field of view for points or areas which have particular properties. These properties are used to recognise an image or an object. Then this process could be taken as a model to develop computer algorithms for images identification. This paper proposes the idea of applying the simplified firefly algorithm to search for key-areas in 2D images. For a set of input test images the proposed version of firefly algorithm has been examined. Research results are presented and discussed to show the efficiency of this evolutionary computation method. version:1
arxiv-1412-6451 | Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer | http://arxiv.org/abs/1412.6451 | id:1412.6451 author:Mark Wernsdorfer, Ute Schmid category:cs.LG cs.AI cs.RO  published:2014-12-19 summary:Methods of deep machine learning enable to to reuse low-level representations efficiently for generating more abstract high-level representations. Originally, deep learning has been applied passively (e.g., for classification purposes). Recently, it has been extended to estimate the value of actions for autonomous agents within the framework of reinforcement learning (RL). Explicit models of the environment can be learned to augment such a value function. Although "flat" connectionist methods have already been used for model-based RL, up to now, only model-free variants of RL have been equipped with methods from deep learning. We propose a variant of deep model-based RL that enables an agent to learn arbitrarily abstract hierarchical representations of its environment. In this paper, we present research on how such hierarchical representations can be grounded in sensorimotor interaction between an agent and its environment. version:1
arxiv-1412-6391 | Py3DFreeHandUS: a library for voxel-array reconstruction using Ultrasonography and attitude sensors | http://arxiv.org/abs/1412.6391 | id:1412.6391 author:Davide Monari, Francesco Cenni, Erwin Aertbeliën, Kaat Desloovere category:cs.CV cs.CE  published:2014-12-19 summary:In medical imaging, there is a growing interest to provide real-time images with good quality for large anatomical structures. To cope with this issue, we developed a library that allows to replace, for some specific clinical applications, more robust systems such as Computer Tomography (CT) and Magnetic Resonance Imaging (MRI). Our python library Py3DFreeHandUS is a package for processing data acquired simultaneously by ultra-sonographic systems (US) and marker-based optoelectronic systems. In particular, US data enables to visualize subcutaneous body structures, whereas the optoelectronic system is able to collect the 3D position in space for reflective objects, that are called markers. By combining these two measurement devices, it is possible to reconstruct the real 3D morphology of body structures such as muscles, for relevant clinical implications. In the present research work, the different steps which allow to obtain a relevant 3D data set as well as the procedures for calibrating the systems and for determining the quality of the reconstruction. version:1
arxiv-1412-6388 | Distributed Decision Trees | http://arxiv.org/abs/1412.6388 | id:1412.6388 author:Ozan İrsoy, Ethem Alpaydın category:cs.LG stat.ML  published:2014-12-19 summary:Recently proposed budding tree is a decision tree algorithm in which every node is part internal node and part leaf. This allows representing every decision tree in a continuous parameter space, and therefore a budding tree can be jointly trained with backpropagation, like a neural network. Even though this continuity allows it to be used in hierarchical representation learning, the learned representations are local: Activation makes a soft selection among all root-to-leaf paths in a tree. In this work we extend the budding tree and propose the distributed tree where the children use different and independent splits and hence multiple paths in a tree can be traversed at the same time. This ability to combine multiple paths gives the power of a distributed representation, as in a traditional perceptron layer. We show that distributed trees perform comparably or better than budding and traditional hard trees on classification and regression tasks. version:1
arxiv-1412-6346 | Reverse Engineering Chemical Reaction Networks from Time Series Data | http://arxiv.org/abs/1412.6346 | id:1412.6346 author:Dominic P. Searson, Mark J. Willis, Allen Wright category:cs.NE q-bio.MN  published:2014-12-19 summary:The automated inference of physically interpretable (bio)chemical reaction network models from measured experimental data is a challenging problem whose solution has significant commercial and academic ramifications. It is demonstrated, using simulations, how sets of elementary reactions comprising chemical reaction networks, as well as their rate coefficients, may be accurately recovered from non-equilibrium time series concentration data, such as that obtained from laboratory scale reactors. A variant of an evolutionary algorithm called differential evolution in conjunction with least squares techniques is used to search the space of reaction networks in order to infer both the reaction network topology and its rate parameters. Properties of the stoichiometric matrices of trial networks are used to bias the search towards physically realisable solutions. No other information, such as chemical characterisation of the reactive species is required, although where available it may be used to improve the search process. version:1
arxiv-1412-6285 | From dependency to causality: a machine learning approach | http://arxiv.org/abs/1412.6285 | id:1412.6285 author:Gianluca Bontempi, Maxime Flauder category:cs.LG cs.AI stat.ML  published:2014-12-19 summary:The relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference. Recent results in the ChaLearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in Markov indistinguishable configurations thanks to data driven approaches. This paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with $n>2$ variables. The approach relies on the asymmetry of some conditional (in)dependence relations between the members of the Markov blankets of two variables causally connected. Our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for $n>2$ variate distributions. version:1
arxiv-1412-6264 | Supertagging: Introduction, learning, and application | http://arxiv.org/abs/1412.6264 | id:1412.6264 author:Taraka Rama K category:cs.CL  published:2014-12-19 summary:Supertagging is an approach originally developed by Bangalore and Joshi (1999) to improve the parsing efficiency. In the beginning, the scholars used small training datasets and somewhat na\"ive smoothing techniques to learn the probability distributions of supertags. Since its inception, the applicability of Supertags has been explored for TAG (tree-adjoining grammar) formalism as well as other related yet, different formalisms such as CCG. This article will try to summarize the various chapters, relevant to statistical parsing, from the most recent edited book volume (Bangalore and Joshi, 2010). The chapters were selected so as to blend the learning of supertags, its integration into full-scale parsing, and in semantic parsing. version:1
arxiv-1412-6257 | Gradual training of deep denoising auto encoders | http://arxiv.org/abs/1412.6257 | id:1412.6257 author:Alexander Kalmanovich, Gal Chechik category:cs.LG cs.NE  published:2014-12-19 summary:Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets. version:1
arxiv-1412-1628 | Fisher Kernel for Deep Neural Activations | http://arxiv.org/abs/1412.1628 | id:1412.1628 author:Donggeun Yoo, Sunggyun Park, Joon-Young Lee, In So Kweon category:cs.CV cs.LG cs.NE  published:2014-12-04 summary:Compared to image representation based on low-level local descriptors, deep neural activations of Convolutional Neural Networks (CNNs) are richer in mid-level representation, but poorer in geometric invariance properties. In this paper, we present a straightforward framework for better image representation by combining the two approaches. To take advantages of both representations, we propose an efficient method to extract a fair amount of multi-scale dense local activations from a pre-trained CNN. We then aggregate the activations by Fisher kernel framework, which has been modified with a simple scale-wise normalization essential to make it suitable for CNN activations. Replacing the direct use of a single activation vector with our representation demonstrates significant performance improvements: +17.76 (Acc.) on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that our proposal can be used as a primary image representation for better performances in visual recognition tasks. version:2
arxiv-1412-6219 | Information-Theoretic Methods for Identifying Relationships among Climate Variables | http://arxiv.org/abs/1412.6219 | id:1412.6219 author:Kevin H. Knuth, Deniz Gençağa, William B. Rossow category:physics.data-an physics.ao-ph stat.ML  published:2014-12-19 summary:Information-theoretic quantities, such as entropy, are used to quantify the amount of information a given variable provides. Entropies can be used together to compute the mutual information, which quantifies the amount of information two variables share. However, accurately estimating these quantities from data is extremely challenging. We have developed a set of computational techniques that allow one to accurately compute marginal and joint entropies. These algorithms are probabilistic in nature and thus provide information on the uncertainty in our estimates, which enable us to establish statistical significance of our findings. We demonstrate these methods by identifying relations between cloud data from the International Satellite Cloud Climatology Project (ISCCP) and data from other sources, such as equatorial pacific sea surface temperatures (SST). version:1
arxiv-1412-6211 | Multiple Authors Detection: A Quantitative Analysis of Dream of the Red Chamber | http://arxiv.org/abs/1412.6211 | id:1412.6211 author:Xianfeng Hu, Yang Wang, Qiang Wu category:cs.LG cs.CL  published:2014-12-19 summary:Inspired by the authorship controversy of Dream of the Red Chamber and the application of machine learning in the study of literary stylometry, we develop a rigorous new method for the mathematical analysis of authorship by testing for a so-called chrono-divide in writing styles. Our method incorporates some of the latest advances in the study of authorship attribution, particularly techniques from support vector machines. By introducing the notion of relative frequency as a feature ranking metric our method proves to be highly effective and robust. Applying our method to the Cheng-Gao version of Dream of the Red Chamber has led to convincing if not irrefutable evidence that the first $80$ chapters and the last $40$ chapters of the book were written by two different authors. Furthermore, our analysis has unexpectedly provided strong support to the hypothesis that Chapter 67 was not the work of Cao Xueqin either. We have also tested our method to the other three Great Classical Novels in Chinese. As expected no chrono-divides have been found. This provides further evidence of the robustness of our method. version:1
arxiv-1412-6163 | Automated Objective Surgical Skill Assessment in the Operating Room Using Unstructured Tool Motion | http://arxiv.org/abs/1412.6163 | id:1412.6163 author:Piyush Poddar, Narges Ahmidi, S. Swaroop Vedula, Lisa Ishii, Gregory D. Hager, Masaru Ishii category:cs.CV  published:2014-12-18 summary:Previous work on surgical skill assessment using intraoperative tool motion in the operating room (OR) has focused on highly-structured surgical tasks such as cholecystectomy. Further, these methods only considered generic motion metrics such as time and number of movements, which are of limited instructive value. In this paper, we developed and evaluated an automated approach to the surgical skill assessment of nasal septoplasty in the OR. The obstructed field of view and highly unstructured nature of septoplasty precludes trainees from efficiently learning the procedure. We propose a descriptive structure of septoplasty consisting of two types of activity: (1) brushing activity directed away from the septum plane characterizing the consistency of the surgeon's wrist motion and (2) activity along the septal plane characterizing the surgeon's coverage pattern. We derived features related to these two activity types that classify a surgeon's level of training with an average accuracy of about 72%. The features we developed provide surgeons with personalized, actionable feedback regarding their tool motion. version:1
arxiv-1412-6124 | Semantic Part Segmentation using Compositional Model combining Shape and Appearance | http://arxiv.org/abs/1412.6124 | id:1412.6124 author:Jianyu Wang, Alan Yuille category:cs.CV  published:2014-12-18 summary:In this paper, we study the problem of semantic part segmentation for animals. This is more challenging than standard object detection, object segmentation and pose estimation tasks because semantic parts of animals often have similar appearance and highly varying shapes. To tackle these challenges, we build a mixture of compositional models to represent the object boundary and the boundaries of semantic parts. And we incorporate edge, appearance, and semantic part cues into the compositional model. Given part-level segmentation annotation, we develop a novel algorithm to learn a mixture of compositional models under various poses and viewpoints for certain animal classes. Furthermore, a linear complexity algorithm is offered for efficient inference of the compositional model using dynamic programming. We evaluate our method for horse and cow using a newly annotated dataset on Pascal VOC 2010 which has pixelwise part labels. Experimental results demonstrate the effectiveness of our method. version:1
arxiv-1412-6115 | Compressing Deep Convolutional Networks using Vector Quantization | http://arxiv.org/abs/1412.6115 | id:1412.6115 author:Yunchao Gong, Liu Liu, Ming Yang, Lubomir Bourdev category:cs.CV cs.LG cs.NE  published:2014-12-18 summary:Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN. version:1
arxiv-1412-5968 | Quantized Matrix Completion for Personalized Learning | http://arxiv.org/abs/1412.5968 | id:1412.5968 author:Andrew S. Lan, Christoph Studer, Richard G. Baraniuk category:stat.ML cs.LG  published:2014-12-18 summary:The recently proposed SPARse Factor Analysis (SPARFA) framework for personalized learning performs factor analysis on ordinal or binary-valued (e.g., correct/incorrect) graded learner responses to questions. The underlying factors are termed "concepts" (or knowledge components) and are used for learning analytics (LA), the estimation of learner concept-knowledge profiles, and for content analytics (CA), the estimation of question-concept associations and question difficulties. While SPARFA is a powerful tool for LA and CA, it requires a number of algorithm parameters (including the number of concepts), which are difficult to determine in practice. In this paper, we propose SPARFA-Lite, a convex optimization-based method for LA that builds on matrix completion, which only requires a single algorithm parameter and enables us to automatically identify the required number of concepts. Using a variety of educational datasets, we demonstrate that SPARFALite (i) achieves comparable performance in predicting unobserved learner responses to existing methods, including item response theory (IRT) and SPARFA, and (ii) is computationally more efficient. version:1
arxiv-1412-5967 | Tag-Aware Ordinal Sparse Factor Analysis for Learning and Content Analytics | http://arxiv.org/abs/1412.5967 | id:1412.5967 author:Andrew S. Lan, Christoph Studer, Andrew E. Waters, Richard G. Baraniuk category:stat.ML cs.LG  published:2014-12-18 summary:Machine learning offers novel ways and means to design personalized learning systems wherein each student's educational experience is customized in real time depending on their background, learning goals, and performance to date. SPARse Factor Analysis (SPARFA) is a novel framework for machine learning-based learning analytics, which estimates a learner's knowledge of the concepts underlying a domain, and content analytics, which estimates the relationships among a collection of questions and those concepts. SPARFA jointly learns the associations among the questions and the concepts, learner concept knowledge profiles, and the underlying question difficulties, solely based on the correct/incorrect graded responses of a population of learners to a collection of questions. In this paper, we extend the SPARFA framework significantly to enable: (i) the analysis of graded responses on an ordinal scale (partial credit) rather than a binary scale (correct/incorrect); (ii) the exploitation of tags/labels for questions that partially describe the question{concept associations. The resulting Ordinal SPARFA-Tag framework greatly enhances the interpretability of the estimated concepts. We demonstrate using real educational data that Ordinal SPARFA-Tag outperforms both SPARFA and existing collaborative filtering techniques in predicting missing learner responses. version:1
arxiv-1412-5949 | Large Scale Distributed Distance Metric Learning | http://arxiv.org/abs/1412.5949 | id:1412.5949 author:Pengtao Xie, Eric Xing category:cs.LG  published:2014-12-18 summary:In large scale machine learning and data mining problems with high feature dimensionality, the Euclidean distance between data points can be uninformative, and Distance Metric Learning (DML) is often desired to learn a proper similarity measure (using side information such as example data pairs being similar or dissimilar). However, high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al. (2002) and later extensions. In this paper, we present a distributed algorithm for DML, and a large-scale implementation on a parameter server architecture. Our approach builds on a parallelizable reformulation of Xing et al. (2002), and an asynchronous stochastic gradient descent optimization procedure. To our knowledge, this is the first distributed solution to DML, and we show that, on a system with 256 CPU cores, our program is able to complete a DML task on a dataset with 1 million data points, 22-thousand features, and 200 million labeled data pairs, in 15 hours; and the learned metric shows great effectiveness in properly measuring distances. version:1
arxiv-1407-5949 | Deep Recurrent Neural Networks for Time Series Prediction | http://arxiv.org/abs/1407.5949 | id:1407.5949 author:Sharat C. Prasad, Piyush Prasad category:cs.NE 62M45  82C32  92B20  published:2014-07-22 summary:Ability of deep networks to extract high level features and of recurrent networks to perform time-series inference have been studied. In view of universality of one hidden layer network at approximating functions under weak constraints, the benefit of multiple layers is to enlarge the space of dynamical systems approximated or, given the space, reduce the number of units required for a certain error. Traditionally shallow networks with manually engineered features are used, back-propagation extent is limited to one and attempt to choose a large number of hidden units to satisfy the Markov condition is made. In case of Markov models, it has been shown that many systems need to be modeled as higher order. In the present work, we present deep recurrent networks with longer backpropagation through time extent as a solution to modeling systems that are high order and to predicting ahead. We study epileptic seizure suppression electro-stimulator. Extraction of manually engineered complex features and prediction employing them has not allowed small low-power implementations as, to avoid possibility of surgery, extraction of any features that may be required has to be included. In this solution, a recurrent neural network performs both feature extraction and prediction. We prove analytically that adding hidden layers or increasing backpropagation extent increases the rate of decrease of approximation error. A Dynamic Programming (DP) training procedure employing matrix operations is derived. DP and use of matrix operations makes the procedure efficient particularly when using data-parallel computing. The simulation studies show the geometry of the parameter space, that the network learns the temporal structure, that parameters converge while model output displays same dynamic behavior as the system and greater than .99 Average Detection Rate on all real seizure data tried. version:2
arxiv-1412-5862 | A theoretical basis for efficient computations with noisy spiking neurons | http://arxiv.org/abs/1412.5862 | id:1412.5862 author:Zeno Jonke, Stefan Habenschuss, Wolfgang Maass category:cs.NE q-bio.NC 68Q10  published:2014-12-18 summary:Network of neurons in the brain apply - unlike processors in our current generation of computer hardware - an event-based processing strategy, where short pulses (spikes) are emitted sparsely by neurons to signal the occurrence of an event at a particular point in time. Such spike-based computations promise to be substantially more power-efficient than traditional clocked processing schemes. However it turned out to be surprisingly difficult to design networks of spiking neurons that are able to carry out demanding computations. We present here a new theoretical framework for organizing computations of networks of spiking neurons. In particular, we show that a suitable design enables them to solve hard constraint satisfaction problems from the domains of planning - optimization and verification - logical inference. The underlying design principles employ noise as a computational resource. Nevertheless the timing of spikes (rather than just spike rates) plays an essential role in the resulting computations. Furthermore, one can demonstrate for the Traveling Salesman Problem a surprising computational advantage of networks of spiking neurons compared with traditional artificial neural networks and Gibbs sampling. The identification of such advantage has been a well-known open problem. version:1
arxiv-1412-5802 | Contour Detection Using Contrast Formulas in the Framework of Logarithmic Models | http://arxiv.org/abs/1412.5802 | id:1412.5802 author:Vasile Patrascu category:cs.CV  published:2014-12-18 summary:In this paper we use a new logarithmic model of image representation, developed in [1,2], for edge detection. In fact, in the framework of the new model we obtain the formulas for computing the "contrast of a pixel" and the "contrast" image is just the "contour" or edge image. In our setting the range of values is preserved and the quality of the contour is good for high as well as for low luminosity regions. We present the comparison of our results with the results using classical edge detection operators. version:1
arxiv-1412-5796 | Image Enhancement Using a Generalization of Homographic Function | http://arxiv.org/abs/1412.5796 | id:1412.5796 author:Vasile Patrascu category:cs.CV  published:2014-12-18 summary:This paper presents a new method of gray level image enhancement, based on point transforms. In order to define the transform function, it was used a generalization of the homographic function. version:1
arxiv-1412-6092 | Image enhancement using the mean dynamic range maximization with logarithmic operations | http://arxiv.org/abs/1412.6092 | id:1412.6092 author:Vasile Patrascu category:cs.CV  published:2014-12-18 summary:In this paper we use a logarithmic model for gray level image enhancement. We begin with a short presentation of the model and then, we propose a new formula for the mean dynamic range. After that we present two image transforms: one performs an optimal enhancement of the mean dynamic range using the logarithmic addition, and the other does the same for positive and negative values using the logarithmic scalar multiplication. We present the comparison of the results obtained by dynamic ranges optimization with the results obtained using classical image enhancement methods like gamma correction and histogram equalization. version:1
arxiv-1412-5787 | Gray Level Image Enhancement Using Polygonal Functions | http://arxiv.org/abs/1412.5787 | id:1412.5787 author:Vasile Patrascu category:cs.CV  published:2014-12-18 summary:This paper presents a method for enhancing the gray level images. This method takes part from the category of point transforms and it is based on interpolation functions. The latter have a graphic represented by polygonal lines. The interpolation nodes of these functions are calculated taking into account the statistics of gray levels belonging to the image. version:1
arxiv-1412-5769 | Gray level image enhancement using the Bernstein polynomials | http://arxiv.org/abs/1412.5769 | id:1412.5769 author:Vasile Patrascu category:cs.CV  published:2014-12-18 summary:This paper presents a method for enhancing the gray level images. This presented method takes part from the category of point operations and it is based on piecewise linear functions. The interpolation nodes of these functions are calculated using the Bernstein polynomials. version:1
