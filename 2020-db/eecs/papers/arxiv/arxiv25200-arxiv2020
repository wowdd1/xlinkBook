arxiv-1702-02744 | L1-regularized Reconstruction Error as Alpha Matte | http://arxiv.org/abs/1702.02744 | id:1702.02744 author:Jubin Johnson, Hisham Cholakkal, Deepu Rajan category:cs.CV  published:2017-02-09 summary:Sampling-based alpha matting methods have traditionally followed the compositing equation to estimate the alpha value at a pixel from a pair of foreground (F) and background (B) samples. The (F,B) pair that produces the least reconstruction error is selected, followed by alpha estimation. The significance of that residual error has been left unexamined. In this letter, we propose a video matting algorithm that uses L1-regularized reconstruction error of F and B samples as a measure of the alpha matte. A multi-frame non-local means framework using coherency sensitive hashing is utilized to ensure temporal coherency in the video mattes. Qualitative and quantitative evaluations on a dataset exclusively for video matting demonstrate the effectiveness of the proposed matting algorithm. version:1
arxiv-1702-02743 | Incorporation of prior knowledge of the signal behavior into the reconstruction to accelerate the acquisition of MR diffusion data | http://arxiv.org/abs/1702.02743 | id:1702.02743 author:Juan F P J Abascal, Manuel Desco, Juan Parra-Robles category:physics.med-ph cs.CV  published:2017-02-09 summary:Diffusion MRI measurements using hyperpolarized gases are generally acquired during patient breath hold, which yields a compromise between achievable image resolution, lung coverage and number of b-values. In this work, we propose a novel method that accelerates the acquisition of MR diffusion data by undersampling in both spatial and b-value dimensions, thanks to incorporating knowledge about the signal decay into the reconstruction (SIDER). SIDER is compared to total variation (TV) reconstruction by assessing their effect on both the recovery of ventilation images and estimated mean alveolar dimensions (MAD). Both methods are assessed by retrospectively undersampling diffusion datasets of normal volunteers and COPD patients (n=8) for acceleration factors between x2 and x10. TV led to large errors and artefacts for acceleration factors equal or larger than x5. SIDER improved TV, presenting lower errors and histograms of MAD closer to those obtained from fully sampled data for accelerations factors up to x10. SIDER preserved image quality at all acceleration factors but images were slightly smoothed and some details were lost at x10. In conclusion, we have developed and validated a novel compressed sensing method for lung MRI imaging and achieved high acceleration factors, which can be used to increase the amount of data acquired during a breath-hold. This methodology is expected to improve the accuracy of estimated lung microstructure dimensions and widen the possibilities of studying lung diseases with MRI. version:1
arxiv-1702-02741 | CNN-based Estimation of Abdominal Circumference from Ultrasound images | http://arxiv.org/abs/1702.02741 | id:1702.02741 author:Jaeseong Jang, Ja-Young Kwon, Bukweon Kim, Sung Min Lee, Yejin Park, Jin Keun Seo category:cs.CV stat.ML  published:2017-02-09 summary:The obstetrics and gynecology ultrasound diagnosis is routinely used to check fetal biometry, and due to its time-consuming routine process, there has been great demand of automatic estimation. Automated analysis of ultrasound images is complicated because ultrasound images are patient-specific, operator-dependent, and machine specific. Among fetal biometry, abdominal circumference (AC) is more difficult to make accurate measurement automatically because abdomen has low contrast against surroundings, non-uniform contrast and irregular shape compared to other parameters. This paper proposes a framework for estimation of the fetal AC from 2D ultrasound data by a specially designed convolutional neural network (CNN) which takes account of doctors' decision process, anatomical structure, and the characteristics of ultrasound image. The proposed framework uses CNN to classify ultrasound images (stomach bubble, amniotic fluid, and umbilical vein) and the Hough transform for the measurement of the AC. We tested the proposed method using clinical ultrasound data acquired from 10 pregnant women. Experimental results showed that, with relatively small training samples, the proposed CNN provided sufficient classification results for AC estimation through the Hough transform. This framework showed good performance on most cases and even for ultrasound images deteriorated by shadowing artifacts. However, for oversized fetus cases, when amniotic fluid is not seen or abdominal area was distorted, it could not estimate correct AC. version:1
arxiv-1702-02738 | Joint Discovery of Object States and Manipulating Actions | http://arxiv.org/abs/1702.02738 | id:1702.02738 author:Jean-Baptiste Alayrac, Josev Sivic, Ivan Laptev, Simon Lacoste-Julien category:cs.CV cs.LG I.5.1; I.5.4; I.2  published:2017-02-09 summary:Many activities involve object manipulations aiming to modify object state. Examples of common state changes include full/empty bottle, open/closed door, and attached/detached car wheel. In this work, we seek to automatically discover the states of objects and the associated manipulating actions. Given a set of videos for a particular task, we propose a joint model that learns to identify object states and to localize state-modifying actions. Our model is formulated as a discriminative clustering cost. We assume a consistent temporal order for the changes in object states and manipulating actions, and learn the model without additional supervision. Our method is validated on a new dataset of videos depicting real-life object manipulations. We demonstrate the successful discovery of seven manipulating actions and corresponding object states. Moreover, we emphasize our joint formulation and show the improvement of object state discovery by action recognition and vice versa. version:1
arxiv-1702-02737 | Mining User/Movie Preferred Features Based on Reviews for Video Recommendation System | http://arxiv.org/abs/1702.02737 | id:1702.02737 author:Xuan-Son Vu, Seong-Bae Park category:cs.IR cs.CL  published:2017-02-09 summary:In this work, we present an approach for mining user preferences and recommendation based on reviews. There have been various studies worked on recommendation problem. However, most of the studies beyond one aspect user generated- content such as user ratings, user feedback and so on to state user preferences. There is a prob- lem in one aspect mining is lacking for stating user preferences. As a demonstration, in collaborative filter recommendation, we try to figure out the preference trend of crowded users, then use that trend to predict current user preference. Therefore, there is a gap between real user preferences and the trend of the crowded people. Additionally, user preferences can be addressed from mining user reviews since user often comment about various aspects of products. To solve this problem, we mainly focus on mining product aspects and user aspects inside user reviews to directly state user preferences. We also take into account Social Network Analysis for cold-start item problem. With cold-start user problem, collaborative filter algorithm is employed in our work. The framework is general enough to be applied to different recommendation domains. Theoretically, our method would achieve a significant enhancement. version:1
arxiv-1702-02736 | Challenges in Providing Automatic Affective Feedback in Instant Messaging Applications | http://arxiv.org/abs/1702.02736 | id:1702.02736 author:Chieh-Yang Huang, Ting-Hao, Huang, Lun-Wei Ku category:cs.CL cs.HC H.5.2; H.5.3; I.2.7  published:2017-02-09 summary:Instant messaging is one of the major channels of computer mediated communication. However, humans are known to be very limited in understanding others' emotions via text-based communication. Aiming on introducing emotion sensing technologies to instant messaging, we developed EmotionPush, a system that automatically detects the emotions of the messages end-users received on Facebook Messenger and provides colored cues on their smartphones accordingly. We conducted a deployment study with 20 participants during a time span of two weeks. In this paper, we revealed five challenges, along with examples, that we observed in our study based on both user's feedback and chat logs, including (i)the continuum of emotions, (ii)multi-user conversations, (iii)different dynamics between different users, (iv)misclassification of emotions and (v)unconventional content. We believe this discussion will benefit the future exploration of affective computing for instant messaging, and also shed light on research of conversational emotion sensing. version:1
arxiv-1702-02719 | Effective face landmark localization via single deep network | http://arxiv.org/abs/1702.02719 | id:1702.02719 author:Zongping Deng, Ke Li, Qijun Zhao, Yi Zhang, Hu Chen category:cs.CV  published:2017-02-09 summary:In this paper, we propose a novel face alignment method using single deep network (SDN) on existing limited training data. Rather than using a max-pooling layer followed one convolutional layer in typical convolutional neural networks (CNN), SDN adopts a stack of 3 layer groups instead. Each group layer contains two convolutional layers and a max-pooling layer, which can extract the features hierarchically. Moreover, an effective data augmentation strategy and corresponding training skills are also proposed to over-come the lack of training images on COFW and 300-W da-tasets. The experiment results show that our method outper-forms state-of-the-art methods in both detection accuracy and speed. version:1
arxiv-1702-02715 | A Fast and Scalable Joint Estimator for Learning Multiple Related Sparse Gaussian Graphical Models | http://arxiv.org/abs/1702.02715 | id:1702.02715 author:Beilun Wang, Ji Gao, Yanjun Qi category:stat.ML  published:2017-02-09 summary:Estimating multiple sparse Gaussian Graphical Models (sGGMs) jointly for many related tasks (large $K$) under a high-dimensional (large $p$) situation is an important task. Most previous studies for the joint estimation of multiple sGGMs rely on penalized log-likelihood estimators that involve expensive and difficult non-smooth optimizations. We propose a novel approach, FASJEM for \underline{fa}st and \underline{s}calable \underline{j}oint structure-\underline{e}stimation of \underline{m}ultiple sGGMs at a large scale. As the first study of joint sGGM using the M-estimator framework, our work has three major contributions: (1) We solve FASJEM through an entry-wise manner which is parallelizable. (2) We choose a proximal algorithm to optimize FASJEM. This improves the computational efficiency from $O(Kp^3)$ to $O(Kp^2)$ and reduces the memory requirement from $O(Kp^2)$ to $O(K)$. (3) We theoretically prove that FASJEM achieves a consistent estimation with a convergence rate of $O(\log(Kp)/n_{tot})$. On several synthetic and four real-world datasets, FASJEM shows significant improvements over baselines on accuracy, computational complexity and memory costs. version:1
arxiv-1702-02709 | Predicting Privileged Information for Height Estimation | http://arxiv.org/abs/1702.02709 | id:1702.02709 author:Nikolaos Sarafianos, Christophoros Nikou, Ioannis A. Kakadiaris category:cs.CV  published:2017-02-09 summary:In this paper, we propose a novel regression-based method for employing privileged information to estimate the height using human metrology. The actual values of the anthropometric measurements are difficult to estimate accurately using state-of-the-art computer vision algorithms. Hence, we use ratios of anthropometric measurements as features. Since many anthropometric measurements are not available at test time in real-life scenarios, we employ a learning using privileged information (LUPI) framework in a regression setup. Instead of using the LUPI paradigm for regression in its original form (i.e., \epsilon-SVR+), we train regression models that predict the privileged information at test time. The predictions are then used, along with observable features, to perform height estimation. Once the height is estimated, a mapping to classes is performed. We demonstrate that the proposed approach can estimate the height better and faster than the \epsilon-SVR+ algorithm and report results for different genders and quartiles of humans. version:1
arxiv-1702-02706 | Semi-Supervised Deep Learning for Monocular Depth Map Prediction | http://arxiv.org/abs/1702.02706 | id:1702.02706 author:Yevhen Kuznietsov, Jörg Stückler, Bastian Leibe category:cs.CV  published:2017-02-09 summary:Supervised deep learning often suffers from the lack of sufficient training data. Specifically in the context of monocular depth map prediction, it is barely possible to determine dense ground truth depth images in realistic dynamic outdoor environments. When using LiDAR sensors, for instance, noise is present in the distance measurements, the calibration between sensors cannot be perfect, and the measurements are typically much sparser than the camera images. In this paper, we propose a novel approach to depth map prediction from monocular images that learns in a semi-supervised way. While we use sparse ground-truth depth for supervised learning, we also enforce our deep network to produce photoconsistent dense depth maps in a stereo setup using a direct image alignment loss. In experiments we demonstrate superior performance in depth map prediction from single images compared to the state-of-the-art methods. version:1
arxiv-1702-02686 | Rate Optimal Estimation and Confidence Intervals for High-dimensional Regression with Missing Covariates | http://arxiv.org/abs/1702.02686 | id:1702.02686 author:Yining Wang, Jialei Wang, Sivaraman Balakrishnan, Aarti Singh category:stat.ML cs.LG stat.ME  published:2017-02-09 summary:We consider the problem of estimating and constructing component-wise confidence intervals of a sparse high-dimensional linear regression model when some covariates of the design matrix are missing completely at random. A variant of the Dantzig selector (Candes & Tao, 2007) is analyzed for estimating the regression model and a de-biasing argument is employed to construct component-wise confidence intervals under additional assumptions on the covariance of the design matrix. We also derive rates of convergence of the mean-square estimation error and the average confidence interval length, and show that the dependency over several model parameters (e.g., sparsity $s$, portion of observed covariates $\rho_*$, signal level $\ \beta_0\ _2$) are optimal in a minimax sense. version:1
arxiv-1702-02680 | Manifold Based Low-rank Regularization for Image Restoration and Semi-supervised Learning | http://arxiv.org/abs/1702.02680 | id:1702.02680 author:Rongjie Lai, Jia Li category:cs.CV math.NA  published:2017-02-09 summary:Low-rank structures play important role in recent advances of many problems in image science and data science. As a natural extension of low-rank structures for data with nonlinear structures, the concept of the low-dimensional manifold structure has been considered in many data processing problems. Inspired by this concept, we consider a manifold based low-rank regularization as a linear approximation of manifold dimension. This regularization is less restricted than the global low-rank regularization, and thus enjoy more flexibility to handle data with nonlinear structures. As applications, we demonstrate the proposed regularization to classical inverse problems in image sciences and data sciences including image inpainting, image super-resolution, X-ray computer tomography (CT) image reconstruction and semi-supervised learning. We conduct intensive numerical experiments in several image restoration problems and a semi-supervised learning problem of classifying handwritten digits using the MINST data. Our numerical tests demonstrate the effectiveness of the proposed methods and illustrate that the new regularization methods produce outstanding results by comparing with many existing methods. version:1
arxiv-1702-02676 | Energy Saving Additive Neural Network | http://arxiv.org/abs/1702.02676 | id:1702.02676 author:Arman Afrasiyabi, Ozan Yildiz, Baris Nasir, Fatos T. Yarman Vural, A. Enis Cetin category:cs.NE cs.AI cs.LG  published:2017-02-09 summary:In recent years, machine learning techniques based on neural networks for mobile computing become increasingly popular. Classical multi-layer neural networks require matrix multiplications at each stage. Multiplication operation is not an energy efficient operation and consequently it drains the battery of the mobile device. In this paper, we propose a new energy efficient neural network with the universal approximation property over space of Lebesgue integrable functions. This network, called, additive neural network, is very suitable for mobile computing. The neural structure is based on a novel vector product definition, called ef-operator, that permits a multiplier-free implementation. In ef-operation, the "product" of two real numbers is defined as the sum of their absolute values, with the sign determined by the sign of the product of the numbers. This "product" is used to construct a vector product in $R^N$. The vector product induces the $l_1$ norm. The proposed additive neural network successfully solves the XOR problem. The experiments on MNIST dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi-layer perceptron and convolutional neural networks (LeNet). version:1
arxiv-1702-02670 | Stochastic Neighbor Embedding separates well-separated clusters | http://arxiv.org/abs/1702.02670 | id:1702.02670 author:Uri Shaham, Stefan Steinerberger category:stat.ML math.ST stat.TH  published:2017-02-09 summary:Stochastic Neighbor Embedding and its variants are widely used dimensionality reduction techniques -- despite their popularity, no theoretical results are known. We prove that the optimal SNE embedding of well-separated clusters from high dimensions to the real line $\mathbb{R}$ manages to successfully separate the clusters in a quantitative way. version:1
arxiv-1702-02661 | Inductive Pairwise Ranking: Going Beyond the n log(n) Barrier | http://arxiv.org/abs/1702.02661 | id:1702.02661 author:U. N. Niranjan, Arun Rajkumar category:cs.LG cs.IT math.IT stat.ML  published:2017-02-09 summary:We study the problem of ranking a set of items from nonactively chosen pairwise preferences where each item has feature information with it. We propose and characterize a very broad class of preference matrices giving rise to the Feature Low Rank (FLR) model, which subsumes several models ranging from the classic Bradley-Terry-Luce (BTL) (Bradley and Terry 1952) and Thurstone (Thurstone 1927) models to the recently proposed blade-chest (Chen and Joachims 2016) and generic low-rank preference (Rajkumar and Agarwal 2016) models. We use the technique of matrix completion in the presence of side information to develop the Inductive Pairwise Ranking (IPR) algorithm that provably learns a good ranking under the FLR model, in a sample-efficient manner. In practice, through systematic synthetic simulations, we confirm our theoretical findings regarding improvements in the sample complexity due to the use of feature information. Moreover, on popular real-world preference learning datasets, with as less as 10% sampling of the pairwise comparisons, our method recovers a good ranking. version:1
arxiv-1702-02655 | EEG Representation Using Multi-instance Framework on The Manifold of Symmetric Positive Definite Matrices for EEG-based Computer Aided Diagnosis | http://arxiv.org/abs/1702.02655 | id:1702.02655 author:Khadijeh Sadatnejad, Saeed S. Ghidary, Reza Rostami, Reza Kazemi category:cs.LG  published:2017-02-08 summary:The generalization and robustness of an electroencephalogram (EEG)-based computer aided diagnostic system are crucial requirements in actual clinical practice. To reach these goals, we propose a new EEG representation that provides a more realistic view of brain functionality by applying multi-instance (MI) framework to consider the non-stationarity of the EEG signal. The non-stationary characteristic of EEG is considered by describing the signal as a bag of relevant and irrelevant concepts. The concepts are provided by a robust representation of homogenous segments of EEG signal using spatial covariance matrices. Due to the nonlinear geometry of the space of covariance matrices, we determine the boundaries of the homogeneous segments based on adaptive segmentation of the signal in a Riemannian framework. Each subject is described as a bag of covariance matrices of homogenous segments and the bag-level discriminative information is used for classification. To evaluate the performance of the proposed approach, we examine it in attention deficit hyperactivity/bipolar mood disorder detection and depression/normal diagnosis applications. Experimental results confirm the superiority of the proposed approach, which is gained due to the robustness of covariance descriptor, the effectiveness of Riemannian geometry, and the benefits of considering the inherent non-stationary nature of the brain. version:1
arxiv-1702-02640 | Character-level Deep Conflation for Business Data Analytics | http://arxiv.org/abs/1702.02640 | id:1702.02640 author:Zhe Gan, P. D. Singh, Ameet Joshi, Xiaodong He, Jianshu Chen, Jianfeng Gao, Li Deng category:cs.CL cs.LG  published:2017-02-08 summary:Connecting different text attributes associated with the same entity (conflation) is important in business data analytics since it could help merge two different tables in a database to provide a more comprehensive profile of an entity. However, the conflation task is challenging because two text strings that describe the same entity could be quite different from each other for reasons such as misspelling. It is therefore critical to develop a conflation model that is able to truly understand the semantic meaning of the strings and match them at the semantic level. To this end, we develop a character-level deep conflation model that encodes the input text strings from character level into finite dimension feature vectors, which are then used to compute the cosine similarity between the text strings. The model is trained in an end-to-end manner using back propagation and stochastic gradient descent to maximize the likelihood of the correct association. Specifically, we propose two variants of the deep conflation model, based on long-short-term memory (LSTM) recurrent neural network (RNN) and convolutional neural network (CNN), respectively. Both models perform well on a real-world business analytics dataset and significantly outperform the baseline bag-of-character (BoC) model. version:1
arxiv-1702-02604 | Neural Causal Regularization under the Independence of Mechanisms Assumption | http://arxiv.org/abs/1702.02604 | id:1702.02604 author:Mohammad Taha Bahadori, Krzysztof Chalupka, Edward Choi, Robert Chen, Walter F. Stewart, Jimeng Sun category:cs.LG cs.AI cs.NE stat.ML  published:2017-02-08 summary:Neural networks provide a powerful framework for learning the association between input and response variables and making accurate predictions and offer promise in using the rapidly growing volume of health care data to surface causal relationships that cannot necessarily be tested in randomized clinical trials. In pursuit of models whose predictive power comes maximally from causal variables, we propose a novel causal regularizer based on the assumption about independence of different steps of data generation process. We use the causal regularizer to steer deep neural network architectures towards causally-interpretable solutions. We perform a large-scale analysis of electronic health records. Our causally-regularized algorithm outperforms its L1-regularized counterpart both in predictive performance as well as causal relevance. Finally, we show that the proposed causal regularizer can be used together with representation learning algorithms to yield up to 20% improvement in the causality score of the generated multivariate hypotheses. version:1
arxiv-1702-02584 | Convolutional Neural Network for Humor Recognition | http://arxiv.org/abs/1702.02584 | id:1702.02584 author:Lei Chen, Chong MIn Lee category:cs.CL  published:2017-02-08 summary:For the purpose of automatically evaluating speakers' humor usage, we build a presentation corpus containing humorous utterances based on TED talks. Compared to previous data resources supporting humor recognition research, ours has several advantages, including (a) both positive and negative instances coming from a homogeneous data set, (b) containing a large number of speakers, and (c) being open. Focusing on using lexical cues for humor recognition, we systematically compare a newly emerging text classification method based on Convolutional Neural Networks (CNNs) with a well-established conventional method using linguistic knowledge. The CNN method shows its advantages on both higher recognition accuracies and being able to learn essential features automatically. version:1
arxiv-1702-02549 | Backpropagation Training for Fisher Vectors within Neural Networks | http://arxiv.org/abs/1702.02549 | id:1702.02549 author:Patrick Wieschollek, Fabian Groh, Hendrik P. A. Lensch category:cs.CV  published:2017-02-08 summary:Fisher-Vectors (FV) encode higher-order statistics of a set of multiple local descriptors like SIFT features. They already show good performance in combination with shallow learning architectures on visual recognitions tasks. Current methods using FV as a feature descriptor in deep architectures assume that all original input features are static. We propose a framework to jointly learn the representation of original features, FV parameters and parameters of the classifier in the style of traditional neural networks. Our proof of concept implementation improves the performance of FV on the Pascal Voc 2007 challenge in a multi-GPU setting in comparison to a default SVM setting. We demonstrate that FV can be embedded into neural networks at arbitrary positions, allowing end-to-end training with back-propagation. version:1
arxiv-1702-02540 | Automatic Rule Extraction from Long Short Term Memory Networks | http://arxiv.org/abs/1702.02540 | id:1702.02540 author:W. James Murdoch, Arthur Szlam category:cs.CL cs.AI cs.NE stat.ML  published:2017-02-08 summary:Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM. version:1
arxiv-1702-02537 | Soft Biometrics: Gender Recognition from Unconstrained Face Images using Local Feature Descriptor | http://arxiv.org/abs/1702.02537 | id:1702.02537 author:Olasimbo Ayodeji Arigbabu, Sharifah Mumtazah Syed Ahmad, Wan Azizun Wan Adnan, Salman Yussof, Saif Mahmood category:cs.CV  published:2017-02-08 summary:Gender recognition from unconstrained face images is a challenging task due to the high degree of misalignment, pose, expression, and illumination variation. In previous works, the recognition of gender from unconstrained face images is approached by utilizing image alignment, exploiting multiple samples per individual to improve the learning ability of the classifier, or learning gender based on prior knowledge about pose and demographic distributions of the dataset. However, image alignment increases the complexity and time of computation, while the use of multiple samples or having prior knowledge about data distribution is unrealistic in practical applications. This paper presents an approach for gender recognition from unconstrained face images. Our technique exploits the robustness of local feature descriptor to photometric variations to extract the shape description of the 2D face image using a single sample image per individual. The results obtained from experiments on Labeled Faces in the Wild (LFW) dataset describe the effectiveness of the proposed method. The essence of this study is to investigate the most suitable functions and parameter settings for recognizing gender from unconstrained face images. version:1
arxiv-1702-02535 | Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization | http://arxiv.org/abs/1702.02535 | id:1702.02535 author:Ye Zhang, Matthew Lease, Byron C. Wallace category:cs.CL  published:2017-02-08 summary:A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing. version:1
arxiv-1702-02530 | Learning detectors of malicious web requests for intrusion detection in network traffic | http://arxiv.org/abs/1702.02530 | id:1702.02530 author:Lukas Machlica, Karel Bartos, Michal Sofka category:stat.ML cs.LG  published:2017-02-08 summary:This paper proposes a generic classification system designed to detect security threats based on the behavior of malware samples. The system relies on statistical features computed from proxy log fields to train detectors using a database of malware samples. The behavior detectors serve as basic reusable building blocks of the multi-level detection architecture. The detectors identify malicious communication exploiting encrypted URL strings and domains generated by a Domain Generation Algorithm (DGA) which are frequently used in Command and Control (C&C), phishing, and click fraud. Surprisingly, very precise detectors can be built given only a limited amount of information extracted from a single proxy log. This way, the computational requirements of the detectors are kept low which allows for deployment on a wide range of security devices and without depending on traffic context such as DNS logs, Whois records, webpage content, etc. Results on several weeks of live traffic from 100+ companies having 350k+ hosts show correct detection with a precision exceeding 95% of malicious flows, 95% of malicious URLs and 90% of infected hosts. In addition, a comparison with a signature and rule-based solution shows that our system is able to detect significant amount of new threats. version:1
arxiv-1702-02526 | Deep Kernelized Autoencoders | http://arxiv.org/abs/1702.02526 | id:1702.02526 author:Michael Kampffmeyer, Sigurd Løkse, Filippo Maria Bianchi, Robert Jenssen, Lorenzo Livi category:stat.ML cs.LG cs.NE  published:2017-02-08 summary:In this paper we introduce the deep kernelized autoencoder, a neural network model that allows an explicit approximation of (i) the mapping from an input space to an arbitrary, user-specified kernel space and (ii) the back-projection from such a kernel space to input space. The proposed method is based on traditional autoencoders and is trained through a new unsupervised loss function. During training, we optimize both the reconstruction accuracy of input samples and the alignment between a kernel matrix given as prior and the inner products of the hidden representations computed by the autoencoder. Kernel alignment provides control over the hidden representation learned by the autoencoder. Experiments have been performed to evaluate both reconstruction and kernel alignment performance. Additionally, we applied our method to emulate kPCA on a denoising task obtaining promising results. version:1
arxiv-1702-02519 | Deep Generalized Canonical Correlation Analysis | http://arxiv.org/abs/1702.02519 | id:1702.02519 author:Adrian Benton, Huda Khayrallah, Biman Gujral, Drew Reisinger, Sheng Zhang, Raman Arora category:cs.LG cs.AI stat.ML  published:2017-02-08 summary:We present Deep Generalized Canonical Correlation Analysis (DGCCA) -- a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA repre- sentations on two distinct datasets for three downstream tasks: phonetic transcrip- tion from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques. version:1
arxiv-1702-02514 | Monocular LSD-SLAM Integration within AR System | http://arxiv.org/abs/1702.02514 | id:1702.02514 author:Markus Höll, Vincent Lepetit category:cs.CV cs.GR cs.SE  published:2017-02-08 summary:In this paper, we cover the process of integrating Large-Scale Direct Simultaneous Localization and Mapping (LSD-SLAM) algorithm into our existing AR stereo engine, developed for our modified "Augmented Reality Oculus Rift". With that, we are able to track one of our realworld cameras which are mounted on the rift, within a complete unknown environment. This makes it possible to achieve a constant and full augmentation, synchronizing our 3D movement (x, y, z) in both worlds, the real world and the virtual world. The development for the basic AR setup using the Oculus Rift DK1 and two fisheye cameras is fully documented in our previous paper. After an introduction to image-based registration, we detail the LSD-SLAM algorithm and document our code implementing our integration. The AR stereo engine with Oculus Rift support can be accessed via the GIT repository https://github.com/MaXvanHeLL/ARift.git and the modified LSD-SLAM project used for the integration is available here https://github.com/MaXvanHeLL/LSD-SLAM.git. version:1
arxiv-1702-00887 | Structured Attention Networks | http://arxiv.org/abs/1702.00887 | id:1702.00887 author:Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush category:cs.CL cs.LG cs.NE  published:2017-02-03 summary:Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention. version:2
arxiv-1702-02463 | Video Frame Synthesis using Deep Voxel Flow | http://arxiv.org/abs/1702.02463 | id:1702.02463 author:Ziwei Liu, Raymond Yeh, Xiaoou Tang, Yiming Liu, Aseem Agarwala category:cs.CV cs.GR cs.LG  published:2017-02-08 summary:We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-of-the-art. version:1
arxiv-1702-02453 | Preparing for the Unknown: Learning a Universal Policy with Online System Identification | http://arxiv.org/abs/1702.02453 | id:1702.02453 author:Wenhao Yu, C. Karen Liu, Greg Turk category:cs.LG cs.RO cs.SY  published:2017-02-08 summary:We present a new method of learning control policies that successfully operate under unknown dynamic models. We create such policies by leveraging a large number of training examples that are generated using a physical simulator. Our system is made of two components: a Universal Policy (UP) and a function for Online System Identification (OSI). We describe our control policy as universal because it is trained over a wide array of dynamic models. These variations in the dynamic model may include differences in mass and inertia of the robots' components, variable friction coefficients, or unknown mass of an object to be manipulated. By training the Universal Policy with this variation, the control policy is prepared for a wider array of possible conditions when executed in an unknown environment. The second part of our system uses the recent state and action history of the system to predict the dynamics model parameters mu. The value of mu from the Online System Identification is then provided as input to the control policy (along with the system state). Together, UP-OSI is a robust control policy that can be used across a wide range of dynamic models, and that is also responsive to sudden changes in the environment. We have evaluated the performance of this system on a variety of tasks, including the problem of cart-pole swing-up, the double inverted pendulum, locomotion of a hopper, and block-throwing of a manipulator. UP-OSI is effective at these tasks across a wide range of dynamic models. Moreover, when tested with dynamic models outside of the training range, UP-OSI outperforms the Universal Policy alone, even when UP is given the actual value of the model dynamics. In addition to the benefits of creating more robust controllers, UP-OSI also holds out promise of narrowing the Reality Gap between simulated and real physical systems. version:1
arxiv-1702-02447 | Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation | http://arxiv.org/abs/1702.02447 | id:1702.02447 author:Hengkai Guo, Guijin Wang, Xinghao Chen, Cairong Zhang, Fei Qiao, Huazhong Yang category:cs.CV  published:2017-02-08 summary:Hand pose estimation from monocular depth images is an important and challenging problem for human-computer interaction. Recently deep convolutional networks (ConvNet) with sophisticated design have been employed to address it, but the improvement over traditional methods is not so apparent. To promote the performance of directly 3D coordinate regression, we propose a tree-structured Region Ensemble Network (REN), which partitions the convolution outputs into regions and integrates the results from multiple regressors on each regions. Compared with multi-model ensemble, our model is completely end-to-end training. The experimental results demonstrate that our approach achieves the best performance among state-of-the-arts on two public datasets. version:1
arxiv-1702-02445 | Hyperspectral Sharpening using Scene-adapted Gaussian Mixture Priors | http://arxiv.org/abs/1702.02445 | id:1702.02445 author:Afonso M. Teodoro, José M. Bioucas-Dias, Mário A. T. Figueiredo category:cs.CV 94A08  68U10  47N10 I.4.5; I.4.4  published:2017-02-08 summary:This paper tackles a hyperspectral data fusion problem, using the so-called plug-and-play approach, which combines an ADMM algorithm with a denoiser based on a Gaussian mixture prior. We build upon the concept of scene-adapted prior where, as the name suggests, we learn a model that is targeted to the specific scene being imaged, and show state-of-the-art results on several hyperspectral sharpening experiments. Additionally, we prove that the algorithm is guaranteed to converge. version:1
arxiv-1702-02098 | Fast and Accurate Sequence Labeling with Iterated Dilated Convolutions | http://arxiv.org/abs/1702.02098 | id:1702.02098 author:Emma Strubell, Patrick Verga, David Belanger, Andrew McCallum category:cs.CL  published:2017-02-07 summary:Bi-directional LSTMs have emerged as a standard method for obtaining per-token vector representations serving as input to various token labeling tasks (whether followed by Viterbi prediction or independent classification). This paper proposes an alternative to Bi-LSTMs for this purpose: iterated dilated convolutional neural networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. We describe a distinct combination of network structure, parameter sharing and training procedures that is not only more accurate than Bi-LSTM-CRFs, but also 8x faster at test time on long sequences. Moreover, ID-CNNs with independent classification enable a dramatic 14x test-time speedup, while still attaining accuracy comparable to the Bi-LSTM-CRF. We further demonstrate the ability of ID-CNNs to combine evidence over long sequences by demonstrating their improved accuracy on whole-document (rather than per-sentence) inference. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of parallelism, IDCNNs permit fixed-depth convolutions to run in parallel across entire documents. Today when many companies run basic NLP on the entire web and large-volume traffic, faster methods are paramount to saving time and energy costs. version:2
arxiv-1702-02429 | Trainable Greedy Decoding for Neural Machine Translation | http://arxiv.org/abs/1702.02429 | id:1702.02429 author:Jiatao Gu, Kyunghyun Cho, Victor O. K. Li category:cs.CL cs.LG  published:2017-02-08 summary:Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead. version:1
arxiv-1702-02426 | Data Selection Strategies for Multi-Domain Sentiment Analysis | http://arxiv.org/abs/1702.02426 | id:1702.02426 author:Sebastian Ruder, Parsa Ghaffari, John G. Breslin category:cs.CL cs.LG  published:2017-02-08 summary:Domain adaptation is important in sentiment analysis as sentiment-indicating words vary between domains. Recently, multi-domain adaptation has become more pervasive, but existing approaches train on all available source domains including dissimilar ones. However, the selection of appropriate training data is as important as the choice of algorithm. We undertake -- to our knowledge for the first time -- an extensive study of domain similarity metrics in the context of sentiment analysis and propose novel representations, metrics, and a new scope for data selection. We evaluate the proposed methods on two large-scale multi-domain adaptation settings on tweets and reviews and demonstrate that they consistently outperform strong random and balanced baselines, while our proposed selection strategy outperforms instance-level selection and yields the best score on a large reviews corpus. version:1
arxiv-1702-02390 | A Hybrid Convolutional Variational Autoencoder for Text Generation | http://arxiv.org/abs/1702.02390 | id:1702.02390 author:Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth category:cs.CL  published:2017-02-08 summary:In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data. version:1
arxiv-1702-02382 | An Adversarial Regularisation for Semi-Supervised Training of Structured Output Neural Networks | http://arxiv.org/abs/1702.02382 | id:1702.02382 author:Mateusz Koziński, Loïc Simon, Frédéric Jurie category:cs.CV  published:2017-02-08 summary:We propose a method for semi-supervised training of structured-output neural networks. Inspired by the framework of Generative Adversarial Networks (GAN), we train a discriminator network to capture the notion of a quality of network output. To this end, we leverage the qualitative difference between outputs obtained on the labelled training data and unannotated data. We then use the discriminator as a source of error signal for unlabelled data. This effectively boosts the performance of a network on a held out test set. Initial experiments in image segmentation demonstrate that the proposed framework enables achieving the same network performance as in a fully supervised scenario, while using two times less annotations. version:1
arxiv-1702-02047 | Preference-based Teaching | http://arxiv.org/abs/1702.02047 | id:1702.02047 author:Ziyuan Gao, Christoph Ries, Hans Ulrich Simon, Sandra Zilles category:cs.LG  published:2017-02-06 summary:We introduce a new model of teaching named "preference-based teaching" and a corresponding complexity parameter---the preference-based teaching dimension (PBTD)---representing the worst-case number of examples needed to teach any concept in a given concept class. Although the PBTD coincides with the well-known recursive teaching dimension (RTD) on finite classes, it is radically different on infinite ones: the RTD becomes infinite already for trivial infinite classes (such as half-intervals) whereas the PBTD evaluates to reasonably small values for a wide collection of infinite classes including classes consisting of so-called closed sets w.r.t. a given closure operator, including various classes related to linear sets over $\mathbb{N}_0$ (whose RTD had been studied quite recently) and including the class of Euclidean half-spaces. On top of presenting these concrete results, we provide the reader with a theoretical framework (of a combinatorial flavor) which helps to derive bounds on the PBTD. version:2
arxiv-1702-02367 | Iterative Multi-document Neural Attention for Multiple Answer Prediction | http://arxiv.org/abs/1702.02367 | id:1702.02367 author:Claudio Greco, Alessandro Suglia, Pierpaolo Basile, Gaetano Rossiello, Giovanni Semeraro category:cs.CL  published:2017-02-08 summary:People have information needs of varying complexity, which can be solved by an intelligent agent able to answer questions formulated in a proper way, eventually considering user context and preferences. In a scenario in which the user profile can be considered as a question, intelligent agents able to answer questions can be used to find the most relevant answers for a given user. In this work we propose a novel model based on Artificial Neural Networks to answer questions with multiple answers by exploiting multiple facts retrieved from a knowledge base. The model is evaluated on the factoid Question Answering and top-n recommendation tasks of the bAbI Movie Dialog dataset. After assessing the performance of the model on both tasks, we try to define the long-term goal of a conversational recommender system able to interact using natural language and to support users in their information seeking processes in a personalized way. version:1
arxiv-1702-02359 | Multi-scale Convolutional Neural Networks for Crowd Counting | http://arxiv.org/abs/1702.02359 | id:1702.02359 author:Lingke Zeng, Xiangmin Xu, Bolun Cai, Suo Qiu, Tong Zhang category:cs.CV  published:2017-02-08 summary:Crowd counting on static images is a challenging problem due to scale variations. Recently deep neural networks have been shown to be effective in this task. However, existing neural-networks-based methods often use the multi-column or multi-network model to extract the scale-relevant features, which is more complicated for optimization and computation wasting. To this end, we propose a novel multi-scale convolutional neural network (MSCNN) for single image crowd counting. Based on the multi-scale blobs, the network is able to generate scale-relevant features for higher crowd counting performances in a single-column architecture, which is both accuracy and cost effective for practical applications. Complemental results show that our method outperforms the state-of-the-art methods on both accuracy and robustness with far less number of parameters. version:1
arxiv-1702-02873 | Multi-feature classifiers for burst detection in single EEG channels from preterm infants | http://arxiv.org/abs/1702.02873 | id:1702.02873 author:X. Navarro, F. Porée, M. Kuchenbuch, M. Chavez, A. Beuchée, G. Carrault category:q-bio.NC cs.LG  published:2017-02-08 summary:The study of electroencephalographic (EEG) bursts in preterm infants provides valuable information about maturation or prognostication after perinatal asphyxia. Over the last two decades, a number of works proposed algorithms to automatically detect EEG bursts in preterm infants, but they were designed for populations under 35 weeks of post menstrual age (PMA). However, as the brain activity evolves rapidly during postnatal life, these solutions might be under-performing with increasing PMA. In this work we focused on preterm infants reaching term ages (PMA $\geq$ 36 weeks) using multi-feature classification on a single EEG channel. Five EEG burst detectors relying on different machine learning approaches were compared: Logistic regression (LR), linear discriminant analysis (LDA), k-nearest neighbors (kNN), support vector machines (SVM) and thresholding (Th). Classifiers were trained by visually labeled EEG recordings from 14 very preterm infants (born after 28 weeks of gestation) with 36 - 41 weeks PMA. The most performing classifiers reached about 95\% accuracy (kNN, SVM and LR) whereas Th obtained 84\%. Compared to human-automatic agreements, LR provided the highest scores (Cohen's kappa = 0.71) and the best computational efficiency using only three EEG features. Applying this classifier in a test database of 21 infants $\geq$ 36 weeks PMA, we show that long EEG bursts and short inter-bust periods are characteristic of infants with the highest PMA and weights. In view of these results, LR-based burst detection could be a suitable tool to study maturation in monitoring or portable devices using a single EEG channel. version:1
arxiv-1702-02138 | An Implementation of Faster RCNN with Study for Region Sampling | http://arxiv.org/abs/1702.02138 | id:1702.02138 author:Xinlei Chen, Abhinav Gupta category:cs.CV  published:2017-02-07 summary:We adapted the join-training scheme of Faster RCNN framework from Caffe to TensorFlow as a baseline implementation for object detection. Our code is made publicly available. This report documents the simplifications made to the original pipeline, with justifications from ablation analysis on both PASCAL VOC 2007 and COCO 2014. We further investigated the role of non-maximal suppression (NMS) in selecting regions-of-interest (RoIs) for region classification, and found that a biased sampling toward small regions helps performance and can achieve on-par mAP to NMS-based sampling when converged sufficiently. version:2
arxiv-1702-02295 | Guided Optical Flow Learning | http://arxiv.org/abs/1702.02295 | id:1702.02295 author:Yi Zhu, Zhenzhong Lan, Shawn Newsam, Alexander G. Hauptmann category:cs.CV  published:2017-02-08 summary:We study the unsupervised learning of CNNs for optical flow estimation using proxy ground truth data. Supervised CNNs, due to their immense learning capacity, have shown superior performance on a range of computer vision problems including optical flow prediction. They however require the ground truth flow which is usually not accessible except on limited synthetic data. Without the guidance of ground truth optical flow, unsupervised CNNs often perform worse as they are naturally ill-conditioned. We therefore propose a novel framework in which proxy ground truth data generated from classical approaches is used to guide the CNN learning. The models are further refined in an unsupervised fashion using an image reconstruction loss. Our guided learning approach is competitive with or superior to state-of-the-art approaches on three standard benchmark datasets yet is completely unsupervised and can run in real time. version:1
arxiv-1702-02287 | Name Entity Disambiguation in Anonymized Graphs using Link Analysis: A Network Embedding based Solution | http://arxiv.org/abs/1702.02287 | id:1702.02287 author:Baichuan Zhang, Mohammad Al Hasan category:cs.SI cs.CL cs.IR  published:2017-02-08 summary:In real-world, our DNA is unique but many people share same names. This phenomenon often causes erroneous aggregation of documents of multiple persons who are namesake of one another. Such mistakes deteriorate the performance of document retrieval, web search, and more seriously, cause improper attribution of credit or blame in digital forensic. To resolve this issue, the name entity disambiguation task is designed which aims to partition the documents associated with a name reference such that each partition contains documents pertaining to a unique real-life person. Existing solutions to this task substantially rely on feature engineering, such as biographical feature extraction, or construction of auxiliary features from Wikipedia. However, for many scenarios, such features may be costly to obtain or unavailable due to the risk of privacy violation. In this work, we propose a novel name disambiguation method. Our proposed method is non-intrusive of privacy because instead of using attributes pertaining to a real-life person, our method leverages only relational data in the form of anonymized graphs. In the aspect of methodological novelty, the proposed method uses a representation learning strategy to embed each document in a low dimensional vector space where name disambiguation can be solved by a hierarchical agglomerative clustering algorithm. Our experimental results demonstrate that the proposed method is significantly better than the existing name entity disambiguation methods working in a similar setting. version:1
arxiv-1702-02284 | Adversarial Attacks on Neural Network Policies | http://arxiv.org/abs/1702.02284 | id:1702.02284 author:Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, Pieter Abbeel category:cs.LG cs.CR stat.ML  published:2017-02-08 summary:Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial. version:1
arxiv-1702-02277 | A Historical Review of Forty Years of Research on CMAC | http://arxiv.org/abs/1702.02277 | id:1702.02277 author:Frank Z. Xing category:cs.NE cs.AI A.1  published:2017-02-08 summary:The Cerebellar Model Articulation Controller (CMAC) is an influential brain-inspired computing model in many relevant fields. Since its inception in the 1970s, the model has been intensively studied and many variants of the prototype, such as Kernel-CMAC, Self-Organizing Map CMAC, and Linguistic CMAC, have been proposed. This review article focus on how the CMAC model is gradually developed and refined to meet the demand of fast, adaptive, and robust control. Two perspective, CMAC as a neural network and CMAC as a table look-up technique are presented. Three aspects of the model: the architecture, learning algorithms and applications are discussed. In the end, some potential future research directions on this model are suggested. version:1
arxiv-1702-02267 | Matrix Completion from $O(n)$ Samples in Linear Time | http://arxiv.org/abs/1702.02267 | id:1702.02267 author:David Gamarnik, Quan Li, Hongyi Zhang category:stat.ML cs.DS cs.LG math.OC  published:2017-02-08 summary:We consider the problem of reconstructing a rank-$k$ $n \times n$ matrix $M$ from a sampling of its entries. Under a certain incoherence assumption on $M$ and for the case when both the rank and the condition number of $M$ are bounded, it was shown in \cite{CandesRecht2009, CandesTao2010, keshavan2010, Recht2011, Jain2012, Hardt2014} that $M$ can be recovered exactly or approximately (depending on some trade-off between accuracy and computational complexity) using $O(n \, \text{poly}(\log n))$ samples in super-linear time $O(n^{a} \, \text{poly}(\log n))$ for some constant $a \geq 1$. In this paper, we propose a new matrix completion algorithm using a novel sampling scheme based on a union of independent sparse random regular bipartite graphs. We show that under the same conditions w.h.p. our algorithm recovers an $\epsilon$-approximation of $M$ in terms of the Frobenius norm using $O(n \log^2(1/\epsilon))$ samples and in linear time $O(n \log^2(1/\epsilon))$. This provides the best known bound on the sample complexity and computational cost for reconstructing an unknown low-rank matrix. The novelty of our algorithm is a new step of thresholding singular values and rescaling singular vectors in the application of the "vanilla" alternating minimization algorithm. The structure of sparse random regular graphs is used heavily for controlling the impact of these regularization steps. version:1
arxiv-1701-08473 | Model-based Classification and Novelty Detection For Point Pattern Data | http://arxiv.org/abs/1701.08473 | id:1701.08473 author:Ba-Ngu Vo, Quang N. Tran, Dinh Phung, Ba-Tuong Vo category:cs.LG stat.ML  published:2017-01-30 summary:Point patterns are sets or multi-sets of unordered elements that can be found in numerous data sources. However, in data analysis tasks such as classification and novelty detection, appropriate statistical models for point pattern data have not received much attention. This paper proposes the modelling of point pattern data via random finite sets (RFS). In particular, we propose appropriate likelihood functions, and a maximum likelihood estimator for learning a tractable family of RFS models. In novelty detection, we propose novel ranking functions based on RFS models, which substantially improve performance. version:2
arxiv-1702-00824 | YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video | http://arxiv.org/abs/1702.00824 | id:1702.00824 author:Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, Vincent Vanhoucke category:cs.CV  published:2017-02-02 summary:We introduce a new large-scale data set of video URLs with densely-sampled object bounding box annotations called YouTube-BoundingBoxes (YT-BB). The data set consists of approximately 380,000 video segments about 19s long, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera. The objects represent a subset of the MS COCO label set. All video segments were human-annotated with high-precision classification labels and bounding boxes at 1 frame per second. The use of a cascade of increasingly precise human annotations ensures a label accuracy above 95% for every class and tight bounding boxes. Finally, we train and evaluate well-known deep network architectures and report baseline figures for per-frame classification and localization to provide a point of comparison for future work. We also demonstrate how the temporal contiguity of video can potentially be used to improve such inferences. Please see the PDF file to find the URL to download the data. We hope the availability of such large curated corpus will spur new advances in video object detection and tracking. version:3
arxiv-1702-02265 | Neural Machine Translation with Source-Side Latent Graph Parsing | http://arxiv.org/abs/1702.02265 | id:1702.02265 author:Kazuma Hashimoto, Yoshimasa Tsuruoka category:cs.CL  published:2017-02-08 summary:This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, so the parser is optimized according to the translation objective. Experimental results show that our model significantly outperforms the previous best results on the standard English-to-Japanese translation dataset. version:1
arxiv-1702-02262 | Clustering For Point Pattern Data | http://arxiv.org/abs/1702.02262 | id:1702.02262 author:Quang N. Tran, Ba-Ngu Vo, Dinh Phung, Ba-Tuong Vo category:cs.LG stat.ML  published:2017-02-08 summary:Clustering is one of the most common unsupervised learning tasks in machine learning and data mining. Clustering algorithms have been used in a plethora of applications across several scientific fields. However, there has been limited research in the clustering of point patterns - sets or multi-sets of unordered elements - that are found in numerous applications and data sources. In this paper, we propose two approaches for clustering point patterns. The first is a non-parametric method based on novel distances for sets. The second is a model-based approach, formulated via random finite set theory, and solved by the Expectation-Maximization algorithm. Numerical experiments show that the proposed methods perform well on both simulated and real data. version:1
arxiv-1702-02261 | Social media mining for identification and exploration of health-related information from pregnant women | http://arxiv.org/abs/1702.02261 | id:1702.02261 author:Pramod Bharadwaj Chandrashekar, Arjun Magge, Abeed Sarker, Graciela Gonzalez category:cs.CL  published:2017-02-08 summary:Widespread use of social media has led to the generation of substantial amounts of information about individuals, including health-related information. Social media provides the opportunity to study health-related information about selected population groups who may be of interest for a particular study. In this paper, we explore the possibility of utilizing social media to perform targeted data collection and analysis from a particular population group -- pregnant women. We hypothesize that we can use social media to identify cohorts of pregnant women and follow them over time to analyze crucial health-related information. To identify potentially pregnant women, we employ simple rule-based searches that attempt to detect pregnancy announcements with moderate precision. To further filter out false positives and noise, we employ a supervised classifier using a small number of hand-annotated data. We then collect their posts over time to create longitudinal health timelines and attempt to divide the timelines into different pregnancy trimesters. Finally, we assess the usefulness of the timelines by performing a preliminary analysis to estimate drug intake patterns of our cohort at different trimesters. Our rule-based cohort identification technique collected 53,820 users over thirty months from Twitter. Our pregnancy announcement classification technique achieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user timelines. Analysis of the timelines revealed that pertinent health-related information, such as drug-intake and adverse reactions can be mined from the data. Our approach to using user timelines in this fashion has produced very encouraging results and can be employed for other important tasks where cohorts, for which health-related information may not be available from other sources, are required to be followed over time to derive population-based estimates. version:1
arxiv-1702-02258 | Generating Multiple Hypotheses for Human 3D Pose Consistent with 2D Joint Detections | http://arxiv.org/abs/1702.02258 | id:1702.02258 author:Ehsan Jahangiri, Alan L. Yuille category:cs.CV cs.AI cs.MM stat.ML  published:2017-02-08 summary:We propose a method to generate multiple hypotheses for human 3D pose all of them consistent with the 2D detection of joints in a monocular RGB image. To generate these pose hypotheses we use a novel generative model defined in the space of anatomically plausible 3D poses satisfying the joint angle limits and limb length ratios. The proposed generative model is uniform in the space of anatomically valid poses and as a result, does not suffer from the dataset bias in existing motion capture datasets such as Human3.6M (H36M), HumanEva, and CMU MoCap. A good model that spans the full variability of human pose and generalizes to unseen poses must be compositional i.e., produce a pose by combining parts. Our model is flexible and compositional and consequently can generalize to every plausible human 3D pose since it is only limited by physical constraints. We discuss sampling from this model and use these samples to generate multiple diverse human 3D pose hypotheses given the 2D detection of joints. We argue that generating multiple pose hypotheses from a monocular RGB image is more reasonable than generating only a single 3D pose given the depth ambiguity and the uncertainty caused by occlusion and imperfect 2D joint detection. To support this argument, we have performed empirical evaluation on the popular Human3.6M dataset that confirms that most often, at least one of our pose hypotheses is closer to the true 3D pose compared to the estimated pose by other recent baseline methods for 3D pose reconstruction from monocular RGB images. The idea of generating multiple consistent and valid pose hypotheses can give rise to a new line of future work that has not previously been addressed in the literature. version:1
arxiv-1702-02235 | Automated Low-cost Terrestrial Laser Scanner for Measuring Diameters at Breast Height and Heights of Forest Trees | http://arxiv.org/abs/1702.02235 | id:1702.02235 author:Pei Wang, Guochao Bu, Ronghao Li, Rui Zhao category:cs.CV  published:2017-02-08 summary:Terrestrial laser scanner is a kind of fast, high-precision data acquisition device, which had been more and more applied to the research areas of forest inventory. In this study, a kind of automated low-cost terrestrial laser scanner was designed and implemented based on a two-dimensional laser radar sensor SICK LMS-511 and a stepper motor. The new scanner was named as BEE, which can scan the forest trees in three dimension. The BEE scanner and its supporting software are specifically designed for forest inventory. The experiments have been performed by using the BEE scanner in an artificial ginkgo forest which was located in Haidian district of Beijing. Four square plots were selected to do the experiments. The BEE scanner scanned in the four plots and acquired the single scan data respectively. The DBH, tree height and tree position of trees in the four plots were estimated and analyzed. For comparison, the manual measured data was also collected in the four plots. The tree stem detection rate for all four plots was 92.75%; the root mean square error of the DBH estimation was 1.27cm; the root mean square error of the tree height estimation was 0.24m; the tree position estimation was in line with the actual position. Experimental results show that the BEE scanner can efficiently estimate the structure parameters of forest trees and has a good potential in practical application of forest inventory. version:1
arxiv-1702-02223 | Comparison of machine learning methods for classifying mediastinal lymph node metastasis of non-small cell lung cancer from 18F-FDG PET/CT images | http://arxiv.org/abs/1702.02223 | id:1702.02223 author:Hongkai Wang, Zongwei Zhou, Yingci Li, Zhonghua Chen, Peiou Lu, Wenzhi Wang, Wanyu Liu, Lijuan Yu category:cs.CV physics.med-ph  published:2017-02-07 summary:The present study shows that the performance of CNN is not significantly different from the best classical methods and human doctors for classifying mediastinal lymph node metastasis of NSCLC from PET/CT images. Because CNN does not need tumor segmentation or feature calculation, it is more convenient and more objective than the classical methods. However, CNN does not make use of the import diagnostic features, which have been proved more discriminative than the texture features for classifying small-sized lymph nodes. Therefore, incorporating the diagnostic features into CNN is a promising direction for future research. version:1
arxiv-1702-02217 | Multitask Evolution with Cartesian Genetic Programming | http://arxiv.org/abs/1702.02217 | id:1702.02217 author:Eric O. Scott, Kenneth A. De Jong category:cs.NE  published:2017-02-07 summary:We introduce a genetic programming method for solving multiple Boolean circuit synthesis tasks simultaneously. This allows us to solve a set of elementary logic functions twice as easily as with a direct, single-task approach. version:1
arxiv-1702-00317 | On SGD's Failure in Practice: Characterizing and Overcoming Stalling | http://arxiv.org/abs/1702.00317 | id:1702.00317 author:Vivak Patel category:stat.ML cs.LG math.OC stat.CO 62L20  62L12  90C99 G.1.6; G.3; I.2.6  published:2017-02-01 summary:Stochastic Gradient Descent (SGD) is widely used in machine learning problems to efficiently perform empirical risk minimization, yet, in practice, SGD is known to stall before reaching the actual minimizer of the empirical risk. SGD stalling has often been attributed to its sensitivity to the conditioning of the problem; however, as we demonstrate, SGD will stall even when applied to a simple linear regression problem with unity condition number for standard learning rates. Thus, in this work, we numerically demonstrate and mathematically argue that stalling is a crippling and generic limitation of SGD and its variants in practice. Once we have established the problem of stalling, we generalize an existing framework for hedging against its effects, which (1) deters SGD and its variants from stalling, (2) still provides convergence guarantees, and (3) makes SGD and its variants more practical methods for minimization. version:2
arxiv-1702-02206 | Semi-Supervised QA with Generative Domain-Adaptive Nets | http://arxiv.org/abs/1702.02206 | id:1702.02206 author:Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, William W. Cohen category:cs.CL cs.LG  published:2017-02-07 summary:We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text. version:1
arxiv-1702-02184 | Transfer from Multiple Linear Predictive State Representations (PSR) | http://arxiv.org/abs/1702.02184 | id:1702.02184 author:Sri Ramana Sekharan, Ramkumar Natarajan, Siddharthan Rajasekaran category:cs.LG  published:2017-02-07 summary:In this paper, we tackle the problem of transferring policy from multiple partially observable source environments to a partially observable target environment modeled as predictive state representation. This is an entirely new approach with no previous work, other than the case of transfer in fully observable domains. We develop algorithms to successfully achieve policy transfer when we have the model of both the source and target tasks and discuss in detail their performance and shortcomings. These algorithms could be a starting point for the field of transfer learning in partial observability. version:1
arxiv-1702-02181 | Deep Learning with Dynamic Computation Graphs | http://arxiv.org/abs/1702.02181 | id:1702.02181 author:Moshe Looks, Marcello Herreshoff, DeLesley Hutchins, Peter Norvig category:cs.NE cs.LG stat.ML  published:2017-02-07 summary:Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature. version:1
arxiv-1702-02175 | Keyframe-Based Visual-Inertial Online SLAM with Relocalization | http://arxiv.org/abs/1702.02175 | id:1702.02175 author:Anton Kasyanov, Francis Engelmann, Jörg Stückler, Bastian Leibe category:cs.CV  published:2017-02-07 summary:Complementing images with inertial measurements has become one of the most popular approaches to achieve highly accurate and robust real-time camera pose tracking. In this paper, we present a keyframe-based approach to visual-inertial simultaneous localization and mapping (SLAM) for monocular and stereo cameras. Our method is based on a real-time capable visual-inertial odometry method that provides locally consistent trajectory and map estimates. We achieve global consistency in the estimate through online loop-closing and non-linear optimization. Furthermore, our approach supports relocalization in a map that has been previously obtained and allows for continued SLAM operation. We evaluate our approach in terms of accuracy, relocalization capability and run-time efficiency on public benchmark datasets and on newly recorded sequences. We demonstrate state-of-the-art performance of our approach towards a visual-inertial odometry method in recovering the trajectory of the camera. version:1
arxiv-1702-02171 | Question Answering through Transfer Learning from Large Fine-grained Supervision Data | http://arxiv.org/abs/1702.02171 | id:1702.02171 author:Sewon Min, Minjoon Seo, Hannaneh Hajishirzi category:cs.CL  published:2017-02-07 summary:We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task. version:1
arxiv-1702-02170 | How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks | http://arxiv.org/abs/1702.02170 | id:1702.02170 author:Stanisław Jastrzebski, Damian Leśniak, Wojciech Marian Czarnecki category:cs.CL  published:2017-02-07 summary:Maybe the single most important goal of representation learning is making subsequent learning faster. Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning). In order to illustrate significance of such analysis, a comprehensive evaluation of selected word embeddings is presented. Proposed approach yields a more complete picture and brings new insight into performance characteristics, for instance information about word similarity or analogy tends to be non--linearly encoded in the embedding space, which questions the cosine-based, unsupervised, evaluation methods. All results and analysis scripts are available online. version:1
arxiv-1702-02165 | Robust Clustering for Time Series Using Spectral Densities and Functional Data Analysis | http://arxiv.org/abs/1702.02165 | id:1702.02165 author:Diego Rivera-García, Luis Angel García-Escudero, Agustín Mayo-Iscar, Joaquín Ortega category:stat.ML 62G35  62H30  published:2017-02-07 summary:In this work a robust clustering algorithm for stationary time series is proposed. The algorithm is based on the use of estimated spectral densities, which are considered as functional data, as the basic characteristic of stationary time series for clustering purposes. A robust algorithm for functional data is then applied to the set of spectral densities. Trimming techniques and restrictions on the scatter within groups reduce the effect of noise in the data and help to prevent the identification of spurious clusters. The procedure is tested in a simulation study, and is also applied to a real data set. version:1
arxiv-1702-00500 | AMR-to-text Generation with Synchronous Node Replacement Grammar | http://arxiv.org/abs/1702.00500 | id:1702.00500 author:Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, Daniel Gildea category:cs.CL  published:2017-02-01 summary:This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which is the best reported so far. version:2
arxiv-1702-02125 | Estimation of classrooms occupancy using a multi-layer perceptron | http://arxiv.org/abs/1702.02125 | id:1702.02125 author:Eugénio Rodrigues, Luísa Dias Pereira, Adélio Rodrigues Gaspar, Álvaro Gomes, Manuel Carlos Gameiro da Silva category:cs.NE cs.LG 68T01 I.5.1  published:2017-02-07 summary:This paper presents a multi-layer perceptron model for the estimation of classrooms number of occupants from sensed indoor environmental data-relative humidity, air temperature, and carbon dioxide concentration. The modelling datasets were collected from two classrooms in the Secondary School of Pombal, Portugal. The number of occupants and occupation periods were obtained from class attendance reports. However, post-class occupancy was unknown and the developed model is used to reconstruct the classrooms occupancy by filling the unreported periods. Different model structure and environment variables combination were tested. The model with best accuracy had as input vector 10 variables of five averaged time intervals of relative humidity and carbon dioxide concentration. The model presented a mean square error of 1.99, coefficient of determination of 0.96 with a significance of p-value < 0.001, and a mean absolute error of 1 occupant. These results show promising estimation capabilities in uncertain indoor environment conditions. version:1
arxiv-1702-02103 | An Integrated Simulator and Dataset that Combines Grasping and Vision for Deep Learning | http://arxiv.org/abs/1702.02103 | id:1702.02103 author:Matthew Veres, Medhat Moussa, Graham W. Taylor category:cs.RO stat.ML  published:2017-02-07 summary:Deep learning is an established framework for learning hierarchical data representations. While compute power is in abundance, one of the main challenges in applying this framework to robotic grasping has been obtaining the amount of data needed to learn these representations, and structuring the data to the task at hand. Among contemporary approaches in the literature, we highlight key properties that have encouraged the use of deep learning techniques, and in this paper, detail our experience in developing a simulator for collecting cylindrical precision grasps of a multi-fingered dexterous robotic hand. version:1
arxiv-1702-02144 | Rapid parametric density estimation | http://arxiv.org/abs/1702.02144 | id:1702.02144 author:Jarek Duda category:cs.LG  published:2017-02-07 summary:Parametric density estimation, for example as Gaussian distribution, is the base of the field of statistics. Machine learning requires inexpensive estimation of much more complex densities, and the basic approach is relatively costly maximum likelihood estimation (MLE). There will be discussed inexpensive density estimators, for example literally fitting polynomial to the sample, which coefficients are calculated from just averaging monomials over the sample (estimators of moments). Another discussed basic application is fitting distortion to some standard distribution like Gaussian. The estimated parameters are approaching the optimal values with error dropping like $1/\sqrt{n}$, where $n$ is the sample size. version:1
arxiv-1701-07481 | Learning Word-Like Units from Joint Audio-Visual Analysis | http://arxiv.org/abs/1701.07481 | id:1701.07481 author:David Harwath, James R. Glass category:cs.CL cs.CV  published:2017-01-25 summary:Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word 'lighthouse' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images. version:2
arxiv-1702-02052 | Knowledge Adaptation: Teaching to Adapt | http://arxiv.org/abs/1702.02052 | id:1702.02052 author:Sebastian Ruder, Parsa Ghaffari, John G. Breslin category:cs.CL cs.LG  published:2017-02-07 summary:Domain adaptation is crucial in many real-world applications where the distribution of the training data differs from the distribution of the test data. Previous Deep Learning-based approaches to domain adaptation need to be trained jointly on source and target domain data and are therefore unappealing in scenarios where models need to be adapted to a large number of domains or where a domain is evolving, e.g. spam detection where attackers continuously change their tactics. To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain adaptation scenario. We show how a student model achieves state-of-the-art results on unsupervised domain adaptation from multiple sources on a standard sentiment analysis benchmark by taking into account the domain-specific expertise of multiple teachers and the similarities between their domains. When learning from a single teacher, using domain similarity to gauge trustworthiness is inadequate. To this end, we propose a simple metric that correlates well with the teacher's accuracy in the target domain. We demonstrate that incorporating high-confidence examples selected by this metric enables the student model to achieve state-of-the-art performance in the single-source scenario. version:1
arxiv-1702-02030 | Empirical Risk Minimization for Stochastic Convex Optimization: $O(1/n)$- and $O(1/n^2)$-type of Risk Bounds | http://arxiv.org/abs/1702.02030 | id:1702.02030 author:Lijun Zhang, Tianbao Yang, Rong Jin category:cs.LG  published:2017-02-07 summary:Although there exist plentiful theories of empirical risk minimization (ERM) for supervised learning, current theoretical understandings of ERM for a related problem---stochastic convex optimization (SCO), are limited. In this work, we strengthen the realm of ERM for SCO by exploiting smoothness and strong convexity conditions to improve the risk bounds. First, we establish an $\widetilde{O}(d/n + \sqrt{F_*/n})$ risk bound when the random function is nonnegative, convex and smooth, and the expected function is Lipschitz continuous, where $d$ is the dimensionality of the problem, $n$ is the number of samples, and $F_*$ is the minimal risk. Thus, when $F_*$ is small we obtain an $\widetilde{O}(d/n)$ risk bound, which is analogous to the $\widetilde{O}(1/n)$ optimistic rate of ERM for supervised learning. Second, if the objective function is also $\lambda$-strongly convex, we prove an $\widetilde{O}(d/n + \kappa F_*/n )$ risk bound where $\kappa$ is the condition number, and improve it to $O(1/[\lambda n^2] + \kappa F_*/n)$ when $n=\widetilde{\Omega}(\kappa d)$. As a result, we obtain an $O(\kappa/n^2)$ risk bound under the condition that $n$ is large and $F_*$ is small, which to the best of our knowledge, is the first $O(1/n^2)$-type of risk bound of ERM. Third, we stress that the above results are established in a unified framework, which allows us to derive new risk bounds under weaker conditions, e.g., without convexity of the random function and Lipschitz continuity of the expected function. Finally, we demonstrate that to achieve an $O(1/[\lambda n^2] + \kappa F_*/n)$ risk bound for supervised learning, the $\widetilde{\Omega}(\kappa d)$ requirement on $n$ can be replaced with $\Omega(\kappa^2)$, which is dimensionality-independent. version:1
arxiv-1702-02025 | Efficient fetal-maternal ECG signal separation from two channel maternal abdominal ECG via diffusion-based channel selection | http://arxiv.org/abs/1702.02025 | id:1702.02025 author:Ruilin Li, Martin G. Frasch, Hau-tieng Wu category:physics.med-ph physics.data-an stat.AP stat.ML  published:2017-02-07 summary:There is a need for affordable, widely deployable maternal-fetal ECG monitors to improve maternal and fetal health during pregnancy and delivery. Based on the diffusion-based channel selection, here we present the mathematical formalism and clinical validation of an algorithm capable of accurate separation of maternal and fetal ECG from a two channel signal acquired over maternal abdomen. version:1
arxiv-1702-02012 | Tracking using Numerous Anchor points | http://arxiv.org/abs/1702.02012 | id:1702.02012 author:Tanushri Chakravorty, Guillaume-Alexandre Bilodeau, Eric Granger category:cs.CV  published:2017-02-07 summary:In this paper, an online adaptive model-free tracker is proposed to track single objects in video sequences to deal with real-world tracking challenges like low-resolution, object deformation, occlusion and motion blur. The novelty lies in the construction of a strong appearance model that captures features from the initialized bounding box and then are assembled into anchor-point features. These features memorize the global pattern of the object and have an internal star graph-like structure. These features are unique and flexible and helps tracking generic and deformable objects with no limitation on specific objects. In addition, the relevance of each feature is evaluated online using short-term consistency and long-term consistency. These parameters are adapted to retain consistent features that vote for the object location and that deal with outliers for long-term tracking scenarios. Additionally, voting in a Gaussian manner helps in tackling inherent noise of the tracking system and in accurate object localization. Furthermore, the proposed tracker uses pairwise distance measure to cope with scale variations and combines pixel-level binary features and global weighted color features for model update. Finally, experimental results on a visual tracking benchmark dataset are presented to demonstrate the effectiveness and competitiveness of the proposed tracker. version:1
arxiv-1702-01997 | Truncated Variational EM for Semi-Supervised Neural Simpletrons | http://arxiv.org/abs/1702.01997 | id:1702.01997 author:Dennis Forster, Jörg Lücke category:stat.ML cs.LG  published:2017-02-07 summary:Inference and learning for probabilistic generative networks is often very challenging and typically prevents scalability to as large networks as used for deep discriminative approaches. To obtain efficiently trainable, large-scale and well performing generative networks for semi-supervised learning, we here combine two recent developments: a neural network reformulation of hierarchical Poisson mixtures (Neural Simpletrons), and a novel truncated variational EM approach (TV-EM). TV-EM provides theoretical guarantees for learning in generative networks, and its application to Neural Simpletrons results in particularly compact, yet approximately optimal, modifications of learning equations. If applied to standard benchmarks, we empirically find, that learning converges in fewer EM iterations, that the complexity per EM iteration is reduced, and that final likelihood values are higher on average. For the task of classification on data sets with few labels, learning improvements result in consistently lower error rates if compared to applications without truncation. Experiments on the MNIST data set herein allow for comparison to standard and state-of-the-art models in the semi-supervised setting. Further experiments on the NIST SD19 data set show the scalability of the approach when a manifold of additional unlabeled data is available. version:1
arxiv-1702-01992 | Gated Multimodal Units for Information Fusion | http://arxiv.org/abs/1702.01992 | id:1702.01992 author:John Arevalo, Thamar Solorio, Manuel Montes-y-Gómez, Fabio A. González category:stat.ML cs.LG  published:2017-02-07 summary:This paper presents a novel model for multimodal learning based on gated neural networks. The Gated Multimodal Unit (GMU) model is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. It was evaluated on a multilabel scenario for genre classification of movies using the plot and the poster. The GMU improved the macro f-score performance of single-modality approaches and outperformed other fusion strategies, including mixture of experts models. Along with this work, the MM-IMDb dataset is released which, to the best of our knowledge, is the largest publicly available multimodal dataset for genre prediction on movies. version:1
arxiv-1702-01991 | Representations of language in a model of visually grounded speech signal | http://arxiv.org/abs/1702.01991 | id:1702.01991 author:Grzegorz Chrupała, Lieke Gelderloos, Afra Alishahi category:cs.CL cs.AI cs.LG  published:2017-02-07 summary:We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease. version:1
arxiv-1702-01983 | Face Aging With Conditional Generative Adversarial Networks | http://arxiv.org/abs/1702.01983 | id:1702.01983 author:Grigory Antipov, Moez Baccouche, Jean-Luc Dugelay category:cs.CV  published:2017-02-07 summary:It has been recently shown that Generative Adversarial Networks (GANs) can produce synthetic images of exceptional visual fidelity. In this work, we propose the GAN-based method for automatic face aging. Contrary to previous works employing GANs for altering of facial attributes, we make a particular emphasize on preserving the original person's identity in the aged version of his/her face. To this end, we introduce a novel approach for "Identity-Preserving" optimization of GAN's latent vectors. The objective evaluation of the resulting aged and rejuvenated face images by the state-of-the-art face recognition and age estimation solutions demonstrate the high potential of the proposed method. version:1
arxiv-1506-08621 | A spectral method for community detection in moderately-sparse degree-corrected stochastic block models | http://arxiv.org/abs/1506.08621 | id:1506.08621 author:Lennart Gulikers, Marc Lelarge, Laurent Massoulié category:math.PR cs.LG cs.SI stat.ML  published:2015-06-29 summary:We consider community detection in Degree-Corrected Stochastic Block Models (DC-SBM). We propose a spectral clustering algorithm based on a suitably normalized adjacency matrix. We show that this algorithm consistently recovers the block-membership of all but a vanishing fraction of nodes, in the regime where the lowest degree is of order log$(n)$ or higher. Recovery succeeds even for very heterogeneous degree-distributions. The used algorithm does not rely on parameters as input. In particular, it does not need to know the number of communities. version:3
arxiv-1702-01970 | Image Reconstruction using Matched Wavelet Estimated from Data Sensed Compressively using Partial Canonical Identity Matrix | http://arxiv.org/abs/1702.01970 | id:1702.01970 author:Naushad Ansari, Anubha Gupta category:cs.CV  published:2017-02-07 summary:This paper proposes a joint framework wherein lifting-based, separable, image-matched wavelets are estimated from compressively sensed (CS) images and used for the reconstruction of the same. Matched wavelet can be easily designed if full image is available. Also matched wavelet may provide better reconstruction results in CS application compared to standard wavelet sparsifying basis. Since in CS application, we have compressively sensed image instead of full image, existing methods of designing matched wavelet cannot be used. Thus, we propose a joint framework that estimates matched wavelet from the compressively sensed images and also reconstructs full images. This paper has three significant contributions. First, lifting-based, image-matched separable wavelet is designed from compressively sensed images and is also used to reconstruct the same. Second, a simple sensing matrix is employed to sample data at sub-Nyquist rate such that sensing and reconstruction time is reduced considerably without any noticeable degradation in the reconstruction performance. Third, a new multi-level L-Pyramid wavelet decomposition strategy is provided for separable wavelet implementation on images that leads to improved reconstruction performance. Compared to CS-based reconstruction using standard wavelets with Gaussian sensing matrix and with existing wavelet decomposition strategy, the proposed methodology provides faster and better image reconstruction in compressive sensing application. version:1
arxiv-1702-01961 | A Region Based Easy Path Wavelet Transform For Sparse Image Representation | http://arxiv.org/abs/1702.01961 | id:1702.01961 author:Renato Budinich category:cs.IT cs.CV cs.GR math.IT  published:2017-02-07 summary:The Easy Path Wavelet Transform is an adaptive transform for bivariate functions (in particular natural images) which has been proposed in [1]. It provides a sparse representation by finding a path in the domain of the function leveraging the local correlations of the function values. It then applies a one dimensional wavelet transform to the obtained vector, decimates the points and iterates the procedure. The main drawback of such method is the need to store, for each level of the transform, the path which vectorizes the two dimensional data. Here we propose a variation on the method which consists of firstly applying a segmentation procedure to the function domain, partitioning it into regions where the variation in the function values is low; in a second step, inside each such region, a path is found in some deterministic way, i.e. not data-dependent. This circumvents the need to store the paths at each level, while still obtaining good quality lossy compression. This method is particularly well suited to encode a Region of Interest in the image with different quality than the rest of the image. [1] Gerlind Plonka. The easy path wavelet transform: A new adaptive wavelet transform for sparse representation of two-dimensional data. Multiscale Modeling & Simulation, 7(3):1474$-$1496, 2008. version:1
arxiv-1702-01948 | Continuous-Time User Modeling in the Presence of Badges: A Probabilistic Approach | http://arxiv.org/abs/1702.01948 | id:1702.01948 author:Ali Khodadadi, Seyed Abbas Hosseini, Erfan Tavakoli, Hamid R. Rabiee category:cs.SI cs.LG  published:2017-02-07 summary:User modeling plays an important role in delivering customized web services to the users and improving their engagement. However, most user models in the literature do not explicitly consider the temporal behavior of users. More recently, continuous-time user modeling has gained considerable attention and many user behavior models have been proposed based on temporal point processes. However, typical point process based models often considered the impact of peer influence and content on the user participation and neglected other factors. Gamification elements, are among those factors that are neglected, while they have a strong impact on user participation in online services. In this paper, we propose interdependent multi-dimensional temporal point processes that capture the impact of badges on user participation besides the peer influence and content factors. We extend the proposed processes to model user actions over the community based question and answering websites, and propose an inference algorithm based on Variational-EM that can efficiently learn the model parameters. Extensive experiments on both synthetic and real data gathered from Stack Overflow show that our inference algorithm learns the parameters efficiently and the proposed method can better predict the user behavior compared to the alternatives. version:1
arxiv-1702-01944 | EliXa: A Modular and Flexible ABSA Platform | http://arxiv.org/abs/1702.01944 | id:1702.01944 author:Iñaki San Vicente, Xabier Saralegi, Rodrigo Agerri category:cs.CL I.2.7; H.3.1  published:2017-02-07 summary:This paper presents a supervised Aspect Based Sentiment Analysis (ABSA) system. Our aim is to develop a modular platform which allows to easily conduct experiments by replacing the modules or adding new features. We obtain the best result in the Opinion Target Extraction (OTE) task (slot 2) using an off-the-shelf sequence labeler. The target polarity classification (slot 3) is addressed by means of a multiclass SVM algorithm which includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80. version:1
arxiv-1702-01935 | Sparse Algorithm for Robust LSSVM in Primal Space | http://arxiv.org/abs/1702.01935 | id:1702.01935 author:Li Chen, Shuisheng Zhou category:cs.LG stat.ML  published:2017-02-07 summary:As enjoying the closed form solution, least squares support vector machine (LSSVM) has been widely used for classification and regression problems having the comparable performance with other types of SVMs. However, LSSVM has two drawbacks: sensitive to outliers and lacking sparseness. Robust LSSVM (R-LSSVM) overcomes the first partly via nonconvex truncated loss function, but the current algorithms for R-LSSVM with the dense solution are faced with the second drawback and are inefficient for training large-scale problems. In this paper, we interpret the robustness of R-LSSVM from a re-weighted viewpoint and give a primal R-LSSVM by the representer theorem. The new model may have sparse solution if the corresponding kernel matrix has low rank. Then approximating the kernel matrix by a low-rank matrix and smoothing the loss function by entropy penalty function, we propose a convergent sparse R-LSSVM (SR-LSSVM) algorithm to achieve the sparse solution of primal R-LSSVM, which overcomes two drawbacks of LSSVM simultaneously. The proposed algorithm has lower complexity than the existing algorithms and is very efficient for training large-scale problems. Many experimental results illustrate that SR-LSSVM can achieve better or comparable performance with less training time than related algorithms, especially for training large scale problems. version:1
arxiv-1702-01933 | Hashing in the Zero Shot Framework via Domain Adaptation | http://arxiv.org/abs/1702.01933 | id:1702.01933 author:Shubham Pachori, Ameya Deshpande, Shanmuganathan Raman category:cs.CV  published:2017-02-07 summary:Techniques to learn hash codes which can store and retrieve large dimensional multimedia data efficiently have attracted broad research interests in the recent years. With rapid explosion of newly emerged concepts and online data, existing supervised hashing algorithms suffer from the problem of scarcity of ground truth annotations due to the high cost of obtaining manual annotations. Therefore, we propose an algorithm to learn a hash function from training images belonging to seen classes which can efficiently encode images of unseen classes to binary codes. Specifically, we project the image features from visual space and semantic features from semantic space into a common Hamming subspace. Earlier works to generate hash codes have tried to relax the discrete constraints on hash codes and solve the continuous optimization problem. However, it often leads to quantization errors. In this work, we use the max-margin classifier to learn an efficient hash function. To address the concern of domain-shift which may arise due to the introduction of new classes, we also introduce an unsupervised domain adaptation model in the proposed hashing framework. Results on the three image datasets show the advantage of using domain adaptation in learning a high-quality hash function and superiority of our method for the task of image retrieval performance as compared to several state-of-the-art hashing methods. version:1
arxiv-1702-01932 | A Knowledge-Grounded Neural Conversation Model | http://arxiv.org/abs/1702.01932 | id:1702.01932 author:Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, Michel Galley category:cs.CL  published:2017-02-07 summary:Neural network models are capable of generating extremely natural sounding conversational interactions. Nevertheless, these models have yet to demonstrate that they can incorporate content in the form of factual information or entity-grounded opinion that would enable them to serve in more task-oriented conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. We generalize the widely-used Seq2Seq approach by conditioning responses on both conversation history and external "facts", allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive Seq2Seq baseline. Human judges found that our outputs are significantly more informative. version:1
arxiv-1702-01925 | Effects of Stop Words Elimination for Arabic Information Retrieval: A Comparative Study | http://arxiv.org/abs/1702.01925 | id:1702.01925 author:Ibrahim Abu El-Khair category:cs.CL cs.IR  published:2017-02-07 summary:The effectiveness of three stop words lists for Arabic Information Retrieval---General Stoplist, Corpus-Based Stoplist, Combined Stoplist ---were investigated in this study. Three popular weighting schemes were examined: the inverse document frequency weight, probabilistic weighting, and statistical language modelling. The Idea is to combine the statistical approaches with linguistic approaches to reach an optimal performance, and compare their effect on retrieval. The LDC (Linguistic Data Consortium) Arabic Newswire data set was used with the Lemur Toolkit. The Best Match weighting scheme used in the Okapi retrieval system had the best overall performance of the three weighting algorithms used in the study, stoplists improved retrieval effectiveness especially when used with the BM25 weight. The overall performance of a general stoplist was better than the other two lists. version:1
arxiv-1702-01923 | Comparative Study of CNN and RNN for Natural Language Processing | http://arxiv.org/abs/1702.01923 | id:1702.01923 author:Wenpeng Yin, Katharina Kann, Mo Yu, Hinrich Schütze category:cs.CL  published:2017-02-07 summary:Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection. version:1
arxiv-1701-06333 | Normative theory of visual receptive fields | http://arxiv.org/abs/1701.06333 | id:1701.06333 author:Tony Lindeberg category:q-bio.NC cs.CV  published:2017-01-23 summary:This article gives an overview of a normative computational theory of visual receptive fields, by which idealized functional models of early spatial, spatio-chromatic and spatio-temporal receptive fields can be derived in an axiomatic way based on structural properties of the environment in combination with assumptions about the internal structure of a vision system to guarantee consistent handling of image representations over multiple spatial and temporal scales. Interestingly, this theory leads to predictions about visual receptive field shapes with qualitatively very good similarity to biological receptive fields measured in the retina, the LGN and the primary visual cortex (V1) of mammals. version:3
arxiv-1702-00458 | Electron-Proton Dynamics in Deep Learning | http://arxiv.org/abs/1702.00458 | id:1702.00458 author:Qiuyi Zhang, Rina Panigrahy, Sushant Sachdeva category:cs.DS cs.LG physics.data-an  published:2017-02-01 summary:We study the efficacy of learning neural networks with neural networks by the (stochastic) gradient descent method. While gradient descent enjoys empirical success in a variety of applications, there is a lack of theoretical guarantees that explains the practical utility of deep learning. We focus on two-layer neural networks with a linear activation on the output node. We show that under some mild assumptions and certain classes of activation functions, gradient descent does learn the parameters of the neural network and converges to the global minima. Using a node-wise gradient descent algorithm, we show that learning can be done in finite, sometimes $poly(d,1/\epsilon)$, time and sample complexity. version:2
arxiv-1702-01870 | A New Point-set Registration Algorithm for Fingerprint Matching | http://arxiv.org/abs/1702.01870 | id:1702.01870 author:A. Pasha Hosseinbor, Renat Zhdanov, Alexander Ushveridze category:cs.CV  published:2017-02-07 summary:A novel minutia-based fingerprint matching algorithm is proposed that employs iterative global alignment on two minutia sets. The matcher considers all possible minutia pairings and iteratively aligns the two sets until the number of minutia pairs does not exceed the maximum number of allowable one-to-one pairings. The optimal alignment parameters are derived analytically via linear least squares. The first alignment establishes a region of overlap between the two minutia sets, which is then (iteratively) refined by each successive alignment. After each alignment, minutia pairs that exhibit weak correspondence are discarded. The process is repeated until the number of remaining pairs no longer exceeds the maximum number of allowable one-to-one pairings. The proposed algorithm is tested on both the FVC2000 and FVC2002 databases, and the results indicate that the proposed matcher is both effective and efficient for fingerprint authentication; it is fast and does not utilize any computationally expensive mathematical functions (e.g. trigonometric, exponential). In addition to the proposed matcher, another contribution of the paper is the analytical derivation of the least squares solution for the optimal alignment parameters for two point-sets lacking exact correspondence. version:1
arxiv-1701-08655 | Structural Analysis of Hindi Phonetics and A Method for Extraction of Phonetically Rich Sentences from a Very Large Hindi Text Corpus | http://arxiv.org/abs/1701.08655 | id:1701.08655 author:Shrikant Malviya, Rohit Mishra, Uma Shanker Tiwary category:cs.CL  published:2017-01-30 summary:Automatic speech recognition (ASR) and Text to speech (TTS) are two prominent area of research in human computer interaction nowadays. A set of phonetically rich sentences is in a matter of importance in order to develop these two interactive modules of HCI. Essentially, the set of phonetically rich sentences has to cover all possible phone units distributed uniformly. Selecting such a set from a big corpus with maintaining phonetic characteristic based similarity is still a challenging problem. The major objective of this paper is to devise a criteria in order to select a set of sentences encompassing all phonetic aspects of a corpus with size as minimum as possible. First, this paper presents a statistical analysis of Hindi phonetics by observing the structural characteristics. Further a two stage algorithm is proposed to extract phonetically rich sentences with a high variety of triphones from the EMILLE Hindi corpus. The algorithm consists of a distance measuring criteria to select a sentence in order to improve the triphone distribution. Moreover, a special preprocessing method is proposed to score each triphone in terms of inverse probability in order to fasten the algorithm. The results show that the approach efficiently build uniformly distributed phonetically-rich corpus with optimum number of sentences. version:2
arxiv-1702-01847 | Low Rank Matrix Recovery with Simultaneous Presence of Outliers and Sparse Corruption | http://arxiv.org/abs/1702.01847 | id:1702.01847 author:Mostafa Rahmani, George Atia category:stat.ML cs.CV cs.LG  published:2017-02-07 summary:We study a data model in which the data matrix D can be expressed as D = L + S + C, where L is a low rank matrix, S an element-wise sparse matrix and C a matrix whose non-zero columns are outlying data points. To date, robust PCA algorithms have solely considered models with either S or C, but not both. As such, existing algorithms cannot account for simultaneous element-wise and column-wise corruptions. In this paper, a new robust PCA algorithm that is robust to simultaneous types of corruption is proposed. Our approach hinges on the sparse approximation of a sparsely corrupted column so that the sparse expansion of a column with respect to the other data points is used to distinguish a sparsely corrupted inlier column from an outlying data point. We also develop a randomized design which provides a scalable implementation of the proposed approach. The core idea of sparse approximation is analyzed analytically where we show that the underlying ell_1-norm minimization can obtain the representation of an inlier in presence of sparse corruptions. version:1
arxiv-1702-01846 | Development of JavaScript-based deep learning platform and application to distributed training | http://arxiv.org/abs/1702.01846 | id:1702.01846 author:Masatoshi Hidaka, Ken Miura, Tatsuya Harada category:cs.CV cs.DC  published:2017-02-07 summary:Deep learning is increasingly attracting attention for processing big data. Existing frameworks for deep learning must be set up to specialized computer systems. Gaining sufficient computing resources therefore entails high costs of deployment and maintenance. In this work, we implement a matrix library and deep learning framework that uses JavaScript. It can run on web browsers operating on ordinary personal computers and smartphones. Using JavaScript, deep learning can be accomplished in widely diverse environments without the necessity for software installation. Using GPGPU from WebCL framework, our framework can train large scale convolutional neural networks such as VGGNet and ResNet. In the experiments, we demonstrate their practicality by training VGGNet in a distributed manner using web browsers as the client. version:1
arxiv-1702-01841 | The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task | http://arxiv.org/abs/1702.01841 | id:1702.01841 author:Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila Zilles, Yejin Choi, Noah A. Smith category:cs.CL  published:2017-02-07 summary:A writer's style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed with stylistic features is able to successfully distinguish between the three cases, without even looking at the story context. In addition, our style-based classifier establishes a new state-of-the-art result on the story cloze challenge, substantially higher than previous results based on deep learning models. Our results demonstrate that different task framings can dramatically affect the way people write. version:1
arxiv-1701-08431 | Source localization in an ocean waveguide using supervised machine learning | http://arxiv.org/abs/1701.08431 | id:1701.08431 author:Haiqiang Niu, Peter Gerstoft, Emma Reeves category:physics.ao-ph cs.NE physics.geo-ph  published:2017-01-29 summary:Source localization is solved as a classification problem by training a feed-forward neural network (FNN) on ocean acoustic data. The pressure received by a vertical linear array is preprocessed by constructing a normalized sample covariance matrix (SCM), which is used as input for the FNN. Each neuron of the output layer represents a discrete source range. FNN is a data-driven method that learns features directly from observed acoustic data, unlike model-based localization methods such as matched-field processing that require accurate sound propagation modeling. The FNN achieves a good performance (the mean absolute percentage error below 10\%) for predicting source ranges for vertical array data from the Noise09 experiment. The effects of varying the parameters of the method, such as number of hidden neurons and layers, number of output neurons and number of snapshots in each input sample are discussed. version:2
arxiv-1702-01831 | Trimming the Independent Fat: Sufficient Statistics, Mutual Information, and Predictability from Effective Channel States | http://arxiv.org/abs/1702.01831 | id:1702.01831 author:Ryan G. James, John R. Mahoney, James P. Crutchfield category:cond-mat.stat-mech cs.IT math.IT nlin.CD stat.ML  published:2017-02-07 summary:One of the most fundamental questions one can ask about a pair of random variables X and Y is the value of their mutual information. Unfortunately, this task is often stymied by the extremely large dimension of the variables. We might hope to replace each variable by a lower-dimensional representation that preserves the relationship with the other variable. The theoretically ideal implementation is the use of minimal sufficient statistics, where it is well-known that either X or Y can be replaced by their minimal sufficient statistic about the other while preserving the mutual information. While intuitively reasonable, it is not obvious or straightforward that both variables can be replaced simultaneously. We demonstrate that this is in fact possible: the information X's minimal sufficient statistic preserves about Y is exactly the information that Y's minimal sufficient statistic preserves about X. As an important corollary, we consider the case where one variable is a stochastic process' past and the other its future and the present is viewed as a memoryful channel. In this case, the mutual information is the channel transmission rate between the channel's effective states. That is, the past-future mutual information (the excess entropy) is the amount of information about the future that can be predicted using the past. Translating our result about minimal sufficient statistics, this is equivalent to the mutual information between the forward- and reverse-time causal states of computational mechanics. We close by discussing multivariate extensions to this use of minimal sufficient statistics. version:1
arxiv-1702-01829 | Neural Discourse Structure for Text Categorization | http://arxiv.org/abs/1702.01829 | id:1702.01829 author:Yangfeng Ji, Noah Smith category:cs.CL cs.LG  published:2017-02-07 summary:We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses. version:1
arxiv-1702-01825 | Deep Learning Models of the Retinal Response to Natural Scenes | http://arxiv.org/abs/1702.01825 | id:1702.01825 author:Lane T. McIntosh, Niru Maheswaranathan, Aran Nayebi, Surya Ganguli, Stephen A. Baccus category:q-bio.NC stat.ML  published:2017-02-06 summary:A central challenge in neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli. Here we demonstrate that deep convolutional neural networks (CNNs) capture retinal responses to natural scenes nearly to within the variability of a cell's response, and are markedly more accurate than linear-nonlinear (LN) models and Generalized Linear Models (GLMs). Moreover, we find two additional surprising properties of CNNs: they are less susceptible to overfitting than their LN counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g. between natural scenes and white noise). Examination of trained CNNs reveals several properties. First, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise. Second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms. Third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-Poisson spiking variability observed in retinal ganglion cells. Fourth, augmenting our CNNs with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes. These methods can be readily generalized to other sensory modalities and stimulus ensembles. Overall, this work demonstrates that CNNs not only accurately capture sensory circuit responses to natural scenes, but also yield information about the circuit's internal structure and function. version:1
arxiv-1702-01824 | Learning similarity preserving representations with neural similarity encoders | http://arxiv.org/abs/1702.01824 | id:1702.01824 author:Franziska Horn, Klaus-Robert Müller category:stat.ML cs.LG  published:2017-02-06 summary:Many dimensionality reduction or manifold learning algorithms optimize for retaining the pairwise similarities, distances, or local neighborhoods of data points. Spectral methods like Kernel PCA (kPCA) or isomap achieve this by computing the singular value decomposition (SVD) of some similarity matrix to obtain a low dimensional representation of the original data. However, this is computationally expensive if a lot of training examples are available and, additionally, representations for new (out-of-sample) data points can only be created when the similarities to the original training examples can be computed. We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model optimizes the same objective as kPCA but in the process it learns a linear or non-linear embedding function (in the form of the tuned neural network), with which the representations of novel data points can be computed - even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. By creating embeddings for both image and text datasets, we demonstrate that SimEc can, on the one hand, reach the same solution as spectral methods, and, on the other hand, obtain meaningful embeddings from similarities based on human labels. version:1
arxiv-1702-01816 | Prediction of Kidney Function from Biopsy Images Using Convolutional Neural Networks | http://arxiv.org/abs/1702.01816 | id:1702.01816 author:David Ledbetter, Long Ho, Kevin V Lemley category:stat.ML q-bio.QM  published:2017-02-06 summary:A Convolutional Neural Network was used to predict kidney function in patients with chronic kidney disease from high-resolution digital pathology scans of their kidney biopsies. Kidney biopsies were taken from participants of the NEPTUNE study, a longitudinal cohort study whose goal is to set up infrastructure for observing the evolution of 3 forms of idiopathic nephrotic syndrome, including developing predictors for progression of kidney disease. The knowledge of future kidney function is desirable as it can identify high-risk patients and influence treatment decisions, reducing the likelihood of irreversible kidney decline. version:1
arxiv-1702-01815 | Living a discrete life in a continuous world: Reference with distributed representations | http://arxiv.org/abs/1702.01815 | id:1702.01815 author:Gemma Boleda, Sebastian Padó, Nghia The Pham, Marco Baroni category:cs.CL  published:2017-02-06 summary:Reference is the crucial property of language that allows us to connect linguistic expressions to the world. Modeling it requires handling both continuous and discrete aspects of meaning. Data-driven models excel at the former, but struggle with the latter, and the reverse is true for symbolic models. We propose a fully data-driven, end-to-end trainable model that, while operating on continuous multimodal representations, learns to organize them into a discrete-like entity library. We also introduce a referential task to test it, cross-modal tracking. Our model beats standard neural network architectures, but is outperformed by some parametrizations of Memory Networks, another model with external memory. version:1
arxiv-1702-01811 | Hierarchical Symbolic Dynamic Filtering of Streaming Non-stationary Time Series Data | http://arxiv.org/abs/1702.01811 | id:1702.01811 author:Adedotun Akintayo, Soumik Sarkar category:stat.ML 6006  published:2017-02-06 summary:This paper proposes a hierarchical feature extractor for non-stationary streaming time series based on the concept of switching observable Markov chain models. The slow time-scale non-stationary behaviors are considered to be a mixture of quasi-stationary fast time-scale segments that are exhibited by complex dynamical systems. The idea is to model each unique stationary characteristic without a priori knowledge (e.g., number of possible unique characteristics) at a lower logical level, and capture the transitions from one low-level model to another at a higher level. In this context, the concepts in the recently developed Symbolic Dynamic Filtering (SDF) is extended, to build an online algorithm suited for handling quasi-stationary data at a lower level and a non-stationary behavior at a higher level without a priori knowledge. A key observation made in this study is that the rate of change of data likelihood seems to be a better indicator of change in data characteristics compared to the traditional methods that mostly consider data likelihood for change detection. The algorithm minimizes model complexity and captures data likelihood. Efficacy demonstration and comparative evaluation of the proposed algorithm are performed using time series data simulated from systems that exhibit nonlinear dynamics. We discuss results that show that the proposed hierarchical SDF algorithm can identify underlying features with significantly high degree of accuracy, even under very noisy conditions. Algorithm is demonstrated to perform better than the baseline Hierarchical Dirichlet Process-Hidden Markov Models (HDP-HMM). The low computational complexity of algorithm makes it suitable for on-board, real time operations. version:1
arxiv-1702-01806 | Beam Search Strategies for Neural Machine Translation | http://arxiv.org/abs/1702.01806 | id:1702.01806 author:Markus Freitag, Yaser Al-Onaizan category:cs.CL  published:2017-02-06 summary:The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to- right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the draw- back of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43% for the two language pairs German-English and Chinese-English without losing any translation quality. version:1
arxiv-1702-01802 | Ensemble Distillation for Neural Machine Translation | http://arxiv.org/abs/1702.01802 | id:1702.01802 author:Markus Freitag, Yaser Al-Onaizan, Baskaran Sankaran category:cs.CL  published:2017-02-06 summary:Knowledge distillation describes a method for training a student network to perform better by learning from a stronger teacher network. In this work, we run experiments with different kinds of teacher net- works to enhance the translation performance of a student Neural Machine Translation (NMT) network. We demonstrate techniques based on an ensemble and a best BLEU teacher network. We also show how to benefit from a teacher network that has the same architecture and dimensions of the student network. Further- more, we introduce a data filtering technique based on the dissimilarity between the forward translation (obtained during knowledge distillation) of a given source sentence and its target reference. We use TER to measure dissimilarity. Finally, we show that an ensemble teacher model can significantly reduce the student model size while still getting performance improvements compared to the baseline student network. version:1
arxiv-1701-06084 | Linear-Complexity Exponentially-Consistent Tests for Universal Outlying Sequence Detection | http://arxiv.org/abs/1701.06084 | id:1701.06084 author:Yuheng Bu, Shaofeng Zou, Venugopal V. Veeravalli category:cs.IT math.IT stat.ML  published:2017-01-21 summary:We study a universal outlying sequence detection problem, in which there are $M$ sequences of samples out of which a small subset of outliers need to be detected. A sequence is considered as an outlier if the observations therein are generated by a distribution different from those generating the observations in the majority of the sequences. In the universal setting, the goal is to identify all the outliers without any knowledge about the underlying generating distributions. In prior work, this problem was studied as a universal hypothesis testing problem, and a generalized likelihood (GL) test was constructed and its asymptotic performance characterized. In this paper, we propose a different class of tests for this problem based on distribution clustering. Such tests are shown to be exponentially consistent and their time complexity is linear in the total number of sequences, in contrast with the GL test, which has time complexity that is exponential in the number of outliers. Furthermore, our tests based on clustering are applicable to more general scenarios. For example, when both the typical and outlier distributions form clusters, the clustering based test is exponentially consistent, but the GL test is not even applicable. version:3
arxiv-1702-01780 | Toward the automated analysis of complex diseases in genome-wide association studies using genetic programming | http://arxiv.org/abs/1702.01780 | id:1702.01780 author:Andrew Sohn, Randal S. Olson, Jason H. Moore category:cs.NE cs.LG q-bio.QM stat.ML  published:2017-02-06 summary:Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods. Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets. We demonstrate TPOT-MDR's capabilities using a combination of simulated and real world data sets from human genetics and find that TPOT-MDR significantly outperforms modern machine learning methods such as logistic regression and eXtreme Gradient Boosting (XGBoost). We further analyze the best pipeline discovered by TPOT-MDR for a real world problem and highlight TPOT-MDR's ability to produce a high-accuracy solution that is also easily interpretable. version:1
arxiv-1702-01776 | Multi-task Coupled Attentions for Category-specific Aspect and Opinion Terms Co-extraction | http://arxiv.org/abs/1702.01776 | id:1702.01776 author:Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier category:cs.CL  published:2017-02-06 summary:In aspect-based sentiment analysis, most existing methods either focus on aspect/opinion terms extraction or aspect terms categorization. However, each task by itself only provides partial information to end users. To generate more detailed and structured opinion analysis, we propose a finer-grained problem, which we call category-specific aspect and opinion terms extraction. This problem involves the identification of aspect and opinion terms within each sentence, as well as the categorization of the identified terms. To this end, we propose an end-to-end multi-task attention model, where each task corresponds to aspect/opinion terms extraction for a specific category. Our model benefits from exploring the commonalities and relationships among different tasks to address the data sparsity issue. We demonstrate its state-of-the-art performance on three benchmark datasets. version:1
arxiv-1702-01731 | A Deep Convolutional Neural Network for Background Subtraction | http://arxiv.org/abs/1702.01731 | id:1702.01731 author:Mohammadreza Babaee, Duc Tung Dinh, Gerhard Rigoll category:cs.CV  published:2017-02-06 summary:In this work, we present a novel background subtraction system that uses a deep Convolutional Neural Network (CNN) to perform the segmentation. With this approach, feature engineering and parameter tuning become unnecessary since the network parameters can be learned from data by training a single CNN that can handle various video scenes. Additionally, we propose a new approach to estimate background model from video. For the training of the CNN, we employed randomly 5 percent video frames and their ground truth segmentations taken from the Change Detection challenge 2014(CDnet 2014). We also utilized spatial-median filtering as the post-processing of the network outputs. Our method is evaluated with different data-sets, and the network outperforms the existing algorithms with respect to the average ranking over different evaluation metrics. Furthermore, due to the network architecture, our CNN is capable of real time processing. version:1
arxiv-1702-01721 | View Independent Vehicle Make, Model and Color Recognition Using Convolutional Neural Network | http://arxiv.org/abs/1702.01721 | id:1702.01721 author:Afshin Dehghan, Syed Zain Masood, Guang Shu, Enrique. G. Ortiz category:cs.CV cs.AI  published:2017-02-06 summary:This paper describes the details of Sighthound's fully automated vehicle make, model and color recognition system. The backbone of our system is a deep convolutional neural network that is not only computationally inexpensive, but also provides state-of-the-art results on several competitive benchmarks. Additionally, our deep network is trained on a large dataset of several million images which are labeled through a semi-automated process. Finally we test our system on several public datasets as well as our own internal test dataset. Our results show that we outperform other methods on all benchmarks by significant margins. Our model is available to developers through the Sighthound Cloud API at https://www.sighthound.com/products/cloud version:1
arxiv-1702-01717 | Search Intelligence: Deep Learning For Dominant Category Prediction | http://arxiv.org/abs/1702.01717 | id:1702.01717 author:Zeeshan Khawar Malik, Mo Kobrosli, Peter Maas category:cs.IR cs.LG stat.ML  published:2017-02-06 summary:Deep Neural Networks, and specifically fully-connected convolutional neural networks are achieving remarkable results across a wide variety of domains. They have been trained to achieve state-of-the-art performance when applied to problems such as speech recognition, image classification, natural language processing and bioinformatics. Most of these deep learning models when applied to classification employ the softmax activation function for prediction and aim to minimize cross-entropy loss. In this paper, we have proposed a supervised model for dominant category prediction to improve search recall across all eBay classifieds platforms. The dominant category label for each query in the last 90 days is first calculated by summing the total number of collaborative clicks among all categories. The category having the highest number of collaborative clicks for the given query will be considered its dominant category. Second, each query is transformed to a numeric vector by mapping each unique word in the query document to a unique integer value; all padded to equal length based on the maximum document length within the pre-defined vocabulary size. A fully-connected deep convolutional neural network (CNN) is then applied for classification. The proposed model achieves very high classification accuracy compared to other state-of-the-art machine learning techniques. version:1
arxiv-1702-01714 | DNN adaptation by automatic quality estimation of ASR hypotheses | http://arxiv.org/abs/1702.01714 | id:1702.01714 author:Daniele Falavigna, Marco Matassoni, Shahab Jalalvand, Matteo Negri, Marco Turchi category:cs.CL  published:2017-02-06 summary:In this paper we propose to exploit the automatic Quality Estimation (QE) of ASR hypotheses to perform the unsupervised adaptation of a deep neural network modeling acoustic probabilities. Our hypothesis is that significant improvements can be achieved by: i)automatically transcribing the evaluation data we are currently trying to recognise, and ii) selecting from it a subset of "good quality" instances based on the word error rate (WER) scores predicted by a QE component. To validate this hypothesis, we run several experiments on the evaluation data sets released for the CHiME-3 challenge. First, we operate in oracle conditions in which manual transcriptions of the evaluation data are available, thus allowing us to compute the "true" sentence WER. In this scenario, we perform the adaptation with variable amounts of data, which are characterised by different levels of quality. Then, we move to realistic conditions in which the manual transcriptions of the evaluation data are not available. In this case, the adaptation is performed on data selected according to the WER scores "predicted" by a QE component. Our results indicate that: i) QE predictions allow us to closely approximate the adaptation results obtained in oracle conditions, and ii) the overall ASR performance based on the proposed QE-driven adaptation method is significantly better than the strong, most recent, CHiME-3 baseline. version:1
arxiv-1702-01711 | Q-WordNet PPV: Simple, Robust and (almost) Unsupervised Generation of Polarity Lexicons for Multiple Languages | http://arxiv.org/abs/1702.01711 | id:1702.01711 author:Iñaki San Vicente, Rodrigo Agerri, German Rigau category:cs.CL  published:2017-02-06 summary:This paper presents a simple, robust and (almost) unsupervised dictionary-based method, qwn-ppv (Q-WordNet as Personalized PageRanking Vector) to automatically generate polarity lexicons. We show that qwn-ppv outperforms other automatically generated lexicons for the four extrinsic evaluations presented here. It also shows very competitive and robust results with respect to manually annotated ones. Results suggest that no single lexicon is best for every task and dataset and that the intrinsic evaluation of polarity lexicons is not a good performance indicator on a Sentiment Analysis task. The qwn-ppv method allows to easily create quality polarity lexicons whenever no domain-based annotated corpora are available for a given language. version:1
arxiv-1702-01692 | Distributed Evolutionary k-way Node Separators | http://arxiv.org/abs/1702.01692 | id:1702.01692 author:Peter Sanders, Christian Schulz, Darren Strash, Robert Williger category:cs.NE cs.DC cs.DS  published:2017-02-06 summary:Computing high quality node separators in large graphs is necessary for a variety of applications, ranging from divide-and-conquer algorithms to VLSI design. In this work, we present a novel distributed evolutionary algorithm tackling the k-way node separator problem. A key component of our contribution includes new k-way local search algorithms based on maximum flows. We combine our local search with a multilevel approach to compute an initial population for our evolutionary algorithm, and further show how to modify the coarsening stage of our multilevel algorithm to create effective combine and mutation operations. Lastly, we combine these techniques with a scalable communication protocol, producing a system that is able to compute high quality solutions in a short amount of time. Our experiments against competing algorithms show that our advanced evolutionary algorithm computes the best result on 94% of the chosen benchmark instances. version:1
arxiv-1702-01691 | Calibrating Energy-based Generative Adversarial Networks | http://arxiv.org/abs/1702.01691 | id:1702.01691 author:Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard Hovy, Aaron Courville category:cs.LG  published:2017-02-06 summary:In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.Specifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution. version:1
arxiv-1702-01638 | Concurrent Activity Recognition with Multimodal CNN-LSTM Structure | http://arxiv.org/abs/1702.01638 | id:1702.01638 author:Xinyu Li, Yanyi Zhang, Jianyu Zhang, Shuhong Chen, Ivan Marsic, Richard A. Farneth, Randall S. Burd category:cs.CV  published:2017-02-06 summary:We introduce a system that recognizes concurrent activities from real-world data captured by multiple sensors of different types. The recognition is achieved in two steps. First, we extract spatial and temporal features from the multimodal data. We feed each datatype into a convolutional neural network that extracts spatial features, followed by a long-short term memory network that extracts temporal information in the sensory data. The extracted features are then fused for decision making in the second step. Second, we achieve concurrent activity recognition with a single classifier that encodes a binary output vector in which elements indicate whether the corresponding activity types are currently in progress. We tested our system with three datasets from different domains recorded using different sensors and achieved performance comparable to existing systems designed specifically for those domains. Our system is the first to address the concurrent activity recognition with multisensory data using a single model, which is scalable, simple to train and easy to deploy. version:1
arxiv-1702-01636 | Slice-to-volume medical image registration: a survey | http://arxiv.org/abs/1702.01636 | id:1702.01636 author:Enzo Ferrante, Nikos Paragios category:cs.CV  published:2017-02-06 summary:During the last decades, the research community of medical imaging has witnessed continuous advances in image registration methods, which pushed the limits of the state-of-the-art and enabled the development of novel medical procedures. A particular type of image registration problem, known as slice-to-volume registration, played a fundamental role in areas like image guided surgeries and volumetric image reconstruction. However, to date, and despite the extensive literature available on this topic, no survey has been written to discuss this challenging problem. This paper introduces the first comprehensive survey of the literature about slice-to-volume registration, presenting a categorical study of the algorithms according to an ad-hoc taxonomy and analyzing advantages and disadvantages of every category. We draw some general conclusions from this analysis and present our perspectives on the future of the field. version:1
arxiv-1702-01618 | Learning of state-space models with highly informative observations: a tempered Sequential Monte Carlo solution | http://arxiv.org/abs/1702.01618 | id:1702.01618 author:Andreas Svensson, Thomas B. Schön, Fredrik Lindsten category:stat.CO stat.ML  published:2017-02-06 summary:Probabilistic (or Bayesian) modeling and learning offers interesting possibilities for systematic representation of uncertainty based on probability theory. Recent advances in Monte Carlo based methods have made previously intractable problem possible to solve using only the computational power available in a standard personal computer. For probabilistic learning of unknown parameters in nonlinear state-space models, methods based on the particle filter have proven useful. However, a notoriously challenging problem occurs when the observations are highly informative, i.e. when there is very little or no measurement noise present. The particle filter will then struggle in estimating one of the basic component in most parameter learning algorithms, the likelihood p(data parameters). To this end we suggest an algorithm which initially assumes that there is artificial measurement noise present. The variance of this noise is sequentially decreased in an adaptive fashion such that we in the end recover the original problem or possibly a very close approximation of it. Computationally the parameters are learned using a sequential Monte Carlo (SMC) sampler, which gives our proposed method a clear resemblance to the SMC^2 method. Another natural link is also made to the ideas underlying the so-called approximate Bayesian computation (ABC). We provide a theoretical justification (implying convergence results) for the suggested approach. We also illustrate it with numerical examples, and in particular show promising results for a challenging Wiener-Hammerstein benchmark. version:1
arxiv-1702-01587 | A Hybrid Approach For Hindi-English Machine Translation | http://arxiv.org/abs/1702.01587 | id:1702.01587 author:Omkar Dhariya, Shrikant Malviya, Uma Shanker Tiwary category:cs.CL  published:2017-02-06 summary:In this paper, an extended combined approach of phrase based statistical machine translation (SMT), example based MT (EBMT) and rule based MT (RBMT) is proposed to develop a novel hybrid data driven MT system capable of outperforming the baseline SMT, EBMT and RBMT systems from which it is derived. In short, the proposed hybrid MT process is guided by the rule based MT after getting a set of partial candidate translations provided by EBMT and SMT subsystems. Previous works have shown that EBMT systems are capable of outperforming the phrase-based SMT systems and RBMT approach has the strength of generating structurally and morphologically more accurate results. This hybrid approach increases the fluency, accuracy and grammatical precision which improve the quality of a machine translation system. A comparison of the proposed hybrid machine translation (HTM) model with renowned translators i.e. Google, BING and Babylonian is also presented which shows that the proposed model works better on sentences with ambiguity as well as comprised of idioms than others. version:1
arxiv-1702-01569 | Neural Semantic Parsing over Multiple Knowledge-bases | http://arxiv.org/abs/1702.01569 | id:1702.01569 author:Jonathan Herzig, Jonathan Berant category:cs.CL  published:2017-02-06 summary:A fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language utterances annotated with logical form. In this paper, we propose to exploit structural regularities in language in different domains, and train semantic parsers over multiple knowledge-bases (KBs), while sharing information across datasets. We find that we can substantially improve parsing accuracy by training a single sequence-to-sequence model over multiple KBs, when providing an encoding of the domain at decoding time. Our model achieves state-of-the-art performance on the Overnight dataset (containing eight domains), improves performance over a single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the number of model parameters. version:1
arxiv-1702-01528 | Textually Customized Video Summaries | http://arxiv.org/abs/1702.01528 | id:1702.01528 author:Jinsoo Choi, Tae-Hyun Oh, In So Kweon category:cs.CV cs.MM  published:2017-02-06 summary:The best summary of a long video differs among different people due to its highly subjective nature. Even for the same person, the best summary may change with time or mood. In this paper, we introduce the task of generating customized video summaries through simple text. First, we train a deep architecture to effectively learn semantic embeddings of video frames by leveraging the abundance of image-caption data via a progressive and residual manner. Given a user-specific text description, our algorithm is able to select semantically relevant video segments and produce a temporally aligned video summary. In order to evaluate our textually customized video summaries, we conduct experimental comparison with baseline methods that utilize ground-truth information. Despite the challenging baselines, our method still manages to show comparable or even exceeding performance. We also show that our method is able to generate semantically diverse video summaries by only utilizing the learned visual embeddings. version:1
arxiv-1702-01517 | Opinion Recommendation using Neural Memory Model | http://arxiv.org/abs/1702.01517 | id:1702.01517 author:Zhongqing Wang, Yue Zhang category:cs.CL  published:2017-02-06 summary:We present opinion recommendation, a novel task of jointly predicting a custom review with a rating score that a certain user would give to a certain product or service, given existing reviews and rating scores to the product or service by other users, and the reviews that the user has given to other products and services. A characteristic of opinion recommendation is the reliance of multiple data sources for multi-task joint learning, which is the strength of neural models. We use a single neural network to model users and products, capturing their correlation and generating customised product representations using a deep memory network, from which customised ratings and reviews are constructed jointly. Results show that our opinion recommendation system gives ratings that are closer to real user ratings on Yelp.com data compared with Yelp's own ratings, and our methods give better results compared to several pipelines baselines using state-of-the-art sentiment rating and summarization systems. version:1
arxiv-1702-01507 | Challenge of Multi-Camera Tracking | http://arxiv.org/abs/1702.01507 | id:1702.01507 author:Yong Wang, Ke Lu category:cs.CV  published:2017-02-06 summary:Multi-camera tracking is quite different from single camera tracking, and it faces new technology and system architecture challenges. By analyzing the corresponding characteristics and disadvantages of the existing algorithms, problems in multi-camera tracking are summarized and some new directions for future work are also generalized. version:1
arxiv-1702-01504 | Optimizing Cost-Sensitive SVM for Imbalanced Data :Connecting Cluster to Classification | http://arxiv.org/abs/1702.01504 | id:1702.01504 author:Qiuyan Yan, Shixiong Xia, Fanrong Meng category:cs.LG  published:2017-02-06 summary:Class imbalance is one of the challenging problems for machine learning in many real-world applications, such as coal and gas burst accident monitoring: the burst premonition data is extreme smaller than the normal data, however, which is the highlight we truly focus on. Cost-sensitive adjustment approach is a typical algorithm-level method resisting the data set imbalance. For SVMs classifier, which is modified to incorporate varying penalty parameter(C) for each of considered groups of examples. However, the C value is determined empirically, or is calculated according to the evaluation metric, which need to be computed iteratively and time consuming. This paper presents a novel cost-sensitive SVM method whose penalty parameter C optimized on the basis of cluster probability density function(PDF) and the cluster PDF is estimated only according to similarity matrix and some predefined hyper-parameters. Experimental results on various standard benchmark data sets and real-world data with different ratios of imbalance show that the proposed method is effective in comparison with commonly used cost-sensitive techniques. version:1
arxiv-1702-01499 | Designing Deep Convolutional Neural Networks for Continuous Object Orientation Estimation | http://arxiv.org/abs/1702.01499 | id:1702.01499 author:Kota Hara, Raviteja Vemulapalli, Rama Chellappa category:cs.CV  published:2017-02-06 summary:Deep Convolutional Neural Networks (DCNN) have been proven to be effective for various computer vision problems. In this work, we demonstrate its effectiveness on a continuous object orientation estimation task, which requires prediction of 0 to 360 degrees orientation of the objects. We do so by proposing and comparing three continuous orientation prediction approaches designed for the DCNNs. The first two approaches work by representing an orientation as a point on a unit circle and minimizing either L2 loss or angular difference loss. The third method works by first converting the continuous orientation estimation task into a set of discrete orientation estimation tasks and then converting the discrete orientation outputs back to the continuous orientation using a mean-shift algorithm. By evaluating on a vehicle orientation estimation task and a pedestrian orientation estimation task, we demonstrate that the discretization-based approach not only works better than the other two approaches but also achieves state-of-the-art performance. We also demonstrate that finding an appropriate feature representation is critical to achieve a good performance when adapting a DCNN trained for an image recognition task. version:1
arxiv-1702-00956 | KU-ISPL Speaker Recognition Systems under Language mismatch condition for NIST 2016 Speaker Recognition Evaluation | http://arxiv.org/abs/1702.00956 | id:1702.00956 author:Suwon Shon, Hanseok Ko category:cs.SD cs.CL  published:2017-02-03 summary:Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed speaker recognition system for SRE16 fixed training condition. Data for evaluation trials are collected from outside North America, spoken in Tagalog and Cantonese while training data only is spoken English. Thus, main issue for SRE16 is compensating the discrepancy between different languages. As development dataset which is spoken in Cebuano and Mandarin, we could prepare the evaluation trials through preliminary experiments to compensate the language mismatched condition. Our team developed 4 different approaches to extract i-vectors and applied state-of-the-art techniques as backend. To compensate language mismatch, we investigated and endeavored unique method such as unsupervised language clustering, inter language variability compensation and gender/language dependent score normalization. version:2
arxiv-1702-01486 | Detailed Surface Geometry and Albedo Recovery from RGB-D Video Under Natural Illumination | http://arxiv.org/abs/1702.01486 | id:1702.01486 author:Xinxin Zuo, Sen Wang, Jiangbin Zheng, Ruigang Yang category:cs.CV  published:2017-02-06 summary:In this paper we present a novel approach for depth map enhancement from an RGB-D video sequence. The basic idea is to exploit the shading information in the color image. Instead of making assumption about surface albedo or controlled object motion and lighting, we use the lighting variations introduced by casual object movement. We are effectively calculating photometric stereo from a moving object under natural illuminations. The key technical challenge is to establish correspondences over the entire image set. We therefore develop a lighting insensitive robust pixel matching technique that out-performs optical flow method in presence of lighting variations. In addition we present an expectation-maximization framework to recover the surface normal and albedo simultaneously, without any regularization term. We have validated our method on both synthetic and real datasets to show its superior performance on both surface details recovery and intrinsic decomposition. version:1
arxiv-1702-01478 | Attentional Network for Visual Object Detection | http://arxiv.org/abs/1702.01478 | id:1702.01478 author:Kota Hara, Ming-Yu Liu, Oncel Tuzel, Amir-massoud Farahmand category:cs.CV  published:2017-02-06 summary:We propose augmenting deep neural networks with an attention mechanism for the visual object detection task. As perceiving a scene, humans have the capability of multiple fixation points, each attended to scene content at different locations and scales. However, such a mechanism is missing in the current state-of-the-art visual object detection methods. Inspired by the human vision system, we propose a novel deep network architecture that imitates this attention mechanism. As detecting objects in an image, the network adaptively places a sequence of glimpses of different shapes at different locations in the image. Evidences of the presence of an object and its location are extracted from these glimpses, which are then fused for estimating the object class and bounding box coordinates. Due to lacks of ground truth annotations of the visual attention mechanism, we train our network using a reinforcement learning algorithm with policy gradients. Experiment results on standard object detection benchmarks show that the proposed network consistently outperforms the baseline networks that does not model the attention mechanism. version:1
arxiv-1702-02512 | Semi-Dense Visual Odometry for RGB-D Cameras Using Approximate Nearest Neighbour Fields | http://arxiv.org/abs/1702.02512 | id:1702.02512 author:Yi Zhou, Laurent Kneip, Hongdong Li category:cs.CV  published:2017-02-06 summary:This paper presents a robust and efficient semi-dense visual odometry solution for RGB-D cameras. The core of our method is a 2D-3D ICP pipeline which estimates the pose of the sensor by registering the projection of a 3D semi-dense map of the reference frame with the 2D semi-dense region extracted in the current frame. The processing is speeded up by efficiently implemented approximate nearest neighbour fields under the Euclidean distance criterion, which permits the use of compact Gauss-Newton updates in the optimization. The registration is formulated as a maximum a posterior problem to deal with outliers and sensor noises, and consequently the equivalent weighted least squares problem is solved by iteratively reweighted least squares method. A variety of robust weight functions are tested and the optimum is determined based on the characteristics of the sensor model. Extensive evaluation on publicly available RGB-D datasets shows that the proposed method predominantly outperforms existing state-of-the-art methods. version:1
arxiv-1702-01466 | Prepositions in Context | http://arxiv.org/abs/1702.01466 | id:1702.01466 author:Hongyu Gong, Jiaqi Mu, Suma Bhat, Pramod Viswanath category:cs.CL  published:2017-02-05 summary:Prepositions are highly polysemous, and their variegated senses encode significant semantic information. In this paper we match each preposition's complement and attachment and their interplay crucially to the geometry of the word vectors to the left and right of the preposition. Extracting such features from the vast number of instances of each preposition and clustering them makes for an efficient preposition sense disambigution (PSD) algorithm, which is comparable to and better than state-of-the-art on two benchmark datasets. Our reliance on no external linguistic resource allows us to scale the PSD algorithm to a large WikiCorpus and learn sense-specific preposition representations -- which we show to encode semantic relations and paraphrasing of verb particle compounds, via simple vector operations. version:1
arxiv-1702-01444 | Printed Arabic Text Recognition using Linear and Nonlinear Regression | http://arxiv.org/abs/1702.01444 | id:1702.01444 author:Ashraf A. Shahin category:cs.CV  published:2017-02-05 summary:Arabic language is one of the most popular languages in the world. Hundreds of millions of people in many countries around the world speak Arabic as their native speaking. However, due to complexity of Arabic language, recognition of printed and handwritten Arabic text remained untouched for a very long time compared with English and Chinese. Although, in the last few years, significant number of researches has been done in recognizing printed and handwritten Arabic text, it stills an open research field due to cursive nature of Arabic script. This paper proposes automatic printed Arabic text recognition technique based on linear and ellipse regression techniques. After collecting all possible forms of each character, unique code is generated to represent each character form. Each code contains a sequence of lines and ellipses. To recognize fonts, a unique list of codes is identified to be used as a fingerprint of font. The proposed technique has been evaluated using over 14000 different Arabic words with different fonts and experimental results show that average recognition rate of the proposed technique is 86%. version:1
arxiv-1702-01426 | Robust Features Face | http://arxiv.org/abs/1702.01426 | id:1702.01426 author:Nadav Israel, Lior Wolf, Ran Barzilay, Gal Shoval category:cs.CV  published:2017-02-05 summary:Automatic recognition of facial gestures is becoming increasingly important as real world AI agents become a reality. In this paper, we present an automated system that recognizes facial gestures by capturing local changes and encoding the motion into a histogram of frequencies. We evaluate the proposed method by demonstrating its effectiveness on spontaneous face action benchmarks: the FEEDTUM dataset, the Pain dataset and the HMDB51 dataset. The results show that, compared to known methods, the new encoding methods significantly improve the recognition accuracy and the robustness of analysis for a variety of applications. version:1
arxiv-1702-01417 | All-but-the-Top: Simple and Effective Postprocessing for Word Representations | http://arxiv.org/abs/1702.01417 | id:1702.01417 author:Jiaqi Mu, Suma Bhat, Pramod Viswanath category:cs.CL stat.ML  published:2017-02-05 summary:Real-valued word representations have transformed NLP applications, popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a very simple, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations even stronger. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level extrinsic tasks (semantic textual similarity) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages, in each case, the processed representations are consistently better than the original ones. Furthermore, we demonstrate quantitatively in downstream applications that neural network architectures "automatically learn" the postprocessing operation. version:1
arxiv-1702-01414 | Shape-Based Approach to Household Load Curve Clustering and Prediction | http://arxiv.org/abs/1702.01414 | id:1702.01414 author:Thanchanok Teeraratkul, Daniel O'Neill, Sanjay Lall category:stat.ML  published:2017-02-05 summary:Consumer Demand Response (DR) is an important research and industry problem, which seeks to categorize, predict and modify consumer's energy consumption. Unfortunately, traditional clustering methods have resulted in many hundreds of clusters, with a given consumer often associated with several clusters, making it difficult to classify consumers into stable representative groups and to predict individual energy consumption patterns. In this paper, we present a shape-based approach that better classifies and predicts consumer energy consumption behavior at the household level. The method is based on Dynamic Time Warping. DTW seeks an optimal alignment between energy consumption patterns reflecting the effect of hidden patterns of regular consumer behavior. Using real consumer 24-hour load curves from Opower Corporation, our method results in a 50% reduction in the number of representative groups and an improvement in prediction accuracy measured under DTW distance. We extend the approach to estimate which electrical devices will be used and in which hours. version:1
arxiv-1702-01381 | Relative Camera Pose Estimation Using Convolutional Neural Networks | http://arxiv.org/abs/1702.01381 | id:1702.01381 author:Iaroslav Melekhov, Juho Kannala, Esa Rahtu category:cs.CV  published:2017-02-05 summary:We present a method for estimating relative camera pose between a pair of images. The goal is to propose accurate estimations the relative orientation vector representing by rotation matrix and translation vector of two cameras capturing the same scene. Our approach is based on convolutional neural networks and directly estimates camera motion between two RGB images by solving regression problem. The proposed network is trained in an end-to-end manner utilizing transfer learning from large scale classification data. The method is compared to a classical local feature based pipeline (SURF, ORB) of relative pose estimation and we demonstrate the cases where our deep model outperforms the traditional approach significantly. Finally, we evaluated experiments with applying Spatial Pyramid Pooling (SPP) layer which can produce a fixed-size representation regardless the size of the input image. The results confirm that SPP further improves the performance of the proposed approach. version:1
arxiv-1702-01373 | Exact heat kernel on a hypersphere and its applications in kernel SVM | http://arxiv.org/abs/1702.01373 | id:1702.01373 author:Chenchao Zhao, Jun S. Song category:stat.ML q-bio.QM stat.CO  published:2017-02-05 summary:Many contemporary statistical learning methods assume a Euclidean feature space, however, the "curse of dimensionality" associated with high feature dimensions is particularly severe for the Euclidean distance. This paper presents a method for defining similarity based on hyperspherical geometry and shows that it often improves the performance of support vector machine compared to other competing similarity measures. Specifically, the idea of using heat diffusion on a hypersphere to measure similarity has been proposed and tested by \citet{Lafferty:2015uy}, demonstrating promising results based on an approximate heat kernel, however, the exact hyperspherical heat kernel hitherto remains unknown. In this paper, we derive an exact form of the heat kernel on a unit hypersphere in terms of a uniformly and absolutely convergent series in high-dimensional angular momentum eigenmodes. Being a natural measure of similarity between sample points dwelling on a hypersphere, the exact kernel often shows superior performance in kernel SVM classifications applied to text mining, tumor somatic mutation imputation, and stock market analysis. The improvement in classification accuracy compared with kernels based on Euclidean geometry may arise from ameliorating the curse of dimensionality on compact manifolds. version:1
arxiv-1702-01361 | Deep learning and the Schrödinger equation | http://arxiv.org/abs/1702.01361 | id:1702.01361 author:K. Mills, M. Spanner, I. Tamblyn category:cond-mat.mtrl-sci cs.LG physics.chem-ph  published:2017-02-05 summary:We have trained a deep (convolutional) neural network to predict the ground-state energy of an electron in four classes of confining two-dimensional electrostatic potentials. On randomly generated potentials, for which there is no analytic form for either the potential or the ground-state energy, the neural network model was able to predict the ground-state energy to within chemical accuracy, with a median absolute error of 1.49 mHa. We also investigate the performance of the model in predicting other quantities such as the kinetic energy and the first excited-state energy of random potentials. While we demonstrated this approach on a simple, tractable problem, the transferability and excellent performance of the resulting model suggests further applications of deep neural networks to problems of electronic structure. version:1
arxiv-1702-01360 | An Empirical Evaluation of Zero Resource Acoustic Unit Discovery | http://arxiv.org/abs/1702.01360 | id:1702.01360 author:Chunxi Liu, Jinyi Yang, Ming Sun, Santosh Kesiraju, Alena Rott, Lucas Ondel, Pegah Ghahremani, Najim Dehak, Lukas Burget, Sanjeev Khudanpur category:cs.CL  published:2017-02-05 summary:Acoustic unit discovery (AUD) is a process of automatically identifying a categorical acoustic unit inventory from speech and producing corresponding acoustic unit tokenizations. AUD provides an important avenue for unsupervised acoustic model training in a zero resource setting where expert-provided linguistic knowledge and transcribed speech are unavailable. Therefore, to further facilitate zero-resource AUD process, in this paper, we demonstrate acoustic feature representations can be significantly improved by (i) performing linear discriminant analysis (LDA) in an unsupervised self-trained fashion, and (ii) leveraging resources of other languages through building a multilingual bottleneck (BN) feature extractor to give effective cross-lingual generalization. Moreover, we perform comprehensive evaluations of AUD efficacy on multiple downstream speech applications, and their correlated performance suggests that AUD evaluations are feasible using different alternative language resources when only a subset of these evaluation resources can be available in typical zero resource applications. version:1
arxiv-1702-01339 | Entropy-guided Retinex anisotropic diffusion algorithm based on partial differential equations (PDE) for illumination correction | http://arxiv.org/abs/1702.01339 | id:1702.01339 author:U. A. Nnolim category:cs.CV  published:2017-02-04 summary:This report describes the experimental results obtained using a proposed variational Retinex algorithm for controlled illumination correction. Two colour restoration and enhancement schemes of the algorithm are presented for drastically improved results. The algorithm modifies the reflectance image using global and local contrast enhancement approaches and gradually removes the residual illumination to yield highly pleasing results. The proposed algorithms are optimized by way of simultaneous perceptual quality metric (PQM) stabilization and entropy maximization for fully automated processing solving the problem of determination of stopping time. The usage of the HSI or HSV colour space ensures a unique solution to the optimization problem unlike in the RGB space where there is none (forcing manual selection of number of iteration. The proposed approach preserves and enhances details in both bright and dark regions of underexposed images in addition to eliminating the colour distortion, over-exposure in bright image regions, halo effect and grey-world violations observed in Retinex-based approaches. Extensive experiments indicate consistent performance as the proposed approach exploits and augments the advantages of PDE-based formulation, performing illumination correction, colour enhancement correction and restoration, contrast enhancement and noise suppression. Comparisons shows that the proposed approach surpasses most of the other conventional algorithms found in the literature. version:1
arxiv-1702-01334 | An Experimental Study of Deep Convolutional Features For Iris Recognition | http://arxiv.org/abs/1702.01334 | id:1702.01334 author:Shervin Minaee, Amirali Abdolrashidi, Yao Wang category:cs.CV  published:2017-02-04 summary:Iris is one of the popular biometrics that is widely used for identity authentication. Different features have been used to perform iris recognition in the past. Most of them are based on hand-crafted features designed by biometrics experts. Due to tremendous success of deep learning in computer vision problems, there has been a lot of interest in applying features learned by convolutional neural networks on general image recognition to other tasks such as segmentation, face recognition, and object detection. In this paper, we have investigated the application of deep features extracted from VGG-Net for iris recognition. The proposed scheme has been tested on two well-known iris databases, and has shown promising results with the best accuracy rate of 99.4\%, which outperforms the previous best result. version:1
arxiv-1702-01315 | Fast and easy blind deblurring using an inverse filter and PROBE | http://arxiv.org/abs/1702.01315 | id:1702.01315 author:Naftali Zon, Rana Hanocka, Nahum Kiryati category:cs.CV  published:2017-02-04 summary:PROBE (Progressive Removal of Blur Residual) is a recursive framework for blind deblurring. Using the elementary modified inverse filter at its core, PROBE's experimental performance meets or exceeds the state of the art, both visually and quantitatively. Remarkably, PROBE lends itself to analysis that reveals its convergence properties. PROBE is motivated by recent ideas on progressive blind deblurring, but breaks away from previous research by its simplicity, speed, performance and potential for analysis. PROBE is neither a functional minimization approach, nor an open-loop sequential method (blur kernel estimation followed by non-blind deblurring). PROBE is a feedback scheme, deriving its unique strength from the closed-loop architecture rather than from the accuracy of its algorithmic components. version:1
arxiv-1702-01313 | Cluster-based Kriging Approximation Algorithms for Complexity Reduction | http://arxiv.org/abs/1702.01313 | id:1702.01313 author:Bas van Stein, Hao Wang, Wojtek Kowalczyk, Michael Emmerich, Thomas Bäck category:cs.LG cs.AI stat.ML  published:2017-02-04 summary:Kriging or Gaussian Process Regression is applied in many fields as a non-linear regression model as well as a surrogate model in the field of evolutionary computation. However, the computational and space complexity of Kriging, that is cubic and quadratic in the number of data points respectively, becomes a major bottleneck with more and more data available nowadays. In this paper, we propose a general methodology for the complexity reduction, called cluster Kriging, where the whole data set is partitioned into smaller clusters and multiple Kriging models are built on top of them. In addition, four Kriging approximation algorithms are proposed as candidate algorithms within the new framework. Each of these algorithms can be applied to much larger data sets while maintaining the advantages and power of Kriging. The proposed algorithms are explained in detail and compared empirically against a broad set of existing state-of-the-art Kriging approximation methods on a well-defined testing framework. According to the empirical study, the proposed algorithms consistently outperform the existing algorithms. Moreover, some practical suggestions are provided for using the proposed algorithms. version:1
arxiv-1701-03961 | Communication-Efficient Algorithms for Decentralized and Stochastic Optimization | http://arxiv.org/abs/1701.03961 | id:1701.03961 author:Guanghui Lan, Soomin Lee, Yi Zhou category:math.OC cs.LG  published:2017-01-14 summary:We present a new class of decentralized first-order methods for nonsmooth and stochastic optimization problems defined over multiagent networks. Considering that communication is a major bottleneck in decentralized optimization, our main goal in this paper is to develop algorithmic frameworks which can significantly reduce the number of inter-node communications. We first propose a decentralized primal-dual method which can find an $\epsilon$-solution both in terms of functional optimality gap and feasibility residual in $O(1/\epsilon)$ inter-node communication rounds when the objective functions are convex and the local primal subproblems are solved exactly. Our major contribution is to present a new class of decentralized primal-dual type algorithms, namely the decentralized communication sliding (DCS) methods, which can skip the inter-node communications while agents solve the primal subproblems iteratively through linearizations of their local objective functions. By employing DCS, agents can still find an $\epsilon$-solution in $O(1/\epsilon)$ (resp., $O(1/\sqrt{\epsilon})$) communication rounds for general convex functions (resp., strongly convex functions), while maintaining the $O(1/\epsilon^2)$ (resp., $O(1/\epsilon)$) bound on the total number of intra-node subgradient evaluations. We also present a stochastic counterpart for these algorithms, denoted by SDCS, for solving stochastic optimization problems whose objective function cannot be evaluated exactly. In comparison with existing results for decentralized nonsmooth and stochastic optimization, we can reduce the total number of inter-node communication rounds by orders of magnitude while still maintaining the optimal complexity bounds on intra-node stochastic subgradient evaluations. The bounds on the subgradient evaluations are actually comparable to those required for centralized nonsmooth and stochastic optimization. version:2
arxiv-1702-01304 | Gender-From-Iris or Gender-From-Mascara? | http://arxiv.org/abs/1702.01304 | id:1702.01304 author:Andrey Kuehlkamp, Benedict Becker, Kevin Bowyer category:cs.CV  published:2017-02-04 summary:Predicting a person's gender based on the iris texture has been explored by several researchers. This paper considers several dimensions of experimental work on this problem, including person-disjoint train and test, and the effect of cosmetics on eyelash occlusion and imperfect segmentation. We also consider the use of multi-layer perceptron and convolutional neural networks as classifiers, comparing the use of data-driven and hand-crafted features. Our results suggest that the gender-from-iris problem is more difficult than has so far been appreciated. Estimating accuracy using a mean of N person-disjoint train and test partitions, and considering the effect of makeup - a combination of experimental conditions not present in any previous work - we find a much weaker ability to predict gender-from-iris texture than has been suggested in previous work. version:1
arxiv-1701-05957 | Image De-raining Using a Conditional Generative Adversarial Network | http://arxiv.org/abs/1701.05957 | id:1701.05957 author:He Zhang, Vishwanath Sindagi, Vishal M. Patel category:cs.CV  published:2017-01-21 summary:Severe weather conditions such as rain and snow adversely affect the visual quality of images captured under such conditions thus rendering them useless for further usage and sharing. In addition, such degraded images drastically affect performance of vision systems. Hence, it is important to solve the problem of single image de-raining/de-snowing. However, this is a difficult problem to solve due to its inherent ill-posed nature. Existing approaches attempt to introduce prior information to convert it into a well-posed problem. In this paper, we investigate a new point of view in addressing the single image de-raining problem. Instead of focusing only on deciding what is a good prior or a good framework to achieve good quantitative and qualitative performance, we also ensure that the de-rained image itself does not degrade the performance of a given computer vision algorithm such as detection and classification. In other words, the de-rained result should be indistinguishable from its corresponding clear image to a given discriminator. This criterion can be directly incorporated into the optimization framework by using the recently introduced conditional generative adversarial networks (GANs). To minimize artifacts introduced by GANs and ensure better visual quality, a new refined loss function is introduced. Based on this, we propose a novel single image de-raining method called Image De-raining Conditional General Adversarial Network (ID-CGAN), which considers quantitative, visual and also discriminative performance into the objective function. Experiments evaluated on synthetic images and real images show that the proposed method outperforms many recent state-of-the-art single image de-raining methods in terms of quantitative and visual performance. version:3
arxiv-1702-01293 | Latent Hinge-Minimax Risk Minimization for Inference from a Small Number of Training Samples | http://arxiv.org/abs/1702.01293 | id:1702.01293 author:Dolev Raviv, Margarita Osadchy category:cs.LG cs.CV  published:2017-02-04 summary:Deep Learning (DL) methods show very good performance when trained on large, balanced data sets. However, many practical problems involve imbalanced data sets, or/and classes with a small number of training samples. The performance of DL methods as well as more traditional classifiers drops significantly in such settings. Most of the existing solutions for imbalanced problems focus on customizing the data for training. A more principled solution is to use mixed Hinge-Minimax risk [19] specifically designed to solve binary problems with imbalanced training sets. Here we propose a Latent Hinge Minimax (LHM) risk and a training algorithm that generalizes this paradigm to an ensemble of hyperplanes that can form arbitrary complex, piecewise linear boundaries. To extract good features, we combine LHM model with CNN via transfer learning. To solve multi-class problem we map pre-trained category-specific LHM classifiers to a multi-class neural network and adjust the weights with very fast tuning. LHM classifier enables the use of unlabeled data in its training and the mapping allows for multi-class inference, resulting in a classifier that performs better than alternatives when trained on a small number of training samples. version:1
arxiv-1702-01287 | Doubly-Attentive Decoder for Multi-modal Neural Machine Translation | http://arxiv.org/abs/1702.01287 | id:1702.01287 author:Iacer Calixto, Qun Liu, Nick Campbell category:cs.CL I.2.7  published:2017-02-04 summary:We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set. version:1
arxiv-1702-01276 | Using Complex Wavelet Transform and Bilateral Filtering for Image Denoising | http://arxiv.org/abs/1702.01276 | id:1702.01276 author:Seyede Mahya Hazavei, Hamid Reza Shahdoosti category:cs.CV  published:2017-02-04 summary:The bilateral filter is a useful nonlinear filter which without smoothing edges, it does spatial averaging. In the literature, the effectiveness of this method for image denoising is shown. In this paper, an extension of this method is proposed which is based on complex wavelet transform. In fact, the bilateral filtering is applied to the low-frequency (approximation) subbands of the decomposed image using complex wavelet transform, while the thresholding approach is applied to the high frequency subbands. Using the bilateral filter in the complex wavelet domain forms a new image denoising framework. Experimental results for real data are provided, by which one can see the effectiveness of the proposed method in eliminating noise. version:1
arxiv-1702-01268 | Network-based methods for outcome prediction in the "sample space" | http://arxiv.org/abs/1702.01268 | id:1702.01268 author:Jessica Gliozzo category:cs.LG stat.ML I.2.6  published:2017-02-04 summary:In this thesis we present the novel semi-supervised network-based algorithm P-Net, which is able to rank and classify patients with respect to a specific phenotype or clinical outcome under study. The peculiar and innovative characteristic of this method is that it builds a network of samples/patients, where the nodes represent the samples and the edges are functional or genetic relationships between individuals (e.g. similarity of expression profiles), to predict the phenotype under study. In other words, it constructs the network in the "sample space" and not in the "biomarker space" (where nodes represent biomolecules (e.g. genes, proteins) and edges represent functional or genetic relationships between nodes), as usual in state-of-the-art methods. To assess the performances of P-Net, we apply it on three different publicly available datasets from patients afflicted with a specific type of tumor: pancreatic cancer, melanoma and ovarian cancer dataset, by using the data and following the experimental set-up proposed in two recently published papers [Barter et al., 2014, Winter et al., 2012]. We show that network-based methods in the "sample space" can achieve results competitive with classical supervised inductive systems. Moreover, the graph representation of the samples can be easily visualized through networks and can be used to gain visual clues about the relationships between samples, taking into account the phenotype associated or predicted for each sample. To our knowledge this is one of the first works that proposes graph-based algorithms working in the "sample space" of the biomolecular profiles of the patients to predict their phenotype or outcome, thus contributing to a novel research line in the framework of the Network Medicine. version:1
arxiv-1701-04024 | A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue | http://arxiv.org/abs/1701.04024 | id:1701.04024 author:Mihail Eric, Christopher D. Manning category:cs.CL cs.AI  published:2017-01-15 summary:Task-oriented dialogue focuses on conversational agents that participate in user-initiated dialogues on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing task-oriented agents usually explicitly model user intent and belief states. This paper examines bypassing such an explicit representation by depending on a latent neural embedding of state and learning selective attention to dialogue history together with copying to incorporate relevant prior context. We complement recent work by showing the effectiveness of simple sequence-to-sequence neural architectures with a copy mechanism. Our model outperforms more complex memory-augmented models by 7% in per-response generation and is on par with the current state-of-the-art on DSTC2. version:2
arxiv-1702-01247 | Towards Unsupervised Weed Scouting for Agricultural Robotics | http://arxiv.org/abs/1702.01247 | id:1702.01247 author:David Hall, Feras Dayoub, Jason Kulk, Chris McCool category:cs.CV  published:2017-02-04 summary:Weed scouting is an important part of modern integrated weed management but can be time consuming and sparse when performed manually. Automated weed scouting and weed destruction has typically been performed using classification systems able to classify a set group of species known a priori. This greatly limits deployability as classification systems must be retrained for any field with a different set of weed species present within them. In order to overcome this limitation, this paper works towards developing a clustering approach to weed scouting which can be utilized in any field without the need for prior species knowledge. We demonstrate our system using challenging data collected in the field from an agricultural robotics platform. We show that considerable improvements can be made by (i) learning low-dimensional (bottleneck) features using a deep convolutional neural network to represent plants in general and (ii) tying views of the same area (plant) together. Deploying this algorithm on in-field data collected by AgBotII, we are able to successfully cluster cotton plants from grasses without prior knowledge or training for the specific plants in the field. version:1
arxiv-1702-01243 | Wide-Residual-Inception Networks for Real-time Object Detection | http://arxiv.org/abs/1702.01243 | id:1702.01243 author:Youngwan Lee, Huien Kim, Eunsoo Park, Xuenan Cui, Hakil Kim category:cs.CV  published:2017-02-04 summary:Since convolutional neural network(CNN)models emerged,several tasks in computer vision have actively deployed CNN models for feature extraction. However,the conventional CNN models have a high computational cost and require high memory capacity, which is impractical and unaffordable for commercial applications such as real-time on-road object detection on embedded boards or mobile platforms. To tackle this limitation of CNN models, this paper proposes a wide-residual-inception (WR-Inception) network, which constructs the architecture based on a residual inception unit that captures objects of various sizes on the same feature map, as well as shallower and wider layers, compared to state-of-the-art networks like ResNet. To verify the proposed networks, this paper conducted two experiments; one is a classification task on CIFAR-10/100 and the other is an on-road object detection task using a Single-Shot Multi-box Detector(SSD) on the KITTI dataset. version:1
arxiv-1702-01229 | Simple to Complex Cross-modal Learning to Rank | http://arxiv.org/abs/1702.01229 | id:1702.01229 author:Minnan Luo, Xiaojun Chang, Yi Yang, Liqiang Nie, Alexander G. Hauptmann, Qinghua Zheng category:cs.LG stat.ML  published:2017-02-04 summary:The heterogeneity-gap between different modalities brings a significant challenge to multimedia information retrieval. Some studies formalize the cross-modal retrieval tasks as a ranking problem and learn a shared multi-modal embedding space to measure the cross-modality similarity. However, previous methods often establish the shared embedding space based on linear mapping functions which might not be sophisticated enough to reveal more complicated inter-modal correspondences. Additionally, current studies assume that the rankings are of equal importance, and thus all rankings are used simultaneously, or a small number of rankings are selected randomly to train the embedding space at each iteration. Such strategies, however, always suffer from outliers as well as reduced generalization capability due to their lack of insightful understanding of procedure of human cognition. In this paper, we involve the self-paced learning theory with diversity into the cross-modal learning to rank and learn an optimal multi-modal embedding space based on non-linear mapping functions. This strategy enhances the model's robustness to outliers and achieves better generalization via training the model gradually from easy rankings by diverse queries to more complex ones. An efficient alternative algorithm is exploited to solve the proposed challenging problem with fast convergence in practice. Extensive experimental results on several benchmark datasets indicate that the proposed method achieves significant improvements over the state-of-the-arts in this literature. version:1
arxiv-1702-01228 | A Learning-Based Approach for Lane Departure Warning Systems with a Personalized Driver Model | http://arxiv.org/abs/1702.01228 | id:1702.01228 author:Wenshuo Wang, Ding Zhao, Junqiang Xi, Wei Han category:cs.LG cs.SY  published:2017-02-04 summary:Misunderstanding of driver correction behaviors (DCB) is the primary reason for false warnings of lane-departure-prediction systems. We propose a learning-based approach to predicting unintended lane-departure behaviors (LDB) and the chance for drivers to bring the vehicle back to the lane. First, in this approach, a personalized driver model for lane-departure and lane-keeping behavior is established by combining the Gaussian mixture model and the hidden Markov model. Second, based on this model, we develop an online model-based prediction algorithm to predict the forthcoming vehicle trajectory and judge whether the driver will demonstrate an LDB or a DCB. We also develop a warning strategy based on the model-based prediction algorithm that allows the lane-departure warning system to be acceptable for drivers according to the predicted trajectory. In addition, the naturalistic driving data of 10 drivers is collected through the University of Michigan Safety Pilot Model Deployment program to train the personalized driver model and validate this approach. We compare the proposed method with a basic time-to-lane-crossing (TLC) method and a TLC-directional sequence of piecewise lateral slopes (TLC-DSPLS) method. The results show that the proposed approach can reduce the false-warning rate to 3.07\%. version:1
arxiv-1702-00254 | Evolving Boxes for fast Vehicle Detection | http://arxiv.org/abs/1702.00254 | id:1702.00254 author:Li Wang, Yao Lu, Hong Wang, Yingbin Zheng, Hao Ye, Xiangyang Xue category:cs.CV  published:2017-02-01 summary:We perform fast vehicle detection from traffic surveillance cameras. The classic cascade object detection is revisited and a novel deep learning framework, namely Evolving Boxes, is developed that proposes and refines the object boxes under different feature representations. Specifically, our framework is embedded with a light-weight proposal network to generate initial anchor boxes as well as to early discard unlikely regions; a fine-turning network produces detailed features for these candidate boxes. We show intriguingly that by apply different feature fusion techniques, the initial boxes can be refined in terms of both localization and recognition, leading to evolved boxes. We evaluate our network on the recent DETRAC benchmark and obtain a significant improvement over the state-of-the-art Faster RCNN by 9.5% mAP. Further, our network achieves 9-13 FPS detection speed on a moderate commercial GPU. version:2
arxiv-1702-01226 | Towards Better Analysis of Machine Learning Models: A Visual Analytics Perspective | http://arxiv.org/abs/1702.01226 | id:1702.01226 author:Shixia Liu, Xiting Wang, Mengchen Liu, Jun Zhu category:cs.LG stat.ML  published:2017-02-04 summary:Interactive model analysis, the process of understanding, diagnosing, and refining a machine learning model with the help of interactive visualization, is very important for users to efficiently solve real-world artificial intelligence and data mining problems. Dramatic advances in big data analytics has led to a wide variety of interactive model analysis tasks. In this paper, we present a comprehensive analysis and interpretation of this rapidly developing area. Specifically, we classify the relevant work into three categories: understanding, diagnosis, and refinement. Each category is exemplified by recent influential work. Possible future research opportunities are also explored and discussed. version:1
arxiv-1702-01209 | Probabilistic Sensor Fusion for Ambient Assisted Living | http://arxiv.org/abs/1702.01209 | id:1702.01209 author:Tom Diethe, Niall Twomey, Meelis Kull, Peter Flach, Ian Craddock category:stat.ML cs.HC  published:2017-02-04 summary:There is a widely-accepted need to revise current forms of health-care provision, with particular interest in sensing systems in the home. Given a multiple-modality sensor platform with heterogeneous network connectivity, as is under development in the Sensor Platform for HEalthcare in Residential Environment (SPHERE) Interdisciplinary Research Collaboration (IRC), we face specific challenges relating to the fusion of the heterogeneous sensor modalities. We introduce Bayesian models for sensor fusion, which aims to address the challenges of fusion of heterogeneous sensor modalities. Using this approach we are able to identify the modalities that have most utility for each particular activity, and simultaneously identify which features within that activity are most relevant for a given activity. We further show how the two separate tasks of location prediction and activity recognition can be fused into a single model, which allows for simultaneous learning an prediction for both tasks. We analyse the performance of this model on data collected in the SPHERE house, and show its utility. We also compare against some benchmark models which do not have the full structure,and show how the proposed model compares favourably to these methods version:1
arxiv-1702-01208 | A Theoretical Analysis of First Heuristics of Crowdsourced Entity Resolution | http://arxiv.org/abs/1702.01208 | id:1702.01208 author:Arya Mazumdar, Barna Saha category:cs.DB cs.AI cs.LG  published:2017-02-03 summary:Entity resolution (ER) is the task of identifying all records in a database that refer to the same underlying entity, and are therefore duplicates of each other. Due to inherent ambiguity of data representation and poor data quality, ER is a challenging task for any automated process. As a remedy, human-powered ER via crowdsourcing has become popular in recent years. Using crowd to answer queries is costly and time consuming. Furthermore, crowd-answers can often be faulty. Therefore, crowd-based ER methods aim to minimize human participation without sacrificing the quality and use a computer generated similarity matrix actively. While, some of these methods perform well in practice, no theoretical analysis exists for them, and further their worst case performances do not reflect the experimental findings. This creates a disparity in the understanding of the popular heuristics for this problem. In this paper, we make the first attempt to close this gap. We provide a thorough analysis of the prominent heuristic algorithms for crowd-based ER. We justify experimental observations with our analysis and information theoretic lower bounds. version:1
arxiv-1702-01205 | Traffic Lights with Auction-Based Controllers: Algorithms and Real-World Data | http://arxiv.org/abs/1702.01205 | id:1702.01205 author:Shumeet Baluja, Michele Covell, Rahul Sukthankar category:cs.AI cs.LG cs.SY  published:2017-02-03 summary:Real-time optimization of traffic flow addresses important practical problems: reducing a driver's wasted time, improving city-wide efficiency, reducing gas emissions and improving air quality. Much of the current research in traffic-light optimization relies on extending the capabilities of traffic lights to either communicate with each other or communicate with vehicles. However, before such capabilities become ubiquitous, opportunities exist to improve traffic lights by being more responsive to current traffic situations within the current, already deployed, infrastructure. In this paper, we introduce a traffic light controller that employs bidding within micro-auctions to efficiently incorporate traffic sensor information; no other outside sources of information are assumed. We train and test traffic light controllers on large-scale data collected from opted-in Android cell-phone users over a period of several months in Mountain View, California and the River North neighborhood of Chicago, Illinois. The learned auction-based controllers surpass (in both the relevant metrics of road-capacity and mean travel time) the currently deployed lights, optimized static-program lights, and longer-term planning approaches, in both cities, measured using real user driving data. version:1
arxiv-1702-01200 | Fuzzy Clustering Data Given on the Ordinal Scale Based on Membership and Likelihood Functions Sharing | http://arxiv.org/abs/1702.01200 | id:1702.01200 author:Zhengbing Hu, Yevgeniy V. Bodyanskiy, Oleksii K. Tyshchenko, Viktoriia O. Samitova category:cs.LG  published:2017-02-03 summary:A task of clustering data given in the ordinal scale under conditions of overlapping clusters has been considered. It's proposed to use an approach based on memberhsip and likelihood functions sharing. A number of performed experiments proved effectiveness of the proposed method. The proposed method is characterized by robustness to outliers due to a way of ordering values while constructing membership functions. version:1
arxiv-1702-01187 | Named Entity Evolution Recognition on the Blogosphere | http://arxiv.org/abs/1702.01187 | id:1702.01187 author:Helge Holzmann, Nina Tahmasebi, Thomas Risse category:cs.CL cs.DL  published:2017-02-03 summary:Advancements in technology and culture lead to changes in our language. These changes create a gap between the language known by users and the language stored in digital archives. It affects user's possibility to firstly find content and secondly interpret that content. In previous work we introduced our approach for Named Entity Evolution Recognition~(NEER) in newspaper collections. Lately, increasing efforts in Web preservation lead to increased availability of Web archives covering longer time spans. However, language on the Web is more dynamic than in traditional media and many of the basic assumptions from the newspaper domain do not hold for Web data. In this paper we discuss the limitations of existing methodology for NEER. We approach these by adapting an existing NEER method to work on noisy data like the Web and the Blogosphere in particular. We develop novel filters that reduce the noise and make use of Semantic Web resources to obtain more information about terms. Our evaluation shows the potentials of the proposed approach. version:1
arxiv-1702-01182 | Uncertainty-Aware Reinforcement Learning for Collision Avoidance | http://arxiv.org/abs/1702.01182 | id:1702.01182 author:Gregory Kahn, Adam Villaflor, Vitchyr Pong, Pieter Abbeel, Sergey Levine category:cs.LG cs.RO  published:2017-02-03 summary:Reinforcement learning can enable complex, adaptive behavior to be learned automatically for autonomous robotic platforms. However, practical deployment of reinforcement learning methods must contend with the fact that the training process itself can be unsafe for the robot. In this paper, we consider the specific case of a mobile robot learning to navigate an a priori unknown environment while avoiding collisions. In order to learn collision avoidance, the robot must experience collisions at training time. However, high-speed collisions, even at training time, could damage the robot. A successful learning method must therefore proceed cautiously, experiencing only low-speed collisions until it gains confidence. To this end, we present an uncertainty-aware model-based learning algorithm that estimates the probability of collision together with a statistical estimate of uncertainty. By formulating an uncertainty-dependent cost function, we show that the algorithm naturally chooses to proceed cautiously in unfamiliar environments, and increases the velocity of the robot in settings where it has high confidence. Our predictive model is based on bootstrapped neural networks using dropout, allowing it to process raw sensory inputs from high-bandwidth sensors such as cameras. Our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time in an obstacle avoidance task for a simulated and real-world quadrotor, and a real-world RC car. Videos of the experiments can be found at https://sites.google.com/site/probcoll. version:1
arxiv-1702-01179 | Extraction of Evolution Descriptions from the Web | http://arxiv.org/abs/1702.01179 | id:1702.01179 author:Helge Holzmann, Thomas Risse category:cs.CL cs.DL  published:2017-02-03 summary:The evolution of named entities affects exploration and retrieval tasks in digital libraries. An information retrieval system that is aware of name changes can actively support users in finding former occurrences of evolved entities. However, current structured knowledge bases, such as DBpedia or Freebase, do not provide enough information about evolutions, even though the data is available on their resources, like Wikipedia. Our \emph{Evolution Base} prototype will demonstrate how excerpts describing name evolutions can be identified on these websites with a promising precision. The descriptions are classified by means of models that we trained based on a recent analysis of named entity evolutions on Wikipedia. version:1
arxiv-1701-03214 | An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation | http://arxiv.org/abs/1701.03214 | id:1701.03214 author:Chenhui Chu, Raj Dabre, Sadao Kurohashi category:cs.CL  published:2017-01-12 summary:In this paper, we propose a novel domain adaptation method named "mixed fine tuning" for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings. version:2
arxiv-1702-01176 | Named Entity Evolution Analysis on Wikipedia | http://arxiv.org/abs/1702.01176 | id:1702.01176 author:Helge Holzmann, Thomas Risse category:cs.CL cs.DL  published:2017-02-03 summary:Accessing Web archives raises a number of issues caused by their temporal characteristics. Additional knowledge is needed to find and understand older texts. Especially entities mentioned in texts are subject to change. Most severe in terms of information retrieval are name changes. In order to find entities that have changed their name over time, search engines need to be aware of this evolution. We tackle this problem by analyzing Wikipedia in terms of entity evolutions mentioned in articles. We present statistical data on excerpts covering name changes, which will be used to discover similar text passages and extract evolution knowledge in future work. version:1
arxiv-1702-01172 | Insights into Entity Name Evolution on Wikipedia | http://arxiv.org/abs/1702.01172 | id:1702.01172 author:Helge Holzmann, Thomas Risse category:cs.CL cs.DL  published:2017-02-03 summary:Working with Web archives raises a number of issues caused by their temporal characteristics. Depending on the age of the content, additional knowledge might be needed to find and understand older texts. Especially facts about entities are subject to change. Most severe in terms of information retrieval are name changes. In order to find entities that have changed their name over time, search engines need to be aware of this evolution. We tackle this problem by analyzing Wikipedia in terms of entity evolutions mentioned in articles regardless the structural elements. We gathered statistics and automatically extracted minimum excerpts covering name changes by incorporating lists dedicated to that subject. In future work, these excerpts are going to be used to discover patterns and detect changes in other sources. In this work we investigate whether or not Wikipedia is a suitable source for extracting the required knowledge. version:1
arxiv-1702-01167 | An Analysis of 1-to-First Matching in Iris Recognition | http://arxiv.org/abs/1702.01167 | id:1702.01167 author:Andrey Kuehlkamp, Kevin W. Bowyer category:cs.CV  published:2017-02-03 summary:Iris recognition systems are a mature technology that is widely used throughout the world. In identification (as opposed to verification) mode, an iris to be recognized is typically matched against all N enrolled irises. This is the classic "1-to-N search". In order to improve the speed of large-scale identification, a modified "1-to-First" search has been used in some operational systems. A 1-to-First search terminates with the first below-threshold match that is found, whereas a 1-to-N search always finds the best match across all enrollments. We know of no previous studies that evaluate how the accuracy of 1-to-First search differs from that of 1-to-N search. Using a dataset of over 50,000 iris images from 2,800 different irises, we perform experiments to evaluate the relative accuracy of 1-to-First and 1-to-N search. We evaluate how the accuracy difference changes with larger numbers of enrolled irises, and with larger ranges of rotational difference allowed between iris images. We find that False Match error rate for 1-to-First is higher than for 1-to-N, and the the difference grows with larger number of enrolled irises and with larger range of rotation. version:1
arxiv-1702-01166 | Optimal Subsampling for Large Sample Logistic Regression | http://arxiv.org/abs/1702.01166 | id:1702.01166 author:HaiYing Wang, Rong Zhu, Ping Ma category:stat.CO stat.ME stat.ML  published:2017-02-03 summary:For massive data, the family of subsampling algorithms is popular to downsize the data volume and reduce computational burden. Existing studies focus on approximating the ordinary least squares estimate in linear regression, where statistical leverage scores are often used to define subsampling probabilities. In this paper, we propose fast subsampling algorithms to efficiently approximate the maximum likelihood estimate in logistic regression. We first establish consistency and asymptotic normality of the estimator from a general subsampling algorithm, and then derive optimal subsampling probabilities that minimize the asymptotic mean squared error of the resultant estimator. An alternative minimization criterion is also proposed to further reduce the computational cost. The optimal subsampling probabilities depend on the full data estimate, so we develop a two-step algorithm to approximate the optimal subsampling procedure. This algorithm is computationally efficient and has a significant reduction in computing time compared to the full data approach. Consistency and asymptotic normality of the estimator from a two-step algorithm are also established. Synthetic and real data sets are used to evaluate the practical performance of the proposed method. version:1
arxiv-1701-06549 | Learning to Decode for Future Success | http://arxiv.org/abs/1701.06549 | id:1701.06549 author:Jiwei Li, Will Monroe, Dan Jurafsky category:cs.CL  published:2017-01-23 summary:We introduce a simple, general strategy to manipulate the behavior of a neural decoder that enables it to generate outputs that have specific properties of interest (e.g., sequences of a pre-specified length). The model can be thought of as a simple version of the actor-critic model that uses an interpolation of the actor (the MLE-based token generation policy) and the critic (a value function that estimates the future values of the desired property) for decision making. We demonstrate that the approach is able to incorporate a variety of properties that cannot be handled by standard neural sequence decoders, such as sequence length and backward probability (probability of sources given targets), in addition to yielding consistent improvements in abstractive summarization and machine translation when the property to be optimized is BLEU or ROUGE scores. version:2
arxiv-1702-01147 | Syntax-aware Neural Machine Translation Using CCG | http://arxiv.org/abs/1702.01147 | id:1702.01147 author:Maria Nadejde, Siva Reddy, Rico Sennrich, Tomasz Dwojak, Marcin Junczys-Dowmunt, Philipp Koehn, Alexandra Birch category:cs.CL  published:2017-02-03 summary:Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling syntax improves machine translation quality for English-German, a high-resource pair, and for English-Romanian, a low-resource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. version:1
arxiv-1702-01145 | Query Efficient Posterior Estimation in Scientific Experiments via Bayesian Active Learning | http://arxiv.org/abs/1702.01145 | id:1702.01145 author:Kirthevasan Kandasamy, Jeff Schneider, Barnabás Póczos category:stat.ML  published:2017-02-03 summary:A common problem in disciplines of applied Statistics research such as Astrostatistics is of estimating the posterior distribution of relevant parameters. Typically, the likelihoods for such models are computed via expensive experiments such as cosmological simulations of the universe. An urgent challenge in these research domains is to develop methods that can estimate the posterior with few likelihood evaluations. In this paper, we study active posterior estimation in a Bayesian setting when the likelihood is expensive to evaluate. Existing techniques for posterior estimation are based on generating samples representative of the posterior. Such methods do not consider efficiency in terms of likelihood evaluations. In order to be query efficient we treat posterior estimation in an active regression framework. We propose two myopic query strategies to choose where to evaluate the likelihood and implement them using Gaussian processes. Via experiments on a series of synthetic and real examples we demonstrate that our approach is significantly more query efficient than existing techniques and other heuristics for posterior estimation. version:1
arxiv-1702-01125 | Energy Prediction using Spatiotemporal Pattern Networks | http://arxiv.org/abs/1702.01125 | id:1702.01125 author:Zhanhong Jiang, Chao Liu, Adedotun Akintayo, Gregor Henze, Soumik Sarkar category:stat.ML 60-04  published:2017-02-03 summary:This paper presents a novel data-driven technique based on the spatiotemporal pattern network (STPN) for energy/power prediction for complex dynamical systems. Built on symbolic dynamic filtering, the STPN framework is used to capture not only the individual system characteristics but also the pair-wise causal dependencies among different sub-systems. For quantifying the causal dependency, a mutual information based metric is presented. An energy prediction approach is subsequently proposed based on the STPN framework. For validating the proposed scheme, two case studies are presented, one involving wind turbine power prediction (supply side energy) using the Western Wind Integration data set generated by the National Renewable Energy Laboratory (NREL) for identifying the spatiotemporal characteristics, and the other, residential electric energy disaggregation (demand side energy) using the Building America 2010 data set from NREL for exploring the temporal features. In the energy disaggregation context, convex programming techniques beyond the STPN framework are developed and applied to achieve improved disaggregation performance. version:1
arxiv-1702-01105 | Joint 2D-3D-Semantic Data for Indoor Scene Understanding | http://arxiv.org/abs/1702.01105 | id:1702.01105 author:Iro Armeni, Sasha Sax, Amir R. Zamir, Silvio Savarese category:cs.CV cs.RO  published:2017-02-03 summary:We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000 m2 and contains over 102,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360{\deg} equirectangular images) as well as camera information. It also includes registered raw and semantically an- notated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: http://3Dsemantics.stanford.edu/ version:1
arxiv-1702-01101 | Multilingual Multi-modal Embeddings for Natural Language Processing | http://arxiv.org/abs/1702.01101 | id:1702.01101 author:Iacer Calixto, Qun Liu, Nick Campbell category:cs.CL I.2.7  published:2017-02-03 summary:We propose a novel discriminative model that learns embeddings from multilingual and multi-modal data, meaning that our model can take advantage of images and descriptions in multiple languages to improve embedding quality. To that end, we introduce a modification of a pairwise contrastive estimation optimisation function as our training objective. We evaluate our embeddings on an image-sentence ranking (ISR), a semantic textual similarity (STS), and a neural machine translation (NMT) task. We find that the additional multilingual signals lead to improvements on both the ISR and STS tasks, and the discriminative cost can also be used in re-ranking $n$-best lists produced by NMT models, yielding strong improvements. version:1
arxiv-1702-00414 | Weakly Supervised Classification in High Energy Physics | http://arxiv.org/abs/1702.00414 | id:1702.00414 author:Lucio Mwinmaarong Dery, Benjamin Nachman, Francesco Rubbo, Ariel Schwartzman category:hep-ph physics.data-an stat.ML  published:2017-02-01 summary:As machine learning algorithms become increasingly sophisticated to exploit subtle features of the data, they often become more dependent on simulations. This paper presents a new approach called weakly supervised classification in which class proportions are the only input into the machine learning algorithm. Using one of the most challenging binary classification tasks in high energy physics - quark versus gluon tagging - we show that weakly supervised classification can match the performance of fully supervised algorithms. Furthermore, by design, the new algorithm is insensitive to any mis-modeling of discriminating features in the data by the simulation. Weakly supervised classification is a general procedure that can be applied to a wide variety of learning problems to boost performance and robustness when detailed simulations are not reliable or not available. version:2
arxiv-1702-01090 | Multi-level computational methods for interdisciplinary research in the HathiTrust Digital Library | http://arxiv.org/abs/1702.01090 | id:1702.01090 author:Jaimie Murdock, Colin Allen, Katy Börner, Robert Light, Simon McAlister, Robert Rose, Doori Rose, Jun Otsuka, David Bourget, John Lawrence, Andrew Ravenscroft, Chris Reed category:cs.DL cs.CL cs.IR  published:2017-02-03 summary:We show how faceted search using a combination of traditional classification systems and mixed-membership models can move beyond keyword search to inform resource discovery, hypothesis formulation, and argument extraction for interdisciplinary research. Our test domain is the history and philosophy of scientific work on animal mind and cognition. We demonstrate an application of our methods to the problem of identifying and extracting arguments about anthropomorphism during a critical period in the development of comparative psychology. We show how a combination of classification systems and mixed-membership models trained over large digital libraries can inform resource discovery in this domain, using methods that can be generalized to other interdisciplinary research questions. Through a novel approach of drill-down topic modeling, we are able to reduce a collection of 1,315 fulltext volumes to 6 focal volumes that did not appear in the first ten search results in the HathiTrust digital library. This ultimately supports a system for semi-automatic identification of argument structures to augment the kind of "close reading" that leads to novel interpretations at the heart of scholarly work in the humanities, drilling down from massive quantities of text to very specific passages. This multi-level view advances understanding of the intellectual and societal contexts in which writings are interpreted. version:1
arxiv-1702-01005 | Intrinsic Grassmann Averages for Online Linear and Robust Subspace Learning | http://arxiv.org/abs/1702.01005 | id:1702.01005 author:Rudrasis Chakraborty, Søren Hauberg, Baba C. Vemuri category:cs.LG cs.CV  published:2017-02-03 summary:Principal Component Analysis (PCA) is a fundamental method for estimating a linear subspace approximation to high-dimensional data. Many algorithms exist in literature to achieve a statistically robust version of PCA called RPCA. In this paper, we present a geometric framework for computing the principal linear subspaces in both situations that amounts to computing the intrinsic average on the space of all subspaces (the Grassmann manifold). Points on this manifold are defined as the subspaces spanned by $K$-tuples of observations. We show that the intrinsic Grassmann average of these subspaces coincide with the principal components of the observations when they are drawn from a Gaussian distribution. Similar results are also shown to hold for the RPCA. Further, we propose an efficient online algorithm to do subspace averaging which is of linear complexity in terms of number of samples and has a linear convergence rate. When the data has outliers, our proposed online robust subspace averaging algorithm shows significant performance (accuracy and computation time) gain over a recently published RPCA methods with publicly accessible code. We have demonstrated competitive performance of our proposed online subspace algorithm method on one synthetic and two real data sets. Experimental results depicting stability of our proposed method are also presented. Furthermore, on two real outlier corrupted datasets, we present comparison experiments showing lower reconstruction error using our online RPCA algorithm. In terms of reconstruction error and time required, both our algorithms outperform the competition. version:1
arxiv-1702-01000 | Sharp Convergence Rates for Forward Regression in High-Dimensional Sparse Linear Models | http://arxiv.org/abs/1702.01000 | id:1702.01000 author:Damian Kozbur category:stat.ML  published:2017-02-03 summary:Forward regression is a statistical model selection and estimation procedure which inductively selects covariates that add predictive power into a working statistical regression model. Once a model is selected, unknown regression parameters are estimated by least squares. This paper analyzes forward regression in high-dimensional sparse linear models. Probabilistic bounds for prediction error norm and number of selected covariates are proved. The analysis in this paper gives sharp rates and does not require beta-min or irrepresentability conditions. version:1
arxiv-1702-00993 | Robust Particle Swarm Optimizer based on Chemomimicry | http://arxiv.org/abs/1702.00993 | id:1702.00993 author:Casey Kneale, Karl S. Booksh category:cs.NE  published:2017-02-03 summary:A particle swarm optimizer (PSO) loosely based on the phenomena of crystallization and a chaos factor which follows the complimentary error function is described. The method features three phases: diffusion, directed motion, and nucleation. During the diffusion phase random walk is the only contributor to particle motion. As the algorithm progresses the contribution from chaos decreases and movement toward global best locations is pursued until convergence has occurred. The algorithm was found to be more robust to local minima in multimodal test functions than a standard PSO algorithm and is designed for problems which feature experimental precision. version:1
arxiv-1702-00992 | Automatic Prediction of Discourse Connectives | http://arxiv.org/abs/1702.00992 | id:1702.00992 author:Eric Malmi, Daniele Pighin, Sebastian Krause, Mikhail Kozhevnikov category:cs.CL  published:2017-02-03 summary:Accurate prediction of suitable discourse connectives (however, furthermore, etc.) is a key component of any system aimed at building coherent and fluent discourses from shorter sentences and passages. As an example, a dialog system might assemble a long and informative answer by sampling passages extracted from different documents retrieved from the web. We formulate the task of discourse connective prediction and release a dataset of 2.9M sentence pairs separated by discourse connectives for this task. Then, we evaluate the hardness of the task for human raters, apply a recently proposed decomposable attention (DA) model to this task and observe that the automatic predictor has a higher F1 than human raters (32 vs. 30). Nevertheless, under specific conditions the raters still outperform the DA model, suggesting that there is headroom for future improvements. Finally, we further demonstrate the usefulness of the connectives dataset by showing that it improves implicit discourse relation prediction when used for model pre-training. version:1
arxiv-1701-01672 | Detecting changes in slope with an $L_0$ penalty | http://arxiv.org/abs/1701.01672 | id:1701.01672 author:Robert Maidstone, Paul Fearnhead, Adam Letchford category:stat.CO stat.ME stat.ML  published:2017-01-06 summary:Whilst there are many approaches to detecting changes in mean for a univariate time-series, the problem of detecting multiple changes in slope has comparatively been ignored. Part of the reason for this is that detecting changes in slope is much more challenging. For example, simple binary segmentation procedures do not work for this problem, whilst efficient dynamic programming methods that work well for the change in mean problem cannot be directly used for detecting changes in slope. We present a novel dynamic programming approach, CPOP, for finding the "best" continuous piecewise-linear fit to data. We define best based on a criterion that measures fit to data using the residual sum of squares, but penalises complexity based on an $L_0$ penalty on changes in slope. We show that using such a criterion is more reliable at estimating changepoint locations than approaches that penalise complexity using an $L_1$ penalty. Empirically CPOP has good computational properties, and can analyse a time-series with over 10,000 observations and over 100 changes in a few minutes. Our method is used to analyse data on the motion of bacteria, and provides fits to the data that both have substantially smaller residual sum of squares and are more parsimonious than two competing approaches. version:2
arxiv-1702-00953 | Deep Learning with Low Precision by Half-wave Gaussian Quantization | http://arxiv.org/abs/1702.00953 | id:1702.00953 author:Zhaowei Cai, Xiaodong He, Jian Sun, Nuno Vasconcelos category:cs.CV cs.AI cs.LG  published:2017-02-03 summary:The problem of quantizing the activations of a deep neural network is considered. An examination of the popular binary quantization approach shows that this consists of approximating a classical non-linearity, the hyperbolic tangent, by two functions: a piecewise constant sign function, which is used in feedforward network computations, and a piecewise linear hard tanh function, used in the backpropagation step during network learning. The problem of approximating the ReLU non-linearity, widely used in the recent deep learning literature, is then considered. An half-wave Gaussian quantizer (HWGQ) is proposed for forward approximation and shown to have efficient implementation, by exploiting the statistics of of network activations and batch normalization operations commonly used in the literature. To overcome the problem of gradient mismatch, due to the use of different forward and backward approximations, several piece-wise backward approximators are then investigated. The implementation of the resulting quantized network, denoted as HWGQ-Net, is shown to achieve much closer performance to full precision networks, such as AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision networks, with 1-bit binary weights and 2-bit quantized activations. version:1
arxiv-1701-08475 | Scalable Nearest Neighbor Search based on kNN Graph | http://arxiv.org/abs/1701.08475 | id:1701.08475 author:Wan-Lei Zhao, Jie Yang, Cheng-Hao Deng category:cs.CV cs.DB  published:2017-01-30 summary:Nearest neighbor search is known as a challenging issue that has been studied for several decades. Recently, this issue becomes more and more imminent in viewing that the big data problem arises from various fields. In this paper, a scalable solution based on hill-climbing strategy with the support of k-nearest neighbor graph (kNN) is presented. Two major issues have been considered in the paper. Firstly, an efficient kNN graph construction method based on two means tree is presented. For the nearest neighbor search, an enhanced hill-climbing procedure is proposed, which sees considerable performance boost over original procedure. Furthermore, with the support of inverted indexing derived from residue vector quantization, our method achieves close to 100% recall with high speed efficiency in two state-of-the-art evaluation benchmarks. In addition, a comparative study on both the compressional and traditional nearest neighbor search methods is presented. We show that our method achieves the best trade-off between search quality, efficiency and memory complexity. version:2
arxiv-1702-00932 | A method of limiting performance loss of CNNs in noisy environments | http://arxiv.org/abs/1702.00932 | id:1702.00932 author:James R. Geraci, Parichay Kapoor category:cs.CV  published:2017-02-03 summary:Convolutional Neural Network (CNN) recognition rates drop in the presence of noise. We demonstrate a novel method of counteracting this drop in recognition rate by adjusting the biases of the neurons in the convolutional layers according to the noise conditions encountered at runtime. We compare our technique to training one network for all possible noise levels, dehazing via preprocessing a signal with a denoising autoencoder, and training a network specifically for each noise level. Our system compares favorably in terms of robustness, computational complexity and recognition rate. version:1
arxiv-1702-00926 | FCSS: Fully Convolutional Self-Similarity for Dense Semantic Correspondence | http://arxiv.org/abs/1702.00926 | id:1702.00926 author:Seungryong Kim, Dongbo Min, Bumsub Ham, Sangryul Jeon, Stephen Lin, Kwanghoon Sohn category:cs.CV  published:2017-02-03 summary:We present a descriptor, called fully convolutional self-similarity (FCSS), for dense semantic correspondence. To robustly match points among different instances within the same object class, we formulate FCSS using local self-similarity (LSS) within a fully convolutional network. In contrast to existing CNN-based descriptors, FCSS is inherently insensitive to intra-class appearance variations because of its LSS-based structure, while maintaining the precise localization ability of deep neural networks. The sampling patterns of local structure and the self-similarity measure are jointly learned within the proposed network in an end-to-end and multi-scale manner. As training data for semantic correspondence is rather limited, we propose to leverage object candidate priors provided in existing image datasets and also correspondence consistency between object pairs to enable weakly-supervised learning. Experiments demonstrate that FCSS outperforms conventional handcrafted descriptors and CNN-based descriptors on various benchmarks. version:1
arxiv-1702-00882 | Seeded Laplaican: An Eigenfunction Solution for Scribble Based Interactive Image Segmentation | http://arxiv.org/abs/1702.00882 | id:1702.00882 author:Ahmed Taha, Marwan Torki category:cs.CV  published:2017-02-03 summary:In this paper, we cast the scribble-based interactive image segmentation as a semi-supervised learning problem. Our novel approach alleviates the need to solve an expensive generalized eigenvector problem by approximating the eigenvectors using efficiently computed eigenfunctions. The smoothness operator defined on feature densities at the limit n tends to infinity recovers the exact eigenvectors of the graph Laplacian, where n is the number of nodes in the graph. To further reduce the computational complexity without scarifying our accuracy, we select pivots pixels from user annotations. In our experiments, we evaluate our approach using both human scribble and "robot user" annotations to guide the foreground/background segmentation. We developed a new unbiased collection of five annotated images datasets to standardize the evaluation procedure for any scribble-based segmentation method. We experimented with several variations, including different feature vectors, pivot count and the number of eigenvectors. Experiments are carried out on datasets that contain a wide variety of natural images. We achieve better qualitative and quantitative results compared to state-of-the-art interactive segmentation algorithms. version:1
arxiv-1702-00871 | Deep Learning For Video Saliency Detection | http://arxiv.org/abs/1702.00871 | id:1702.00871 author:Wenguan Wang, Jianbing Shen, Ling Shao category:cs.CV  published:2017-02-02 summary:This paper proposes a deep learning model to efficiently detect salient regions in videos. It addresses two important issues: (1) deep video saliency model training with the absence of sufficiently large and pixel-wise annotated video data; and (2) fast video saliency training and detection. The proposed deep video saliency network consists of two modules, for capturing the spatial and temporal saliency stimuli, respectively. The dynamic saliency model, explicitly incorporating saliency estimates from the static saliency model, directly produces spatiotemporal saliency inference without time-consuming optical flow computation. We further propose a novel data augmentation technique that simulates video training data from existing annotated image datasets, which enables our network to learn diverse saliency stimuli and prevents overfitting with the limited number of training videos. Leveraging our synthetic video data (150K video sequences) and real videos, our deep video saliency model successfully learns both spatial and temporal saliency stimuli, thus producing accurate spatiotemporal saliency estimate. We advance the state-of-the-art on the DAVIS dataset (MAE of .06) and the FBMS dataset (MAE of .07), and do so with much improved speed (2fps with all steps) on one GPU. version:1
arxiv-1702-00860 | Topic Modeling the Hàn diăn Ancient Classics | http://arxiv.org/abs/1702.00860 | id:1702.00860 author:Colin Allen, Hongliang Luo, Jaimie Murdock, Jianghuai Pu, Xiaohong Wang, Yanjie Zhai, Kun Zhao category:cs.CL cs.CY cs.DL cs.HC cs.IR  published:2017-02-02 summary:Ancient Chinese texts present an area of enormous challenge and opportunity for humanities scholars interested in exploiting computational methods to assist in the development of new insights and interpretations of culturally significant materials. In this paper we describe a collaborative effort between Indiana University and Xi'an Jiaotong University to support exploration and interpretation of a digital corpus of over 18,000 ancient Chinese documents, which we refer to as the "Handian" ancient classics corpus (H\`an di\u{a}n g\u{u} j\'i, i.e, the "Han canon" or "Chinese classics"). It contains classics of ancient Chinese philosophy, documents of historical and biographical significance, and literary works. We begin by describing the Digital Humanities context of this joint project, and the advances in humanities computing that made this project feasible. We describe the corpus and introduce our application of probabilistic topic modeling to this corpus, with attention to the particular challenges posed by modeling ancient Chinese documents. We give a specific example of how the software we have developed can be used to aid discovery and interpretation of themes in the corpus. We outline more advanced forms of computer-aided interpretation that are also made possible by the programming interface provided by our system, and the general implications of these methods for understanding the nature of meaning in these texts. version:1
arxiv-1702-00852 | Guided Signal Reconstruction Theory | http://arxiv.org/abs/1702.00852 | id:1702.00852 author:Andrew Knyazev, Akshay Gadde, Hassan Mansour, Dong Tian category:cs.IT math.FA math.IT stat.ML  published:2017-02-02 summary:An axiomatic approach to signal reconstruction is formulated, involving a sample consistent set and a guiding set, describing desired reconstructions. New frame-less reconstruction methods are proposed, based on a novel concept of a reconstruction set, defined as a shortest pathway between the sample consistent set and the guiding set. Existence and uniqueness of the reconstruction set are investigated in a Hilbert space, where the guiding set is a closed subspace and the sample consistent set is a closed plane, formed by a sampling subspace. Connections to earlier known consistent, generalized, and regularized reconstructions are clarified. New stability and reconstruction error bounds are derived, using the largest nontrivial angle between the sampling and guiding subspaces. Conjugate gradient iterative reconstruction algorithms are proposed and illustrated numerically for image magnification. version:1
arxiv-1702-00837 | Eye-Movement behavior identification for AD diagnosis | http://arxiv.org/abs/1702.00837 | id:1702.00837 author:Juan Biondi, Gerardo Fernandez, Silvia Castro, Osvaldo Agamenonni category:cs.NE q-bio.NC  published:2017-02-02 summary:In the present work, we develop a deep-learning approach for differentiating the eye-movement behavior of people with neurodegenerative diseases over healthy control subjects during reading well-defined sentences. We define an information compaction of the eye-tracking data of subjects without and with probable Alzheimer's disease when reading a set of well-defined, previously validated, sentences including high-, low-predictable sentences, and proverbs. Using this information we train a set of denoising sparse-autoencoders and build a deep neural network with these and a softmax classifier. Our results are very promising and show that these models may help to understand the dynamics of eye movement behavior and its relationship with underlying neuropsychological correlates. version:1
arxiv-1702-00833 | Recurrent Neural Networks for anomaly detection in the Post-Mortem time series of LHC superconducting magnets | http://arxiv.org/abs/1702.00833 | id:1702.00833 author:Maciej Wielgosz, Andrzej Skoczeń, Matej Mertik category:physics.ins-det cs.LG physics.acc-ph  published:2017-02-02 summary:This paper presents a model based on Deep Learning algorithms of LSTM and GRU for facilitating an anomaly detection in Large Hadron Collider superconducting magnets. We used high resolution data available in Post Mortem database to train a set of models and chose the best possible set of their hyper-parameters. Using Deep Learning approach allowed to examine a vast body of data and extract the fragments which require further experts examination and are regarded as anomalies. The presented method does not require tedious manual threshold setting and operator attention at the stage of the system setup. Instead, the automatic approach is proposed, which achieves according to our experiments accuracy of 99%. This is reached for the largest dataset of 302 MB and the following architecture of the network: single layer LSTM, 128 cells, 20 epochs of training, look_back=16, look_ahead=128, grid=100 and optimizer Adam. All the experiments were run on GPU Nvidia Tesla K80 version:1
arxiv-1702-00832 | An Introduction to Machine Learning Communications Systems | http://arxiv.org/abs/1702.00832 | id:1702.00832 author:Timothy J. O'Shea, Jakob Hoydis category:cs.IT cs.LG cs.NI math.IT  published:2017-02-02 summary:We introduce and motivate machine learning (ML) communications systems that aim to improve on and to even replace the vast expert knowledge in the field of communications using modern machine learning techniques. These have recently achieved breakthroughs in many different domains, but not yet in communications. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about radio communications system design as an end-to-end reconstruction optimization task that seeks to jointly optimize transmitter and receiver components in a single process. We further present the concept of Radio Transformer Networks (RTNs) as a means to incorporate expert domain knowledge in the ML model and study the application of convolutional neural networks (CNNs) on raw IQ time-series data for modulation classification. We conclude the paper with a deep discussion of open challenges and areas for future investigation. version:1
arxiv-1702-00783 | Pixel Recursive Super Resolution | http://arxiv.org/abs/1702.00783 | id:1702.00783 author:Ryan Dahl, Mohammad Norouzi, Jonathon Shlens category:cs.CV cs.LG  published:2017-02-02 summary:We present a pixel recursive super resolution model that synthesizes realistic details into images while enhancing their resolution. A low resolution image may correspond to multiple plausible high resolution images, thus modeling the super resolution process with a pixel independent conditional model often results in averaging different details--hence blurry edges. By contrast, our model is able to represent a multimodal conditional distribution by properly modeling the statistical dependencies among the high resolution image pixels, conditioned on a low resolution input. We employ a PixelCNN architecture to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network. Human evaluations indicate that samples from our proposed model look more photo realistic than a strong L2 regression baseline. version:1
arxiv-1702-00768 | Scaling Properties of Human Brain Functional Networks | http://arxiv.org/abs/1702.00768 | id:1702.00768 author:Riccardo Zucca, Xerxes D. Arsiwalla, Hoang Le, Mikail Rubinov, Paul Verschure category:q-bio.NC cond-mat.dis-nn cs.NE physics.data-an  published:2017-02-02 summary:We investigate scaling properties of human brain functional networks in the resting-state. Analyzing network degree distributions, we statistically test whether their tails scale as power-law or not. Initial studies, based on least-squares fitting, were shown to be inadequate for precise estimation of power-law distributions. Subsequently, methods based on maximum-likelihood estimators have been proposed and applied to address this question. Nevertheless, no clear consensus has emerged, mainly because results have shown substantial variability depending on the data-set used or its resolution. In this study, we work with high-resolution data (10K nodes) from the Human Connectome Project and take into account network weights. We test for the power-law, exponential, log-normal and generalized Pareto distributions. Our results show that the statistics generally do not support a power-law, but instead these degree distributions tend towards the thin-tail limit of the generalized Pareto model. This may have implications for the number of hubs in human brain functional networks. version:1
arxiv-1702-00764 | Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey | http://arxiv.org/abs/1702.00764 | id:1702.00764 author:Lorenzo Ferrone, Fabio Massimo Zanzotto category:cs.CL 68T05  68T50 I.2.7; I.2.6  published:2017-02-02 summary:Natural language and symbols are intimately correlated. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. In this paper we make a survey that aims to draw the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks. version:1
arxiv-1702-00763 | Natasha: Faster Stochastic Non-Convex Optimization via Strongly Non-Convex Parameter | http://arxiv.org/abs/1702.00763 | id:1702.00763 author:Zeyuan Allen-Zhu category:math.OC cs.DS cs.LG stat.ML  published:2017-02-02 summary:Given a non-convex function $f(x)$ that is an average of $n$ smooth functions, we design stochastic first-order methods to find its approximate stationary points. The performance of our new methods depend on the smallest (negative) eigenvalue $-\sigma$ of the Hessian. This parameter $\sigma$ captures how strongly non-convex $f(x)$ is, and is analogous to the strong convexity parameter for convex optimization. Our methods outperform the best known results for a wide range of $\sigma$, and can also be used to find approximate local minima. In particular, we find an interesting dichotomy: there exists a threshold $\sigma_0$ so that the fastest methods for $\sigma>\sigma_0$ and for $\sigma<\sigma_0$ have drastically different behaviors: the former scales with $n^{2/3}$ and the latter scales with $n^{3/4}$. version:1
arxiv-1702-00758 | HashNet: Deep Learning to Hash by Continuation | http://arxiv.org/abs/1702.00758 | id:1702.00758 author:Zhangjie Cao, Mingsheng Long, Jianmin Wang, Philip S. Yu category:cs.LG cs.CV  published:2017-02-02 summary:Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality by end-to-end representation learning and hash encoding, has received increasing attention recently. Subject to the vanishing gradient difficulty in the optimization with binary activations, existing deep learning to hash methods need to first learn continuous representations and then generate binary hash codes in a separated binarization step, which suffer from substantial loss of retrieval quality. This paper presents HashNet, a novel deep architecture for deep learning to hash by continuation method, which learns exactly binary hash codes from imbalanced similarity data where the number of similar pairs is much smaller than the number of dissimilar pairs. The key idea is to attack the vanishing gradient problem in optimizing deep networks with non-smooth binary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks. version:1
arxiv-1702-00754 | Maritime situational awareness using adaptive multi-sensor management under hazy conditions | http://arxiv.org/abs/1702.00754 | id:1702.00754 author:D. K. Prasad, C. K. Prasath, D. Rajan, L. Rachmawati, E. Rajabally, C. Quek category:cs.CV  published:2017-02-02 summary:This paper presents a multi-sensor architecture with an adaptive multi-sensor management system suitable for control and navigation of autonomous maritime vessels in hazy and poor-visibility conditions. This architecture resides in the autonomous maritime vessels. It augments the data from on-board imaging sensors and weather sensors with the AIS data and weather data from sensors on other vessels and the on-shore vessel traffic surveillance system. The combined data is analyzed using computational intelligence and data analytics to determine suitable course of action while utilizing historically learnt knowledge and performing live learning from the current situation. Such framework is expected to be useful in diverse weather conditions and shall be a useful architecture to provide autonomy to maritime vessels. version:1
arxiv-1702-00748 | QCD-Aware Recursive Neural Networks for Jet Physics | http://arxiv.org/abs/1702.00748 | id:1702.00748 author:Gilles Louppe, Kyunghyun Cho, Cyril Becot, Kyle Cranmer category:hep-ph physics.data-an stat.ML  published:2017-02-02 summary:Recent progress in applying machine learning for jet physics has been built upon an analogy between calorimeters and images. In this work, we present a novel class of recursive neural networks built instead upon an analogy between QCD and natural languages. In the analogy, four-momenta are like words and the clustering history of sequential recombination jet algorithms is like the parsing of a sentence. Our approach works directly with the four-momenta of a variable-length set of particles, and the jet-based tree structure varies on an event-by-event basis. Our experiments highlight the flexibility of our method for building task-specific jet embeddings and show that recursive architectures are significantly more accurate and data efficient than previous image-based networks. We extend the analogy from individual jets (sentences) to full events (paragraphs), and show for the first time an event-level classifier operating on all the stable particles produced in an LHC event. version:1
arxiv-1702-00716 | Analysing Temporal Evolution of Interlingual Wikipedia Article Pairs | http://arxiv.org/abs/1702.00716 | id:1702.00716 author:Simon Gottschalk, Elena Demidova category:cs.CL  published:2017-02-02 summary:Wikipedia articles representing an entity or a topic in different language editions evolve independently within the scope of the language-specific user communities. This can lead to different points of views reflected in the articles, as well as complementary and inconsistent information. An analysis of how the information is propagated across the Wikipedia language editions can provide important insights in the article evolution along the temporal and cultural dimensions and support quality control. To facilitate such analysis, we present MultiWiki - a novel web-based user interface that provides an overview of the similarities and differences across the article pairs originating from different language editions on a timeline. MultiWiki enables users to observe the changes in the interlingual article similarity over time and to perform a detailed visual comparison of the article snapshots at a particular time point. version:1
arxiv-1702-00714 | Learning a time-dependent master saliency map from eye-tracking data in videos | http://arxiv.org/abs/1702.00714 | id:1702.00714 author:Antoine Coutrot, Nathalie Guyader category:cs.CV  published:2017-02-02 summary:To predict the most salient regions of complex natural scenes, saliency models commonly compute several feature maps (contrast, orientation, motion...) and linearly combine them into a master saliency map. Since feature maps have different spatial distribution and amplitude dynamic ranges, determining their contributions to overall saliency remains an open problem. Most state-of-the-art models do not take time into account and give feature maps constant weights across the stimulus duration. However, visual exploration is a highly dynamic process shaped by many time-dependent factors. For instance, some systematic viewing patterns such as the center bias are known to dramatically vary across the time course of the exploration. In this paper, we use maximum likelihood and shrinkage methods to dynamically and jointly learn feature map and systematic viewing pattern weights directly from eye-tracking data recorded on videos. We show that these weights systematically vary as a function of time, and heavily depend upon the semantic visual category of the videos being processed. Our fusion method allows taking these variations into account, and outperforms other state-of-the-art fusion schemes using constant weights over time. The code, videos and eye-tracking data we used for this study are available online: http://antoinecoutrot.magix.net/public/research.html version:1
arxiv-1702-00709 | IQN: An Incremental Quasi-Newton Method with Local Superlinear Convergence Rate | http://arxiv.org/abs/1702.00709 | id:1702.00709 author:Aryan Mokhtari, Mark Eisen, Alejandro Ribeiro category:math.OC cs.LG  published:2017-02-02 summary:This paper studies the problem of minimizing a global objective function which can be written as the average of a set of $n$ smooth and strongly convex functions. Quasi-Newton methods, which build on the idea of approximating the Newton step using the first-order information of the objective function, are successful in reducing the computational complexity of Newton's method by avoiding the Hessian and its inverse computation at each iteration, while converging at a superlinear rate to the optimal argument. However, quasi-Newton methods are impractical for solving the finite sum minimization problem since they operate on the information of all $n$ functions at each iteration. This issue has been addressed by incremental quasi-Newton methods which use the information of a subset of functions at each iteration. Although incremental quasi-Newton methods are able to reduce the computational complexity of traditional quasi-Newton methods significantly, they fail to converge at a superlinear rate. In this paper, we propose the IQN method as the first incremental quasi-Newton method with a local superlinear convergence rate. In IQN, we compute and update the information of only a single function at each iteration and use the gradient information to approximate the Newton direction without a computationally expensive inversion. IQN differs from state-of-the-art incremental quasi-Newton methods in three criteria. First, the use of aggregated information of variables, gradients, and quasi-Newton Hessian approximations; second, the approximation of each individual function by its Taylor's expansion in which the linear and quadratic terms are evaluated with respect to the same iterate; and third, the use of a cyclic scheme to update the functions in lieu of a random selection routine. We use these fundamental properties of IQN to establish its local superlinear convergence rate. version:1
arxiv-1702-00700 | Multilingual and Cross-lingual Timeline Extraction | http://arxiv.org/abs/1702.00700 | id:1702.00700 author:Egoitz Laparra, Rodrigo Agerri, Itziar Aldabe, German Rigau category:cs.CL cs.AI  published:2017-02-02 summary:In this paper we present an approach to extract ordered timelines of events, their participants, locations and times from a set of multilingual and cross-lingual data sources. Based on the assumption that event-related information can be recovered from different documents written in different languages, we extend the Cross-document Event Ordering task presented at SemEval 2015 by specifying two new tasks for, respectively, Multilingual and Cross-lingual Timeline Extraction. We then develop three deterministic algorithms for timeline extraction based on two main ideas. First, we address implicit temporal relations at document level since explicit time-anchors are too scarce to build a wide coverage timeline extraction system. Second, we leverage several multilingual resources to obtain a single, inter-operable, semantic representation of events across documents and across languages. The result is a highly competitive system that strongly outperforms the current state-of-the-art. Nonetheless, further analysis of the results reveals that linking the event mentions with their target entities and time-anchors remains a difficult challenge. The systems, resources and scorers are freely available to facilitate its use and guarantee the reproducibility of results. version:1
arxiv-1702-00648 | Side Information in Robust Principle Component Analysis: Algorithms and Applications | http://arxiv.org/abs/1702.00648 | id:1702.00648 author:Niannan Xue, Yannis Panagakis, Stefanos Zafeiriou category:cs.CV  published:2017-02-02 summary:Robust rank minimisation aims at recovering a low-rank subspace from grossly corrupted high-dimensional (often visual) data and is a cornerstone in many machine learning and computer vision applications. The most prominent method for this task is the Robust Principal Component Analysis (PCA). It recovers a low-rank matrix from sparse corruptions of unknown magnitude and support by Principal Component Pursuit (PCP), which is a convex approximation to the otherwise NP-hard rank minimisation problem. Even though PCP has been shown to be very successful in solving many rank minimisation problems, there are cases where degenerate or suboptimal solutions are obtained. This can be attributed to the fact that domain-dependent prior knowledge is not taken into account by PCP. In this paper, we address the problem of PCP when prior information is available. To this end, we propose algorithms for solving the PCP problem with the aid of prior information on the low-rank structure of the data. The versatility of the proposed methods is demonstrated by applying them to four applications, namely background substraction, facial image denoising, face and facial expression recognition. Experimental results on synthetic and five real world datasets indicate the robustness and effectiveness of the proposed methods on these application domains, largely outperforming previous approaches that incorporate side information within Robust PCA. version:1
arxiv-1702-00615 | A Fast and Compact Salient Score Regression Network Based on Fully Convolutional Network | http://arxiv.org/abs/1702.00615 | id:1702.00615 author:Xuanyang Xi, Yongkang Luo, Fengfu Li, Peng Wang, Hong Qiao category:cs.CV  published:2017-02-02 summary:Visual saliency detection aims at identifying the most visually distinctive parts in an image, and serves as a pre-processing step for a variety of computer vision and image processing tasks. To this end, the saliency detection procedure must be as fast and compact as possible and optimally processes input images in a real time manner. However, contemporary detection methods always take hundreds of milliseconds to pursue feeble improvements on the detection precession. In this paper, we tackle this problem by proposing a fast and compact salient score regression network which employs deep convolutional neural networks (CNN) to estimate the saliency of objects in images. It operates (including training and testing) in an end-to-end manner (image-to-image prediction) and also directly produces whole saliency maps from original images without any pre-processings and post-processings. Comparing with contemporary CNN-based saliency detection methods, the proposed method extremely simplifies the detection procedure and further promotes the representation ability of CNN for the saliency detection. Our method is evaluated on six public datasets, and experimental results show that the precision can be comparable to the published state-of-the-art methods while the speed gets a significant improvement (35 FPS, processing in real time). version:1
arxiv-1702-00614 | Learning Criticality in an Embodied Boltzmann Machine | http://arxiv.org/abs/1702.00614 | id:1702.00614 author:Miguel Aguilera, Manuel G. Bedia category:nlin.AO cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC  published:2017-02-02 summary:Many biological and cognitive systems do not operate deep into one or other regime of activity. Instead, they exploit critical surfaces poised at transitions in their parameter space. The pervasiveness of criticality in natural systems suggests that there may be general principles inducing this behaviour. However, there is a lack of conceptual models explaining how embodied agents propel themselves towards these critical points. In this paper, we present a learning model driving an embodied Boltzmann Machine towards critical behaviour by maximizing the heat capacity of the network. We test and corroborate the model implementing an embodied agent in the mountain car benchmark, controlled by a Boltzmann Machine that adjust its weights according to the model. We find that the neural controller reaches a point of criticality, which coincides with a transition point of the behaviour of the agent between two regimes of behaviour, maximizing the synergistic information between its sensors and the hidden and motor neurons. Finally, we discuss the potential of our learning model to study the contribution of criticality to the behaviour of embodied living systems in scenarios not necessarily constrained by biological restrictions of the examples of criticality we find in nature. version:1
arxiv-1702-00610 | Optimal Schemes for Discrete Distribution Estimation under Locally Differential Privacy | http://arxiv.org/abs/1702.00610 | id:1702.00610 author:Min Ye, Alexander Barg category:cs.LG cs.IT math.IT  published:2017-02-02 summary:We consider the minimax estimation problem of a discrete distribution with support size $k$ under privacy constraints. A privatization scheme is applied to each raw sample independently, and we need to estimate the distribution of the raw samples from the privatized samples. A positive number $\epsilon$ measures the privacy level of a privatization scheme. For a given $\epsilon,$ we consider the problem of constructing optimal privatization schemes with $\epsilon$-privacy level, i.e., schemes that minimize the expected estimation loss for the worst-case distribution. Two schemes in the literature provide order optimal performance in the high privacy regime where $\epsilon$ is very close to $0,$ and in the low privacy regime where $e^{\epsilon}\approx k,$ respectively. In this paper, we propose a new family of schemes which substantially improve the performance of the existing schemes in the medium privacy regime when $1\ll e^{\epsilon} \ll k.$ More concretely, we prove that when $3.8 < \epsilon <\ln(k/9) ,$ our schemes reduce the expected estimation loss by $50\%$ under $\ell_2^2$ metric and by $30\%$ under $\ell_1$ metric over the existing schemes. We also prove a lower bound for the region $e^{\epsilon} \ll k,$ which implies that our schemes are order optimal in this regime. version:1
arxiv-1702-00061 | Vertical Landing for Micro Air Vehicles using Event-Based Optical Flow | http://arxiv.org/abs/1702.00061 | id:1702.00061 author:Bas J. Pijnacker Hordijk, Kirk Y. W. Scheper, Guido C. H. E. de Croon category:cs.CV  published:2017-01-31 summary:Small flying robots can perform landing maneuvers using bio-inspired optical flow by maintaining a constant divergence. However, optical flow is typically estimated from frame sequences recorded by standard miniature cameras. This requires processing full images on-board, limiting the update rate of divergence measurements, and thus the speed of the control loop and the robot. Event-based cameras overcome these limitations by only measuring pixel-level brightness changes at microsecond temporal accuracy, hence providing an efficient mechanism for optical flow estimation. This paper presents, to the best of our knowledge, the first work integrating event-based optical flow estimation into the control loop of a flying robot. We extend an existing 'local plane fitting' algorithm to obtain an improved and more computationally efficient optical flow estimation method, valid for a wide range of optical flow velocities. This method is validated for real event sequences. In addition, a method for estimating the divergence from event-based optical flow is introduced, which accounts for the aperture problem. The developed algorithms are implemented in a constant divergence landing controller on-board of a quadrotor. Experiments show that, using event-based optical flow, accurate divergence estimates can be obtained over a wide range of speeds. This enables the quadrotor to perform very fast landing maneuvers. version:2
arxiv-1702-00583 | Automating Image Analysis by Annotating Landmarks with Deep Neural Networks | http://arxiv.org/abs/1702.00583 | id:1702.00583 author:Mikhail Breslav, Tyson L. Hedrick, Stan Sclaroff, Margrit Betke category:cs.CV  published:2017-02-02 summary:Image and video analysis is often a crucial step in the study of animal behavior and kinematics. Often these analyses require that the position of one or more animal landmarks are annotated (marked) in numerous images. The process of annotating landmarks can require a significant amount of time and tedious labor, which motivates the need for algorithms that can automatically annotate landmarks. In the community of scientists that use image and video analysis to study the 3D flight of animals, there has been a trend of developing more automated approaches for annotating landmarks, yet they fall short of being generally applicable. Inspired by the success of Deep Neural Networks (DNNs) on many problems in the field of computer vision, we investigate how suitable DNNs are for accurate and automatic annotation of landmarks in video datasets representative of those collected by scientists studying animals. Our work shows, through extensive experimentation on videos of hawkmoths, that DNNs are suitable for automatic and accurate landmark localization. In particular, we show that one of our proposed DNNs is more accurate than the current best algorithm for automatic localization of landmarks on hawkmoth videos. Moreover, we demonstrate how these annotations can be used to quantitatively analyze the 3D flight of a hawkmoth. To facilitate the use of DNNs by scientists from many different fields, we provide a self contained explanation of what DNNs are, how they work, and how to apply them to other datasets using the freely available library Caffe and supplemental code that we provide. version:1
arxiv-1701-01941 | Multi-Objective Software Suite of Two-Dimensional Shape Descriptors for Object-Based Image Analysis | http://arxiv.org/abs/1701.01941 | id:1701.01941 author:Andrea Baraldi, João V. B. Soares category:cs.CV  published:2017-01-08 summary:In recent years two sets of planar (2D) shape attributes, provided with an intuitive physical meaning, were proposed to the remote sensing community by, respectively, Nagao & Matsuyama and Shackelford & Davis in their seminal works on the increasingly popular geographic object based image analysis (GEOBIA) paradigm. These two published sets of intuitive geometric features were selected as initial conditions by the present R&D software project, whose multi-objective goal was to accomplish: (i) a minimally dependent and maximally informative design (knowledge/information representation) of a general purpose, user and application independent dictionary of 2D shape terms provided with a physical meaning intuitive to understand by human end users and (ii) an effective (accurate, scale invariant, easy to use) and efficient implementation of 2D shape descriptors. To comply with the Quality Assurance Framework for Earth Observation guidelines, the proposed suite of geometric functions is validated by means of a novel quantitative quality assurance policy, centered on inter feature dependence (causality) assessment. This innovative multivariate feature validation strategy is alternative to traditional feature selection procedures based on either inductive data learning classification accuracy estimation, which is inherently case specific, or cross correlation estimation, because statistical cross correlation does not imply causation. The project deliverable is an original general purpose software suite of seven validated off the shelf 2D shape descriptors intuitive to use. Alternative to existing commercial or open source software libraries of tens of planar shape functions whose informativeness remains unknown, it is eligible for use in (GE)OBIA systems in operating mode, expected to mimic human reasoning based on a convergence of evidence approach. version:2
arxiv-1702-00167 | SMPOST: Parts of Speech Tagger for Code-Mixed Indic Social Media Text | http://arxiv.org/abs/1702.00167 | id:1702.00167 author:Deepak Gupta, Shubham Tripathi, Asif Ekbal, Pushpak Bhattacharyya category:cs.CL  published:2017-02-01 summary:Use of social media has grown dramatically during the last few years. Users follow informal languages in communicating through social media. The language of communication is often mixed in nature, where people transcribe their regional language with English and this technique is found to be extremely popular. Natural language processing (NLP) aims to infer the information from these text where Part-of-Speech (PoS) tagging plays an important role in getting the prosody of the written text. For the task of PoS tagging on Code-Mixed Indian Social Media Text, we develop a supervised system based on Conditional Random Field classifier. In order to tackle the problem effectively, we have focused on extracting rich linguistic features. We participate in three different language pairs, ie. English-Hindi, English-Bengali and English-Telugu on three different social media platforms, Twitter, Facebook & WhatsApp. The proposed system is able to successfully assign coarse as well as fine-grained PoS tag labels for a given a code-mixed sentence. Experiments show that our system is quite generic that shows encouraging performance levels on all the three language pairs in all the domains. version:2
arxiv-1702-00564 | Modelling dependency completion in sentence comprehension as a Bayesian hierarchical mixture process: A case study involving Chinese relative clauses | http://arxiv.org/abs/1702.00564 | id:1702.00564 author:Shravan Vasishth, Nicolas Chopin, Robin Ryder, Bruno Nicenboim category:stat.AP cs.CL stat.ME stat.ML  published:2017-02-02 summary:In sentence comprehension, it is widely assumed (Gibson 2000, Lewis & Vasishth, 2005) that the distance between linguistic co-dependents affects the latency of dependency resolution: the longer the distance, the longer the retrieval time (the distance-based account). An alternative theory of dependency resolution difficulty is the direct-access model (McElree et al., 2003); this model assumes that retrieval times are a mixture of two distributions: one distribution represents successful retrieval and the other represents an initial failure to retrieve the correct dependent, followed by a reanalysis that leads to successful retrieval. The time needed for a successful retrieval is independent of the dependency distance (cf. the distance-based account), but reanalyses cost extra time, and the proportion of failures increases with increasing dependency distance. We implemented a series of increasingly complex hierarchical Bayesian models to compare the distance-based account and the direct-access model; the latter was implemented as a hierarchical finite mixture model with heterogeneous variances for the two mixture distributions. We evaluated the models using two published data-sets on Chinese relative clauses which have been used to argue in favour of the distance account, but this account has found little support in subsequent work (e.g., J\"ager et al., 2015). The hierarchical finite mixture model, i.e., an implementation of direct-access, is shown to provide a superior account of the data than the distance account. version:1
arxiv-1701-00874 | Neural Probabilistic Model for Non-projective MST Parsing | http://arxiv.org/abs/1701.00874 | id:1701.00874 author:Xuezhe Ma, Eduard Hovy category:cs.CL cs.LG stat.ML  published:2017-01-04 summary:In this paper, we propose a probabilistic parsing model, which defines a proper conditional probability distribution over non-projective dependency trees for a given sentence, using neural representations as inputs. The neural network architecture is based on bi-directional LSTM-CNNs which benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM and CNN. On top of the neural network, we introduce a probabilistic structured layer, defining a conditional log-linear model over non-projective trees. We evaluate our model on 17 different datasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree Theorem (Tutte, 1984), the partition functions and marginals can be computed efficiently, leading to a straight-forward end-to-end model training procedure via back-propagation. Our parser achieves state-of-the-art parsing performance on nine datasets. version:2
arxiv-1702-00523 | Deep Learning the Indus Script | http://arxiv.org/abs/1702.00523 | id:1702.00523 author:Satish Palaniappan, Ronojoy Adhikari category:cs.CV cs.CL cs.LG  published:2017-02-02 summary:Standardized corpora of undeciphered scripts, a necessary starting point for computational epigraphy, requires laborious human effort for their preparation from raw archaeological records. Automating this process through machine learning algorithms can be of significant aid to epigraphical research. Here, we take the first steps in this direction and present a deep learning pipeline that takes as input images of the undeciphered Indus script, as found in archaeological artifacts, and returns as output a string of graphemes, suitable for inclusion in a standard corpus. The image is first decomposed into regions using Selective Search and these regions are classified as containing textual and/or graphical information using a convolutional neural network. Regions classified as potentially containing text are hierarchically merged and trimmed to remove non-textual information. The remaining textual part of the image is segmented using standard image processing techniques to isolate individual graphemes. This set is finally passed to a second convolutional neural network to classify the graphemes, based on a standard corpus. The classifier can identify the presence or absence of the most frequent Indus grapheme, the "jar" sign, with an accuracy of 92%. Our results demonstrate the great potential of deep learning approaches in computational epigraphy and, more generally, in the digital humanities. version:1
arxiv-1702-00518 | Recovering True Classifier Performance in Positive-Unlabeled Learning | http://arxiv.org/abs/1702.00518 | id:1702.00518 author:Shantanu Jain, Martha White, Predrag Radivojac category:stat.ML cs.LG  published:2017-02-02 summary:A common approach in positive-unlabeled learning is to train a classification model between labeled and unlabeled data. This strategy is in fact known to give an optimal classifier under mild conditions; however, it results in biased empirical estimates of the classifier performance. In this work, we show that the typically used performance measures such as the receiver operating characteristic curve, or the precision-recall curve obtained on such data can be corrected with the knowledge of class priors; i.e., the proportions of the positive and negative examples in the unlabeled data. We extend the results to a noisy setting where some of the examples labeled positive are in fact negative and show that the correction also requires the knowledge of the proportion of noisy examples in the labeled positives. Using state-of-the-art algorithms to estimate the positive class prior and the proportion of noise, we experimentally evaluate two correction approaches and demonstrate their efficacy on real-life data. version:1
arxiv-1702-00509 | Segmentation of optic disc, fovea and retinal vasculature using a single convolutional neural network | http://arxiv.org/abs/1702.00509 | id:1702.00509 author:Jen Hong Tan, U. Rajendra Acharya, Sulatha V. Bhandary, Kuang Chua Chua, Sobha Sivaprasad category:cs.CV cs.LG  published:2017-02-02 summary:We have developed and trained a convolutional neural network to automatically and simultaneously segment optic disc, fovea and blood vessels. Fundus images were normalised before segmentation was performed to enforce consistency in background lighting and contrast. For every effective point in the fundus image, our algorithm extracted three channels of input from the neighbourhood of the point and forward the response across the 7 layer network. In average, our segmentation achieved an accuracy of 92.68 percent on the testing set from Drive database. version:1
arxiv-1702-00506 | Solving Uncalibrated Photometric Stereo Using Fewer Images by Jointly Optimizing Low-rank Matrix Completion and Integrability | http://arxiv.org/abs/1702.00506 | id:1702.00506 author:Soumyadip Sengupta, Hao Zhou, Walter Forkel, Ronen Basri, Tom Goldstein, David W. Jacobs category:cs.CV  published:2017-02-02 summary:We introduce a new, integrated approach to uncalibrated photometric stereo. We perform 3D reconstruction of Lambertian objects using multiple images produced by unknown, directional light sources. We show how to formulate a single optimization that includes rank and integrability constraints, allowing also for missing data. We then solve this optimization using the Alternate Direction Method of Multipliers (ADMM). We conduct extensive experimental evaluation on real and synthetic data sets. Our integrated approach is particularly valuable when performing photometric stereo using as few as 4-6 images, since the integrability constraint is capable of improving estimation of the linear subspace of possible solutions. We show good improvements over prior work in these cases. version:1
arxiv-1702-00505 | Algorithmic Performance-Accuracy Trade-off in 3D Vision Applications Using HyperMapper | http://arxiv.org/abs/1702.00505 | id:1702.00505 author:Luigi Nardi, Bruno Bodin, Sajad Saeedi, Emanuele Vespa, Andrew J. Davison, Paul H. J. Kelly category:cs.CV cs.DC cs.LG cs.PF  published:2017-02-02 summary:In this paper we investigate an emerging application, 3D scene understanding, likely to be significant in the mobile space in the near future. The goal of this exploration is to reduce execution time while meeting our quality of result objectives. In previous work we showed for the first time that it is possible to map this application to power constrained embedded systems, highlighting that decision choices made at the algorithmic design-level have the most impact. As the algorithmic design space is too large to be exhaustively evaluated, we use a previously introduced multi-objective Random Forest Active Learning prediction framework dubbed HyperMapper, to find good algorithmic designs. We show that HyperMapper generalizes on a recent cutting edge 3D scene understanding algorithm and on a modern GPU-based computer architecture. HyperMapper is able to beat an expert human hand-tuning the algorithmic parameters of the class of Computer Vision applications taken under consideration in this paper automatically. In addition, we use crowd-sourcing using a 3D scene understanding Android app to show that the Pareto front obtained on an embedded system can be used to accelerate the same application on all the 83 smart-phones and tablets crowd-sourced with speedups ranging from 2 to over 12. version:1
arxiv-1702-00503 | Learning to Compose with Professional Photographs on the Web | http://arxiv.org/abs/1702.00503 | id:1702.00503 author:Yi-Ling Chen, Jan Klopp, Min Sun, Shao-Yi Chien, Kwan-Liu Ma category:cs.CV  published:2017-02-01 summary:Photo composition is an important factor affecting the aesthetics in photography. However, it is a highly challenging task to model the aesthetic properties of good compositions due to the lack of globally applicable rules to the wide variety of photographic styles. Inspired by the thinking process of photo taking, we treat the photo composition problem as a view finding process which successively examines pairs of views and determines the aesthetic preference. Without devising complex hand-crafted features, the ranking model is built upon a deep convolutional neural network through joint representation learning from raw pixels. Exploiting rich professional photographs on the web as data source, we devise a nearly unsupervised approach to generate unlimited high quality image pairs for training the network. The resulting ranking model is generic and without any heuristics. The experimental results show that the proposed view finding network achieves state-of-the-art performance with simple sliding window search strategy on two image cropping datasets. version:1
arxiv-1701-08893 | Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses | http://arxiv.org/abs/1701.08893 | id:1701.08893 author:Eric Risser, Pierre Wilmot, Connelly Barnes category:cs.GR cs.CV cs.NE  published:2017-01-31 summary:Recently, methods have been proposed that perform texture synthesis and style transfer by using convolutional neural networks (e.g. Gatys et al. [2015,2016]). These methods are exciting because they can in some cases create results with state-of-the-art quality. However, in this paper, we show these methods also have limitations in texture quality, stability, requisite parameter tuning, and lack of user controls. This paper presents a multiscale synthesis pipeline based on convolutional neural networks that ameliorates these issues. We first give a mathematical explanation of the source of instabilities in many previous approaches. We then improve these instabilities by using histogram losses to synthesize textures that better statistically match the exemplar. We also show how to integrate localized style losses in our multiscale framework. These losses can improve the quality of large features, improve the separation of content and style, and offer artistic controls such as paint by numbers. We demonstrate that our approach offers improved quality, convergence in fewer iterations, and more stability over the optimization. version:2
arxiv-1702-00493 | Information-theoretic interpretation of tuning curves for multiple motion directions | http://arxiv.org/abs/1702.00493 | id:1702.00493 author:Wentao Huang, Xin Huang, Kechen Zhang category:cs.IT cs.NE math.IT q-bio.NC q-bio.QM  published:2017-02-01 summary:We have developed an efficient information-maximization method for computing the optimal shapes of tuning curves of sensory neurons by optimizing the parameters of the underlying feedforward network model. When applied to the problem of population coding of visual motion with multiple directions, our method yields several types of tuning curves with both symmetric and asymmetric shapes that resemble what have been found in the visual cortex. Our result suggests that the diversity or heterogeneity of tuning curve shapes as observed in neurophysiological experiment might actually constitute an optimal population representation of visual motions with multiple components. version:1
arxiv-1702-00482 | Sub-Gaussian estimators of the mean of a random vector | http://arxiv.org/abs/1702.00482 | id:1702.00482 author:Gábor Lugosi, Shahar Mendelson category:math.ST stat.ML stat.TH  published:2017-02-01 summary:We study the problem of estimating the mean of a random vector $X$ given a sample of $N$ independent, identically distributed points. We introduce a new estimator that achieves a purely sub-Gaussian performance under the only condition that the second moment of $X$ exists. The estimator is based on a novel concept of a multivariate median. version:1
arxiv-1702-00477 | Dominance Move: A Measure of Comparing Solution Sets in Multiobjective Optimization | http://arxiv.org/abs/1702.00477 | id:1702.00477 author:Miqing Li, Xin Yao category:cs.NE math.OC  published:2017-02-01 summary:One of the most common approaches for multiobjective optimization is to generate a solution set that well approximates the whole Pareto-optimal frontier to facilitate the later decision-making process. However, how to evaluate and compare the quality of different solution sets remains challenging. Existing measures typically require additional problem knowledge and information, such as a reference point or a substituted set of the Pareto-optimal frontier. In this paper, we propose a quality measure, called dominance move (DoM), to compare solution sets generated by multiobjective optimizers. Given two solution sets, DoM measures the minimum sum of move distances for one set to weakly Pareto dominate the other set. DoM can be seen as a natural reflection of the difference between two solutions, capturing all aspects of solution sets' quality, being compliant with Pareto dominance, and does not need any additional problem knowledge and parameters. We present an exact method to calculate the DoM in the biobjective case. We show the necessary condition of constructing the optimal partition for a solution set's minimum move, and accordingly propose an efficient algorithm to recursively calculate the DoM. Finally, DoM is evaluated on several groups of artificial and real test cases as well as by a comparison with two well-established quality measures. version:1
arxiv-1702-00403 | Generative Adversarial Networks recover features in astrophysical images of galaxies beyond the deconvolution limit | http://arxiv.org/abs/1702.00403 | id:1702.00403 author:Kevin Schawinski, Ce Zhang, Hantian Zhang, Lucas Fowler, Gokula Krishnan Santhanam category:astro-ph.IM astro-ph.GA cs.LG stat.ML  published:2017-02-01 summary:Observations of astrophysical objects such as galaxies are limited by various sources of random and systematic noise from the sky background, the optical system of the telescope and the detector used to record the data. Conventional deconvolution techniques are limited in their ability to recover features in imaging data by the Shannon-Nyquist sampling theorem. Here we train a generative adversarial network (GAN) on a sample of $4,550$ images of nearby galaxies at $0.01<z<0.02$ from the Sloan Digital Sky Survey and conduct $10\times$ cross validation to evaluate the results. We present a method using a GAN trained on galaxy images that can recover features from artificially degraded images with worse seeing and higher noise than the original with a performance which far exceeds simple deconvolution. The ability to better recover detailed features such as galaxy morphology from low-signal-to-noise and low angular resolution imaging data significantly increases our ability to study existing data sets of astrophysical objects as well as future observations with observatories such as the Large Synoptic Sky Telescope (LSST) and the Hubble and James Webb space telescopes. version:1
arxiv-1702-00391 | Product Graph-based Higher Order Contextual Similarities for Inexact Subgraph Matching | http://arxiv.org/abs/1702.00391 | id:1702.00391 author:Anjan Dutta, Josep Lladós, Horst Bunke, Umapada Pal category:cs.CV  published:2017-02-01 summary:Many algorithms formulate graph matching as an optimization of an objective function of pairwise quantification of nodes and edges of two graphs to be matched. Pairwise measurements usually consider local attributes but disregard contextual information involved in graph structures. We address this issue by proposing contextual similarities between pairs of nodes. This is done by considering the tensor product graph (TPG) of two graphs to be matched, where each node is an ordered pair of nodes of the operand graphs. Contextual similarities between a pair of nodes are computed by accumulating weighted walks (normalized pairwise similarities) terminating at the corresponding paired node in TPG. Once the contextual similarities are obtained, we formulate subgraph matching as a node and edge selection problem in TPG. We use contextual similarities to construct an objective function and optimize it with a linear programming approach. Since random walk formulation through TPG takes into account higher order information, it is not a surprise that we obtain more reliable similarities and better discrimination among the nodes and edges. Experimental results shown on synthetic as well as real benchmarks illustrate that higher order contextual similarities add discriminating power and allow one to find approximate solutions to the subgraph matching problem. version:1
arxiv-1702-00723 | Handwritten Recognition Using SVM, KNN and Neural Network | http://arxiv.org/abs/1702.00723 | id:1702.00723 author:Norhidayu Abdul Hamid, Nilam Nur Amir Sjarif category:cs.CV 68Txx  published:2017-02-01 summary:Handwritten recognition (HWR) is the ability of a computer to receive and interpret intelligible handwritten input from source such as paper documents, photographs, touch-screens and other devices. In this paper we will using three (3) classification t o re cognize the handwritten which is SVM, KNN and Neural Network. version:1
arxiv-1702-00382 | Understanding trained CNNs by indexing neuron selectivity | http://arxiv.org/abs/1702.00382 | id:1702.00382 author:Ivet Rafegas, Maria Vanrell, Luis A. Alexandre category:cs.CV  published:2017-02-01 summary:The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifying their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties. Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. version:1
arxiv-1702-00372 | Visual Saliency Prediction Using a Mixture of Deep Neural Networks | http://arxiv.org/abs/1702.00372 | id:1702.00372 author:Samuel Dodge, Lina Karam category:cs.CV  published:2017-02-01 summary:Visual saliency models have recently begun to incorporate deep learning to achieve predictive capacity much greater than previous unsupervised methods. However, most existing models predict saliency using local mechanisms limited to the receptive field of the network. We propose a model that incorporates global scene semantic information in addition to local information gathered by a convolutional neural network. Our model is formulated as a mixture of experts. Each expert network is trained to predict saliency for a set of closely related images. The final saliency map is computed as a weighted mixture of the expert networks' output, with weights determined by a separate gating network. This gating network is guided by global scene information to predict weights. The expert networks and the gating network are trained simultaneously in an end-to-end manner. We show that our mixture formulation leads to improvement in performance over an otherwise identical non-mixture model that does not incorporate global scene information. version:1
arxiv-1701-08711 | Predicting Auction Price of Vehicle License Plate with Deep Recurrent Neural Network | http://arxiv.org/abs/1701.08711 | id:1701.08711 author:Vinci Chow category:cs.CL cs.LG q-fin.EC stat.ML  published:2017-01-30 summary:In Chinese societies where superstition is of paramount importance, vehicle license plates with desirable numbers can fetch for very high prices in auctions. Unlike auctions of other valuable items, however, license plates do not get an estimated price before auction. In this paper, I propose that the task of predicting plate prices can be viewed as a natural language processing task, because the value of a plate depends on the meaning of each individual character on the plate as well as the semantics. I construct a deep recurrent neural network to predict the prices of vehicle license plates in Hong Kong based on the characters on a plate. Trained with 13-years of historical auction prices, the deep RNN outperforms previous models by significant margin. version:2
arxiv-1702-00815 | Optimal Experimental Design of Field Trials using Differential Evolution | http://arxiv.org/abs/1702.00815 | id:1702.00815 author:Vitaliy Feoktistov, Stephane Pietravalle, Nicolas Heslot category:cs.NE q-bio.QM  published:2017-02-01 summary:When setting up field experiments, to test and compare a range of genotypes (e.g. maize hybrids), it is important to account for any possible field effect that may otherwise bias performance estimates of genotypes. To do so, we propose a model-based method aimed at optimizing the allocation of the tested genotypes and checks between fields and placement within field, according to their kinship. This task can be formulated as a combinatorial permutation-based problem. We used Differential Evolution concept to solve this problem. We then present results of optimal strategies for between-field and within-field placements of genotypes and compare them to existing optimization strategies, both in terms of convergence time and result quality. The new algorithm gives promising results in terms of convergence and search space exploration. version:1
arxiv-1701-09177 | A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering | http://arxiv.org/abs/1701.09177 | id:1701.09177 author:Hongteng Xu, Hongyuan Zha category:cs.LG stat.ML  published:2017-01-31 summary:We propose an effective method to solve the event sequence clustering problems based on a novel Dirichlet mixture model of a special but significant type of point processes --- Hawkes process. In this model, each event sequence belonging to a cluster is generated via the same Hawkes process with specific parameters, and different clusters correspond to different Hawkes processes. The prior distribution of the Hawkes processes is controlled via a Dirichlet process. We learn the model via a maximum likelihood estimator (MLE) and propose an effective variational Bayesian inference algorithm. We specifically analyze the resulting EM-type algorithm in the context of inner-outer iterations and discuss several inner iteration allocation strategies. The identifiability of our model, the convergence of our learning method, and its sample complexity are analyzed in both theoretical and empirical ways, which demonstrate the superiority of our method to other competitors. The proposed method learns the number of clusters automatically and is robust to model misspecification. Experiments on both synthetic and real-world data show that our method can learn diverse triggering patterns hidden in asynchronous event sequences and achieve encouraging performance on clustering purity and consistency. version:2
arxiv-1702-00338 | Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval | http://arxiv.org/abs/1702.00338 | id:1702.00338 author:Eng-Jon Ong, Sameed Husain, Miroslaw Bober category:cs.CV  published:2017-02-01 summary:This paper addresses the problem of large scale image retrieval, with the aim of accurately ranking the similarity of a large number of images to a given query image. To achieve this, we propose a novel Siamese network. This network consists of two computational strands, each comprising of a CNN component followed by a Fisher vector component. The CNN component produces dense, deep convolutional descriptors that are then aggregated by the Fisher Vector method. Crucially, we propose to simultaneously learn both the CNN filter weights and Fisher Vector model parameters. This allows us to account for the evolving distribution of deep descriptors over the course of the learning process. We show that the proposed approach gives significant improvements over the state-of-the-art methods on the Oxford and Paris image retrieval datasets. Additionally, we provide a baseline performance measure for both these datasets with the inclusion of 1 million distractors. version:1
arxiv-1702-00307 | Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks | http://arxiv.org/abs/1702.00307 | id:1702.00307 author:Žiga Emeršič, Luka Lan Gabriel, Vitomir Štruc, Peter Peer category:cs.CV  published:2017-02-01 summary:Object detection and segmentation represents the basis for many tasks in computer and machine vision. In biometric recognition systems the detection of the region-of-interest (ROI) is one of the most crucial steps in the overall processing pipeline, significantly impacting the performance of the entire recognition system. Existing approaches to ear detection, for example, are commonly susceptible to the presence of severe occlusions, ear accessories or variable illumination conditions and often deteriorate in their performance if applied on ear images captured in unconstrained settings. To address these shortcomings, we present in this paper a novel ear detection technique based on convolutional encoder-decoder networks (CEDs). For our technique, we formulate the problem of ear detection as a two-class segmentation problem and train a convolutional encoder-decoder network based on the SegNet architecture to distinguish between image-pixels belonging to either the ear or the non-ear class. The output of the network is then post-processed to further refine the segmentation result and return the final locations of the ears in the input image. Different from competing techniques from the literature, our approach does not simply return a bounding box around the detected ear, but provides detailed, pixel-wise information about the location of the ears in the image. Our experiments on a dataset gathered from the web (a.k.a. in the wild) show that the proposed technique ensures good detection results in the presence of various covariate factors and significantly outperforms the existing state-of-the-art. version:1
arxiv-1702-00210 | Foreign-language Reviews: Help or Hindrance? | http://arxiv.org/abs/1702.00210 | id:1702.00210 author:Scott A. Hale, Irene Eleta category:cs.HC cs.CL cs.CY H.5.m  published:2017-02-01 summary:The number and quality of user reviews greatly affects consumer purchasing decisions. While reviews in all languages are increasing, it is still often the case (especially for non-English speakers) that there are only a few reviews in a person's first language. Using an online experiment, we examine the value that potential purchasers receive from interfaces showing additional reviews in a second language. The results paint a complicated picture with both positive and negative reactions to the inclusion of foreign-language reviews. Roughly 26-28% of subjects clicked to see translations of the foreign-language content when given the opportunity, and those who did so were more likely to select the product with foreign-language reviews than those who did not. version:1
arxiv-1702-00196 | Communication-Optimal Distributed Clustering | http://arxiv.org/abs/1702.00196 | id:1702.00196 author:Jiecao Chen, He Sun, David P. Woodruff, Qin Zhang category:cs.DS cs.LG  published:2017-02-01 summary:Clustering large datasets is a fundamental problem with a number of applications in machine learning. Data is often collected on different sites and clustering needs to be performed in a distributed manner with low communication. We would like the quality of the clustering in the distributed setting to match that in the centralized setting for which all the data resides on a single site. In this work, we study both graph and geometric clustering problems in two distributed models: (1) a point-to-point model, and (2) a model with a broadcast channel. We give protocols in both models which we show are nearly optimal by proving almost matching communication lower bounds. Our work highlights the surprising power of a broadcast channel for clustering problems; roughly speaking, to spectrally cluster $n$ points or $n$ vertices in a graph distributed across $s$ servers, for a worst-case partitioning the communication complexity in a point-to-point model is $n \cdot s$, while in the broadcast model it is $n + s$. A similar phenomenon holds for the geometric setting as well. We implement our algorithms and demonstrate this phenomenon on real life datasets, showing that our algorithms are also very efficient in practice. version:1
arxiv-1702-00187 | ImageNet MPEG-7 Visual Descriptors - Technical Report | http://arxiv.org/abs/1702.00187 | id:1702.00187 author:Frédéric Rayar category:cs.CV cs.IR  published:2017-02-01 summary:ImageNet is a large scale and publicly available image database. It currently offers more than 14 millions of images, organised according to the WordNet hierarchy. One of the main objective of the creators is to provide to the research community a relevant database for visual recognition applications such as object recognition, image classification or object localisation. However, only a few visual descriptors of the images are available to be used by the researchers. Only SIFT-based features have been extracted from a subset of the collection. This technical report presents the extraction of some MPEG-7 visual descriptors from the ImageNet database. These descriptors are made publicly available in an effort towards open research. version:1
arxiv-1702-00186 | A Kinematic Chain Space for Monocular Motion Capture | http://arxiv.org/abs/1702.00186 | id:1702.00186 author:Bastian Wandt, Hanno Ackermann, Bodo Rosenhahn category:cs.CV  published:2017-02-01 summary:This paper deals with motion capture of kinematic chains (e.g. human skeletons) from monocular image sequences taken by uncalibrated cameras. We present a method based on projecting an observation into a kinematic chain space (KCS). An optimization of the nuclear norm is proposed that implicitly enforces structural properties of the kinematic chain. Unlike other approaches our method does not require specific camera or object motion and is not relying on training data or previously determined constraints such as particular body lengths. The proposed algorithm is able to reconstruct scenes with limited camera motion and previously unseen motions. It is not only applicable to human skeletons but also to other kinematic chains for instance animals or industrial robots. We achieve state-of-the-art results on different benchmark data bases and real world scenes. version:1
arxiv-1702-00178 | On the Futility of Learning Complex Frame-Level Language Models for Chord Recognition | http://arxiv.org/abs/1702.00178 | id:1702.00178 author:Filip Korzeniowski, Gerhard Widmer category:cs.SD cs.LG  published:2017-02-01 summary:Chord recognition systems use temporal models to post-process frame-wise chord preditions from acoustic models. Traditionally, first-order models such as Hidden Markov Models were used for this task, with recent works suggesting to apply Recurrent Neural Networks instead. Due to their ability to learn longer-term dependencies, these models are supposed to learn and to apply musical knowledge, instead of just smoothing the output of the acoustic model. In this paper, we argue that learning complex temporal models at the level of audio frames is futile on principle, and that non-Markovian models do not perform better than their first-order counterparts. We support our argument through three experiments on the McGill Billboard dataset. The first two show 1) that when learning complex temporal models at the frame level, improvements in chord sequence modelling are marginal; and 2) that these improvements do not translate when applied within a full chord recognition system. The third, still rather preliminary experiment gives first indications that the use of complex sequential models for chord prediction at higher temporal levels might be more promising. version:1
arxiv-1702-00177 | PCA-Initialized Deep Neural Networks Applied To Document Image Analysis | http://arxiv.org/abs/1702.00177 | id:1702.00177 author:Mathias Seuret, Michele Alberti, Rolf Ingold, Marcus Liwicki category:cs.LG stat.ML  published:2017-02-01 summary:In this paper, we present a novel approach for initializing deep neural networks, i.e., by turning PCA into neural layers. Usually, the initialization of the weights of a deep neural network is done in one of the three following ways: 1) with random values, 2) layer-wise, usually as Deep Belief Network or as auto-encoder, and 3) re-use of layers from another network (transfer learning). Therefore, typically, many training epochs are needed before meaningful weights are learned, or a rather similar dataset is required for seeding a fine-tuning of transfer learning. In this paper, we describe how to turn a PCA into an auto-encoder, by generating an encoder layer of the PCA parameters and furthermore adding a decoding layer. We analyze the initialization technique on real documents. First, we show that a PCA-based initialization is quick and leads to a very stable initialization. Furthermore, for the task of layout analysis we investigate the effectiveness of PCA-based initialization and show that it outperforms state-of-the-art random weight initialization methods. version:1
arxiv-1701-08991 | A novel method for automatic localization of joint area on knee plain radiographs | http://arxiv.org/abs/1701.08991 | id:1701.08991 author:Aleksei Tiulpin, Jérôme Thevenot, Esa Rahtu, Simo Saarakkala category:cs.CV  published:2017-01-31 summary:Osteoarthritis (OA) is a common musculoskeletal condition typically diagnosed from radiographic assessment after clinical examination. However, a visual evaluation made by a practitioner suffers from subjectivity and is highly dependent on the experience. Computer-aided diagnostics (CAD) could improve the objectivity of knee radiographic examination. The first essential step of knee OA CAD is to automatically localize the joint area. However, according to the literature this task itself remains challenging. The aim of this study was to develop novel and computationally efficient method to tackle the issue. Here, three different datasets of knee radiographs were used (n = 473/93/77) to validate the overall performance of the method. Our pipeline consists of two parts: anatomically-based joint area proposal and their evaluation using Histogram of Oriented Gradients and the pre-trained Support Vector Machine classifier scores. The obtained results for the used datasets show the mean intersection over the union equal to: 0.84, 0.79 and 0.78. Using a high-end computer, the method allows to automatically annotate conventional knee radiographs within 14-16ms and high resolution ones within 170ms. Our results demonstrate that the developed method is suitable for large-scale analyses. version:2
arxiv-1702-00159 | Robust Order Scheduling in the Fashion Industry: A Multi-Objective Optimization Approach | http://arxiv.org/abs/1702.00159 | id:1702.00159 author:Wei Du, Yang Tang, Sunney Yung Sun Leung, Le Tong, Athanasios V. Vasilakos, Feng Qian category:cs.AI cs.NE  published:2017-02-01 summary:In the fashion industry, order scheduling focuses on the assignment of production orders to appropriate production lines. In reality, before a new order can be put into production, a series of activities known as pre-production events need to be completed. In addition, in real production process, owing to various uncertainties, the daily production quantity of each order is not always as expected. In this research, by considering the pre-production events and the uncertainties in the daily production quantity, robust order scheduling problems in the fashion industry are investigated with the aid of a multi-objective evolutionary algorithm (MOEA) called nondominated sorting adaptive differential evolution (NSJADE). The experimental results illustrate that it is of paramount importance to consider pre-production events in order scheduling problems in the fashion industry. We also unveil that the existence of the uncertainties in the daily production quantity heavily affects the order scheduling. version:1
arxiv-1702-00158 | Design, Analysis and Application of A Volumetric Convolutional Neural Network | http://arxiv.org/abs/1702.00158 | id:1702.00158 author:Xiaqing Pan, Yueru Chen, C. -C. Jay Kuo category:cs.CV  published:2017-02-01 summary:The design, analysis and application of a volumetric convolutional neural network (VCNN) are studied in this work. Although many CNNs have been proposed in the literature, their design is empirical. In the design of the VCNN, we propose a feed-forward K-means clustering algorithm to determine the filter number and size at each convolutional layer systematically. For the analysis of the VCNN, the cause of confusing classes in the output of the VCNN is explained by analyzing the relationship between the filter weights (also known as anchor vectors) from the last fully-connected layer to the output. Furthermore, a hierarchical clustering method followed by a random forest classification method is proposed to boost the classification performance among confusing classes. For the application of the VCNN, we examine the 3D shape classification problem and conduct experiments on a popular ModelNet40 dataset. The proposed VCNN offers the state-of-the-art performance among all volume-based CNN methods. version:1
arxiv-1702-00156 | High Order Stochastic Graphlet Embedding for Graph-Based Pattern Recognition | http://arxiv.org/abs/1702.00156 | id:1702.00156 author:Anjan Dutta category:cs.CV  published:2017-02-01 summary:Graph-based methods are known to be successful for pattern description and comparison purpose. However, a lot of mathematical tools are unavailable in graph domain, thus restricting the generic graph-based techniques to be applicable within the machine learning framework. A way to tackle this problem is graph embedding into high dimensional space in either an explicit or implicit manner. In this paper, we propose high order stochastic graphlet embedding (SGE) that explicitly embed a graph into a real vector space. Our main contribution includes a new stochastic search procedure that allows one to efficiently parse a given graph and extract or sample unlimitedly high order graphlets. We consider these graphlets with increasing size in order to model local features, as well as, their complex interactions. We also introduce or design graph hash functions with very low probability of collision to hash those sampled graphlets for partitioning them into sets of isomorphic ones and measure their distribution in large graph collections, which results in accurate graph descriptions. When combined with support vector machines, these high order graphlet-based descriptions have positive impact on the performance of graph-based pattern comparison and classification as corroborated through experiments on different standard benchmark databases. version:1
arxiv-1701-08978 | Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point | http://arxiv.org/abs/1701.08978 | id:1701.08978 author:Naveen Mellempudi, Abhisek Kundu, Dipankar Das, Dheevatsa Mudigere, Bharat Kaul category:cs.LG cs.NE  published:2017-01-31 summary:We propose a cluster-based quantization method to convert pre-trained full precision weights into ternary weights with minimal impact on the accuracy. In addition, we also constrain the activations to 8-bits thus enabling sub 8-bit full integer inference pipeline. Our method uses smaller clusters of N filters with a common scaling factor to minimize the quantization loss, while also maximizing the number of ternary operations. We show that with a cluster size of N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best full precision results while replacing ~85% of all multiplications with 8-bit accumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1 accuracy which within 2% of the full precision result. We also study the impact of the size of the cluster on both performance and accuracy, larger cluster sizes N=64 can replace ~98% of the multiplications with ternary operations but introduces significant drop in accuracy which necessitates fine tuning the parameters with retraining the network at lower precision. To address this we have also trained low-precision Resnet-50 with 8-bit activations and ternary weights by pre-initializing the network with full precision weights and achieve 68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can run on a full 8-bit compute pipeline, with a potential 16x improvement in performance compared to baseline full-precision models. version:2
arxiv-1702-00098 | Denoising Hyperspectral Image with Non-i.i.d. Noise Structure | http://arxiv.org/abs/1702.00098 | id:1702.00098 author:Yang Chen, Xiangyong Cao, Qian Zhao, Deyu Meng, Zongben Xu category:cs.CV  published:2017-02-01 summary:Hyperspectral image (HSI) denoising has been attracting much research attention in remote sensing area due to its importance in improving the HSI qualities. The existing HSI denoising methods mainly focus on specific spectral and spatial prior knowledge in HSIs, and share a common underlying assumption that the embedded noise in HSI is independent and identically distributed (i.i.d.). In real scenarios, however, the noise existed in a natural HSI is always with much more complicated non-i.i.d. statistical structures and the under-estimation to this noise complexity often tends to evidently degenerate the robustness of current methods. To alleviate this issue, this paper attempts the first effort to model the HSI noise using a non-i.i.d. mixture of Gaussians (NMoG) noise assumption, which is finely in accordance with the noise characteristics possessed by a natural HSI and thus is capable of adapting various noise shapes encountered in real applications. Then we integrate such noise modeling strategy into the low-rank matrix factorization (LRMF) model and propose a NMoG-LRMF model in the Bayesian framework. A variational Bayes algorithm is designed to infer the posterior of the proposed model. All involved parameters can be recursively updated in closed-form. Compared with the current techniques, the proposed method performs more robust beyond the state-of-the-arts, as substantiated by our experiments implemented on synthetic and real noisy HSIs. version:1
arxiv-1702-02215 | Integration of Machine Learning Techniques to Evaluate Dynamic Customer Segmentation Analysis for Mobile Customers | http://arxiv.org/abs/1702.02215 | id:1702.02215 author:Cormac Dullaghan, Eleni Rozaki category:cs.CY cs.LG stat.ML  published:2017-01-31 summary:The telecommunications industry is highly competitive, which means that the mobile providers need a business intelligence model that can be used to achieve an optimal level of churners, as well as a minimal level of cost in marketing activities. Machine learning applications can be used to provide guidance on marketing strategies. Furthermore, data mining techniques can be used in the process of customer segmentation. The purpose of this paper is to provide a detailed analysis of the C.5 algorithm, within naive Bayesian modelling for the task of segmenting telecommunication customers behavioural profiling according to their billing and socio-demographic aspects. Results have been experimentally implemented. version:1
arxiv-1702-00071 | On orthogonality and learning recurrent networks with long term dependencies | http://arxiv.org/abs/1702.00071 | id:1702.00071 author:Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, Chris Pal category:cs.LG cs.NE  published:2017-01-31 summary:It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. version:1
arxiv-1701-08588 | Estimating the risk associated with transportation technology using multifidelity simulation | http://arxiv.org/abs/1701.08588 | id:1701.08588 author:Erik J. Schlicht, Nichole L. Morris category:stat.AP stat.ML  published:2017-01-30 summary:This paper provides a quantitative method for estimating the risk associated with candidate transportation technology, before it is developed and deployed. The proposed solution extends previous methods that rely exclusively on low-fidelity human-in-the-loop experimental data, or high-fidelity traffic data, by adopting a multifidelity approach that leverages data from both low- and high-fidelity sources. The multifidelity method overcomes limitations inherent to existing approaches by allowing a model to be trained inexpensively, while still assuring that its predictions generalize to the real-world. This allows for candidate technologies to be evaluated at the stage of conception, and enables a mechanism for only the safest and most effective technology to be developed and released. version:2
arxiv-1702-00045 | Spatial Aggregation of Holistically-Nested Convolutional Neural Networks for Automated Pancreas Localization and Segmentation | http://arxiv.org/abs/1702.00045 | id:1702.00045 author:Holger R. Roth, Le Lu, Nathan Lay, Adam P. Harrison, Amal Farag, Andrew Sohn, Ronald M. Summers category:cs.CV  published:2017-01-31 summary:Accurate and automatic organ segmentation from 3D radiological scans is an important yet challenging problem for medical image analysis. Specifically, the pancreas demonstrates very high inter-patient anatomical variability in both its shape and volume. In this paper, we present an automated system using 3D computed tomography (CT) volumes via a two-stage cascaded approach: pancreas localization and segmentation. For the first step, we localize the pancreas from the entire 3D CT scan, providing a reliable bounding box for the more refined segmentation step. We introduce a fully deep-learning approach, based on an efficient application of holistically-nested convolutional networks (HNNs) on the three orthogonal axial, sagittal, and coronal views. The resulting HNN per-pixel probability maps are then fused using pooling to reliably produce a 3D bounding box of the pancreas that maximizes the recall. We show that our introduced localizer compares favorably to both a conventional non-deep-learning method and a recent hybrid approach based on spatial aggregation of superpixels using random forest classification. The second, segmentation, phase operates within the computed bounding box and integrates semantic mid-level cues of deeply-learned organ interior and boundary maps, obtained by two additional and separate realizations of HNNs. By integrating these two mid-level cues, our method is capable of generating boundary-preserving pixel-wise class label maps that result in the final pancreas segmentation. Quantitative evaluation is performed on a publicly available dataset of 82 patient CT scans using 4-fold cross-validation (CV). We achieve a Dice similarity coefficient (DSC) of 81.27+/-6.27% in validation, which significantly outperforms previous state-of-the art methods that report DSCs of 71.80+/-10.70% and 78.01+/-8.20%, respectively, using the same dataset. version:1
arxiv-1702-00027 | Representation of big data by dimension reduction | http://arxiv.org/abs/1702.00027 | id:1702.00027 author:A. G. Ramm, C. Van category:cs.IT cs.LG math.IT stat.ML  published:2017-01-31 summary:Suppose the data consist of a set $S$ of points $x_j, 1 \leq j \leq J$, distributed in a bounded domain $D \subset R^N$, where $N$ and $J$ are large numbers. In this paper an algorithm is proposed for checking whether there exists a manifold $\mathbb{M}$ of low dimension near which many of the points of $S$ lie and finding such $\mathbb{M}$ if it exists. There are many dimension reduction algorithms, both linear and non-linear. Our algorithm is simple to implement and has some advantages compared with the known algorithms. If there is a manifold of low dimension near which most of the data points lie, the proposed algorithm will find it. Some numerical results are presented illustrating the algorithm and analyzing its performance compared to the classical PCA (principal component analysis) and Isomap. version:1
arxiv-1702-00020 | Towards "AlphaChem": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies | http://arxiv.org/abs/1702.00020 | id:1702.00020 author:Marwin Segler, Mike Preuß, Mark P. Waller category:cs.AI cs.LG physics.chem-ph  published:2017-01-31 summary:Retrosynthesis is a technique to plan the chemical synthesis of organic molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a search tree is built by analysing molecules recursively and dissecting them into simpler molecular building blocks until one obtains a set of known building blocks. The search space is intractably large, and it is difficult to determine the value of retrosynthetic positions. Here, we propose to model retrosynthesis as a Markov Decision Process. In combination with a Deep Neural Network policy learned from essentially the complete published knowledge of chemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In exploratory studies, we demonstrate that MCTS with neural network policies outperforms the traditionally used best-first search with hand-coded heuristics. version:1
arxiv-1701-08302 | A Study of FOSS'2013 Survey Data Using Clustering Techniques | http://arxiv.org/abs/1701.08302 | id:1701.08302 author:Mani A, Rebeka Mukherjee category:cs.AI cs.CY cs.SE stat.ML  published:2017-01-28 summary:FOSS is an acronym for Free and Open Source Software. The FOSS 2013 survey primarily targets FOSS contributors and relevant anonymized dataset is publicly available under CC by SA license. In this study, the dataset is analyzed from a critical perspective using statistical and clustering techniques (especially multiple correspondence analysis) with a strong focus on women contributors towards discovering hidden trends and facts. Important inferences are drawn about development practices and other facets of the free software and OSS worlds. version:2
arxiv-1701-09135 | DeepNav: Learning to Navigate Large Cities | http://arxiv.org/abs/1701.09135 | id:1701.09135 author:Samarth Brahmbhatt, James Hays category:cs.CV  published:2017-01-31 summary:We present DeepNav, a Convolutional Neural Network (CNN) based algorithm for navigating large cities using locally visible street-view images. The DeepNav agent learns to reach its destination quickly by making the correct navigation decisions at intersections. We collect a large-scale dataset of street-view images organized in a graph where nodes are connected by roads. This dataset contains 10 city graphs and more than 1 million street-view images. We propose 3 supervised learning approaches for the navigation task and show how A* search in the city graph can be used to generate supervision for the learning. Our annotation process is fully automated using publicly available mapping services and requires no human input. We evaluate the proposed DeepNav models on 4 held-out cities for navigating to 5 different types of destinations. Our algorithms outperform previous work that uses hand-crafted features and Support Vector Regression (SVR)[19]. version:1
arxiv-1701-09123 | Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features | http://arxiv.org/abs/1701.09123 | id:1701.09123 author:Rodrigo Agerri, German Rigau category:cs.CL cs.AI  published:2017-01-31 summary:We present a multilingual Named Entity Recognition approach based on a robust and general set of features across languages and datasets. Our system combines shallow local information with clustering semi-supervised features induced on large amounts of unlabeled text. Understanding via empirical experimentation how to effectively combine various types of clustering features allows us to seamlessly export our system to other datasets and languages. The result is a simple but highly competitive system which obtains state of the art results across five languages and twelve datasets. The results are reported on standard shared task evaluation data such as CoNLL for English, Spanish and Dutch. Furthermore, and despite the lack of linguistically motivated features, we also report best results for languages such as Basque and German. In addition, we demonstrate that our method also obtains very competitive results even when the amount of supervised data is cut by half, alleviating the dependency on manually annotated data. Finally, the results show that our emphasis on clustering features is crucial to develop robust out-of-domain models. The system and models are freely available to facilitate its use and guarantee the reproducibility of results. version:1
arxiv-1701-09055 | Gaussian Process Regression Model for Distribution Inputs | http://arxiv.org/abs/1701.09055 | id:1701.09055 author:François Bachoc, Fabrice Gamboa, Jean-Michel Loubes, Nil Venet category:stat.ML math.PR  published:2017-01-31 summary:Monge-Kantorovich distances, otherwise known as Wasserstein distances, have received a growing attention in statistics and machine learning as a powerful discrepancy measure for probability distributions. In this paper, we focus on forecasting a Gaussian process indexed by probability distributions. For this, we provide a family of positive definite kernels built using transportation based distances. We provide a probabilistic understanding of these kernels and characterize the corresponding stochastic processes. We prove that the Gaussian processes indexed by distributions corresponding to these kernels can be efficiently forecast, opening new perspectives in Gaussian process modeling. version:1
arxiv-1701-09037 | A New Method for Removing the Moire' Pattern from Images | http://arxiv.org/abs/1701.09037 | id:1701.09037 author:Seyede Mahya Hazavei, Hamid Reza Shahdoosti category:cs.CV  published:2017-01-31 summary:During the last decades, denoising methods have attracted much attention of researchers. The conventional method for removing the Moire' pattern from images is using notch filters in the Frequency-domain. In this paper a new method is proposed that can achieve a better performance in comparison with the traditional method. Median filter is used in some part of spectrum of noisy images to reduce the noise. At the second part of this paper, to demonstrate the robustness of the proposed method, it is implemented for some noisy images that have moire' pattern. Experiments on noisy images with different characteristics show that the proposed method increases the PSNR values compared with previous methods. version:1
arxiv-1702-02508 | Computational Techniques in Multispectral Image Processing: Application to the Syriac Galen Palimpsest | http://arxiv.org/abs/1702.02508 | id:1702.02508 author:Corneliu Arsene, Peter Pormann, William Sellers, Siam Bhayro category:cs.CV  published:2017-01-31 summary:Multispectral and hyperspectral image analysis has experienced much development in the last decade. The application of these methods to palimpsests has produced significant results, enabling researchers to recover texts that would be otherwise lost under the visible overtext, by improving the contrast between the undertext and the overtext. In this paper we explore an extended number of multispectral and hyperspectral image analysis methods, consisting of supervised and unsupervised dimensionality reduction techniques, on a part of the Syriac Galen Palimpsest dataset (www.digitalgalen.net). Of this extended set of methods, eight methods gave good results: three were supervised methods Generalized Discriminant Analysis (GDA), Linear Discriminant Analysis (LDA), and Neighborhood Component Analysis (NCA); and the other five methods were unsupervised methods (but still used in a supervised way) Gaussian Process Latent Variable Model (GPLVM), Isomap, Landmark Isomap, Principal Component Analysis (PCA), and Probabilistic Principal Component Analysis (PPCA). The relative success of these methods was determined visually, using color pictures, on the basis of whether the undertext was distinguishable from the overtext, resulting in the following ranking of the methods: LDA, NCA, GDA, Isomap, Landmark Isomap, PPCA, PCA, and GPLVM. These results were compared with those obtained using the Canonical Variates Analysis (CVA) method on the same dataset, which showed remarkably accuracy (LDA is a particular case of CVA where the objects are classified to two classes). version:1
arxiv-1701-05221 | Parsimonious Inference on Convolutional Neural Networks: Learning and applying on-line kernel activation rules | http://arxiv.org/abs/1701.05221 | id:1701.05221 author:I. Theodorakopoulos, V. Pothos, D. Kastaniotis, N. Fragoulis category:cs.CV cs.AI cs.LG cs.NE  published:2017-01-18 summary:A new, radical CNN design approach is presented in this paper, considering the reduction of the total computational load during inference. This is achieved by a new holistic intervention on both the CNN architecture and the training procedure, which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a CNN architecture. This is accomplished, by the introduction of a new structural element that can be inserted as an add-on to any contemporary CNN architecture, whilst preserving or even improving its recognition accuracy. Our approach formulates a systematic and data-driven method for developing CNNs that are trained to eventually change size and form in real-time during inference, targeting to the smaller possible computational footprint. Results are provided for the optimal implementation on a few modern, high-end mobile computing platforms indicating a significant speed-up of up to x3 times. version:5
arxiv-1701-08985 | Deep Multitask Architecture for Integrated 2D and 3D Human Sensing | http://arxiv.org/abs/1701.08985 | id:1701.08985 author:Alin-Ionut Popa, Mihai Zanfir, Cristian Sminchisescu category:cs.CV  published:2017-01-31 summary:We propose a deep multitask architecture for \emph{fully automatic 2d and 3d human sensing} (DMHS), including \emph{recognition and reconstruction}, in \emph{monocular images}. The system computes the figure-ground segmentation, semantically identifies the human body parts at pixel level, and estimates the 2d and 3d pose of the person. The model supports the joint training of all components by means of multi-task losses where early processing stages recursively feed into advanced ones for increasingly complex calculations, accuracy and robustness. The design allows us to tie a complete training protocol, by taking advantage of multiple datasets that would otherwise restrictively cover only some of the model components: complex 2d image data with no body part labeling and without associated 3d ground truth, or complex 3d data with limited 2d background variability. In detailed experiments based on several challenging 2d and 3d datasets (LSP, HumanEva, Human3.6M), we evaluate the sub-structures of the model, the effect of various types of training data in the multitask loss, and demonstrate that state-of-the-art results can be achieved at all processing levels. We also show that in the wild our monocular RGB architecture is perceptually competitive to a state-of-the art (commercial) Kinect system based on RGB-D data. version:1
arxiv-1702-00260 | Machine learning based compact photonic structure design for strong light confinement | http://arxiv.org/abs/1702.00260 | id:1702.00260 author:Mirbek Turduev, Çağrı Latifoğlu, İbrahim Halil Giden, Y. Sinan Hanay category:physics.optics cs.LG  published:2017-01-31 summary:We present a novel approach based on machine learning for designing photonic structures. In particular, we focus on strong light confinement that allows the design of an efficient free-space-to-waveguide coupler which is made of Si- slab overlying on the top of silica substrate. The learning algorithm is implemented using bitwise square Si- cells and the whole optimized device has a footprint of $\boldsymbol{2 \, \mu m \times 1\, \mu m}$, which is the smallest size ever achieved numerically. To find the effect of Si- slab thickness on the sub-wavelength focusing and strong coupling characteristics of optimized photonic structure, we carried out three-dimensional time-domain numerical calculations. Corresponding optimum values of full width at half maximum and coupling efficiency were calculated as $\boldsymbol{0.158 \lambda}$ and $\boldsymbol{-1.87\,dB}$ with slab thickness of $\boldsymbol{280nm}$. Compared to the conventional counterparts, the optimized lens and coupler designs are easy-to-fabricate via optical lithography techniques, quite compact, and can operate at telecommunication wavelengths. The outcomes of the presented study show that machine learning can be beneficial for efficient photonic designs in various potential applications such as polarization-division, beam manipulation and optical interconnects. version:1
arxiv-1701-08974 | Towards Adversarial Retinal Image Synthesis | http://arxiv.org/abs/1701.08974 | id:1701.08974 author:Pedro Costa, Adrian Galdran, Maria Inês Meyer, Michael David Abràmoff, Meindert Niemeijer, Ana Maria Mendonça, Aurélio Campilho category:cs.CV cs.LG stat.ML  published:2017-01-31 summary:Synthesizing images of the eye fundus is a challenging task that has been previously approached by formulating complex models of the anatomy of the eye. New images can then be generated by sampling a suitable parameter space. In this work, we propose a method that learns to synthesize eye fundus images directly from data. For that, we pair true eye fundus images with their respective vessel trees, by means of a vessel segmentation technique. These pairs are then used to learn a mapping from a binary vessel tree to a new retinal image. For this purpose, we use a recent image-to-image translation technique, based on the idea of adversarial learning. Experimental results show that the original and the generated images are visually different in terms of their global appearance, in spite of sharing the same vessel tree. Additionally, a quantitative quality analysis of the synthetic retinal images confirms that the produced images retain a high proportion of the true image set quality. version:1
arxiv-1701-08968 | Supervised Learning in Automatic Channel Selection for Epileptic Seizure Detection | http://arxiv.org/abs/1701.08968 | id:1701.08968 author:Nhan Truong, Levin Kuhlmann, Mohammad Reza Bonyadi, Jiawei Yang, Andrew Faulks, Omid Kavehei category:cs.CV  published:2017-01-31 summary:Detecting seizure using brain neuroactivations recorded by intracranial electroencephalogram (iEEG) has been widely used for monitoring, diagnosing, and closed-loop therapy of epileptic patients, however, computational efficiency gains are needed if state-of-the-art methods are to be implemented in implanted devices. We present a novel method for automatic seizure detection based on iEEG data that outperforms current state-of-the-art seizure detection methods in terms of computational efficiency while maintaining the accuracy. The proposed algorithm incorporates an automatic channel selection (ACS) engine as a pre-processing stage to the seizure detection procedure. The ACS engine consists of supervised classifiers which aim to find iEEGchannelswhich contribute the most to a seizure. Seizure detection stage involves feature extraction and classification. Feature extraction is performed in both frequency and time domains where spectral power and correlation between channel pairs are calculated. Random Forest is used in classification of interictal, ictal and early ictal periods of iEEG signals. Seizure detection in this paper is retrospective and patient-specific. iEEG data is accessed via Kaggle, provided by International Epilepsy Electro-physiology Portal. The dataset includes a training set of 6.5 hours of interictal data and 41 minin ictal data and a test set of 9.14 hours. Compared to the state-of-the-art on the same dataset, we achieve 49.4% increase in computational efficiency and 400 mins better in average for detection delay. The proposed model is able to detect a seizure onset at 91.95% sensitivity and 94.05% specificity with a mean detection delay of 2.77 s. The area under the curve (AUC) is 96.44%, that is comparable to the current state-of-the-art with AUC of 96.29%. version:1
arxiv-1701-08954 | CommAI: Evaluating the first steps towards a useful general AI | http://arxiv.org/abs/1701.08954 | id:1701.08954 author:Marco Baroni, Armand Joulin, Allan Jabri, Germàn Kruszewski, Angeliki Lazaridou, Klemen Simonic, Tomas Mikolov category:cs.LG cs.AI cs.CL  published:2017-01-31 summary:With machine learning successfully applied to new daunting problems almost every day, general AI starts looking like an attainable goal. However, most current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum. version:1
arxiv-1701-08951 | A Hybrid Approach for Secured Optimal Power Flow and Voltage Stability with TCSC Placement | http://arxiv.org/abs/1701.08951 | id:1701.08951 author:Sheila Mahapatra, Nitin Malik category:cs.NE cs.SY  published:2017-01-31 summary:This paper proposes a hybrid technique for secured optimal power flow coupled with enhancing voltage stability with FACTS device installation. The hybrid approach of Improved Gravitational Search algorithm (IGSA) and Firefly algorithm (FA) performance is analyzed by optimally placing TCSC controller. The algorithm is implemented in MATLAB working platform and the power flow security and voltage stability is evaluated with IEEE 30 bus transmission systems. The optimal results generated are compared with those available in literature and the superior performance of algorithm is depicted as minimum generation cost, reduced real power losses along with sustaining voltage stability. version:1
arxiv-1701-08946 | Variable selection for clustering with Gaussian mixture models: state of the art | http://arxiv.org/abs/1701.08946 | id:1701.08946 author:Abdelghafour Talibi, Boujemâa Achchab, Rafik Lasri category:stat.ML cs.LG  published:2017-01-31 summary:The mixture models have become widely used in clustering, given its probabilistic framework in which its based, however, for modern databases that are characterized by their large size, these models behave disappointingly in setting out the model, making essential the selection of relevant variables for this type of clustering. After recalling the basics of clustering based on a model, this article will examine the variable selection methods for model-based clustering, as well as presenting opportunities for improvement of these methods. version:1
arxiv-1701-08939 | Deep Submodular Functions | http://arxiv.org/abs/1701.08939 | id:1701.08939 author:Jeffrey Bilmes, Wenruo Bai category:cs.LG  published:2017-01-31 summary:We start with an overview of a class of submodular functions called SCMMs (sums of concave composed with non-negative modular functions plus a final arbitrary modular). We then define a new class of submodular functions we call {\em deep submodular functions} or DSFs. We show that DSFs are a flexible parametric family of submodular functions that share many of the properties and advantages of deep neural networks (DNNs). DSFs can be motivated by considering a hierarchy of descriptive concepts over ground elements and where one wishes to allow submodular interaction throughout this hierarchy. Results in this paper show that DSFs constitute a strictly larger class of submodular functions than SCMMs. We show that, for any integer $k>0$, there are $k$-layer DSFs that cannot be represented by a $k'$-layer DSF for any $k'<k$. This implies that, like DNNs, there is a utility to depth, but unlike DNNs, the family of DSFs strictly increase with depth. Despite this, we show (using a "backpropagation" like method) that DSFs, even with arbitrarily large $k$, do not comprise all submodular functions. In offering the above results, we also define the notion of an antitone superdifferential of a concave function and show how this relates to submodular functions (in general), DSFs (in particular), negative second-order partial derivatives, continuous submodularity, and concave extensions. To further motivate our analysis, we provide various special case results from matroid theory, comparing DSFs with forms of matroid rank, in particular the laminar matroid. Lastly, we discuss strategies to learn DSFs, and define the classes of deep supermodular functions, deep difference of submodular functions, and deep multivariate submodular functions, and discuss where these can be useful in applications. version:1
arxiv-1701-08936 | Deep Reinforcement Learning for Visual Object Tracking in Videos | http://arxiv.org/abs/1701.08936 | id:1701.08936 author:Da Zhang, Hamid Maei, Xin Wang, Yuan-Fang Wang category:cs.CV cs.LG  published:2017-01-31 summary:Convolutional neural network (CNN) models have achieved tremendous success in many visual detection and recognition tasks. Unfortunately, visual tracking, a fundamental computer vision problem, is not handled well using the existing CNN models, because most object trackers implemented with CNN do not effectively leverage temporal and contextual information among consecutive frames. Recurrent neural network (RNN) models, on the other hand, are often used to process text and voice data due to their ability to learn intrinsic representations of sequential and temporal data. Here, we propose a novel neural network tracking model that is capable of integrating information over time and tracking a selected target in video. It comprises three components: a CNN extracting best tracking features in each video frame, an RNN constructing video memory state, and a reinforcement learning (RL) agent making target location decisions. The tracking problem is formulated as a decision-making process, and our model can be trained with RL algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. We compare our model with an existing neural-network based tracking method and show that the proposed tracking approach works well in various scenarios by performing rigorous validation experiments on artificial video sequences with ground truth. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms. version:1
arxiv-1702-00001 | Learning the distribution with largest mean: two bandit frameworks | http://arxiv.org/abs/1702.00001 | id:1702.00001 author:Emilie Kaufmann, Aurélien Garivier category:cs.LG math.ST stat.ML stat.TH  published:2017-01-31 summary:Over the past few years, the multi-armed bandit model has become increasingly popular in the machine learning community, in part because of applications including online content optimization. This paper reviews two different sequential learning tasks that have been considered in the bandit literature ; they can be formulated as (sequentially) learning the distribution that has the highest mean among a set of distributions, with some constraints on the learning process. For both of them (regret minimization and best arm identification), we present (asymptotically) optimal algorithms, some of which are quite recent. We compare the behavior of the sampling rule of each algorithm as well as the complexity terms associated to each problem. version:1
arxiv-1701-08931 | Co-segmentation for Space-Time Co-located Collections | http://arxiv.org/abs/1701.08931 | id:1701.08931 author:Hadar Averbuch-Elor, Johannes Kopf, Tamir Hazan, Daniel Cohen-Or category:cs.CV  published:2017-01-31 summary:We present a co-segmentation technique for space-time co-located image collections. These prevalent collections capture various dynamic events, usually by multiple photographers, and may contain multiple co-occurring objects which are not necessarily part of the intended foreground object, resulting in ambiguities for traditional co-segmentation techniques. Thus, to disambiguate what the common foreground object is, we introduce a weakly-supervised technique, where we assume only a small seed, given in the form of a single segmented image. We take a distributed approach, where local belief models are propagated and reinforced with similar images. Our technique progressively expands the foreground and background belief models across the entire collection. The technique exploits the power of the entire set of image without building a global model, and thus successfully overcomes large variability in appearance of the common foreground object. We demonstrate that our method outperforms previous co-segmentation techniques on challenging space-time co-located collections, including dense benchmark datasets which were adapted for our novel problem setting. version:1
arxiv-1701-08918 | Feature Selection based on PCA and PSO for Multimodal Medical Image Fusion using DTCWT | http://arxiv.org/abs/1701.08918 | id:1701.08918 author:Padmavathi K, Mahima Bhat, Maya V Karki category:cs.CV  published:2017-01-31 summary:Multimodal medical image fusion helps to increase efficiency in medical diagnosis. This paper presents multimodal medical image fusion by selecting relevant features using Principle Component Analysis (PCA) and Particle Swarm Optimization techniques (PSO). DTCWT is used for decomposition of the images into low and high frequency coefficients. Fusion rules such as combination of minimum, maximum and simple averaging are applied to approximate and detailed coefficients. The fused image is reconstructed by inverse DTCWT. Performance metrics are evaluated and it shows that DTCWT-PCA performs better than DTCWT-PSO in terms of Structural Similarity Index Measure (SSIM) and Cross Correlation (CC). Computation time and feature vector size is reduced in DTCWT-PCA compared to DTCWT-PSO for feature selection which proves robustness and storage capacity. version:1
arxiv-1701-08916 | Statistical Archetypal Analysis | http://arxiv.org/abs/1701.08916 | id:1701.08916 author:Chenyue Wu, Esteban G. Tabak category:stat.ML  published:2017-01-31 summary:Statistical Archetypal Analysis (SAA) is introduced for the dimensional reduction of a collection of probability distributions known via samples. Applications include medical diagnosis from clinical data in the form of distributions (such as distributions of blood pressure or heart rates from different patients), the analysis of climate data such as temperature or wind speed at different locations, and the study of bifurcations in stochastic dynamical systems. Distributions can be embedded into a Hilbert space with a suitable metric, and then analyzed similarly to feature vectors in Euclidean space. However, most dimensional reduction techniques --such as Principal Component Analysis-- are not interpretable for distributions, as neither the components nor the reconstruction of input data by components are themselves distributions. To obtain an interpretable result, Archetypal Analysis (AA) is extended to distributions, requiring the components to be mixtures of the input distributions and approximating the input distributions by mixtures of components. version:1
arxiv-1701-08886 | SenseGen: A Deep Learning Architecture for Synthetic Sensor Data Generation | http://arxiv.org/abs/1701.08886 | id:1701.08886 author:Moustafa Alzantot, Supriyo Chakraborty, Mani B. Srivastava category:cs.LG cs.CV  published:2017-01-31 summary:Our ability to synthesize sensory data that preserves specific statistical properties of the real data has had tremendous implications on data privacy and big data analytics. The synthetic data can be used as a substitute for selective real data segments,that are sensitive to the user, thus protecting privacy and resulting in improved analytics.However, increasingly adversarial roles taken by data recipients such as mobile apps, or other cloud-based analytics services, mandate that the synthetic data, in addition to preserving statistical properties, should also be difficult to distinguish from the real data. Typically, visual inspection has been used as a test to distinguish between datasets. But more recently, sophisticated classifier models (discriminators), corresponding to a set of events, have also been employed to distinguish between synthesized and real data. The model operates on both datasets and the respective event outputs are compared for consistency. In this paper, we take a step towards generating sensory data that can pass a deep learning based discriminator model test, and make two specific contributions: first, we present a deep learning based architecture for synthesizing sensory data. This architecture comprises of a generator model, which is a stack of multiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network. second, we use another LSTM network based discriminator model for distinguishing between the true and the synthesized data. Using a dataset of accelerometer traces, collected using smartphones of users doing their daily activities, we show that the deep learning based discriminator model can only distinguish between the real and synthesized traces with an accuracy in the neighborhood of 50%. version:1
arxiv-1701-08869 | 3D Shape Retrieval via Irrelevance Filtering and Similarity Ranking (IF/SR) | http://arxiv.org/abs/1701.08869 | id:1701.08869 author:Xiaqing Pan, Yueru Chen, C. -C. Jay Kuo category:cs.CV  published:2017-01-30 summary:A novel solution for the content-based 3D shape retrieval problem using an unsupervised clustering approach, which does not need any label information of 3D shapes, is presented in this work. The proposed shape retrieval system consists of two modules in cascade: the irrelevance filtering (IF) module and the similarity ranking (SR) module. The IF module attempts to cluster gallery shapes that are similar to each other by examining global and local features simultaneously. However, shapes that are close in the local feature space can be distant in the global feature space, and vice versa. To resolve this issue, we propose a joint cost function that strikes a balance between two distances. Irrelevant samples that are close in the local feature space but distant in the global feature space can be removed in this stage. The remaining gallery samples are ranked in the SR module using the local feature. The superior performance of the proposed IF/SR method is demonstrated by extensive experiments conducted on the popular SHREC12 dataset. version:1
arxiv-1701-08840 | Spatial Projection of Multiple Climate Variables using Hierarchical Multitask Learning | http://arxiv.org/abs/1701.08840 | id:1701.08840 author:André R. Gonçalves, Arindam Banerjee, Fernando J. Von Zuben category:cs.LG stat.ML  published:2017-01-30 summary:Future projection of climate is typically obtained by combining outputs from multiple Earth System Models (ESMs) for several climate variables such as temperature and precipitation. While IPCC has traditionally used a simple model output average, recent work has illustrated potential advantages of using a multitask learning (MTL) framework for projections of individual climate variables. In this paper we introduce a framework for hierarchical multitask learning (HMTL) with two levels of tasks such that each super-task, i.e., task at the top level, is itself a multitask learning problem over sub-tasks. For climate projections, each super-task focuses on projections of specific climate variables spatially using an MTL formulation. For the proposed HMTL approach, a group lasso regularization is added to couple parameters across the super-tasks, which in the climate context helps exploit relationships among the behavior of different climate variables at a given spatial location. We show that some recent works on MTL based on learning task dependency structures can be viewed as special cases of HMTL. Experiments on synthetic and real climate data show that HMTL produces better results than decoupled MTL methods applied separately on the super-tasks and HMTL significantly outperforms baselines for climate projection. version:1
arxiv-1701-08837 | Emergence of Selective Invariance in Hierarchical Feed Forward Networks | http://arxiv.org/abs/1701.08837 | id:1701.08837 author:Dipan K. Pal, Vishnu Boddeti, Marios Savvides category:cs.LG  published:2017-01-30 summary:Many theories have emerged which investigate how in- variance is generated in hierarchical networks through sim- ple schemes such as max and mean pooling. The restriction to max/mean pooling in theoretical and empirical studies has diverted attention away from a more general way of generating invariance to nuisance transformations. We con- jecture that hierarchically building selective invariance (i.e. carefully choosing the range of the transformation to be in- variant to at each layer of a hierarchical network) is im- portant for pattern recognition. We utilize a novel pooling layer called adaptive pooling to find linear pooling weights within networks. These networks with the learnt pooling weights have performances on object categorization tasks that are comparable to max/mean pooling networks. In- terestingly, adaptive pooling can converge to mean pooling (when initialized with random pooling weights), find more general linear pooling schemes or even decide not to pool at all. We illustrate the general notion of selective invari- ance through object categorization experiments on large- scale datasets such as SVHN and ILSVRC 2012. version:1
arxiv-1701-08835 | Language Independent Single Document Image Super-Resolution using CNN for improved recognition | http://arxiv.org/abs/1701.08835 | id:1701.08835 author:Ram Krishna Pandey, A G Ramakrishnan category:cs.CV  published:2017-01-30 summary:Recognition of document images have important applications in restoring old and classical texts. The problem involves quality improvement before passing it to a properly trained OCR to get accurate recognition of the text. The image enhancement and quality improvement constitute important steps as subsequent recognition depends upon the quality of the input image. There are scenarios when high resolution images are not available and our experiments show that the OCR accuracy reduces significantly with decrease in the spatial resolution of document images. Thus the only option is to improve the resolution of such document images. The goal is to construct a high resolution image, given a single low resolution binary image, which constitutes the problem of single image super-resolution. Most of the previous work in super-resolution deal with natural images which have more information-content than the document images. Here, we use Convolution Neural Network to learn the mapping between low and the corresponding high resolution images. We experiment with different number of layers, parameter settings and non-linear functions to build a fast end-to-end framework for document image super-resolution. Our proposed model shows a very good PSNR improvement of about 4 dB on 75 dpi Tamil images, resulting in a 3 % improvement of word level accuracy by the OCR. It takes less time than the recent sparse based natural image super-resolution technique, making it useful for real-time document recognition applications. version:1
arxiv-1701-08816 | Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs | http://arxiv.org/abs/1701.08816 | id:1701.08816 author:Alexey A. Novikov, David Major, Dimitrios Lenis, Jiri Hladůvka, Maria Wimmer, Katja Bühler category:cs.CV cs.LG  published:2017-01-30 summary:The recent success of Deep Convolutional Neural Networks on image classification and recognition tasks has led to new applications in very diversifying contexts. One of these is medical imaging where scarcity and imbalance of training data has hindered rapid development of neural network related applications. This paper investigates and proposes neural network architectures within the context of automated segmentation of anatomical organs in chest radiographs, namely for lung, clavicles and heart. By relating prior class data distributions to the objective function sparsely represented structures are methodologically emphasized. Scarce training sets and data augmentation are encountered with aggressive data regularization. The problem of highly imbalanced target object appearance in the input data is solved by modifying the objective function. The models are trained and tested on the publicly available JSRT database consisting of 247 X-Ray images the ground-truth masks for which available in the SCR database. The networks have been trained in a multi-class setup with three target classes. Our best performing model trained with the negative Dice loss function was able to reach mean Jaccard overlap scores of 94.1\% for lungs, 86.6\% for heart and 88.7\% for clavicles in the multi-label setup, therefore, outperforming the best state-of-the art methods for heart and clavicle and human observer on lung and heart segmentation tasks. version:1
arxiv-1701-08810 | Algorithm selection of off-policy reinforcement learning algorithm | http://arxiv.org/abs/1701.08810 | id:1701.08810 author:Laroche Romain, Feraud Raphael category:stat.ML cs.AI cs.LG math.OC  published:2017-01-30 summary:Dialogue systems rely on a careful reinforcement learning design: the learning algorithm and its state space representation. In lack of more rigorous knowledge, the designer resorts to its practical experience to choose the best option. In order to automate and to improve the performance of the aforementioned process, this article formalises the problem of online off-policy reinforcement learning algorithm selection. A meta-algorithm is given for input a portfolio constituted of several off-policy reinforcement learning algorithms. It then determines at the beginning of each new trajectory, which algorithm in the portfolio is in control of the behaviour during the full next trajectory, in order to maximise the return. The article presents a novel meta-algorithm, called Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is to freeze the policy updates at each epoch, and to leave a rebooted stochastic bandit in charge of the algorithm selection. Under some assumptions, a thorough theoretical analysis demonstrates its near-optimality considering the structural sampling budget limitations. Then, ESBAS is put to the test in a set of experiments with various portfolios, on a negotiation dialogue game. The results show the practical benefits of the algorithm selection for dialogue systems, in most cases even outperforming the best algorithm in the portfolio, even when the aforementioned assumptions are transgressed. version:1
arxiv-1701-07847 | Structural Connectome Validation Using Pairwise Classification | http://arxiv.org/abs/1701.07847 | id:1701.07847 author:Dmitry Petrov, Boris Gutman, Alexander Ivanov, Joshua Faskowitz, Neda Jahanshad, Mikhail Belyaev, Paul Thompson category:q-bio.NC cs.CV  published:2017-01-26 summary:In this work, we study the extent to which structural connectomes and topological derivative measures are unique to individual changes within human brains. To do so, we classify structural connectome pairs from two large longitudinal datasets as either belonging to the same individual or not. Our data is comprised of 227 individuals from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and 226 from the Parkinson's Progression Markers Initiative (PPMI). We achieve 0.99 area under the ROC curve score for features which represent either weights or network structure of the connectomes (node degrees, PageRank and local efficiency). Our approach may be useful for eliminating noisy features as a preprocessing step in brain aging studies and early diagnosis classification problems. version:2
arxiv-1701-08796 | Learning from various labeling strategies for suicide-related messages on social media: An experimental study | http://arxiv.org/abs/1701.08796 | id:1701.08796 author:Tong Liu, Qijin Cheng, Christopher M. Homan, Vincent M. B. Silenzio category:cs.LG cs.CY cs.SI  published:2017-01-30 summary:Suicide is an important but often misunderstood problem, one that researchers are now seeking to better understand through social media. Due in large part to the fuzzy nature of what constitutes suicidal risks, most supervised approaches for learning to automatically detect suicide-related activity in social media require a great deal of human labor to train. However, humans themselves have diverse or conflicting views on what constitutes suicidal thoughts. So how to obtain reliable gold standard labels is fundamentally challenging and, we hypothesize, depends largely on what is asked of the annotators and what slice of the data they label. We conducted multiple rounds of data labeling and collected annotations from crowdsourcing workers and domain experts. We aggregated the resulting labels in various ways to train a series of supervised models. Our preliminary evaluations show that using unanimously agreed labels from multiple annotators is helpful to achieve robust machine models. version:1
arxiv-1701-08795 | A Mixture Model and Task Allocation Scheme for Crowdsourcing | http://arxiv.org/abs/1701.08795 | id:1701.08795 author:Irineo Cabreros, Karan Singh, Angela Zhou category:cs.LG stat.ML  published:2017-01-30 summary:Aggregating crowdsourced labels to generate reliable estimates of the true answer is an important challenge for crowdsourcing systems. We introduce the mixture of experts model, an extension of the classical Dawid-Skene [2] model, where each question is comprised of a mixture of k topics upon which each user has varying expertise. Under this new model, we (1) provide a spectral algorithm for estimating true labels with provable asymptotic guarantees and (2) introduce a task assignment algorithm in a quasi-online setting. We test these algorithms on simulated data, finding that both methods perform well for large budget allocations and that the assignment algorithm boosts performance in the low-sampling regime. As a consequence of this work, we also develop a simple method for efficiently fitting the mixture of experts model that may be more widely applicable. version:1
arxiv-1701-08789 | Understanding food inflation in India: A Machine Learning approach | http://arxiv.org/abs/1701.08789 | id:1701.08789 author:Akash Malhotra, Mayank Maloo category:stat.ML q-fin.EC  published:2017-01-30 summary:Over the past decade, the stellar growth of Indian economy has been challenged by persistently high levels of inflation, particularly in food prices. The primary reason behind this stubborn food inflation is mismatch in supply-demand, as domestic agricultural production has failed to keep up with rising demand owing to a number of proximate factors. The relative significance of these factors in determining the change in food prices have been analysed using gradient boosted regression trees (BRT), a machine learning technique. The results from BRT indicates all predictor variables to be fairly significant in explaining the change in food prices, with MSP and farm wages being relatively more important than others. International food prices were found to have limited relevance in explaining the variation in domestic food prices. The challenge of ensuring food and nutritional security for growing Indian population with rising incomes needs to be addressed through resolute policy reforms. version:1
arxiv-1701-08756 | Methodologies for realizing natural-language-facilitated human-robot cooperation: A review | http://arxiv.org/abs/1701.08756 | id:1701.08756 author:Rui Liu, Xiaoli Zhang category:cs.RO cs.AI cs.CL cs.HC  published:2017-01-30 summary:Natural Language (NL) for transferring knowledge from a human to a robot. Recently, research on using NL to support human-robot cooperation (HRC) has received increasing attention in several domains such as robotic daily assistance, robotic health caregiving, intelligent manufacturing, autonomous navigation and robot social accompany. However, a high-level review that can reveal the realization process and the latest methodologies of using NL to facilitate HRC is missing. In this review, a comprehensive summary about the methodology development of natural-language-facilitated human-robot cooperation (NLC) has been made. We first analyzed driving forces for NLC developments. Then, with a temporal realization order, we reviewed three main steps of NLC: human NL understanding, knowledge representation, and knowledge-world mapping. Last, based on our paper review and perspectives, potential research trends in NLC was discussed. version:1
arxiv-1701-08744 | Click Through Rate Prediction for Contextual Advertisment Using Linear Regression | http://arxiv.org/abs/1701.08744 | id:1701.08744 author:Muhammad Junaid Effendi, Syed Abbas Ali category:cs.IR cs.LG  published:2017-01-30 summary:This research presents an innovative and unique way of solving the advertisement prediction problem which is considered as a learning problem over the past several years. Online advertising is a multi-billion-dollar industry and is growing every year with a rapid pace. The goal of this research is to enhance click through rate of the contextual advertisements using Linear Regression. In order to address this problem, a new technique propose in this paper to predict the CTR which will increase the overall revenue of the system by serving the advertisements more suitable to the viewers with the help of feature extraction and displaying the advertisements based on context of the publishers. The important steps include the data collection, feature extraction, CTR prediction and advertisement serving. The statistical results obtained from the dynamically used technique show an efficient outcome by fitting the data close to perfection for the LR technique using optimized feature selection. version:1
arxiv-1701-08734 | PathNet: Evolution Channels Gradient Descent in Super Neural Networks | http://arxiv.org/abs/1701.08734 | id:1701.08734 author:Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, Daan Wierstra category:cs.NE cs.LG  published:2017-01-30 summary:For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks. Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm. During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation. Pathway fitness is the performance of that pathway measured according to a cost function. We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning. Paths evolved on task B re-use parts of the optimal path evolved on task A. Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training. Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C). version:1
arxiv-1701-08718 | Memory Augmented Neural Networks with Wormhole Connections | http://arxiv.org/abs/1701.08718 | id:1701.08718 author:Caglar Gulcehre, Sarath Chandar, Yoshua Bengio category:cs.LG cs.NE stat.ML  published:2017-01-30 summary:Recent empirical results on long-term dependency tasks have shown that neural networks augmented with an external memory can learn the long-term dependency tasks more easily and achieve better generalization than vanilla recurrent neural networks (RNN). We suggest that memory augmented neural networks can reduce the effects of vanishing gradients by creating shortcut (or wormhole) connections. Based on this observation, we propose a novel memory augmented neural network model called TARDIS (Temporal Automatic Relation Discovery in Sequences). The controller of TARDIS can store a selective set of embeddings of its own previous hidden states into an external memory and revisit them as and when needed. For TARDIS, memory acts as a storage for wormhole connections to the past to propagate the gradients more effectively and it helps to learn the temporal dependencies. The memory structure of TARDIS has similarities to both Neural Turing Machines (NTM) and Dynamic Neural Turing Machines (D-NTM), but both read and write operations of TARDIS are simpler and more efficient. We use discrete addressing for read/write operations which helps to substantially to reduce the vanishing gradient problem with very long sequences. Read and write operations in TARDIS are tied with a heuristic once the memory becomes full, and this makes the learning problem simpler when compared to NTM or D-NTM type of architectures. We provide a detailed analysis on the gradient propagation in general for MANNs. We evaluate our models on different long-term dependency tasks and report competitive results in all of them. version:1
arxiv-1701-08701 | Signal Recovery from Unlabeled Samples | http://arxiv.org/abs/1701.08701 | id:1701.08701 author:Saeid Haghighatshoar, Giuseppe Caire category:cs.IT math.IT stat.ML  published:2017-01-30 summary:In this paper, we study the recovery of a signal from a collection of unlabeled and possibly noisy measurements via a measurement matrix with random i.i.d. Gaussian components. We call the measurements unlabeled since their order is missing, namely, it is not known a priori which elements of the resulting measurements correspond to which row of the measurement matrix. We focus on the special case of ordered measurements, where only a subset of the measurements is kept and the order of the taken measurements is preserved. We identify a natural duality between this problem and the traditional Compressed Sensing, where we show that the unknown support (location of nonzero elements) of a sparse signal in Compressed Sensing corresponds in a natural way to the unknown location of the measurements kept in unlabeled sensing. While in Compressed Sensing it is possible to recover a sparse signal from an under-determined set of linear equations (less equations than the dimension of the signal), successful recovery in unlabeled sensing requires taking more samples than the dimension of the signal. We develop a low-complexity alternating minimization algorithm to recover the initial signal from the set of its unlabeled samples. We also study the behavior of the proposed algorithm for different signal dimensions and number of measurements both theoretically and empirically via numerical simulations. The results are a reminiscent of the phase-transition similar to that occurring in Compressed Sensing. version:1
arxiv-1701-08687 | Double/Debiased/Neyman Machine Learning of Treatment Effects | http://arxiv.org/abs/1701.08687 | id:1701.08687 author:Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey category:stat.ML stat.ME  published:2017-01-30 summary:Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (2016) provide a generic double/de-biased machine learning (DML) approach for obtaining valid inferential statements about focal parameters, using Neyman-orthogonal scores and cross-fitting, in settings where nuisance parameters are estimated using a new generation of nonparametric fitting methods for high-dimensional data, called machine learning methods. In this note, we illustrate the application of this method in the context of estimating average treatment effects (ATE) and average treatment effects on the treated (ATTE) using observational data. A more general discussion and references to the existing literature are available in Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (2016). version:1
arxiv-1701-08608 | Peduncle Detection of Sweet Pepper for Autonomous Crop Harvesting - Combined Colour and 3D Information | http://arxiv.org/abs/1701.08608 | id:1701.08608 author:Inkyu Sa, Chris Lehnert, Andrew English, Chris McCool, Feras Dayoub, Ben Upcroft, Tristan Perez category:cs.RO cs.CV  published:2017-01-30 summary:This paper presents a 3D visual detection method for the challenging task of detecting peduncles of sweet peppers (Capsicum annuum) in the field. Cutting the peduncle cleanly is one of the most difficult stages of the harvesting process, where the peduncle is the part of the crop that attaches it to the main stem of the plant. Accurate peduncle detection in 3D space is therefore a vital step in reliable autonomous harvesting of sweet peppers, as this can lead to precise cutting while avoiding damage to the surrounding plant. This paper makes use of both colour and geometry information acquired from an RGB-D sensor and utilises a supervised-learning approach for the peduncle detection task. The performance of the proposed method is demonstrated and evaluated using qualitative and quantitative results (the Area-Under-the-Curve (AUC) of the detection precision-recall curve). We are able to achieve an AUC of 0.71 for peduncle detection on field-grown sweet peppers. We release a set of manually annotated 3D sweet pepper and peduncle images to assist the research community in performing further research on this topic. version:1
arxiv-1701-08585 | A Unifying Framework for Guiding Point Processes with Stochastic Intensity Functions | http://arxiv.org/abs/1701.08585 | id:1701.08585 author:Yichen Wang, Grady Williams, Evangelos Theodorou, Le Song category:cs.LG cs.SI cs.SY math.OC  published:2017-01-30 summary:Temporal point processes are powerful tools to model event occurrences and have a plethora of applications in social sciences. While the majority of prior works focus on the modeling and learning of these processes, we consider the problem of how to design the optimal control policy for general point process with stochastic intensities, such that the stochastic system driven by the process is steered to a target state. In particular, we exploit the novel insight from the information theoretic formulations of stochastic optimal control. We further propose a novel convex optimization framework and a highly efficient online algorithm to update the policy adaptively to the current system state. Experiments on synthetic and real-world data show that our algorithm can steer the user activities much more accurately than state-of-arts. version:1
arxiv-1701-08533 | Graph-Based Semi-Supervised Conditional Random Fields For Spoken Language Understanding Using Unaligned Data | http://arxiv.org/abs/1701.08533 | id:1701.08533 author:Mohammad Aliannejadi, Masoud Kiaeeha, Shahram Khadivi, Saeed Shiry Ghidary category:cs.CL  published:2017-01-30 summary:We experiment graph-based Semi-Supervised Learning (SSL) of Conditional Random Fields (CRF) for the application of Spoken Language Understanding (SLU) on unaligned data. The aligned labels for examples are obtained using IBM Model. We adapt a baseline semi-supervised CRF by defining new feature set and altering the label propagation algorithm. Our results demonstrate that our proposed approach significantly improves the performance of the supervised model by utilizing the knowledge gained from the graph. version:1
arxiv-1701-08528 | Self-Adaptation of Activity Recognition Systems to New Sensors | http://arxiv.org/abs/1701.08528 | id:1701.08528 author:David Bannach, Martin Jänicke, Vitor F. Rey, Sven Tomforde, Bernhard Sick, Paul Lukowicz category:cs.CV cs.LG stat.ML 68T05 K.3.2  published:2017-01-30 summary:Traditional activity recognition systems work on the basis of training, taking a fixed set of sensors into account. In this article, we focus on the question how pattern recognition can leverage new information sources without any, or with minimal user input. Thus, we present an approach for opportunistic activity recognition, where ubiquitous sensors lead to dynamically changing input spaces. Our method is a variation of well-established principles of machine learning, relying on unsupervised clustering to discover structure in data and inferring cluster labels from a small number of labeled dates in a semi-supervised manner. Elaborating the challenges, evaluations of over 3000 sensor combinations from three multi-user experiments are presented in detail and show the potential benefit of our approach. version:1
arxiv-1701-08511 | Binary adaptive embeddings from order statistics of random projections | http://arxiv.org/abs/1701.08511 | id:1701.08511 author:Diego Valsesia, Enrico Magli category:cs.LG cs.IR  published:2017-01-30 summary:We use some of the largest order statistics of the random projections of a reference signal to construct a binary embedding that is adapted to signals correlated with such signal. The embedding is characterized from the analytical standpoint and shown to provide improved performance on tasks such as classification in a reduced-dimensionality space. version:1
arxiv-1701-08493 | A Survey on Structure from Motion | http://arxiv.org/abs/1701.08493 | id:1701.08493 author:Onur Ozyesil, Vladislav Voroninski, Ronen Basri, Amit Singer category:cs.CV  published:2017-01-30 summary:The structure from motion (SfM) problem in computer vision is the problem of recovering the $3$D structure of a stationary scene from a set of projective measurements, represented as a collection of $2$D images, via estimation of motion of the cameras corresponding to these images. In essence, SfM involves the three main stages of (1) extraction of features in images (e.g., points of interest, lines, etc.) and matching of these features between images, (2) camera motion estimation (e.g., using relative pairwise camera poses estimated from the extracted features), (3) recovery of the $3$D structure using the estimated motion and features (e.g., by minimizing the so-called reprojection error). This survey mainly focuses on the relatively recent developments in the literature pertaining to stages (2) and (3). More specifically, after touching upon the early factorization-based techniques for motion and structure estimation, we provide a detailed account of some of the recent camera location estimation methods in the literature, which precedes the discussion of notable techniques for $3$D structure recovery. We also cover the basics of the simultaneous localization and mapping (SLAM) problem, which can be considered to be a specific case of the SfM problem. Additionally, a review of the fundamentals of feature extraction and matching (i.e., stage (1) above), various recent methods for handling ambiguities in $3$D scenes, SfM techniques involving relatively uncommon camera models and image features, and popular sources of data and SfM software is included in our survey. version:1
arxiv-1701-08481 | CNN as Guided Multi-layer RECOS Transform | http://arxiv.org/abs/1701.08481 | id:1701.08481 author:C. -C. Jay Kuo category:cs.CV  published:2017-01-30 summary:There is a resurging interest in developing a neural-network-based solution to the supervised machine learning problem. The convolutional neural network (CNN), which is also known as the feedforward neural network and the multi-layer perceptron (MLP), will be studied in this note. To begin with, we introduce a RECOS transform as a basic building block of CNNs. The "RECOS" is an acronym for "REctified-COrrelations on a Sphere". It consists of two main concepts: 1) data clustering on a sphere and 2) rectification. Afterwards, we interpret a CNN as a network that implements the guided multi-layer RECOS transform with three highlights. First, we compare the traditional single-layer and modern multi-layer signal analysis approaches, point out key ingredients that enable the multi-layer approach, and provide a full explanation to the operating principle of CNNs. Second, we discuss how guidance is provided by labels through backpropagation in the training. Third, we show that a trained network can be greatly simplified in the testing stage demanding only one-bit representation for both filter weights and inputs. version:1
arxiv-1701-07879 | A Radically New Theory of how the Brain Represents and Computes with Probabilities | http://arxiv.org/abs/1701.07879 | id:1701.07879 author:Gerard Rinkus category:q-bio.NC cs.CV cs.NE  published:2017-01-26 summary:It is widely acknowledged that the brain i) implements probabilistic reasoning, and ii) represents information via population/distributed coding. Most previous population-based probabilistic (PPC) theories share several basic properties: 1) continuous (graded) neurons; 2) full/dense distributed coding: all/most coding field neurons formally participate in every code; 3) graded synapses; 4) rate coding; individual neurons are assumed to 5) have unimodal, e.g., bell-shaped, tuning functions (TFs) and 6) be fundamentally noisy; and 7) noise/correlation are generally viewed as problems requiring mitigation. In contrast, our theory assumes: 1) binary neurons; 2) only a small subset of neurons, i.e., a sparse distributed code (SDC), comprises any individual code; 3) binary synapses; 4) formally requires only single, i.e., first, spikes; individual neurons 5) initially have completely flat TFs (all weights zero) and 6) are not noisy; and 7) noise is a resource generated and used to cause similar inputs to map to similar codes, which controls a tradeoff between storage capacity and embedding the input space statistics in the pattern of intersections over the codes, indirectly yielding particular correlation patterns. The theory, Sparsey, was introduced 20 years ago as a canonical cortical circuit/algorithm model, but not discussed as a probabilistic model. Assuming input similarity correlates with likelihood, the active SDC code simultaneously represents both the most probable hypothesis and the full probability distribution. We show this for spatial and spatiotemporal (sequential) cases. Finally, consistent with moving beyond the Neuron Doctrine to the view that the SDC (cell assembly, ensemble) is the atomic unit of neural representation, Sparsey suggests that classical unimodal TFs emerge as an artifact of a single/few-trial learning process in which SDC codes are laid down in superposition. version:2
arxiv-1701-08466 | Predicting SMT Solver Performance for Software Verification | http://arxiv.org/abs/1701.08466 | id:1701.08466 author:Andrew Healy, Rosemary Monahan, James F. Power category:cs.SE cs.LG cs.LO  published:2017-01-30 summary:The Why3 IDE and verification system facilitates the use of a wide range of Satisfiability Modulo Theories (SMT) solvers through a driver-based architecture. We present Where4: a portfolio-based approach to discharge Why3 proof obligations. We use data analysis and machine learning techniques on static metrics derived from program source code. Our approach benefits software engineers by providing a single utility to delegate proof obligations to the solvers most likely to return a useful result. It does this in a time-efficient way using existing Why3 and solver installations - without requiring low-level knowledge about SMT solver operation from the user. version:1
arxiv-1701-08449 | SafeDrive: A Robust Lane Tracking System for Autonomous and Assisted Driving Under Limited Visibility | http://arxiv.org/abs/1701.08449 | id:1701.08449 author:Junaed Sattar, Jiawei Mo category:cs.RO cs.CV  published:2017-01-29 summary:We present an approach towards robust lane tracking for assisted and autonomous driving, particularly under poor visibility. Autonomous detection of lane markers improves road safety, and purely visual tracking is desirable for widespread vehicle compatibility and reducing sensor intrusion, cost, and energy consumption. However, visual approaches are often ineffective because of a number of factors, including but not limited to occlusion, poor weather conditions, and paint wear-off. Our method, named SafeDrive, attempts to improve visual lane detection approaches in drastically degraded visual conditions without relying on additional active sensors. In scenarios where visual lane detection algorithms are unable to detect lane markers, the proposed approach uses location information of the vehicle to locate and access alternate imagery of the road and attempts detection on this secondary image. Subsequently, by using a combination of feature-based and pixel-based alignment, an estimated location of the lane marker is found in the current scene. We demonstrate the effectiveness of our system on actual driving data from locations in the United States with Google Street View as the source of alternate imagery. version:1
arxiv-1701-08435 | Transformation-Based Models of Video Sequences | http://arxiv.org/abs/1701.08435 | id:1701.08435 author:Joost van Amersfoort, Anitha Kannan, Marc'Aurelio Ranzato, Arthur Szlam, Du Tran, Soumith Chintala category:cs.LG cs.CV  published:2017-01-29 summary:In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model. In order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost. version:1
arxiv-1701-08423 | One Size Fits All : Effectiveness of Local Search on Structured Data | http://arxiv.org/abs/1701.08423 | id:1701.08423 author:Vincent Cohen-Addad, Chris Schwiegelshohn category:cs.DS cs.CG cs.LG  published:2017-01-29 summary:In this paper, we analyze the performance of a simple and standard Local Search algorithm for clustering on well behaved data. Since the seminal paper by Ostrovsky, Rabani, Schulman and Swamy [FOCS 2006], much progress has been made to characterize real-world instances. We distinguish the three main definitions -- Distribution Stability (Awasthi, Blum, Sheffet, FOCS 2010) -- Spectral Separability (Kumar, Kannan, FOCS 2010) -- Perturbation Resilience (Bilu, Linial, ICS 2010) We show that Local Search performs well on the instances with the aforementioned stability properties. Specifically, for the $k$-means and $k$-median objective, we show that Local Search exactly recovers the optimal clustering if the dataset is $3+\varepsilon$-perturbation resilient, and is a PTAS for distribution stability and spectral separability. This implies the first PTAS for instances satisfying the spectral separability condition. For the distribution stability condition we also go beyond previous work by showing that the clustering output by the algorithm and the optimal clustering are very similar. This is a significant step toward understanding the success of Local Search heuristics in clustering applications and supports the legitimacy of the stability conditions: They characterize some of the structure of real-world instances that make Local Search a popular heuristic. version:1
arxiv-1701-09046 | An Extremal Optimization approach to parallel resonance constrained capacitor placement problem | http://arxiv.org/abs/1701.09046 | id:1701.09046 author:André R. Goncalves, Celso Cavelucci, Christiano Lyra Filho, Fernando J. Von Zuben category:math.OC cs.NE  published:2017-01-29 summary:Installation of capacitors in distribution networks is one of the most used procedure to compensate reactive power generated by loads and, consequently, to reduce technical losses. So, the problem consists in identifying the optimal placement and sizing of capacitors. This problem is known in the literature as optimal capacitor placement problem. Neverthless, depending on the location and size of the capacitor, it may become a harmonic source, allowing capacitor to enter into resonance with the distribution network, causing several undesired side effects. In this work we propose a parsimonious method to deal with the capacitor placement problem that incorporates resonance constraints, ensuring that every allocated capacitor will not act as a harmonic source. This proposed algorithm is based upon a physical inspired metaheuristic known as Extremal Optimization. The results achieved showed that this proposal has reached significant gains when compared with other proposals that attempt repair, in a post-optimization stage, already obtained solutions which violate resonance constraints. version:1
