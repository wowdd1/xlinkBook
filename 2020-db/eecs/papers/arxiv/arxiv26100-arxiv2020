arxiv-1703-02185 | Indoor Localization by Fusing a Group of Fingerprints Based on Random Forests | http://arxiv.org/abs/1703.02185 | id:1703.02185 author:Xiansheng Guo, Nirwan Ansari, Huiyong Li category:stat.ML cs.CV  published:2017-03-07 summary:Indoor localization based on SIngle Of Fingerprint (SIOF) is rather susceptible to the changing environment, multipath, and non-line-of-sight (NLOS) propagation. Building SIOF is also a very time-consuming process. Recently, we first proposed a GrOup Of Fingerprints (GOOF) to improve the localization accuracy and reduce the burden of building fingerprints. However, the main drawback is the timeliness. In this paper, we propose a novel localization framework by Fusing A Group Of fingerprinTs (FAGOT) based on random forests. In the offline phase, we first build a GOOF from different transformations of the received signals of multiple antennas. Then, we design multiple GOOF strong classifiers based on Random Forests (GOOF-RF) by training each fingerprint in the GOOF. In the online phase, we input the corresponding transformations of the real measurements into these strong classifiers to obtain multiple independent decisions. Finally, we propose a Sliding Window aIded Mode-based (SWIM) fusion algorithm to balance the localization accuracy and time. Our proposed approaches can work better in an unknown indoor scenario. The burden of building fingerprints can also be reduced drastically. We demonstrate the performance of our algorithms through simulations and real experimental data using two Universal Software Radio Peripheral (USRP) platforms. version:1
arxiv-1703-02184 | Indoor Localization Using Visible Light Via Fusion Of Multiple Classifiers | http://arxiv.org/abs/1703.02184 | id:1703.02184 author:Xiansheng Guo, Sihua Shao, Nirwan Ansari, Abdallah Khreishah category:stat.ML cs.CV  published:2017-03-07 summary:A multiple classifiers fusion localization technique using received signal strengths (RSSs) of visible light is proposed, in which the proposed system transmits different intensity modulated sinusoidal signals by LEDs and the signals received by a Photo Diode (PD) placed at various grid points. First, we obtain some {\emph{approximate}} received signal strengths (RSSs) fingerprints by capturing the peaks of power spectral density (PSD) of the received signals at each given grid point. Unlike the existing RSSs based algorithms, several representative machine learning approaches are adopted to train multiple classifiers based on these RSSs fingerprints. The multiple classifiers localization estimators outperform the classical RSS-based LED localization approaches in accuracy and robustness. To further improve the localization performance, two robust fusion localization algorithms, namely, grid independent least square (GI-LS) and grid dependent least square (GD-LS), are proposed to combine the outputs of these classifiers. We also use a singular value decomposition (SVD) based LS (LS-SVD) method to mitigate the numerical stability problem when the prediction matrix is singular. Experiments conducted on intensity modulated direct detection (IM/DD) systems have demonstrated the effectiveness of the proposed algorithms. The experimental results show that the probability of having mean square positioning error (MSPE) of less than 5cm achieved by GD-LS is improved by 93.03\% and 93.15\%, respectively, as compared to those by the RSS ratio (RSSR) and RSS matching methods with the FFT length of 2000. version:1
arxiv-1703-00439 | Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization | http://arxiv.org/abs/1703.00439 | id:1703.00439 author:Tomoya Murata, Taiji Suzuki category:math.OC cs.LG stat.ML  published:2017-03-01 summary:In this paper, we develop a new accelerated stochastic gradient method for efficiently solving the convex regularized empirical risk minimization problem in mini-batch settings. The use of mini-batches is becoming a golden standard in the machine learning community, because mini-batch settings stabilize the gradient estimate and can easily make good use of parallel computing. The core of our proposed method is the incorporation of our new "double acceleration" technique and variance reduction technique. We theoretically analyze our proposed method and show that our method much improves the mini-batch efficiencies of previous accelerated stochastic methods, and essentially only needs size $\sqrt{n}$ mini-batches for achieving the optimal iteration complexities for both non-strongly and strongly convex objectives, where $n$ is the training set size. Further, we show that even in non-mini-batch settings, our method surpasses the best known convergence rate for non-strongly convex objectives, and it achieves the one for strongly convex objectives. version:2
arxiv-1703-02168 | Deep View Morphing | http://arxiv.org/abs/1703.02168 | id:1703.02168 author:Dinghuang Ji, Junghyun Kwon, Max McFarland, Silvio Savarese category:cs.CV  published:2017-03-07 summary:Recently, convolutional neural networks (CNN) have been successfully applied to view synthesis problems. However, such CNN-based methods can suffer from lack of texture details, shape distortions, or high computational complexity. In this paper, we propose a novel CNN architecture for view synthesis called "Deep View Morphing" that does not suffer from these issues. To synthesize a middle view of two input images, a rectification network first rectifies the two input images. An encoder-decoder network then generates dense correspondences between the rectified images and blending masks to predict the visibility of pixels of the rectified images in the middle view. A view morphing network finally synthesizes the middle view using the dense correspondences and blending masks. We experimentally show the proposed method significantly outperforms the state-of-the-art CNN-based view synthesis method. version:1
arxiv-1703-02166 | Building a Syllable Database to Solve the Problem of Khmer Word Segmentation | http://arxiv.org/abs/1703.02166 | id:1703.02166 author:Nam Tran Van category:cs.CL  published:2017-03-07 summary:Word segmentation is a basic problem in natural language processing. With the languages having the complex writing system like the Khmer language in Southern of Vietnam, this problem really very intractable, posing the significant challenges. Although there are some experts in Vietnam as well as international having deeply researched this problem, there are still no reasonable results meeting the demand, in particular, no treated thoroughly the ambiguous phenomenon, in the process of Khmer language processing so far. This paper present a solution based on the syllable division into component clusters using two syllable models proposed, thereby building a Khmer syllable database, is still not actually available. This method using a lexical database updated from the online Khmer dictionaries and some supported dictionaries serving role of training data and complementary linguistic characteristics. Each component cluster is labelled and located by the first and last letter to identify entirety a syllable. This approach is workable and the test results achieve high accuracy, eliminate the ambiguity, contribute to solving the problem of word segmentation and applying efficiency in Khmer language processing. version:1
arxiv-1703-02161 | Distance Metric Learning using Graph Convolutional Networks: Application to Functional Brain Networks | http://arxiv.org/abs/1703.02161 | id:1703.02161 author:Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew Lee, Ben Glocker, Daniel Rueckert category:cs.CV cs.LG  published:2017-03-07 summary:Evaluating similarity between graphs is of major importance in several computer vision and pattern recognition problems, where graph representations are often used to model objects or interactions between elements. The choice of a distance or similarity metric is, however, not trivial and can be highly dependent on the application at hand. In this work, we propose a novel metric learning method to evaluate distance between graphs that leverages the power of convolutional neural networks, while exploiting concepts from spectral graph theory to allow these operations on irregular graphs. We demonstrate the potential of our method in the field of connectomics, where neuronal pathways or functional connections between brain regions are commonly modelled as graphs. In this problem, the definition of an appropriate graph similarity function is critical to unveil patterns of disruptions associated with certain brain disorders. Experimental results on the ABIDE dataset show that our method can learn a graph similarity metric tailored for a clinical application, improving the performance of a simple k-nn classifier by 11.9% compared to a traditional distance metric. version:1
arxiv-1703-02156 | On the Limits of Learning Representations with Label-Based Supervision | http://arxiv.org/abs/1703.02156 | id:1703.02156 author:Jiaming Song, Russell Stewart, Shengjia Zhao, Stefano Ermon category:cs.LG cs.AI stat.ML  published:2017-03-07 summary:Advances in neural network based classifiers have transformed automatic feature learning from a pipe dream of stronger AI to a routine and expected property of practical systems. Since the emergence of AlexNet every winning submission of the ImageNet challenge has employed end-to-end representation learning, and due to the utility of good representations for transfer learning, representation learning has become as an important and distinct task from supervised learning. At present, this distinction is inconsequential, as supervised methods are state-of-the-art in learning transferable representations. But recent work has shown that generative models can also be powerful agents of representation learning. Will the representations learned from these generative methods ever rival the quality of those from their supervised competitors? In this work, we argue in the affirmative, that from an information theoretic perspective, generative models have greater potential for representation learning. Based on several experimentally validated assumptions, we show that supervised learning is upper bounded in its capacity for representation learning in ways that certain generative models, such as Generative Adversarial Networks (GANs) are not. We hope that our analysis will provide a rigorous motivation for further exploration of generative representation learning. version:1
arxiv-1703-02155 | Model-Based Multiple Instance Learning | http://arxiv.org/abs/1703.02155 | id:1703.02155 author:Ba-Ngu Vo, Dinh Phung, Quang N. Tran, Ba-Tuong Vo category:stat.ML cs.LG  published:2017-03-07 summary:While Multiple Instance (MI) data are point patterns -- sets or multi-sets of unordered points -- appropriate statistical point pattern models have not been used in MI learning. This article proposes a framework for model-based MI learning using point process theory. Likelihood functions for point pattern data derived from point process theory enable principled yet conceptually transparent extensions of learning tasks, such as classification, novelty detection and clustering, to point pattern data. Furthermore, tractable point pattern models as well as solutions for learning and decision making from point pattern data are developed. version:1
arxiv-1703-02150 | LiDAR Point Cloud Segmentation via Minimum-cost Perfect Matching in a Bipartite Graph | http://arxiv.org/abs/1703.02150 | id:1703.02150 author:Sheng Xu, Ruisheng Wang, Han Zheng category:cs.CV  published:2017-03-06 summary:This paper proposes an optimization model based on the minimum-cost perfect matching in a bipartite graph for the segmentation of 3D mobile LiDAR point clouds. The segmentation is formed as a maximum a posteriori estimation. The penalty function is based on Euclidean distances and normal vectors of the points. To deal with the Gaussian noise generated in the data collection, a robust estimation is introduced in the optimization using a novel robust estimator. The objective function is minimized by a new minimum-cost perfect matching model based on a formed bipartite graph. The evaluation is on a large-scale residential and urban points. Results show that the presented model succeeds to achieve the optimal segmentation from multiple scenes automatically and is superior to state-of-the-art LiDAR point segmentation approaches in terms of the accuracy and robustness. version:1
arxiv-1703-02144 | Contextual Motifs: Increasing the Utility of Motifs using Contextual Data | http://arxiv.org/abs/1703.02144 | id:1703.02144 author:Ian Fox, Lynn Ang, Mamta Jaiswal, Rodica Pop-Busui, Jenna Wiens category:cs.LG 68T05  published:2017-03-06 summary:Motifs are a powerful tool for analyzing long physiological signals. Standard motif methods, however, ignore important contextual information (e.g., what the patient was doing at the time the data were collected). We hypothesize that these additional contextual data could increase the utility of motifs. Thus, we propose an extension to motifs, \textit{contextual motifs}, that incorporates context. We present methods to discover contextual motifs, both with observed and inferred contextual information. Oftentimes, we may not observe context, or collecting context may simply be too burdensome. In this setting, we present methods to jointly infer motifs and context. Through experiments on simulated data, we illustrate the potential discriminative power of contextual motifs across a range of settings, improving discriminative performance, measured using AUROC, by up to 11 percentage points over a contextless baseline. We further validate the proposed approach on a dataset of continuous glucose monitor data collected from type 1 diabetics. Applied to the task of predicting hypo- and hyper-glycemic events, use of contextual motifs led to a 7.2 percentage point improvement in AUROC compared with a state-of-the-art baseline. version:1
arxiv-1703-02136 | English Conversational Telephone Speech Recognition by Humans and Machines | http://arxiv.org/abs/1703.02136 | id:1703.02136 author:George Saon, Gakuto Kurata, Tom Sercu, Kartik Audhkhasi, Samuel Thomas, Dimitrios Dimitriadis, Xiaodong Cui, Bhuvana Ramabhadran, Michael Picheny, Lynn-Li Lim, Bergul Roomi, Phil Hall category:cs.CL  published:2017-03-06 summary:One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models. version:1
arxiv-1703-02111 | Classification and clustering for samples of event time data using non-homogeneous Poisson process models | http://arxiv.org/abs/1703.02111 | id:1703.02111 author:Duncan Barrack, James Goulding, Simon Preston category:cs.LG stat.ML  published:2017-03-06 summary:Data of the form of event times arise in various applications. A simple model for such data is a non-homogeneous Poisson process (NHPP) which is specified by a rate function that depends on time. We consider the problem of having access to multiple independent samples of event time data, observed on a common interval, from which we wish to classify or cluster the samples according to their rate functions. Each rate function is unknown but assumed to belong to a small set of rate functions defining distinct classes. We model the rate functions using a spline basis expansion, the coefficients of which need to be estimated from data. The classification approach consists of using training data for which the class membership is known and to calculate maximum likelihood estimates of the coefficients for each group, then assigning test samples to a class by a maximum likelihood criterion. For clustering, by analogy to the Gaussian mixture model approach for Euclidean data, we consider a mixture of NHPP models and use the expectation maximisation algorithm to estimate the coefficients of the rate functions for the component models and probability of membership for each sample to each model. The classification and clustering approaches perform well on both synthetic and real-world data sets considered. Code associated with this paper is available at https://github.com/duncan-barrack/NHPP. version:1
arxiv-1703-02100 | Guarantees for Greedy Maximization of Non-submodular Functions with Applications | http://arxiv.org/abs/1703.02100 | id:1703.02100 author:Andrew An Bian, Joachim M. Buhmann, Andreas Krause, Sebastian Tschiatschek category:cs.DM cs.AI cs.DS cs.LG math.OC  published:2017-03-06 summary:We investigate the performance of the Greedy algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of Greedy for maximizing submodular functions, there are few guarantees for non-submodular ones. However, Greedy enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our guarantees are characterized by the (generalized) submodularity ratio $\gamma$ and the (generalized) curvature $\alpha$. In particular, we prove that Greedy enjoys a tight approximation guarantee of $\frac{1}{\alpha}(1- e^{-\gamma\alpha})$ for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, e.g., the Bayesian A-optimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for several real-world applications. version:1
arxiv-1703-04393 | Randomized Iterative Reconstruction for Sparse View X-ray Computed Tomography | http://arxiv.org/abs/1703.04393 | id:1703.04393 author:D. Trinca, Y. Zhong category:cs.CV  published:2017-03-06 summary:With the availability of more powerful computers, iterative reconstruction algorithms are the subject of an ongoing work in the design of more efficient reconstruction algorithms for X-ray computed tomography. In this work, we show how two analytical reconstruction algorithms can be improved by correcting the corresponding reconstructions using a randomized iterative reconstruction algorithm. The combined analytical reconstruction followed by randomized iterative reconstruction can also be viewed as a reconstruction algorithm which, in the experiments we have conducted, uses up to $35\%$ less projection angles as compared to the analytical reconstruction algorithms and produces the same results in terms of quality of reconstruction, without increasing the execution time significantly. version:1
arxiv-1703-02083 | Auto-context Convolutional Neural Network for Geometry-Independent Brain Extraction in Magnetic Resonance Imaging | http://arxiv.org/abs/1703.02083 | id:1703.02083 author:Seyed Sadegh Mohseni Salehi, Deniz Erdogmus, Ali Gholipour category:cs.CV  published:2017-03-06 summary:Brain extraction or whole brain segmentation is an important first step in many of the neuroimage analysis pipelines. The accuracy and robustness of brain extraction, therefore, is crucial for the accuracy of the entire brain analysis process. With the aim of designing a learning-based, geometry-independent and registration-free brain extraction tool in this study, we present a technique based on an auto-context convolutional neural network (CNN), in which intrinsic local and global image features are learned through 2D patches of different window sizes. In this architecture three parallel 2D convolutional pathways for three different directions (axial, coronal, and sagittal) implicitly learn 3D image information without the need for computationally expensive 3D convolutions. Posterior probability maps generated by the network are used iteratively as context information along with the original image patches to learn the local shape and connectedness of the brain, to extract it from non-brain tissue. The brain extraction results we have obtained from our algorithm are superior to the recently reported results in the literature on two publicly available benchmark datasets, namely LPBA40 and OASIS, in which we obtained Dice overlap coefficients of 97.42% and 95.40%, respectively. Furthermore, we evaluated the performance of our algorithm in the challenging problem of extracting arbitrarily-oriented fetal brains in reconstructed fetal brain magnetic resonance imaging (MRI) datasets. In this application our algorithm performed much better than the other methods (Dice coefficient: 95.98%), where the other methods performed poorly due to the non-standard orientation and geometry of the fetal brain in MRI. Our CNN-based method can provide accurate, geometry-independent brain extraction in challenging applications. version:1
arxiv-1703-00565 | Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ | http://arxiv.org/abs/1703.00565 | id:1703.00565 author:Jason S. Kessler category:cs.CL cs.IR  published:2017-03-02 summary:Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents. Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them. Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics. version:2
arxiv-1703-02065 | On the Expressive Power of Overlapping Operations of Deep Networks | http://arxiv.org/abs/1703.02065 | id:1703.02065 author:Or Sharir, Amnon Shashua category:cs.LG cs.NE stat.ML  published:2017-03-06 summary:Expressive Efficiency with respect to a network architectural attribute P refers to the property where an architecture without P must grow exponentially large in order to approximate the expressivity of a network with attribute P. For example, it is known that depth is an architectural attribute that generates exponential efficiency in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this paper we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of "overlaps" in the convolutional process, i.e., when the stride of the convolution is smaller than its kernel size (receptive field). Our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers. version:1
arxiv-1703-02059 | Cheshire: An Online Algorithm for Activity Maximization in Social Networks | http://arxiv.org/abs/1703.02059 | id:1703.02059 author:Ali Zarezade, Abir De, Hamid Rabiee, Manuel Gomez Rodriguez category:stat.ML cs.DS cs.LG cs.SI  published:2017-03-06 summary:User engagement in social networks depends critically on the number of online actions their users take in the network. Can we design an algorithm that finds when to incentivize users to take actions to maximize the overall activity in a social network? In this paper, we model the number of online actions over time using multidimensional Hawkes processes, derive an alternate representation of these processes based on stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, address the above question from the perspective of stochastic optimal control of SDEs with jumps. We find that the optimal level of incentivized actions depends linearly on the current level of overall actions. Moreover, the coefficients of this linear relationship can be found by solving a matrix Riccati differential equation, which can be solved efficiently, and a first order differential equation, which has a closed form solution. As a result, we are able to design an efficient online algorithm, Cheshire, to sample the optimal times of the users' incentivized actions. Experiments on both synthetic and real data gathered from Twitter show that our algorithm is able to consistently maximize the number of online actions more effectively than the state of the art. version:1
arxiv-1703-02019 | Performing Stance Detection on Twitter Data using Computational Linguistics Techniques | http://arxiv.org/abs/1703.02019 | id:1703.02019 author:Gourav G. Shenoy, Erika H. Dsouza, Sandra Kübler category:cs.CL I.2.7  published:2017-03-06 summary:As humans, we can often detect from a persons utterances if he or she is in favor of or against a given target entity (topic, product, another person, etc). But from the perspective of a computer, we need means to automatically deduce the stance of the tweeter, given just the tweet text. In this paper, we present our results of performing stance detection on twitter data using a supervised approach. We begin by extracting bag-of-words to perform classification using TIMBL, then try and optimize the features to improve stance detection accuracy, followed by extending the dataset with two sets of lexicons - arguing, and MPQA subjectivity; next we explore the MALT parser and construct features using its dependency triples, finally we perform analysis using Scikit-learn Random Forest implementation. version:1
arxiv-1703-02018 | Combining Self-Supervised Learning and Imitation for Vision-Based Rope Manipulation | http://arxiv.org/abs/1703.02018 | id:1703.02018 author:Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola, Pieter Abbeel, Jitendra Malik, Sergey Levine category:cs.CV cs.LG cs.RO  published:2017-03-06 summary:Manipulation of deformable objects, such as ropes and cloth, is an important but challenging problem in robotics. We present a learning-based system where a robot takes as input a sequence of images of a human manipulating a rope from an initial to goal configuration, and outputs a sequence of actions that can reproduce the human demonstration, using only monocular images as input. To perform this task, the robot learns a pixel-level inverse dynamics model of rope manipulation directly from images in a self-supervised manner, using about 60K interactions with the rope collected autonomously by the robot. The human demonstration provides a high-level plan of what to do and the low-level inverse model is used to execute the plan. We show that by combining the high and low-level plans, the robot can successfully manipulate a rope into a variety of target shapes using only a sequence of human-provided images for direction. version:1
arxiv-1703-02016 | Fast Back-Projection for Non-Line of Sight Reconstruction | http://arxiv.org/abs/1703.02016 | id:1703.02016 author:Victor Arellano, Diego Gutierrez, Adrian Jarabo category:cs.CV cs.GR  published:2017-03-06 summary:Recent works have demonstrated non-line of sight (NLOS) reconstruction by using the time-resolved signal frommultiply scattered light. These works combine ultrafast imaging systems with computation, which back-projects the recorded space-time signal to build a probabilistic map of the hidden geometry. Unfortunately, this computation is slow, becoming a bottleneck as the imaging technology improves. In this work, we propose a new back-projection technique for NLOS reconstruction, which is up to a thousand times faster than previous work, with almost no quality loss. We base on the observation that the hidden geometry probability map can be built as the intersection of the three-bounce space-time manifolds defined by the light illuminating the hidden geometry and the visible point receiving the scattered light from such hidden geometry. This allows us to pose the reconstruction of the hidden geometry as the voxelization of these space-time manifolds, which has lower theoretic complexity and is easily implementable in the GPU. We demonstrate the efficiency and quality of our technique compared against previous methods in both captured and synthetic data version:1
arxiv-1703-02009 | Learning across scales - A multiscale method for Convolution Neural Networks | http://arxiv.org/abs/1703.02009 | id:1703.02009 author:Eldad Haber, Lars Ruthotto, Elliot Holtham category:cs.NE cs.CV  published:2017-03-06 summary:In this work we explore the connection between Convolution Neural Networks, partial differential equations, multigrid/multiscale methods and and optimal control. We show that convolution neural networks can be represented as a discretization of nonlinear partial differential equations, and that the learning process can be interpreted as a control problem where we attempt to estimate the coefficients of the differential equations from data. This interpretation allows us to generate an efficient multilevel/multiscale process (sometimes referred to as image pyramid) and algorithms for the learning problem that can substantially reduce the cost of learning. Furthermore, this interpretation allows us to use the coefficients that are calculated on high resolution images for low resolution images directly. version:1
arxiv-1703-02000 | Generative Adversarial Nets with Labeled Data by Activation Maximization | http://arxiv.org/abs/1703.02000 | id:1703.02000 author:Zhiming Zhou, Shu Rong, Han Cai, Weinan Zhang, Yong Yu, Jun Wang category:cs.LG cs.AI cs.CV stat.ML  published:2017-03-06 summary:In this paper, we study the impact and role of multi-class labels on adversarial training for generative adversarial nets (GANs). Our derivation of the gradient shows that the current GAN model with labeled data still results in undesirable properties due to the overlay of the gradients from multiple classes. We thus argue that a better gradient should follow the intensity and direction that maximize each sample's activation on one and the only one class in each iteration, rather than weighted-averaging their gradients. We show, mathematically, that the proposed activation-maximized adversarial training (AM-GAN) is a general one covering two major complementary solutions exploring labeled information. Additionally, we investigate related metrics for evaluating generative models. Empirical experiments show that our approach has achieved the best Inception score (8.34) compared with previously reported results. Moreover, our adversarial training produces faster convergence with no mode collapse observed. version:1
arxiv-1703-01988 | Neural Episodic Control | http://arxiv.org/abs/1703.01988 | id:1703.01988 author:Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adrià Puigdomènech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles Blundell category:cs.LG stat.ML  published:2017-03-06 summary:Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents. version:1
arxiv-1703-01976 | Incorporating the Knowledge of Dermatologists to Convolutional Neural Networks for the Diagnosis of Skin Lesions | http://arxiv.org/abs/1703.01976 | id:1703.01976 author:Iván González Díaz category:cs.CV  published:2017-03-06 summary:This report describes our system towards the automatic diagnosis of skin lesions. We aim to incorporate the expert knowledge of dermatologists into the well known framework of Convolutional Neural Networks (CNN), which have shown impressive performance in many visual recognition tasks. In particular, we have designed several networks providing lesion area identification, lesion segmentation into structural patterns and final diagnosis of clinical cases. Furthermore, novel blocks for CNNs have been designed to integrate this information with the diagnosis processing pipeline. version:1
arxiv-1703-01973 | Batched High-dimensional Bayesian Optimization via Structural Kernel Learning | http://arxiv.org/abs/1703.01973 | id:1703.01973 author:Zi Wang, Chengtao Li, Stefanie Jegelka, Pushmeet Kohli category:stat.ML  published:2017-03-06 summary:Optimization of high-dimensional black-box functions is an extremely challenging problem. While Bayesian optimization has emerged as a popular approach for optimizing black-box functions, its applicability has been limited to low-dimensional problems due to its computational and statistical challenges arising from high-dimensional settings. In this paper, we propose to tackle these challenges by (1) assuming a latent additive structure in the function and inferring it properly for more efficient and effective BO, and (2) performing multiple evaluations in parallel to reduce the number of iterations required by the method. Our novel approach learns the latent structure with Gibbs sampling and constructs batched queries using determinantal point processes. Experimental validations on both synthetic and real-world functions demonstrate that the proposed method significantly outperforms existing state-of-the-art approaches. version:1
arxiv-1703-01972 | Mesh-to-raster based non-rigid registration of multi-modal images | http://arxiv.org/abs/1703.01972 | id:1703.01972 author:Rosalia Tatano, Benjamin Berkels, Thomas M. Deserno category:cs.CV  published:2017-03-06 summary:Region of interest (ROI) alignment in medical images plays a crucial role in diagnostics, procedure planning, treatment, and follow-up. Frequently, a model is represented as triangulated mesh while the patient data is provided from CAT scanners as pixel or voxel data. Previously, we presented a 2D method for curve-to-pixel registration. This paper contributes (i) a general mesh-to-raster (M2R) framework to register ROIs in multi-modal images; (ii) a 3D surface-to-voxel application, and (iii) a comprehensive quantitative evaluation in 2D using ground truth provided by the simultaneous truth and performance level estimation (STAPLE) method. The registration is formulated as a minimization problem where the objective consists of a data term, which involves the signed distance function of the ROI from the reference image, and a higher order elastic regularizer for the deformation. The evaluation is based on quantitative light-induced fluoroscopy (QLF) and digital photography (DP) of decalcified teeth. STAPLE is computed on 150 image pairs from 32 subjects, each showing one corresponding tooth in both modalities. The ROI in each image is manually marked by three experts (900 curves in total). In the QLF-DP setting, our approach significantly outperforms the Insight Segmentation and Registration Toolkit (ITK) mutual information-based registration algorithm. version:1
arxiv-1703-01970 | Concentration Bounds for High Sensitivity Functions Through Differential Privacy | http://arxiv.org/abs/1703.01970 | id:1703.01970 author:Kobbi Nissim, Uri Stemmer category:cs.LG  published:2017-03-06 summary:A new line of work [Dwork et al. STOC 2015], [Hardt and Ullman FOCS 2014], [Steinke and Ullman COLT 2015], [Bassily et al. STOC 2016] demonstrates how differential privacy [Dwork et al. TCC 2006] can be used as a mathematical tool for guaranteeing generalization in adaptive data analysis. Specifically, if a differentially private analysis is applied on a sample S of i.i.d. examples to select a low-sensitivity function f, then w.h.p. f(S) is close to its expectation, although f is being chosen based on the data. Very recently, Steinke and Ullman observed that these generalization guarantees can be used for proving concentration bounds in the non-adaptive setting, where the low-sensitivity function is fixed beforehand. In particular, they obtain alternative proofs for classical concentration bounds for low-sensitivity functions, such as the Chernoff bound and McDiarmid's Inequality. In this work, we set out to examine the situation for functions with high-sensitivity, for which differential privacy does not imply generalization guarantees under adaptive analysis. We show that differential privacy can be used to prove concentration bounds for such functions in the non-adaptive setting. version:1
arxiv-1703-01968 | Max-value Entropy Search for Efficient Bayesian Optimization | http://arxiv.org/abs/1703.01968 | id:1703.01968 author:Zi Wang, Stefanie Jegelka category:stat.ML  published:2017-03-06 summary:Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the $\arg\max$ of the unknown function. Yet, both are plagued by expensive computation, e.g., for estimating entropy. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum value. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing entropy, and hence more efficient. We show relations of MES to other BO methods, and establish a regret bound. Empirical evaluations on a variety of tasks demonstrate the good performance of MES. version:1
arxiv-1703-01962 | Probabilistic Reduced-Order Modeling for Stochastic Partial Differential Equations | http://arxiv.org/abs/1703.01962 | id:1703.01962 author:Constantin Grigo, Phaedon-Stelios Koutsourelakis category:stat.ML  published:2017-03-06 summary:We discuss a Bayesian formulation to coarse-graining (CG) of PDEs where the coefficients (e.g. material parameters) exhibit random, fine scale variability. The direct solution to such problems requires grids that are small enough to resolve this fine scale variability which unavoidably requires the repeated solution of very large systems of algebraic equations. We establish a physically inspired, data-driven coarse-grained model which learns a low- dimensional set of microstructural features that are predictive of the fine-grained model (FG) response. Once learned, those features provide a sharp distribution over the coarse scale effec- tive coefficients of the PDE that are most suitable for prediction of the fine scale model output. This ultimately allows to replace the computationally expensive FG by a generative proba- bilistic model based on evaluating the much cheaper CG several times. Sparsity enforcing pri- ors further increase predictive efficiency and reveal microstructural features that are important in predicting the FG response. Moreover, the model yields probabilistic rather than single-point predictions, which enables the quantification of the unavoidable epistemic uncertainty that is present due to the information loss that occurs during the coarse-graining process. version:1
arxiv-1703-01961 | Multiplicative Normalizing Flows for Variational Bayesian Neural Networks | http://arxiv.org/abs/1703.01961 | id:1703.01961 author:Christos Louizos, Max Welling category:stat.ML cs.LG  published:2017-03-06 summary:We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty. version:1
arxiv-1703-01958 | Network Inference via the Time-Varying Graphical Lasso | http://arxiv.org/abs/1703.01958 | id:1703.01958 author:David Hallac, Youngsuk Park, Stephen Boyd, Jure Leskovec category:cs.LG cs.SI math.OC  published:2017-03-06 summary:Many important problems can be modeled as a system of interconnected entities, where each entity is recording time-dependent observations or measurements. In order to spot trends, detect anomalies, and interpret the temporal dynamics of such data, it is essential to understand the relationships between the different entities and how these relationships evolve over time. In this paper, we introduce the time-varying graphical lasso (TVGL), a method of inferring time-varying networks from raw time series data. We cast the problem in terms of estimating a sparse time-varying inverse covariance matrix, which reveals a dynamic network of interdependencies between the entities. Since dynamic network inference is a computationally expensive task, we derive a scalable message-passing algorithm based on the Alternating Direction Method of Multipliers (ADMM) to solve this problem in an efficient way. We also discuss several extensions, including a streaming algorithm to update the model and incorporate new observations in real time. Finally, we evaluate our TVGL algorithm on both real and synthetic datasets, obtaining interpretable results and outperforming state-of-the-art baselines in terms of both accuracy and scalability. version:1
arxiv-1703-02611 | Cellulyzer - Automated analysis and interactive visualization/simulation of select cellular processes | http://arxiv.org/abs/1703.02611 | id:1703.02611 author:Aliakbar Jafarpour, Holger Lorenz category:physics.bio-ph cs.CV physics.data-an q-bio.SC  published:2017-03-06 summary:Here we report on a set of programs developed at the ZMBH Bio-Imaging Facility for tracking real-life images of cellular processes. These programs perform 1) automated tracking; 2) quantitative and comparative track analyses of different images in different groups; 3) different interactive visualization schemes; and 4) interactive realistic simulation of different cellular processes for validation and optimal problem-specific adjustment of image acquisition parameters (tradeoff between speed, resolution, and quality with feedback from the very final results). The collection of programs is primarily developed for the common bio-image analysis software ImageJ (as a single Java Plugin). Some programs are also available in other languages (C++ and Javascript) and may be run simply with a web-browser; even on a low-end Tablet or Smartphone. The programs are available at https://github.com/nurlicht/CellulyzerDemo version:1
arxiv-1702-07360 | Neural Decision Trees | http://arxiv.org/abs/1702.07360 | id:1702.07360 author:Randall Balestriero category:stat.ML cs.LG  published:2017-02-23 summary:In this paper we propose a synergistic melting of neural networks and decision trees (DT) we call neural decision trees (NDT). NDT is an architecture a la decision tree where each splitting node is an independent multilayer perceptron allowing oblique decision functions or arbritrary nonlinear decision function if more than one layer is used. This way, each MLP can be seen as a node of the tree. We then show that with the weight sharing asumption among those units, we end up with a Hashing Neural Network (HNN) which is a multilayer perceptron with sigmoid activation function for the last layer as opposed to the standard softmax. The output units then jointly represent the probability to be in a particular region. The proposed framework allows for global optimization as opposed to greedy in DT and differentiability w.r.t. all parameters and the input, allowing easy integration in any learnable pipeline, for example after CNNs for computer vision tasks. We also demonstrate the modeling power of HNN allowing to learn union of disjoint regions for final clustering or classification making it more general and powerful than standard softmax MLP requiring linear separability thus reducing the need on the inner layer to perform complex data transformations. We finally show experiments for supervised, semi-suppervised and unsupervised tasks and compare results with standard DTs and MLPs. version:2
arxiv-1703-01925 | Grammar Variational Autoencoder | http://arxiv.org/abs/1703.01925 | id:1703.01925 author:Matt J. Kusner, Brooks Paige, José Miguel Hernández-Lobato category:stat.ML  published:2017-03-06 summary:Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis. version:1
arxiv-1703-01923 | A time series distance measure for efficient clustering of input output signals by their underlying dynamics | http://arxiv.org/abs/1703.01923 | id:1703.01923 author:Oliver Lauwers, Bart De Moor category:cs.SY math.DS stat.ML  published:2017-03-06 summary:Starting from a dataset with input/output time series generated by multiple deterministic linear dynamical systems, this paper tackles the problem of automatically clustering these time series. We propose an extension to the so-called Martin cepstral distance, that allows to efficiently cluster these time series, and apply it to simulated electrical circuits data. Traditionally, two ways of handling the problem are used. The first class of methods employs a distance measure on time series (e.g. Euclidean, Dynamic Time Warping) and a clustering technique (e.g. k-means, k-medoids, hierarchical clustering) to find natural groups in the dataset. It is, however, often not clear whether these distance measures effectively take into account the specific temporal correlations in these time series. The second class of methods uses the input/output data to identify a dynamic system using an identification scheme, and then applies a model norm-based distance (e.g. H2, H-infinity) to find out which systems are similar. This, however, can be very time consuming for large amounts of long time series data. We show that the new distance measure presented in this paper performs as good as when every input/output pair is modelled explicitly, but remains computationally much less complex. The complexity of calculating this distance between two time series of length N is O(N logN). version:1
arxiv-1703-01918 | High-Resolution Multispectral Dataset for Semantic Segmentation | http://arxiv.org/abs/1703.01918 | id:1703.01918 author:Ronald Kemker, Carl Salvaggio, Christopher Kanan category:cs.CV cs.AI  published:2017-03-06 summary:Unmanned aircraft have decreased the cost required to collect remote sensing imagery, which has enabled researchers to collect high-spatial resolution data from multiple sensor modalities more frequently and easily. The increase in data will push the need for semantic segmentation frameworks that are able to classify non-RGB imagery, but this type of algorithmic development requires an increase in publicly available benchmark datasets with class labels. In this paper, we introduce a high-resolution multispectral dataset with image labels. This new benchmark dataset has been pre-split into training/testing folds in order to standardize evaluation and continue to push state-of-the-art classification frameworks for non-RGB imagery. version:1
arxiv-1703-01913 | Near-Optimal Closeness Testing of Discrete Histogram Distributions | http://arxiv.org/abs/1703.01913 | id:1703.01913 author:Ilias Diakonikolas, Daniel M. Kane, Vladimir Nikishkin category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH  published:2017-03-06 summary:We investigate the problem of testing the equivalence between two discrete histograms. A {\em $k$-histogram} over $[n]$ is a probability distribution that is piecewise constant over some set of $k$ intervals over $[n]$. Histograms have been extensively studied in computer science and statistics. Given a set of samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to distinguish (with high probability) between the cases that $p = q$ and $\ p-q\ _1 \geq \epsilon$. The main contribution of this paper is a new algorithm for this testing problem and a nearly matching information-theoretic lower bound. Specifically, the sample complexity of our algorithm matches our lower bound up to a logarithmic factor, improving on previous work by polynomial factors in the relevant parameters. Our algorithmic approach applies in a more general setting and yields improved sample upper bounds for testing closeness of other structured distributions as well. version:1
arxiv-1703-01909 | Neuromorphic Hardware In The Loop: Training a Deep Spiking Network on the BrainScaleS Wafer-Scale System | http://arxiv.org/abs/1703.01909 | id:1703.01909 author:Sebastian Schmitt, Johann Klaehn, Guillaume Bellec, Andreas Gruebl, Maurice Guettler, Andreas Hartel, Stephan Hartmann, Dan Husmann, Kai Husmann, Vitali Karasenko, Mitja Kleider, Christoph Koke, Christian Mauch, Eric Mueller, Paul Mueller, Johannes Partzsch, Mihai A. Petrovici, Stefan Schiefer, Stefan Scholze, Bernhard Vogginger, Robert Legenstein, Wolfgang Maass, Christian Mayr, Johannes Schemmel, Karlheinz Meier category:cs.NE  published:2017-03-06 summary:Emulating spiking neural networks on analog neuromorphic hardware offers several advantages over simulating them on conventional computers, particularly in terms of speed and energy consumption. However, this usually comes at the cost of reduced control over the dynamics of the emulated networks. In this paper, we demonstrate how iterative training of a hardware-emulated network can compensate for anomalies induced by the analog substrate. We first convert a deep neural network trained in software to a spiking network on the BrainScaleS wafer-scale neuromorphic system, thereby enabling an acceleration factor of 10 000 compared to the biological time domain. This mapping is followed by the in-the-loop training, where in each training step, the network activity is first recorded in hardware and then used to compute the parameter updates in software via backpropagation. An essential finding is that the parameter updates do not have to be precise, but only need to approximately follow the correct gradient, which simplifies the computation of updates. Using this approach, after only several tens of iterations, the spiking network shows an accuracy close to the ideal software-emulated prototype. The presented techniques show that deep spiking networks emulated on analog neuromorphic devices can attain good computational performance despite the inherent variations of the analog substrate. version:1
arxiv-1703-01898 | Generative and Discriminative Text Classification with Recurrent Neural Networks | http://arxiv.org/abs/1703.01898 | id:1703.01898 author:Dani Yogatama, Chris Dyer, Wang Ling, Phil Blunsom category:stat.ML cs.CL cs.LG  published:2017-03-06 summary:We empirically characterize the performance of discriminative and generative LSTM models for text classification. We find that although RNN-based generative models are more powerful than their bag-of-words ancestors (e.g., they account for conditional dependencies across words in a document), they have higher asymptotic error rates than discriminatively trained RNN models. However we also find that generative models approach their asymptotic error rate more rapidly than their discriminative counterparts---the same pattern that Ng & Jordan (2001) proved holds for linear classification models that make more naive conditional independence assumptions. Building on this finding, we hypothesize that RNN-based generative classification models will be more robust to shifts in the data distribution. This hypothesis is confirmed in a series of experiments in zero-shot and continual learning settings that show that generative models substantially outperform discriminative models. version:1
arxiv-1703-02036 | Direct White Matter Bundle Segmentation using Stacked U-Nets | http://arxiv.org/abs/1703.02036 | id:1703.02036 author:Jakob Wasserthal, Peter F. Neher, Fabian Isensee, Klaus H. Maier-Hein category:cs.CV q-bio.NC q-bio.QM  published:2017-03-06 summary:The state-of-the-art method for automatically segmenting white matter bundles in diffusion-weighted MRI is tractography in conjunction with streamline cluster selection. This process involves long chains of processing steps which are not only computationally expensive but also complex to setup and tedious with respect to quality control. Direct bundle segmentation methods treat the task as a traditional image segmentation problem. While they so far did not deliver competitive results, they can potentially mitigate many of the mentioned issues. We present a novel supervised approach for direct tract segmentation that shows major performance gains. It builds upon a stacked U-Net architecture which is trained on manual bundle segmentations from Human Connectome Project subjects. We evaluate our approach \textit{in vivo} as well as \textit{in silico} using the ISMRM 2015 Tractography Challenge phantom dataset. We achieve human segmentation performance and a major performance gain over previous pipelines. We show how the learned spatial priors efficiently guide the segmentation even at lower image qualities with little quality loss. version:1
arxiv-1703-01883 | Deep Head Pose Estimation from Depth Data for In-car Automotive Applications | http://arxiv.org/abs/1703.01883 | id:1703.01883 author:Marco Venturelli, Guido Borghi, Roberto Vezzani, Rita Cucchiara category:cs.CV  published:2017-03-06 summary:Recently, deep learning approaches have achieved promising results in various fields of computer vision. In this paper, we tackle the problem of head pose estimation through a Convolutional Neural Network (CNN). Differently from other proposals in the literature, the described system is able to work directly and based only on raw depth data. Moreover, the head pose estimation is solved as a regression problem and does not rely on visual facial features like facial landmarks. We tested our system on a well known public dataset, Biwi Kinect Head Pose, showing that our approach achieves state-of-art results and is able to meet real time performance requirements. version:1
arxiv-1703-01872 | Computational Eco-Systems for Handwritten Digits Recognition | http://arxiv.org/abs/1703.01872 | id:1703.01872 author:Antonio Loquercio, Francesca Della Torre, Massimo Buscema category:stat.ML  published:2017-03-06 summary:On tasks such as the recognition of handwritten digits, traditional methods from machine learning and computer vision have always failed to beat human performance. Inspired by the importance of diversity in biological system, we built an heterogeneous system that could achieve this goal. Our architecture could be summarized in two basic steps. First, we generate a diverse set of classification hypothesis using both Convolutional Neural Networks, currently the state-of-the-art technique for this task, among with other traditional and innovative machine learning techniques. Then, we optimally combine them through Meta-Nets, a family of recently developed and performing ensemble methods. In the resulting parliament of classifiers all hypothesis, despite of their accuracy or methodology, are considered. On the very competitive MNIST handwriting benchmark, our method is the first to beat human performance with 0.17% error rate, surprisingly showing diversity to be the key for success in decision making. version:1
arxiv-1702-08782 | ShaResNet: reducing residual network parameter number by sharing weights | http://arxiv.org/abs/1702.08782 | id:1702.08782 author:Alexandre Boulch category:cs.CV cs.LG 68T05  68T45 C.1.3; I.2.10  published:2017-02-28 summary:Deep Residual Networks have reached the state of the art in many image processing tasks such image classification. However, the cost for a gain in accuracy in terms of depth and memory is prohibitive as it requires a higher number of residual blocks, up to double the initial value. To tackle this problem, we propose in this paper a way to reduce the redundant information of the networks. We share the weights of convolutional layers between residual blocks operating at the same spatial scale. The signal flows multiple times in the same convolutional layer. The resulting architecture, called ShaResNet, contains block specific layers and shared layers. These ShaResNet are trained exactly in the same fashion as the commonly used residual networks. We show, on the one hand, that they are almost as efficient as their sequential counterparts while involving less parameters, and on the other hand that they are more efficient than a residual network with the same number of parameters. For example, a 152-layer-deep residual network can be reduced to 106 convolutional layers, i.e. a parameter gain of 39\%, while loosing less than 0.2\% accuracy on ImageNet. version:2
arxiv-1703-01842 | Evaluating Graph Signal Processing for Neuroimaging Through Classification and Dimensionality Reduction | http://arxiv.org/abs/1703.01842 | id:1703.01842 author:Mathilde Ménoret, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon category:cs.CV q-bio.NC stat.ML  published:2017-03-06 summary:Graph Signal Processing (GSP) is a promising method to analyze high-dimensional neuroimaging datasets while taking into account both the spatial and functional dependencies between brain signals. In the present work, we apply GSP with dimensionality reduction techniques to decode brain activity from real and simulated fMRI datasets. We introduce seven graphs obtained from a) geometric structure and/or b) functional connectivity between brain areas at rest and compare them when performing dimension reduction for classification. We show that mixed graphs using both a) and b) offer the best performance. We also show that graph sampling methods works better than classical dimension reduction methods including Principal Component Analysis (PCA) and Independent Component Analysis (ICA). version:1
arxiv-1703-01830 | Decomposable Submodular Function Minimization: Discrete and Continuous | http://arxiv.org/abs/1703.01830 | id:1703.01830 author:Alina Ene, Huy L. Nguyen, László A. Végh category:cs.LG cs.DS  published:2017-03-06 summary:This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms. version:1
arxiv-1703-01827 | All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation | http://arxiv.org/abs/1703.01827 | id:1703.01827 author:Di Xie, Jiang Xiong, Shiliang Pu category:cs.CV cs.LG cs.NE  published:2017-03-06 summary:Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BNReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve 4% improvement for a 44-layer plain network and almost 50% improvement for a 110-layer plain network on the CIFAR-10 dataset. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts. Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset. version:1
arxiv-1703-01804 | Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use | http://arxiv.org/abs/1703.01804 | id:1703.01804 author:Vatsal Sharan, Gregory Valiant category:cs.LG stat.ML  published:2017-03-06 summary:The popular Alternating Least Squares (ALS) algorithm for tensor decomposition is extremely efficient, but often converges to poor local optima, particularly when the weights of the factors are non-uniform. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS (with both random initialization and SVD-based initialization) for a variety of tasks on synthetic data - including tensor factorization on exact, noisy and over-complete tensors, as well as tensor completion - and for computing word embeddings from a third-order word tri-occurrence tensor. version:1
arxiv-1703-01793 | Multi-Level and Multi-Scale Feature Aggregation Using Pre-trained Convolutional Neural Networks for Music Auto-tagging | http://arxiv.org/abs/1703.01793 | id:1703.01793 author:Jongpil Lee, Juhan Nam category:cs.NE cs.LG cs.MM cs.SD  published:2017-03-06 summary:Music auto-tagging is often handled in a similar manner to image classification by regarding the 2D audio spectrogram as image data. However, music auto-tagging is distinguished from image classification in that the tags are highly diverse and have different levels of abstractions. Considering this issue, we propose a convolutional neural networks (CNN)-based architecture that embraces multi-level and multi-scaled features. The architecture is trained in three steps. First, we conduct supervised feature learning to capture local audio features using a set of CNNs with different input sizes. Second, we extract audio features from each layer of the pre-trained convolutional networks separately and aggregate them altogether given a long audio clip. Finally, we put them into fully-connected networks and make final predictions of the tags. Our experiments show that using the combination of multi-level and multi-scale features is highly effective in music auto-tagging and the proposed method outperforms previous state-of-the-arts on the Magnatagatune dataset and the million song dataset. We further show that the proposed architecture is useful in transfer learning. version:1
arxiv-1703-01790 | All the people around me: face discovery in egocentric photo-streams | http://arxiv.org/abs/1703.01790 | id:1703.01790 author:Maedeh Aghaei, Mariella Dimiccoli, Petia Radeva category:cs.CV  published:2017-03-06 summary:Given an unconstrained stream of images captured by a wearable photo-camera (2fpm), we propose an unsupervised bottom-up approach for automatic clustering appearing faces into the individual identities present in these data. The problem is challenging since images are acquired under real world conditions; hence the visible appearance of the people in the images undergoes intensive variations. Our proposed pipeline consists of first arranging the photo-stream into events, later, localizing the appearance of multiple people in them, and finally, grouping various appearances of the same person across different events. Experimental results performed on a dataset acquired by wearing a photo-camera during one month, demonstrate the effectiveness of the proposed approach for the considered purpose. version:1
arxiv-1703-01789 | Sample-level Deep Convolutional Neural Networks for Music Auto-tagging Using Raw Waveforms | http://arxiv.org/abs/1703.01789 | id:1703.01789 author:Jongpil Lee, Jiyoung Park, Keunhyoung Luke Kim, Juhan Nam category:cs.SD cs.LG cs.MM cs.NE  published:2017-03-06 summary:Recently, the end-to-end approach that learns hierarchical representations from raw data using deep convolutional neural networks has been successfully explored in the image, text and speech domains. This approach was applied to musical signals as well but has been not fully explored yet. To this end, we propose sample-level deep convolutional neural networks which learn representations from very small grains of waveforms (e.g. 2 or 3 samples) beyond typical frame-level input representations. This allows the networks to hierarchically learn filters that are sensitive to log-scaled frequency, such as mel-frequency spectrogram that is widely used in music classification systems. It also helps learning high-level abstraction of music by increasing the depth of layers. We show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results that are com- parable to previous state-of-the-art performances for the Magnatagatune dataset and Million song dataset. In addition, we visualize filters learned in a sample-level DCNN in each layer to identify hierarchically learned features. version:1
arxiv-1703-01785 | Forward and Reverse Gradient-Based Hyperparameter Optimization | http://arxiv.org/abs/1703.01785 | id:1703.01785 author:Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil category:stat.ML  published:2017-03-06 summary:We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters of any iterative learning algorithm such as stochastic gradient descent. These procedures mirror two methods of computing gradients for recurrent neural networks and have different trade-offs in terms of running time and space requirements. Our formulation of the reverse-mode procedure is linked to previous work by Maclaurin et al. [2015] but does not require reversible dynamics. The forward-mode procedure is suitable for real-time hyperparameter updates, which may significantly speed up hyperparameter optimization on large datasets. We present experiments on data cleaning and on learning task interactions. We also present one large-scale experiment where the use of previous gradient-based methods would be prohibitive. version:1
arxiv-1703-01780 | Weight-averaged consistency targets improve semi-supervised deep learning results | http://arxiv.org/abs/1703.01780 | id:1703.01780 author:Antti Tarvainen, Harri Valpola category:cs.NE cs.LG stat.ML  published:2017-03-06 summary:The recently proposed temporal ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, temporal ensembling becomes unwieldy when using large datasets. To overcome this problem, we propose a method that averages model weights instead of label predictions. As an additional benefit, the method improves test accuracy and enables training with fewer labels than earlier methods. We report state-of-the-art results on semi-supervised SVHN, reducing the error rate from 5.12% to 4.41% with 500 labels, and achieving 5.39% error rate with 250 labels. By using extra unlabeled data, we reduce the error rate to 2.76% on 500-label SVHN. version:1
arxiv-1703-01775 | Building a Regular Decision Boundary with Deep Networks | http://arxiv.org/abs/1703.01775 | id:1703.01775 author:Edouard Oyallon category:cs.CV cs.LG  published:2017-03-06 summary:In this work, we build a generic architecture of Convolutional Neural Networks to discover empirical properties of neural networks. Our first contribution is to introduce a state-of-the-art framework that depends upon few hyper parameters and to study the network when we vary them. It has no max pooling, no biases, only 13 layers, is purely convolutional and yields up to 95.4% and 79.6% accuracy respectively on CIFAR10 and CIFAR100. We show that the nonlinearity of a deep network does not need to be continuous, non expansive or point-wise, to achieve good performance. We show that increasing the width of our network permits being competitive with very deep networks. Our second contribution is an analysis of the contraction and separation properties of this network. Indeed, a 1-nearest neighbor classifier applied on deep features progressively improves with depth, which indicates that the representation is progressively more regular. Besides, we defined and analyzed local support vectors that separate classes locally. All our experiments are reproducible and code is available online, based on TensorFlow. version:1
arxiv-1703-01760 | Trust-aware Top-N Recommender Systems with Correlative Denoising Autoencoder | http://arxiv.org/abs/1703.01760 | id:1703.01760 author:Yiteng Pan, Fazhi He, Haiping Yu category:cs.IR cs.LG cs.SI  published:2017-03-06 summary:Both feedback of ratings and trust relationships can be used to reveal user preference to improve recommendation performance, especially for cold users. However, the high-order correlations between tow kind of data are always ignored by existing works. Towards this problem, we propose a Correlative Denoising Autoencoder (CoDAE) model to learn correlations from both rating and trust data for Top-N recommendation. First, a novel deep learning model CoDAE, in which two mid-layers from separate stack denoising autoencoders are fused into one shared layer. Advancing previous works which utilize these data in shallow level, this model can effectively extract high-order correlations from low-level representations of these data for recommendation. Second, to further learn implicit corrections between these two autoencoders, we develop a novel correlative regulation to build the relations between other hidden layers of the two separate autoencoders. In this way, this model can learn correlations more effectively and thus improve the recommendation quality. Comprehensive experiments on two public datasets demonstrate that the CoDAE significantly outperforms other state-of-the-art approaches in top-N recommendation task. version:1
arxiv-1703-01732 | Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning | http://arxiv.org/abs/1703.01732 | id:1703.01732 author:Joshua Achiam, Shankar Sastry category:cs.LG  published:2017-03-06 summary:Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques. version:1
arxiv-1703-01726 | A Novel Comprehensive Approach for Estimating Concept Semantic Similarity in WordNet | http://arxiv.org/abs/1703.01726 | id:1703.01726 author:Xiao-gang Zhang, Shou-qian Sun, Ke-jun Zhang category:cs.CL  published:2017-03-06 summary:Computation of semantic similarity between concepts is an important foundation for many research works. This paper focuses on IC computing methods and IC measures, which estimate the semantic similarities between concepts by exploiting the topological parameters of the taxonomy. Based on analyzing representative IC computing methods and typical semantic similarity measures, we propose a new hybrid IC computing method. Through adopting the parameter dhyp and lch, we utilize the new IC computing method and propose a novel comprehensive measure of semantic similarity between concepts. An experiment based on WordNet "is a" taxonomy has been designed to test representative measures and our measure on benchmark dataset R&G, and the results show that our measure can obviously improve the similarity accuracy. We evaluate the proposed approach by comparing the correlation coefficients between five measures and the artificial data. The results show that our proposal outperforms the previous measures. version:1
arxiv-1703-01725 | Cats and Captions vs. Creators and the Clock: Comparing Multimodal Content to Context in Predicting Relative Popularity | http://arxiv.org/abs/1703.01725 | id:1703.01725 author:Jack Hessel, Lillian Lee, David Mimno category:cs.SI cs.CL cs.CV physics.soc-ph  published:2017-03-06 summary:The content of today's social media is becoming more and more rich, increasingly mixing text, images, videos, and audio. It is an intriguing research question to model the interplay between these different modes in attracting user attention and engagement. But in order to pursue this study of multimodal content, we must also account for context: timing effects, community preferences, and social factors (e.g., which authors are already popular) also affect the amount of feedback and reaction that social-media posts receive. In this work, we separate out the influence of these non-content factors in several ways. First, we focus on ranking pairs of submissions posted to the same community in quick succession, e.g., within 30 seconds, this framing encourages models to focus on time-agnostic and community-specific content features. Within that setting, we determine the relative performance of author vs. content features. We find that victory usually belongs to "cats and captions," as visual and textual features together tend to outperform identity-based features. Moreover, our experiments show that when considered in isolation, simple unigram text features and deep neural network visual features yield the highest accuracy individually, and that the combination of the two modalities generally leads to the best accuracies overall. version:1
arxiv-1703-01720 | Sound-Word2Vec: Learning Word Representations Grounded in Sounds | http://arxiv.org/abs/1703.01720 | id:1703.01720 author:Ashwin K Vijayakumar, Ramakrishna Vedantam, Devi Parikh category:cs.CL cs.AI cs.SD  published:2017-03-06 summary:Sound and vision are the primary modalities that influence how we perceive the world around us. Thus, it is crucial to incorporate information from these modalities into language to help machines interact better with humans. While existing works have explored incorporating visual cues into language embeddings, the task of learning word representations that respect auditory grounding remains under-explored. In this work, we propose a new embedding scheme, sound-word2vec that learns language embeddings by grounding them in sound -- for example, two seemingly unrelated concepts, leaves and paper are closer in our embedding space as they produce similar rustling sounds. We demonstrate that the proposed embeddings perform better than language-only word representations, on two purely textual tasks that require reasoning about aural cues -- sound retrieval and foley-sound discovery. Finally, we analyze nearest neighbors to highlight the unique dependencies captured by sound-w2v as compared to language-only embeddings. version:1
arxiv-1703-01717 | Measuring Sample Quality with Kernels | http://arxiv.org/abs/1703.01717 | id:1703.01717 author:Jackson Gorham, Lester Mackey category:stat.ML cs.LG  published:2017-03-06 summary:Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein's method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement. version:1
arxiv-1702-06378 | Multitask Learning with CTC and Segmental CRF for Speech Recognition | http://arxiv.org/abs/1702.06378 | id:1702.06378 author:Liang Lu, Lingpeng Kong, Chris Dyer, Noah A. Smith category:cs.CL  published:2017-02-21 summary:Segmental conditional random fields (SCRFs) and connectionist temporal classification (CTC) are two sequence labeling methods used for end-to-end training of speech recognition models. Both models define a transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a "continuation" of the previous label. In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network (RNN) encoder is used for feature extraction for both outputs. We find that this multitask objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model. version:2
arxiv-1703-01703 | Third-Person Imitation Learning | http://arxiv.org/abs/1703.01703 | id:1703.01703 author:Bradly C. Stadie, Pieter Abbeel, Ilya Sutskever category:cs.LG  published:2017-03-06 summary:Reinforcement learning (RL) makes it possible to train agents capable of achiev- ing sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstra- tions are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collect- ing first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learn- ing. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum. version:1
arxiv-1703-01702 | Viewpoint Selection for Photographing Architectures | http://arxiv.org/abs/1703.01702 | id:1703.01702 author:Jingwu He, Linbo Wang, Wenzhe Zhou, Hongjie Zhang, Xiufen Cui, Yanwen Guo category:cs.CV  published:2017-03-06 summary:This paper studies the problem of how to choose good viewpoints for taking photographs of architectures. We achieve this by learning from professional photographs of world famous landmarks that are available on the Internet. Unlike previous efforts devoted to photo quality assessment which mainly rely on 2D image features, we show in this paper combining 2D image features extracted from images with 3D geometric features computed on the 3D models can result in more reliable evaluation of viewpoint quality. Specifically, we collect a set of photographs for each of 15 world famous architectures as well as their 3D models from the Internet. Viewpoint recovery for images is carried out through an image-model registration process, after which a newly proposed viewpoint clustering strategy is exploited to validate users' viewpoint preferences when photographing landmarks. Finally, we extract a number of 2D and 3D features for each image based on multiple visual and geometric cues and perform viewpoint recommendation by learning from both 2D and 3D features using a specifically designed SVM-2K multi-view learner, achieving superior performance over using solely 2D or 3D features. We show the effectiveness of the proposed approach through extensive experiments. The experiments also demonstrate that our system can be used to recommend viewpoints for rendering textured 3D models of buildings for the use of architectural design, in addition to viewpoint evaluation of photographs and recommendation of viewpoints for photographing architectures in practice. version:1
arxiv-1703-01698 | 4-DoF Tracking for Robot Fine Manipulation Tasks | http://arxiv.org/abs/1703.01698 | id:1703.01698 author:Mennatullah Siam, Abhineet Singh, Camilo Perez, Martin Jagersand category:cs.CV  published:2017-03-06 summary:This paper presents two visual trackers from the different paradigms of learning and registration based tracking and evaluates their application in image based visual servoing. They can track object motion with four degrees of freedom (DOF) which, as we will show here, is sufficient for many fine manipulation tasks. One of these is a newly developed learning based tracker that relies on learning discriminative correlation filters while the other is a refinement of a recent 8 DoF RANSAC based tracker adapted with a new appearance model for 4 DoF motion. Both trackers are shown to have superior performance to other state of the art trackers on an existing dataset for manipulation tasks. Further, a new dataset with challenging sequences for fine manipulation tasks captured from the robot mounted eye-in-hand (EIH) cameras is also presented. These sequences have a variety of challenges encountered during real tasks including jittery camera movement, motion blur, drastic scale changes and partial occlusions. Quantitative and qualitative results of both trackers in comparison to eight recent state of the art trackers are shown on these sequences. It proves that these two trackers are robust to failures while maintaining high precision that makes them suitable for such fine manipulation tasks. version:1
arxiv-1703-01694 | Word forms - not just their lengths- are optimized for efficient communication | http://arxiv.org/abs/1703.01694 | id:1703.01694 author:Stephan C. Meylan, Thomas L. Griffiths category:cs.CL  published:2017-03-06 summary:The inverse relationship between the length of a word and the frequency of its use, first identified by G.K. Zipf in 1935, is a classic empirical law that holds across a wide range of human languages. We demonstrate that length is one aspect of a much more general property of words: how distinctive they are with respect to other words in a language. Distinctiveness plays a critical role in recognizing words in fluent speech, in that it reflects the strength of potential competitors when selecting the best candidate for an ambiguous signal. Phonological information content, a measure of a word's string probability under a statistical model of a language's sound or character sequences, concisely captures distinctiveness. Examining large-scale corpora from 13 languages, we find that distinctiveness significantly outperforms word length as a predictor of frequency. This finding provides evidence that listeners' processing constraints shape fine-grained aspects of word forms across languages. version:1
arxiv-1703-01678 | Data-Dependent Stability of Stochastic Gradient Descent | http://arxiv.org/abs/1703.01678 | id:1703.01678 author:Ilja Kuzborskij, Christoph Lampert category:cs.LG  published:2017-03-05 summary:We establish a data-dependent notion of algorithmic stability for Stochastic Gradient Descent (SGD) and employ it to develop novel generalization bounds. This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants. By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on convex and non-convex problems. In the convex case, we show that the bound on the generalization error is multiplicative in the risk at the initialization point. In the non-convex case, we prove that the expected curvature of the objective function around the initialization point has crucial influence on the generalization error. In both cases, our results suggest a simple data-driven strategy to stabilize SGD by pre-screening its initialization. version:1
arxiv-1703-01671 | Controlling for Unobserved Confounds in Classification Using Correlational Constraints | http://arxiv.org/abs/1703.01671 | id:1703.01671 author:Virgile Landeiro, Aron Culotta category:cs.AI cs.CL  published:2017-03-05 summary:As statistical classifiers become integrated into real-world applications, it is important to consider not only their accuracy but also their robustness to changes in the data distribution. In this paper, we consider the case where there is an unobserved confounding variable $z$ that influences both the features $\mathbf{x}$ and the class variable $y$. When the influence of $z$ changes from training to testing data, we find that the classifier accuracy can degrade rapidly. In our approach, we assume that we can predict the value of $z$ at training time with some error. The prediction for $z$ is then fed to Pearl's back-door adjustment to build our model. Because of the attenuation bias caused by measurement error in $z$, standard approaches to controlling for $z$ are ineffective. In response, we propose a method to properly control for the influence of $z$ by first estimating its relationship with the class variable $y$, then updating predictions for $z$ to match that estimated relationship. By adjusting the influence of $z$, we show that we can build a model that exceeds competing baselines on accuracy as well as on robustness over a range of confounding relationships. version:1
arxiv-1703-01664 | Diversified Texture Synthesis with Feed-forward Networks | http://arxiv.org/abs/1703.01664 | id:1703.01664 author:Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang category:cs.CV  published:2017-03-05 summary:Recent progresses on deep discriminative and generative modeling have shown promising results on texture synthesis. However, existing feed-forward based methods trade off generality for efficiency, which suffer from many issues, such as shortage of generality (i.e., build one network per texture), lack of diversity (i.e., always produce visually identical output) and suboptimality (i.e., generate less satisfying visual effects). In this work, we focus on solving these issues for improved texture synthesis. We propose a deep generative feed-forward network which enables efficient synthesis of multiple textures within one single network and meaningful interpolation between them. Meanwhile, a suite of important techniques are introduced to achieve better convergence and diversity. With extensive experiments, we demonstrate the effectiveness of the proposed model and techniques for synthesizing a large number of textures and show its applications with the stylization. version:1
arxiv-1703-01661 | SegICP: Integrated Deep Semantic Segmentation and Pose Estimation | http://arxiv.org/abs/1703.01661 | id:1703.01661 author:Jay M. Wong, Vincent Kee, Tiffany Le, Syler Wagner, Gian-Luca Mariottini, Abraham Schneider, Lei Hamilton, Rahul Chipalkatty, Mitchell Hebert, David M. S. Johnson, Jimmy Wu, Bolei Zhou, Antonio Torralba category:cs.RO cs.CV  published:2017-03-05 summary:Recent robotic manipulation competitions have highlighted that sophisticated robots still struggle to achieve fast and reliable perception of task-relevant objects in complex, realistic scenarios. To improve these systems' perceptive speed and robustness, we present SegICP, a novel integrated solution to object recognition and pose estimation. SegICP couples convolutional neural networks and multi-hypothesis point cloud registration to achieve both robust pixel-wise semantic segmentation as well as accurate and real-time 6-DOF pose estimation for relevant objects, even in the presence of occlusions and sensor noise. Our architecture achieves 1cm position error and <5^\circ$ angle error in real time without an initial seed. We evaluate and benchmark SegICP against an annotated dataset generated by motion capture. version:1
arxiv-1703-01656 | Reasoning About Liquids via Closed-Loop Simulation | http://arxiv.org/abs/1703.01656 | id:1703.01656 author:Connor Schenck, Dieter Fox category:cs.RO cs.CV  published:2017-03-05 summary:Simulators are a powerful tool for reasoning about a robot's interactions with its environment. However, when simulations diverge from reality, that reasoning becomes less useful. In this paper, we evaluate closing the loop between simulation and reality by using observations of real liquids to correct errors when tracking the liquid's state in a simulator. Our results show that closed-loop simulation is an effective way to prevent large divergence between the simulated and real liquid states, and additionally that this method can enable reasoning about liquids that would otherwise be infeasible. version:1
arxiv-1703-01619 | Neural Machine Translation and Sequence-to-sequence Models: A Tutorial | http://arxiv.org/abs/1703.01619 | id:1703.01619 author:Graham Neubig category:cs.CL cs.LG stat.ML  published:2017-03-05 summary:This tutorial introduces a new and powerful set of techniques variously called "neural machine translation" or "neural sequence-to-sequence models". These techniques have been used in a number of tasks regarding the handling of human language, and can be a powerful tool in the toolbox of anyone who wants to model sequential data of some sort. The tutorial assumes that the reader knows the basics of math and programming, but does not assume any particular experience with neural networks or natural language processing. It attempts to explain the intuition behind the various methods covered, then delves into them with enough mathematical detail to understand them concretely, and culiminates with a suggestion for an implementation exercise, where readers can test that they understood the content in practice. version:1
arxiv-1703-01610 | Tighter Regret Bounds for Influence Maximization and Other Combinatorial Semi-Bandits with Probabilistically Triggered Arms | http://arxiv.org/abs/1703.01610 | id:1703.01610 author:Qinshi Wang, Wei Chen category:cs.LG stat.ML  published:2017-03-05 summary:We study combinatorial multi-armed bandit with probabilistically triggered arms (CMAB-T) and semi-bandit feedback. We resolve a serious issue in the prior CMAB-T studies where the regret bounds contain a possibly exponentially large factor of $1/p^*$, where $p^*$ is the minimum positive probability that an arm is triggered by any action. We address this issue by introducing triggering probability moderated (TPM) bounded smoothness conditions into the general CMAB-T framework, and show that many applications such as influence maximization bandit and combinatorial cascading bandit satisfy such TPM conditions. As a result, we completely remove the factor of $1/p^*$ from the regret bounds, achieving significantly better regret bounds for influence maximization and cascading bandits than before. Finally, we provide lower bound results showing that the factor $1/p^*$ is unavoidable for general CMAB-T problems, suggesting that TPM conditions are crucial in removing this factor. version:1
arxiv-1703-01606 | A Theory of Output-Side Unsupervised Domain Adaptation | http://arxiv.org/abs/1703.01606 | id:1703.01606 author:Tomer Galanti, Lior Wolf category:cs.LG stat.ML  published:2017-03-05 summary:When learning a mapping from an input space to an output space, the assumption that the sample distribution of the training data is the same as that of the test data is often violated. Unsupervised domain shift methods adapt the learned function in order to correct for this shift. Previous work has focused on utilizing unlabeled samples from the target distribution. We consider the complementary problem in which the unlabeled samples are given post mapping, i.e., we are given the outputs of the mapping of unknown samples from the shifted domain. Two other variants are also studied: the two sided version, in which unlabeled samples are give from both the input and the output spaces, and the Domain Transfer problem, which was recently formalized. In all cases, we derive generalization bounds that employ discrepancy terms. version:1
arxiv-1703-02031 | Random vector generation of a semantic space | http://arxiv.org/abs/1703.02031 | id:1703.02031 author:Jean-François Delpech, Sabine Ploux category:cs.CL  published:2017-03-05 summary:We show how random vectors and random projection can be implemented in the usual vector space model to construct a Euclidean semantic space from a French synonym dictionary. We evaluate theoretically the resulting noise and show the experimental distribution of the similarities of terms in a neighborhood according to the choice of parameters. We also show that the Schmidt orthogonalization process is applicable and can be used to separate homonyms with distinct semantic meanings. Neighboring terms are easily arranged into semantically significant clusters which are well suited to the generation of realistic lists of synonyms and to such applications as word selection for automatic text generation. This process, applicable to any language, can easily be extended to collocations, is extremely fast and can be updated in real time, whenever new synonyms are proposed. version:1
arxiv-1703-01605 | L2GSCI: Local to Global Seam Cutting and Integrating for Accurate Face Contour Extraction | http://arxiv.org/abs/1703.01605 | id:1703.01605 author:Yongwei Nie, Xu Cao, Chengjiang Long, Ping Li, Guiqing Li category:cs.CV  published:2017-03-05 summary:Current face alignment algorithms can robustly find a set of landmarks along face contour. However, the landmarks are sparse and lack curve details, especially in chin and cheek areas where a lot of concave-convex bending information exists. In this paper, we propose a local to global seam cutting and integrating algorithm (L2GSCI) to extract continuous and accurate face contour. Our method works in three steps with the help of a rough initial curve. First, we sample small and overlapped squares along the initial curve. Second, the seam cutting part of L2GSCI extracts a local seam in each square region. Finally, the seam integrating part of L2GSCI connects all the redundant seams together to form a continuous and complete face curve. Overall, the proposed method is much more straightforward than existing face alignment algorithms, but can achieve pixel-level continuous face curves rather than discrete and sparse landmarks. Moreover, experiments on two face benchmark datasets (i.e., LFPW and HELEN) show that our method can precisely reveal concave-convex bending details of face contours, which has significantly improved the performance when compared with the state-ofthe- art face alignment approaches. version:1
arxiv-1703-01597 | Face Alignment with Cascaded Semi-Parametric Deep Greedy Neural Forests | http://arxiv.org/abs/1703.01597 | id:1703.01597 author:Arnaud Dapogny, Kévin Bailly, Séverine Dubuisson category:cs.CV  published:2017-03-05 summary:Face alignment is an active topic in computer vision, consisting in aligning a shape model on the face. To this end, most modern approaches refine the shape in a cascaded manner, starting from an initial guess. Those shape updates can either be applied in the feature point space (\textit{i.e.} explicit updates) or in a low-dimensional, parametric space. In this paper, we propose a semi-parametric cascade that first aligns a parametric shape, then captures more fine-grained deformations of an explicit shape. For the purpose of learning shape updates at each cascade stage, we introduce a deep greedy neural forest (GNF) model, which is an improved version of deep neural forest (NF). GNF appears as an ideal regressor for face alignment, as it combines differentiability, high expressivity and fast evaluation runtime. The proposed framework is very fast and achieves high accuracies on multiple challenging benchmarks, including small, medium and large pose experiments. version:1
arxiv-1703-01594 | Graph sampling with determinantal processes | http://arxiv.org/abs/1703.01594 | id:1703.01594 author:Nicolas Tremblay, Pierre-Olivier Amblard, Simon Barthelmé category:cs.LG cs.SI stat.ML  published:2017-03-05 summary:We present a new random sampling strategy for k-bandlimited signals defined on graphs, based on determinantal point processes (DPP). For small graphs, ie, in cases where the spectrum of the graph is accessible, we exhibit a DPP sampling scheme that enables perfect recovery of bandlimited signals. For large graphs, ie, in cases where the graph's spectrum is not accessible, we investigate, both theoretically and empirically, a sub-optimal but much faster DPP based on loop-erased random walks on the graph. Preliminary experiments show promising results especially in cases where the number of measurements should stay as small as possible and for graphs that have a strong community structure. Our sampling scheme is efficient and can be applied to graphs with up to $10^6$ nodes. version:1
arxiv-1702-03522 | Spectral Clustering via Graph Filtering: Consistency on the High-Dimensional Stochastic Block Model | http://arxiv.org/abs/1702.03522 | id:1702.03522 author:Muni Sreenivas Pydi, Ambedkar Dukkipati category:stat.ML  published:2017-02-12 summary:Spectral clustering is amongst the most popular methods for community detection in graphs. A key step in spectral clustering algorithms is the eigen-decomposition of the $n{\times}n$ graph Laplacian matrix to extract its $k$ leading eigenvectors, where $k$ is the desired number of clusters among $n$ objects. This is prohibitively complex to implement for very large datasets. However, it has recently been shown that it is possible to bypass the eigen-decomposition by computing an approximate spectral embedding through graph filtering of random signals. In this paper, we prove that spectral clustering performed via graph filtering can still recover the planted clusters consistently, under mild conditions. We analyse the effects of sparsity, dimensionality and filter approximation error on the consistency of the algorithm. version:2
arxiv-1703-00767 | Attentive Recurrent Comparators | http://arxiv.org/abs/1703.00767 | id:1703.00767 author:Pranav Shyam, Shubham Gupta, Ambedkar Dukkipati category:cs.CV cs.LG  published:2017-03-02 summary:Rapid and continual learning models require that the representation space they use be dynamic and constantly changing as the model encounters new evidence. While recently there have been many end-to-end, meta-learning based approaches, they have significant drawbacks. We present a novel model that crudely approximates having a dynamic representation space at inference time. The entire representation space is defined relative to the test example and the entire context of this relative representation space is considered before the model makes a prediction. We extensively test all aspects of our model across various real world datasets. In the challenging task of one-shot learning on the Omniglot dataset, our model achieves the first super-human performance for a neural method with an error rate of 1.5\%. version:2
arxiv-1703-01564 | Perceiving and Reasoning About Liquids Using Fully Convolutional Networks | http://arxiv.org/abs/1703.01564 | id:1703.01564 author:Conor Schenck, Dieter Fox category:cs.RO cs.CV  published:2017-03-05 summary:Liquids are an important part of many common manipulation tasks in human environments. If we wish to have robots that can accomplish these types of tasks, they must be able to interact with liquids in an intelligent manner. In this paper, we investigate ways for robots to perceive and reason about liquids. That is, the robots ask the questions What in my sensory data stream is liquid? and How can I use that to infer all the potential places liquid might be? We collected two datasets to evaluate these questions, one using a realistic liquid simulator and another on our robot. We used fully convolutional neural networks to learn to detect and track liquids across pouring sequences. Our results show that our networks are able to perceive and reason about liquids, and that integrating temporal information is important to performing these tasks well. version:1
arxiv-1702-07717 | When confidence and competence collide: Effects on online decision-making discussions | http://arxiv.org/abs/1702.07717 | id:1702.07717 author:Liye Fu, Lillian Lee, Cristian Danescu-Niculescu-Mizil category:cs.CL cs.CY cs.HC cs.SI physics.soc-ph  published:2017-02-24 summary:Group discussions are a way for individuals to exchange ideas and arguments in order to reach better decisions than they could on their own. One of the premises of productive discussions is that better solutions will prevail, and that the idea selection process is mediated by the (relative) competence of the individuals involved. However, since people may not know their actual competence on a new task, their behavior is influenced by their self-estimated competence --- that is, their confidence --- which can be misaligned with their actual competence. Our goal in this work is to understand the effects of confidence-competence misalignment on the dynamics and outcomes of discussions. To this end, we design a large-scale natural setting, in the form of an online team-based geography game, that allows us to disentangle confidence from competence and thus separate their effects. We find that in task-oriented discussions, the more-confident individuals have a larger impact on the group's decisions even when these individuals are at the same level of competence as their teammates. Furthermore, this unjustified role of confidence in the decision-making process often leads teams to under-perform. We explore this phenomenon by investigating the effects of confidence on conversational dynamics. version:2
arxiv-1703-01560 | LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation | http://arxiv.org/abs/1703.01560 | id:1703.01560 author:Jianwei Yang, Anitha Kannan, Dhruv Batra, Devi Parikh category:cs.CV cs.LG  published:2017-03-05 summary:We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN. version:1
arxiv-1703-01557 | Using Graphs of Classifiers to Impose Declarative Constraints on Semi-supervised Learning | http://arxiv.org/abs/1703.01557 | id:1703.01557 author:Lidong Bing, William W. Cohen, Bhuwan Dhingra category:cs.LG cs.CL stat.ML  published:2017-03-05 summary:We propose a general approach to modeling semi-supervised learning (SSL) algorithms. Specifically, we present a declarative language for modeling both traditional supervised classification tasks and many SSL heuristics, including both well-known heuristics such as co-training and novel domain-specific heuristics. In addition to representing individual SSL heuristics, we show that multiple heuristics can be automatically combined using Bayesian optimization methods. We experiment with two classes of tasks, link-based text classification and relation extraction. We show modest improvements on well-studied link-based classification benchmarks, and state-of-the-art results on relation-extraction tasks for two realistic domains. version:1
arxiv-1703-01550 | Deep-Learning for Classification of Colorectal Polyps on Whole-Slide Images | http://arxiv.org/abs/1703.01550 | id:1703.01550 author:Bruno Korbar, Andrea M. Olofson, Allen P. Miraflor, Katherine M. Nicka, Matthew A. Suriawinata, Lorenzo Torresani, Arief A. Suriawinata, Saeed Hassanpour category:cs.CV  published:2017-03-05 summary:Histopathological characterization of colorectal polyps is an important principle for determining the risk of colorectal cancer and future rates of surveillance for patients. This characterization is time-intensive, requires years of specialized training, and suffers from significant inter-observer and intra-observer variability. In this work, we built an automatic image-understanding method that can accurately classify different types of colorectal polyps in whole-slide histology images to help pathologists with histopathological characterization and diagnosis of colorectal polyps. The proposed image-understanding method is based on deep-learning techniques, which rely on numerous levels of abstraction for data representation and have shown state-of-the-art results for various image analysis tasks. Our image-understanding method covers all five polyp types (hyperplastic polyp, sessile serrated polyp, traditional serrated adenoma, tubular adenoma, and tubulovillous/villous adenoma) that are included in the US multi-society task force guidelines for colorectal cancer risk assessment and surveillance, and encompasses the most common occurrences of colorectal polyps. Our evaluation on 239 independent test samples shows our proposed method can identify the types of colorectal polyps in whole-slide images with a high efficacy (accuracy: 93.0%, precision: 89.7%, recall: 88.3%, F1 score: 88.8%). The presented method in this paper can reduce the cognitive burden on pathologists and improve their accuracy and efficiency in histopathological characterization of colorectal polyps, and in subsequent risk assessment and follow-up recommendations. version:1
arxiv-1703-01541 | Soft-DTW: a Differentiable Loss Function for Time-Series | http://arxiv.org/abs/1703.01541 | id:1703.01541 author:Marco Cuturi, Mathieu Blondel category:stat.ML  published:2017-03-05 summary:We propose in this paper a differentiable learning loss between time series. Our proposal builds upon the celebrated Dynamic Time Warping (DTW) discrepancy. Unlike the Euclidean distance, DTW is able to compare asynchronous time series of varying size and is robust to elastic transformations in time. To be robust to such invariances, DTW computes a minimal cost alignment between time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a differentiable loss function, and that both its value and its gradient can be computed with quadratic time/space complexity (DTW has quadratic time and linear space complexity). We show that our regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense. version:1
arxiv-1703-01536 | A Statistical Machine Learning Approach to Yield Curve Forecasting | http://arxiv.org/abs/1703.01536 | id:1703.01536 author:Rajiv Sambasivan, Sourish Das category:stat.ML  published:2017-03-04 summary:Yield curve forecasting is an important problem in finance. In this work we explore the use of Gaussian Processes in conjunction with a dynamic modeling strategy, much like the Kalman Filter, to model the yield curve. Gaussian Processes have been successfully applied to model functional data in a variety of applications. A Gaussian Process is used to model the yield curve. The hyper-parameters of the Gaussian Process model are updated as the algorithm receives yield curve data. Yield curve data is typically available as a time series with a frequency of one day. We compare existing methods to forecast the yield curve with the proposed method. The results of this study showed that while a competing method (a multivariate time series method) performed well in forecasting the yields at the short term structure region of the yield curve, Gaussian Processes perform well in the medium and long term structure regions of the yield curve. Accuracy in the long term structure region of the yield curve has important practical implications. The Gaussian Process framework yields uncertainty and probability estimates directly in contrast to other competing methods. Analysts are frequently interested in this information. In this study the proposed method has been applied to yield curve forecasting, however it can be applied to model high frequency time series data or data streams in other domains. version:1
arxiv-1703-00548 | Evolving Deep Neural Networks | http://arxiv.org/abs/1703.00548 | id:1703.00548 author:Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, Babak Hodjat category:cs.NE cs.AI  published:2017-03-01 summary:The success of deep learning depends on finding an architecture to fit the task. As deep learning has scaled up to more challenging tasks, the architectures have become difficult to design by hand. This paper proposes an automated method, CoDeepNEAT, for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future. version:2
arxiv-1703-01526 | High Accuracy Classification of Parkinson's Disease through Shape Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging | http://arxiv.org/abs/1703.01526 | id:1703.01526 author:R. Prashanth, Sumantra Dutta Roy, Pravat K. Mandal, Shantanu Ghosh category:stat.AP cs.CV physics.data-an stat.CO stat.ML  published:2017-03-04 summary:Early and accurate identification of parkinsonian syndromes (PS) involving presynaptic degeneration from non-degenerative variants such as Scans Without Evidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for effective patient management as the course, therapy and prognosis differ substantially between the two groups. In this study, we use Single Photon Emission Computed Tomography (SPECT) images from healthy normal, early PD and SWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative (PPMI) database, and process them to compute shape- and surface fitting-based features for the three groups. We use these features to develop and compare various classification models that can discriminate between scans showing dopaminergic deficit, as in PD, from scans without the deficit, as in healthy normal or SWEDD. Along with it, we also compare these features with Striatal Binding Ratio (SBR)-based features, which are well-established and clinically used, by computing a feature importance score using Random forests technique. We observe that the Support Vector Machine (SVM) classifier gave the best performance with an accuracy of 97.29%. These features also showed higher importance than the SBR-based features. We infer from the study that shape analysis and surface fitting are useful and promising methods for extracting discriminatory features that can be used to develop diagnostic models that might have the potential to help clinicians in the diagnostic process. version:1
arxiv-1703-01515 | CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos | http://arxiv.org/abs/1703.01515 | id:1703.01515 author:Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, Shih-Fu Chang category:cs.CV  published:2017-03-04 summary:Temporal action localization is an important yet challenging problem. Given a long, untrimmed video consisting of multiple action instances and complex background contents, we need not only to recognize their action categories, but also to localize the start time and end time of each instance. Many state-of-the-art systems use segment-level classifiers to select and rank proposal segments of pre-determined boundaries. However, a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. To this end, we design a novel Convolutional-De-Convolutional (CDC) network that places CDC filters on top of 3D ConvNets, which have been shown to be effective for abstracting action semantics but reduce the temporal length of the input data. The proposed CDC filter performs the required temporal upsampling and spatial downsampling operations simultaneously to predict actions at the frame-level granularity. It is unique in jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network in an end-to-end manner efficiently. Our model not only achieves superior performance in detecting actions in every frame, but also significantly boosts the precision of localizing temporal boundaries. Finally, the CDC network demonstrates a very high efficiency with the ability to process 500 frames per second on a single GPU server. We will update the camera-ready version and publish the source codes online soon. version:1
arxiv-1703-01513 | Genetic CNN | http://arxiv.org/abs/1703.01513 | id:1703.01513 author:Lingxi Xie, Alan Yuille category:cs.CV  published:2017-03-04 summary:The deep Convolutional Neural Network (CNN) is the state-of-the-art solution for large-scale visual recognition. Following basic principles such as increasing the depth and constructing highway connections, researchers have manually designed a lot of fixed network structures and verified their effectiveness. In this paper, we discuss the possibility of learning deep network structures automatically. Note that the number of possible network structures increases exponentially with the number of layers in the network, which inspires us to adopt the genetic algorithm to efficiently traverse this large search space. We first propose an encoding method to represent each network structure in a fixed-length binary string, and initialize the genetic algorithm by generating a set of randomized individuals. In each generation, we define standard genetic operations, e.g., selection, mutation and crossover, to eliminate weak individuals and then generate more competitive ones. The competitiveness of each individual is defined as its recognition accuracy, which is obtained via training the network from scratch and evaluating it on a validation set. We run the genetic process on two small datasets, i.e., MNIST and CIFAR10, demonstrating its ability to evolve and find high-quality structures which are little studied before. These structures are also transferrable to the large-scale ILSVRC2012 dataset. version:1
arxiv-1703-01507 | Machine Learning Friendly Set Version of Johnson-Lindenstrauss Lemma | http://arxiv.org/abs/1703.01507 | id:1703.01507 author:Mieczysław A. Kłopotek category:cs.DS cs.LG  published:2017-03-04 summary:In this paper we make a novel use of the Johnson-Lindenstrauss Lemma. The Lemma has an existential form saying that there exists a JL transformation $f$ of the data points into lower dimensional space such that all of them fall into predefined error range $\delta$. We formulate in this paper a theorem stating that we can choose the target dimensionality in a random projection type JL linear transformation in such a way that with probability $1-\epsilon$ all of them fall into predefined error range $\delta$ for any user-predefined failure probability $\epsilon$. This result is important for applications such a data clustering where we want to have a priori dimensionality reducing transformation instead of trying out a (large) number of them, as with traditional Johnson-Lindenstrauss Lemma. version:1
arxiv-1703-01506 | Accelerating Permutation Testing in Voxel-wise Analysis through Subspace Tracking: A new plugin for SnPM | http://arxiv.org/abs/1703.01506 | id:1703.01506 author:Felipe Gutierrez-Barragan, Vamsi K. Ithapu, Chris Hinrichs, Camille Maumet, Sterling C. Johnson, Thomas E. Nichols, Vikas Singh, the ADNI category:stat.AP cs.CV stat.ML  published:2017-03-04 summary:Permutation testing is a non-parametric method for obtaining the max null distribution used to compute corrected $p$-values to provide strong control of false positives. In neuroimaging, however, the computational burden of running such algorithm can be significant. We find that by viewing the permutation testing procedure as the construction of a very large permutation testing matrix $T$, one can exploit structural properties derived from the data and the test statistics to reduce the runtime under certain conditions. In particular, we see that $T$ has a low-rank plus a low-variance residual. This makes $T$ a good candidate for low-rank matrix completion methods, where only a very small number of entries of $T$ ($~0.35\%$ of all entries in our experiments) have to be computed to obtain good estimate of it. Based on this observation, we developed an algorithm, RapidPT, that is able to efficiently recover the max null distribution commonly obtained through regular permutation testing in neuroimage analysis. We present an extensive experimental validation on four varying sized datasets against two baselines: Statistical NonParametric Mapping (SnPM13) and a standard permutation testing implementation (referred to as NaivePT). We find that RapidPT achieves its best runtime performance on medium sized datasets ($50 \leq n \leq 200$), with speedup gains of 1.5x - 38x (vs. SnPM13) and 20x-1000x (vs. NaivePT). For larger datasets ($n \geq 200$) RapidPT outperforms NaivePT (6x - 200x), and provides substantial speedups over SnPM13 when performing more than 10000 permutations (2x - 15x). The Matlab implementation is available as a standalone toolbox called RapidPT. Our code is also integrated within SnPM13, and is able to leverage multi-core architectures when available. version:1
arxiv-1703-01488 | Autoencoding Variational Inference For Topic Models | http://arxiv.org/abs/1703.01488 | id:1703.01488 author:Akash Srivastava, Charles Sutton category:stat.ML  published:2017-03-04 summary:Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling. version:1
arxiv-1703-01485 | Lexical Resources for Hindi Marathi MT | http://arxiv.org/abs/1703.01485 | id:1703.01485 author:Sreelekha S, Pushpak Bhattacharyya category:cs.CL  published:2017-03-04 summary:In this paper we describe some ways to utilize various lexical resources to improve the quality of statistical machine translation system. We have augmented the training corpus with various lexical resources such as IndoWordnet semantic relation set, function words, kridanta pairs and verb phrases etc. Our research on the usage of lexical resources mainly focused on two ways such as augmenting parallel corpus with more vocabulary and augmenting with various word forms. We have described case studies, evaluations and detailed error analysis for both Marathi to Hindi and Hindi to Marathi machine translation systems. From the evaluations we observed that, there is an incremental growth in the quality of machine translation as the usage of various lexical resources increases. Moreover usage of various lexical resources helps to improve the coverage and quality of machine translation where limited parallel corpus is available. version:1
arxiv-1703-01474 | Sharp bounds for population recovery | http://arxiv.org/abs/1703.01474 | id:1703.01474 author:Anindya De, Ryan O'Donnell, Rocco Servedio category:cs.DS cs.LG math.ST stat.TH  published:2017-03-04 summary:The population recovery problem is a basic problem in noisy unsupervised learning that has attracted significant research attention in recent years [WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this problem have been studied, often under assumptions on the unknown distribution (such as that it has restricted support size). In this work we study the sample complexity and algorithmic complexity of the most general version of the problem, under both bit-flip noise and erasure noise model. We give essentially matching upper and lower sample complexity bounds for both noise models, and efficient algorithms matching these sample complexity bounds up to polynomial factors. version:1
arxiv-1703-00035 | Context-Sensitive Super-Resolution for Fast Fetal Magnetic Resonance Imaging | http://arxiv.org/abs/1703.00035 | id:1703.00035 author:Steven McDonagh, Benjamin Hou, Konstantinos Kamnitsas, Ozan Oktay, Amir Alansary, Mary Rutherford, Jo V. Hajnal, Bernhard Kainz category:cs.CV  published:2017-02-28 summary:3D Magnetic Resonance Imaging (MRI) is often a trade-off between fast but low-resolution image acquisition and highly detailed but slow image acquisition. Fast imaging is required for targets that move to avoid motion artefacts. This is in particular difficult for fetal MRI. Spatially independent upsampling techniques, which are the state-of-the-art to address this problem, are error prone and disregard contextual information. In this paper we propose a context-sensitive upsampling method based on a residual convolutional neural network model that learns organ specific appearance and adopts semantically to input data allowing for the generation of high resolution images with sharp edges and fine scale detail. By making contextual decisions about appearance and shape, present in different parts of an image, we gain a maximum of structural detail at a similar contrast as provided by high-resolution data. We experiment on $145$ fetal scans and show that our approach yields an increased PSNR of $1.25$ $dB$ when applied to under-sampled fetal data cf. baseline upsampling. Furthermore, our method yields an increased PSNR of $1.73$ $dB$ when utilizing under-sampled fetal data to perform brain volume reconstruction on motion corrupted captured data. version:2
arxiv-1703-01467 | Generative Compression | http://arxiv.org/abs/1703.01467 | id:1703.01467 author:Shibani Santurkar, David Budden, Nir Shavit category:cs.CV  published:2017-03-04 summary:Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and show its potential to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length entropy coding schemes. version:1
arxiv-1703-01461 | Addressing Appearance Change in Outdoor Robotics with Adversarial Domain Adaptation | http://arxiv.org/abs/1703.01461 | id:1703.01461 author:Markus Wulfmeier, Alex Bewley, Ingmar Posner category:cs.RO cs.LG  published:2017-03-04 summary:Appearance changes due to weather and seasonal conditions represent a strong impediment to the robust implementation of machine learning systems in outdoor robotics. While the model is optimised for the training domain it will deliver degraded performance in application domains that underlie distributional shifts caused by these changes. Traditionally, this problem has been addressed via the collection of labelled data in multiple domains or by imposing priors on the type of shift between both domains. We frame the problem in the context of unsupervised domain adaptation and apply an adversarial framework to train a deep neural network with the additional objective to align features across domains. This approach benefits from adding unlabelled data and is generally applicable to many state-of-the-art architectures. Moreover, as adversarial training is notoriously hard to stabilise, we first perform an extensive ablation study on a surrogate classification task underlying the same appearance change and then apply the distilled insights to the problem of free-space segmentation for motion planning. version:1
arxiv-1703-01460 | Extracting Real-time Feedback with Neural Networks for Simulation-based Learning | http://arxiv.org/abs/1703.01460 | id:1703.01460 author:Xingjun Ma, James Bailey, Sudanthi Wijewickrema, Shuo Zhou, Zakaria Mhammedi, Yun Zhou, Stephen O'Leary category:cs.LG cs.HC stat.ML  published:2017-03-04 summary:Simulation-based learning (SBL) is gaining popularity as a low-cost and convenient training technique in a vast range of applications. However, for a SBL platform to be fully utilized as an effective training tool, it is essential that feedback on performance is provided automatically in real-time during training. It is the aim of this paper to develop an efficient and effective feedback extraction method for the provision of real-time feedback in SBL. Existing methods either have low effectiveness in improving novice skills or suffer from low efficiency, resulting in their inability to be used in real-time. In this paper, we propose a neural network based method (NNFB) to extract feedback based on the adversarial technique. NNFB utilizes a bounded update to minimize a L1 regularized loss via back-propagation. We empirically show that NNFB can be used to extract simple, yet effective feedback. Also, NNFB was observed to have high effectiveness and efficiency when compared to existing methods, thus making it a promising option for real-time feedback extraction in SBL. version:1
arxiv-1703-01457 | Chain-NN: An Energy-Efficient 1D Chain Architecture for Accelerating Deep Convolutional Neural Networks | http://arxiv.org/abs/1703.01457 | id:1703.01457 author:Shihao Wang, Dajiang Zhou, Xushen Han, Takeshi Yoshimura category:cs.AR cs.LG  published:2017-03-04 summary:Deep convolutional neural networks (CNN) have shown their good performances in many computer vision tasks. However, the high computational complexity of CNN involves a huge amount of data movements between the computational processor core and memory hierarchy which occupies the major of the power consumption. This paper presents Chain-NN, a novel energy-efficient 1D chain architecture for accelerating deep CNNs. Chain-NN consists of the dedicated dual-channel process engines (PE). In Chain-NN, convolutions are done by the 1D systolic primitives composed of a group of adjacent PEs. These systolic primitives, together with the proposed column-wise scan input pattern, can fully reuse input operand to reduce the memory bandwidth requirement for energy saving. Moreover, the 1D chain architecture allows the systolic primitives to be easily reconfigured according to specific CNN parameters with fewer design complexity. The synthesis and layout of Chain-NN is under TSMC 28nm process. It costs 3751k logic gates and 352KB on-chip memory. The results show a 576-PE Chain-NN can be scaled up to 700MHz. This achieves a peak throughput of 806.4GOPS with 567.5mW and is able to accelerate the five convolutional layers in AlexNet at a frame rate of 326.2fps. 1421.0GOPS/W power efficiency is at least 2.5 to 4.1x times better than the state-of-the-art works. version:1
arxiv-1703-01454 | Matrix-centric Neural Networks | http://arxiv.org/abs/1703.01454 | id:1703.01454 author:Kien Do, Truyen Tran, Svetha Venkatesh category:cs.LG  published:2017-03-04 summary:We present a new distributed representation in deep neural nets wherein the information is represented in native form as a matrix. This differs from current neural architectures that rely on vector representations. We consider matrices as central to the architecture and they compose the input, hidden and output layers. The model representation is more compact and elegant - the number of parameters grows only with the largest dimension of the incoming layer rather than the number of hidden units. We derive feed-forward nets that map an input matrix into an output matrix, and recurrent nets which map a sequence of input matrices into a sequence of output matrices. Experiments on handwritten digits recognition, face reconstruction, sequence to sequence learning and EEG classification demonstrate the efficacy and compactness of the matrix-centric architectures. version:1
arxiv-1702-08891 | Predicting Slice-to-Volume Transformation in Presence of Arbitrary Subject Motion | http://arxiv.org/abs/1702.08891 | id:1702.08891 author:Benjamin Hou, Amir Alansary, Steven McDonagh, Alice Davidson, Mary Rutherford, Jo V. Hajnal, Daniel Rueckert, Ben Glocker, Bernhard Kainz category:cs.CV  published:2017-02-28 summary:This paper aims to solve a fundamental problem in intensity-based 2D/3D registration, which concerns the limited capture range and need for very good initialization of state-of-the-art image registration methods. We propose a regression approach that learns to predict rotation and translations of arbitrary 2D image slices from 3D volumes, with respect to a learned canonical atlas co-ordinate system. To this end, we utilize Convolutional Neural Networks (CNNs) to learn the highly complex regression function that maps 2D image slices into their correct position and orientation in 3D space. Our approach is attractive in challenging imaging scenarios, where significant subject motion complicates reconstruction performance of 3D volumes from 2D slice data. We extensively evaluate the effectiveness of our approach quantitatively on simulated MRI brain data with extreme random motion. We further demonstrate qualitative results on fetal MRI where our method is integrated into a full reconstruction and motion compensation pipeline. With our CNN regression approach we obtain an average prediction error of 7mm on simulated data, and convincing reconstruction quality of images of very young fetuses where previous methods fail. We further discuss applications to Computed Tomography and X-ray projections. Our approach is a general solution to the 2D/3D initialization problem. It is computationally efficient, with prediction times per slice of a few milliseconds, making it suitable for real-time scenarios. version:2
arxiv-1703-01444 | An unsupervised bayesian approach for the joint reconstruction and classification of cutaneous reflectance confocal microscopy images | http://arxiv.org/abs/1703.01444 | id:1703.01444 author:Abdelghafour Halimi, Hadj Batatia, Jimmy Le Digabel, Gwendal Josse, Jean-Yves Tourneret category:stat.ML  published:2017-03-04 summary:This paper studies a new Bayesian algorithm for the joint reconstruction and classification of reflectance confocal microscopy (RCM) images, with application to the identification of human skin lentigo. The proposed Bayesian approach takes advantage of the distribution of the multiplicative speckle noise affecting the true reflectivity of these images and of appropriate priors for the unknown model parameters. A Markov chain Monte Carlo (MCMC) algorithm is proposed to jointly estimate the model parameters and the image of true reflectivity while classifying images according to the distribution of their reflectivity. Precisely, a Metropolis-whitin-Gibbs sampler is investigated to sample the posterior distribution of the Bayesian model associated with RCM images and to build estimators of its parameters, including labels indicating the class of each RCM image. The resulting algorithm is applied to synthetic data and to real images from a clinical study containing healthy and lentigo patients. version:1
arxiv-1703-01442 | Recurrent Poisson Factorization for Temporal Recommendation | http://arxiv.org/abs/1703.01442 | id:1703.01442 author:Seyed Abbas Hosseini, Keivan Alizadeh, Ali Khodadadi, Ali Arabzadeh, Mehrdad Farajtabar, Hongyuan Zha, Hamid R. Rabiee category:cs.SI cs.LG stat.ML  published:2017-03-04 summary:Poisson factorization is a probabilistic model of users and items for recommendation systems, where the so-called implicit consumer data is modeled by a factorized Poisson distribution. There are many variants of Poisson factorization methods who show state-of-the-art performance on real-world recommendation tasks. However, most of them do not explicitly take into account the temporal behavior and the recurrent activities of users which is essential to recommend the right item to the right user at the right time. In this paper, we introduce Recurrent Poisson Factorization (RPF) framework that generalizes the classical PF methods by utilizing a Poisson process for modeling the implicit feedback. RPF treats time as a natural constituent of the model and brings to the table a rich family of time-sensitive factorization models. To elaborate, we instantiate several variants of RPF who are capable of handling dynamic user preferences and item specification (DRPF), modeling the social-aspect of product adoption (SRPF), and capturing the consumption heterogeneity among users and items (HRPF). We also develop a variational algorithm for approximate posterior inference that scales up to massive data sets. Furthermore, we demonstrate RPF's superior performance over many state-of-the-art methods on synthetic dataset, and large scale real-world datasets on music streaming logs, and user-item interactions in M-Commerce platforms. version:1
arxiv-1703-01437 | Automated Top View Registration of Broadcast Football Videos | http://arxiv.org/abs/1703.01437 | id:1703.01437 author:Rahul Anand Sharma, Bharath Bhat, Vineet Gandhi, C. V. Jawahar category:cs.CV  published:2017-03-04 summary:In this paper, we propose a novel method to register football broadcast video frames on the static top view model of the playing surface. The proposed method is fully automatic in contrast to the current state of the art which requires manual initialization of point correspondences between the image and the static model. Automatic registration using existing approaches has been difficult due to the lack of sufficient point correspondences. We investigate an alternate approach exploiting the edge information from the line markings on the field. We formulate the registration problem as a nearest neighbour search over a synthetically generated dictionary of edge map and homography pairs. The synthetic dictionary generation allows us to exhaustively cover a wide variety of camera angles and positions and reduce this problem to a minimal per-frame edge map matching procedure. We show that the per-frame results can be improved in videos using an optimization framework for temporal camera stabilization. We demonstrate the efficacy of our approach by presenting extensive results on a dataset collected from matches of football World Cup 2014. version:1
arxiv-1703-01425 | Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection | http://arxiv.org/abs/1703.01425 | id:1703.01425 author:Yuliang Liu, Lianwen Jin category:cs.CV  published:2017-03-04 summary:Detecting incidental scene text is a challenging task because of multi-orientation, perspective distortion, and variation of text size, color and scale. Retrospective research has only focused on using rectangular bounding box or horizontal sliding window to localize text, which may result in redundant background noise, unnecessary overlap or even information loss. To address these issues, we propose a new Convolutional Neural Networks (CNNs) based method, named Deep Matching Prior Network (DMPNet), to detect text with tighter quadrangle. First, we use quadrilateral sliding windows in several specific intermediate convolutional layers to roughly recall the text with higher overlapping area and then a shared Monte-Carlo method is proposed for fast and accurate computing of the polygonal areas. After that, we designed a sequential protocol for relative regression which can exactly predict text with compact quadrangle. Moreover, a auxiliary smooth Ln loss is also proposed for further regressing the position of text, which has better overall performance than L2 loss and smooth L1 loss in terms of robustness and stability. The effectiveness of our approach is evaluated on a public word-level, multi-oriented scene text database, ICDAR 2015 Robust Reading Competition Challenge 4 "Incidental scene text localization". The performance of our method is evaluated by using F-measure and found to be 70.64%, outperforming the existing state-of-the-art method with F-measure 63.76%. version:1
arxiv-1702-08652 | Scene Flow to Action Map: A New Representation for RGB-D based Action Recognition with Convolutional Neural Networks | http://arxiv.org/abs/1702.08652 | id:1702.08652 author:Pichao Wang, Wanqing Li, Zhimin Gao, Yuyao Zhang, Chang Tang, Philip Ogunbona category:cs.CV  published:2017-02-28 summary:Scene flow describes the motion of 3D objects in real world and potentially could be the basis of a good feature for 3D action recognition. However, its use for action recognition, especially in the context of convolutional neural networks (ConvNets), has not been previously studied. In this paper, we propose the extraction and use of scene flow for action recognition from RGB-D data. Previous works have considered the depth and RGB modalities as separate channels and extract features for later fusion. We take a different approach and consider the modalities as one entity, thus allowing feature extraction for action recognition at the beginning. Two key questions about the use of scene flow for action recognition are addressed: how to organize the scene flow vectors and how to represent the long term dynamics of videos based on scene flow. In order to calculate the scene flow correctly on the available datasets, we propose an effective self-calibration method to align the RGB and depth data spatially without knowledge of the camera parameters. Based on the scene flow vectors, we propose a new representation, namely, Scene Flow to Action Map (SFAM), that describes several long term spatio-temporal dynamics for action recognition. We adopt a channel transform kernel to transform the scene flow vectors to an optimal color space analogous to RGB. This transformation takes better advantage of the trained ConvNets models over ImageNet. Experimental results indicate that this new representation can surpass the performance of state-of-the-art methods on two large public datasets. version:2
arxiv-1703-01402 | Skin Lesion Classification Using Deep Multi-scale Convolutional Neural Networks | http://arxiv.org/abs/1703.01402 | id:1703.01402 author:Terrance DeVries, Dhanesh Ramachandram category:cs.CV  published:2017-03-04 summary:We present a deep learning approach to the ISIC 2017 Skin Lesion Classification Challenge using a multi-scale convolutional neural network. Our approach utilizes an Inception-v3 network pre-trained on the ImageNet dataset, which is fine-tuned for skin lesion classification using two different scales of input images. version:1
arxiv-1703-01396 | Stacking-based Deep Neural Network: Deep Analytic Network on Convolutional Spectral Histogram Features | http://arxiv.org/abs/1703.01396 | id:1703.01396 author:Cheng-Yaw Low, Andrew Beng-Jin Teoh category:cs.CV  published:2017-03-04 summary:Stacking-based deep neural network (S-DNN), in general, denotes a deep neural network (DNN) resemblance in terms of its very deep, feedforward network architecture. The typical S-DNN aggregates a variable number of individual learning modules in series to assemble a DNN-alike alternative to the targeted object recognition tasks. This work likewise conceives an S-DNN instantiation, dubbed deep analytic network (DAN), on top of the spectral histogram (SH) features. The DAN learning principle relies on ridge regression, and some key DNN constituents, specifically, rectified linear unit, fine-tuning, and normalization. The DAN aptitude is scrutinized on three repositories of varying domains, including FERET (faces), MNIST (handwritten digits), and CIFAR10 (natural objects). The empirical results unveil that DAN escalates the SH baseline performance over a sufficiently deep layer. version:1
arxiv-1703-01386 | Looking at Outfit to Parse Clothing | http://arxiv.org/abs/1703.01386 | id:1703.01386 author:Pongsate Tangseng, Zhipeng Wu, Kota Yamaguchi category:cs.CV  published:2017-03-04 summary:This paper extends fully-convolutional neural networks (FCN) for the clothing parsing problem. Clothing parsing requires higher-level knowledge on clothing semantics and contextual cues to disambiguate fine-grained categories. We extend FCN architecture with a side-branch network which we refer outfit encoder to predict a consistent set of clothing labels to encourage combinatorial preference, and with conditional random field (CRF) to explicitly consider coherent label assignment to the given image. The empirical results using Fashionista and CFPD datasets show that our model achieves state-of-the-art performance in clothing parsing, without additional supervision during training. We also study the qualitative influence of annotation on the current clothing parsing benchmarks, with our Web-based tool for multi-scale pixel-wise annotation and manual refinement effort to the Fashionista dataset. Finally, we show that the image representation of the outfit encoder is useful for dress-up image retrieval application. version:1
arxiv-1703-01383 | Wavelet Domain Residual Network (WavResNet) for Low-Dose X-ray CT Reconstruction | http://arxiv.org/abs/1703.01383 | id:1703.01383 author:Eunhee Kang, junhong Min, Jong Chul Ye category:cs.CV  published:2017-03-04 summary:Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT are computationally complex because of the repeated use of the forward and backward projection. Inspired by this success of deep learning in computer vision applications, we recently proposed a deep convolutional neural network (CNN) for low-dose X-ray CT and won the second place in 2016 AAPM Low-Dose CT Grand Challenge. However, some of the texture are not fully recovered, which was unfamiliar to some radiologists. To cope with this problem, here we propose a direct residual learning approach on directional wavelet domain to solve this problem and to improve the performance against previous work. In particular, the new network estimates the noise of each input wavelet transform, and then the de-noised wavelet coefficients are obtained by subtracting the noise from the input wavelet transform bands. The experimental results confirm that the proposed network has significantly improved performance, preserving the detail texture of the original images. version:1
arxiv-1703-01382 | Multi-Scale Wavelet Domain Residual Learning for Limited-Angle CT Reconstruction | http://arxiv.org/abs/1703.01382 | id:1703.01382 author:Jawook Gu, Jong Chul Ye category:cs.CV  published:2017-03-04 summary:Limited-angle computed tomography (CT) is often used in clinical applications such as C-arm CT for interventional imaging. However, CT images from limited angles suffers from heavy artifacts due to incomplete projection data. Existing iterative methods require extensive calculations but can not deliver satisfactory results. Based on the observation that the artifacts from limited angles have some directional property and are globally distributed, we propose a novel multi-scale wavelet domain residual learning architecture, which compensates for the artifacts. Experiments have shown that the proposed method effectively eliminates artifacts, thereby preserving edge and global structures of the image. version:1
arxiv-1702-04280 | DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional Neural Network | http://arxiv.org/abs/1702.04280 | id:1702.04280 author:Afshin Dehghan, Enrique G. Ortiz, Guang Shu, Syed Zain Masood category:cs.CV cs.AI  published:2017-02-14 summary:This paper describes the details of Sighthound's fully automated age, gender and emotion recognition system. The backbone of our system consists of several deep convolutional neural networks that are not only computationally inexpensive, but also provide state-of-the-art results on several competitive benchmarks. To power our novel deep networks, we collected large labeled datasets through a semi-supervised pipeline to reduce the annotation effort/time. We tested our system on several public benchmarks and report outstanding results. Our age, gender and emotion recognition models are available to developers through the Sighthound Cloud API at https://www.sighthound.com/products/cloud version:2
arxiv-1703-01365 | Axiomatic Attribution for Deep Networks | http://arxiv.org/abs/1703.01365 | id:1703.01365 author:Mukund Sundararajan, Ankur Taly, Qiqi Yan category:cs.LG  published:2017-03-04 summary:We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a deep network, and to enable users to engage with models better. version:1
arxiv-1703-01363 | Convex Geometry of the Generalized Matrix-Fractional Function | http://arxiv.org/abs/1703.01363 | id:1703.01363 author:James V. Burke, Yuan Gao, Tim Hoheisel category:math.OC cs.LG stat.ML  published:2017-03-04 summary:Generalized matrix-fractional (GMF) functions are a class of matrix support functions introduced by Burke and Hoheisel as a tool for unifying a range of seemingly divergent matrix optimization problems associated with inverse problems, regularization and learning. In this paper we dramatically simplify the support function representation for GMF functions as well as the representation of their subdifferentials. These new representations allow the ready computation of a range of important related geometric objects whose formulations were previously unavailable. version:1
arxiv-1703-00598 | The Second Order Linear Model | http://arxiv.org/abs/1703.00598 | id:1703.00598 author:Ming Lin, Shuang Qiu, Bin Hong, Jieping Ye category:stat.ML  published:2017-03-02 summary:The second order linear model (SLM) extends the linear model to high order functional space. Special cases of the SLM have been widely studied under various restricted assumptions during the past decade. Yet how to efficiently learn the SLM under full generality still remains an open question due to several fundamental limitations of the conventional gradient descent learning framework. In this introductory study, we try to attack this problem from a gradient-free approach which we call the moment-estimation-sequence (MES) method. We show that the conventional gradient descent heuristic is biased by the skewness of the distribution therefore is no longer the best practice of learning the SLM. Based on the MES framework, we design a nonconvex alternating iteration process to train a $d$-dimension rank-$k$ SLM within $O(kd)$ memory and one-pass of the dataset. The proposed method converges globally and linearly, achieves $\epsilon$ recovery error after retrieving $O[k^{2}d\cdot\mathrm{polylog}(kd/\epsilon)]$ samples. Furthermore, our theoretical analysis reveals that not all SLMs can be learned on every sub-gaussian distribution. When the instances are sampled from a so-called $\tau$-MIP distribution, the SLM can be learned by $O(p/\tau^{2})$ samples where $p$ and $\tau$ are positive constants depending on the skewness and kurtosis of the distribution. For non-MIP distribution, an addition diagonal-free oracle is necessary and sufficient to guarantee the learnability of the SLM. Numerical simulations verify the sharpness of our bounds on the sampling complexity and the linear convergence rate of our algorithm. Finally we demonstrate several applications of the SLM on large-scale high dimensional datasets. version:2
arxiv-1703-01347 | Contextual Multi-armed Bandits under Feature Uncertainty | http://arxiv.org/abs/1703.01347 | id:1703.01347 author:Se-Young Yun, Jun Hyun Nam, Sangwoo Mo, Jinwoo Shin category:cs.AI cs.LG stat.ML  published:2017-03-03 summary:We study contextual multi-armed bandit problems under linear realizability on rewards and uncertainty (or noise) on features. For the case of identical noise on features across actions, we propose an algorithm, coined {\em NLinRel}, having $O\left(T^{\frac{7}{8}} \left(\log{(dT)}+K\sqrt{d}\right)\right)$ regret bound for $T$ rounds, $K$ actions, and $d$-dimensional feature vectors. Next, for the case of non-identical noise, we observe that popular linear hypotheses including {\em NLinRel} are impossible to achieve such sub-linear regret. Instead, under assumption of Gaussian feature vectors, we prove that a greedy algorithm has $O\left(T^{\frac23}\sqrt{\log d}\right)$ regret bound with respect to the optimal linear hypothesis. Utilizing our theoretical understanding on the Gaussian case, we also design a practical variant of {\em NLinRel}, coined {\em Universal-NLinRel}, for arbitrary feature distributions. It first runs {\em NLinRel} for finding the `true' coefficient vector using feature uncertainties and then adjust it to minimize its regret using the statistical feature information. We justify the performance of {\em Universal-NLinRel} on both synthetic and real-world datasets. version:1
arxiv-1703-00556 | Conversion Rate Optimization through Evolutionary Computation | http://arxiv.org/abs/1703.00556 | id:1703.00556 author:Risto Miikkulainen, Neil Iscoe, Aaron Shagrin, Ron Cordell, Sam Nazari, Cory Schoolland, Myles Brundage, Jonathan Epstein, Randy Dean, Gurmeet Lamba category:cs.HC cs.AI cs.NE  published:2017-03-01 summary:Conversion optimization means designing a web interface so that as many users as possible take a desired action on it, such as register or purchase. Such design is usually done by hand, testing one change at a time through A/B testing, or a limited number of combinations through multivariate testing, making it possible to evaluate only a small fraction of designs in a vast design space. This paper describes Sentient Ascend, an automatic conversion optimization system that uses evolutionary optimization to create effective web interface designs. Ascend makes it possible to discover and utilize interactions between the design elements that are difficult to identify otherwise. Moreover, evaluation of design candidates is done in parallel online, i.e. with a large number of real users interacting with the system. A case study on a lead generation site learnhotobecome.org shows that significant improvements (i.e. over 43%) are possible beyond human design. Ascend can therefore be seen as an approach to massively multivariate conversion optimization, based on a massively parallel interactive evolution. version:2
arxiv-1703-01340 | Generative Poisoning Attack Method Against Neural Networks | http://arxiv.org/abs/1703.01340 | id:1703.01340 author:Chaofei Yang, Qing Wu, Hai Li, Yiran Chen category:cs.CR cs.LG stat.ML  published:2017-03-03 summary:Poisoning attack is identified as a severe security threat to machine learning algorithms. In many applications, for example, deep neural network (DNN) models collect public data as the inputs to perform re-training, where the input data can be poisoned. Although poisoning attack against support vector machines (SVM) has been extensively studied before, there is still very limited knowledge about how such attack can be implemented on neural networks (NN), especially DNNs. In this work, we first examine the possibility of applying traditional gradient-based method (named as the direct gradient method) to generate poisoned data against NNs by leveraging the gradient of the target model w.r.t. the normal data. We then propose a generative method to accelerate the generation rate of the poisoned data: an auto-encoder (generator) used to generate poisoned data is updated by a reward function of the loss, and the target NN model (discriminator) receives the poisoned data to calculate the loss w.r.t. the normal data. Our experiment results show that the generative method can speed up the poisoned data generation rate by up to 239.38x compared with the direct gradient method, with slightly lower model accuracy degradation. A countermeasure is also designed to detect such poisoning attack methods by checking the loss of the target model. version:1
arxiv-1703-01327 | Multi-step Reinforcement Learning: A Unifying Algorithm | http://arxiv.org/abs/1703.01327 | id:1703.01327 author:Kristopher De Asis, J. Fernando Hernandez-Garcia, G. Zacharias Holland, Richard S. Sutton category:cs.AI cs.LG  published:2017-03-03 summary:Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD($\lambda$) elegantly unifies one-step TD prediction with Monte Carlo methods through the use of eligibility traces and the trace-decay parameter $\lambda$. Currently, there are a multitude of algorithms that can be used to perform TD control, including Sarsa, $Q$-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be extended across multiple time steps to achieve better performance. Each of these algorithms is seemingly distinct, and no one dominates the others for all problems. In this paper, we study a new multi-step action-value algorithm called $Q(\sigma)$ which unifies and generalizes these existing algorithms, while subsuming them as special cases. A new parameter, $\sigma$, is introduced to allow the degree of sampling performed by the algorithm at each step during its backup to be continuously varied, with Sarsa existing at one extreme (full sampling), and Expected Sarsa existing at the other (pure expectation). $Q(\sigma)$ is generally applicable to both on- and off-policy learning, but in this work we focus on experiments in the on-policy case. Our results show that an intermediate value of $\sigma$, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater performance. version:1
arxiv-1702-08272 | A Dataset for Developing and Benchmarking Active Vision | http://arxiv.org/abs/1702.08272 | id:1702.08272 author:Phil Ammirato, Patrick Poirson, Eunbyung Park, Jana Kosecka, Alexander C. Berg category:cs.CV  published:2017-02-27 summary:We present a new public dataset with a focus on simulating robotic vision tasks in everyday indoor environments using real imagery. The dataset includes 20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely captured in 9 unique scenes. We train a fast object category detector for instance detection on our data. Using the dataset we show that, although increasingly accurate and fast, the state of the art for object detection is still severely impacted by object scale, occlusion, and viewing direction all of which matter for robotics applications. We next validate the dataset for simulating active vision, and use the dataset to develop and evaluate a deep-network-based system for next best move prediction for object classification using reinforcement learning. Our dataset is available for download at cs.unc.edu/~ammirato/active_vision_dataset_website/. version:2
arxiv-1703-01290 | Bridging Saliency Detection to Weakly Supervised Object Detection Based on Self-paced Curriculum Learning | http://arxiv.org/abs/1703.01290 | id:1703.01290 author:Dingwen Zhang, Deyu Meng, Long Zhao, Junwei Han category:cs.CV  published:2017-03-03 summary:Weakly-supervised object detection (WOD) is a challenging problems in computer vision. The key problem is to simultaneously infer the exact object locations in the training images and train the object detectors, given only the training images with weak image-level labels. Intuitively, by simulating the selective attention mechanism of human visual system, saliency detection technique can select attractive objects in scenes and thus is a potential way to provide useful priors for WOD. However, the way to adopt saliency detection in WOD is not trivial since the detected saliency region might be possibly highly ambiguous in complex cases. To this end, this paper first comprehensively analyzes the challenges in applying saliency detection to WOD. Then, we make one of the earliest efforts to bridge saliency detection to WOD via the self-paced curriculum learning, which can guide the learning procedure to gradually achieve faithful knowledge of multi-class objects from easy to hard. The experimental results demonstrate that the proposed approach can successfully bridge saliency detection and WOD tasks and achieve the state-of-the-art object detection results under the weak supervision. version:1
arxiv-1703-01289 | Instance Flow Based Online Multiple Object Tracking | http://arxiv.org/abs/1703.01289 | id:1703.01289 author:Sebastian Bullinger, Christoph Bodensteiner, Michael Arens category:cs.CV  published:2017-03-03 summary:We present a method to perform online Multiple Object Tracking (MOT) of known object categories in monocular video data. Current Tracking-by-Detection MOT approaches build on top of 2D bounding box detections. In contrast, we exploit state-of-the-art instance aware semantic segmentation techniques to compute 2D shape representations of target objects in each frame. We predict position and shape of segmented instances in subsequent frames by exploiting optical flow cues. We define an affinity matrix between instances of subsequent frames which reflects locality and visual similarity. The instance association is solved by applying the Hungarian method. We evaluate different configurations of our algorithm using the MOT 2D 2015 train dataset. The evaluation shows that our tracking approach is able to track objects with high relative motions. In addition, we provide results of our approach on the MOT 2D 2015 test set for comparison with previous works. We achieve a MOTA score of 32.1. version:1
arxiv-1703-01260 | EX2: Exploration with Exemplar Models for Deep Reinforcement Learning | http://arxiv.org/abs/1703.01260 | id:1703.01260 author:Justin Fu, John D. Co-Reyes, Sergey Levine category:cs.LG  published:2017-03-03 summary:Efficient exploration in high-dimensional environments remains a key challenge in reinforcement learning (RL). Deep reinforcement learning methods have demonstrated the ability to learn with highly general policy classes for complex tasks with high-dimensional inputs, such as raw images. However, many of the most effective exploration techniques rely on tabular representations, or on the ability to construct a generative model over states and actions. Both are exceptionally difficult when these inputs are complex and high dimensional. On the other hand, it is comparatively easy to build discriminative models on top of complex states such as images using standard deep neural networks. This paper introduces a novel approach, EX2, which approximates state visitation densities by training an ensemble of discriminators, and assigns reward bonuses to rarely visited states. We demonstrate that EX2 achieves comparable performance to the state-of-the-art methods on low-dimensional tasks, and its effectiveness scales into high-dimensional state spaces such as visual domains without hand-designing features or density models. version:1
arxiv-1703-01250 | Virtual vs. Real: Trading Off Simulations and Physical Experiments in Reinforcement Learning with Bayesian Optimization | http://arxiv.org/abs/1703.01250 | id:1703.01250 author:Alonso Marco, Felix Berkenkamp, Philipp Hennig, Angela P. Schoellig, Andreas Krause, Stefan Schaal, Sebastian Trimpe category:cs.RO cs.LG cs.SY  published:2017-03-03 summary:In practice, the parameters of control policies are often tuned manually. This is time-consuming and frustrating. Reinforcement learning is a promising alternative that aims to automate this process, yet often requires too many experiments to be practical. In this paper, we propose a solution to this problem by exploiting prior knowledge from simulations, which are readily available for most robotic platforms. Specifically, we extend Entropy Search, a Bayesian optimization algorithm that maximizes information gain from each experiment, to the case of multiple information sources. The result is a principled way to automatically combine cheap, but inaccurate information from simulations with expensive and accurate physical experiments in a cost-effective manner. We apply the resulting method to a cart-pole system, which confirms that the algorithm can find good control policies with fewer experiments than standard Bayesian optimization on the physical system only. version:1
arxiv-1703-01248 | Incident Light Frequency-based Image Defogging Algorithm | http://arxiv.org/abs/1703.01248 | id:1703.01248 author:Wenbo Zhang, Xiaorong Hou category:cs.CV  published:2017-03-03 summary:Considering the problem of color distortion caused by the defogging algorithm based on dark channel prior, an improved algorithm was proposed to calculate the transmittance of all channels respectively. First, incident light frequency's effect on the transmittance of various color channels was analyzed according to the Beer-Lambert's Law, from which a proportion among various channel transmittances was derived; afterwards, images were preprocessed by down-sampling to refine transmittance, and then the original size was restored to enhance the operational efficiency of the algorithm; finally, the transmittance of all color channels was acquired in accordance with the proportion, and then the corresponding transmittance was used for image restoration in each channel. The experimental results show that compared with the existing algorithm, this improved image defogging algorithm could make image colors more natural, solve the problem of slightly higher color saturation caused by the existing algorithm, and shorten the operation time by four to nine times. version:1
arxiv-1703-01234 | A Bayesian computer model analysis of Robust Bayesian analyses | http://arxiv.org/abs/1703.01234 | id:1703.01234 author:Ian Vernon, John Paul Gosling category:stat.ME stat.AP stat.CO stat.ML  published:2017-03-03 summary:We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up. version:1
arxiv-1703-00573 | Generalization and Equilibrium in Generative Adversarial Nets (GANs) | http://arxiv.org/abs/1703.00573 | id:1703.00573 author:Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, Yi Zhang category:cs.LG  published:2017-03-02 summary:This paper makes progress on several open theoretical issues related to Generative Adversarial Networks. A definition is provided for what it means for the training to generalize, and it is shown that generalization is not guaranteed for the popular distances between distributions such as Jensen-Shannon or Wasserstein. We introduce a new metric called neural net distance for which generalization does occur. We also show that an approximate pure equilibrium in the 2-player game exists for a natural training objective (Wasserstein). Showing such a result has been an open problem (for any training objective). Finally, the above theoretical ideas lead us to propose a new training protocol, MIX+GAN, which can be combined with any existing method. We present experiments showing that it stabilizes and improves some existing methods. version:2
arxiv-1703-01229 | Deep Collaborative Learning for Visual Recognition | http://arxiv.org/abs/1703.01229 | id:1703.01229 author:Yan Wang, Lingxi Xie, Ya Zhang, Wenjun Zhang, Alan Yuille category:cs.CV  published:2017-03-03 summary:Deep neural networks are playing an important role in state-of-the-art visual recognition. To represent high-level visual concepts, modern networks are equipped with large convolutional layers, which use a large number of filters and contribute significantly to model complexity. For example, more than half of the weights of AlexNet are stored in the first fully-connected layer (4,096 filters). We formulate the function of a convolutional layer as learning a large visual vocabulary, and propose an alternative way, namely Deep Collaborative Learning (DCL), to reduce the computational complexity. We replace a convolutional layer with a two-stage DCL module, in which we first construct a couple of smaller convolutional layers individually, and then fuse them at each spatial position to consider feature co-occurrence. In mathematics, DCL can be explained as an efficient way of learning compositional visual concepts, in which the vocabulary size increases exponentially while the model complexity only increases linearly. We evaluate DCL on a wide range of visual recognition tasks, including a series of multi-digit number classification datasets, and some generic image classification datasets such as SVHN, CIFAR and ILSVRC2012. We apply DCL to several state-of-the-art network structures, improving the recognition accuracy meanwhile reducing the number of parameters (16.82% fewer in AlexNet). version:1
arxiv-1703-01226 | Context Aware Query Image Representation for Particular Object Retrieval | http://arxiv.org/abs/1703.01226 | id:1703.01226 author:Zakaria Laskar, Juho Kannala category:cs.CV I.5.4  published:2017-03-03 summary:The current models of image representation based on Convolutional Neural Networks (CNN) have shown tremendous performance in image retrieval. Such models are inspired by the information flow along the visual pathway in the human visual cortex. We propose that in the field of particular object retrieval, the process of extracting CNN representations from query images with a given region of interest (ROI) can also be modelled by taking inspiration from human vision. Particularly, we show that by making the CNN pay attention on the ROI while extracting query image representation leads to significant improvement over the baseline methods on challenging Oxford5k and Paris6k datasets. Furthermore, we propose an extension to a recently introduced encoding method for CNN representations, regional maximum activations of convolutions (R-MAC). The proposed extension weights the regional representations using a novel saliency measure prior to aggregation. This leads to further improvement in retrieval accuracy. version:1
arxiv-1703-01220 | Denoising Adversarial Autoencoders | http://arxiv.org/abs/1703.01220 | id:1703.01220 author:Antonia Creswell, Anil Anthony Bharath category:cs.CV cs.LG stat.ML  published:2017-03-03 summary:Unsupervised learning is of growing interest because it unlocks the potential held in vast amounts of unlabelled data to learn useful representations for inference. Autoencoders, a form of generative model, may be trained by learning to reconstruct unlabelled input data from a latent representation space. More robust representations may be produced by an autoencoder if it learns to recover clean input samples from corrupted ones. Representations may be further improved by introducing regularisation during training to shape the distribution of the encoded data in latent space. We suggest denoising adversarial autoencoders, which combine denoising and regularisation, shaping the distribution of latent space using adversarial training. We introduce a novel analysis that shows how denoising may be incorporated into the training and sampling of adversarial autoencoders. Experiments are performed to assess the contributions that denoising makes to the learning of representations for classification and sample synthesis. Our results suggest that autoencoders trained using a denoising criterion achieve higher classification performance, and can synthesise samples that are more consistent with the input data than those trained without a corruption process. version:1
arxiv-1703-01218 | Learning Graphical Games from Behavioral Data: Sufficient and Necessary Conditions | http://arxiv.org/abs/1703.01218 | id:1703.01218 author:Asish Ghoshal, Jean Honorio category:cs.LG  published:2017-03-03 summary:In this paper we obtain sufficient and necessary conditions on the number of samples required for exact recovery of the pure-strategy Nash equilibria (PSNE) set of a graphical game from noisy observations of joint actions. We consider sparse linear influence games --- a parametric class of graphical games with linear payoffs, and represented by directed graphs of n nodes (players) and in-degree of at most k. We show that one can efficiently recover the PSNE set of a linear influence game with $O(k^2 \log n)$ samples, under very general observation models. On the other hand, we show that $\Omega(k \log n)$ samples are necessary for any procedure to recover the PSNE set from observations of joint actions. version:1
arxiv-1701-08142 | Modelling Preference Data with the Wallenius Distribution | http://arxiv.org/abs/1701.08142 | id:1701.08142 author:Clara Grazian, Fabrizio Leisen, Brunero Liseo category:stat.ME stat.AP stat.CO stat.ML  published:2017-01-27 summary:The Wallenius distribution is a generalisation of the Hypergeometric distribution where weights are assigned to balls of different colours. This naturally defines a model for ranking categories which can be used for classification purposes. Since, in general, the resulting likelihood is not analytically available, we adopt an approximate Bayesian computational (ABC) approach for estimating the importance of the categories. We illustrate the performance of the estimation procedure on simulated datasets. Finally, we use the new model for analysing two datasets about movies ratings and Italian academic statisticians' journal preferences. The latter is a novel dataset collected by the authors. version:3
arxiv-1703-01210 | EmotioNet Challenge: Recognition of facial expressions of emotion in the wild | http://arxiv.org/abs/1703.01210 | id:1703.01210 author:C. Fabian Benitez-Quiroz, Ramprakash Srinivasan, Qianli Feng, Yan Wang, Aleix M. Martinez category:cs.CV  published:2017-03-03 summary:This paper details the methodology and results of the EmotioNet challenge. This challenge is the first to test the ability of computer vision algorithms in the automatic analysis of a large number of images of facial expressions of emotion in the wild. The challenge was divided into two tracks. The first track tested the ability of current computer vision algorithms in the automatic detection of action units (AUs). Specifically, we tested the detection of 11 AUs. The second track tested the algorithms' ability to recognize emotion categories in images of facial expressions. Specifically, we tested the recognition of 16 basic and compound emotion categories. The results of the challenge suggest that current computer vision and machine learning algorithms are unable to reliably solve these two tasks. The limitations of current algorithms are more apparent when trying to recognize emotion. We also show that current algorithms are not affected by mild resolution changes, small occluders, gender or age, but that 3D pose is a major limiting factor on performance. We provide an in-depth discussion of the points that need special attention moving forward. version:1
arxiv-1703-00203 | Frequency patterns of semantic change: Corpus-based evidence of a near-critical dynamics in language change | http://arxiv.org/abs/1703.00203 | id:1703.00203 author:Quentin Feltgen, Benjamin Fagard, Jean-Pierre Nadal category:physics.soc-ph cs.CL  published:2017-03-01 summary:It is generally believed that, when a linguistic item acquires a new meaning, its overall frequency of use in the language rises with time with an S-shaped growth curve. Yet, this claim has only been supported by a limited number of case studies. In this paper, we provide the first corpus-based quantitative confirmation of the genericity of the S-curve in language change. Moreover, we uncover another generic pattern, a latency phase of variable duration preceding the S-growth, during which the frequency of use of the semantically expanding word remains low and more or less constant. We also propose a usage-based model of language change supported by cognitive considerations, which predicts that both phases, the latency and the fast S-growth, take place. The driving mechanism is a stochastic dynamics, a random walk in the space of frequency of use. The underlying deterministic dynamics highlights the role of a control parameter, the strength of the cognitive impetus governing the onset of change, which tunes the system at the vicinity of a saddle-node bifurcation. In the neighborhood of the critical point, the latency phase corresponds to the diffusion time over the critical region, and the S-growth to the fast convergence that follows. The duration of the two phases is computed as specific first passage times of the random walk process, leading to distributions that fit well the ones extracted from our dataset. We argue that our results are not specific to the studied corpus, but apply to semantic change in general. version:2
arxiv-1703-01203 | Stochastic Separation Theorems | http://arxiv.org/abs/1703.01203 | id:1703.01203 author:A. N. Gorban, I. Y. Tyukin category:cs.LG cs.AI 68T10 I.2.6  published:2017-03-03 summary:A set $S$ is linearly separable if each $x\in S$ can be separated from the rest of $S$ by a linear functional. We study random $N$-element sets in $\mathbb{R}^n$ for large $n$ and demonstrate that for $N<a\exp(b{n})$ they are linearly separable with probability $p$, $p>1-\vartheta$, for a given (small) $\vartheta>0$. Constants $a,b>0$ depend on the probability distribution and the constant $\vartheta$. The results are important for machine learning in high dimension, especially for correction of unavoidable mistakes of legacy Artificial Intelligence systems. version:1
arxiv-1703-00660 | Traffic-Aware Transmission Mode Selection in D2D-enabled Cellular Networks with Token System | http://arxiv.org/abs/1703.00660 | id:1703.00660 author:Yiling Yuan, Tao Yang, Hui Feng, Bo Hu, Jianqiu Zhang, Bin Wang, Qiyong Lu category:cs.IT cs.LG math.IT  published:2017-03-02 summary:We consider a D2D-enabled cellular network where user equipments (UEs) owned by rational users are incentivized to form D2D pairs using tokens. They exchange tokens electronically to "buy" and "sell" D2D services. Meanwhile the devices have the ability to choose the transmission mode, i.e. receiving data via cellular links or D2D links. Thus taking the different benefits brought by diverse traffic types as a prior, the UEs can utilize their tokens more efficiently via transmission mode selection. In this paper, the optimal transmission mode selection strategy as well as token collection policy are investigated to maximize the long-term utility in the dynamic network environment. The optimal policy is proved to be a threshold strategy, and the thresholds have a monotonicity property. Numerical simulations verify our observations and the gain from transmission mode selection is observed. version:2
arxiv-1703-01196 | Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity | http://arxiv.org/abs/1703.01196 | id:1703.01196 author:Asish Ghoshal, Jean Honorio category:cs.LG stat.ML  published:2017-03-03 summary:Learning the directed acyclic graph (DAG) structure of a Bayesian network from observational data is a notoriously difficult problem for which many hardness results are known. In this paper we propose a provably polynomial-time algorithm for learning sparse Gaussian Bayesian networks with equal noise variance --- a class of Bayesian networks for which the DAG structure can be uniquely identified from observational data --- under high-dimensional settings. We show that $O(k^4 \log p)$ number of samples suffices for our method to recover the true DAG structure with high probability, where $p$ is the number of variables and $k$ is the maximum Markov blanket size. We obtain our theoretical guarantees under a condition called Restricted Strong Adjacency Faithfulness, which is strictly weaker than strong faithfulness --- a condition that other methods based on conditional independence testing need for their success. The sample complexity of our method matches the information-theoretic limits in terms of the dependence on $p$. We show that our method out-performs existing state-of-the-art methods for learning Gaussian Bayesian networks in terms of recovering the true DAG structure while being comparable in speed to heuristic methods. version:1
arxiv-1703-01170 | A Survey on Content-Aware Video Analysis for Sports | http://arxiv.org/abs/1703.01170 | id:1703.01170 author:Huang-Chia Shih category:cs.CV cs.MM  published:2017-03-03 summary:Sports data analysis is becoming increasingly large-scale, diversified, and shared, but difficulty persists in rapidly accessing the most crucial information. Previous surveys have focused on the methodologies of sports video analysis from the spatiotemporal viewpoint instead of a content-based viewpoint, and few of these studies have considered semantics. This study develops a deeper interpretation of content-aware sports video analysis by examining the insight offered by research into the structure of content under different scenarios. On the basis of this insight, we provide an overview of the themes particularly relevant to the research on content-aware systems for broadcast sports. Specifically, we focus on the video content analysis techniques applied in sportscasts over the past decade from the perspectives of fundamentals and general review, a content hierarchical model, and trends and challenges. Content-aware analysis methods are discussed with respect to object-, event-, and context-oriented groups. In each group, the gap between sensation and content excitement must be bridged using proper strategies. In this regard, a content-aware approach is required to determine user demands. Finally, the paper summarizes the future trends and challenges for sports video analysis. We believe that our findings can advance the field of research on content-aware video analysis for broadcast sports. version:1
arxiv-1703-01141 | Dynamic State Warping | http://arxiv.org/abs/1703.01141 | id:1703.01141 author:Zhichen Gong, Huanhuan Chen category:cs.LG  published:2017-03-03 summary:The ubiquity of sequences in many domains enhances significant recent interest in sequence learning, for which a basic problem is how to measure the distance between sequences. Dynamic time warping (DTW) aligns two sequences by nonlinear local warping and returns a distance value. DTW shows superior ability in many applications, e.g. video, image, etc. However, in DTW, two points are paired essentially based on point-to-point Euclidean distance (ED) without considering the autocorrelation of sequences. Thus, points with different semantic meanings, e.g. peaks and valleys, may be matched providing their coordinate values are similar. As a result, DTW is sensitive to noise and poorly interpretable. This paper proposes an efficient and flexible sequence alignment algorithm, dynamic state warping (DSW). DSW converts each time point into a latent state, which endows point-wise autocorrelation information. Alignment is performed by using the state sequences. Thus DSW is able to yield alignment that is semantically more interpretable than that of DTW. Using one nearest neighbor classifier, DSW shows significant improvement on classification accuracy in comparison to ED (70/85 wins) and DTW (74/85 wins). We also empirically demonstrate that DSW is more robust and scales better to long sequences than ED and DTW. version:1
arxiv-1703-01135 | Deep Learning with Domain Adaptation for Accelerated Projection Reconstruction MR | http://arxiv.org/abs/1703.01135 | id:1703.01135 author:Yo Seob Han, Jaejun Yoo, Jong Chul Ye category:cs.CV  published:2017-03-03 summary:Purpose: A radial k-space trajectory is one of well-established sampling trajectory in magnetic resonance imaging. However, the radial k-space trajectory requires a large number of radial lines for high-resolution reconstruction. Increasing the number of lines causes longer sampling times, making it more difficult for routine clinical use. If we reduce the radial lines to reduce the sampling time, streaking artifact patterns are unavoidable. To solve this problem, we propose a novel deep learning approach to reconstruct high-resolution MR images from the under-sampled k-space data. Methods: The proposed deep network estimates the streaking artifacts. Once the streaking artifacts are estimated, an artifact-free image is then obtained by subtracting the estimated streaking artifacts from the distorted image. In the case of the limited number of available radial acquisition data, we apply a domain adaptation scheme, which first pre-trains the network with a large number of x-ray computed tomography (CT) data sets and then fine-tunes it with only a few MR data sets. Results: The proposed deep learning method shows better performance than the existing compressed sensing algorithms, such as total variation and PR-FOCUSS. In addition, the calculation time is several order of magnitude faster than total variation and PR-FOCUSS methods. Conclusion: The proposed deep learning method surpasses the image quality as well as the computation times against the existing compressed sensing algorithms. In addition, we demonstrate the possibilities of domain-adaptation approach when a limited number of MR data is available. version:1
arxiv-1703-01120 | Deep artifact learning for compressed sensing and parallel MRI | http://arxiv.org/abs/1703.01120 | id:1703.01120 author:Dongwook Lee, Jaejun Yoo, Jong Chul Ye category:cs.CV  published:2017-03-03 summary:Purpose: Compressed sensing MRI (CS-MRI) from single and parallel coils is one of the powerful ways to reduce the scan time of MR imaging with performance guarantee. However, the computational costs are usually expensive. This paper aims to propose a computationally fast and accurate deep learning algorithm for the reconstruction of MR images from highly down-sampled k-space data. Theory: Based on the topological analysis, we show that the data manifold of the aliasing artifact is easier to learn from a uniform subsampling pattern with additional low-frequency k-space data. Thus, we develop deep aliasing artifact learning networks for the magnitude and phase images to estimate and remove the aliasing artifacts from highly accelerated MR acquisition. Methods: The aliasing artifacts are directly estimated from the distorted magnitude and phase images reconstructed from subsampled k-space data so that we can get an aliasing-free images by subtracting the estimated aliasing artifact from corrupted inputs. Moreover, to deal with the globally distributed aliasing artifact, we develop a multi-scale deep neural network with a large receptive field. Results: The experimental results confirm that the proposed deep artifact learning network effectively estimates and removes the aliasing artifacts. Compared to existing CS methods from single and multi-coli data, the proposed network shows minimal errors by removing the coherent aliasing artifacts. Furthermore, the computational time is by order of magnitude faster. Conclusion: As the proposed deep artifact learning network immediately generates accurate reconstruction, it has great potential for clinical applications. version:1
arxiv-1703-01106 | Differentially Private Bayesian Learning on Distributed Data | http://arxiv.org/abs/1703.01106 | id:1703.01106 author:Mikko Heikkilä, Yusuke Okimoto, Samuel Kaski, Kana Shimizu, Antti Honkela category:stat.ML cs.CR cs.LG stat.CO  published:2017-03-03 summary:Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results, but the proposed algorithms require a single trusted party to have access to the entire data, which is a clear weakness. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a novel method for DP learning in this distributed setting, based on a secure multi-party sum function for aggregating summaries from the data holders. Each data holder adds their share of Gaussian noise to make the total computation differentially private using the Gaussian mechanism. We prove that the system can be made secure against a desired number of colluding data owners and robust against faulting data owners. The method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost. version:1
arxiv-1703-01101 | Adversarial Examples for Semantic Image Segmentation | http://arxiv.org/abs/1703.01101 | id:1703.01101 author:Volker Fischer, Mummadi Chaithanya Kumar, Jan Hendrik Metzen, Thomas Brox category:stat.ML cs.CR cs.CV cs.LG cs.NE  published:2017-03-03 summary:Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations that lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class. version:1
arxiv-1703-01086 | Arbitrary-Oriented Scene Text Detection via Rotation Proposals | http://arxiv.org/abs/1703.01086 | id:1703.01086 author:Jianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong Wang, Yingbin Zheng, Xiangyang Xue category:cs.CV  published:2017-03-03 summary:This paper introduces a novel rotation-based framework for arbitrary-oriented text detection in natural scene images. We present the Rotation Region Proposal Networks (RRPN), which is designed to generate inclined proposals with text orientation angle information. The angle information is then adapted for bounding box regression to make the proposals more accurately fit into the text region in orientation. The Rotation Region-of-Interest (RRoI) pooling layer is proposed to project arbitrary-oriented proposals to the feature map for a text region classifier. The whole framework is built upon the Faster-RCNN architecture, which ensures the computational efficiency of the arbitrary-oriented text detection comparing with previous text detection systems. We conduct experiments using the rotation-based framework on three real-world scene text detection datasets, and demonstrate its superiority in terms of effectiveness and efficiency over previous approaches. version:1
arxiv-1703-00091 | Semi-analytical approximations to statistical moments of sigmoid and softmax mappings of normal variables | http://arxiv.org/abs/1703.00091 | id:1703.00091 author:Jean Daunizeau category:stat.ML q-bio.NC  published:2017-03-01 summary:This note is concerned with accurate and computationally efficient approximations of moments of Gaussian random variables passed through sigmoid or softmax mappings. These approximations are semi-analytical (i.e. they involve the numerical adjustment of parametric forms) and highly accurate (they yield 5% error at most). We also highlight a few niche applications of these approximations, which arise in the context of, e.g., drift-diffusion models of decision making or non-parametric data clustering approaches. We provide these as examples of efficient alternatives to more tedious derivations that would be needed if one was to approach the underlying mathematical issues in a more formal way. We hope that this technical note will be helpful to modellers facing similar mathematical issues, although maybe stemming from different academic prospects. version:2
arxiv-1703-01053 | Skin Lesion Classification using Class Activation Map | http://arxiv.org/abs/1703.01053 | id:1703.01053 author:Xi Jia, Linlin Shen category:cs.CV  published:2017-03-03 summary:We proposed a two stage framework with only one network to analyze skin lesion images, we firstly trained a convolutional network to classify these images, and cropped the import regions which the network has the maximum activation value. In the second stage, we retrained this CNN with the image regions extracted from stage one and output the final probabilities. The two stage framework achieved a mean AUC of 0.857 in ISIC-2017 skin lesion validation set and is 0.04 higher than that of the original inputs, 0.821. version:1
arxiv-1703-01041 | Large-Scale Evolution of Image Classifiers | http://arxiv.org/abs/1703.01041 | id:1703.01041 author:Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc Le, Alex Kurakin category:cs.NE cs.AI cs.CV cs.DC I.2.6; I.5.1; I.5.2  published:2017-03-03 summary:Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Evolutionary algorithms provide a technique to discover such networks automatically. Despite significant computational requirements, we show that evolving models that rival large, hand-designed architectures is possible today. We employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions. To do this, we use novel and intuitive mutation operators that navigate large search spaces. We stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements. version:1
arxiv-1703-01040 | Learning Robot Activities from First-Person Human Videos Using Convolutional Future Regression | http://arxiv.org/abs/1703.01040 | id:1703.01040 author:Jangwon Lee, Michael S. Ryoo category:cs.RO cs.AI cs.CV cs.LG  published:2017-03-03 summary:We design a new approach that allows robot learning of new activities from unlabeled human example videos. Given videos of humans executing the same activity from a human's viewpoint (i.e., first-person videos), our objective is to make the robot learn the temporal structure of the activity as its future regression network, and learn to transfer such model for its own motor execution. We present a new deep learning model: We extend the state-of-the-art convolutional object detection network for the detection of human hands in training videos based on image information, and newly introduce the concept of using a fully convolutional network to regress (i.e., predict) the intermediate scene representation corresponding to the future frame (e.g., 1-2 seconds later). Combining these allows direct prediction of future locations of human hands and objects, which enables the robot to infer the motor control plan using our manipulation network. We experimentally confirm that our approach makes learning of robot activities from unlabeled human interaction videos possible, and demonstrate that our robot is able to execute the learned collaborative activities in real-time directly based on its camera input. version:1
arxiv-1703-01030 | Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction | http://arxiv.org/abs/1703.01030 | id:1703.01030 author:Wen Sun, Arun Venkatraman, Geoffrey J. Gordon, Byron Boots, J. Andrew Bagnell category:cs.LG  published:2017-03-03 summary:Researchers have demonstrated state-of-the-art performance in sequential decision making problems (e.g., robotics control, sequential prediction) with deep neural network models. One often has access to near-optimal oracles that achieve good performance on the task during training. We demonstrate that AggreVaTeD --- a policy gradient extension of the Imitation Learning (IL) approach of (Ross & Bagnell, 2014) --- can leverage such an oracle to achieve faster and better solutions with less training data than a less-informed Reinforcement Learning (RL) technique. Using both feedforward and recurrent neural network predictors, we present stochastic gradient procedures on a sequential prediction task, dependency-parsing from raw image data, as well as on various high dimensional robotics control problems. We also provide a comprehensive theoretical study of IL that demonstrates we can expect up to exponentially lower sample complexity for learning with AggreVaTeD than with RL algorithms, which backs our empirical findings. Our results and theory indicate that the proposed approach can achieve superior performance with respect to the oracle when the demonstrator is sub-optimal. version:1
arxiv-1703-01028 | Outlier Cluster Formation in Spectral Clustering | http://arxiv.org/abs/1703.01028 | id:1703.01028 author:Takuro Ina, Atsushi Hashimoto, Masaaki Iiyama, Hidekazu Kasahara, Mikihiko Mori, Michihiko Minoh category:cs.CV  published:2017-03-03 summary:Outlier detection and cluster number estimation is an important issue for clustering real data. This paper focuses on spectral clustering, a time-tested clustering method, and reveals its important properties related to outliers. The highlights of this paper are the following two mathematical observations: first, spectral clustering's intrinsic property of an outlier cluster formation, and second, the singularity of an outlier cluster with a valid cluster number. Based on these observations, we designed a function that evaluates clustering and outlier detection results. In experiments, we prepared two scenarios, face clustering in photo album and person re-identification in a camera network. We confirmed that the proposed method detects outliers and estimates the number of clusters properly in both problems. Our method outperforms state-of-the-art methods in both the 128-dimensional sparse space for face clustering and the 4,096-dimensional non-sparse space for person re-identification. version:1
arxiv-1703-01026 | Unsupervised Basis Function Adaptation for Reinforcement Learning | http://arxiv.org/abs/1703.01026 | id:1703.01026 author:Edward W. Barker, Charl J. Ras category:cs.AI cs.LG stat.ML  published:2017-03-03 summary:When using reinforcement learning (RL) algorithms to evaluate a policy it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on the accuracy of the VF estimate, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is a large amount of interest in the potential for allowing RL algorithms to adaptively generate approximation architectures. We investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. This method is "unsupervised" in the sense that it makes no direct reference to reward or the VF estimate. We introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line. A common method of scoring a VF estimate is to weight the squared Bellman error of each state-action by the probability of that state-action occurring. Adopting this scoring method, and assuming $S$ states, we demonstrate theoretically that - provided (1) the number of cells $X$ in the state aggregation architecture is of order $\sqrt{S}\log_2{S}\ln{S}$ or greater, (2) the policy and transition function are close to deterministic, and (3) the prior for the transition function is uniformly distributed - our algorithm, used in conjunction with a suitable RL algorithm, can guarantee a score which is arbitrarily close to zero as $S$ becomes large. It is able to do this despite having only $O(X \log_2S)$ space complexity and negligible time complexity. The results take advantage of certain properties of the stationary distributions of Markov chains. version:1
arxiv-1703-01025 | A Novel Multi-task Deep Learning Model for Skin Lesion Segmentation and Classification | http://arxiv.org/abs/1703.01025 | id:1703.01025 author:Xulei Yang, Zeng Zeng, Si Yong Yeo, Colin Tan, Hong Liang Tey, Yi Su category:cs.CV  published:2017-03-03 summary:In this study, a multi-task deep neural network is proposed for skin lesion analysis. The proposed multi-task learning model solves different tasks (e.g., lesion segmentation and two independent binary lesion classifications) at the same time by exploiting commonalities and differences across tasks. This results in improved learning efficiency and potential prediction accuracy for the task-specific models, when compared to training the individual models separately. The proposed multi-task deep learning model is trained and evaluated on the dermoscopic image sets from the International Skin Imaging Collaboration (ISIC) 2017 Challenge - Skin Lesion Analysis towards Melanoma Detection, which consists of 2000 training samples and 150 evaluation samples. The experimental results show that the proposed multi-task deep learning model achieves promising performances on skin lesion segmentation and classification. The average value of Jaccard index for lesion segmentation is 0.724, while the average values of area under the receiver operating characteristic curve (AUC) on two individual lesion classifications are 0.880 and 0.972, respectively. version:1
arxiv-1703-01024 | Exponential Moving Average Model in Parallel Speech Recognition Training | http://arxiv.org/abs/1703.01024 | id:1703.01024 author:Xu Tian, Jun Zhang, Zejun Ma, Yi He, Juan Wei category:cs.CL  published:2017-03-03 summary:As training data rapid growth, large-scale parallel training with multi-GPUs cluster is widely applied in the neural network model learning currently.We present a new approach that applies exponential moving average method in large-scale parallel training of neural network model. It is a non-interference strategy that the exponential moving average model is not broadcasted to distributed workers to update their local models after model synchronization in the training process, and it is implemented as the final model of the training system. Fully-connected feed-forward neural networks (DNNs) and deep unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs) are successfully trained with proposed method for large vocabulary continuous speech recognition on Shenma voice search data in Mandarin. The character error rate (CER) of Mandarin speech recognition further degrades than state-of-the-art approaches of parallel training. version:1
arxiv-1702-07800 | On the Origin of Deep Learning | http://arxiv.org/abs/1702.07800 | id:1702.07800 author:Haohan Wang, Bhiksha Raj category:cs.LG cs.NE stat.ML  published:2017-02-24 summary:This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning. More importantly, along with the path, this paper summarizes the gist behind these milestones and proposes many directions to guide the future research of deep learning. version:4
arxiv-1703-01014 | Active Learning for Cost-Sensitive Classification | http://arxiv.org/abs/1703.01014 | id:1703.01014 author:Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daume III, John Langford category:cs.LG stat.ML  published:2017-03-03 summary:We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regressing on each label's cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. We empirically compare COAL to passive learning, showing significant improvements in labeling effort and test cost. version:1
arxiv-1703-00535 | Human Interaction with Recommendation Systems: On Bias and Exploration | http://arxiv.org/abs/1703.00535 | id:1703.00535 author:Sven Schmit, Carlos Riquelme category:stat.ML cs.LG  published:2017-03-01 summary:Recommendation systems rely on historical user data to provide suggestions. We propose an explicit and simple model for the interaction between users and recommendations provided by a platform, and relate this model to the multi-armed bandit literature. First, we show that this interaction leads to a bias in naive estimators due to selection effects. This bias leads to suboptimal outcomes, which we quantify in terms of linear regret. We end the first part by discussing ways to obtain unbiased estimates. The second part of this work considers exploration of alternatives. We show that although agents are myopic, agents' heterogeneous preferences ensure that recommendation systems 'learn' about all alternatives without explicitly incentivizing this exploration. This work provides new and practical insights relevant to a wide range of systems designed to help users make better decisions. version:2
arxiv-1703-01006 | Scalable Deep Traffic Flow Neural Networks for Urban Traffic Congestion Prediction | http://arxiv.org/abs/1703.01006 | id:1703.01006 author:Mohammadhani Fouladgar, Mostafa Parchami, Ramez Elmasri, Amir Ghaderi category:cs.LG  published:2017-03-03 summary:Tracking congestion throughout the network road is a critical component of Intelligent transportation network management systems. Understanding how the traffic flows and short-term prediction of congestion occurrence due to rush-hour or incidents can be beneficial to such systems to effectively manage and direct the traffic to the most appropriate detours. Many of the current traffic flow prediction systems are designed by utilizing a central processing component where the prediction is carried out through aggregation of the information gathered from all measuring stations. However, centralized systems are not scalable and fail provide real-time feedback to the system whereas in a decentralized scheme, each node is responsible to predict its own short-term congestion based on the local current measurements in neighboring nodes. We propose a decentralized deep learning-based method where each node accurately predicts its own congestion state in real-time based on the congestion state of the neighboring stations. Moreover, historical data from the deployment site is not required, which makes the proposed method more suitable for newly installed stations. In order to achieve higher performance, we introduce a regularized Euclidean loss function that favors high congestion samples over low congestion samples to avoid the impact of the unbalanced training dataset. A novel dataset for this purpose is designed based on the traffic data obtained from traffic control stations in northern California. Extensive experiments conducted on the designed benchmark reflect a successful congestion prediction. version:1
arxiv-1703-00994 | Co-Clustering for Multitask Learning | http://arxiv.org/abs/1703.00994 | id:1703.00994 author:Keerthiram Murugesan, Jaime Carbonell, Yiming Yang category:stat.ML cs.LG  published:2017-03-03 summary:This paper presents a new multitask learning framework that learns a shared representation among the tasks, incorporating both task and feature clusters. The jointly-induced clusters yield a shared latent subspace where task relationships are learned more effectively and more generally than in state-of-the-art multitask learning methods. The proposed general framework enables the derivation of more specific or restricted state-of-the-art multitask methods. The paper also proposes a highly-scalable multitask learning algorithm, based on the new framework, using conjugate gradient descent and generalized \textit{Sylvester equations}. Experimental results on synthetic and benchmark datasets show that the proposed method systematically outperforms several state-of-the-art multitask learning methods. version:1
arxiv-1703-00993 | A Comparative Study of Word Embeddings for Reading Comprehension | http://arxiv.org/abs/1703.00993 | id:1703.00993 author:Bhuwan Dhingra, Hanxiao Liu, Ruslan Salakhutdinov, William W. Cohen category:cs.CL  published:2017-03-02 summary:The focus of past machine learning research for Reading Comprehension tasks has been primarily on the design of novel deep learning architectures. Here we show that seemingly minor choices made on (1) the use of pre-trained word embeddings, and (2) the representation of out-of-vocabulary tokens at test time, can turn out to have a larger impact than architectural choices on the final performance. We systematically explore several options for these choices, and provide recommendations to researchers working in this area. version:1
arxiv-1703-00992 | Estimating the resolution of real images | http://arxiv.org/abs/1703.00992 | id:1703.00992 author:Ryuta Mizutani, Rino Saiga, Susumu Takekoshi, Chie Inomoto, Naoya Nakamura, Makoto Arai, Kenichi Oshima, Masanari Itokawa, Akihisa Takeuchi, Kentaro Uesugi, Yasuko Terada, Yoshio Suzuki category:physics.data-an cs.CV physics.med-ph  published:2017-03-02 summary:Image resolvability is the primary concern in imaging. This paper reports an estimation of the full width at half maximum of the point spread function from a Fourier domain plot of real sample images by neither using test objects, nor defining a threshold criterion. We suggest that this method can be applied to any type of image, independently of the imaging modality. version:1
arxiv-1703-00989 | Optimization of distributions differences for classification | http://arxiv.org/abs/1703.00989 | id:1703.00989 author:Mohammad Reza Bonyadi, Quang M. Tieng, David C. Reutens category:cs.LG stat.ML  published:2017-03-02 summary:In this paper we introduce a new classification algorithm called Optimization of Distributions Differences (ODD). The algorithm aims to find a transformation from the feature space to a new space where the instances in the same class are as close as possible to one another while the gravity centers of these classes are as far as possible from one another. This aim is formulated as a multiobjective optimization problem that is solved by a hybrid of an evolutionary strategy and the Quasi-Newton method. The choice of the transformation function is flexible and could be any continuous space function. We experiment with a linear and a non-linear transformation in this paper. We show that the algorithm can outperform 6 other state-of-the-art classification methods, namely naive Bayes, support vector machines, linear discriminant analysis, multi-layer perceptrons, decision trees, and k-nearest neighbors, in 12 standard classification datasets. Our results show that the method is less sensitive to the imbalanced number of instances comparing to these methods. We also show that ODD maintains its performance better than other classification methods in these datasets, hence, offers a better generalization ability. version:1
arxiv-1703-00986 | Belief Propagation in Conditional RBMs for Structured Prediction | http://arxiv.org/abs/1703.00986 | id:1703.00986 author:Wei Ping, Alexander Ihler category:cs.LG cs.CV stat.ML  published:2017-03-02 summary:Restricted Boltzmann machines~(RBMs) and conditional RBMs~(CRBMs) are popular models for a wide range of applications. In previous work, learning on such models has been dominated by contrastive divergence~(CD) and its variants. Belief propagation~(BP) algorithms are believed to be slow for structured prediction on conditional RBMs~(e.g., Mnih et al. [2011]), and not as good as CD when applied in learning~(e.g., Larochelle et al. [2012]). In this work, we present a matrix-based implementation of belief propagation algorithms on CRBMs, which is easily scalable to tens of thousands of visible and hidden units. We demonstrate that, in both maximum likelihood and max-margin learning, training conditional RBMs with BP as the inference routine can provide significantly better results than current state-of-the-art CD methods on structured prediction problems. We also include practical guidelines on training CRBMs with BP, and some insights on the interaction of learning and inference algorithms for CRBMs. version:1
arxiv-1703-00981 | A Restaurant Process Mixture Model for Connectivity Based Parcellation of the Cortex | http://arxiv.org/abs/1703.00981 | id:1703.00981 author:Daniel Moyer, Boris A Gutman, Neda Jahanshad, Paul M. Thompson category:q-bio.NC cs.CE cs.CV q-bio.QM stat.AP  published:2017-03-02 summary:One of the primary objectives of human brain mapping is the division of the cortical surface into functionally distinct regions, i.e. parcellation. While it is generally agreed that at macro-scale different regions of the cortex have different functions, the exact number and configuration of these regions is not known. Methods for the discovery of these regions are thus important, particularly as the volume of available information grows. Towards this end, we present a parcellation method based on a Bayesian non-parametric mixture model of cortical connectivity. version:1
arxiv-1703-00978 | Compositional Falsification of Cyber-Physical Systems with Machine Learning Components | http://arxiv.org/abs/1703.00978 | id:1703.00978 author:Tommaso Dreossi, Alexandre Donzé, Sanjit A. Seshia category:cs.SY cs.LG  published:2017-03-02 summary:Cyber-physical systems (CPS), such as automotive systems, are starting to include sophisticated machine learning (ML) components. Their correctness, therefore, depends on properties of the inner ML modules. While learning algorithms aim to generalize from examples, they are only as good as the examples provided, and recent efforts have shown that they can produce inconsistent output under small adversarial perturbations. This raises the question: can the output from learning components can lead to a failure of the entire CPS? In this work, we address this question by formulating it as a problem of falsifying signal temporal logic (STL) specifications for CPS with ML components. We propose a compositional falsification framework where a temporal logic falsifier and a machine learning analyzer cooperate with the aim of finding falsifying executions of the considered model. The efficacy of the proposed technique is shown on an automatic emergency braking system model with a perception component based on deep neural networks. version:1
arxiv-1703-00977 | Self-Paced Multitask Learning with Shared Knowledge | http://arxiv.org/abs/1703.00977 | id:1703.00977 author:Keerthiram Murugesan, Jaime Carbonell category:stat.ML cs.LG  published:2017-03-02 summary:This paper introduces self-paced task selection to multitask learning, where instances from more closely related tasks are selected in a progression of easier-to-harder tasks, to emulate an effective human education strategy but applied to multitask machine learning. We develop the mathematical foundation for the approach based on an iterative selection of the most appropriate task, learning the task parameters, and updating the shared knowledge, optimizing a new bi-convex loss function. This proposed method applies quite generally, including to multitask feature learning, multitask learning with alternating structure optimization and multitask manifold regularization learning. Results show that in each of the above formulations self-paced (easier-to-harder) task selection outperforms the baseline version of these methods in all the experiments. version:1
arxiv-1703-04392 | Enhancement of human color vision by breaking the binocular redundancy | http://arxiv.org/abs/1703.04392 | id:1703.04392 author:Bradley S. Gundlach, Alireza Shahsafi, Gregory Vershbow, Chenghao Wan, Jad Salman, Bas Rokers, Laurent Lessard, Mikhail A. Kats category:cs.CV physics.bio-ph  published:2017-03-02 summary:To see color, the human visual system combines the responses of three types of cone cells in the retina - a process that discards a significant amount of spectral information. We present an approach that can enhance human color vision by breaking the inherent redundancy in binocular vision, providing different spectral content to each eye. Using a psychophysical color model and thin-film optimization, we designed a wearable passive multispectral device that uses two distinct transmission filters, one for each eye, to enhance the user's ability to perceive spectral information. We fabricated and tested a design that "splits" the response of the short-wavelength cone of individuals with typical trichromatic vision, effectively simulating the presence of four distinct cone types between the two eyes ("tetrachromacy"). Users of this device were able to differentiate metamers (distinct spectra that resolve to the same perceived color in typical observers) without apparent adverse effects to vision. The increase in the number of effective cones from the typical three reduces the number of possible metamers that can be encountered, enhancing the ability to discriminate objects based on their emission, reflection, or transmission spectra. This technique represents a significant enhancement of the spectral perception of typical humans, and may have applications ranging from camouflage detection and anti-counterfeiting to art and data visualization. version:1
arxiv-1703-00956 | A Laplacian Framework for Option Discovery in Reinforcement Learning | http://arxiv.org/abs/1703.00956 | id:1703.00956 author:Marlos C. Machado, Marc G. Bellemare, Michael Bowling category:cs.LG cs.AI  published:2017-03-02 summary:Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-RL is a well known approach for representation learning in MDPs. The representations learned with this framework are called proto-value functions (PVFs). In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are independent of the agents' intentions. Moreover, by capturing the diffusion process of a random walk, different options act at different time scales, making them helpful for exploration strategies. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games. version:1
arxiv-1703-00955 | Controllable Text Generation | http://arxiv.org/abs/1703.00955 | id:1703.00955 author:Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P. Xing category:cs.LG cs.AI cs.CL stat.ML  published:2017-03-02 summary:Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation. version:1
arxiv-1703-00948 | DAWT: Densely Annotated Wikipedia Texts across multiple languages | http://arxiv.org/abs/1703.00948 | id:1703.00948 author:Nemanja Spasojevic, Preeti Bhargava, Guoning Hu category:cs.IR cs.AI cs.CL cs.SI  published:2017-03-02 summary:In this work, we open up the DAWT dataset - Densely Annotated Wikipedia Texts across multiple languages. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of the entity. The data set contains total of 13.6M articles, 5.0B tokens, 13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text to entity links than originally present in the Wikipedia markup. Moreover, it spans several languages including English, Spanish, Italian, German, French and Arabic. We also present the methodology used to generate the dataset which enriches Wikipedia markup in order to increase number of links. In addition to the main dataset, we open up several derived datasets including mention entity co-occurrence counts and entity embeddings, as well as mappings between Freebase ids and Wikidata item ids. We also discuss two applications of these datasets and hope that opening them up would prove useful for the Natural Language Processing and Information Retrieval communities, as well as facilitate multi-lingual research. version:1
arxiv-1702-08608 | Towards A Rigorous Science of Interpretable Machine Learning | http://arxiv.org/abs/1702.08608 | id:1702.08608 author:Finale Doshi-Velez, Been Kim category:stat.ML cs.AI cs.LG  published:2017-02-28 summary:As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning. version:2
arxiv-1703-00919 | Depth Estimation using Modified Cost Function for Occlusion Handling | http://arxiv.org/abs/1703.00919 | id:1703.00919 author:Krzysztof Wegner, Olgierd Stankiewicz category:cs.MM cs.CV  published:2017-03-02 summary:The paper presents a novel approach to occlusion handling problem in depth estimation using three views. A solution based on modification of similarity cost function is proposed. During the depth estimation via optimization algorithms like Graph Cut similarity metric is constantly updated so that only non-occluded fragments in side views are considered. At each iteration of the algorithm non-occluded fragments are detected based on side view virtual depth maps synthesized from the best currently estimated depth map of the center view. Then similarity metric is updated for correspondence search only in non-occluded regions of the side views. The experimental results, conducted on well-known 3D video test sequences, have proved that the depth maps estimated with the proposed approach provide about 1.25 dB virtual view quality improvement in comparison to the virtual view synthesized based on depth maps generated by the state-of-the-art MPEG Depth Estimation Reference Software. version:1
arxiv-1702-08134 | Online Multiview Representation Learning: Dropping Convexity for Better Efficiency | http://arxiv.org/abs/1702.08134 | id:1702.08134 author:Zhehui Chen, Forest L. Yang, Chris J. Li, Tuo Zhao category:cs.LG math.OC stat.ML  published:2017-02-27 summary:Multiview representation learning is very popular for latent factor analysis. It naturally arises in many data analysis, machine learning, and information retrieval applications to model dependent structures between a pair of data matrices. For computational convenience, existing approaches usually formulate the multiview representation learning as convex optimization problems, where global optima can be obtained by certain algorithms in polynomial time. However, many evidences have corroborated that heuristic nonconvex approaches also have good empirical computational performance and convergence to the global optima, although there is a lack of theoretical justification. Such a gap between theory and practice motivates us to study a nonconvex formulation for multiview representation learning, which can be efficiently solved by two stochastic gradient descent (SGD) methods. Theoretically, by analyzing the dynamics of the algorithms based on diffusion processes, we establish global rates of convergence to the global optima with high probability. Numerical experiments are provided to support our theory. version:2
arxiv-1703-00893 | Being Robust (in High Dimensions) Can Be Practical | http://arxiv.org/abs/1703.00893 | id:1703.00893 author:Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, Alistair Stewart category:cs.LG cs.DS cs.IT math.IT stat.ML  published:2017-03-02 summary:Robust estimation is much more challenging in high dimensions than it is in one dimension: Most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, in appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions, independent of the dimension. However, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications. In this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions. Finally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility. version:1
arxiv-1703-00887 | How to Escape Saddle Points Efficiently | http://arxiv.org/abs/1703.00887 | id:1703.00887 author:Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, Michael I. Jordan category:cs.LG math.OC stat.ML  published:2017-03-02 summary:This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost "dimension-free"). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community. version:1
arxiv-1702-08634 | Super-Trajectory for Video Segmentation | http://arxiv.org/abs/1702.08634 | id:1702.08634 author:Wenguan Wang, Jianbing Shen category:cs.CV  published:2017-02-28 summary:We propose a semi-supervised video segmentation via an efficient video representation, called as "super-trajectory". Each super-trajectory corresponds to a group of compact trajectories that exhibit consistent motion patterns, similar appearance and close spatiotemporal relationships. To handle occlusions and drifts, we develop a trajectory generation method based on probabilistic model, which is more reasonable and interpretable than traditional trajectory methods using hard thresholding. We then modify a density peaks based clustering algorithm for reliably grouping trajectories, thus capturing a rich set of spatial and temporal relations among trajectories. Via this discriminative video representation, manual effort on the first frame can be efficiently propagated into the rest of frames. Experimental results on challenging benchmark demonstrate the proposed approach is capable of distinguishing object from complex background and even re-identifying object with long-term occlusions. version:2
arxiv-1703-00868 | Using Synthetic Data to Train Neural Networks is Model-Based Reasoning | http://arxiv.org/abs/1703.00868 | id:1703.00868 author:Tuan Anh Le, Atilim Gunes Baydin, Robert Zinkov, Frank Wood category:cs.LG cs.CV stat.ML 68T05  68T10 I.2.6; I.7.5  published:2017-03-02 summary:We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data. version:1
arxiv-1703-00864 | The Unreasonable Effectiveness of Random Orthogonal Embeddings | http://arxiv.org/abs/1703.00864 | id:1703.00864 author:Krzysztof Choromanski, Mark Rowland, Adrian Weller category:stat.ML stat.CO  published:2017-03-02 summary:We present a general class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction, kernel approximation and locality-sensitive hashing. We show that this class yields improvements over previous state-of-the-art methods either in computational efficiency (while providing similar accuracy) or in accuracy, or both. In particular, we propose the \textit{Orthogonal Johnson-Lindenstrauss Transform} (OJLT) which is as fast as earlier methods yet provably outperforms them in terms of accuracy, leading to a `free lunch' improvement over previous dimensionality reduction mechanisms. We introduce matrices with complex entries that further improve accuracy. Other applications include estimators for certain pointwise nonlinear Gaussian kernels, and speed improvements for approximate nearest-neighbor search in massive datasets with high-dimensional feature vectors. version:1
arxiv-1703-00862 | Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment with Limited Resources | http://arxiv.org/abs/1703.00862 | id:1703.00862 author:Adrian Bulat, Georgios Tzimiropoulos category:cs.CV  published:2017-03-02 summary:This work is on landmark localization using binarized approximations of Convolutional Neural Networks (CNNs). Our goal is to design architectures that retain the groundbreaking performance of CNNs for landmark localization and at the same time are lightweight, compact and suitable for applications with limited computational resources. To this end, we make the following contributions: (a) we are the first to study the effect of neural network binarization on localization tasks, namely human pose estimation and face alignment. We exhaustively evaluate various design choices, identify performance bottlenecks, and more importantly propose multiple orthogonal ways to boost performance. (b) Based on our analysis, we propose a novel hierarchical, parallel and multi-scale residual block architecture that yields large performance improvement over the standard bottleneck block when having the same number of parameters, thus bridging the gap between the original network and its binarized counterpart. (c) We also show that the performance boost offered by the proposed architecture is not only observed for the case of binary networks but also generalizes for the case of real valued weights and activations. (d) We perform a large number of ablation studies that shed light on the properties and the performance of the proposed block. (e) We present results for experiments on the most challenging datasets for human pose estimation and face alignment, reporting in many cases state-of-the-art performance. Code can be download from http://www.cs.nott.ac.uk/~psxab5/binary-cnn-landmarks/ version:1
arxiv-1703-00856 | Araguaia Medical Vision Lab at ISIC 2017 Skin Lesion Classification Challenge | http://arxiv.org/abs/1703.00856 | id:1703.00856 author:Rafael Teixeira Sousa, Larissa Vasconcellos de Moraes category:cs.CV  published:2017-03-02 summary:This paper describes the participation of Araguaia Medical Vision Lab at the International Skin Imaging Collaboration 2017 Skin Lesion Challenge. We describe the use of deep convolutional neural networks in attempt to classify images of Melanoma and Seborrheic Keratosis lesions. With use of finetuned GoogleNet and AlexNet we attained results of 0.950 and 0.846 AUC on Seborrheic Keratosis and Melanoma respectively. version:1
arxiv-1703-02124 | Non-line-of-sight tracking of people at long range | http://arxiv.org/abs/1703.02124 | id:1703.02124 author:Susan Chan, Ryan E. Warburton, Genevieve Gariepy, Jonathan Leach, Daniele Faccio category:cs.CV physics.ins-det  published:2017-03-02 summary:A remote-sensing system that can determine the position of hidden objects has applications in many critical real-life scenarios, such as search and rescue missions and safe autonomous driving. Previous work has shown the ability to range and image objects hidden from the direct line of sight, employing advanced optical imaging technologies aimed at small objects at short range. In this work we demonstrate a long-range tracking system based on single laser illumination and single-pixel single-photon detection. This enables us to track one or more people hidden from view at a stand-off distance of over 50~m. These results pave the way towards next generation LiDAR systems that will reconstruct not only the direct-view scene but also the main elements hidden behind walls or corners. version:1
arxiv-1703-00854 | Learning the Structure of Generative Models without Labeled Data | http://arxiv.org/abs/1703.00854 | id:1703.00854 author:Stephen H. Bach, Bryan He, Alexander Ratner, Christopher Ré category:cs.LG stat.ML  published:2017-03-02 summary:Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model's dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the $\ell_1$-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Experiments on synthetic data show that our method is 100$\times$ faster than a maximum likelihood approach and selects $1/4$ as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal articles. version:1
arxiv-1703-00848 | Unsupervised Image-to-Image Translation Networks | http://arxiv.org/abs/1703.00848 | id:1703.00848 author:Ming-Yu Liu, Thomas Breuel, Jan Kautz category:cs.CV cs.AI  published:2017-03-02 summary:Most of the existing image-to-image translation frameworks---mapping an image in one domain to a corresponding image in another---are based on supervised learning, i.e., pairs of corresponding images in two domains are required for learning the translation function. This largely limits their applications, because capturing corresponding images in two different domains is often a difficult task. To address the issue, we propose the UNsupervised Image-to-image Translation (UNIT) framework, which is based on variational autoencoders and generative adversarial networks. The proposed framework can learn the translation function without any corresponding images in two domains. We enable this learning capability by combining a weight-sharing constraint and an adversarial training objective. Through visualization results from various unsupervised image translation tasks, we verify the effectiveness of the proposed framework. An ablation study further reveals the critical design choices. Moreover, we apply the UNIT framework to the unsupervised domain adaptation task and achieve better results than competing algorithms do in benchmark datasets. version:1
arxiv-1703-00847 | Exact Topology Reconstruction of Radial Dynamical Systems with Applications to Distribution System of the Power Grid | http://arxiv.org/abs/1703.00847 | id:1703.00847 author:Saurav Talukdar, Deepjyoti Deka, Donatello Materassi, Murti V. Salapaka category:cs.LG cs.SY  published:2017-03-02 summary:In this article we present a method to reconstruct the interconnectedness of dynamically related stochastic processes, where the interactions are bi-directional and the underlying topology is a tree. Our approach is based on multivariate Wiener filtering which recovers spurious edges apart from the true edges in the topology reconstruction. The main contribution of this work is to show that all spurious links obtained using Wiener filtering can be eliminated if the underlying topology is a tree based on which we present a three stage network reconstruction procedure for trees. We illustrate the effectiveness of the method developed by applying it on a typical distribution system of the electric grid. version:1
arxiv-1703-00845 | Towards CNN Map Compression for camera relocalisation | http://arxiv.org/abs/1703.00845 | id:1703.00845 author:Luis Contreras, Walterio Mayol-Cuevas category:cs.CV  published:2017-03-02 summary:This paper presents a study on the use of Convolutional Neural Networks for camera relocalisation and its application to map compression. We follow state of the art visual relocalisation results and evaluate response to different data inputs -- namely, depth, grayscale, RGB, spatial position and combinations of these. We use a CNN map representation and introduce the notion of CNN map compression by using a smaller CNN architecture. We evaluate our proposal in a series of publicly available datasets. This formulation allows us to improve relocalisation accuracy by increasing the number of training trajectories while maintaining a constant-size CNN. version:1
arxiv-1703-00839 | Encrypted accelerated least squares regression | http://arxiv.org/abs/1703.00839 | id:1703.00839 author:Pedro M. Esperança, Louis J. M. Aslett, Chris C. Holmes category:stat.ML cs.LG  published:2017-03-02 summary:Information that is stored in an encrypted format is, by definition, usually not amenable to statistical analysis or machine learning methods. In this paper we present detailed analysis of coordinate and accelerated gradient descent algorithms which are capable of fitting least squares and penalised ridge regression models, using data encrypted under a fully homomorphic encryption scheme. Gradient descent is shown to dominate in terms of encrypted computational speed, and theoretical results are proven to give parameter bounds which ensure correctness of decryption. The characteristics of encrypted computation are empirically shown to favour a non-standard acceleration technique. This demonstrates the possibility of approximating conventional statistical regression methods using encrypted data without compromising privacy. version:1
arxiv-1703-00837 | Meta Networks | http://arxiv.org/abs/1703.00837 | id:1703.00837 author:Tsendsuren Munkhdalai, Hong Yu category:cs.LG stat.ML  published:2017-03-02 summary:Deep neural networks have been successfully applied in applications with a large amount of labeled data. However, there are major drawbacks of the neural networks that are related to rapid generalization with small data and continual learning of new concepts without forgetting. We present a novel meta learning method, Meta Networks (MetaNet), that acquires a meta-level knowledge across tasks and shifts its inductive bias via fast parameterization for the rapid generalization. When tested on the standard one-shot learning benchmarks, our MetaNet models achieved near human-level accuracy. We demonstrated several appealing properties of MetaNet relating to generalization and continual learning. version:1
arxiv-1703-00830 | General and Robust Communication-Efficient Algorithms for Distributed Clustering | http://arxiv.org/abs/1703.00830 | id:1703.00830 author:Pranjal Awasthi, Maria-Florina Balcan, Colin White category:cs.DS cs.DC cs.LG  published:2017-03-02 summary:As datasets become larger and more distributed, algorithms for distributed clustering have become more and more important. In this work, we present a general framework for designing distributed clustering algorithms that are robust to outliers. Using our framework, we give a distributed approximation algorithm for k-means, k-median, or generally any L_p objective, with z outliers and/or balance constraints, using O(m(k+z)(d+log n)) bits of communication, where m is the number of machines, n is the size of the point set, and d is the dimension. This generalizes and improves over previous work of Bateni et al. and Malkomes et al. As a special case, we achieve the first distributed algorithm for k-median with outliers, answering an open question posed by Malkomes et al. For distributed k-means clustering, we provide the first dimension-dependent communication complexity lower bound for finding the optimal clustering. This improves over the lower bound from Chen et al. which is dimension-agnostic. Furthermore, we give distributed clustering algorithms which return nearly optimal solutions, provided the data satisfies the approximation stability condition of Balcan et al. or the spectral stability condition of Kumar and Kannan. In certain clustering applications where a local clustering consistent among all machines is sufficient, we show that no communication is necessary if the data satisfies approximation stability. version:1
arxiv-1703-00797 | A Simple, Fast and Fully Automated Approach for Midline Shift Measurement on Brain Computed Tomography | http://arxiv.org/abs/1703.00797 | id:1703.00797 author:Huan-Chih Wang, Shih-Hao Ho, Furen Xiao, Jen-Hai Chou category:physics.med-ph cs.CV  published:2017-03-02 summary:Brain CT has become a standard imaging tool for emergent evaluation of brain condition, and measurement of midline shift (MLS) is one of the most important features to address for brain CT assessment. We present a simple method to estimate MLS and propose a new alternative parameter to MLS: the ratio of MLS over the maximal width of intracranial region (MLS/ICWMAX). Three neurosurgeons and our automated system were asked to measure MLS and MLS/ICWMAX in the same sets of axial CT images obtained from 41 patients admitted to ICU under neurosurgical service. A weighted midline (WML) was plotted based on individual pixel intensities, with higher weighted given to the darker portions. The MLS could then be measured as the distance between the WML and ideal midline (IML) near the foramen of Monro. The average processing time to output an automatic MLS measurement was around 10 seconds. Our automated system achieved an overall accuracy of 90.24% when the CT images were calibrated automatically, and performed better when the calibrations of head rotation were done manually (accuracy: 92.68%). MLS/ICWMAX and MLS both gave results in same confusion matrices and produced similar ROC curve results. We demonstrated a simple, fast and accurate automated system of MLS measurement and introduced a new parameter (MLS/ICWMAX) as a good alternative to MLS in terms of estimating the degree of brain deformation, especially when non-DICOM images (e.g. JPEG) are more easily accessed. version:1
arxiv-1703-00796 | Unsupervised Steganalysis Based on Artificial Training Sets | http://arxiv.org/abs/1703.00796 | id:1703.00796 author:Daniel Lerch-Hostalot, David Megías category:cs.MM cs.LG  published:2017-03-02 summary:In this paper, an unsupervised steganalysis method that combines artificial training setsand supervised classification is proposed. We provide a formal framework for unsupervisedclassification of stego and cover images in the typical situation of targeted steganalysis (i.e.,for a known algorithm and approximate embedding bit rate). We also present a completeset of experiments using 1) eight different image databases, 2) image features based on RichModels, and 3) three different embedding algorithms: Least Significant Bit (LSB) matching,Highly undetectable steganography (HUGO) and Wavelet Obtained Weights (WOW). Weshow that the experimental results outperform previous methods based on Rich Models inthe majority of the tested cases. At the same time, the proposed approach bypasses theproblem of Cover Source Mismatch -when the embedding algorithm and bit rate are known-, since it removes the need of a training database when we have a large enough testing set.Furthermore, we provide a generic proof of the proposed framework in the machine learningcontext. Hence, the results of this paper could be extended to other classification problemssimilar to steganalysis. version:1
arxiv-1703-00792 | Robust Spatial Filtering with Graph Convolutional Neural Networks | http://arxiv.org/abs/1703.00792 | id:1703.00792 author:Felipe Petroski Such, Shagan Sah, Miguel Dominguez, Suhas Pillai, Chao Zhang, Andrew Michael, Nathan Cahill, Raymond Ptucha category:cs.CV  published:2017-03-02 summary:Convolutional Neural Networks (CNNs) have recently led to incredible breakthroughs on a variety of pattern recognition problems. Banks of finite impulse response filters are learned on a hierarchy of layers, each contributing more abstract information than the previous layer. The simplicity and elegance of the convolutional filtering process makes them perfect for structured problems such as image, video, or voice, where vertices are homogeneous in the sense of number, location, and strength of neighbors. The vast majority of classification problems, for example in the pharmaceutical, homeland security, and financial domains are unstructured. As these problems are formulated into unstructured graphs, the heterogeneity of these problems, such as number of vertices, number of connections per vertex, and edge strength, cannot be tackled with standard convolutional techniques. We propose a novel neural learning framework that is capable of handling both homogeneous and heterogeneous data, while retaining the benefits of traditional CNN successes. Recently, researchers have proposed variations of CNNs that can handle graph data. In an effort to create learnable filter banks of graphs, these methods either induce constraints on the data or require considerable preprocessing. As opposed to defining filters as spectral multipliers applied to the eigenvectors of the graph Laplacian, our framework, which we term Graph-CNNs, defines filters as polynomials of functions of the graph adjacency matrix. Graph-CNNs can handle both heterogeneous and homogeneous graph data, including graphs having entirely different vertex or edge sets. We compare Graph-CNN to traditional CNNs using the CIFAR-10 and Imagenet image classification datasets. version:1
arxiv-1703-00788 | A Robust Adaptive Stochastic Gradient Method for Deep Learning | http://arxiv.org/abs/1703.00788 | id:1703.00788 author:Caglar Gulcehre, Jose Sotelo, Marcin Moczulski, Yoshua Bengio category:cs.LG  published:2017-03-02 summary:Stochastic gradient algorithms are the main focus of large-scale optimization problems and led to important successes in the recent advancement of the deep learning algorithms. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose an adaptive learning rate algorithm, which utilizes stochastic curvature information of the loss function for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms. version:1
arxiv-1703-00787 | Linearly constrained Gaussian processes | http://arxiv.org/abs/1703.00787 | id:1703.00787 author:Carl Jidling, Niklas Wahlström, Adrian Wills, Thomas B. Schön category:stat.ML  published:2017-03-02 summary:We consider a modification of the covariance function in Gaussian processes to correctly account for known linear constraints. By modelling the target function as a transformation of an underlying function, the constraints are explicitly incorporated in the model such that they are guaranteed to be fulfilled by any sample drawn or prediction made. We also propose a constructive procedure for designing the transformation operator and illustrate the result on both simulated and real-data examples. version:1
arxiv-1703-00786 | A Generic Online Parallel Learning Framework for Large Margin Models | http://arxiv.org/abs/1703.00786 | id:1703.00786 author:Shuming Ma, Xu Sun category:cs.CL cs.LG  published:2017-03-02 summary:To speed up the training process, many existing systems use parallel technology for online learning algorithms. However, most research mainly focus on stochastic gradient descent (SGD) instead of other algorithms. We propose a generic online parallel learning framework for large margin models, and also analyze our framework on popular large margin algorithms, including MIRA and Structured Perceptron. Our framework is lock-free and easy to implement on existing systems. Experiments show that systems with our framework can gain near linear speed up by increasing running threads, and with no loss in accuracy. version:1
arxiv-1703-00782 | Lock-Free Parallel Perceptron for Graph-based Dependency Parsing | http://arxiv.org/abs/1703.00782 | id:1703.00782 author:Xu Sun, Shuming Ma category:cs.CL  published:2017-03-02 summary:Dependency parsing is an important NLP task. A popular approach for dependency parsing is structured perceptron. Still, graph-based dependency parsing has the time complexity of $O(n^3)$, and it suffers from slow training. To deal with this problem, we propose a parallel algorithm called parallel perceptron. The parallel algorithm can make full use of a multi-core computer which saves a lot of training time. Based on experiments we observe that dependency parsing with parallel perceptron can achieve 8-fold faster training speed than traditional structured perceptron methods when using 10 threads, and with no loss at all in accuracy. version:1
arxiv-1703-00757 | Predicting Rankings of Software Verification Competitions | http://arxiv.org/abs/1703.00757 | id:1703.00757 author:Mike Czech, Eyke Hüllermeier, Marie-Christine Jakobs, Heike Wehrheim category:cs.LG cs.SE I.2.6; D.2.4  published:2017-03-02 summary:Software verification competitions, such as the annual SV-COMP, evaluate software verification tools with respect to their effectivity and efficiency. Typically, the outcome of a competition is a (possibly category-specific) ranking of the tools. For many applications, such as building portfolio solvers, it would be desirable to have an idea of the (relative) performance of verification tools on a given verification task beforehand, i.e., prior to actually running all tools on the task. In this paper, we present a machine learning approach to predicting rankings of tools on verification tasks. The method builds upon so-called label ranking algorithms, which we complement with appropriate kernels providing a similarity measure for verification tasks. Our kernels employ a graph representation for software source code that mixes elements of control flow and program dependence graphs with abstract syntax trees. Using data sets from SV-COMP, we demonstrate our rank prediction technique to generalize well and achieve a rather high predictive accuracy. In particular, our method outperforms a recently proposed feature-based approach of Demyanova et al. (when applied to rank predictions). version:1
arxiv-1703-00737 | Wireless Interference Identification with Convolutional Neural Networks | http://arxiv.org/abs/1703.00737 | id:1703.00737 author:Malte Schmidt, Dimitri Block, Uwe Meier category:cs.LG cs.CV  published:2017-03-02 summary:The steadily growing use of license-free frequency bands requires reliable coexistence management for deterministic medium utilization. For interference mitigation, proper wireless interference identification (WII) is essential. In this work we propose the first WII approach based upon deep convolutional neural networks (CNNs). The CNN naively learns its features through self-optimization during an extensive data-driven GPU-based training process. We propose a CNN example which is based upon sensing snapshots with a limited duration of 12.8 {\mu}s and an acquisition bandwidth of 10 MHz. The CNN differs between 15 classes. They represent packet transmissions of IEEE 802.11 b/g, IEEE 802.15.4 and IEEE 802.15.1 with overlapping frequency channels within the 2.4 GHz ISM band. We show that the CNN outperforms state-of-the-art WII approaches and has a classification accuracy greater than 95% for signal-to-noise ratio of at least -5 dB. version:1
arxiv-1703-00734 | Distributed Bayesian Matrix Factorization with Minimal Communication | http://arxiv.org/abs/1703.00734 | id:1703.00734 author:Xiangju Qin, Paul Blomstedt, Eemeli Leppäaho, Pekka Parviainen, Samuel Kaski category:stat.ML cs.DC cs.LG cs.NA stat.ME  published:2017-03-02 summary:Bayesian matrix factorization (BMF) is a powerful tool for producing low-rank representations of matrices, and giving principled predictions of missing values. However, scaling up MCMC samplers to large matrices has proven to be difficult with parallel algorithms that require communication between MCMC iterations. On the other hand, designing communication-free algorithms is challenging due to the inherent unidentifiability of BMF solutions. We propose posterior propagation, an embarrassingly parallel inference procedure, which hierarchically introduces dependencies between data subsets and thus alleviates the unidentifiability problem. version:1
arxiv-1703-00729 | Mixing Complexity and its Applications to Neural Networks | http://arxiv.org/abs/1703.00729 | id:1703.00729 author:Michal Moshkovitz, Naftali Tishby category:cs.LG  published:2017-03-02 summary:We suggest analyzing neural networks through the prism of space constraints. We observe that most training algorithms applied in practice use bounded memory, which enables us to use a new notion introduced in the study of space-time tradeoffs that we call mixing complexity. This notion was devised in order to measure the (in)ability to learn using a bounded-memory algorithm. In this paper we describe how we use mixing complexity to obtain new results on what can and cannot be learned using neural networks. version:1
arxiv-1702-04517 | Application of Multi-channel 3D-cube Successive Convolution Network for Convective Storm Nowcasting | http://arxiv.org/abs/1702.04517 | id:1702.04517 author:Wei Zhang, Lei Han, Juanzhen Sun, Hanyang Guo, Jie Dai category:cs.CV  published:2017-02-15 summary:Convective storm nowcasting has attracted substantial attention in various fields. Existing methods under a deep learning framework rely primarily on radar data. Although they perform nowcast storm advection well, it is still challenging to nowcast storm initiation and growth, due to the limitations of the radar observations. This paper describes the first attempt to nowcast storm initiation, growth, and advection simultaneously under a deep learning framework using multi-source meteorological data. To this end, we present a multi-channel 3D-cube successive convolution network (3D-SCN). As real-time re-analysis meteorological data can now provide valuable atmospheric boundary layer thermal dynamic information, which is essential to predict storm initiation and growth, both raw 3D radar and re-analysis data are used directly without any handcraft feature engineering. These data are formulated as multi-channel 3D cubes, to be fed into our network, which are convolved by cross-channel 3D convolutions. By stacking successive convolutional layers without pooling, we build an end-to-end trainable model for nowcasting. Experimental results show that deep learning methods achieve better performance than traditional extrapolation methods. The qualitative analyses of 3D-SCN show encouraging results of nowcasting of storm initiation, growth, and advection. version:2
arxiv-1703-02089 | The variational Laplace approach to approximate Bayesian inference | http://arxiv.org/abs/1703.02089 | id:1703.02089 author:Jean Daunizeau category:stat.ME q-bio.NC stat.ML  published:2017-03-02 summary:Variational approaches to approximate Bayesian inference provide very efficient means of performing parameter estimation and model selection. Among these, so-called variational-Laplace or VL schemes rely on Gaussian approximations to posterior densities on model parameters. In this note, we review the main variants of VL approaches, that follow from considering nonlinear models of continuous and/or categorical data. En passant, we also derive a few novel theoretical results that complete the portfolio of existing analyses of variational Bayesian approaches, including investigations of their asymptotic convergence. We also suggest practical ways of extending existing VL approaches to hierarchical generative models that include (e.g., precision) hyperparameters. version:1
arxiv-1703-00371 | ste-GAN-ography: Generating Steganographic Images via Adversarial Training | http://arxiv.org/abs/1703.00371 | id:1703.00371 author:Jamie Hayes, George Danezis category:stat.ML cs.CR cs.MM  published:2017-03-01 summary:Adversarial training was recently shown to be competitive against supervised learning methods on computer vision tasks, however, studies have mainly been confined to generative tasks such as image synthesis. In this paper, we apply adversarial training techniques to the discriminative task of learning a steganographic algorithm. Steganography is a collection of techniques for concealing information by embedding it within a non-secret medium, such as cover texts or images. We show that adversarial training can produce robust steganographic techniques: our unsupervised training scheme produces a steganographic algorithm that competes with state-of-the-art steganographic techniques, and produces a robust steganalyzer, which performs the discriminative task of deciding if an image contains secret information. We define a game between three parties, Alice, Bob and Eve, in order to simultaneously train both a steganographic algorithm and a steganalyzer. Alice and Bob attempt to communicate a secret message contained within an image, while Eve eavesdrops on their conversation and attempts to determine if secret information is embedded within the image. We represent Alice, Bob and Eve by neural networks, and validate our scheme on two independent image datasets, showing our novel method of studying steganographic problems is surprisingly competitive against established steganographic techniques. version:2
arxiv-1703-00686 | BoxCars: Improving Vehicle Fine-Grained Recognition using 3D Bounding Boxes in Traffic Surveillance | http://arxiv.org/abs/1703.00686 | id:1703.00686 author:Jakub Sochor, Jakub Špaňhel, Adam Herout category:cs.CV  published:2017-03-02 summary:In this paper, we focus on fine-grained recognition of vehicles mainly in traffic surveillance applications. We propose an approach orthogonal to recent advancement in fine-grained recognition (automatic part discovery, bilinear pooling). Also, in contrast to other methods focused on fine-grained recognition of vehicles, we do not limit ourselves to frontal/rear viewpoint but allow the vehicles to be seen from any viewpoint. Our approach is based on 3D bounding boxes built around the vehicles. The bounding box can be automatically constructed from traffic surveillance data. For scenarios where it is not possible to use the precise construction, we propose a method for estimation of the 3D bounding box. The 3D bounding box is used to normalize the image viewpoint by unpacking the image into plane. We also propose to randomly alter the color of the image and add a rectangle with random noise to random position in the image during training Convolutional Neural Networks. We have collected a large fine-grained vehicle dataset BoxCars116k, with 116k images of vehicles from various viewpoints taken by numerous surveillance cameras. We performed a number of experiments which show that our proposed method significantly improves CNN classification accuracy (the accuracy is increased by up to 12 percent points and the error is reduced by up to 50% compared to CNNs without the proposed modifications). We also show that our method outperforms state-of-the-art methods for fine-grained recognition. version:1
arxiv-1702-06186 | Survey of reasoning using Neural networks | http://arxiv.org/abs/1702.06186 | id:1702.06186 author:Amit Sahu category:cs.LG cs.AI cs.NE  published:2017-02-14 summary:Reason and inference require process as well as memory skills by humans. Neural networks are able to process tasks like image recognition (better than humans) but in memory aspects are still limited (by attention mechanism, size). Recurrent Neural Network (RNN) and it's modified version LSTM are able to solve small memory contexts, but as context becomes larger than a threshold, it is difficult to use them. The Solution is to use large external memory. Still, it poses many challenges like, how to train neural networks for discrete memory representation, how to describe long term dependencies in sequential data etc. Most prominent neural architectures for such tasks are Memory networks: inference components combined with long term memory and Neural Turing Machines: neural networks using external memory resources. Also, additional techniques like attention mechanism, end to end gradient descent on discrete memory representation are needed to support these solutions. Preliminary results of above neural architectures on simple algorithms (sorting, copying) and Question Answering (based on story, dialogs) application are comparable with the state of the art. In this paper, I explain these architectures (in general), the additional techniques used and the results of their application. version:2
arxiv-1703-00311 | Multi-stage Neural Networks with Single-sided Classifiers for False Positive Reduction and its Evaluation using Lung X-ray CT Images | http://arxiv.org/abs/1703.00311 | id:1703.00311 author:Masaharu Sakamoto, Hiroki Nakano, Kun Zhao, Taro Sekiyama category:cs.CV  published:2017-03-01 summary:Lung nodule classification is a class imbalanced problem because nodules are found with much lower frequency than non-nodules. In the class imbalanced problem, conventional classifiers tend to be overwhelmed by the majority class and ignore the minority class. We therefore propose cascaded convolutional neural networks to cope with the class imbalanced problem. In the proposed approach, multi-stage convolutional neural networks that perform as single-sided classifiers filter out obvious non-nodules. Successively, a convolutional neural network trained with a balanced data set calculates nodule probabilities. The proposed method achieved the sensitivity of 92.4\% and 94.5% at 4 and 8 false positives per scan in Free Receiver Operating Characteristics (FROC) curve analysis, respectively. version:2
arxiv-1703-00674 | Adaptive Matching for Expert Systems with Uncertain Task Types | http://arxiv.org/abs/1703.00674 | id:1703.00674 author:Virag Shah, Lennart Gulikers, Laurent Massoulie, Milan Vojnovic category:cs.AI cs.LG stat.ML  published:2017-03-02 summary:Online two-sided matching markets such as Q&A forums (e.g. StackOverflow, Quora) and online labour platforms (e.g. Upwork) critically rely on the ability to propose adequate matches based on imperfect knowledge of the two parties to be matched. This prompts the following question: Which matching recommendation algorithms can, in the presence of such uncertainty, lead to efficient platform operation? To answer this question, we develop a model of a task / server matching system. For this model, we give a necessary and sufficient condition for an incoming stream of tasks to be manageable by the system. We further identify a so-called back-pressure policy under which the throughput that the system can handle is optimized. We show that this policy achieves strictly larger throughput than a natural greedy policy. Finally, we validate our model and confirm our theoretical findings with experiments based on logs of Math.StackExchange, a StackOverflow forum dedicated to mathematics. version:1
arxiv-1703-00312 | Perturb-and-MPM: Quantifying Segmentation Uncertainty in Dense Multi-Label CRFs | http://arxiv.org/abs/1703.00312 | id:1703.00312 author:Raphael Meier, Urspeter Knecht, Alain Jungo, Roland Wiest, Mauricio Reyes category:cs.CV  published:2017-03-01 summary:This paper proposes a novel approach for uncertainty quantification in dense Conditional Random Fields (CRFs). The presented approach, called Perturb-and-MPM, enables efficient, approximate sampling from dense multi-label CRFs via random perturbations. An analytic error analysis was performed which identified the main cause of approximation error as well as showed that the error is bounded. Spatial uncertainty maps can be derived from the Perturb-and-MPM model, which can be used to visualize uncertainty in image segmentation results. The method is validated on synthetic and clinical Magnetic Resonance Imaging data. The effectiveness of the approach is demonstrated on the challenging problem of segmenting the tumor core in glioblastoma. We found that areas of high uncertainty correspond well to wrongly segmented image regions. Furthermore, we demonstrate the potential use of uncertainty maps to refine imaging biomarkers in the case of extent of resection and residual tumor volume in brain tumor patients. version:2
arxiv-1703-04391 | Extrinsic Calibration of 3D Range Finder and Camera without Auxiliary Object or Human Intervention | http://arxiv.org/abs/1703.04391 | id:1703.04391 author:Qinghai Liao, Ming Liu, Lei Tai, Haoyang Ye category:cs.CV  published:2017-03-02 summary:Fusion of heterogeneous extroceptive sensors is the most effient and effective way to representing the environment precisely, as it overcomes various defects of each homogeneous sensor. The rigid transformation (aka. extrinsic parameters) of heterogeneous sensory systems should be available before precisely fusing the multisensor information. Researchers have proposed several approaches to estimating the extrinsic parameters. These approaches require either auxiliary objects, like chessboards, or extra help from human to select correspondences. In this paper, we proposed a novel extrinsic calibration approach for the extrinsic calibration of range and image sensors. As far as we know, it is the first automatic approach with no requirement of auxiliary objects or any human interventions. First, we estimate the initial extrinsic parameters from the individual motion of the range finder and the camera. Then we extract lines in the image and point-cloud pairs, to refine the line feature associations by the initial extrinsic parameters. At the end, we discussed the degenerate case which may lead to the algorithm failure and validate our approach by simulation. The results indicate high-precision extrinsic calibration results against the ground-truth. version:1
arxiv-1703-00663 | Introduction to Nonnegative Matrix Factorization | http://arxiv.org/abs/1703.00663 | id:1703.00663 author:Nicolas Gillis category:cs.NA cs.CV cs.LG math.OC stat.ML  published:2017-03-02 summary:In this paper, we introduce and provide a short overview of nonnegative matrix factorization (NMF). Several aspects of NMF are discussed, namely, the application in hyperspectral imaging, geometry and uniqueness of NMF solutions, complexity, algorithms, and its link with extended formulations of polyhedra. In order to put NMF into perspective, the more general problem class of constrained low-rank matrix approximation problems is first briefly introduced. version:1
arxiv-1703-00420 | Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation | http://arxiv.org/abs/1703.00420 | id:1703.00420 author:Lei Tai, Giuseppe Paolo, Ming Liu category:cs.RO cs.AI cs.LG  published:2017-03-01 summary:Deep Reinforcement Learning has been successful in various virtual tasks, but it is still rarely used in real world applications especially for continuous control of mobile robots navigation. In this paper, we present a learning-based mapless motion planner by taking the sparse 10-dimensional range findings and the target position with respect to the mobile robot coordinate frame as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the obstacle map of the navigation environment where both the highly precise laser sensor and the obstacle map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. We also evaluated this learning-based motion planner and compared it with the traditional motion planning method, both in virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles. version:2
arxiv-1703-00645 | TumorNet: Lung Nodule Characterization Using Multi-View Convolutional Neural Network with Gaussian Process | http://arxiv.org/abs/1703.00645 | id:1703.00645 author:Sarfaraz Hussein, Robert Gillies, Kunlin Cao, Qi Song, Ulas Bagci category:cs.CV  published:2017-03-02 summary:Characterization of lung nodules as benign or malignant is one of the most important tasks in lung cancer diagnosis, staging and treatment planning. While the variation in the appearance of the nodules remains large, there is a need for a fast and robust computer aided system. In this work, we propose an end-to-end trainable multi-view deep Convolutional Neural Network (CNN) for nodule characterization. First, we use median intensity projection to obtain a 2D patch corresponding to each dimension. The three images are then concatenated to form a tensor, where the images serve as different channels of the input image. In order to increase the number of training samples, we perform data augmentation by scaling, rotating and adding noise to the input image. The trained network is used to extract features from the input image followed by a Gaussian Process (GP) regression to obtain the malignancy score. We also empirically establish the significance of different high level nodule attributes such as calcification, sphericity and others for malignancy determination. These attributes are found to be complementary to the deep multi-view CNN features and a significant improvement over other methods is obtained. version:1
arxiv-1702-06683 | Using Deep Learning and Google Street View to Estimate the Demographic Makeup of the US | http://arxiv.org/abs/1702.06683 | id:1702.06683 author:Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei category:cs.CV  published:2017-02-22 summary:The United States spends more than $1B each year on initiatives such as the American Community Survey (ACS), a labor-intensive door-to-door study that measures statistics relating to race, gender, education, occupation, unemployment, and other demographic factors. Although a comprehensive source of data, the lag between demographic changes and their appearance in the ACS can exceed half a decade. As digital imagery becomes ubiquitous and machine vision techniques improve, automated data analysis may provide a cheaper and faster alternative. Here, we present a method that determines socioeconomic trends from 50 million images of street scenes, gathered in 200 American cities by Google Street View cars. Using deep learning-based computer vision techniques, we determined the make, model, and year of all motor vehicles encountered in particular neighborhoods. Data from this census of motor vehicles, which enumerated 22M automobiles in total (8% of all automobiles in the US), was used to accurately estimate income, race, education, and voting patterns, with single-precinct resolution. (The average US precinct contains approximately 1000 people.) The resulting associations are surprisingly simple and powerful. For instance, if the number of sedans encountered during a 15-minute drive through a city is higher than the number of pickup trucks, the city is likely to vote for a Democrat during the next Presidential election (88% chance); otherwise, it is likely to vote Republican (82%). Our results suggest that automated systems for monitoring demographic trends may effectively complement labor-intensive approaches, with the potential to detect trends with fine spatial resolution, in close to real time. version:2
arxiv-1703-00617 | In Search of an Entity Resolution OASIS: Optimal Asymptotic Sequential Importance Sampling | http://arxiv.org/abs/1703.00617 | id:1703.00617 author:Neil G. Marchant, Benjamin I. P. Rubinstein category:cs.LG cs.DB stat.ML  published:2017-03-02 summary:Entity resolution (ER) presents unique challenges for evaluation methodology. While crowd sourcing provides a platform to acquire ground truth, sound approaches to sampling must drive labelling efforts. In ER, extreme class imbalance between matching and non-matching records can lead to enormous labelling requirements when seeking statistically consistent estimates of population parameters. This paper addresses this important challenge with the OASIS algorithm. OASIS draws samples from a (biased) instrumental distribution, chosen to have optimal asymptotic variance. As new labels are collected OASIS updates this instrumental distribution via a Bayesian latent variable model of the annotator oracle, to quickly focus on regions providing more information. We prove that resulting estimates of F-measure, precision, recall converge to the true population values. Thorough comparisons of sampling methods on a variety of ER datasets demonstrate significant labelling reductions of up to 75% without loss to estimate accuracy. version:1
arxiv-1703-00607 | Discovery of Evolving Semantics through Dynamic Word Embedding Learning | http://arxiv.org/abs/1703.00607 | id:1703.00607 author:Zijun Yao, Yifan Sun, Weicong Ding, Nikhil Rao, Hui Xiong category:cs.CL stat.ML  published:2017-03-02 summary:During the course of human language evolution, the semantic meanings of words keep evolving with time. The understanding of evolving semantics enables us to capture the true meaning of the words in different usage contexts, and thus is critical for various applications, such as machine translation. While it is naturally promising to study word semantics in a time-aware manner, traditional methods to learn word vector representation do not adequately capture the change over time. To this end, in this paper, we aim at learning time-aware vector representation of words through dynamic word embedding modeling. Specifically, we first propose a method that captures time-specific semantics and across-time alignment simultaneously in a way that is robust to data sparsity. Then, we solve the resulting optimization problem using a scalable coordinate descent method. Finally, we perform the empirical study on New York Times data to learn the temporal embeddings and develop multiple evaluations that illustrate the semantic evolution of words, discovered from news media. Moreover, our qualitative and quantitative tests indicate that the our method not only reliably captures the semantic evolution over time, but also onsistently outperforms state-of-the-art temporal embedding approaches on both semantic accuracy and alignment quality. version:1
arxiv-1703-00593 | Positive-Unlabeled Learning with Non-Negative Risk Estimator | http://arxiv.org/abs/1703.00593 | id:1703.00593 author:Ryuichi Kiryo, Gang Niu, Marthinus C. du Plessis, Masashi Sugiyama category:cs.LG stat.ML  published:2017-03-02 summary:From only positive (P) and unlabeled (U) data, a binary classifier could be trained with PU learning. Unbiased PU learning that is based on unbiased risk estimators is now state of the art. However, if its model is very flexible, its empirical risk on training data will go negative, and we will suffer from overfitting seriously. In this paper, we propose a novel non-negative risk estimator for PU learning. When being minimized, it is more robust against overfitting, and thus we are able to train very flexible models given limited P data. Moreover, we analyze the bias, consistency and mean-squared-error reduction of the proposed risk estimator as well as the estimation error of the corresponding risk minimizer. Experiments show that the non-negative risk estimator outperforms unbiased counterparts when they disagree. version:1
arxiv-1703-00586 | A novel image tag completion method based on convolutional neural network | http://arxiv.org/abs/1703.00586 | id:1703.00586 author:Yanyan Geng, Weizhi Li, Gaoyuan Liang, Jingbin Wang, Yanbin Wu, Nitin Patil, Jing-Yan Wang category:cs.CV  published:2017-03-02 summary:In the problems of image retrieval and annotation, complete textual tag lists of images play critical roles. However, in real-world applications, the image tags are usually incomplete, thus it is important to learn the complete tags for images. In this paper, we study the problem of image tag complete and proposed a novel method for this problem based on a popular image representation method, convolutional neural network (CNN). The method estimates the complete tags from the CNN representations of images based on a linear predictor. The CNN parameters, linear predictor, and the complete tags are learned jointly by our method. We build a minimization problem to encourage the consistency between the complete tags and the available incomplete tags, reduce the estimation error, and reduce the model complexity. An iterative algorithm is developed to solve the minimization problem. Experiments over benchmark image data sets show its effectiveness. version:1
arxiv-1702-06230 | Beating the World's Best at Super Smash Bros. with Deep Reinforcement Learning | http://arxiv.org/abs/1702.06230 | id:1702.06230 author:Vlad Firoiu, William F. Whitney, Joshua B. Tenenbaum category:cs.LG cs.AI I.2.6  published:2017-02-21 summary:There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting. version:2
arxiv-1703-00579 | Active Learning for Accurate Estimation of Linear Models | http://arxiv.org/abs/1703.00579 | id:1703.00579 author:Carlos Riquelme, Mohammad Ghavamzadeh, Alessandro Lazaric category:stat.ML cs.LG  published:2017-03-02 summary:We explore the sequential decision making problem where the goal is to estimate uniformly well a number of linear models, given a shared budget of random contexts independently sampled from a known distribution. The decision maker must query one of the linear models for each incoming context, and receives an observation corrupted by noise levels that are unknown, and depend on the model instance. We present Trace-UCB, an adaptive allocation algorithm that learns the noise levels while balancing contexts accordingly across the different linear functions, and derive guarantees for simple regret in both expectation and high-probability. Finally, we extend the algorithm and its guarantees to high dimensional settings, where the number of linear models times the dimension of the contextual space is higher than the total budget of samples. Simulations with real data suggest that Trace-UCB is remarkably robust, outperforming a number of baselines even when its assumptions are violated. version:1
arxiv-1703-00577 | Skin Lesion Analysis Towards Melanoma Detection Using Deep Learning Network | http://arxiv.org/abs/1703.00577 | id:1703.00577 author:Yuexiang Li, Linlin Shen category:cs.CV  published:2017-03-02 summary:Skin lesion is a severe disease in world-wide extent. Reliable automatic detection of skin tumors is very useful to help increase the accuracy and efficiency of pathologists. International Skin Imaging Collaboration (ISIC) is a challenge focusing on the automatic analysis of skin lesion. In this brief paper, we introduce two deep learning methods to address all the three tasks announced in ISIC 2017, i.e. lesion segmentation (task 1), lesion dermoscopic feature extraction (task 2) and lesion classification (task 3). A fully-convolutional network is proposed to simultaneously address the tasks of lesion segmentation and classification and a straight-forward CNN is proposed for the dermoscopic feature extraction task. Experimental results on the validation set show promising accuracies, i.e. 75.1% for task 1, 84.4% for task 2 and 90.8% for task 3 were achieved. version:1
arxiv-1703-00572 | Structural Embedding of Syntactic Trees for Machine Comprehension | http://arxiv.org/abs/1703.00572 | id:1703.00572 author:Rui Liu, Junjie Hu, Wei Wei, Zi Yang, Eric Nyberg category:cs.CL  published:2017-03-02 summary:This paper develops a model that addresses syntactic embedding for machine comprehension, a key task of natural language understanding. Our proposed model, structural embedding of syntactic trees (SEST), takes each word in a sentence, constructs a sequence of syntactic nodes extracted from syntactic parse trees, and encodes the sequence into a vector representation. The learned vector is then incorporated into neural attention models, which allows learning the mapping of syntactic structures between question and context pairs. We evaluate our approach on SQuAD dataset and demonstrate that our model can accurately identify the syntactic boundaries of the sentences and to extract answers that are syntactically coherent over the baseline methods. version:1
arxiv-1703-00564 | MoleculeNet: A Benchmark for Molecular Machine Learning | http://arxiv.org/abs/1703.00564 | id:1703.00564 author:Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande category:cs.LG physics.chem-ph stat.ML  published:2017-03-02 summary:Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations, and in particular graph convolutional networks, are powerful tools for molecular machine learning and broadly offer the best performance. However, for quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be significantly more important than choice of particular learning algorithm. version:1
arxiv-1703-00561 | Signal-based Bayesian Seismic Monitoring | http://arxiv.org/abs/1703.00561 | id:1703.00561 author:David A. Moore, Stuart J. Russell category:cs.LG physics.geo-ph  published:2017-03-02 summary:Detecting weak seismic events from noisy sensors is a difficult perceptual task. We formulate this task as Bayesian inference and propose a generative model of seismic events and signals across a network of spatially distributed stations. Our system, SIGVISA, is the first to directly model seismic waveforms, allowing it to incorporate a rich representation of the physics underlying the signal generation process. We use Gaussian processes over wavelet parameters to predict detailed waveform fluctuations based on historical events, while degrading smoothly to simple parametric envelopes in regions with no historical seismicity. Evaluating on data from the western US, we recover three times as many events as previous work, and reduce mean location errors by a factor of four while greatly increasing sensitivity to low-magnitude events. version:1
arxiv-1703-00560 | An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis | http://arxiv.org/abs/1703.00560 | id:1703.00560 author:Yuandong Tian category:cs.LG  published:2017-03-02 summary:In this paper, we explore theoretical properties of training a two-layered ReLU network $g(\mathbf{x}; \mathbf{w}) = \sum_{j=1}^K \sigma(\mathbf{w}_j^T\mathbf{x})$ with centered $d$-dimensional spherical Gaussian input $\mathbf{x}$ ($\sigma$=ReLU). We train our network with gradient descent on $\mathbf{w}$ to mimic the output of a teacher network with the same architecture and fixed parameters $\mathbf{w}^*$. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters ("out-of-plane") are not isolated and form manifolds, and characterize in-plane critical-point-free regions for two ReLU case. On the other hand, convergence to $\mathbf{w}^*$ for one ReLU node is guaranteed with at least $(1-\epsilon)/2$ probability, if weights are initialized randomly with standard deviation upper-bounded by $O(\epsilon/\sqrt{d})$, consistent with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards $\mathbf{w}^*$ (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings. version:1
arxiv-1703-00557 | Diffusion Independent Semi-Bandit Influence Maximization | http://arxiv.org/abs/1703.00557 | id:1703.00557 author:Sharan Vaswani, Branislav Kveton, Zheng Wen, Mohammad Ghavamzadeh, Laks Lakshmanan, Mark Schmidt category:cs.LG  published:2017-03-01 summary:We consider \emph{influence maximization} (IM) in social networks, which is the problem of maximizing the number of users that become aware of a product by selecting a set of "seed" users to expose the product to. While prior work assumes a known model of information diffusion, we propose a parametrization in terms of pairwise reachability which makes our framework agnostic to the underlying diffusion model. We give a corresponding monotone, submodular surrogate function, and show that it is a good approximation to the original IM objective. We also consider the case of a new marketer looking to exploit an existing social network, while simultaneously learning the factors governing information propagation. For this, we propose a pairwise-influence semi-bandit feedback model and develop a LinUCB-based bandit algorithm. Our model-independent regret analysis shows that our bound on the cumulative regret has a better (as compared to previous work) dependence on the size of the network. By using the graph Laplacian eigenbasis to construct features, we describe a practical LinUCB implementation. Experimental evaluation suggests that our framework is robust to the underlying diffusion model and can efficiently learn a near-optimal solution. version:1
arxiv-1703-00555 | A Deep Cascade of Convolutional Neural Networks for MR Image Reconstruction | http://arxiv.org/abs/1703.00555 | id:1703.00555 author:Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony Price, Daniel Rueckert category:cs.CV  published:2017-03-01 summary:The acquisition of Magnetic Resonance Imaging (MRI) is inherently slow. Inspired by recent advances in deep learning, we propose a framework for reconstructing MR images from undersampled data using a deep cascade of convolutional neural networks to accelerate the data acquisition process. We show that for Cartesian undersampling of 2D cardiac MR images, the proposed method outperforms the state-of-the-art compressed sensing approaches, such as dictionary learning-based MRI (DLMRI) reconstruction, in terms of reconstruction error, perceptual quality and reconstruction speed for both 3-fold and 6-fold undersampling. Compared to DLMRI, the error produced by the method proposed is approximately twice as small, allowing to preserve anatomical structures more faithfully. Using our method, each image can be reconstructed in 23 ms, which is fast enough to enable real-time applications. version:1
arxiv-1703-00552 | Change Detection under Global Viewpoint Uncertainty | http://arxiv.org/abs/1703.00552 | id:1703.00552 author:Murase Tomoya, Tanaka Kanji category:cs.CV  published:2017-03-01 summary:This paper addresses the problem of change detection from a novel perspective of long-term map learning. We are particularly interested in designing an approach that can scale to large maps and that can function under global uncertainty in the viewpoint (i.e., GPS-denied situations). Our approach, which utilizes a compact bag-of-words (BoW) scene model, makes several contributions to the problem: 1) Two kinds of prior information are extracted from the view sequence map and used for change detection. Further, we propose a novel type of prior, called motion prior, to predict the relative motions of stationary objects and anomaly ego-motion detection. The proposed prior is also useful for distinguishing stationary from non-stationary objects. 2) A small set of good reference images (e.g., 10) are efficiently retrieved from the view sequence map by employing the recently developed Bag-of-Local-Convolutional-Features (BoLCF) scene model. 3) Change detection is reformulated as a scene retrieval over these reference images to find changed objects using a novel spatial Bag-of-Words (SBoW) scene model. Evaluations conducted of individual techniques and also their combinations on a challenging dataset of highly dynamic scenes in the publicly available Malaga dataset verify their efficacy. version:1
arxiv-1703-00551 | Label Refinement Network for Coarse-to-Fine Semantic Segmentation | http://arxiv.org/abs/1703.00551 | id:1703.00551 author:Md Amirul Islam, Shujon Naha, Mrigank Rochan, Neil Bruce, Yang Wang category:cs.CV  published:2017-03-01 summary:We consider the problem of semantic image segmentation using deep convolutional neural networks. We propose a novel network architecture called the label refinement network that predicts segmentation labels in a coarse-to-fine fashion at several resolutions. The segmentation labels at a coarse resolution are used together with convolutional features to obtain finer resolution segmentation labels. We define loss functions at several stages in the network to provide supervisions at different stages. Our experimental results on several standard datasets demonstrate that the proposed model provides an effective way of producing pixel-wise dense image labeling. version:1
arxiv-1703-00538 | Unsupervised Ensemble Ranking of Terms in Electronic Health Record Notes Based on Their Importance to Patients | http://arxiv.org/abs/1703.00538 | id:1703.00538 author:Jinying Chen, Hong Yu category:cs.CL I.2.7  published:2017-03-01 summary:Background: Electronic health record (EHR) notes contain abundant medical jargon that can be difficult for patients to comprehend. One way to help patients is to reduce information overload and help them focus on medical terms that matter most to them. Objective: The aim of this work was to develop FIT (Finding Important Terms for patients), an unsupervised natural language processing (NLP) system that ranks medical terms in EHR notes based on their importance to patients. Methods: We built FIT on a new unsupervised ensemble ranking model derived from the biased random walk algorithm to combine heterogeneous information resources for ranking candidate terms from each EHR note. Specifically, FIT integrates four single views for term importance: patient use of medical concepts, document-level term salience, word-occurrence based term relatedness, and topic coherence. It also incorporates partial information of term importance as conveyed by terms' unfamiliarity levels and semantic types. We evaluated FIT on 90 expert-annotated EHR notes and compared it with three benchmark unsupervised ensemble ranking methods. Results: FIT achieved 0.885 AUC-ROC for ranking candidate terms from EHR notes to identify important terms. When including term identification, the performance of FIT for identifying important terms from EHR notes was 0.813 AUC-ROC. It outperformed the three ensemble rankers for most metrics. Its performance is relatively insensitive to its parameter. Conclusions: FIT can automatically identify EHR terms important to patients and may help develop personalized interventions to improve quality of care. By using unsupervised learning as well as a robust and flexible framework for information fusion, FIT can be readily applied to other domains and applications. version:1
arxiv-1703-00534 | Skin cancer reorganization and classification with deep neural network | http://arxiv.org/abs/1703.00534 | id:1703.00534 author:Hao Chang category:cs.CV 68T20  published:2017-03-01 summary:As one kind of skin cancer, melanoma is very dangerous. Dermoscopy based early detection and recarbonization strategy is critical for melanoma therapy. However, well-trained dermatologists dominant the diagnostic accuracy. In order to solve this problem, many effort focus on developing automatic image analysis systems. Here we report a novel strategy based on deep learning technique, and achieve very high skin lesion segmentation and melanoma diagnosis accuracy: 1) we build a segmentation neural network (skin_segnn), which achieved very high lesion boundary detection accuracy; 2) We build another very deep neural network based on Google inception v3 network (skin_recnn) and its well-trained weight. The novel designed transfer learning based deep neural network skin_inceptions_v3_nn helps to achieve a high prediction accuracy. version:1
arxiv-1703-00523 | ISIC 2017 - Skin Lesion Analysis Towards Melanoma Detection | http://arxiv.org/abs/1703.00523 | id:1703.00523 author:Matt Berseth category:cs.CV  published:2017-03-01 summary:Our system addresses Part 1, Lesion Segmentation and Part 3, Lesion Classification of the ISIC 2017 challenge. Both algorithms make use of deep convolutional networks to achieve the challenge objective. version:1
arxiv-1703-00522 | Understanding Synthetic Gradients and Decoupled Neural Interfaces | http://arxiv.org/abs/1703.00522 | id:1703.00522 author:Wojciech Marian Czarnecki, Grzegorz Świrszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals, Koray Kavukcuoglu category:cs.LG cs.NE  published:2017-03-01 summary:When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison. version:1
arxiv-1703-00512 | PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison | http://arxiv.org/abs/1703.00512 | id:1703.00512 author:Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, Jason H. Moore category:cs.LG cs.AI  published:2017-03-01 summary:The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists. The present study introduces an accessible, curated, and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. This work is an important first step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future. version:1
arxiv-1703-00503 | Learning Social Affordance Grammar from Videos: Transferring Human Interactions to Human-Robot Interactions | http://arxiv.org/abs/1703.00503 | id:1703.00503 author:Tianmin Shu, Xiaofeng Gao, Michael S. Ryoo, Song-Chun Zhu category:cs.RO cs.AI cs.CV  published:2017-03-01 summary:In this paper, we present a general framework for learning social affordance grammar as a spatiotemporal AND-OR graph (ST-AOG) from RGB-D videos of human interactions, and transfer the grammar to humanoids to enable a real-time motion inference for human-robot interaction (HRI). Based on Gibbs sampling, our weakly supervised grammar learning can automatically construct a hierarchical representation of an interaction with long-term joint sub-tasks of both agents and short term atomic actions of individual agents. Based on a new RGB-D video dataset with rich instances of human interactions, our experiments of Baxter simulation, human evaluation, and real Baxter test demonstrate that the model learned from limited training data successfully generates human-like behaviors in unseen scenarios and outperforms both baselines. version:1
arxiv-1703-00495 | Making 360° Video Watchable in 2D: Learning Videography for Click Free Viewing | http://arxiv.org/abs/1703.00495 | id:1703.00495 author:Yu-Chuan Su, Kristen Grauman category:cs.CV  published:2017-03-01 summary:360{\deg} video requires human viewers to actively control "where" to look while watching the video. Although it provides a more immersive experience of the visual content, it also introduces additional burden for viewers; awkward interfaces to navigate the video lead to suboptimal viewing experiences. Virtual cinematography is an appealing direction to remedy these problems, but conventional methods are limited to virtual environments or rely on hand-crafted heuristics. We propose a new algorithm for virtual cinematography that automatically controls a virtual camera within a 360{\deg} video. Compared to the state of the art, our algorithm allows more general camera control, avoids redundant outputs, and extracts its output videos substantially more efficiently. Experimental results on over 7 hours of real "in the wild" video show that our generalized camera control is crucial for viewing 360{\deg} video, while the proposed efficient algorithm is essential for making the generalized control computationally tractable. version:1
arxiv-1703-00484 | Truth and Regret in Online Scheduling | http://arxiv.org/abs/1703.00484 | id:1703.00484 author:Shuchi Chawla, Nikhil Devanur, Janardhan Kulkarni, Rad Niazadeh category:cs.GT cs.AI cs.DS cs.LG  published:2017-03-01 summary:We consider a scheduling problem where a cloud service provider has multiple units of a resource available over time. Selfish clients submit jobs, each with an arrival time, deadline, length, and value. The service provider's goal is to implement a truthful online mechanism for scheduling jobs so as to maximize the social welfare of the schedule. Recent work shows that under a stochastic assumption on job arrivals, there is a single-parameter family of mechanisms that achieves near-optimal social welfare. We show that given any such family of near-optimal online mechanisms, there exists an online mechanism that in the worst case performs nearly as well as the best of the given mechanisms. Our mechanism is truthful whenever the mechanisms in the given family are truthful and prompt, and achieves optimal (within constant factors) regret. We model the problem of competing against a family of online scheduling mechanisms as one of learning from expert advice. A primary challenge is that any scheduling decisions we make affect not only the payoff at the current step, but also the resource availability and payoffs in future steps. Furthermore, switching from one algorithm (a.k.a. expert) to another in an online fashion is challenging both because it requires synchronization with the state of the latter algorithm as well as because it affects the incentive structure of the algorithms. We further show how to adapt our algorithm to a non-clairvoyant setting where job lengths are unknown until jobs are run to completion. Once again, in this setting, we obtain truthfulness along with asymptotically optimal regret (within poly-logarithmic factors). version:1
arxiv-1703-00472 | Reinforcement Learning for Pivoting Task | http://arxiv.org/abs/1703.00472 | id:1703.00472 author:Rika Antonova, Silvia Cruciani, Christian Smith, Danica Kragic category:cs.RO cs.LG  published:2017-03-01 summary:In this work we propose an approach to learn a robust policy for solving the pivoting task. Recently, several model-free continuous control algorithms were shown to learn successful policies without prior knowledge of the dynamics of the task. However, obtaining successful policies required thousands to millions of training episodes, limiting the applicability of these approaches to real hardware. We developed a training procedure that allows us to use a simple custom simulator to learn policies robust to the mismatch of simulation vs robot. In our experiments, we demonstrate that the policy learned in the simulator is able to pivot the object to the desired target angle on the real robot. We also show generalization to an object with different inertia, shape, mass and friction properties than those used during training. This result is a step towards making model-free reinforcement learning available for solving robotics tasks via pre-training in simulators that offer only an imprecise match to the real-world dynamics. version:1
arxiv-1703-00443 | OptNet: Differentiable Optimization as a Layer in Neural Networks | http://arxiv.org/abs/1703.00443 | id:1703.00443 author:Brandon Amos, J. Zico Kolter category:cs.LG cs.AI math.OC stat.ML  published:2017-03-01 summary:This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers allow complex dependencies between the hidden states to be captured that traditional convolutional and fully-connected layers are not able to capture. In this paper, we develop the foundations for such an architecture: we derive the equations to perform exact differentiation through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one particularly standout example, we show that the method is capable of learning to play Sudoku given just input and output games, with no a priori information about the rules of the game; this task is virtually impossible for other neural network architectures that we have experimented with, and highlights the representation capabilities of our approach. version:1
arxiv-1703-00441 | Learning to Optimize Neural Nets | http://arxiv.org/abs/1703.00441 | id:1703.00441 author:Ke Li, Jitendra Malik category:cs.LG cs.AI math.OC stat.ML  published:2017-03-01 summary:Learning to Optimize is a recently proposed framework for learning optimization algorithms using reinforcement learning. In this paper, we explore learning an optimization algorithm for training shallow neural nets. Such high-dimensional stochastic optimization problems present interesting challenges for existing reinforcement learning algorithms. We develop an extension that is suited to learning optimization algorithms in this setting and demonstrate that the learned optimization algorithm consistently outperforms other known optimization algorithms even on unseen tasks and is robust to changes in stochasticity of gradients and the neural net architecture. More specifically, we show that an optimization algorithm trained with the proposed method on the problem of training a neural net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset, CIFAR-10 and CIFAR-100. version:1
arxiv-1703-00440 | Fast k-Nearest Neighbour Search via Prioritized DCI | http://arxiv.org/abs/1703.00440 | id:1703.00440 author:Ke Li, Jitendra Malik category:cs.LG cs.AI cs.DS cs.IR stat.ML  published:2017-03-01 summary:Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exponential dependence on either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing (DCI) offers a promising way of circumventing the curse by avoiding space partitioning and achieves a query time that grows sublinearly in the intrinsic dimensionality. In this paper, we develop a variant of DCI, which we call Prioritized DCI, and show a further improvement in the dependence on the intrinsic dimensionality compared to standard DCI, thereby improving the performance of DCI on datasets with high intrinsic dimensionality. We also demonstrate empirically that Prioritized DCI compares favourably to standard DCI and Locality-Sensitive Hashing (LSH) both in terms of running time and space consumption at all levels of approximation quality. In particular, relative to LSH, Prioritized DCI reduces the number of distance evaluations by a factor of 5 to 30 and the space consumption by a factor of 47 to 55. version:1
arxiv-1703-01243 | Augmented Reality for Depth Cues in Monocular Minimally Invasive Surgery | http://arxiv.org/abs/1703.01243 | id:1703.01243 author:Long Chen, Wen Tang, Nigel W. John, Tao Ruan Wan, Jian Jun Zhang category:cs.CV  published:2017-03-01 summary:One of the major challenges in Minimally Invasive Surgery (MIS) such as laparoscopy is the lack of depth perception. In recent years, laparoscopic scene tracking and surface reconstruction has been a focus of investigation to provide rich additional information to aid the surgical process and compensate for the depth perception issue. However, robust 3D surface reconstruction and augmented reality with depth perception on the reconstructed scene are yet to be reported. This paper presents our work in this area. First, we adopt a state-of-the-art visual simultaneous localization and mapping (SLAM) framework - ORB-SLAM - and extend the algorithm for use in MIS scenes for reliable endoscopic camera tracking and salient point mapping. We then develop a robust global 3D surface reconstruction frame- work based on the sparse point clouds extracted from the SLAM framework. Our approach is to combine an outlier removal filter within a Moving Least Squares smoothing algorithm and then employ Poisson surface reconstruction to obtain smooth surfaces from the unstructured sparse point cloud. Our proposed method has been quantitatively evaluated compared with ground-truth camera trajectories and the organ model surface we used to render the synthetic simulation videos. In vivo laparoscopic videos used in the tests have demonstrated the robustness and accuracy of our proposed framework on both camera tracking and surface reconstruction, illustrating the potential of our algorithm for depth augmentation and depth-corrected augmented reality in MIS with monocular endoscopes. version:1
arxiv-1703-00403 | Preserving Differential Privacy Between Features in Distributed Estimation | http://arxiv.org/abs/1703.00403 | id:1703.00403 author:Christina Heinze-Deml, Brian McWilliams, Nicolai Meinshausen category:stat.ML cs.CR cs.DC cs.LG  published:2017-03-01 summary:Privacy is crucial in many applications of machine learning. Legal, ethical and societal issues restrict the sharing of sensitive data making it difficult to learn from datasets that are partitioned between many parties. One important instance of such a distributed setting arises when information about each record in the dataset is held by different data owners (the design matrix is "vertically-partitioned"). In this setting few approaches exist for private data sharing for the purposes of statistical estimation and the classical setup of differential privacy with a "trusted curator" preparing the data does not apply. We introduce S-differential privacy which extends single-party differential privacy to the distributed, vertically-partitioned case. We then propose PriDE, a scalable framework for distributed estimation where each party communicates perturbed sketches of their locally held features ensuring S-differential privacy is preserved. For L2-penalized supervised learning problems PriDE has bounded estimation error compared with the optimal estimates obtained without privacy constraints in the non-distributed setting. We confirm this empirically on real world and synthetic datasets. version:1
arxiv-1703-00395 | Lossy Image Compression with Compressive Autoencoders | http://arxiv.org/abs/1703.00395 | id:1703.00395 author:Lucas Theis, Wenzhe Shi, Andrew Cunningham, Ferenc Huszár category:stat.ML cs.CV  published:2017-03-01 summary:We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images. version:1
arxiv-1703-00381 | The Statistical Recurrent Unit | http://arxiv.org/abs/1703.00381 | id:1703.00381 author:Junier B. Oliva, Barnabas Poczos, Jeff Schneider category:cs.LG cs.AI stat.ML  published:2017-03-01 summary:Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU's architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures' hyperparameters in a Bayesian optimization scheme for both synthetic and real-world tasks. version:1
arxiv-1703-00380 | Personal Model Training under Privacy Constraints | http://arxiv.org/abs/1703.00380 | id:1703.00380 author:Sandra Servia-Rodriguez, Liang Wang, Jianxin R. Zhao, Richard Mortier, Hamed Haddadi category:cs.LG  published:2017-03-01 summary:Many current Internet services rely on inferences from models trained on user data. Commonly, both the training and inference tasks are carried out using cloud resources fed by personal data collected at scale from users. Holding and using such large collections of personal data in the cloud creates privacy risks to the data subjects, but is currently required for users to benefit from such services. We explore how to provide for model training and inference in a system where computation is moved to the data in preference to moving data to the cloud, obviating many current privacy risks. Specifically, we take an initial model learnt from a small set of users and retrain it locally using data from a single user. We evaluate on two tasks: one supervised learning task, using a neural network to recognise users' current activity from accelerometer traces; and one unsupervised learning task, identifying topics in a large set of documents. In both cases the accuracy is improved. We also demonstrate the feasibility of our approach by presenting a performance evaluation on a representative resource-constrained device (a Raspberry Pi). version:1
arxiv-1703-00377 | Gradient Boosting on Stochastic Data Streams | http://arxiv.org/abs/1703.00377 | id:1703.00377 author:Hanzhang Hu, Wen Sun, Arun Venkatraman, Martial Hebert, J. Andrew Bagnell category:cs.LG  published:2017-03-01 summary:Boosting is a popular ensemble algorithm that generates more powerful learners by linearly combining base models from a simpler hypothesis class. In this work, we investigate the problem of adapting batch gradient boosting for minimizing convex loss functions to online setting where the loss at each iteration is i.i.d sampled from an unknown distribution. To generalize from batch to online, we first introduce the definition of online weak learning edge with which for strongly convex and smooth loss functions, we present an algorithm, Streaming Gradient Boosting (SGB) with exponential shrinkage guarantees in the number of weak learners. We further present an adaptation of SGB to optimize non-smooth loss functions, for which we derive a O(ln N/N) convergence rate. We also show that our analysis can extend to adversarial online learning setting under a stronger assumption that the online weak learning edge will hold in adversarial setting. We finally demonstrate experimental results showing that in practice our algorithms can achieve competitive results as classic gradient boosting while using less computation. version:1
arxiv-1703-00356 | Graph-based Isometry Invariant Representation Learning | http://arxiv.org/abs/1703.00356 | id:1703.00356 author:Renata Khasanova, Pascal Frossard category:cs.CV cs.LG  published:2017-03-01 summary:Learning transformation invariant representations of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However, they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and pooling layers in deep networks with graph spectral convolution and dynamic graph pooling layers that together contribute to invariance to isometric transformations. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that are very sensitive to transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets. version:1
arxiv-1703-00329 | Convergence rate of a simulated annealing algorithm with noisy observations | http://arxiv.org/abs/1703.00329 | id:1703.00329 author:Clément Bouttier, Ioana Gavra category:stat.ML math.OC math.ST stat.TH  published:2017-03-01 summary:In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples. version:1
arxiv-1703-00317 | Tracing Linguistic Relations in Winning and Losing Sides of Explicit Opposing Groups | http://arxiv.org/abs/1703.00317 | id:1703.00317 author:Ceyda Sanli, Anupam Mondal, Erik Cambria category:cs.CL cs.AI  published:2017-03-01 summary:Linguistic relations in oral conversations present how opinions are constructed and developed in a restricted time. The relations bond ideas, arguments, thoughts, and feelings, re-shape them during a speech, and finally build knowledge out of all information provided in the conversation. Speakers share a common interest to discuss. It is expected that each speaker's reply includes duplicated forms of words from previous speakers. However, linguistic adaptation is observed and evolves in a more complex path than just transferring slightly modified versions of common concepts. A conversation aiming a benefit at the end shows an emergent cooperation inducing the adaptation. Not only cooperation, but also competition drives the adaptation or an opposite scenario and one can capture the dynamic process by tracking how the concepts are linguistically linked. To uncover salient complex dynamic events in verbal communications, we attempt to discover self-organized linguistic relations hidden in a conversation with explicitly stated winners and losers. We examine open access data of the United States Supreme Court. Our understanding is crucial in big data research to guide how transition states in opinion mining and decision-making should be modeled and how this required knowledge to guide the model should be pinpointed, by filtering large amount of data. version:1
arxiv-0712-4273 | Online EM Algorithm for Latent Data Models | http://arxiv.org/abs/0712.4273 | id:0712.4273 author:Olivier Cappé, Eric Moulines category:stat.CO cs.LG  published:2007-12-27 summary:In this contribution, we propose a generic online (also sometimes called adaptive or recursive) version of the Expectation-Maximisation (EM) algorithm applicable to latent variable models of independent observations. Compared to the algorithm of Titterington (1984), this approach is more directly connected to the usual EM algorithm and does not rely on integration with respect to the complete data distribution. The resulting algorithm is usually simpler and is shown to achieve convergence to the stationary points of the Kullback-Leibler divergence between the marginal distribution of the observation and the model distribution at the optimal rate, i.e., that of the maximum likelihood estimator. In addition, the proposed approach is also suitable for conditional (or regression) models, as illustrated in the case of the mixture of linear regressions model. version:4
arxiv-1703-00286 | Phylogenetic Tools in Astrophysics | http://arxiv.org/abs/1703.00286 | id:1703.00286 author:Didier Fraix-Burnet category:astro-ph.IM stat.ML  published:2017-03-01 summary:Multivariate clustering in astrophysics is a recent development justified by the bigger and bigger surveys of the sky. The phylogenetic approach is probably the most unexpected technique that has appeared for the unsupervised classification of galaxies, stellar populations or globular clusters. On one side, this is a somewhat natural way of classifying astrophysical entities which are all evolving objects. On the other side, several conceptual and practical difficulties arize, such as the hierarchical representation of the astrophysical diversity, the continuous nature of the parameters, and the adequation of the result to the usual practice for the physical interpretation. Most of these have now been solved through the studies of limited samples of stellar clusters and galaxies. Up to now, only the Maximum Parsimony (cladistics) has been used since it is the simplest and most general phylogenetic technique. Probabilistic and network approaches are obvious extensions that should be explored in the future. version:1
arxiv-1703-00284 | L$^3$-SVMs: Landmarks-based Linear Local Support Vectors Machines | http://arxiv.org/abs/1703.00284 | id:1703.00284 author:Valentina Zantedeschi, Rémi Emonet, Marc Sebban category:stat.ML cs.LG  published:2017-03-01 summary:For their ability to capture non-linearities in the data and to scale to large training sets, local Support Vector Machines (SVMs) have received a special attention during the past decade. In this paper, we introduce a new local SVM method, called L$^3$-SVMs, which clusters the input space, carries out dimensionality reduction by projecting the data on landmarks, and jointly learns a linear combination of local models. Simple and effective, our algorithm is also theoretically well-founded. Using the framework of Uniform Stability, we show that our SVM formulation comes with generalization guarantees on the true risk. The experiments based on the simplest configuration of our model (i.e. landmarks randomly selected, linear projection, linear kernel) show that L$^3$-SVMs is very competitive w.r.t. the state of the art and opens the door to new exciting lines of research. version:1
arxiv-1703-00249 | Human Eye Visual Hyperacuity: A New Paradigm for Sensing? | http://arxiv.org/abs/1703.00249 | id:1703.00249 author:Adur Lagunas, Oier Dominguez, Susana Martinez-Conde, Stephen L. Macknik, Carlos del-Rio category:cs.CV I.4.3; I.4.4  published:2017-03-01 summary:The human eye appears to be using a low number of sensors for image capturing. Furthermore, regarding the physical dimensions of cones-photoreceptors responsible for the sharp central vision-, we may realize that these sensors are of a relatively small size and area. Nonetheless, the eye is capable to obtain high resolution images due to visual hyperacuity and presents an impressive sensitivity and dynamic range when set against conventional digital cameras of similar characteristics. This article is based on the hypothesis that the human eye may be benefiting from diffraction to improve both image resolution and acquisition process. The developed method intends to explain and simulate using MATLAB software the visual hyperacuity: the introduction of a controlled diffraction pattern at an initial stage, enables the use of a reduced number of sensors for capturing the image and makes possible a subsequent processing to improve the final image resolution. The results have been compared with the outcome of an equivalent system but in absence of diffraction, achieving promising results. The main conclusion of this work is that diffraction could be helpful for capturing images or signals when a small number of sensors available, which is far from being a resolution-limiting factor. version:1
arxiv-1703-00247 | Learning A Physical Long-term Predictor | http://arxiv.org/abs/1703.00247 | id:1703.00247 author:Sebastien Ehrhardt, Aron Monszpart, Niloy J. Mitra, Andrea Vedaldi category:cs.AI cs.NE  published:2017-03-01 summary:Evolution has resulted in highly developed abilities in many natural intelligences to quickly and accurately predict mechanical phenomena. Humans have successfully developed laws of physics to abstract and model such mechanical phenomena. In the context of artificial intelligence, a recent line of work has focused on estimating physical parameters based on sensory data and use them in physical simulators to make long-term predictions. In contrast, we investigate the effectiveness of a single neural network for end-to-end long-term prediction of mechanical phenomena. Based on extensive evaluation, we demonstrate that such networks can outperform alternate approaches having even access to ground-truth physical simulators, especially when some physical parameters are unobserved or not known a-priori. Further, our network outputs a distribution of outcomes to capture the inherent uncertainty in the data. Our approach demonstrates for the first time the possibility of making actionable long-term predictions from sensor data without requiring to explicitly model the underlying physical laws. version:1
arxiv-1703-00234 | Improving Object Detection with Region Similarity Learning | http://arxiv.org/abs/1703.00234 | id:1703.00234 author:Feng Gao, Yihang Lou, Yan Bai, Shiqi Wang, Tiejun Huang, Ling-Yu Duan category:cs.CV  published:2017-03-01 summary:Object detection aims to identify instances of semantic objects of a certain class in images or videos. The success of state-of-the-art approaches is attributed to the significant progress of object proposal and convolutional neural networks (CNNs). Most promising detectors involve multi-task learning with an optimization objective of softmax loss and regression loss. The first is for multi-class categorization, while the latter is for improving localization accuracy. However, few of them attempt to further investigate the hardness of distinguishing different sorts of distracting background regions (i.e., negatives) from true object regions (i.e., positives). To improve the performance of classifying positive object regions vs. a variety of negative background regions, we propose to incorporate triplet embedding into learning objective. The triplet units are formed by assigning each negative region to a meaningful object class and establishing class- specific negatives, followed by triplets construction. Over the benchmark PASCAL VOC 2007, the proposed triplet em- bedding has improved the performance of well-known FastRCNN model with a mAP gain of 2.1%. In particular, the state-of-the-art approach OHEM can benefit from the triplet embedding and has achieved a mAP improvement of 1.2%. version:1
arxiv-1702-08720 | Learning Discrete Representations via Information Maximizing Self Augmented Training | http://arxiv.org/abs/1702.08720 | id:1702.08720 author:Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, Masashi Sugiyama category:stat.ML cs.LG  published:2017-02-28 summary:Learning discrete representations of data is a central machine learning task because of the compactness of the representations and ease of interpretation. The task includes clustering and hash learning as special cases. Deep neural networks are promising to be used because they can model the non-linearity of data and scale to large datasets. However, their model complexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest. To this end, we propose a method called Information Maximizing Self Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose the invariance on discrete representations. More specifically, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion. At the same time, we maximize the information-theoretic dependency between data and their predicted discrete representations. Extensive experiments on benchmark datasets show that IMSAT produces state-of-the-art results for both clustering and unsupervised hash learning. version:2
arxiv-1703-00209 | Online Natural Gradient as a Kalman Filter | http://arxiv.org/abs/1703.00209 | id:1703.00209 author:Yann Ollivier category:stat.ML math.OC  published:2017-03-01 summary:We review the relationship between Kalman filtering and Amari's natural gradient in statistical learning. Namely, using an online natural gradient descent on data log-likelihood to evaluate the parameter of a probabilistic model given a series of observations, is exactly equivalent to using an extended Kalman filter to estimate the parameter (assumed to have constant dynamics). In the non-recurrent case, this relation is a consequence of the "information filter" phrasing of the Kalman filter. In the recurrent case, we prove that the joint Kalman filter over states and parameters is a natural gradient on top of real-time recurrent learning (RTRL), a classical algorithm to train recurrent models. This correspondence provides relevant settings for natural gradient hyperparameters such as learning rates or initialization and regularization of the Fisher information matrix. version:1
arxiv-1703-00196 | Incorporating Intra-Class Variance to Fine-Grained Visual Recognition | http://arxiv.org/abs/1703.00196 | id:1703.00196 author:Yan Bai, Feng Gao, Yihang Lou, Shiqi Wang, Tiejun Huang, Ling-Yu Duan category:cs.CV  published:2017-03-01 summary:Fine-grained visual recognition aims to capture discriminative characteristics amongst visually similar categories. The state-of-the-art research work has significantly improved the fine-grained recognition performance by deep metric learning using triplet network. However, the impact of intra-category variance on the performance of recognition and robust feature representation has not been well studied. In this paper, we propose to leverage intra-class variance in metric learning of triplet network to improve the performance of fine-grained recognition. Through partitioning training images within each category into a few groups, we form the triplet samples across different categories as well as different groups, which is called Group Sensitive TRiplet Sampling (GS-TRS). Accordingly, the triplet loss function is strengthened by incorporating intra-class variance with GS-TRS, which may contribute to the optimization objective of triplet network. Extensive experiments over benchmark datasets CompCar and VehicleID show that the proposed GS-TRS has significantly outperformed state-of-the-art approaches in both classification and retrieval tasks. version:1
arxiv-1702-05373 | EMNIST: an extension of MNIST to handwritten letters | http://arxiv.org/abs/1702.05373 | id:1702.05373 author:Gregory Cohen, Saeed Afshar, Jonathan Tapson, André van Schaik category:cs.CV  published:2017-02-17 summary:The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits. version:2
arxiv-1703-00177 | Optical Flow-based 3D Human Motion Estimation from Monocular Video | http://arxiv.org/abs/1703.00177 | id:1703.00177 author:Thiemo Alldieck, Marc Kassubeck, Marcus Magnor category:cs.CV  published:2017-03-01 summary:We present a generative method to estimate 3D human motion and body shape from monocular video. Under the assumption that starting from an initial pose optical flow constrains subsequent human motion, we exploit flow to find temporally coherent human poses of a motion sequence. We estimate human motion by minimizing the difference between computed flow fields and the output of an artificial flow renderer. A single initialization step is required to estimate motion over multiple frames. Several regularization functions enhance robustness over time. Our test scenarios demonstrate that optical flow effectively regularizes the under-constrained problem of human shape and motion estimation from monocular video. version:1
arxiv-1703-00168 | Modular Representation of Layered Neural Networks | http://arxiv.org/abs/1703.00168 | id:1703.00168 author:Chihiro Watanabe, Kaoru Hiramatsu, Kunio Kashino category:stat.ML cs.LG  published:2017-03-01 summary:Deep neural networks have greatly improved the performance of various applications including image processing, speech recognition, natural language processing, and bioinformatics. However, it is still difficult to discover or interpret knowledge from the inference provided by a deep neural network, since its internal representation has many nonlinear and complex parameters embedded in hierarchical layers. Therefore, it becomes important to establish a new methodology by which deep neural networks can be understood. In this paper, we propose a new method for extracting a global and simplified structure from a layered neural network. Based on network analysis, the proposed method detects communities or clusters of units with similar connection patterns. We show its effectiveness by applying it to three use cases. (1) Network decomposition: it can decompose a trained neural network into multiple small independent networks thus dividing the problem and reducing the computation time. (2) Training assessment: the appropriateness of a trained result with a given hyperparameter or randomly chosen initial parameters can be evaluated by using a modularity index. And (3) data analysis: in practical data it reveals the community structure in the input, hidden, and output layers, which serves as a clue for discovering knowledge from a trained neural network. version:1
arxiv-1703-00160 | Saliency Fusion in Eigenvector Space with Multi-Channel Pulse Coupled Neural Network | http://arxiv.org/abs/1703.00160 | id:1703.00160 author:Nevrez Imamoglu, Zhixuan Wei, Huangjun Shi, Yuki Yoshida, Myagmarbayar Nergui, Jose Gonzalez, Dongyun Gu, Weidong Chen, Kenzo Nonami, Wenwei Yu category:cs.CV  published:2017-03-01 summary:Saliency computation has become a popular research field for many applications due to the useful information provided by saliency maps. For a saliency map, local relations around the salient regions in multi-channel perspective should be taken into consideration by aiming uniformity on the region of interest as an internal approach. And, irrelevant salient regions have to be avoided as much as possible. Most of the works achieve these criteria with external processing modules; however, these can be accomplished during the conspicuity map fusion process. Therefore, in this paper, a new model is proposed for saliency/conspicuity map fusion with two concepts: a) input image transformation relying on the principal component analysis (PCA), and b) saliency conspicuity map fusion with multi-channel pulsed coupled neural network (m-PCNN). Experimental results, which are evaluated by precision, recall, F-measure, and area under curve (AUC), support the reliability of the proposed method by enhancing the saliency computation. version:1
arxiv-1703-00154 | Inertial Odometry on Handheld Smartphones | http://arxiv.org/abs/1703.00154 | id:1703.00154 author:Arno Solin, Santiago Cortes, Esa Rahtu, Juho Kannala category:cs.CV stat.AP  published:2017-03-01 summary:Building a complete inertial navigation system using the limited quality data provided by current smartphones has been regarded challenging, if not impossible. We present a probabilistic approach for orientation and use-case free inertial odometry, which is based on double-integrating rotated accelerations. Our approach uses a probabilistic approach in fusing the noisy sensor data and learning the model parameters online. It is able to track the phone position, velocity, and pose in real-time and in a computationally lightweight fashion. The information fusion is completed with altitude correction from barometric pressure readings (if available), zero-velocity updates (if the phone remains stationary), and pseudo-updates limiting the momentary speed. We demonstrate our approach using a standard iPad and iPhone in several indoor dead-reckoning applications and in a measurement tool setup. version:1
arxiv-1703-00152 | Saliency Detection by Forward and Backward Cues in Deep-CNNs | http://arxiv.org/abs/1703.00152 | id:1703.00152 author:Nevrez Imamoglu, Chi Zhang, Wataru Shimoda, Yuming Fang, Boxin Shi category:cs.CV  published:2017-03-01 summary:As prior knowledge of objects or object features helps us make relations for similar objects on attentional tasks, pre-trained deep convolutional neural networks (CNNs) can be used to detect salient objects on images regardless of the object class is in the network knowledge or not. In this paper, we propose a top-down saliency model using CNN, a weakly supervised CNN model trained for 1000 object labelling task from RGB images. The model detects attentive regions based on their objectness scores predicted by selected features from CNNs. To estimate the salient objects effectively, we combine both forward and backward features, while demonstrating that partially-guided backpropagation will provide sufficient information for selecting the features from forward run of CNN model. Finally, these top-down cues are enhanced with a state-of-the-art bottom-up model as complementing the overall saliency. As the proposed model is an effective integration of forward and backward cues through objectness without any supervision or regression to ground truth data, it gives promising results compared to state-of-the-art models in two different datasets. version:1
arxiv-1702-06943 | When Lempel-Ziv-Welch Meets Machine Learning: A Case Study of Accelerating Machine Learning using Coding | http://arxiv.org/abs/1702.06943 | id:1702.06943 author:Fengan Li, Lingjiao Chen, Arun Kumar, Jeffrey F. Naughton, Jignesh M. Patel, Xi Wu category:cs.LG cs.DB stat.ML  published:2017-02-22 summary:In this paper we study the use of coding techniques to accelerate machine learning (ML). Coding techniques, such as prefix codes, have been extensively studied and used to accelerate low-level data processing primitives such as scans in a relational database system. However, there is little work on how to exploit them to accelerate ML algorithms. In fact, applying coding techniques for faster ML faces a unique challenge: one needs to consider both how the codes fit into the optimization algorithm used to train a model, and the interplay between the model structure and the coding scheme. Surprisingly and intriguingly, our study demonstrates that a slight variant of the classical Lempel-Ziv-Welch (LZW) coding scheme is a good fit for several popular ML algorithms, resulting in substantial runtime savings. Comprehensive experiments on several real-world datasets show that our LZW-based ML algorithms exhibit speedups of up to 31x compared to a popular and state-of-the-art ML library, with no changes to ML accuracy, even though the implementations of our LZW variants are not heavily tuned. Thus, our study reveals a new avenue for accelerating ML algorithms using coding techniques and we hope this opens up a new direction for more research. version:2
arxiv-1703-00144 | Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank | http://arxiv.org/abs/1703.00144 | id:1703.00144 author:Liang Zhao, Siyu Liao, Yanzhi Wang, Jian Tang, Bo Yuan category:cs.LG cs.CV stat.ML  published:2017-03-01 summary:Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. We formally study LDR matrices in deep learning. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose back-propagation based training algorithm for general LDR neural networks. version:1
arxiv-1703-00133 | Easy over Hard: A Case Study on Deep Learning | http://arxiv.org/abs/1703.00133 | id:1703.00133 author:Wei Fu, Tim Menzies category:cs.SE cs.LG  published:2017-03-01 summary:While deep learning is an exciting new technique, the benefits of this method need to be assessed w.r.t. its computational cost. This is particularly important for deep learning since these learners need hours (to weeks) to train the model. Such long CPU times limit the ability of (a) a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b)other researchers to repeat, improve, or even refute that original work. For example, recently, deep learning was used to find which questions in the Stack Overflow programmer discussion forum can be linked together. That system took 14 hours to execute. We show here that a very simple optimizer called DE (differential evolution) can achieve similar (and sometimes better). The DE approach terminated in 10 minutes; i.e. 84 times faster hours than deep learning. We offer these results as a cautionary tale to the software analytics community and suggest that not every new innovation should be applied without critical analysis. If researchers deploy some new and expensive process, that work should be baselined against some simpler and faster alternatives. version:1
arxiv-1703-00132 | Revisiting Unsupervised Learning for Defect Prediction | http://arxiv.org/abs/1703.00132 | id:1703.00132 author:Wei Fu, Tim Menzies category:cs.SE cs.LG  published:2017-03-01 summary:Collecting quality data from software projects can be time-consuming and expensive. Hence, some researchers explore "unsupervised" approaches to quality prediction that does not require labelled data. An alternate technique is to use "supervised" approaches that learn models from project data labelled with, say, "defective" or "not-defective".Most researchers use these supervised models since, it is argued, they can exploit more knowledge of the projects. At FSE'16, Yang et al. reported startling results where unsupervised defect predictors outperformed supervised predictors for effort-aware just-in-time defect prediction. If confirmed, these results would lead to a dramatic simplification of a seemingly complex task (data mining) that is widely explored in the SE literature. This paper repeats and refutes those results as follows. (1)There is much variability in the efficacy of the Yang et al. models so even with their approach, some supervised data is required to prune weaker models. (2)Their findings were grouped across $N$ projects. When we repeat their analysis on a project-by-project basis, supervised predictors are seen to work better. Even though this paper rejects the specific conclusions of Yang et al., we still endorse their general goal. In our our experiments, supervisedpredictors did not perform outstandingly better than unsupervised ones. Hence, they may indeed be some combination of unsupervisedlearners to achieve comparable performance to supervised. We therefore encourage others to work in this promising area. version:1
arxiv-1703-00122 | RGB-D Salient Object Detection Based on Discriminative Cross-modal Transfer Learning | http://arxiv.org/abs/1703.00122 | id:1703.00122 author:Hao Chen, Y. F. Li, Dan Su category:cs.CV  published:2017-03-01 summary:In this work, we propose to utilize Convolutional Neural Networks to boost the performance of depth-induced salient object detection by capturing the high-level representative features for depth modality. We formulate the depth-induced saliency detection as a CNN-based cross-modal transfer problem to bridge the gap between the "data-hungry" nature of CNNs and the unavailability of sufficient labeled training data in depth modality. In the proposed approach, we leverage the auxiliary data from the source modality effectively by training the RGB saliency detection network to obtain the task-specific pre-understanding layers for the target modality. Meanwhile, we exploit the depth-specific information by pre-training a modality classification network that encourages modal-specific representations during the optimizing course. Thus, it could make the feature representations of the RGB and depth modalities as discriminative as possible. These two modules are pre-trained independently and then stitched to initialize and optimize the eventual depth-induced saliency detection model. Experiments demonstrate the effectiveness of the proposed novel pre-training strategy as well as the significant and consistent improvements of the proposed approach over other state-of-the-art methods. version:1
arxiv-1703-00121 | Remote Sensing Image Scene Classification: Benchmark and State of the Art | http://arxiv.org/abs/1703.00121 | id:1703.00121 author:Gong Cheng, Junwei Han, Xiaoqiang Lu category:cs.CV  published:2017-03-01 summary:Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various datasets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning datasets and methods for scene classification is still lacking. In addition, almost all existing datasets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale dataset, termed "NWPU-RESISC45", which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 (i) is large-scale on the scene classes and the total image number, (ii) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion, and (iii) has high within-class diversity and between-class similarity. The creation of this dataset will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed dataset and the results are reported as a useful baseline for future research. version:1
arxiv-1703-00119 | Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to Non-smooth Concave Maximization | http://arxiv.org/abs/1703.00119 | id:1703.00119 author:Bo Liu, Xiao-Tong Yuan, Lezi Wang, Qingshan Liu, Dimitris N. Metaxas category:cs.LG stat.ML  published:2017-03-01 summary:Iterative Hard Thresholding (IHT) is a class of projected gradient descent methods for optimizing sparsity-constrained minimization models, with the best known efficiency and scalability in practice. As far as we know, the existing IHT-style methods are designed for sparse minimization in primal form. It remains open to explore duality theory and algorithms in such a non-convex and NP-hard setting. In this article, we bridge the gap by establishing a duality theory for sparsity-constrained minimization with $\ell_2$-regularized objective and proposing an IHT-style algorithm for dual maximization. Our sparse duality theory provides a set of sufficient and necessary conditions under which the original NP-hard/non-convex problem can be equivalently solved in a dual space. The proposed dual IHT algorithm is a super-gradient method for maximizing the non-smooth dual objective. An interesting finding is that the sparse recovery performance of dual IHT is invariant to the Restricted Isometry Property (RIP), which is required by all the existing primal IHT without sparsity relaxation. Moreover, a stochastic variant of dual IHT is proposed for large-scale stochastic optimization. Numerical results demonstrate that dual IHT algorithms can achieve more accurate model estimation given small number of training data and have higher computational efficiency than the state-of-the-art primal IHT-style algorithms. version:1
arxiv-1703-00102 | SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient | http://arxiv.org/abs/1703.00102 | id:1703.00102 author:Lam Nguyen, Jie Liu, Katya Scheinberg, Martin Takáč category:stat.ML cs.LG math.OC  published:2017-03-01 summary:In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm. version:1
arxiv-1703-00099 | Learning Conversational Systems that Interleave Task and Non-Task Content | http://arxiv.org/abs/1703.00099 | id:1703.00099 author:Zhou Yu, Alan W Black, Alexander I. Rudnicky category:cs.CL cs.AI cs.HC  published:2017-03-01 summary:Task-oriented dialog systems have been applied in various tasks, such as automated personal assistants, customer service providers and tutors. These systems work well when users have clear and explicit intentions that are well-aligned to the systems' capabilities. However, they fail if users intentions are not explicit. To address this shortcoming, we propose a framework to interleave non-task content (i.e. everyday social conversation) into task conversations. When the task content fails, the system can still keep the user engaged with the non-task content. We trained a policy using reinforcement learning algorithms to promote long-turn conversation coherence and consistency, so that the system can have smooth transitions between task and non-task content. To test the effectiveness of the proposed framework, we developed a movie promotion dialog system. Experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system. version:1
arxiv-1703-00096 | Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling | http://arxiv.org/abs/1703.00096 | id:1703.00096 author:Hairong Liu, Zhenyao Zhu, Xiangang Li, Sanjeev Satheesh category:cs.CL cs.LG cs.NE  published:2017-03-01 summary:Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa- per, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark. version:1
arxiv-1702-04528 | A deep learning model integrating FCNNs and CRFs for brain tumor segmentation | http://arxiv.org/abs/1702.04528 | id:1702.04528 author:Xiaomei Zhao, Yihong Wu, Guidong Song, Zhenye Li, Yazhuo Zhang, Yong Fan category:cs.CV  published:2017-02-15 summary:Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation. Build upon successful deep learning techniques, we propose a novel brain tumor segmentation method by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency. We train the deep learning based segmentation model using image patches and image slices in following steps: 1) training FCNNs using image patches; 2) training CRF-RNN using image slices of axial view with parameters of FCNNs fixed; and 3) fine-tuning the whole network using image slices. Our method could segment brain images slice-by-slice, much faster than those image patch based tumor segmentation methods. We have evaluated our method based on imaging data provided by the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013 and the BRATS 2016. The experimental results have demonstrated that our method could build a segmentation model with Flair, T1c, and T2 scans and achieve competitive performance as those built with Flair, T1, T1c, and T2 scans. version:2
arxiv-1703-00089 | A Joint Identification Approach for Argumentative Writing Revisions | http://arxiv.org/abs/1703.00089 | id:1703.00089 author:Fan Zhang, Diane Litman category:cs.CL  published:2017-02-28 summary:Prior work on revision identification typically uses a pipeline method: revision extraction is first conducted to identify the locations of revisions and revision classification is then conducted on the identified revisions. Such a setting propagates the errors of the revision extraction step to the revision classification step. This paper proposes an approach that identifies the revision location and the revision type jointly to solve the issue of error propagation. It utilizes a sequence representation of revisions and conducts sequence labeling for revision identification. A mutation-based approach is utilized to update identification sequences. Results demonstrate that our proposed approach yields better performance on both revision location extraction and revision type classification compared to a pipeline baseline. version:1
arxiv-1703-00084 | Multi-Sensor Data Pattern Recognition for Multi-Target Localization: A Machine Learning Approach | http://arxiv.org/abs/1703.00084 | id:1703.00084 author:Kasthurirengan Suresh, Samuel Silva, Johnathan Votion, Yongcan Cao category:cs.SY cs.LG stat.ML  published:2017-02-28 summary:Data-target pairing is an important step towards multi-target localization for the intelligent operation of unmanned systems. Target localization plays a crucial role in numerous applications, such as search, and rescue missions, traffic management and surveillance. The objective of this paper is to present an innovative target location learning approach, where numerous machine learning approaches, including K-means clustering and supported vector machines (SVM), are used to learn the data pattern across a list of spatially distributed sensors. To enable the accurate data association from different sensors for accurate target localization, appropriate data pre-processing is essential, which is then followed by the application of different machine learning algorithms to appropriately group data from different sensors for the accurate localization of multiple targets. Through simulation examples, the performance of these machine learning algorithms is quantified and compared. version:1
arxiv-1703-00075 | Discrete Wavelet Transform Based Algorithm for Recognition of QRS Complexes | http://arxiv.org/abs/1703.00075 | id:1703.00075 author:Rachid Haddadi, Elhassane Abdelmounim, Mustapha El Hanine, Abdelaziz Belaguid category:cs.CV  published:2017-02-28 summary:This paper proposes the application of Discrete Wavelet Transform (DWT) to detect the QRS (ECG is characterized by a recurrent wave sequence of P, QRS and T-wave) of an electrocardiogram (ECG) signal. Wavelet Transform provides localization in both time and frequency. In preprocessing stage, DWT is used to remove the baseline wander in the ECG signal. The performance of the algorithm of QRS detection is evaluated against the standard MIT BIH (Massachusetts Institute of Technology, Beth Israel Hospital) Arrhythmia database. The average QRS complexes detection rate of 98.1 % is achieved. version:1
arxiv-1703-00069 | Deep Image Harmonization | http://arxiv.org/abs/1703.00069 | id:1703.00069 author:Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, Ming-Hsuan Yang category:cs.CV  published:2017-02-28 summary:Compositing is one of the most common operations in photo editing. To generate realistic composites, the appearances of foreground and background need to be adjusted to make them compatible. Previous approaches to harmonize composites have focused on learning statistical relationships between hand-crafted appearance features of the foreground and background, which is unreliable especially when the contents in the two layers are vastly different. In this work, we propose an end-to-end deep convolutional neural network for image harmonization, which can capture both the context and semantic information of the composite images during harmonization. We also introduce an efficient way to collect large-scale and high-quality training data that can facilitate the training process. Experiments on the synthesized dataset and real composite images show that the proposed network outperforms previous state-of-the-art methods. version:1
arxiv-1703-00066 | On the Power of Learning from $k$-Wise Queries | http://arxiv.org/abs/1703.00066 | id:1703.00066 author:Vitaly Feldman, Badih Ghazi category:cs.LG cs.DS  published:2017-02-28 summary:Several well-studied models of access to data samples, including statistical queries, local differential privacy and low-communication algorithms rely on queries that provide information about a function of a single sample. (For example, a statistical query (SQ) gives an estimate of $Ex_{x \sim D}[q(x)]$ for any choice of the query function $q$ mapping $X$ to the reals, where $D$ is an unknown data distribution over $X$.) Yet some data analysis algorithms rely on properties of functions that depend on multiple samples. Such algorithms would be naturally implemented using $k$-wise queries each of which is specified by a function $q$ mapping $X^k$ to the reals. Hence it is natural to ask whether algorithms using $k$-wise queries can solve learning problems more efficiently and by how much. Blum, Kalai and Wasserman (2003) showed that for any weak PAC learning problem over a fixed distribution, the complexity of learning with $k$-wise SQs is smaller than the (unary) SQ complexity by a factor of at most $2^k$. We show that for more general problems over distributions the picture is substantially richer. For every $k$, the complexity of distribution-independent PAC learning with $k$-wise queries can be exponentially larger than learning with $(k+1)$-wise queries. We then give two approaches for simulating a $k$-wise query using unary queries. The first approach exploits the structure of the problem that needs to be solved. It generalizes and strengthens (exponentially) the results of Blum et al.. It allows us to derive strong lower bounds for learning DNF formulas and stochastic constraint satisfaction problems that hold against algorithms using $k$-wise queries. The second approach exploits the $k$-party communication complexity of the $k$-wise query function. version:1
arxiv-1703-00060 | Achieving non-discrimination in prediction | http://arxiv.org/abs/1703.00060 | id:1703.00060 author:Lu Zhang, Yongkai Wu, Xintao Wu category:cs.LG stat.ML  published:2017-02-28 summary:Discrimination-aware classification is receiving an increasing attention in the data mining and machine learning fields. The data preprocessing methods for constructing a discrimination-free classifier remove discrimination from the training data, and learn the classifier from the cleaned data. However, there lacks of a theoretical guarantee for the performance of these methods. In this paper, we fill this theoretical gap by mathematically bounding the probability that the discrimination in predictions is within a given interval in terms of the given training data and classifier. In our analysis, we adopt the causal model for modeling the mechanisms in data generation, and formally defining discrimination in the population, in a dataset, and in the prediction. The theoretical results show that the fundamental assumption made by the data preprocessing methods is not correct. Finally, we develop a framework for constructing a discrimination-free classifier with a theoretical guarantee. version:1
arxiv-1703-00056 | Fair prediction with disparate impact: A study of bias in recidivism prediction instruments | http://arxiv.org/abs/1703.00056 | id:1703.00056 author:Alexandra Chouldechova category:stat.AP cs.CY stat.ML  published:2017-02-28 summary:Recidivism prediction instruments (RPI's) provide decision makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. While such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This paper discusses several fairness criteria that have recently been applied to assess the fairness of recidivism prediction instruments. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when a recidivism prediction instrument fails to satisfy the criterion of error rate balance. version:1
arxiv-1702-07956 | Generative Adversarial Active Learning | http://arxiv.org/abs/1702.07956 | id:1702.07956 author:Jia-Jie Zhu, José Bento category:cs.LG stat.ML  published:2017-02-25 summary:We propose a new active learning approach using Generative Adversarial Networks (GAN). Different from regular active learning, we adaptively synthesize training instances for querying to increase learning speed. Our approach outperforms random generation using GAN alone in active learning experiments. We demonstrate the effectiveness of the proposed algorithm in various datasets when compared to other algorithms. To the best our knowledge, this is the first active learning work using GAN. version:2
arxiv-1702-05860 | Robust Sparse Estimation Tasks in High Dimensions | http://arxiv.org/abs/1702.05860 | id:1702.05860 author:Jerry Li category:cs.LG cs.DS  published:2017-02-20 summary:In this paper we initiate the study of whether or not sparse estimation tasks can be performed efficiently in high dimensions, in the robust setting where an $\eps$-fraction of samples are corrupted adversarially. We study the natural robust version of two classical sparse estimation problems, namely, sparse mean estimation and sparse PCA in the spiked covariance model. For both of these problems, we provide the first efficient algorithms that provide non-trivial error guarantees in the presence of noise, using only a number of samples which is similar to the number required for these problems without noise. In particular, our sample complexities are sublinear in the ambient dimension $d$. Our work also suggests evidence for new computational-vs-statistical gaps for these problems (similar to those for sparse PCA without noise) which only arise in the presence of noise. version:2
arxiv-1703-00050 | SceneSeer: 3D Scene Design with Natural Language | http://arxiv.org/abs/1703.00050 | id:1703.00050 author:Angel X. Chang, Mihail Eric, Manolis Savva, Christopher D. Manning category:cs.GR cs.CL cs.HC  published:2017-02-28 summary:Designing 3D scenes is currently a creative task that requires significant expertise and effort in using complex 3D design interfaces. This effortful design process starts in stark contrast to the easiness with which people can use language to describe real and imaginary environments. We present SceneSeer: an interactive text to 3D scene generation system that allows a user to design 3D scenes using natural language. A user provides input text from which we extract explicit constraints on the objects that should appear in the scene. Given these explicit constraints, the system then uses a spatial knowledge base learned from an existing database of 3D scenes and 3D object models to infer an arrangement of the objects forming a natural scene matching the input description. Using textual commands the user can then iteratively refine the created scene by adding, removing, replacing, and manipulating objects. We evaluate the quality of 3D scenes generated by SceneSeer in a perceptual evaluation experiment where we compare against manually designed scenes and simpler baselines for 3D scene generation. We demonstrate how the generated scenes can be iteratively refined through simple natural language commands. version:1
arxiv-1703-00048 | Provable Optimal Algorithms for Generalized Linear Contextual Bandits | http://arxiv.org/abs/1703.00048 | id:1703.00048 author:Lihong Li, Yu Lu, Dengyong Zhou category:cs.LG cs.AI stat.ML  published:2017-02-28 summary:Contextual bandits are widely used in Internet services from news recommendation to advertising. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an $\tilde{O}(\sqrt{dT})$ regret over $T$ rounds with $d$ dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a $\sqrt{d}$ factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximum-likelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for the certain cases. version:1
arxiv-1703-00039 | A description length approach to determining the number of k-means clusters | http://arxiv.org/abs/1703.00039 | id:1703.00039 author:Hiromitsu Mizutani, Ryota Kanai category:stat.ML cs.LG  published:2017-02-28 summary:We present an asymptotic criterion to determine the optimal number of clusters in k-means. We consider k-means as data compression, and propose to adopt the number of clusters that minimizes the estimated description length after compression. Here we report two types of compression ratio based on two ways to quantify the description length of data after compression. This approach further offers a way to evaluate whether clusters obtained with k-means have a hierarchical structure by examining whether multi-stage compression can further reduce the description length. We applied our criteria to determine the number of clusters to synthetic data and empirical neuroimaging data to observe the behavior of the criteria across different types of data set and suitability of the two types of criteria for different datasets. We found that our method can offer reasonable clustering results that are useful for dimension reduction. While our numerical results revealed dependency of our criteria on the various aspects of dataset such as the dimensionality, the description length approach proposed here provides a useful guidance to determine the number of clusters in a principled manner when underlying properties of the data are unknown and only inferred from observation of data. version:1
arxiv-1702-08898 | Lipschitz Optimisation for Lipschitz Interpolation | http://arxiv.org/abs/1702.08898 | id:1702.08898 author:Jan-Peter Calliess category:cs.LG stat.ML  published:2017-02-28 summary:Techniques known as Nonlinear Set Membership prediction, Kinky Inference or Lipschitz Interpolation are fast and numerically robust approaches to nonparametric machine learning that have been proposed to be utilised in the context of system identification and learning-based control. They utilise presupposed Lipschitz properties in order to compute inferences over unobserved function values. Unfortunately, most of these approaches rely on exact knowledge about the input space metric as well as about the Lipschitz constant. Furthermore, existing techniques to estimate the Lipschitz constants from the data are not robust to noise or seem to be ad-hoc and typically are decoupled from the ultimate learning and prediction task. To overcome these limitations, we propose an approach for optimising parameters of the presupposed metrics by minimising validation set prediction errors. To avoid poor performance due to local minima, we propose to utilise Lipschitz properties of the optimisation objective to ensure global optimisation success. The resulting approach is a new flexible method for nonparametric black-box learning. We provide experimental evidence of the competitiveness of our approach on artificial as well as on real data. version:1
arxiv-1702-08892 | Bridging the Gap Between Value and Policy Based Reinforcement Learning | http://arxiv.org/abs/1702.08892 | id:1702.08892 author:Ofir Nachum, Mohammad Norouzi, Kelvin Xu, Dale Schuurmans category:cs.AI cs.LG stat.ML  published:2017-02-28 summary:We formulate a new notion of softmax temporal consistency that generalizes the standard hard-max Bellman consistency usually considered in value based reinforcement learning (RL). In particular, we show how softmax consistent action values correspond to optimal policies that maximize entropy regularized expected reward. More importantly, we establish that softmax consistent action values and the optimal policy must satisfy a mutual compatibility property that holds across any state-action subsequence. Based on this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes the total inconsistency measured along multi-step subsequences extracted from both both on and off policy traces. An experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmark tasks. version:1
arxiv-1702-08887 | Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning | http://arxiv.org/abs/1702.08887 | id:1702.08887 author:Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Philip. H. S. Torr, Pushmeet Kohli, Shimon Whiteson category:cs.AI cs.LG cs.MA  published:2017-02-28 summary:Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A key stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep RL relies. This paper proposes two methods that address this problem: 1) conditioning each agent's value function on a footprint that disambiguates the age of the data sampled from the replay memory and 2) using a multi-agent variant of importance sampling to naturally decay obsolete data. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL. version:1
arxiv-1702-08884 | Low-rank Label Propagation for Semi-supervised Learning with 100 Millions Samples | http://arxiv.org/abs/1702.08884 | id:1702.08884 author:Raphael Petegrosso, Wei Zhang, Zhuliu Li, Yousef Saad, Rui Kuang category:cs.LG  published:2017-02-28 summary:The success of semi-supervised learning crucially relies on the scalability to a huge amount of unlabelled data that are needed to capture the underlying manifold structure for better classification. Since computing the pairwise similarity between the training data is prohibitively expensive in most kinds of input data, currently, there is no general ready-to-use semi-supervised learning method/tool available for learning with tens of millions or more data points. In this paper, we adopted the idea of two low-rank label propagation algorithms, GLNP (Global Linear Neighborhood Propagation) and Kernel Nystr\"om Approximation, and implemented the parallelized version of the two algorithms accelerated with Nesterov's accelerated projected gradient descent for Big-data Label Propagation (BigLP). The parallel algorithms are tested on five real datasets ranging from 7000 to 10,000,000 in size and a simulation dataset of 100,000,000 samples. In the experiments, the implementation can scale up to datasets with 100,000,000 samples and hundreds of features and the algorithms also significantly improved the prediction accuracy when only a very small percentage of the data is labeled. The results demonstrate that the BigLP implementation is highly scalable to big data and effective in utilizing the unlabeled data for semi-supervised learning. version:1
arxiv-1702-08848 | Distributionally Robust Semi-supervised Learning | http://arxiv.org/abs/1702.08848 | id:1702.08848 author:Jose Blanchet, Yang Kang category:stat.ML  published:2017-02-28 summary:We propose a novel method for semi-supervised learning based on data-driven distributionally robust optimization (DRO) using optimal transport metrics. Our proposed method enhances generalization error by using the non-labeled data to restrict the support of the worst case distribution in our DRO formulation. We enable the implementation of the DRO formulation by proposing a stochastic gradient descent algorithm which allows to easily implement the training procedure. We demonstrate the improvement in generalization error in semi-supervised extensions of regularized logistic regression and square-root LASSO. Finally, we include a discussion on the large sample behavior of the optimal uncertainty region in the DRO formulation. Our discussion exposes important aspects such as the role of dimension reduction in semi-supervised learning. version:1
arxiv-1702-08840 | Efficient Learning for Crowdsourced Regression | http://arxiv.org/abs/1702.08840 | id:1702.08840 author:Jungseul Ok, Sewoong Oh, Jinwoo Shin, Yunhun Jang, Yung Yi category:cs.LG stat.ML  published:2017-02-28 summary:Crowdsourcing platforms emerged as popular venues for purchasing human intelligence at low cost for large volume of tasks. As many low-paid workers are prone to give noisy answers, one of the fundamental questions is how to identify more reliable workers and exploit this heterogeneity to infer the true answers. Despite significant research efforts for classification tasks with discrete answers, little attention has been paid to regression tasks where the answers take continuous values. We consider the task of recovering the position of target objects, and introduce a new probabilistic model capturing the heterogeneity of the workers. We propose the belief propagation (BP) algorithm for inferring the positions and prove that it achieves optimal mean squared error by comparing its performance to that of an oracle estimator. Our experimental results on synthetic datasets confirm our theoretical predictions. We further emulate a crowdsourcing system using PASCAL visual object classes datasets and show that de-noising the crowdsourced data using BP can significantly improve the performance for the downstream vision task. version:1
arxiv-1702-08835 | Deep Forest: Towards An Alternative to Deep Neural Networks | http://arxiv.org/abs/1702.08835 | id:1702.08835 author:Zhi-Hua Zhou, Ji Feng category:cs.LG stat.ML  published:2017-02-28 summary:In this paper, we propose gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train. Actually, even when gcForest is applied to different data from different domains, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient and scalable. In our experiments its training time running on a PC is comparable to that of deep neural networks running with GPU facilities, and the efficiency advantage may be more apparent because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require large-scale training data, gcForest can work well even when there are only small-scale training data. Moreover, as a tree-based approach, gcForest should be easier for theoretical analysis than deep neural networks. version:1
arxiv-1702-08833 | Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees | http://arxiv.org/abs/1702.08833 | id:1702.08833 author:Daniel Zoran, Balaji Lakshminarayanan, Charles Blundell category:cs.LG  published:2017-02-28 summary:Nearest neighbor (kNN) methods have been gaining popularity in recent years in light of advances in hardware and efficiency of algorithms. There is a plethora of methods to choose from today, each with their own advantages and disadvantages. One requirement shared between all kNN based methods is the need for a good representation and distance measure between samples. We introduce a new method called differentiable boundary tree which allows for learning deep kNN representations. We build on the recently proposed boundary tree algorithm which allows for efficient nearest neighbor classification, regression and retrieval. By modelling traversals in the tree as stochastic events, we are able to form a differentiable cost function which is associated with the tree's predictions. Using a deep neural network to transform the data and back-propagating through the tree allows us to learn good representations for kNN methods. We demonstrate that our method is able to learn suitable representations allowing for very efficient trees with a clearly interpretable structure. version:1
arxiv-1702-08811 | Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning | http://arxiv.org/abs/1702.08811 | id:1702.08811 author:Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschläger, Susanne Saminger-Platz category:stat.ML cs.LG  published:2017-02-28 summary:The learning of domain-invariant representations in the context of domain adaptation with neural networks is considered. We propose a new regularization method that minimizes the domain-specific latent feature representations directly in the hidden activation space. Although some standard distribution matching approaches exist that can be interpreted as the matching of weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit order-wise matching of higher order moments has not been considered before. We propose to match the higher order central moments of probability distributions by means of order-wise moment differences. Our model does not require computationally expensive distance and kernel matrix computations. We utilize the equivalent representation of probability distributions by moment sequences to define a new distance function, called Central Moment Discrepancy (CMD). We prove that CMD is a metric on the set of probability distributions on a compact interval. We further prove that convergence of probability distributions on compact intervals w.r.t. the new metric implies convergence in distribution of the respective random variables. We test our approach on two different benchmark data sets for object recognition (Office) and sentiment analysis of product reviews (Amazon reviews). CMD achieves a new state-of-the-art performance on most domain adaptation tasks of Office and outperforms networks trained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity analysis shows that the new approach is stable w.r.t. parameter changes in a certain interval. The source code of the experiments is publicly available. version:1
arxiv-1702-07398 | Deep Nonparametric Estimation of Discrete Conditional Distributions via Smoothed Dyadic Partitioning | http://arxiv.org/abs/1702.07398 | id:1702.07398 author:Wesley Tansey, Karl Pichotta, James G. Scott category:stat.ML  published:2017-02-23 summary:We present an approach to deep estimation of discrete conditional probability distributions. Such models have several applications, including generative modeling of audio, image, and video data. Our approach combines two main techniques: dyadic partitioning and graph-based smoothing of the discrete space. By recursively decomposing each dimension into a series of binary splits and smoothing over the resulting distribution using graph-based trend filtering, we impose a strict structure to the model and achieve much higher sample efficiency. We demonstrate the advantages of our model through a series of benchmarks on both synthetic and real-world datasets, in some cases reducing the error by nearly half in comparison to other popular methods in the literature. All of our models are implemented in Tensorflow and publicly available at https://github.com/tansey/sdp . version:2
arxiv-1702-08798 | Unsupervised Triplet Hashing for Fast Image Retrieval | http://arxiv.org/abs/1702.08798 | id:1702.08798 author:Shanshan Huang, Yichao Xiong, Ya Zhang, Jia Wang category:cs.CV  published:2017-02-28 summary:Hashing has played a pivotal role in large-scale image retrieval. With the development of Convolutional Neural Network (CNN), hashing learning has shown great promise. But existing methods are mostly tuned for classification, which are not optimized for retrieval tasks, especially for instance-level retrieval. In this study, we propose a novel hashing method for large-scale image retrieval. Considering the difficulty in obtaining labeled datasets for image retrieval task in large scale, we propose a novel CNN-based unsupervised hashing method, namely Unsupervised Triplet Hashing (UTH). The unsupervised hashing network is designed under the following three principles: 1) more discriminative representations for image retrieval; 2) minimum quantization loss between the original real-valued feature descriptors and the learned hash codes; 3) maximum information entropy for the learned hash codes. Extensive experiments on CIFAR-10, MNIST and In-shop datasets have shown that UTH outperforms several state-of-the-art unsupervised hashing methods in terms of retrieval accuracy. version:1
arxiv-1702-08791 | Robust Budget Allocation via Continuous Submodular Functions | http://arxiv.org/abs/1702.08791 | id:1702.08791 author:Matthew Staib, Stefanie Jegelka category:cs.LG cs.AI cs.DS cs.SI math.OC  published:2017-02-28 summary:The optimal allocation of resources for maximizing influence, spread of information or coverage, has gained attention in the past years, in particular in machine learning and data mining. But, in applications, the parameters of the problem are rarely known exactly, and using wrong parameters can lead to undesirable outcomes. We hence revisit a continuous version of the Budget Allocation or Bipartite Influence Maximization problem introduced by Alon et al. (2012) from a robust optimization perspective, where an adversary may choose the least favorable parameters within a confidence set. The resulting problem is a nonconvex-concave saddle point problem (or game). We show that this nonconvex problem can be solved exactly by leveraging connections to continuous submodular functions, and by solving a constrained submodular minimization problem. Although constrained submodular minimization is hard in general, here, we establish conditions under which such a problem can be solved to arbitrary precision $\epsilon$. version:1
arxiv-1702-03814 | Bilateral Multi-Perspective Matching for Natural Language Sentences | http://arxiv.org/abs/1702.03814 | id:1702.03814 author:Zhiguo Wang, Wael Hamza, Radu Florian category:cs.AI cs.CL  published:2017-02-13 summary:Natural language sentence matching is a fundamental technology for a variety of tasks. Previous approaches either match sentences from a single direction or only apply single granular (word-by-word or sentence-by-sentence) matching. In this work, we propose a bilateral multi-perspective matching (BiMPM) model under the "matching-aggregation" framework. Given two sentences $P$ and $Q$, our model first encodes them with a BiLSTM encoder. Next, we match the two encoded sentences in two directions $P \rightarrow Q$ and $P \leftarrow Q$. In each matching direction, each time step of one sentence is matched against all time-steps of the other sentence from multiple perspectives. Then, another BiLSTM layer is utilized to aggregate the matching results into a fix-length matching vector. Finally, based on the matching vector, the decision is made through a fully connected layer. We evaluate our model on three tasks: paraphrase identification, natural language inference and answer sentence selection. Experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks. version:2
arxiv-1702-08780 | MILD: Multi-Index hashing for Loop closure Detection | http://arxiv.org/abs/1702.08780 | id:1702.08780 author:Lei Han, Lu Fang category:cs.CV  published:2017-02-28 summary:Loop Closure Detection (LCD) has been proved to be extremely useful in global consistent visual Simultaneously Localization and Mapping (SLAM) and appearance-based robot relocalization. Methods exploiting binary features in bag of words representation have recently gained a lot of popularity for their efficiency, but suffer from low recall due to the inherent drawback that high dimensional binary feature descriptors lack well-defined centroids. In this paper, we propose a realtime LCD approach called MILD (Multi-Index Hashing for Loop closure Detection), in which image similarity is measured by feature matching directly to achieve high recall without introducing extra computational complexity with the aid of Multi-Index Hashing (MIH). A theoretical analysis of the approximate image similarity measurement using MIH is presented, which reveals the trade-off between efficiency and accuracy from a probabilistic perspective. Extensive comparisons with state-of-the-art LCD methods demonstrate the superiority of MILD in both efficiency and accuracy. version:1
