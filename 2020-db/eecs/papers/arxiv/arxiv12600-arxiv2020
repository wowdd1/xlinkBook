arxiv-1510-06375 | Towards Direct Medical Image Analysis without Segmentation | http://arxiv.org/abs/1510.06375 | id:1510.06375 author:Xiantong Zhen, Shuo Li category:cs.CV  published:2015-10-21 summary:Direct methods have recently emerged as an effective and efficient tool in automated medical image analysis and become a trend to solve diverse challenging tasks in clinical practise. Compared to traditional methods, direct methods are of much more clinical significance by straightly targeting to the final clinical goal rather than relying on any intermediate steps. These intermediate steps, e.g., segmentation, registration and tracking, are actually not necessary and only limited to very constrained tasks far from being used in practical clinical applications; moreover they are computationally expensive and time-consuming, which causes a high waste of research resources. The advantages of direct methods stem from \textbf{1)} removal of intermediate steps, e.g., segmentation, tracking and registration; \textbf{2)} avoidance of user inputs and initialization; \textbf{3)} reformulation of conventional challenging problems, e.g., inversion problem, with efficient solutions. version:1
arxiv-1510-06356 | Application of Quantum Annealing to Training of Deep Neural Networks | http://arxiv.org/abs/1510.06356 | id:1510.06356 author:Steven H. Adachi, Maxwell P. Henderson category:quant-ph cs.LG stat.ML  published:2015-10-21 summary:In Deep Learning, a well-known approach for training a Deep Neural Network starts by training a generative Deep Belief Network model, typically using Contrastive Divergence (CD), then fine-tuning the weights using backpropagation or other discriminative techniques. However, the generative training can be time-consuming due to the slow mixing of Gibbs sampling. We investigated an alternative approach that estimates model expectations of Restricted Boltzmann Machines using samples from a D-Wave quantum annealing machine. We tested this method on a coarse-grained version of the MNIST data set. In our tests we found that the quantum sampling-based training approach achieves comparable or better accuracy with significantly fewer iterations of generative training than conventional CD-based training. Further investigation is needed to determine whether similar improvements can be achieved for other data sets, and to what extent these improvements can be attributed to quantum effects. version:1
arxiv-1502-01988 | Computational and Statistical Boundaries for Submatrix Localization in a Large Noisy Matrix | http://arxiv.org/abs/1502.01988 | id:1502.01988 author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST stat.ML stat.TH  published:2015-02-06 summary:The interplay between computational efficiency and statistical accuracy in high-dimensional inference has drawn increasing attention in the literature. In this paper, we study computational and statistical boundaries for submatrix localization. Given one observation of (one or multiple non-overlapping) signal submatrix (of magnitude $\lambda$ and size $k_m \times k_n$) contaminated with a noise matrix (of size $m \times n$), we establish two transition thresholds for the signal to noise $\lambda/\sigma$ ratio in terms of $m$, $n$, $k_m$, and $k_n$. The first threshold, $\sf SNR_c$, corresponds to the computational boundary. Below this threshold, it is shown that no polynomial time algorithm can succeed in identifying the submatrix, under the \textit{hidden clique hypothesis}. We introduce adaptive linear time spectral algorithms that identify the submatrix with high probability when the signal strength is above the threshold $\sf SNR_c$. The second threshold, $\sf SNR_s$, captures the statistical boundary, below which no method can succeed with probability going to one in the minimax sense. The exhaustive search method successfully finds the submatrix above this threshold. The results show an interesting phenomenon that $\sf SNR_c$ is always significantly larger than $\sf SNR_s$, which implies an essential gap between statistical optimality and computational efficiency for submatrix localization. version:2
arxiv-1510-06342 | Prevalence and recoverability of syntactic parameters in sparse distributed memories | http://arxiv.org/abs/1510.06342 | id:1510.06342 author:Jeong Joon Park, Ronnel Boettcher, Andrew Zhao, Alex Mun, Kevin Yuh, Vibhor Kumar, Matilde Marcolli category:cs.CL cs.IT math.IT 91F20  published:2015-10-21 summary:We propose a new method, based on Sparse Distributed Memory (Kanerva Networks), for studying dependency relations between different syntactic parameters in the Principles and Parameters model of Syntax. We store data of syntactic parameters of world languages in a Kanerva Network and we check the recoverability of corrupted parameter data from the network. We find that different syntactic parameters have different degrees of recoverability. We identify two different effects: an overall underlying relation between the prevalence of parameters across languages and their degree of recoverability, and a finer effect that makes some parameters more easily recoverable beyond what their prevalence would indicate. We interpret a higher recoverability for a syntactic parameter as an indication of the existence of a dependency relation, through which the given parameter can be determined using the remaining uncorrupted data. version:1
arxiv-1510-06299 | GLASSES: Relieving The Myopia Of Bayesian Optimisation | http://arxiv.org/abs/1510.06299 | id:1510.06299 author:Javier González, Michael Osborne, Neil D. Lawrence category:stat.ML  published:2015-10-21 summary:We present GLASSES: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. The majority of global optimisation approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, GLASSES, permits the consideration of dozens of evaluations into the future. This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss.We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests. version:1
arxiv-1411-2374 | Similarity Learning for High-Dimensional Sparse Data | http://arxiv.org/abs/1411.2374 | id:1411.2374 author:Kuan Liu, Aurélien Bellet, Fei Sha category:cs.LG stat.ML  published:2014-11-10 summary:A good measure of similarity between data points is crucial to many tasks in machine learning. Similarity and metric learning methods learn such measures automatically from data, but they do not scale well respect to the dimensionality of the data. In this paper, we propose a method that can learn efficiently similarity measure from high-dimensional sparse data. The core idea is to parameterize the similarity measure as a convex combination of rank-one matrices with specific sparsity structures. The parameters are then optimized with an approximate Frank-Wolfe procedure to maximally satisfy relative similarity constraints on the training data. Our algorithm greedily incorporates one pair of features at a time into the similarity measure, providing an efficient way to control the number of active features and thus reduce overfitting. It enjoys very appealing convergence guarantees and its time and memory complexity depends on the sparsity of the data instead of the dimension of the feature space. Our experiments on real-world high-dimensional datasets demonstrate its potential for classification, dimensionality reduction and data exploration. version:2
arxiv-1510-03623 | Elastic regularization in restricted Boltzmann machines: Dealing with $p\gg N$ | http://arxiv.org/abs/1510.03623 | id:1510.03623 author:Sai Zhang category:cs.LG  published:2015-10-13 summary:Restricted Boltzmann machines (RBMs) are endowed with the universal power of modeling (binary) joint distributions. Meanwhile, as a result of their confining network structure, training RBMs confronts less difficulties (compared with more complicated models, e.g., Boltzmann machines) when dealing with approximation and inference issues. However, in certain computational biology scenarios, such as the cancer data analysis, employing RBMs to model data features may lose its efficacy due to the "$p\gg N$" problem, in which the number of features/predictors is much larger than the sample size. The "$p\gg N$" problem puts the bias-variance trade-off in a more crucial place when designing statistical learning methods. In this manuscript, we try to address this problem by proposing a novel RBM model, called elastic restricted Boltzmann machine (eRBM), which incorporates the elastic regularization term into the likelihood/cost function. We provide several theoretical analysis on the superiority of our model. Furthermore, attributed to the classic contrastive divergence (CD) algorithm, eRBMs can be trained efficiently. Our novel model is a promising method for future cancer data analysis. version:2
arxiv-1510-06168 | Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network | http://arxiv.org/abs/1510.06168 | id:1510.06168 author:Peilu Wang, Yao Qian, Frank K. Soong, Lei He, Hai Zhao category:cs.CL  published:2015-10-21 summary:Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents. While word embedding has been demoed as a powerful representation for characterizing the statistical properties of natural language. In this study, we propose to use BLSTM-RNN with word embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is achieved. Without using morphological features, this approach can also achieve a good performance comparable with the Stanford POS tagger. version:1
arxiv-1512-04582 | Interactive Volumetry Of Liver Ablation Zones | http://arxiv.org/abs/1512.04582 | id:1512.04582 author:Jan Egger, Harald Busse, Philipp Brandmaier, Daniel Seider, Matthias Gawlitza, Steffen Strocka, Philip Voglreiter, Mark Dokter, Michael Hofmann, Bernhard Kainz, Alexander Hann, Xiaojun Chen, Tuomas Alhonnoro, Mika Pollari, Dieter Schmalstieg, Michael Moche category:cs.CV cs.GR cs.HC physics.med-ph  published:2015-10-21 summary:Percutaneous radiofrequency ablation (RFA) is a minimally invasive technique that destroys cancer cells by heat. The heat results from focusing energy in the radiofrequency spectrum through a needle. Amongst others, this can enable the treatment of patients who are not eligible for an open surgery. However, the possibility of recurrent liver cancer due to incomplete ablation of the tumor makes post-interventional monitoring via regular follow-up scans mandatory. These scans have to be carefully inspected for any conspicuousness. Within this study, the RF ablation zones from twelve post-interventional CT acquisitions have been segmented semi-automatically to support the visual inspection. An interactive, graph-based contouring approach, which prefers spherically shaped regions, has been applied. For the quantitative and qualitative analysis of the algorithm's results, manual slice-by-slice segmentations produced by clinical experts have been used as the gold standard (which have also been compared among each other). As evaluation metric for the statistical validation, the Dice Similarity Coefficient (DSC) has been calculated. The results show that the proposed tool provides lesion segmentation with sufficient accuracy much faster than manual segmentation. The visual feedback and interactivity make the proposed tool well suitable for the clinical workflow. version:1
arxiv-1406-0764 | Constructing Dynamic Treatment Regimes in Infinite-Horizon Settings | http://arxiv.org/abs/1406.0764 | id:1406.0764 author:Ashkan Ertefaie category:stat.ME stat.ML  published:2014-06-03 summary:The application of existing methods for constructing optimal dynamic treatment regimes is limited to cases where investigators are interested in optimizing a utility function over a fixed period of time (finite horizon). In this manuscript, we develop an inferential procedure based on temporal difference residuals for optimal dynamic treatment regimes in infinite-horizon settings, where there is no a priori fixed end of follow-up point. The proposed method can be used to determine the optimal regime in chronic diseases where patients are monitored and treated throughout their life. We derive large sample results necessary for conducting inference. We also simulate a cohort of patients with diabetes to mimic the third wave of the National Health and Nutrition Examination Survey, and we examine the performance of the proposed method in controlling the level of hemoglobin A1c. Supplementary materials for this article are available online. version:2
arxiv-1510-06138 | Multiple co-clustering based on nonparametric mixture models with heterogeneous marginal distributions | http://arxiv.org/abs/1510.06138 | id:1510.06138 author:Tomoki Tokuda, Junichiro Yoshimoto, Yu Shimizu, Shigeru Toki, Go Okada, Masahiro Takamura, Tetsuya Yamamoto, Shinpei Yoshimura, Yasumasa Okamoto, Shigeto Yamawaki, Kenji Doya category:stat.ML  published:2015-10-21 summary:We propose a novel method for multiple clustering that assumes a co-clustering structure (partitions in both rows and columns of the data matrix) in each view. The new method is applicable to high-dimensional data. It is based on a nonparametric Bayesian approach in which the number of views and the number of feature-/subject clusters are inferred in a data-driven manner. We simultaneously model different distribution families, such as Gaussian, Poisson, and multinomial distributions in each cluster block. This makes our method applicable to datasets consisting of both numerical and categorical variables, which biomedical data typically do. Clustering solutions are based on variational inference with mean field approximation. We apply the proposed method to synthetic and real data, and show that our method outperforms other multiple clustering methods both in recovering true cluster structures and in computation time. Finally, we apply our method to a depression dataset with no true cluster structure available, from which useful inferences are drawn about possible clustering structures of the data. version:1
arxiv-1510-06112 | Dimensionality Reduction for Binary Data through the Projection of Natural Parameters | http://arxiv.org/abs/1510.06112 | id:1510.06112 author:Andrew J. Landgraf, Yoonkyung Lee category:stat.ML stat.ME  published:2015-10-21 summary:Principal component analysis (PCA) for binary data, known as logistic PCA, has become a popular alternative to dimensionality reduction of binary data. It is motivated as an extension of ordinary PCA by means of a matrix factorization, akin to the singular value decomposition, that maximizes the Bernoulli log-likelihood. We propose a new formulation of logistic PCA which extends Pearson's formulation of a low dimensional data representation with minimum error to binary data. Our formulation does not require a matrix factorization, as previous methods do, but instead looks for projections of the natural parameters from the saturated model. Due to this difference, the number of parameters does not grow with the number of observations and the principal component scores on new data can be computed with simple matrix multiplication. We derive explicit solutions for data matrices of special structure and provide computationally efficient algorithms for solving for the principal component loadings. Through simulation experiments and an analysis of medical diagnoses data, we compare our formulation of logistic PCA to the previous formulation as well as ordinary PCA to demonstrate its benefits. version:1
arxiv-1510-06093 | Content adaptive screen image scaling | http://arxiv.org/abs/1510.06093 | id:1510.06093 author:Yao Zhai, Qifei Wang, Yan Lu, Shipeng Li category:cs.CV  published:2015-10-21 summary:This paper proposes an efficient content adaptive screen image scaling scheme for the real-time screen applications like remote desktop and screen sharing. In the proposed screen scaling scheme, a screen content classification step is first introduced to classify the screen image into text and pictorial regions. Afterward, we propose an adaptive shift linear interpolation algorithm to predict the new pixel values with the shift offset adapted to the content type of each pixel. The shift offset for each screen content type is offline optimized by minimizing the theoretical interpolation error based on the training samples respectively. The proposed content adaptive screen image scaling scheme can achieve good visual quality and also keep the low complexity for real-time applications. version:1
arxiv-1510-06083 | Regularization vs. Relaxation: A conic optimization perspective of statistical variable selection | http://arxiv.org/abs/1510.06083 | id:1510.06083 author:Hongbo Dong, Kun Chen, Jeff Linderoth category:cs.LG math.NA math.OC stat.ML 90C22  90C47  62J07 G.1.3; G.1.6  published:2015-10-20 summary:Variable selection is a fundamental task in statistical data analysis. Sparsity-inducing regularization methods are a popular class of methods that simultaneously perform variable selection and model estimation. The central problem is a quadratic optimization problem with an l0-norm penalty. Exactly enforcing the l0-norm penalty is computationally intractable for larger scale problems, so dif- ferent sparsity-inducing penalty functions that approximate the l0-norm have been introduced. In this paper, we show that viewing the problem from a convex relaxation perspective offers new insights. In particular, we show that a popular sparsity-inducing concave penalty function known as the Minimax Concave Penalty (MCP), and the reverse Huber penalty derived in a recent work by Pilanci, Wainwright and Ghaoui, can both be derived as special cases of a lifted convex relaxation called the perspective relaxation. The optimal perspective relaxation is a related minimax problem that balances the overall convexity and tightness of approximation to the l0 norm. We show it can be solved by a semidefinite relaxation. Moreover, a probabilistic interpretation of the semidefinite relaxation reveals connections with the boolean quadric polytope in combinatorial optimization. Finally by reformulating the l0-norm pe- nalized problem as a two-level problem, with the inner level being a Max-Cut problem, our proposed semidefinite relaxation can be realized by replacing the inner level problem with its semidefinite relaxation studied by Goemans and Williamson. This interpretation suggests using the Goemans-Williamson rounding procedure to find approximate solutions to the l0-norm penalized problem. Numerical experiments demonstrate the tightness of our proposed semidefinite relaxation, and the effectiveness of finding approximate solutions by Goemans-Williamson rounding. version:1
arxiv-1509-00117 | Discovery Radiomics for Computed Tomography Cancer Detection | http://arxiv.org/abs/1509.00117 | id:1509.00117 author:Devinder Kumar, Mohammad Javad Shafiee, Audrey G. Chung, Farzad Khalvati, Masoom A. Haider, Alexander Wong category:cs.CV  published:2015-09-01 summary:Objective: Lung cancer is the leading cause for cancer related deaths. As such, there is an urgent need for a streamlined process that can allow radiologists to provide diagnosis with greater efficiency and accuracy. A powerful tool to do this is radiomics. Method: In this study, we take the idea of radiomics one step further by introducing the concept of discovery radiomics for lung cancer detection using CT imaging data. Rather than using pre-defined, hand-engineered feature models as with current radiomics-driven methods, we discover custom radiomic sequencers that can generate radiomic sequences consisting of abstract imaging-based features tailored for characterizing lung tumour phenotype. In this study, we realize these custom radiomic sequencers as deep convolutional sequencers using a deep convolutional neural network learning architecture based on a wealth of CT imaging data. Results: To illustrate the prognostic power and effectiveness of the radiomic sequences produced by the discovered sequencer, we perform a classification between malignant and benign lesions from 93 patients with diagnostic data from the LIDC-IDRI dataset. Using the clinically provided diagnostic data as ground truth, proposed framework provided an average accuracy of 77.52% via 10-fold cross-validation with a sensitivity of 79.06% and specificity of 76.11%. We also perform quantitative analysis to establish the effectiveness of the radiomics sequences. Conclusion: The proposed framework outperforms the state-of-the art approach for lung lesion classification. Significance: These results illustrate the potential for the proposed discovery radiomics approach in aiding radiologists in improving screening efficiency and accuracy. version:2
arxiv-1510-05981 | A latent shared-component generative model for real-time disease surveillance using Twitter data | http://arxiv.org/abs/1510.05981 | id:1510.05981 author:Roberto C. S. N. P. Souza, Denise E. F de Brito, Renato M. Assunção, Wagner Meira Jr category:cs.SI stat.ML  published:2015-10-20 summary:Exploiting the large amount of available data for addressing relevant social problems has been one of the key challenges in data mining. Such efforts have been recently named "data science for social good" and attracted the attention of several researchers and institutions. We give a contribution in this objective in this paper considering a difficult public health problem, the timely monitoring of dengue epidemics in small geographical areas. We develop a generative simple yet effective model to connect the fluctuations of disease cases and disease-related Twitter posts. We considered a hidden Markov process driving both, the fluctuations in dengue reported cases and the tweets issued in each region. We add a stable but random source of tweets to represent the posts when no disease cases are recorded. The model is learned through a Markov chain Monte Carlo algorithm that produces the posterior distribution of the relevant parameters. Using data from a significant number of large Brazilian towns, we demonstrate empirically that our model is able to predict well the next weeks of the disease counts using the tweets and disease cases jointly. version:1
arxiv-1510-05976 | Transductive Optimization of Top k Precision | http://arxiv.org/abs/1510.05976 | id:1510.05976 author:Li-Ping Liu, Thomas G. Dietterich, Nan Li, Zhi-Hua Zhou category:cs.LG  published:2015-10-20 summary:Consider a binary classification problem in which the learner is given a labeled training set, an unlabeled test set, and is restricted to choosing exactly $k$ test points to output as positive predictions. Problems of this kind---{\it transductive precision@$k$}---arise in information retrieval, digital advertising, and reserve design for endangered species. Previous methods separate the training of the model from its use in scoring the test points. This paper introduces a new approach, Transductive Top K (TTK), that seeks to minimize the hinge loss over all training instances under the constraint that exactly $k$ test instances are predicted as positive. The paper presents two optimization methods for this challenging problem. Experiments and analysis confirm the importance of incorporating the knowledge of $k$ into the learning process. Experimental evaluations of the TTK approach show that the performance of TTK matches or exceeds existing state-of-the-art methods on 7 UCI datasets and 3 reserve design problem instances. version:1
arxiv-1409-4326 | Computing the Stereo Matching Cost with a Convolutional Neural Network | http://arxiv.org/abs/1409.4326 | id:1409.4326 author:Jure Žbontar, Yann LeCun category:cs.CV cs.LG cs.NE  published:2014-09-15 summary:We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset. version:2
arxiv-1506-04448 | Fast and Guaranteed Tensor Decomposition via Sketching | http://arxiv.org/abs/1506.04448 | id:1506.04448 author:Yining Wang, Hsiao-Yu Tung, Alexander Smola, Animashree Anandkumar category:stat.ML cs.LG  published:2015-06-14 summary:Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized computation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic modeling and obtain competitive results. version:2
arxiv-1508-05151 | Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation | http://arxiv.org/abs/1508.05151 | id:1508.05151 author:Christian Bailer, Bertram Taetz, Didier Stricker category:cs.CV I.4.8  published:2015-08-21 summary:Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this paper we present a dense correspondence field approach that is much less outlier prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach is conceptually novel as it does not require explicit regularization, smoothing (like median filtering) or a new data term, but solely our novel purely data based search strategy that finds most inliers (even for small objects), while it effectively avoids finding outliers. Moreover, we present novel enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than state-of-the-art descriptor matching techniques. We do so by initializing EpicFlow (so far the best method on MPI-Sintel) with our Flow Fields instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI and Middlebury. version:2
arxiv-1510-05879 | What's the point? Frame-wise Pointing Gesture Recognition with Latent-Dynamic Conditional Random Fields | http://arxiv.org/abs/1510.05879 | id:1510.05879 author:Christian Wittner, Boris Schauerte, Rainer Stiefelhagen category:cs.HC cs.CV cs.RO  published:2015-10-20 summary:We use Latent-Dynamic Conditional Random Fields to perform skeleton-based pointing gesture classification at each time instance of a video sequence, where we achieve a frame-wise pointing accuracy of roughly 83%. Subsequently, we determine continuous time sequences of arbitrary length that form individual pointing gestures and this way reliably detect pointing gestures at a false positive detection rate of 0.63%. version:1
arxiv-1504-05006 | Partition MCMC for inference on acyclic digraphs | http://arxiv.org/abs/1504.05006 | id:1504.05006 author:Jack Kuipers, Giusi Moffa category:stat.ML  published:2015-04-20 summary:Acyclic digraphs are the underlying representation of Bayesian networks, a widely used class of probabilistic graphical models. Learning the underlying graph from data is a way of gaining insights about the structural properties of a domain. Structure learning forms one of the inference challenges of statistical graphical models. MCMC methods, notably structure MCMC, to sample graphs from the posterior distribution given the data are probably the only viable option for Bayesian model averaging. Score modularity and restrictions on the number of parents of each node allow the graphs to be grouped into larger collections, which can be scored as a whole to improve the chain's convergence. Current examples of algorithms taking advantage of grouping are the biased order MCMC, which acts on the alternative space of permuted triangular matrices, and non ergodic edge reversal moves. Here we propose a novel algorithm, which employs the underlying combinatorial structure of DAGs to define a new grouping. As a result convergence is improved compared to structure MCMC, while still retaining the property of producing an unbiased sample. Finally the method can be combined with edge reversal moves to improve the sampler further. version:2
arxiv-1510-05822 | Sequential Score Adaptation with Extreme Value Theory for Robust Railway Track Inspection | http://arxiv.org/abs/1510.05822 | id:1510.05822 author:Xavier Gibert, Vishal M. Patel, Rama Chellappa category:cs.CV  published:2015-10-20 summary:Periodic inspections are necessary to keep railroad tracks in state of good repair and prevent train accidents. Automatic track inspection using machine vision technology has become a very effective inspection tool. Because of its non-contact nature, this technology can be deployed on virtually any railway vehicle to continuously survey the tracks and send exception reports to track maintenance personnel. However, as appearance and imaging conditions vary, false alarm rates can dramatically change, making it difficult to select a good operating point. In this paper, we use extreme value theory (EVT) within a Bayesian framework to optimally adjust the sensitivity of anomaly detectors. We show that by approximating the lower tail of the probability density function (PDF) of the scores with an Exponential distribution (a special case of the Generalized Pareto distribution), and using the Gamma conjugate prior learned from the training data, it is possible to reduce the variability in false alarm rate and improve the overall performance. This method has shown an increase in the defect detection rate of rail fasteners in the presence of clutter (at PFA 0.1%) from 95.40% to 99.26% on the 85-mile Northeast Corridor (NEC) 2012-2013 concrete tie dataset. version:1
arxiv-1509-00111 | Discovery Radiomics for Multi-Parametric MRI Prostate Cancer Detection | http://arxiv.org/abs/1509.00111 | id:1509.00111 author:Audrey G. Chung, Mohammad Javad Shafiee, Devinder Kumar, Farzad Khalvati, Masoom A. Haider, Alexander Wong category:cs.CV physics.med-ph q-bio.QM  published:2015-09-01 summary:Prostate cancer is the most diagnosed form of cancer in Canadian men, and is the third leading cause of cancer death. Despite these statistics, prognosis is relatively good with a sufficiently early diagnosis, making fast and reliable prostate cancer detection crucial. As imaging-based prostate cancer screening, such as magnetic resonance imaging (MRI), requires an experienced medical professional to extensively review the data and perform a diagnosis, radiomics-driven methods help streamline the process and has the potential to significantly improve diagnostic accuracy and efficiency, and thus improving patient survival rates. These radiomics-driven methods currently rely on hand-crafted sets of quantitative imaging-based features, which are selected manually and can limit their ability to fully characterize unique prostate cancer tumour phenotype. In this study, we propose a novel \textit{discovery radiomics} framework for generating custom radiomic sequences tailored for prostate cancer detection. Discovery radiomics aims to uncover abstract imaging-based features that capture highly unique tumour traits and characteristics beyond what can be captured using predefined feature models. In this paper, we discover new custom radiomic sequencers for generating new prostate radiomic sequences using multi-parametric MRI data. We evaluated the performance of the discovered radiomic sequencer against a state-of-the-art hand-crafted radiomic sequencer for computer-aided prostate cancer detection with a feedforward neural network using real clinical prostate multi-parametric MRI data. Results for the discovered radiomic sequencer demonstrate good performance in prostate cancer detection and clinical decision support relative to the hand-crafted radiomic sequencer. The use of discovery radiomics shows potential for more efficient and reliable automatic prostate cancer detection. version:3
arxiv-1510-06024 | Robust Semi-Supervised Classification for Multi-Relational Graphs | http://arxiv.org/abs/1510.06024 | id:1510.06024 author:Junting Ye, Leman Akoglu category:cs.LG  published:2015-10-19 summary:Graph-regularized semi-supervised learning has been used effectively for classification when (i) instances are connected through a graph, and (ii) labeled data is scarce. If available, using multiple relations (or graphs) between the instances can improve the prediction performance. On the other hand, when these relations have varying levels of veracity and exhibit varying relevance for the task, very noisy and/or irrelevant relations may deteriorate the performance. As a result, an effective weighing scheme needs to be put in place. In this work, we propose a robust and scalable approach for multi-relational graph-regularized semi-supervised classification. Under a convex optimization scheme, we simultaneously infer weights for the multiple graphs as well as a solution. We provide a careful analysis of the inferred weights, based on which we devise an algorithm that filters out irrelevant and noisy graphs and produces weights proportional to the informativeness of the remaining graphs. Moreover, the proposed method is linearly scalable w.r.t. the number of edges in the union of the multiple graphs. Through extensive experiments we show that our method yields superior results under different noise models, and under increasing number of noisy graphs and intensity of noise, as compared to a list of baselines and state-of-the-art approaches. version:1
arxiv-1510-05705 | Single Memristor Logic Gates: From NOT to a Full Adder | http://arxiv.org/abs/1510.05705 | id:1510.05705 author:Ella Gale category:cs.ET cond-mat.mes-hall cs.NE 03B-02  68U02  published:2015-10-19 summary:Memristors have been suggested as a novel route to neuromorphic computing based on the similarity between them and neurons (specifically synapses and ion pumps). The d.c. action of the memristor is a current spike which imparts a short-term memory to the device. Here it is demonstrated that this short-term memory works exactly like habituation (e.g. in \emph{Aplysia}). We elucidate the physical rules, based on energy conservation, governing the interaction of these current spikes: summation, `bounce-back', directionality and `diminishing returns'. Using these rules, we introduce 4 different logical systems to implement sequential logic in the memristor and demonstrate how sequential logic works by instantiating a NOT gate, an AND gate, an XOR gate and a Full Adder with a single memristor. The Full Adder makes use of the memristor's short-term memory to add together three binary values and outputs the sum, the carry digit and even the order they were input in. A memristor full adder also outputs the arithmetical sum of bits, allowing for a logically (but not physically) reversible system. Essentially, we can replace an input/output port with an extra time-step, allowing a single memristor to do a hither-to unexpectedly large amount of computation. This makes up for the memristor's slow operation speed and may relate to how neurons do a similarly-large computation with such slow operations speeds. We propose that using spiking logic, either in gates or as neuron-analogues, with plastic rewritable connections between them, would allow the building of a neuromorphic computer. version:1
arxiv-1506-06796 | When slower is faster | http://arxiv.org/abs/1506.06796 | id:1506.06796 author:Carlos Gershenson, Dirk Helbing category:nlin.AO cond-mat.dis-nn cs.NE physics.soc-ph q-bio.QM  published:2015-06-22 summary:The slower is faster (SIF) effect occurs when a system performs worse as its components try to do better. Thus, a moderate individual efficiency actually leads to a better systemic performance. The SIF effect takes place in a variety of phenomena. We review studies and examples of the SIF effect in pedestrian dynamics, vehicle traffic, traffic light control, logistics, public transport, social dynamics, ecological systems, and adaptation. Drawing on these examples, we generalize common features of the SIF effect and suggest possible future lines of research. version:2
arxiv-1510-05684 | NYTRO: When Subsampling Meets Early Stopping | http://arxiv.org/abs/1510.05684 | id:1510.05684 author:Tomas Angles, Raffaello Camoriano, Alessandro Rudi, Lorenzo Rosasco category:stat.ML  published:2015-10-19 summary:Early stopping is a well known approach to reduce the time complexity for performing training and model selection of large scale learning machines. On the other hand, memory/space (rather than time) complexity is the main constraint in many applications, and randomized subsampling techniques have been proposed to tackle this issue. In this paper we ask whether early stopping and subsampling ideas can be combined in a fruitful way. We consider the question in a least squares regression setting and propose a form of randomized iterative regularization based on early stopping and subsampling. In this context, we analyze the statistical and computational properties of the proposed method. Theoretical results are complemented and validated by a thorough experimental analysis. version:1
arxiv-1510-05682 | Protein Structure Prediction by Protein Alignments | http://arxiv.org/abs/1510.05682 | id:1510.05682 author:Jianzhu Ma category:cs.CE cs.LG q-bio.BM  published:2015-10-19 summary:Proteins are the basic building blocks of life. They usually perform functions by folding to a particular structure. Understanding the folding process could help the researchers to understand the functions of proteins and could also help to develop supplemental proteins for people with deficiencies and gain more insight into diseases associated with troublesome folding proteins. Experimental methods are both expensive and time consuming. In this thesis I introduce a new machine learning based method to predict the protein structure. The new method improves the performance from two directions: creating accurate protein alignments and predicting accurate protein contacts. First, I present an alignment framework MRFalign which goes beyond state-of-the-art methods and uses Markov Random Fields to model a protein family and align two proteins by aligning two MRFs together. Compared to other methods, that can only model local-range residue correlation, MRFs can model long-range residue interactions and thus, encodes global information in a protein. Secondly, I present a Group Graphical Lasso method for contact prediction that integrates joint multi-family Evolutionary Coupling analysis and supervised learning to improve accuracy on proteins without many sequence homologs. Different from single-family EC analysis that uses residue co-evolution information in only the target protein family, our joint EC analysis uses residue co-evolution in both the target family and its related families, which may have divergent sequences but similar folds. Our method can also integrate supervised learning methods to further improve accuracy. We evaluate the performance of both methods including each of its components on large public benchmarks. Experiments show that our methods can achieve better accuracy than existing state-of-the-art methods under all the measurements on most of the protein classes. version:1
arxiv-1510-04822 | SGD with Variance Reduction beyond Empirical Risk Minimization | http://arxiv.org/abs/1510.04822 | id:1510.04822 author:Massil Achab, Agathe Guilloux, Stéphane Gaïffas, Emmanuel Bacry category:stat.ML cs.LG  published:2015-10-16 summary:We introduce a doubly stochastic proximal gradient algorithm for optimizing a finite average of smooth convex functions, whose gradients depend on numerically expensive expectations. Our main motivation is the acceleration of the optimization of the regularized Cox partial-likelihood (the core model used in survival analysis), but our algorithm can be used in different settings as well. The proposed algorithm is doubly stochastic in the sense that gradient steps are done using stochastic gradient descent (SGD) with variance reduction, where the inner expectations are approximated by a Monte-Carlo Markov-Chain (MCMC) algorithm. We derive conditions on the MCMC number of iterations guaranteeing convergence, and obtain a linear rate of convergence under strong convexity and a sublinear rate without this assumption. We illustrate the fact that our algorithm improves the state-of-the-art solver for regularized Cox partial-likelihood on several datasets from survival analysis. version:2
arxiv-1506-01060 | Global and Local Structure Preserving Sparse Subspace Learning: An Iterative Approach to Unsupervised Feature Selection | http://arxiv.org/abs/1506.01060 | id:1506.01060 author:Nan Zhou, Yangyang Xu, Hong Cheng, Jun Fang, Witold Pedrycz category:cs.LG 05-04 E.0  published:2015-06-02 summary:As we aim at alleviating the curse of high-dimensionality, subspace learning is becoming more popular. Existing approaches use either information about global or local structure of the data, and few studies simultaneously focus on global and local structures as the both of them contain important information. In this paper, we propose a global and local structure preserving sparse subspace learning (GLoSS) model for unsupervised feature selection. The model can simultaneously realize feature selection and subspace learning. In addition, we develop a greedy algorithm to establish a generic combinatorial model, and an iterative strategy based on an accelerated block coordinate descent is used to solve the GLoSS problem. We also provide whole iterate sequence convergence analysis of the proposed iterative algorithm. Extensive experiments are conducted on real-world datasets to show the superiority of the proposed approach over several state-of-the-art unsupervised feature selection approaches. version:2
arxiv-1505-00487 | Sequence to Sequence -- Video to Text | http://arxiv.org/abs/1505.00487 | id:1505.00487 author:Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko category:cs.CV  published:2015-05-03 summary:Real-world videos often have complex dynamics; and methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem, we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD). version:3
arxiv-1504-06937 | Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits | http://arxiv.org/abs/1504.06937 | id:1504.06937 author:Huasen Wu, R. Srikant, Xin Liu, Chong Jiang category:cs.LG stat.ML  published:2015-04-27 summary:We study contextual bandits with budget and time constraints, referred to as constrained contextual bandits.The time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time.Such coupling effects make it difficult to obtain oracle solutions that assume known statistics of bandits. To gain insight, we first study unit-cost systems with known context distribution. When the expected rewards are known, we develop an approximation of the oracle, referred to Adaptive-Linear-Programming (ALP), which achieves near-optimality and only requires the ordering of expected rewards. With these highly desirable features, we then combine ALP with the upper-confidence-bound (UCB) method in the general case where the expected rewards are unknown {\it a priori}. We show that the proposed UCB-ALP algorithm achieves logarithmic regret except for certain boundary cases. Further, we design algorithms and obtain similar regret analysis results for more general systems with unknown context distribution and heterogeneous costs. To the best of our knowledge, this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits. Moreover, this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits. version:3
arxiv-1505-05489 | A Sparse Gaussian Process Framework for Photometric Redshift Estimation | http://arxiv.org/abs/1505.05489 | id:1505.05489 author:Ibrahim A. Almosallam, Sam N. Lindsay, Matt J. Jarvis, Stephen J. Roberts category:astro-ph.IM astro-ph.GA cs.CV  published:2015-05-20 summary:Accurate photometric redshifts are a lynchpin for many future experiments to pin down the cosmological model and for studies of galaxy evolution. In this study, a novel sparse regression framework for photometric redshift estimation is presented. Simulated and real data from SDSS DR12 were used to train and test the proposed models. We show that approaches which include careful data preparation and model design offer a significant improvement in comparison with several competing machine learning algorithms. Standard implementations of most regression algorithms have as the objective the minimization of the sum of squared errors. For redshift inference, however, this induces a bias in the posterior mean of the output distribution, which can be problematic. In this paper we directly target minimizing $\Delta z = (z_\textrm{s} - z_\textrm{p})/(1+z_\textrm{s})$ and address the bias problem via a distribution-based weighting scheme, incorporated as part of the optimization objective. The results are compared with other machine learning algorithms in the field such as Artificial Neural Networks (ANN), Gaussian Processes (GPs) and sparse GPs. The proposed framework reaches a mean absolute $\Delta z = 0.0026(1+z_\textrm{s})$, over the redshift range of $0 \le z_\textrm{s} \le 2$ on the simulated data, and $\Delta z = 0.0178(1+z_\textrm{s})$ over the entire redshift range on the SDSS DR12 survey, outperforming the standard ANNz used in the literature. We also investigate how the relative size of the training set affects the photometric redshift accuracy. We find that a training set of \textgreater 30 per cent of total sample size, provides little additional constraint on the photometric redshifts, and note that our GP formalism strongly outperforms ANNz in the sparse data regime for the simulated data set. version:3
arxiv-1510-05577 | Application of Machine Learning Techniques in Human Activity Recognition | http://arxiv.org/abs/1510.05577 | id:1510.05577 author:Jitenkumar Babubhai Rana, Rashmi Shetty, Tanya Jha category:cs.LG  published:2015-10-19 summary:Human activity detection has seen a tremendous growth in the last decade playing a major role in the field of pervasive computing. This emerging popularity can be attributed to its myriad of real-life applications primarily dealing with human-centric problems like healthcare and elder care. Many research attempts with data mining and machine learning techniques have been undergoing to accurately detect human activities for e-health systems. This paper reviews some of the predictive data mining algorithms and compares the accuracy and performances of these models. A discussion on the future research directions is subsequently offered. version:1
arxiv-1510-05576 | Optimization for Gaussian Processes via Chaining | http://arxiv.org/abs/1510.05576 | id:1510.05576 author:Emile Contal, Cédric Malherbe, Nicolas Vayatis category:stat.ML  published:2015-10-19 summary:In this paper, we consider the problem of stochastic optimization under a bandit feedback model. We generalize the GP-UCB algorithm [Srinivas and al., 2012] to arbitrary kernels and search spaces. To do so, we use a notion of localized chaining to control the supremum of a Gaussian process, and provide a novel optimization scheme based on the computation of covering numbers. The theoretical bounds we obtain on the cumulative regret are more generic and present the same convergence rates as the GP-UCB algorithm. Finally, the algorithm is shown to be empirically more efficient than its natural competitors on simple and complex input spaces. version:1
arxiv-1401-3376 | Across neighbourhood search for numerical optimization | http://arxiv.org/abs/1401.3376 | id:1401.3376 author:Guohua Wu category:cs.NE  published:2014-01-14 summary:Population-based search algorithms (PBSAs), including swarm intelligence algorithms (SIAs) and evolutionary algorithms (EAs), are competitive alternatives for solving complex optimization problems and they have been widely applied to real-world optimization problems in different fields. In this study, a novel population-based across neighbourhood search (ANS) is proposed for numerical optimization. ANS is motivated by two straightforward assumptions and three important issues raised in improving and designing efficient PBSAs. In ANS, a group of individuals collaboratively search the solution space for an optimal solution of the optimization problem considered. A collection of superior solutions found by individuals so far is maintained and updated dynamically. At each generation, an individual directly searches across the neighbourhoods of multiple superior solutions with the guidance of a Gaussian distribution. This search manner is referred to as across neighbourhood search. The characteristics of ANS are discussed and the concept comparisons with other PBSAs are given. The principle behind ANS is simple. Moreover, ANS is easy for implementation and application with three parameters being required to tune. Extensive experiments on 18 benchmark optimization functions of different types show that ANS has well balanced exploration and exploitation capabilities and performs competitively compared with many efficient PBSAs (Related Matlab codes used in the experiments are available from http://guohuawunudt.gotoip2.com/publications.html). version:3
arxiv-1510-05559 | Sparse + Low Rank Decomposition of Annihilating Filter-based Hankel Matrix for Impulse Noise Removal | http://arxiv.org/abs/1510.05559 | id:1510.05559 author:Kyong Hwan Jin, Jong Chul Ye category:cs.CV  published:2015-10-19 summary:Recently, so called annihilating filer-based low rank Hankel matrix (ALOHA) approach was proposed as a powerful image inpainting method. Based on the observation that smoothness or textures within an image patch corresponds to sparse spectral components in the frequency domain, ALOHA exploits the existence of annihilating filters and the associated rank-deficient Hankel matrices in the image domain to estimate the missing pixels. By extending this idea, here we propose a novel impulse noise removal algorithm using sparse + low rank decomposition of an annihilating filter-based Hankel matrix. The new approach, what we call the robust ALOHA, is motivated by the observation that an image corrupted with impulse noises has intact pixels; so the impulse noises can be modeled as sparse components, whereas the underlying image can be still modeled using a low-rank Hankel structured matrix. To solve the sparse + low rank decomposition problem, we propose an alternating direction method of multiplier (ADMM) method with initial factorized matrices coming from low rank matrix fitting (LMaFit) algorithm. To adapt the local image statistics that have distinct spectral distributions, the robust ALOHA is applied patch by patch. Experimental results from two types of impulse noises - random valued impulse noises and salt/pepper noises - for both single channel and multi-channel color images demonstrate that the robust ALOHA outperforms the existing algorithms up to 8dB in terms of the peak signal to noise ratio (PSNR). version:1
arxiv-1510-05491 | Clustering with Beta Divergences | http://arxiv.org/abs/1510.05491 | id:1510.05491 author:Mehmet Emin Basbug, Barbara Engelhardt category:cs.LG  published:2015-10-19 summary:Clustering algorithms start with a fixed divergence metric, which captures the possibly asymmetric distance between two samples. In a mixture model, the sample distribution plays the role of a divergence metric. It is often the case that the distributional assumption is not validated, which calls for an adaptive approach. We consider a richer model where the underlying distribution belongs to a parametrized exponential family, called Tweedie Models. We first show the connection between the Tweedie models and beta divergences, and derive the corresponding hard-assignment clustering algorithm. We exploit this connection to identify moment conditions and use Generalized Method of Moments(GMoM) to learn the data distribution. Based on this adaptive approach, we propose four new hard clustering algorithms and compare them to the classical k-means and DP-means on synthetic data as well as seven UCI datasets and one large gene expression dataset. We further compare the GMoM routine to an approximate maximum likelihood routine and validate the computational benefits of the GMoM approach. version:1
arxiv-1510-05477 | Accelerometer based Activity Classification with Variational Inference on Sticky HDP-SLDS | http://arxiv.org/abs/1510.05477 | id:1510.05477 author:Mehmet Emin Basbug, Koray Ozcan, Senem Velipasalar category:cs.LG stat.ML  published:2015-10-19 summary:As part of daily monitoring of human activities, wearable sensors and devices are becoming increasingly popular sources of data. With the advent of smartphones equipped with acceloremeter, gyroscope and camera; it is now possible to develop activity classification platforms everyone can use conveniently. In this paper, we propose a fast inference method for an unsupervised non-parametric time series model namely variational inference for sticky HDP-SLDS(Hierarchical Dirichlet Process Switching Linear Dynamical System). We show that the proposed algorithm can differentiate various indoor activities such as sitting, walking, turning, going up/down the stairs and taking the elevator using only the acceloremeter of an Android smartphone Samsung Galaxy S4. We used the front camera of the smartphone to annotate activity types precisely. We compared the proposed method with Hidden Markov Models with Gaussian emission probabilities on a dataset of 10 subjects. We showed that the efficacy of the stickiness property. We further compared the variational inference to the Gibbs sampler on the same model and show that variational inference is faster in one order of magnitude. version:1
arxiv-1510-05461 | Confidence Sets for the Source of a Diffusion in Regular Trees | http://arxiv.org/abs/1510.05461 | id:1510.05461 author:Justin Khim, Po-Ling Loh category:math.ST cs.DM cs.SI math.PR stat.ML stat.TH 62M99  published:2015-10-19 summary:We study the problem of identifying the source of a diffusion spreading over a regular tree. When the degree of each node is at least three, we show that it is possible to construct confidence sets for the diffusion source with size independent of the number of infected nodes. Our estimators are motivated by analogous results in the literature concerning identification of the root node in preferential attachment and uniform attachment trees. At the core of our proofs is a probabilistic analysis of P\'{o}lya urns corresponding to the number of uninfected neighbors in specific subtrees of the infection tree. We also provide an example illustrating the shortcomings of source estimation techniques in settings where the underlying graph is asymmetric. version:1
arxiv-1510-05436 | Color graph based wavelet transform with perceptual information | http://arxiv.org/abs/1510.05436 | id:1510.05436 author:Mohamed Malek, David Helbert, Philippe Carre category:cs.CV  published:2015-10-19 summary:In this paper, we propose a numerical strategy to define a multiscale analysis for color and multicomponent images based on the representation of data on a graph. Our approach consists in computing the graph of an image using the psychovisual information and analysing it by using the spectral graph wavelet transform. We suggest introducing color dimension into the computation of the weights of the graph and using the geodesic distance as a means of distance measurement. We thus have defined a wavelet transform based on a graph with perceptual information by using the CIELab color distance. This new representation is illustrated with denoising and inpainting applications. Overall, by introducing psychovisual information in the graph computation for the graph wavelet transform we obtain very promising results. Therefore results in image restoration highlight the interest of the appropriate use of color information. version:1
arxiv-1501-05194 | A Bayesian alternative to mutual information for the hierarchical clustering of dependent random variables | http://arxiv.org/abs/1501.05194 | id:1501.05194 author:Guillaume Marrelec, Arnaud Messé, Pierre Bellec category:stat.ML cs.LG q-bio.QM  published:2015-01-21 summary:The use of mutual information as a similarity measure in agglomerative hierarchical clustering (AHC) raises an important issue: some correction needs to be applied for the dimensionality of variables. In this work, we formulate the decision of merging dependent multivariate normal variables in an AHC procedure as a Bayesian model comparison. We found that the Bayesian formulation naturally shrinks the empirical covariance matrix towards a matrix set a priori (e.g., the identity), provides an automated stopping rule, and corrects for dimensionality using a term that scales up the measure as a function of the dimensionality of the variables. Also, the resulting log Bayes factor is asymptotically proportional to the plug-in estimate of mutual information, with an additive correction for dimensionality in agreement with the Bayesian information criterion. We investigated the behavior of these Bayesian alternatives (in exact and asymptotic forms) to mutual information on simulated and real data. An encouraging result was first derived on simulations: the hierarchical clustering based on the log Bayes factor outperformed off-the-shelf clustering techniques as well as raw and normalized mutual information in terms of classification accuracy. On a toy example, we found that the Bayesian approaches led to results that were similar to those of mutual information clustering techniques, with the advantage of an automated thresholding. On real functional magnetic resonance imaging (fMRI) datasets measuring brain activity, it identified clusters consistent with the established outcome of standard procedures. On this application, normalized mutual information had a highly atypical behavior, in the sense that it systematically favored very large clusters. These initial experiments suggest that the proposed Bayesian alternatives to mutual information are a useful new tool for hierarchical clustering. version:2
arxiv-1510-05417 | Piecewise-Linear Approximation for Feature Subset Selection in a Sequential Logit Model | http://arxiv.org/abs/1510.05417 | id:1510.05417 author:Toshiki Sato, Yuichi Takano, Ryuhei Miyashiro category:stat.ME cs.LG math.OC stat.ML  published:2015-10-19 summary:This paper concerns a method of selecting a subset of features for a sequential logit model. Tanaka and Nakagawa (2014) proposed a mixed integer quadratic optimization formulation for solving the problem based on a quadratic approximation of the logistic loss function. However, since there is a significant gap between the logistic loss function and its quadratic approximation, their formulation may fail to find a good subset of features. To overcome this drawback, we apply a piecewise-linear approximation to the logistic loss function. Accordingly, we frame the feature subset selection problem of minimizing an information criterion as a mixed integer linear optimization problem. The computational results demonstrate that our piecewise-linear approximation approach found a better subset of features than the quadratic approximation approach. version:1
arxiv-1506-02784 | Estimating Posterior Ratio for Classification: Transfer Learning from Probabilistic Perspective | http://arxiv.org/abs/1506.02784 | id:1506.02784 author:Song Liu, Kenji Fukumizu category:stat.ML cs.LG  published:2015-06-09 summary:Transfer learning assumes classifiers of similar tasks share certain parameter structures. Unfortunately, modern classifiers uses sophisticated feature representations with huge parameter spaces which lead to costly transfer. Under the impression that changes from one classifier to another should be ``simple'', an efficient transfer learning criteria that only learns the ``differences'' is proposed in this paper. We train a \emph{posterior ratio} which turns out to minimizes the upper-bound of the target learning risk. The model of posterior ratio does not have to share the same parameter space with the source classifier at all so it can be easily modelled and efficiently trained. The resulting classifier therefore is obtained by simply multiplying the existing probabilistic-classifier with the learned posterior ratio. version:3
arxiv-1502-03391 | Fast Embedding for JOFC Using the Raw Stress Criterion | http://arxiv.org/abs/1502.03391 | id:1502.03391 author:Vince Lyzinski, Youngser Park, Carey E. Priebe, Michael W. Trosset category:stat.ML stat.ME  published:2015-02-11 summary:The Joint Optimization of Fidelity and Commensurability (JOFC) manifold matching methodology embeds an omnibus dissimilarity matrix consisting of multiple dissimilarities on the same set of objects. One approach to this embedding optimizes the preservation of fidelity to each individual dissimilarity matrix together with commensurability of each given observation across modalities via iterative majorizations of a raw stress error criterion by successive Guttman transforms. In this paper, we exploit the special structure inherent to JOFC to exactly and efficiently compute the successive Guttman transforms, and as a result we are able to greatly speed up and parallelize the JOFC procedure for both in-sample and out-of-sample embedding. We demonstrate the scalability of our implementation on both real and simulated data examples. version:2
arxiv-1510-05336 | Clustering is Easy When ....What? | http://arxiv.org/abs/1510.05336 | id:1510.05336 author:Shai Ben-David category:stat.ML cs.LG  published:2015-10-19 summary:It is well known that most of the common clustering objectives are NP-hard to optimize. In practice, however, clustering is being routinely carried out. One approach for providing theoretical understanding of this seeming discrepancy is to come up with notions of clusterability that distinguish realistically interesting input data from worst-case data sets. The hope is that there will be clustering algorithms that are provably efficient on such "clusterable" instances. This paper addresses the thesis that the computational hardness of clustering tasks goes away for inputs that one really cares about. In other words, that "Clustering is difficult only when it does not matter" (the \emph{CDNM thesis} for short). I wish to present a a critical bird's eye overview of the results published on this issue so far and to call attention to the gap between available and desirable results on this issue. A longer, more detailed version of this note is available as arXiv:1507.05307. I discuss which requirements should be met in order to provide formal support to the the CDNM thesis and then examine existing results in view of these requirements and list some significant unsolved research challenges in that direction. version:1
arxiv-1506-03648 | Constrained Convolutional Neural Networks for Weakly Supervised Segmentation | http://arxiv.org/abs/1506.03648 | id:1506.03648 author:Deepak Pathak, Philipp Krähenbühl, Trevor Darrell category:cs.CV cs.LG  published:2015-06-11 summary:We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm. version:2
arxiv-1510-05318 | Latent Space Model for Multi-Modal Social Data | http://arxiv.org/abs/1510.05318 | id:1510.05318 author:Yoon-Sik Cho, Greg Ver Steeg, Emilio Ferrara, Aram Galstyan category:cs.SI cs.LG physics.data-an physics.soc-ph  published:2015-10-18 summary:With the emergence of social networking services, researchers enjoy the increasing availability of large-scale heterogenous datasets capturing online user interactions and behaviors. Traditional analysis of techno-social systems data has focused mainly on describing either the dynamics of social interactions, or the attributes and behaviors of the users. However, overwhelming empirical evidence suggests that the two dimensions affect one another, and therefore they should be jointly modeled and analyzed in a multi-modal framework. The benefits of such an approach include the ability to build better predictive models, leveraging social network information as well as user behavioral signals. To this purpose, here we propose the Constrained Latent Space Model (CLSM), a generalized framework that combines Mixed Membership Stochastic Blockmodels (MMSB) and Latent Dirichlet Allocation (LDA) incorporating a constraint that forces the latent space to concurrently describe the multiple data modalities. We derive an efficient inference algorithm based on Variational Expectation Maximization that has a computational cost linear in the size of the network, thus making it feasible to analyze massive social datasets. We validate the proposed framework on two problems: prediction of social interactions from user attributes and behaviors, and behavior prediction exploiting network information. We perform experiments with a variety of multi-modal social systems, spanning location-based social networks (Gowalla), social media services (Instagram, Orkut), e-commerce and review sites (Amazon, Ciao), and finally citation networks (Cora). The results indicate significant improvement in prediction accuracy over state of the art methods, and demonstrate the flexibility of the proposed approach for addressing a variety of different learning problems commonly occurring with multi-modal social data. version:1
arxiv-1507-02000 | An optimal randomized incremental gradient method | http://arxiv.org/abs/1507.02000 | id:1507.02000 author:Guanghui Lan, Yi Zhou category:math.OC cs.CC stat.ML  published:2015-07-08 summary:In this paper, we consider a class of finite-sum convex optimization problems whose objective function is given by the summation of $m$ ($\ge 1$) smooth components together with some other relatively simple terms. We first introduce a deterministic primal-dual gradient (PDG) method that can achieve the optimal black-box iteration complexity for solving these composite optimization problems using a primal-dual termination criterion. Our major contribution is to develop a randomized primal-dual gradient (RPDG) method, which needs to compute the gradient of only one randomly selected smooth component at each iteration, but can possibly achieve better complexity than PDG in terms of the total number of gradient evaluations. More specifically, we show that the total number of gradient evaluations performed by RPDG can be ${\cal O} (\sqrt{m})$ times smaller, both in expectation and with high probability, than those performed by deterministic optimal first-order methods under favorable situations. We also show that the complexity of the RPDG method is not improvable by developing a new lower complexity bound for a general class of randomized methods for solving large-scale finite-sum convex optimization problems. Moreover, through the development of PDG and RPDG, we introduce a novel game-theoretic interpretation for these optimal methods for convex optimization. version:3
arxiv-1507-08240 | EESEN: End-to-End Speech Recognition using Deep RNN Models and WFST-based Decoding | http://arxiv.org/abs/1507.08240 | id:1507.08240 author:Yajie Miao, Mohammad Gowayyed, Florian Metze category:cs.CL cs.LG  published:2015-07-29 summary:The performance of automatic speech recognition (ASR) has improved tremendously due to the application of deep neural networks (DNNs). Despite this progress, building a new ASR system remains a challenging task, requiring various resources, multiple training stages and significant expertise. This paper presents our Eesen framework which drastically simplifies the existing pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen involves learning a single recurrent neural network (RNN) predicting context-independent targets (phonemes or characters). To remove the need for pre-generated frame labels, we adopt the connectionist temporal classification (CTC) objective function to infer the alignments between speech and label sequences. A distinctive feature of Eesen is a generalized decoding approach based on weighted finite-state transducers (WFSTs), which enables the efficient incorporation of lexicons and language models into CTC decoding. Experiments show that compared with the standard hybrid DNN systems, Eesen achieves comparable word error rates (WERs), while at the same time speeding up decoding significantly. version:3
arxiv-1509-05360 | Geometry-aware Deep Transform | http://arxiv.org/abs/1509.05360 | id:1509.05360 author:Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro category:cs.CV  published:2015-09-17 summary:Many recent efforts have been devoted to designing sophisticated deep learning structures, obtaining revolutionary results on benchmark datasets. The success of these deep learning methods mostly relies on an enormous volume of labeled training samples to learn a huge number of parameters in a network; therefore, understanding the generalization ability of a learned deep network cannot be overlooked, especially when restricted to a small training set, which is the case for many applications. In this paper, we propose a novel deep learning objective formulation that unifies both the classification and metric learning criteria. We then introduce a geometry-aware deep transform to enable a non-linear discriminative and robust feature transform, which shows competitive performance on small training sets for both synthetic and real-world data. We further support the proposed framework with a formal $(K,\epsilon)$-robustness analysis. version:2
arxiv-1510-05275 | Real-time Tracking Based on Neuromrophic Vision | http://arxiv.org/abs/1510.05275 | id:1510.05275 author:Hongmin Li, Pei Jing, Guoqi Li category:cs.CV  published:2015-10-18 summary:Real-time tracking is an important problem in computer vision in which most methods are based on the conventional cameras. Neuromorphic vision is a concept defined by incorporating neuromorphic vision sensors such as silicon retinas in vision processing system. With the development of the silicon technology, asynchronous event-based silicon retinas that mimic neuro-biological architectures has been developed in recent years. In this work, we combine the vision tracking algorithm of computer vision with the information encoding mechanism of event-based sensors which is inspired from the neural rate coding mechanism. The real-time tracking of single object with the advantage of high speed of 100 time bins per second is successfully realized. Our method demonstrates that the computer vision methods could be used for the neuromorphic vision processing and we can realize fast real-time tracking using neuromorphic vision sensors compare to the conventional camera. version:1
arxiv-1510-05257 | Scalable inference for a full multivariate stochastic volatility model | http://arxiv.org/abs/1510.05257 | id:1510.05257 author:P. Dellaportas, A. Plataniotis, M. K. Titsias category:stat.ML  published:2015-10-18 summary:We introduce a multivariate stochastic volatility model for asset returns that imposes no restrictions to the structure of the volatility matrix and treats all its elements as functions of latent stochastic processes. When the number of assets is prohibitively large, we propose a factor multivariate stochastic volatility model in which the variances and correlations of the factors evolve stochastically over time. Inference is achieved via a carefully designed feasible and scalable Markov chain Monte Carlo algorithm that combines two computationally important ingredients: it utilizes invariant to the prior Metropolis proposal densities for simultaneously updating all latent paths and has quadratic, rather than cubic, computational complexity when evaluating the multivariate normal densities required. We apply our modelling and computational methodology to $571$ stock daily returns of Euro STOXX index for data over a period of $10$ years. version:1
arxiv-1510-05237 | Large Enforced Sparse Non-Negative Matrix Factorization | http://arxiv.org/abs/1510.05237 | id:1510.05237 author:Brendan Gavin, Vijay Gadepally, Jeremy Kepner category:cs.LG cs.NA cs.SI  published:2015-10-18 summary:Non-negative matrix factorization (NMF) is a common method for generating topic models from text data. NMF is widely accepted for producing good results despite its relative simplicity of implementation and ease of computation. One challenge with applying NMF to large datasets is that intermediate matrix products often become dense, stressing the memory and compute elements of a system. In this article, we investigate a simple but powerful modification of a common NMF algorithm that enforces the generation of sparse intermediate and output matrices. This method enables the application of NMF to large datasets through improved memory and compute performance. Further, we demonstrate empirically that this method of enforcing sparsity in the NMF either preserves or improves both the accuracy of the resulting topic model and the convergence rate of the underlying algorithm. version:1
arxiv-1510-05214 | Clustering Noisy Signals with Structured Sparsity Using Time-Frequency Representation | http://arxiv.org/abs/1510.05214 | id:1510.05214 author:Tom Hope, Avishai Wagner, Or Zuk category:cs.LG stat.ML 62H30  65T60  published:2015-10-18 summary:We propose a simple and efficient time-series clustering framework particularly suited for low Signal-to-Noise Ratio (SNR), by simultaneous smoothing and dimensionality reduction aimed at preserving clustering information. We extend the sparse K-means algorithm by incorporating structured sparsity, and use it to exploit the multi-scale property of wavelets and group structure in multivariate signals. Finally, we extract features invariant to translation and scaling with the scattering transform, which corresponds to a convolutional network with filters given by a wavelet operator, and use the network's structure in sparse clustering. By promoting sparsity, this transform can yield a low-dimensional representation of signals that gives improved clustering results on several real datasets. version:1
arxiv-1505-06605 | Expresso : A user-friendly GUI for Designing, Training and Exploring Convolutional Neural Networks | http://arxiv.org/abs/1505.06605 | id:1505.06605 author:Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu category:cs.CV cs.NE  published:2015-05-25 summary:With a view to provide a user-friendly interface for designing, training and developing deep learning frameworks, we have developed Expresso, a GUI tool written in Python. Expresso is built atop Caffe, the open-source, prize-winning framework popularly used to develop Convolutional Neural Networks. Expresso provides a convenient wizard-like graphical interface which guides the user through various common scenarios -- data import, construction and training of deep networks, performing various experiments, analyzing and visualizing the results of these experiments. The multi-threaded nature of Expresso enables concurrent execution and notification of events related to the aforementioned scenarios. The GUI sub-components and inter-component interfaces in Expresso have been designed with extensibility in mind. We believe Expresso's flexibility and ease of use will come in handy to researchers, newcomers and seasoned alike, in their explorations related to deep learning. version:2
arxiv-1510-05203 | Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015 | http://arxiv.org/abs/1510.05203 | id:1510.05203 author:Graham Neubig, Makoto Morishita, Satoshi Nakamura category:cs.CL  published:2015-10-18 summary:This year, the Nara Institute of Science and Technology (NAIST)'s submission to the 2015 Workshop on Asian Translation was based on syntax-based statistical machine translation, with the addition of a reranking component using neural attentional machine translation models. Experiments re-confirmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also confirmed for the first time that these results also carry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words. version:1
arxiv-1507-06370 | Sum-of-Squares Lower Bounds for Sparse PCA | http://arxiv.org/abs/1507.06370 | id:1507.06370 author:Tengyu Ma, Avi Wigderson category:cs.LG cs.CC math.ST stat.CO stat.ML stat.TH  published:2015-07-23 summary:This paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method. Specifically, we consider the {\em Sparse Principal Component Analysis} (Sparse PCA) problem, and the family of {\em Sum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well known that in large dimension $p$, a planted $k$-sparse unit vector can be {\em in principle} detected using only $n \approx k\log p$ (Gaussian or Bernoulli) samples, but all {\em efficient} (polynomial time) algorithms known require $n \approx k^2$ samples. It was also known that this quadratic gap cannot be improved by the the most basic {\em semi-definite} (SDP, aka spectral) relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also degree-4 SoS algorithms cannot improve this quadratic gap. This average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms. Moreover, our design of moments (or "pseudo-expectations") for this lower bound is quite different than previous lower bounds. Establishing lower bounds for higher degree SoS algorithms for remains a challenging problem. version:2
arxiv-1510-00981 | Efficient Hand Articulations Tracking using Adaptive Hand Model and Depth map | http://arxiv.org/abs/1510.00981 | id:1510.00981 author:Byeongkeun Kang, Yeejin Lee, Truong Q. Nguyen category:cs.CV  published:2015-10-04 summary:Real-time hand articulations tracking is important for many applications such as interacting with virtual / augmented reality devices or tablets. However, most of existing algorithms highly rely on expensive and high power-consuming GPUs to achieve real-time processing. Consequently, these systems are inappropriate for mobile and wearable devices. In this paper, we propose an efficient hand tracking system which does not require high performance GPUs. In our system, we track hand articulations by minimizing discrepancy between depth map from sensor and computer-generated hand model. We also initialize hand pose at each frame using finger detection and classification. Our contributions are: (a) propose adaptive hand model to consider different hand shapes of users without generating personalized hand model; (b) improve the highly efficient frame initialization for robust tracking and automatic initialization; (c) propose hierarchical random sampling of pixels from each depth map to improve tracking accuracy while limiting required computations. To the best of our knowledge, it is the first system that achieves both automatic hand model adjustment and real-time tracking without using GPUs. version:3
arxiv-1510-05157 | Performance Characterization of Image Feature Detectors in Relation to the Scene Content Utilizing a Large Image Database | http://arxiv.org/abs/1510.05157 | id:1510.05157 author:Bruno Ferrarini, Shoaib Ehsan, Naveed Ur Rehman, Klaus D. McDonald-Maier category:cs.CV  published:2015-10-17 summary:Selecting the most suitable local invariant feature detector for a particular application has rendered the task of evaluating feature detectors a critical issue in vision research. No state-of-the-art image feature detector works satisfactorily under all types of image transformations. Although the literature offers a variety of comparison works focusing on performance evaluation of image feature detectors under several types of image transformation, the influence of the scene content on the performance of local feature detectors has received little attention so far. This paper aims to bridge this gap with a new framework for determining the type of scenes, which maximize and minimize the performance of detectors in terms of repeatability rate. Several state-of-the-art feature detectors have been assessed utilizing a large database of 12936 images generated by applying uniform light and blur changes to 539 scenes captured from the real world. The results obtained provide new insights into the behaviour of feature detectors. version:1
arxiv-1510-05156 | Assessing The Performance Bounds Of Local Feature Detectors: Taking Inspiration From Electronics Design Practices | http://arxiv.org/abs/1510.05156 | id:1510.05156 author:Shoaib Ehsan, Adrian F. Clark, Bruno Ferrarini, Naveed Ur Rehman, Klaus D. McDonald-Maier category:cs.CV  published:2015-10-17 summary:Since local feature detection has been one of the most active research areas in computer vision, a large number of detectors have been proposed. This has rendered the task of characterizing the performance of various feature detection methods an important issue in vision research. Inspired by the good practices of electronic system design, a generic framework based on the improved repeatability measure is presented in this paper that allows assessment of the upper and lower bounds of detector performance in an effort to design more reliable and effective vision systems. This framework is then employed to establish operating and guarantee regions for several state-of-the art detectors for JPEG compression and uniform light changes. The results are obtained using a newly acquired, large image database (15092 images) with 539 different scenes. These results provide new insights into the behavior of detectors and are also useful from the vision systems design perspective. version:1
arxiv-1510-05154 | A Historical Analysis of the Field of OR/MS using Topic Models | http://arxiv.org/abs/1510.05154 | id:1510.05154 author:Christopher J. Gatti, James D. Brooks, Sarah G. Nurre category:stat.ML cs.DL stat.AP  published:2015-10-17 summary:This study investigates the content of the published scientific literature in the fields of operations research and management science (OR/MS) since the early 1950s. Our study is based on 80,757 published journal abstracts from 37 of the leading OR/MS journals. We have developed a topic model, using Latent Dirichlet Allocation (LDA), and extend this analysis to reveal the temporal dynamics of the field, journals, and topics. Our analysis shows the generality or specificity of each of the journals, and we identify groups of journals with similar content, which are both consistent and inconsistent with intuition. We also show how journals have become more or less unique in their scope. A more detailed analysis of each journals' topics over time shows significant temporal dynamics, especially for journals with niche content. This study presents an observational, yet objective, view of the published literature from OR/MS that would be of interest to authors, editors, journals, and publishers. Furthermore, this work can be used by new entrants to the fields of OR/MS to understand the content landscape, as a starting point for discussions and inquiry of the field at large, or as a model for other fields to perform similar analyses. version:1
arxiv-1510-05149 | Robust Non-linear Wiener-Granger Causality For Large High-dimensional Data | http://arxiv.org/abs/1510.05149 | id:1510.05149 author:Mehrdad Jafari-Mamaghani category:stat.ML stat.ME  published:2015-10-17 summary:Wiener-Granger causality is a widely used framework of causal analysis for temporally resolved events. We introduce a new measure of Wiener-Granger causality based on kernelization of partial canonical correlation analysis with specific advantages in the context of large high-dimensional data. The introduced measure is able to detect non-linear and non-monotonous signals, is designed to be immune to noise, and offers tunability in terms of computational complexity in its estimations. Furthermore, we show that, under specified conditions, the introduced measure can be regarded as an estimate of conditional mutual information (transfer entropy). The functionality of this measure is assessed using comparative simulations where it outperforms other existing methods. The paper is concluded with an application to climatological data. version:1
arxiv-1510-05145 | Rapid Online Analysis of Local Feature Detectors and Their Complementarity | http://arxiv.org/abs/1510.05145 | id:1510.05145 author:Shoaib Ehsan, Adrian F. Clark, Klaus D. McDonald-Maier category:cs.CV  published:2015-10-17 summary:A vision system that can assess its own performance and take appropriate actions online to maximize its effectiveness would be a step towards achieving the long-cherished goal of imitating humans. This paper proposes a method for performing an online performance analysis of local feature detectors, the primary stage of many practical vision systems. It advocates the spatial distribution of local image features as a good performance indicator and presents a metric that can be calculated rapidly, concurs with human visual assessments and is complementary to existing offline measures such as repeatability. The metric is shown to provide a measure of complementarity for combinations of detectors, correctly reflecting the underlying principles of individual detectors. Qualitative results on well-established datasets for several state-of-the-art detectors are presented based on the proposed measure. Using a hypothesis testing approach and a newly-acquired, larger image database, statistically-significant performance differences are identified. Different detector pairs and triplets are examined quantitatively and the results provide a useful guideline for combining detectors in applications that require a reasonable spatial distribution of image features. A principled framework for combining feature detectors in these applications is also presented. Timing results reveal the potential of the metric for online applications. version:1
arxiv-1510-05142 | Memory-Efficient Design Strategy for a Parallel Embedded Integral Image Computation Engine | http://arxiv.org/abs/1510.05142 | id:1510.05142 author:Shoaib Ehsan, Adrian F. Clark, Wah M. Cheung, Arjunsingh M. Bais, Bayar I. Menzat, Nadia Kanwal, Klaus D. McDonald-Maier category:cs.CV  published:2015-10-17 summary:In embedded vision systems, parallel computation of the integral image presents several design challenges in terms of hardware resources, speed and power consumption. Although recursive equations significantly reduce the number of operations for computing the integral image, the required internal memory becomes prohibitively large for an embedded integral image computation engine for increasing image sizes. With the objective of achieving high-throughput with minimum hardware resources, this paper proposes a memory-efficient design strategy for a parallel embedded integral image computation engine. Results show that the design achieves nearly 35% reduction in memory for common HD video. version:1
arxiv-1510-05138 | Integral Images: Efficient Algorithms for Their Computation and Storage in Resource-Constrained Embedded Vision Systems | http://arxiv.org/abs/1510.05138 | id:1510.05138 author:Shoaib Ehsan, Adrian F. Clark, Naveed ur Rehman, Klaus D. McDonald-Maier category:cs.CV  published:2015-10-17 summary:The integral image, an intermediate image representation, has found extensive use in multi-scale local feature detection algorithms, such as Speeded-Up Robust Features (SURF), allowing fast computation of rectangular features at constant speed, independent of filter size. For resource-constrained real-time embedded vision systems, computation and storage of integral image presents several design challenges due to strict timing and hardware limitations. Although calculation of the integral image only consists of simple addition operations, the total number of operations is large owing to the generally large size of image data. Recursive equations allow substantial decrease in the number of operations but require calculation in a serial fashion. This paper presents two new hardware algorithms that are based on the decomposition of these recursive equations, allowing calculation of up to four integral image values in a row-parallel way without significantly increasing the number of operations. An efficient design strategy is also proposed for a parallel integral image computation unit to reduce the size of the required internal memory (nearly 35% for common HD video). Addressing the storage problem of integral image in embedded vision systems, the paper presents two algorithms which allow substantial decrease (at least 44.44%) in the memory requirements. Finally, the paper provides a case study that highlights the utility of the proposed architectures in embedded vision systems. version:1
arxiv-1410-4688 | Large Vocabulary Arabic Online Handwriting Recognition System | http://arxiv.org/abs/1410.4688 | id:1410.4688 author:Ibrahim Abdelaziz, Sherif Abdou, Hassanin Al-Barhamtoshy category:cs.CV  published:2014-10-17 summary:Arabic handwriting is a consonantal and cursive writing. The analysis of Arabic script is further complicated due to obligatory dots/strokes that are placed above or below most letters and usually written delayed in order. Due to ambiguities and diversities of writing styles, recognition systems are generally based on a set of possible words called lexicon. When the lexicon is small, recognition accuracy is more important as the recognition time is minimal. On the other hand, recognition speed as well as the accuracy are both critical when handling large lexicons. Arabic is rich in morphology and syntax which makes its lexicon large. Therefore, a practical online handwriting recognition system should be able to handle a large lexicon with reasonable performance in terms of both accuracy and time. In this paper, we introduce a fully-fledged Hidden Markov Model (HMM) based system for Arabic online handwriting recognition that provides solutions for most of the difficulties inherent in recognizing the Arabic script. A new preprocessing technique for handling the delayed strokes is introduced. We use advanced modeling techniques for building our recognition system from the training data to provide more detailed representation for the differences between the writing units, minimize the variances between writers in the training data and have a better representation for the features space. System results are enhanced using an additional post-processing step with a higher order language model and cross-word HMM models. The system performance is evaluated using two different databases covering small and large lexicons. Our system outperforms the state-of-art systems for the small lexicon database. Furthermore, it shows promising results (accuracy and time) when supporting large lexicon with the possibility for adapting the models for specific writers to get even better results. version:3
arxiv-1510-05078 | A General Method for Robust Bayesian Modeling | http://arxiv.org/abs/1510.05078 | id:1510.05078 author:Chong Wang, David M. Blei category:stat.ML  published:2015-10-17 summary:Robust Bayesian models are appealing alternatives to standard models, providing protection from data that contains outliers or other departures from the model assumptions. Historically, robust models were mostly developed on a case-by-case basis; examples include robust linear regression, robust mixture models, and bursty topic models. In this paper we develop a general approach to robust Bayesian modeling. We show how to turn an existing Bayesian model into a robust model, and then develop a generic strategy for computing with it. We use our method to study robust variants of several models, including linear regression, Poisson regression, logistic regression, and probabilistic topic models. We discuss the connections between our methods and existing approaches, especially empirical Bayes and James-Stein estimation. version:1
arxiv-1506-00019 | A Critical Review of Recurrent Neural Networks for Sequence Learning | http://arxiv.org/abs/1506.00019 | id:1506.00019 author:Zachary C. Lipton, John Berkowitz, Charles Elkan category:cs.LG cs.NE  published:2015-05-29 summary:Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research. version:4
arxiv-1510-05058 | A Distance Measure for the Analysis of Polar Opinion Dynamics in Social Networks | http://arxiv.org/abs/1510.05058 | id:1510.05058 author:Victor Amelkin, Ambuj Singh, Petko Bogdanov category:cs.SI cs.DM cs.DS stat.ML G.2.2; H.2.8; I.5.3  published:2015-10-17 summary:Analysis of opinion dynamics in social networks plays an important role in today's life. For applications such as predicting users' political preference, it is particularly important to be able to analyze the dynamics of competing opinions. While observing the evolution of polar opinions of a social network's users over time, can we tell when the network "behaved" abnormally? Furthermore, can we predict how the opinions of the users will change in the future? Do opinions evolve according to existing network opinion dynamics models? To answer such questions, it is not sufficient to study individual user behavior, since opinions can spread far beyond users' egonets. We need a method to analyze opinion dynamics of all network users simultaneously and capture the effect of individuals' behavior on the global evolution pattern of the social network. In this work, we introduce Social Network Distance (SND) - a distance measure that quantifies the "cost" of evolution of one snapshot of a social network into another snapshot under various models of polar opinion propagation. SND has a rich semantics of a transportation problem, yet, is computable in time linear in the number of users, which makes SND applicable to the analysis of large-scale online social networks. In our experiments with synthetic and real-world Twitter data, we demonstrate the utility of our distance measure for anomalous event detection. It achieves a true positive rate of 0.83, twice as high as that of alternatives. When employed for opinion prediction in Twitter, our method's accuracy is 75.63%, which is 7.5% higher than that of the next best method. Source Code: https://cs.ucsb.edu/~victor/pub/ucsb/dbl/snd/ version:1
arxiv-1510-05043 | A cost function for similarity-based hierarchical clustering | http://arxiv.org/abs/1510.05043 | id:1510.05043 author:Sanjoy Dasgupta category:cs.DS cs.LG stat.ML  published:2015-10-16 summary:The development of algorithms for hierarchical clustering has been hampered by a shortage of precise objective functions. To help address this situation, we introduce a simple cost function on hierarchies over a set of points, given pairwise similarities between those points. We show that this criterion behaves sensibly in canonical instances and that it admits a top-down construction procedure with a provably good approximation ratio. version:1
arxiv-1509-09014 | Stats-Calculus Pose Descriptor Feeding A Discrete HMM Low-latency Detection and Recognition System For 3D Skeletal Actions | http://arxiv.org/abs/1509.09014 | id:1509.09014 author:Rofael Emil Fayez Behnam category:cs.CV  published:2015-09-30 summary:Recognition of human actions, under low observational latency, is a growing interest topic, nowadays. Many approaches have been represented based on a provided set of 3D Cartesian coordinates system originated at a certain specific point located on a root joint. In this paper, We will present a statistical detection and recognition system using Hidden Markov Model using 7 types of pose descriptors. * Cartesian Calculus Pose descriptor. * Angular Calculus Pose descriptor. * Mixed-mode Stats-Calculus Pose descriptor. * Centro-Stats-Calculus Pose descriptor. * Rela-Centro-Stats-Calculus Pose descriptor. * Rela-Centro-Stats-Calculus DCT Pose descriptor. * Rela-Centro-Stats-Calculus DCT-AMDF Pose descriptor. Stats-Calculus is a feature extracting technique, that is developed on Moving Pose descriptor , but using a combination of Statistics measures and Calculus measures. version:4
arxiv-1510-04972 | Normalization of Relative and Incomplete Temporal Expressions in Clinical Narratives | http://arxiv.org/abs/1510.04972 | id:1510.04972 author:Weiyi Sun, Anna Rumshisky, Ozlem Uzuner category:cs.CL cs.AI cs.IR  published:2015-10-16 summary:We analyze the RI-TIMEXes in temporally annotated corpora and propose two hypotheses regarding the normalization of RI-TIMEXes in the clinical narrative domain: the anchor point hypothesis and the anchor relation hypothesis. We annotate the RI-TIMEXes in three corpora to study the characteristics of RI-TMEXes in different domains. This informed the design of our RI-TIMEX normalization system for the clinical domain, which consists of an anchor point classifier, an anchor relation classifier and a rule-based RI-TIMEX text span parser. We experiment with different feature sets and perform error analysis for each system component. The annotation confirmed the hypotheses that we can simplify the RI-TIMEXes normalization task using two multi-label classifiers. Our system achieves anchor point classification, anchor relation classification and rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% under relaxed matching criteria) respectively on the held-out test set of the 2012 i2b2 temporal relation challenge. Experiments with feature sets reveals some interesting findings such as the verbal tense feature does not inform the anchor relation classification in clinical narratives as much as the tokens near the RI-TIMEX. Error analysis shows that underrepresented anchor point and anchor relation classes are difficult to detect. We formulate the RI-TIMEX normalization problem as a pair of multi-label classification problems. Considering only the RI-TIMEX extraction and normalization, the system achieves statistically significant improvement over the RI-TIMEX results of the best systems in the 2012 i2b2 challenge. version:1
arxiv-1510-04953 | Optimizing and Contrasting Recurrent Neural Network Architectures | http://arxiv.org/abs/1510.04953 | id:1510.04953 author:Ben Krause category:stat.ML cs.LG cs.NE  published:2015-10-16 summary:Recurrent Neural Networks (RNNs) have long been recognized for their potential to model complex time series. However, it remains to be determined what optimization techniques and recurrent architectures can be used to best realize this potential. The experiments presented take a deep look into Hessian free optimization, a powerful second order optimization method that has shown promising results, but still does not enjoy widespread use. This algorithm was used to train to a number of RNN architectures including standard RNNs, long short-term memory, multiplicative RNNs, and stacked RNNs on the task of character prediction. The insights from these experiments led to the creation of a new multiplicative LSTM hybrid architecture that outperformed both LSTM and multiplicative RNNs. When tested on a larger scale, multiplicative LSTM achieved character level modelling results competitive with the state of the art for RNNs using very different methodology. version:1
arxiv-1409-7336 | Does network complexity help organize Babel's library? | http://arxiv.org/abs/1409.7336 | id:1409.7336 author:Juan Pablo Cárdenas, Iván González, Gerardo Vidal, Miguel Fuentes category:physics.soc-ph cs.CL nlin.AO physics.data-an  published:2014-09-23 summary:In this work, we study properties of texts from the perspective of complex network theory. Words in given texts are linked by co-occurrence and transformed into networks, and we observe that these display topological properties common to other complex systems. However, there are some properties that seem to be exclusive to texts; many of these properties depend on the frequency of words in the text, while others seem to be strictly determined by the grammar. Precisely, these properties allow for a categorization of texts as either with a sense and others encoded or senseless. version:2
arxiv-1510-04931 | Bad Universal Priors and Notions of Optimality | http://arxiv.org/abs/1510.04931 | id:1510.04931 author:Jan Leike, Marcus Hutter category:cs.AI cs.LG  published:2015-10-16 summary:A big open question of algorithmic information theory is the choice of the universal Turing machine (UTM). For Kolmogorov complexity and Solomonoff induction we have invariance theorems: the choice of the UTM changes bounds only by a constant. For the universally intelligent agent AIXI (Hutter, 2005) no invariance theorem is known. Our results are entirely negative: we discuss cases in which unlucky or adversarial choices of the UTM cause AIXI to misbehave drastically. We show that Legg-Hutter intelligence and thus balanced Pareto optimality is entirely subjective, and that every policy is Pareto optimal in the class of all computable environments. This undermines all existing optimality properties for AIXI. While it may still serve as a gold standard for AI, our results imply that AIXI is a relative theory, dependent on the choice of the UTM. version:1
arxiv-1408-2036 | Characterizing predictable classes of processes | http://arxiv.org/abs/1408.2036 | id:1408.2036 author:Daniil Ryabko category:cs.LG stat.ML  published:2014-08-09 summary:The problem is sequence prediction in the following setting. A sequence x1,..., xn,... of discrete-valued observations is generated according to some unknown probabilistic law (measure) mu. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure mu belongs to an arbitrary class C of stochastic processes. We are interested in predictors ? whose conditional probabilities converge to the 'true' mu-conditional probabilities if any mu { C is chosen to generate the data. We show that if such a predictor exists, then a predictor can also be obtained as a convex combination of a countably many elements of C. In other words, it can be obtained as a Bayesian predictor whose prior is concentrated on a countable set. This result is established for two very different measures of performance of prediction, one of which is very strong, namely, total variation, and the other is very weak, namely, prediction in expected average Kullback-Leibler divergence. version:2
arxiv-1510-04908 | No Spare Parts: Sharing Part Detectors for Image Categorization | http://arxiv.org/abs/1510.04908 | id:1510.04908 author:Pascal Mettes, Jan C. van Gemert, Cees G. M. Snoek category:cs.CV  published:2015-10-16 summary:This work aims for image categorization using a representation of distinctive parts. Different from existing part-based work, we argue that parts are naturally shared between image categories and should be modeled as such. We motivate our approach with a quantitative and qualitative analysis by backtracking where selected parts come from. Our analysis shows that in addition to the category parts defining the class, the parts coming from the background context and parts from other image categories improve categorization performance. Part selection should not be done separately for each category, but instead be shared and optimized over all categories. To incorporate part sharing between categories, we present an algorithm based on AdaBoost to jointly optimize part sharing and selection, as well as fusion with the global image representation. We achieve results competitive to the state-of-the-art on object, scene, and action categories, further improving over deep convolutional neural networks. version:1
arxiv-1510-04905 | Robust Partially-Compressed Least-Squares | http://arxiv.org/abs/1510.04905 | id:1510.04905 author:Stephen Becker, Ban Kawas, Marek Petrik, Karthikeyan N. Ramamurthy category:stat.ML cs.LG  published:2015-10-16 summary:Randomized matrix compression techniques, such as the Johnson-Lindenstrauss transform, have emerged as an effective and practical way for solving large-scale problems efficiently. With a focus on computational efficiency, however, forsaking solutions quality and accuracy becomes the trade-off. In this paper, we investigate compressed least-squares problems and propose new models and algorithms that address the issue of error and noise introduced by compression. While maintaining computational efficiency, our models provide robust solutions that are more accurate--relative to solutions of uncompressed least-squares--than those of classical compressed variants. We introduce tools from robust optimization together with a form of partial compression to improve the error-time trade-offs of compressed least-squares solvers. We develop an efficient solution algorithm for our Robust Partially-Compressed (RPC) model based on a reduction to a one-dimensional search. We also derive the first approximation error bounds for Partially-Compressed least-squares solutions. Empirical results comparing numerous alternatives suggest that robust and partially compressed solutions are effectively insulated against aggressive randomized transforms. version:1
arxiv-1502-07190 | Topic-adjusted visibility metric for scientific articles | http://arxiv.org/abs/1502.07190 | id:1502.07190 author:Linda S. L. Tan, Aik Hui Chan, Tian Zheng category:stat.ML cs.LG  published:2015-02-25 summary:Measuring the impact of scientific articles is important for evaluating the research output of individual scientists, academic institutions and journals. While citations are raw data for constructing impact measures, there exist biases and potential issues if factors affecting citation patterns are not properly accounted for. In this work, we address the problem of field variation and introduce an article level metric useful for evaluating individual articles' visibility. This measure derives from joint probabilistic modeling of the content in the articles and the citations amongst them using latent Dirichlet allocation (LDA) and the mixed membership stochastic blockmodel (MMSB). Our proposed model provides a visibility metric for individual articles adjusted for field variation in citation rates, a structural understanding of citation behavior in different fields, and article recommendations which take into account article visibility and citation patterns. We develop an efficient algorithm for model fitting using variational methods. To scale up to large networks, we develop an online variant using stochastic gradient methods and case-control likelihood approximation. We apply our methods to the benchmark KDD Cup 2003 dataset with approximately 30,000 high energy physics papers. version:3
arxiv-1510-04863 | An Extension to Hough Transform Based on Gradient Orientation | http://arxiv.org/abs/1510.04863 | id:1510.04863 author:Tomislav Petković, Sven Lončarić category:cs.CV  published:2015-10-16 summary:The Hough transform is one of the most common methods for line detection. In this paper we propose a novel extension of the regular Hough transform. The proposed extension combines the extension of the accumulator space and the local gradient orientation resulting in clutter reduction and yielding more prominent peaks, thus enabling better line identification. We demonstrate benefits in applications such as visual quality inspection and rectangle detection. version:1
arxiv-1510-04861 | Towards Reversible De-Identification in Video Sequences Using 3D Avatars and Steganography | http://arxiv.org/abs/1510.04861 | id:1510.04861 author:Martin Blažević, Karla Brkić, Tomislav Hrkać category:cs.CV cs.MM  published:2015-10-16 summary:We propose a de-identification pipeline that protects the privacy of humans in video sequences by replacing them with rendered 3D human models, hence concealing their identity while retaining the naturalness of the scene. The original images of humans are steganographically encoded in the carrier image, i.e. the image containing the original scene and the rendered 3D human models. We qualitatively explore the feasibility of our approach, utilizing the Kinect sensor and its libraries to detect and localize human joints. A 3D avatar is rendered into the scene using the obtained joint positions, and the original human image is steganographically encoded in the new scene. Our qualitative evaluation shows reasonably good results that merit further exploration. version:1
arxiv-1510-04860 | Measurement of Road Traffic Parameters Based on Multi-Vehicle Tracking | http://arxiv.org/abs/1510.04860 | id:1510.04860 author:Kristian Kovačić, Edouard Ivanjko, Niko Jelušić category:cs.CV  published:2015-10-16 summary:Development of computing power and cheap video cameras enabled today's traffic management systems to include more cameras and computer vision applications for transportation system monitoring and control. Combined with image processing algorithms cameras are used as sensors to measure road traffic parameters like flow volume, origin-destination matrices, classify vehicles, etc. In this paper we propose a system for measurement of road traffic parameters (basic motion model parameters and macro-scopic traffic parameters). The system is based on Local Binary Pattern (LBP) image features classification with a cascade of Gentle Adaboost (GAB) classifiers to determine vehicle existence and its location in an image. Additionally, vehicle tracking and counting in a road traffic video is performed by using Extended Kalman Filter (EKF) and virtual markers. The newly proposed system is compared with a system based on background subtraction. Comparison is performed by the means of execution time and accuracy. version:1
arxiv-1509-00106 | Adaptive Smoothing Algorithms for Nonsmooth Composite Convex Minimization | http://arxiv.org/abs/1509.00106 | id:1509.00106 author:Quoc Tran-Dinh category:math.OC stat.ML  published:2015-09-01 summary:We propose novel adaptive smoothing algorithms based on Nesterov's smoothing technique in [26] for solving nonsmooth composite convex optimization problems. Our methods combine both Nesterov's accelerated proximal gradient scheme and a new homotopy strategy for smoothness parameter. By an appropriate choice of smoothing functions, we develop new algorithms that have upto the $\mathcal{O}\left(\frac{1}{\varepsilon}\right)$-optimal worst-case iteration complexity while allow one to automatically update the smoothness parameter at each iteration. We then further exploit the structure of problems to select smoothing functions and develop suitable algorithmic variants that reduce the complexity-per-iteration. We also specify our algorithms to solve constrained convex optimization problems and show their convergence guarantee on the primal sequence of iterates. We demonstrate our algorithms through three numerical examples and compare them with the nonadaptive algorithm in [26]. version:4
arxiv-1510-04842 | Multiresolution hierarchy co-clustering for semantic segmentation in sequences with small variations | http://arxiv.org/abs/1510.04842 | id:1510.04842 author:David Varas, Mónica Alfaro, Ferran Marques category:cs.CV  published:2015-10-16 summary:This paper presents a co-clustering technique that, given a collection of images and their hierarchies, clusters nodes from these hierarchies to obtain a coherent multiresolution representation of the image collection. We formalize the co-clustering as a Quadratic Semi-Assignment Problem and solve it with a linear programming relaxation approach that makes effective use of information from hierarchies. Initially, we address the problem of generating an optimal, coherent partition per image and, afterwards, we extend this method to a multiresolution framework. Finally, we particularize this framework to an iterative multiresolution video segmentation algorithm in sequences with small variations. We evaluate the algorithm on the Video Occlusion/Object Boundary Detection Dataset, showing that it produces state-of-the-art results in these scenarios. version:1
arxiv-1509-00151 | Learning A Task-Specific Deep Architecture For Clustering | http://arxiv.org/abs/1509.00151 | id:1509.00151 author:Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, Thomas S. Huang category:cs.LG cs.CV stat.ML  published:2015-09-01 summary:While sparse coding-based clustering methods have shown to be successful, their bottlenecks in both efficiency and scalability limit the practical usage. In recent years, deep learning has been proved to be a highly effective, efficient and scalable feature learning tool. In this paper, we propose to emulate the sparse coding-based clustering pipeline in the context of deep learning, leading to a carefully crafted deep model benefiting from both. A feed-forward network structure, named TAGnet, is constructed based on a graph-regularized sparse coding algorithm. It is then trained with task-specific loss functions from end to end. We discover that connecting deep learning to sparse coding benefits not only the model performance, but also its initialization and interpretation. Moreover, by introducing auxiliary clustering tasks to the intermediate feature hierarchy, we formulate DTAGnet and obtain a further performance boost. Extensive experiments demonstrate that the proposed model gains remarkable margins over several state-of-the-art methods. version:3
arxiv-1510-04780 | A Graph Traversal Based Approach to Answer Non-Aggregation Questions Over DBpedia | http://arxiv.org/abs/1510.04780 | id:1510.04780 author:Chenhao Zhu, Kan Ren, Xuan Liu, Haofen Wang, Yiding Tian, Yong Yu category:cs.CL cs.IR  published:2015-10-16 summary:We present a question answering system over DBpedia, filling the gap between user information needs expressed in natural language and a structured query interface expressed in SPARQL over the underlying knowledge base (KB). Given the KB, our goal is to comprehend a natural language query and provide corresponding accurate answers. Focusing on solving the non-aggregation questions, in this paper, we construct a subgraph of the knowledge base from the detected entities and propose a graph traversal method to solve both the semantic item mapping problem and the disambiguation problem in a joint way. Compared with existing work, we simplify the process of query intention understanding and pay more attention to the answer path ranking. We evaluate our method on a non-aggregation question dataset and further on a complete dataset. Experimental results show that our method achieves best performance compared with several state-of-the-art systems. version:1
arxiv-1510-02847 | Active Learning from Weak and Strong Labelers | http://arxiv.org/abs/1510.02847 | id:1510.02847 author:Chicheng Zhang, Kamalika Chaudhuri category:cs.LG stat.ML  published:2015-10-09 summary:An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible. This work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone. version:2
arxiv-1510-04734 | A Method for Modeling Co-Occurrence Propensity of Clinical Codes with Application to ICD-10-PCS Auto-Coding | http://arxiv.org/abs/1510.04734 | id:1510.04734 author:Michael Subotin, Anthony R. Davis category:cs.CL  published:2015-10-15 summary:Objective. Natural language processing methods for medical auto-coding, or automatic generation of medical billing codes from electronic health records, generally assign each code independently of the others. They may thus assign codes for closely related procedures or diagnoses to the same document, even when they do not tend to occur together in practice, simply because the right choice can be difficult to infer from the clinical narrative. Materials and Methods. We propose a method that injects awareness of the propensities for code co-occurrence into this process. First, a model is trained to estimate the conditional probability that one code is assigned by a human coder, given than another code is known to have been assigned to the same document. Then, at runtime, an iterative algorithm is used to apply this model to the output of an existing statistical auto-coder to modify the confidence scores of the codes. Results. We tested this method in combination with a primary auto-coder for ICD-10 procedure codes, achieving a 12% relative improvement in F-score over the primary auto-coder baseline. Discussion. The proposed method can be used, with appropriate features, in combination with any auto-coder that generates codes with different levels of confidence. Conclusion. The promising results obtained for ICD-10 procedure codes suggest that the proposed method may have wider applications in auto-coding. version:1
arxiv-1510-04706 | Shape Complexes in Continuous Max-Flow Hierarchical Multi-Labeling Problems | http://arxiv.org/abs/1510.04706 | id:1510.04706 author:John S. H. Baxter, Jing Yuan, Terry M. Peters category:cs.CV  published:2015-10-15 summary:Although topological considerations amongst multiple labels have been previously investigated in the context of continuous max-flow image segmentation, similar investigations have yet to be made about shape considerations in a general and extendable manner. This paper presents shape complexes for segmentation, which capture more complex shapes by combining multiple labels and super-labels constrained by geodesic star convexity. Shape complexes combine geodesic star convexity constraints with hierarchical label organization, which together allow for more complex shapes to be represented. This framework avoids the use of co-ordinate system warping techniques to convert shape constraints into topological constraints, which may be ambiguous or ill-defined for certain segmentation problems. version:1
arxiv-1507-08905 | Deep Networks for Image Super-Resolution with Sparse Prior | http://arxiv.org/abs/1507.08905 | id:1507.08905 author:Zhaowen Wang, Ding Liu, Jianchao Yang, Wei Han, Thomas Huang category:cs.CV  published:2015-07-31 summary:Deep learning techniques have been successfully applied in many areas of computer vision, including low-level image restoration problems. For image super-resolution, several models based on deep neural networks have been recently proposed and attained superior performance that overshadows all previous handcrafted models. The question then arises whether large-capacity and data-driven models have become the dominant solution to the ill-posed super-resolution problem. In this paper, we argue that domain expertise represented by the conventional sparse coding model is still valuable, and it can be combined with the key ingredients of deep learning to achieve further improved results. We show that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network, and trained in a cascaded structure from end to end. The interpretation of the network based on sparse coding leads to much more efficient and effective training, as well as a reduced model size. Our model is evaluated on a wide range of images, and shows clear advantage over existing state-of-the-art methods in terms of both restoration accuracy and human subjective quality. version:4
arxiv-1510-04609 | Layer-Specific Adaptive Learning Rates for Deep Networks | http://arxiv.org/abs/1510.04609 | id:1510.04609 author:Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, Gavin Taylor category:cs.CV cs.AI cs.LG cs.NE  published:2015-10-15 summary:The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to vanishing gradients, in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms. version:1
arxiv-1510-04600 | Telemedicine as a special case of Machine Translation | http://arxiv.org/abs/1510.04600 | id:1510.04600 author:Krzysztof Wołk, Krzysztof Marasek, Wojciech Glinkowski category:cs.CL  published:2015-10-15 summary:Machine translation is evolving quite rapidly in terms of quality. Nowadays, we have several machine translation systems available in the web, which provide reasonable translations. However, these systems are not perfect, and their quality may decrease in some specific domains. This paper examines the effects of different training methods when it comes to Polish - English Statistical Machine Translation system used for the medical data. Numerous elements of the EMEA parallel text corpora and not related OPUS Open Subtitles project were used as the ground for creation of phrase tables and different language models including the development, tuning and testing of these translation systems. The BLEU, NIST, METEOR, and TER metrics have been used in order to evaluate the results of various systems. Our experiments deal with the systems that include POS tagging, factored phrase models, hierarchical models, syntactic taggers, and other alignment methods. We also executed a deep analysis of Polish data as preparatory work before automatized data processing such as true casing or punctuation normalization phase. Normalized metrics was used to compare results. Scores lower than 15% mean that Machine Translation engine is unable to provide satisfying quality, scores greater than 30% mean that translations should be understandable without problems and scores over 50 reflect adequate translations. The average results of Polish to English translations scores for BLEU, NIST, METEOR, and TER were relatively high and ranged from 70,58 to 82,72. The lowest score was 64,38. The average results ranges for English to Polish translations were little lower (67,58 - 78,97). The real-life implementations of presented high quality Machine Translation Systems are anticipated in general medical practice and telemedicine. version:1
arxiv-1502-06648 | Recognizing Fine-Grained and Composite Activities using Hand-Centric Features and Script Data | http://arxiv.org/abs/1502.06648 | id:1502.06648 author:Marcus Rohrbach, Anna Rohrbach, Michaela Regneri, Sikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, Bernt Schiele category:cs.CV  published:2015-02-23 summary:Activity recognition has shown impressive progress in recent years. However, the challenges of detecting fine-grained activities and understanding how they are combined into composite activities have been largely overlooked. In this work we approach both tasks and present a dataset which provides detailed annotations to address them. The first challenge is to detect fine-grained activities, which are defined by low inter-class variability and are typically characterized by fine-grained body motions. We explore how human pose and hands can help to approach this challenge by comparing two pose-based and two hand-centric features with state-of-the-art holistic features. To attack the second challenge, recognizing composite activities, we leverage the fact that these activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts. We show the benefits of our hand-centric approach for fine-grained activity classification and detection. For composite activity recognition we find that decomposition into attributes allows sharing information across composites and is essential to attack this hard task. Using script data we can recognize novel composites without having training data for them. version:2
arxiv-1510-04585 | A Brief Survey of Image Processing Algorithms in Electrical Capacitance Tomography | http://arxiv.org/abs/1510.04585 | id:1510.04585 author:Kezhi Li category:cs.CV  published:2015-10-15 summary:To study the fundamental physics of complex multiphase flow systems using advanced measurement techniques, especially the electrical capacitance tomography (ECT) approach, this article carries out an initial literature review of the ECT method from a point of view of signal processing and algorithm design. After introducing the physical laws governing the ECT system, we will focus on various reconstruction techniques that are capable to recover the image of the internal characteristics of a specified region based on the measuring capacitances of multi-electrode sensors surrounding the region. Each technique has its own advantages and limitations, and many algorithms have been examined by simulations or experiments. Future researches in 3D reconstruction and other potential improvements of the system are discussed in the end. version:1
arxiv-1510-04565 | Beyond Spatial Pyramid Matching: Space-time Extended Descriptor for Action Recognition | http://arxiv.org/abs/1510.04565 | id:1510.04565 author:Zhenzhong Lan, Alexander G. Hauptmann category:cs.CV  published:2015-10-15 summary:We address the problem of generating video features for action recognition. The spatial pyramid and its variants have been very popular feature models due to their success in balancing spatial location encoding and spatial invariance. Although it seems straightforward to extend spatial pyramid to the temporal domain (spatio-temporal pyramid), the large spatio-temporal diversity of unconstrained videos and the resulting significantly higher dimensional representations make it less appealing. This paper introduces the space-time extended descriptor, a simple but efficient alternative way to include the spatio-temporal location into the video features. Instead of only coding motion information and leaving the spatio-temporal location to be represented at the pooling stage, location information is used as part of the encoding step. This method is a much more effective and efficient location encoding method as compared to the fixed grid model because it avoids the danger of over committing to artificial boundaries and its dimension is relatively low. Experimental results on several benchmark datasets show that, despite its simplicity, this method achieves comparable or better results than spatio-temporal pyramid. version:1
arxiv-1510-04563 | Elasticity-based Matching by Minimizing the Symmetric Difference of Shapes | http://arxiv.org/abs/1510.04563 | id:1510.04563 author:Konrad Simon, Ronen Basri category:cs.CV cs.CG  published:2015-10-15 summary:We consider the problem of matching two shapes assuming these shapes are related by an elastic deformation. Using linearized elasticity theory and the finite element method we seek an elastic deformation that is caused by simple external boundary forces and accounts for the difference between the two shapes. Our main contribution is in proposing a cost function and an optimization procedure to minimize the symmetric difference between the deformed and the target shapes as an alternative to point matches that guide the matching in other techniques. We show how to approximate the nonlinear optimization problem by a sequence of convex problems. We demonstrate the utility of our method in experiments and compare it to an ICP-like matching algorithm. version:1
arxiv-1510-04500 | Noisy-parallel and comparable corpora filtering methodology for the extraction of bi-lingual equivalent data at sentence level | http://arxiv.org/abs/1510.04500 | id:1510.04500 author:Krzysztof Wołk category:cs.CL  published:2015-10-15 summary:Text alignment and text quality are critical to the accuracy of Machine Translation (MT) systems, some NLP tools, and any other text processing tasks requiring bilingual data. This research proposes a language independent bi-sentence filtering approach based on Polish (not a position-sensitive language) to English experiments. This cleaning approach was developed on the TED Talks corpus and also initially tested on the Wikipedia comparable corpus, but it can be used for any text domain or language pair. The proposed approach implements various heuristics for sentence comparison. Some of them leverage synonyms and semantic and structural analysis of text as additional information. Minimization of data loss was ensured. An improvement in MT system score with text processed using the tool is discussed. version:1
arxiv-1510-04493 | Sparsity-aware Possibilistic Clustering Algorithms | http://arxiv.org/abs/1510.04493 | id:1510.04493 author:Spyridoula D. Xenaki, Konstantinos D. Koutroumbas, Athanasios A. Rontogiannis category:cs.CV  published:2015-10-15 summary:In this paper two novel possibilistic clustering algorithms are presented, which utilize the concept of sparsity. The first one, called sparse possibilistic c-means, exploits sparsity and can deal well with closely located clusters that may also be of significantly different densities. The second one, called sparse adaptive possibilistic c-means, is an extension of the first, where now the involved parameters are dynamically adapted. The latter can deal well with even more challenging cases, where, in addition to the above, clusters may be of significantly different variances. More specifically, it provides improved estimates of the cluster representatives, while, in addition, it has the ability to estimate the actual number of clusters, given an overestimate of it. Extensive experimental results on both synthetic and real data sets support the previous statements. version:1
arxiv-1509-05177 | Some Theorems for Feed Forward Neural Networks | http://arxiv.org/abs/1509.05177 | id:1509.05177 author:K. Eswaran, Vishwajeet Singh category:cs.NE 62M45  published:2015-09-17 summary:In this paper we introduce a new method which employs the concept of "Orientation Vectors" to train a feed forward neural network and suitable for problems where large dimensions are involved and the clusters are characteristically sparse. The new method is not NP hard as the problem size increases. We `derive' the method by starting from Kolmogrov's method and then relax some of the stringent conditions. We show for most classification problems three layers are sufficient and the network size depends on the number of clusters. We prove as the number of clusters increase from N to N+dN the number of processing elements in the first layer only increases by d(logN), and are proportional to the number of classes, and the method is not NP hard. Many examples are solved to demonstrate that the method of Orientation Vectors requires much less computational effort than Radial Basis Function methods and other techniques wherein distance computations are required, in fact the present method increases logarithmically with problem size compared to the Radial Basis Function method and the other methods which depend on distance computations e.g statistical methods where probabilistic distances are calculated. A practical method of applying the concept of Occum's razor to choose between two architectures which solve the same classification problem has been described. The ramifications of the above findings on the field of Deep Learning have also been briefly investigated and we have found that it directly leads to the existence of certain types of NN architectures which can be used as a "mapping engine", which has the property of "invertibility", thus improving the prospect of their deployment for solving problems involving Deep Learning and hierarchical classification. The latter possibility has a lot of future scope in the areas of machine learning and cloud computing. version:4
arxiv-1412-3613 | A Novel Adaptive Possibilistic Clustering Algorithm | http://arxiv.org/abs/1412.3613 | id:1412.3613 author:Spyridoula D. Xenaki, Konstantinos D. Koutroumbas, Athanasios A. Rontogiannis category:cs.CV  published:2014-12-11 summary:In this paper a novel possibilistic c-means clustering algorithm, called Adaptive Possibilistic c-means, is presented. Its main feature is that {\it all} its parameters, after their initialization, are properly adapted during its execution. Provided that the algorithm starts with a reasonable overestimate of the number of physical clusters formed by the data, it is capable, in principle, to unravel them (a long-standing issue in the clustering literature). This is due to the fully adaptive nature of the proposed algorithm that enables the removal of the clusters that gradually become obsolete. In addition, the adaptation of all its parameters increases the flexibility of the algorithm in following the variations in the formation of the clusters that occur from iteration to iteration. Theoretical results that are indicative of the convergence behavior of the algorithm are also provided. Finally, extensive simulation results on both synthetic and real data highlight the effectiveness of the proposed algorithm. version:3
arxiv-1510-04454 | Online Markov decision processes with policy iteration | http://arxiv.org/abs/1510.04454 | id:1510.04454 author:Yao Ma, Hao Zhang, Masashi Sugiyama category:cs.LG  published:2015-10-15 summary:The online Markov decision process (MDP) is a generalization of the classical Markov decision process that incorporates changing reward functions. In this paper, we propose practical online MDP algorithms with policy iteration and theoretically establish a sublinear regret bound. A notable advantage of the proposed algorithm is that it can be easily combined with function approximation, and thus large and possibly continuous state spaces can be efficiently handled. Through experiments, we demonstrate the usefulness of the proposed algorithm. version:1
arxiv-1508-04186 | Distributed Deep Q-Learning | http://arxiv.org/abs/1508.04186 | id:1508.04186 author:Hao Yi Ong, Kevin Chavez, Augustus Hong category:cs.LG cs.AI cs.DC cs.NE  published:2015-08-18 summary:We propose a distributed deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is based on the deep Q-network, a convolutional neural network trained with a variant of Q-learning. Its input is raw pixels and its output is a value function estimating future rewards from taking an action given a system state. To distribute the deep Q-network training, we adapt the DistBelief software framework to the context of efficiently training reinforcement learning agents. As a result, the method is completely asynchronous and scales well with the number of machines. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to achieve reasonable success on a simple game with minimal parameter tuning. version:2
arxiv-1510-04445 | DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers | http://arxiv.org/abs/1510.04445 | id:1510.04445 author:Amir Ghodrati, Ali Diba, Marco Pedersoli, Tinne Tuytelaars, Luc Van Gool category:cs.CV  published:2015-10-15 summary:In this paper we evaluate the quality of the activation layers of a convolutional neural network (CNN) for the gen- eration of object proposals. We generate hypotheses in a sliding-window fashion over different activation layers and show that the final convolutional layers can find the object of interest with high recall but poor localization due to the coarseness of the feature maps. Instead, the first layers of the network can better localize the object of interest but with a reduced recall. Based on this observation we design a method for proposing object locations that is based on CNN features and that combines the best of both worlds. We build an inverse cascade that, going from the final to the initial convolutional layers of the CNN, selects the most promising object locations and refines their boxes in a coarse-to-fine manner. The method is efficient, because i) it uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals due to the inverse coarse-to-fine cascade. The method is also accurate; it outperforms most of the previously proposed object proposals approaches and when plugged into a CNN-based detector produces state-of-the- art detection performance. version:1
arxiv-1510-04437 | A Novel Approach for Human Action Recognition from Silhouette Images | http://arxiv.org/abs/1510.04437 | id:1510.04437 author:Satyabrata Maity, Debotosh Bhattacharjee, Amlan Chakrabarti category:cs.CV  published:2015-10-15 summary:In this paper, a novel human action recognition technique from video is presented. Any action of human is a combination of several micro action sequences performed by one or more body parts of the human. The proposed approach uses spatio-temporal body parts movement (STBPM) features extracted from foreground silhouette of the human objects. The newly proposed STBPM feature estimates the movements of different body parts for any given time segment to classify actions. We also proposed a rule based logic named rule action classifier (RAC), which uses a series of condition action rules based on prior knowledge and hence does not required training to classify any action. Since we don't require training to classify actions, the proposed approach is view independent. The experimental results on publicly available Wizeman and MuHVAi datasets are compared with that of the related research work in terms of accuracy in the human action detection, and proposed technique outperforms the others. version:1
arxiv-1412-8697 | On Semiparametric Exponential Family Graphical Models | http://arxiv.org/abs/1412.8697 | id:1412.8697 author:Zhuoran Yang, Yang Ning, Han Liu category:stat.ML  published:2014-12-30 summary:We propose a new class of semiparametric exponential family graphical models for the analysis of high dimensional mixed data. Different from the existing mixed graphical models, we allow the nodewise conditional distributions to be semiparametric generalized linear models with unspecified base measure functions. Thus, one advantage of our method is that it is unnecessary to specify the type of each node and the method is more convenient to apply in practice. Under the proposed model, we consider both problems of parameter estimation and hypothesis testing in high dimensions. In particular, we propose a symmetric pairwise score test for the presence of a single edge in the graph. Compared to the existing methods for hypothesis tests, our approach takes into account of the symmetry of the parameters, such that the inferential results are invariant with respect to the different parametrizations of the same edge. Thorough numerical simulations and a real data example are provided to back up our results. version:2
arxiv-1510-04396 | Filtrated Spectral Algebraic Subspace Clustering | http://arxiv.org/abs/1510.04396 | id:1510.04396 author:Manolis C. Tsakiris, Rene Vidal category:cs.CV cs.LG  published:2015-10-15 summary:Algebraic Subspace Clustering (ASC) is a simple and elegant method based on polynomial fitting and differentiation for clustering noiseless data drawn from an arbitrary union of subspaces. In practice, however, ASC is limited to equi-dimensional subspaces because the estimation of the subspace dimension via algebraic methods is sensitive to noise. This paper proposes a new ASC algorithm that can handle noisy data drawn from subspaces of arbitrary dimensions. The key ideas are (1) to construct, at each point, a decreasing sequence of subspaces containing the subspace passing through that point; (2) to use the distances from any other point to each subspace in the sequence to construct a subspace clustering affinity, which is superior to alternative affinities both in theory and in practice. Experiments on the Hopkins 155 dataset demonstrate the superiority of the proposed method with respect to sparse and low rank subspace clustering methods. version:1
arxiv-1510-04390 | Dual Principal Component Pursuit | http://arxiv.org/abs/1510.04390 | id:1510.04390 author:Manolis C. Tsakiris, Rene Vidal category:cs.CV cs.LG  published:2015-10-15 summary:We consider the problem of outlier rejection in single subspace learning. Classical approaches work directly with a low-dimensional representation of the subspace. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement. We pose this problem as an $\ell_1$-minimization problem on the sphere and show that, under certain conditions on the distribution of the data, any global minimizer of this non-convex problem gives a vector orthogonal to the subspace. Moreover, we show that such a vector can still be found by relaxing the non-convex problem with a sequence of linear programs. Experiments on synthetic and real data show that the proposed approach, which we call Dual Principal Component Pursuit (DPCP), outperforms state-of-the art methods, especially in the case of high-dimensional subspaces. version:1
arxiv-1510-04389 | Sketch-based Manga Retrieval using Manga109 Dataset | http://arxiv.org/abs/1510.04389 | id:1510.04389 author:Yusuke Matsui, Kota Ito, Yuji Aramaki, Toshihiko Yamasaki, Kiyoharu Aizawa category:cs.CV cs.IR cs.MM  published:2015-10-15 summary:Manga (Japanese comics) are popular worldwide. However, current e-manga archives offer very limited search support, including keyword-based search by title or author, or tag-based categorization. To make the manga search experience more intuitive, efficient, and enjoyable, we propose a content-based manga retrieval system. First, we propose a manga-specific image-describing framework. It consists of efficient margin labeling, edge orientation histogram feature description, and approximate nearest-neighbor search using product quantization. Second, we propose a sketch-based interface as a natural way to interact with manga content. The interface provides sketch-based querying, relevance feedback, and query retouch. For evaluation, we built a novel dataset of manga images, Manga109, which consists of 109 comic books of 21,142 pages drawn by professional manga artists. To the best of our knowledge, Manga109 is currently the biggest dataset of manga images available for research. We conducted a comparative study, a localization evaluation, and a large-scale qualitative study. From the experiments, we verified that: (1) the retrieval accuracy of the proposed method is higher than those of previous methods; (2) the proposed method can localize an object instance with reasonable runtime and accuracy; and (3) sketch querying is useful for manga search. version:1
arxiv-1510-04378 | Robust Learning for Optimal Treatment Decision with NP-Dimensionality | http://arxiv.org/abs/1510.04378 | id:1510.04378 author:Chengchun Shi, Rui Song, Wenbin Lu category:stat.ML  published:2015-10-15 summary:In order to identify important variables that are involved in making optimal treatment decision, Lu et al. (2013) proposed a penalized least squared regression framework for a fixed number of predictors, which is robust against the misspecification of the conditional mean model. Two problems arise: (i) in a world of explosively big data, effective methods are needed to handle ultra-high dimensional data set, for example, with the dimension of predictors is of the non-polynomial (NP) order of the sample size; (ii) both the propensity score and conditional mean models need to be estimated from data under NP dimensionality. In this paper, we propose a two-step estimation procedure for deriving the optimal treatment regime under NP dimensionality. In both steps, penalized regressions are employed with the non-concave penalty function, where the conditional mean model of the response given predictors may be misspecified. The asymptotic properties, such as weak oracle properties, selection consistency and oracle distributions, of the proposed estimators are investigated. In addition, we study the limiting distribution of the estimated value function for the obtained optimal treatment regime. The empirical performance of the proposed estimation method is evaluated by simulations and an application to a depression dataset from the STAR*D study. version:1
arxiv-1510-04373 | Scatter Component Analysis: A Unified Framework for Domain Adaptation and Domain Generalization | http://arxiv.org/abs/1510.04373 | id:1510.04373 author:Muhammad Ghifary, David Balduzzi, W. Bastiaan Kleijn, Mengjie Zhang category:cs.CV  published:2015-10-15 summary:This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation can leverage unlabeled target information, while domain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. SCA is based on a simple geometrical measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA finds a representation that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separability of data; each of which is quantified through scatter. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which results in a fast and exact solution. Comprehensive experiments on benchmark cross-domain object recognition datasets verify that SCA performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization. We also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation. version:1
arxiv-1505-08052 | Batch Bayesian Optimization via Local Penalization | http://arxiv.org/abs/1505.08052 | id:1505.08052 author:Javier González, Zhenwen Dai, Philipp Hennig, Neil D. Lawrence category:stat.ML  published:2015-05-29 summary:The popularity of Bayesian optimization methods for efficient exploration of parameter spaces has lead to a series of papers applying Gaussian processes as surrogates in the optimization of functions. However, most proposed approaches only allow the exploration of the parameter space to occur sequentially. Often, it is desirable to simultaneously propose batches of parameter values to explore. This is particularly the case when large parallel processing facilities are available. These facilities could be computational or physical facets of the process being optimized. E.g. in biological experiments many experimental set ups allow several samples to be simultaneously processed. Batch methods, however, require modeling of the interaction between the evaluations in the batch, which can be expensive in complex scenarios. We investigate a simple heuristic based on an estimate of the Lipschitz constant that captures the most important aspect of this interaction (i.e. local repulsion) at negligible computational overhead. The resulting algorithm compares well, in running time, with much more elaborate alternatives. The approach assumes that the function of interest, $f$, is a Lipschitz continuous function. A wrap-loop around the acquisition function is used to collect batches of points of certain size minimizing the non-parallelizable computational effort. The speed-up of our method with respect to previous approaches is significant in a set of computationally expensive experiments. version:4
arxiv-1510-04356 | Group-Invariant Subspace Clustering | http://arxiv.org/abs/1510.04356 | id:1510.04356 author:Shuchin Aeron, Eric Kernfeld category:cs.IT cs.LG math.IT stat.ML  published:2015-10-15 summary:In this paper we consider the problem of group invariant subspace clustering where the data is assumed to come from a union of group-invariant subspaces of a vector space, i.e. subspaces which are invariant with respect to action of a given group. Algebraically, such group-invariant subspaces are also referred to as submodules. Similar to the well known Sparse Subspace Clustering approach where the data is assumed to come from a union of subspaces, we analyze an algorithm which, following a recent work [1], we refer to as Sparse Sub-module Clustering (SSmC). The method is based on finding group-sparse self-representation of data points. In this paper we primarily derive general conditions under which such a group-invariant subspace identification is possible. In particular we extend the geometric analysis in [2] and in the process we identify a related problem in geometric functional analysis. version:1
arxiv-1509-00154 | Tumor Motion Tracking in Liver Ultrasound Images Using Mean Shift and Active Contour | http://arxiv.org/abs/1509.00154 | id:1509.00154 author:Jalil Rasekhi category:cs.CV stat.ML  published:2015-09-01 summary:In this paper we present a new method for motion tracking of tumors in liver ultrasound image sequences. Our algorithm has two main steps. In the first step, we apply mean shift algorithm with multiple features to estimate the center of the target in each frame. Target in the first frame is defined using an ellipse. Edge, texture, and intensity features are extracted from the first frame, and then mean shift algorithm is applied to each feature separately to find the center of ellipse related to that feature in the next frame. The center of ellipse will be the weighted average of these centers. By using mean shift actually we estimate the target movement between two consecutive frames. Once the correct ellipsoid in each frame is known, in the second step we apply the Dynamic Directional Gradient Vector Flow (DDGVF) version of active contour models, in order to find the correct boundary of tumors. We sample a few points on the boundary of active contour then translate those points based on the translation of the center of ellipsoid in two consecutive frames to determine the target movement. We use these translated sample points as an initial guess for active contour in the next frame. Our experimental results show that, the suggested method provides a reliable performance for liver tumor tracking in ultrasound image sequences. version:4
arxiv-1409-8359 | Backhaul-Constrained Multi-Cell Cooperation Leveraging Sparsity and Spectral Clustering | http://arxiv.org/abs/1409.8359 | id:1409.8359 author:Swayambhoo Jain, Seung-Jun Kim, Georgios B. Giannakis category:cs.IT cs.NI math.IT stat.ML  published:2014-09-30 summary:Multi-cell cooperative processing with limited backhaul traffic is studied for cellular uplinks. Aiming at reduced backhaul overhead, a sparsity-regularized multi-cell receive-filter design problem is formulated. Both unstructured distributed cooperation as well as clustered cooperation, in which base station groups are formed for tight cooperation, are considered. Dynamic clustered cooperation, where the sparse equalizer and the cooperation clusters are jointly determined, is solved via alternating minimization based on spectral clustering and group-sparse regression. Furthermore, decentralized implementations of both unstructured and clustered cooperation schemes are developed for scalability, robustness and computational efficiency. Extensive numerical tests verify the efficacy of the proposed methods. version:2
arxiv-1505-01809 | Language Models for Image Captioning: The Quirks and What Works | http://arxiv.org/abs/1505.01809 | id:1505.01809 author:Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell category:cs.CL cs.AI cs.CV cs.LG  published:2015-05-07 summary:Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments. version:3
arxiv-1509-03001 | Real-time Sign Language Fingerspelling Recognition using Convolutional Neural Networks from Depth map | http://arxiv.org/abs/1509.03001 | id:1509.03001 author:Byeongkeun Kang, Subarna Tripathi, Truong Q. Nguyen category:cs.CV  published:2015-09-10 summary:Sign language recognition is important for natural and convenient communication between deaf community and hearing majority. We take the highly efficient initial step of automatic fingerspelling recognition system using convolutional neural networks (CNNs) from depth maps. In this work, we consider relatively larger number of classes compared with the previous literature. We train CNNs for the classification of 31 alphabets and numbers using a subset of collected depth data from multiple subjects. While using different learning configurations, such as hyper-parameter selection with and without validation, we achieve 99.99% accuracy for observed signers and 83.58% to 85.49% accuracy for new signers. The result shows that accuracy improves as we include more data from different subjects during training. The processing time is 3 ms for the prediction of a single image. To the best of our knowledge, the system achieves the highest accuracy and speed. The trained model and dataset is available on our repository. version:3
arxiv-1510-04238 | Dynamical spectral unmixing of multitemporal hyperspectral images | http://arxiv.org/abs/1510.04238 | id:1510.04238 author:Simon Henrot, Jocelyn Chanussot, Christian Jutten category:cs.CV  published:2015-10-14 summary:In this paper, we consider the problem of unmixing a time series of hyperspectral images. We propose a dynamical model based on linear mixing processes at each time instant. The spectral signatures and fractional abundances of the pure materials in the scene are seen as latent variables, and assumed to follow a general dynamical structure. Based on a simplified version of this model, we derive an efficient spectral unmixing algorithm to estimate the latent variables by performing alternating minimizations. The performance of the proposed approach is demonstrated on synthetic and real multitemporal hyperspectral images. version:1
arxiv-1504-01515 | Simultaneously sparse and low-rank abundance matrix estimation for hyperspectral image unmixing | http://arxiv.org/abs/1504.01515 | id:1504.01515 author:Paris Giampouras, Konstantinos Themelis, Athanasios Rontogiannis, Konstantinos Koutroumbas category:cs.CV math.OC stat.ML  published:2015-04-07 summary:In a plethora of applications dealing with inverse problems, e.g. in image processing, social networks, compressive sensing, biological data processing etc., the signal of interest is known to be structured in several ways at the same time. This premise has recently guided the research to the innovative and meaningful idea of imposing multiple constraints on the parameters involved in the problem under study. For instance, when dealing with problems whose parameters form sparse and low-rank matrices, the adoption of suitably combined constraints imposing sparsity and low-rankness, is expected to yield substantially enhanced estimation results. In this paper, we address the spectral unmixing problem in hyperspectral images. Specifically, two novel unmixing algorithms are introduced, in an attempt to exploit both spatial correlation and sparse representation of pixels lying in homogeneous regions of hyperspectral images. To this end, a novel convex mixed penalty term is first defined consisting of the sum of the weighted $\ell_1$ and the weighted nuclear norm of the abundance matrix corresponding to a small area of the image determined by a sliding square window. This penalty term is then used to regularize a conventional quadratic cost function and impose simultaneously sparsity and row-rankness on the abundance matrix. The resulting regularized cost function is minimized by a) an incremental proximal sparse and low-rank unmixing algorithm and b) an algorithm based on the alternating minimization method of multipliers (ADMM). The effectiveness of the proposed algorithms is illustrated in experiments conducted both on simulated and real data. version:2
arxiv-1510-03203 | VB calibration to improve the interface between phone recognizer and i-vector extractor | http://arxiv.org/abs/1510.03203 | id:1510.03203 author:Niko Brümmer category:stat.ML cs.LG  published:2015-10-12 summary:The EM training algorithm of the classical i-vector extractor is often incorrectly described as a maximum-likelihood method. The i-vector model is however intractable: the likelihood itself and the hidden-variable posteriors needed for the EM algorithm cannot be computed in closed form. We show here that the classical i-vector extractor recipe is actually a mean-field variational Bayes (VB) recipe. This theoretical VB interpretation turns out to be of further use, because it also offers an interpretation of the newer phonetic i-vector extractor recipe, thereby unifying the two flavours of extractor. More importantly, the VB interpretation is also practically useful: it suggests ways of modifying existing i-vector extractors to make them more accurate. In particular, in existing methods, the approximate VB posterior for the GMM states is fixed, while only the parameters of the generative model are adapted. Here we explore the possibility of also mildly adjusting (calibrating) those posteriors, so that they better fit the generative model. version:2
arxiv-1510-04163 | Embarrassingly Parallel Variational Inference in Nonconjugate Models | http://arxiv.org/abs/1510.04163 | id:1510.04163 author:Willie Neiswanger, Chong Wang, Eric Xing category:stat.ML cs.AI cs.DC cs.LG stat.CO  published:2015-10-14 summary:We develop a parallel variational inference (VI) procedure for use in data-distributed settings, where each machine only has access to a subset of data and runs VI independently, without communicating with other machines. This type of "embarrassingly parallel" procedure has recently been developed for MCMC inference algorithms; however, in many cases it is not possible to directly extend this procedure to VI methods without requiring certain restrictive exponential family conditions on the form of the model. Furthermore, most existing (nonparallel) VI methods are restricted to use on conditionally conjugate models, which limits their applicability. To combat these issues, we make use of the recently proposed nonparametric VI to facilitate an embarrassingly parallel VI procedure that can be applied to a wider scope of models, including to nonconjugate models. We derive our embarrassingly parallel VI algorithm, analyze our method theoretically, and demonstrate our method empirically on a few nonconjugate models. version:1
arxiv-1510-04130 | A Bayesian Network Model for Interesting Itemsets | http://arxiv.org/abs/1510.04130 | id:1510.04130 author:Jaroslav Fowkes, Charles Sutton category:stat.ML cs.DB cs.LG  published:2015-10-14 summary:Mining itemsets that are the most interesting under a statistical model of the underlying data is a frequently used and well-studied technique for exploratory data analysis. The most recent models of interestingness are predominantly based on maximum entropy distributions over items or tile entries with itemset constraints, and while computationally tractable are not easily interpretable. We therefore propose the first, to the best of our knowledge, generative model over itemsets, in the form of a Bayesian network, and an associated novel measure of interestingness. Our model is able to efficiently infer interesting itemsets directly from the transaction database using structural EM, in which the E-step employs the greedy approximation to weighted set cover. Our approach is theoretically simple, straightforward to implement, trivially parallelizable and exhibits competitive performance as we demonstrate on both synthetic and real-world examples. version:1
arxiv-1508-01717 | Structure Learning with Bow-free Acyclic Path Diagrams | http://arxiv.org/abs/1508.01717 | id:1508.01717 author:Christopher Nowzohour, Marloes H. Maathuis, Peter Bühlmann category:stat.ML  published:2015-08-07 summary:We consider the problem of structure learning for bow-free acyclic path diagrams (BAPs). BAPs can be viewed as a generalization of linear Gaussian DAG models that allow for certain hidden variables. We present a first method for this problem using a greedy score-based search algorithm. We also investigate some distributional equivalence properties of BAPs which are used in an algorithmic approach to compute (nearly) equivalent model structures, allowing to infer lower bounds of causal effects. The application of our method to some datasets reveals that BAP models can represent the data much better than DAG models in these cases. version:2
arxiv-1506-06534 | Distributional Sentence Entailment Using Density Matrices | http://arxiv.org/abs/1506.06534 | id:1506.06534 author:Esma Balkir, Mehrnoosh Sadrzadeh, Bob Coecke category:cs.CL cs.IT cs.LO math.CT math.IT  published:2015-06-22 summary:Categorical compositional distributional model of Coecke et al. (2010) suggests a way to combine grammatical composition of the formal, type logical models with the corpus based, empirical word representations of distributional semantics. This paper contributes to the project by expanding the model to also capture entailment relations. This is achieved by extending the representations of words from points in meaning space to density operators, which are probability distributions on the subspaces of the space. A symmetric measure of similarity and an asymmetric measure of entailment is defined, where lexical entailment is measured using von Neumann entropy, the quantum variant of Kullback-Leibler divergence. Lexical entailment, combined with the composition map on word representations, provides a method to obtain entailment relations on the level of sentences. Truth theoretic and corpus-based examples are provided. version:2
arxiv-1510-04074 | Fine-Grained Product Class Recognition for Assisted Shopping | http://arxiv.org/abs/1510.04074 | id:1510.04074 author:Marian George, Dejan Mircic, Gábor Sörös, Christian Floerkemeier, Friedemann Mattern category:cs.CV  published:2015-10-14 summary:Assistive solutions for a better shopping experience can improve the quality of life of people, in particular also of visually impaired shoppers. We present a system that visually recognizes the fine-grained product classes of items on a shopping list, in shelves images taken with a smartphone in a grocery store. Our system consists of three components: (a) We automatically recognize useful text on product packaging, e.g., product name and brand, and build a mapping of words to product classes based on the large-scale GroceryProducts dataset. When the user populates the shopping list, we automatically infer the product class of each entered word. (b) We perform fine-grained product class recognition when the user is facing a shelf. We discover discriminative patches on product packaging to differentiate between visually similar product classes and to increase the robustness against continuous changes in product design. (c) We continuously improve the recognition accuracy through active learning. Our experiments show the robustness of the proposed method against cross-domain challenges, and the scalability to an increasing number of products with minimal re-training. version:1
arxiv-1412-8319 | Quantifying origin and character of long-range correlations in narrative texts | http://arxiv.org/abs/1412.8319 | id:1412.8319 author:Stanisław Drożdż, Paweł Oświęcimka, Andrzej Kulig, Jarosław Kwapień, Katarzyna Bazarnik, Iwona Grabska-Gradzińska, Jan Rybicki, Marek Stanuszek category:cs.CL physics.soc-ph  published:2014-12-29 summary:In natural language using short sentences is considered efficient for communication. However, a text composed exclusively of such sentences looks technical and reads boring. A text composed of long ones, on the other hand, demands significantly more effort for comprehension. Studying characteristics of the sentence length variability (SLV) in a large corpus of world-famous literary texts shows that an appealing and aesthetic optimum appears somewhere in between and involves selfsimilar, cascade-like alternation of various lengths sentences. A related quantitative observation is that the power spectra S(f) of thus characterized SLV universally develop a convincing `1/f^beta' scaling with the average exponent beta =~ 1/2, close to what has been identified before in musical compositions or in the brain waves. An overwhelming majority of the studied texts simply obeys such fractal attributes but especially spectacular in this respect are hypertext-like, "stream of consciousness" novels. In addition, they appear to develop structures characteristic of irreducibly interwoven sets of fractals called multifractals. Scaling of S(f) in the present context implies existence of the long-range correlations in texts and appearance of multifractality indicates that they carry even a nonlinear component. A distinct role of the full stops in inducing the long-range correlations in texts is evidenced by the fact that the above quantitative characteristics on the long-range correlations manifest themselves in variation of the full stops recurrence times along texts, thus in SLV, but to a much lesser degree in the recurrence times of the most frequent words. In this latter case the nonlinear correlations, thus multifractality, disappear even completely for all the texts considered. Treated as one extra word, the full stops at the same time appear to obey the Zipfian rank-frequency distribution, however. version:2
arxiv-1510-04004 | Multiresolution Search of the Rigid Motion Space for Intensity Based Registration | http://arxiv.org/abs/1510.04004 | id:1510.04004 author:Behrooz Nasihatkon, Fredrik Kahl category:cs.CV  published:2015-10-14 summary:We study the relation between the target functions of low-resolution and high-resolution intensity-based registration for the class of rigid transformations. Our results show that low resolution target values can tightly bound the high-resolution target function in natural images. This can help with analyzing and better understanding the process of multiresolution image registration. It also gives a guideline for designing multiresolution algorithms in which the search space in higher resolution registration is restricted given the fitness values for lower resolution image pairs. To demonstrate this, we incorporate our multiresolution technique into a Lipschitz global optimization framework. We show that using the multiresolution scheme can result in large gains in the efficiency of such algorithms. The method is evaluated by applying to 2D and 3D registration problems as well as the detection of reflective symmetry in 2D and 3D images. version:1
arxiv-1508-07192 | Varying-coefficient models with isotropic Gaussian process priors | http://arxiv.org/abs/1508.07192 | id:1508.07192 author:Matthias Bussas, Christoph Sawade, Tobias Scheffer, Niels Landwehr category:cs.LG stat.ML I.2.6  published:2015-08-28 summary:We study learning problems in which the conditional distribution of the output given the input varies as a function of additional task variables. In varying-coefficient models with Gaussian process priors, a Gaussian process generates the functional relationship between the task variables and the parameters of this conditional. Varying-coefficient models subsume hierarchical Bayesian multitask models, but also generalizations in which the conditional varies continuously, for instance, in time or space. However, Bayesian inference in varying-coefficient models is generally intractable. We show that inference for varying-coefficient models with isotropic Gaussian process priors resolves to standard inference for a Gaussian process that can be solved efficiently. MAP inference in this model resolves to multitask learning using task and instance kernels, and inference for hierarchical Bayesian multitask models can be carried out efficiently using graph-Laplacian kernels. We report on experiments for geospatial prediction. version:2
arxiv-1510-03979 | Better Exploiting OS-CNNs for Better Event Recognition in Images | http://arxiv.org/abs/1510.03979 | id:1510.03979 author:Limin Wang, Zhe Wang, Sheng Guo, Yu Qiao category:cs.CV  published:2015-10-14 summary:Event recognition from still images is one of the most important problems for image understanding. However, compared with object recognition and scene recognition, event recognition has received much less research attention in computer vision community. This paper addresses the problem of cultural event recognition in still images and focuses on applying deep learning methods on this problem. In particular, we utilize the successful architecture of Object-Scene Convolutional Neural Networks (OS-CNNs) to perform event recognition. OS-CNNs are composed of object nets and scene nets, which transfer the learned representations from the pre-trained models on large-scale object and scene recognition datasets, respectively. We propose four types of scenarios to explore OS-CNNs for event recognition by treating them as either "end-to-end event predictors" or "generic feature extractors". Our experimental results demonstrate that the global and local representations of OS-CNNs are complementary to each other. Finally, based on our investigation of OS-CNNs, we come up with a solution for the cultural event recognition track at the ICCV ChaLearn Looking at People (LAP) challenge 2015. Our team secures the third place at this challenge and our result is very close to the best performance. version:1
arxiv-1510-03925 | On Equivalence of Martingale Tail Bounds and Deterministic Regret Inequalities | http://arxiv.org/abs/1510.03925 | id:1510.03925 author:Alexander Rakhlin, Karthik Sridharan category:math.PR cs.LG stat.ML  published:2015-10-13 summary:We study an equivalence of (i) deterministic pathwise statements appearing in the online learning literature (termed \emph{regret bounds}), (ii) high-probability tail bounds for the supremum of a collection of martingales (of a specific form arising from uniform laws of large numbers for martingales), and (iii) in-expectation bounds for the supremum. By virtue of the equivalence, we prove exponential tail bounds for norms of Banach space valued martingales via deterministic regret bounds for the online mirror descent algorithm with an adaptive step size. We extend these results beyond the linear structure of the Banach space: we define a notion of \emph{martingale type} for general classes of real-valued functions and show its equivalence (up to a logarithmic factor) to various sequential complexities of the class (in particular, the sequential Rademacher complexity and its offset version). For classes with the general martingale type 2, we exhibit a finer notion of variation that allows partial adaptation to the function indexing the martingale. Our proof technique rests on sequential symmetrization and on certifying the \emph{existence} of regret minimization strategies for certain online prediction problems. version:1
arxiv-1505-07647 | Visual Search at Pinterest | http://arxiv.org/abs/1505.07647 | id:1505.07647 author:Yushi Jing, David Liu, Dmitry Kislyuk, Andrew Zhai, Jiajing Xu, Jeff Donahue, Sarah Tavel category:cs.CV  published:2015-05-28 summary:We demonstrate that, with the availability of distributed computation platforms such as Amazon Web Services and open-source tools, it is possible for a small engineering team to build, launch and maintain a cost-effective, large-scale visual search system with widely available tools. We also demonstrate, through a comprehensive set of live experiments at Pinterest, that content recommendation powered by visual search improve user engagement. By sharing our implementation details and the experiences learned from launching a commercial visual search engines from scratch, we hope visual search are more widely incorporated into today's commercial applications. version:2
arxiv-1510-03909 | Variable-state Latent Conditional Random Fields for Facial Expression Recognition and Action Unit Detection | http://arxiv.org/abs/1510.03909 | id:1510.03909 author:Robert Walecki, Ognjen Rudovic, Vladimir Pavlovic, Maja Pantic category:cs.CV cs.HC  published:2015-10-13 summary:Automated recognition of facial expressions of emotions, and detection of facial action units (AUs), from videos depends critically on modeling of their dynamics. These dynamics are characterized by changes in temporal phases (onset-apex-offset) and intensity of emotion expressions and AUs, the appearance of which may vary considerably among target subjects, making the recognition/detection task very challenging. The state-of-the-art Latent Conditional Random Fields (L-CRF) framework allows one to efficiently encode these dynamics through the latent states accounting for the temporal consistency in emotion expression and ordinal relationships between its intensity levels, these latent states are typically assumed to be either unordered (nominal) or fully ordered (ordinal). Yet, such an approach is often too restrictive. For instance, in the case of AU detection, the goal is to discriminate between the segments of an image sequence in which this AU is active or inactive. While the sequence segments containing activation of the target AU may better be described using ordinal latent states, the inactive segments better be described using unordered (nominal) latent states, as no assumption can be made about their underlying structure (since they can contain either neutral faces or activations of non-target AUs). To address this, we propose the variable-state L-CRF (VSL-CRF) model that automatically selects the optimal latent states for the target image sequence. To reduce the model overfitting either the nominal or ordinal latent states, we propose a novel graph-Laplacian regularization of the latent states. Our experiments on three public expression databases show that the proposed model achieves better generalization performance compared to traditional L-CRFs and other related state-of-the-art models. version:1
arxiv-1510-03891 | Nonlinear memory capacity of parallel time-delay reservoir computers in the processing of multidimensional signals | http://arxiv.org/abs/1510.03891 | id:1510.03891 author:Lyudmila Grigoryeva, Julie Henriques, Laurent Larger, Juan-Pablo Ortega category:cs.NE  published:2015-10-13 summary:This paper addresses the reservoir design problem in the context of delay-based reservoir computers for multidimensional input signals, parallel architectures, and real-time multitasking. First, an approximating reservoir model is presented in those frameworks that provides an explicit functional link between the reservoir parameters and architecture and its performance in the execution of a specific task. Second, the inference properties of the ridge regression estimator in the multivariate context is used to assess the impact of finite sample training on the decrease of the reservoir capacity. Finally, an empirical study is conducted that shows the adequacy of the theoretical results with the empirical performances exhibited by various reservoir architectures in the execution of several nonlinear tasks with multidimensional inputs. Our results confirm the robustness properties of the parallel reservoir architecture with respect to task misspecification and parameter choice that had already been documented in the literature. version:1
arxiv-1510-03797 | Complex Politics: A Quantitative Semantic and Topological Analysis of UK House of Commons Debates | http://arxiv.org/abs/1510.03797 | id:1510.03797 author:Stefano Gurciullo, Michael Smallegan, María Pereda, Federico Battiston, Alice Patania, Sebastian Poledna, Daniel Hedblom, Bahattin Tolga Oztan, Alexander Herzog, Peter John, Slava Mikhaylov category:physics.soc-ph cs.CL cs.SI 91F10  published:2015-10-13 summary:This study is a first, exploratory attempt to use quantitative semantics techniques and topological analysis to analyze systemic patterns arising in a complex political system. In particular, we use a rich data set covering all speeches and debates in the UK House of Commons between 1975 and 2014. By the use of dynamic topic modeling (DTM) and topological data analysis (TDA) we show that both members and parties feature specific roles within the system, consistent over time, and extract global patterns indicating levels of political cohesion. Our results provide a wide array of novel hypotheses about the complex dynamics of political systems, with valuable policy applications. version:1
arxiv-1509-05760 | Accelerating Optimization via Adaptive Prediction | http://arxiv.org/abs/1509.05760 | id:1509.05760 author:Mehryar Mohri, Scott Yang category:stat.ML cs.LG  published:2015-09-18 summary:We present a powerful general framework for designing data-dependent optimization algorithms, building upon and unifying recent techniques in adaptive regularization, optimistic gradient predictions, and problem-dependent randomization. We first present a series of new regret guarantees that hold at any time and under very minimal assumptions, and then show how different relaxations recover existing algorithms, both basic as well as more recent sophisticated ones. Finally, we show how combining adaptivity, optimism, and problem-dependent randomization can guide the design of algorithms that benefit from more favorable guarantees than recent state-of-the-art methods. version:3
arxiv-1510-03743 | Wide-Area Image Geolocalization with Aerial Reference Imagery | http://arxiv.org/abs/1510.03743 | id:1510.03743 author:Scott Workman, Richard Souvenir, Nathan Jacobs category:cs.CV  published:2015-10-13 summary:We propose to use deep convolutional neural networks to address the problem of cross-view image geolocalization, in which the geolocation of a ground-level query image is estimated by matching to georeferenced aerial images. We use state-of-the-art feature representations for ground-level images and introduce a cross-view training approach for learning a joint semantic feature representation for aerial images. We also propose a network architecture that fuses features extracted from aerial images at multiple spatial scales. To support training these networks, we introduce a massive database that contains pairs of aerial and ground-level images from across the United States. Our methods significantly out-perform the state of the art on two benchmark datasets. We also show, qualitatively, that the proposed feature representations are discriminative at both local and continental spatial scales. version:1
arxiv-1510-03730 | Fast sequential forensic camera identification | http://arxiv.org/abs/1510.03730 | id:1510.03730 author:Fernando Pérez-González, Iria González-Iglesias, Miguel Masciopinto, Pedro Comesaña category:cs.CR cs.CV  published:2015-10-13 summary:Two sequential camera source identification methods are proposed. Sequential tests implement a log-likelihood ratio test in an incremental way, thus enabling a reliable decision with a minimal number of observations. One of our methods adapts Goljan et al.'s to sequential operation. The second, which offers better performance in terms of error probabilities and average number of test observations, is based on treating the alternative hypothesis as a doubly stochastic model. We also discuss how the standard sequential test can be corrected to account for the event of weak fingerprints. Finally, we validate the goodness of our methods with experiments. version:1
arxiv-1510-03727 | SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes | http://arxiv.org/abs/1510.03727 | id:1510.03727 author:Stuart Golodetz, Michael Sapienza, Julien P. C. Valentin, Vibhav Vineet, Ming-Ming Cheng, Anurag Arnab, Victor A. Prisacariu, Olaf Kähler, Carl Yuheng Ren, David W. Murray, Shahram Izadi, Philip H. S. Torr category:cs.CV I.2.10  published:2015-10-13 summary:We present an open-source, real-time implementation of SemanticPaint, a system for geometric reconstruction, object-class segmentation and learning of 3D scenes. Using our system, a user can walk into a room wearing a depth camera and a virtual reality headset, and both densely reconstruct the 3D scene and interactively segment the environment into object classes such as 'chair', 'floor' and 'table'. The user interacts physically with the real-world scene, touching objects and using voice commands to assign them appropriate labels. These user-generated labels are leveraged by an online random forest-based machine learning algorithm, which is used to predict labels for previously unseen parts of the scene. The entire pipeline runs in real time, and the user stays 'in the loop' throughout the process, receiving immediate feedback about the progress of the labelling and interacting with the scene as necessary to refine the predicted segmentation. version:1
arxiv-1508-05128 | Lifted Relational Neural Networks | http://arxiv.org/abs/1508.05128 | id:1508.05128 author:Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezny, Ondrej Kuzelka category:cs.AI cs.LG cs.NE  published:2015-08-20 summary:We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reflecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descent algorithm. The framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. Discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method. version:2
arxiv-1505-04938 | Convective regularization for optical flow | http://arxiv.org/abs/1505.04938 | id:1505.04938 author:José A. Iglesias, Clemens Kirisits category:math.OC cs.CV 49N45  68T45  68U10  published:2015-05-19 summary:We argue that the time derivative in a fixed coordinate frame may not be the most appropriate measure of time regularity of an optical flow field. Instead, for a given velocity field $v$ we consider the convective acceleration $v_t + \nabla v v$ which describes the acceleration of objects moving according to $v$. Consequently we investigate the suitability of the nonconvex functional $\ v_t + \nabla v v\ ^2_{L^2}$ as a regularization term for optical flow. We demonstrate that this term acts as both a spatial and a temporal regularizer and has an intrinsic edge-preserving property. We incorporate it into a contrast invariant and time-regularized variant of the Horn-Schunck functional, prove existence of minimizers and verify experimentally that it addresses some of the problems of basic quadratic models. For the minimization we use an iterative scheme that approximates the original nonlinear problem with a sequence of linear ones. We believe that the convective acceleration may be gainfully introduced in a variety of optical flow models. version:2
arxiv-1504-07029 | Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection | http://arxiv.org/abs/1504.07029 | id:1504.07029 author:David Novotny, Jiri Matas category:cs.CV  published:2015-04-27 summary:A novel efficient method for extraction of object proposals is introduced. Its "objectness" function exploits deep spatial pyramid features, a novel fast-to-compute HoG-based edge statistic and the EdgeBoxes score. The efficiency is achieved by the use of spatial bins in a novel combination with sparsity-inducing group normalized SVM. State-of-the-art recall performance is achieved on Pascal VOC07, significantly outperforming methods with comparable speed. Interestingly, when only 100 proposals per image are considered the method attains 78% recall on VOC07. The method improves mAP of the RCNN state-of-the-art class-specific detector, increasing it by 10 points when only 50 proposals are used in each image. The system trained on twenty classes performs well on the two hundred class ILSVRC2013 set confirming generalization capability. version:2
arxiv-1510-03602 | A language model based approach towards large scale and lightweight language identification systems | http://arxiv.org/abs/1510.03602 | id:1510.03602 author:Brij Mohan Lal Srivastava, Hari Krishna Vydana, Anil Kumar Vuppala, Manish Shrivastava category:cs.SD cs.CL  published:2015-10-13 summary:Multilingual spoken dialogue systems have gained prominence in the recent past necessitating the requirement for a front-end Language Identification (LID) system. Most of the existing LID systems rely on modeling the language discriminative information from low-level acoustic features. Due to the variabilities of speech (speaker and emotional variabilities, etc.), large-scale LID systems developed using low-level acoustic features suffer from a degradation in the performance. In this approach, we have attempted to model the higher level language discriminative phonotactic information for developing an LID system. In this paper, the input speech signal is tokenized to phone sequences by using a language independent phone recognizer. The language discriminative phonotactic information in the obtained phone sequences are modeled using statistical and recurrent neural network based language modeling approaches. As this approach, relies on higher level phonotactical information it is more robust to variabilities of speech. Proposed approach is computationally light weight, highly scalable and it can be used in complement with the existing LID systems. version:1
arxiv-1510-03591 | Dual Control for Approximate Bayesian Reinforcement Learning | http://arxiv.org/abs/1510.03591 | id:1510.03591 author:Edgar D. Klenske, Philipp Hennig category:stat.ML cs.SY math.OC  published:2015-10-13 summary:Control of non-episodic, finite-horizon dynamical systems with uncertain dynamics poses a tough and elementary case of the exploration-exploitation trade-off. Bayesian reinforcement learning, reasoning about the effect of actions and future observations, offers a principled solution, but is intractable. We review, then extend an old approximate approach from control theory---where the problem is known as dual control---in the context of modern regression methods, specifically generalized linear regression. Experiments on simulated systems show that this framework offers a useful approximation to the intractable aspects of Bayesian RL, producing structured exploration strategies that differ from standard RL approaches. We provide simple examples for the use of this framework in (approximate) Gaussian process regression and feedforward neural networks for the control of exploration. version:1
arxiv-1510-03528 | $\ell_1$-regularized Neural Networks are Improperly Learnable in Polynomial Time | http://arxiv.org/abs/1510.03528 | id:1510.03528 author:Yuchen Zhang, Jason D. Lee, Michael I. Jordan category:cs.LG  published:2015-10-13 summary:We study the improper learning of multi-layer neural networks. Suppose that the neural network to be learned has $k$ hidden layers and that the $\ell_1$-norm of the incoming weights of any neuron is bounded by $L$. We present a kernel-based method, such that with probability at least $1 - \delta$, it learns a predictor whose generalization error is at most $\epsilon$ worse than that of the neural network. The sample complexity and the time complexity of the presented method are polynomial in the input dimension and in $(1/\epsilon,\log(1/\delta),F(k,L))$, where $F(k,L)$ is a function depending on $(k,L)$ and on the activation function, independent of the number of neurons. The algorithm applies to both sigmoid-like activation functions and ReLU-like activation functions. It implies that any sufficiently sparse neural network is learnable in polynomial time. version:1
arxiv-1509-07244 | Multi-Region Probabilistic Dice Similarity Coefficient using the Aitchison Distance and Bipartite Graph Matching | http://arxiv.org/abs/1509.07244 | id:1509.07244 author:Shawn Andrews, Ghassan Hamarneh category:cs.CV  published:2015-09-24 summary:Validation of image segmentation methods is of critical importance. Probabilistic image segmentation is increasingly popular as it captures uncertainty in the results. Image segmentation methods that support multi-region (as opposed to binary) delineation are more favourable as they capture interactions between the different objects in the image. The Dice similarity coefficient (DSC) has been a popular metric for evaluating the accuracy of automated or semi-automated segmentation methods by comparing their results to the ground truth. In this work, we develop an extension of the DSC to multi-region probabilistic segmentations (with unordered labels). We use bipartite graph matching to establish label correspondences and propose two functions that extend the DSC, one based on absolute probability differences and one based on the Aitchison distance. These provide a robust and accurate measure of multi-region probabilistic segmentation accuracy. version:3
arxiv-1510-03507 | The intrinsic value of HFO features as a biomarker of epileptic activity | http://arxiv.org/abs/1510.03507 | id:1510.03507 author:Stephen V. Gliske, Kevin R. Moon, William C. Stacey, Alfred O. Hero III category:q-bio.NC cs.LG stat.ML  published:2015-10-13 summary:High frequency oscillations (HFOs) are a promising biomarker of epileptic brain tissue and activity. HFOs additionally serve as a prototypical example of challenges in the analysis of discrete events in high-temporal resolution, intracranial EEG data. Two primary challenges are 1) dimensionality reduction, and 2) assessing feasibility of classification. Dimensionality reduction assumes that the data lie on a manifold with dimension less than that of the feature space. However, previous HFO analyses have assumed a linear manifold, global across time, space (i.e. recording electrode/channel), and individual patients. Instead, we assess both a) whether linear methods are appropriate and b) the consistency of the manifold across time, space, and patients. We also estimate bounds on the Bayes classification error to quantify the distinction between two classes of HFOs (those occurring during seizures and those occurring due to other processes). This analysis provides the foundation for future clinical use of HFO features and buides the analysis for other discrete events, such as individual action potentials or multi-unit activity. version:1
arxiv-1509-02237 | On Wasserstein Two Sample Testing and Related Families of Nonparametric Tests | http://arxiv.org/abs/1509.02237 | id:1509.02237 author:Aaditya Ramdas, Nicolas Garcia, Marco Cuturi category:math.ST stat.ML stat.TH  published:2015-09-08 summary:Nonparametric two sample or homogeneity testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. The literature is old and rich, with a wide variety of statistics having being intelligently designed and analyzed, both for the unidimensional and the multivariate setting. Our contribution is to tie together many of these tests, drawing connections between seemingly very different statistics. In this work, our central object is the Wasserstein distance, as we form a chain of connections from univariate methods like the Kolmogorov-Smirnov test, PP/QQ plots and ROC/ODC curves, to multivariate tests involving energy statistics and kernel based maximum mean discrepancy. Some connections proceed through the construction of a \textit{smoothed} Wasserstein distance, and others through the pursuit of a "distribution-free" Wasserstein test. Some observations in this chain are implicit in the literature, while others seem to have not been noticed thus far. Given nonparametric two sample testing's classical and continued importance, we aim to provide useful connections for theorists and practitioners familiar with one subset of methods but not others. version:2
arxiv-1510-03497 | Consistent Estimation of Low-Dimensional Latent Structure in High-Dimensional Data | http://arxiv.org/abs/1510.03497 | id:1510.03497 author:Xiongzhi Chen, John D. Storey category:stat.ML  published:2015-10-13 summary:We consider the problem of extracting a low-dimensional, linear latent variable structure from high-dimensional random variables. Specifically, we show that under mild conditions and when this structure manifests itself as a linear space that spans the conditional means, it is possible to consistently recover the structure using only information up to the second moments of these random variables. This finding, specialized to one-parameter exponential families whose variance function is quadratic in their means, allows for the derivation of an explicit estimator of such latent structure. This approach serves as a latent variable model estimator and as a tool for dimension reduction for a high-dimensional matrix of data composed of many related variables. Our theoretical results are verified by simulation studies and an application to genomic data. version:1
arxiv-1510-00967 | Implicit stochastic approximation | http://arxiv.org/abs/1510.00967 | id:1510.00967 author:Panos Toulis, Edoardo M. Airoldi category:math.ST stat.ML stat.TH  published:2015-10-04 summary:The need to carry out parameter estimation from massive data has reinvigorated interest in iterative estimation methods, in statistics and machine learning. Classic work includes deterministic gradient-based methods, such as quasi-Newton, and stochastic gradient descent and its variants, including adaptive learning rates, acceleration and averaging. Current work increasingly relies on methods that employ proximal operators, leading to updates defined through implicit equations, which need to be solved at each iteration. Such methods are especially attractive in modern problems with massive data because they are numerically stable and converge with minimal assumptions, among other reasons. However, while the majority of existing methods can be subsumed into the gradient-free stochastic approximation framework developed by Robbins and Monro (1951), there is no such framework for methods with implicit updates. Here, we conceptualize a gradient-free implicit stochastic approximation procedure, and develop asymptotic and non-asymptotic theory for it. This new framework provides a theoretical foundation for gradient-based procedures that rely on implicit updates, and opens the door to iterative estimation methods that do not require a gradient, nor a fully known likelihood. version:2
arxiv-1504-07225 | Correlational Neural Networks | http://arxiv.org/abs/1504.07225 | id:1504.07225 author:Sarath Chandar, Mitesh M. Khapra, Hugo Larochelle, Balaraman Ravindran category:cs.CL cs.LG cs.NE stat.ML  published:2015-04-27 summary:Common Representation Learning (CRL), wherein different descriptions (or views) of the data are embedded in a common subspace, is receiving a lot of attention recently. Two popular paradigms here are Canonical Correlation Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace. AE based methods learn a common representation by minimizing the error of reconstructing the two views. Each of these approaches has its own advantages and disadvantages. For example, while CCA based approaches outperform AE based approaches for the task of transfer learning, they are not as scalable as the latter. In this work we propose an AE based approach called Correlational Neural Network (CorrNet), that explicitly maximizes correlation among the views when projected to the common subspace. Through a series of experiments, we demonstrate that the proposed CorrNet is better than the above mentioned approaches with respect to its ability to learn correlated common representations. Further, we employ CorrNet for several cross language tasks and show that the representations learned using CorrNet perform better than the ones learned using other state of the art approaches. version:3
arxiv-1510-03370 | Asymptotic Logical Uncertainty and The Benford Test | http://arxiv.org/abs/1510.03370 | id:1510.03370 author:Scott Garrabrant, Siddharth Bhaskar, Abram Demski, Joanna Garrabrant, George Koleszarik, Evan Lloyd category:cs.LG cs.AI F.4.1  published:2015-10-12 summary:We give an algorithm A which assigns probabilities to logical sentences. For any simple infinite sequence of sentences whose truth-values appear indistinguishable from a biased coin that outputs "true" with probability p, we have that the sequence of probabilities that A assigns to these sentences converges to p. version:1
arxiv-1510-03349 | Toward a Better Understanding of Leaderboard | http://arxiv.org/abs/1510.03349 | id:1510.03349 author:Wenjie Zheng category:stat.ML cs.LG stat.AP  published:2015-10-12 summary:The leaderboard in machine learning competitions is a tool to show the performance of various participants and to compare them. However, the leaderboard quickly becomes no longer accurate, due to hack or overfitting. This article gives two advices to avoid this. It also points out that the Ladder leaderboard successfully prevents this with $\tilde{O}(\epsilon^{-3})$ samples in the validation set. version:1
arxiv-1510-03317 | The Inductive Constraint Programming Loop | http://arxiv.org/abs/1510.03317 | id:1510.03317 author:Christian Bessiere, Luc De Raedt, Tias Guns, Lars Kotthoff, Mirco Nanni, Siegfried Nijssen, Barry O'Sullivan, Anastasia Paparrizou, Dino Pedreschi, Helmut Simonis category:cs.AI cs.LG  published:2015-10-12 summary:Constraint programming is used for a variety of real-world optimisation problems, such as planning, scheduling and resource allocation problems. At the same time, one continuously gathers vast amounts of data about these problems. Current constraint programming software does not exploit such data to update schedules, resources and plans. We propose a new framework, that we call the Inductive Constraint Programming loop. In this approach data is gathered and analyzed systematically, in order to dynamically revise and adapt constraints and optimization criteria. Inductive Constraint Programming aims at bridging the gap between the areas of data mining and machine learning on the one hand, and constraint programming on the other hand. version:1
arxiv-1510-03298 | Penalized estimation in large-scale generalized linear array models | http://arxiv.org/abs/1510.03298 | id:1510.03298 author:Adam Lund, Martin Vincent, Niels Richard Hansen category:stat.CO stat.ML  published:2015-10-12 summary:Large-scale generalized linear array models (GLAMs) can be challenging to fit. Computation and storage of its tensor product design matrix can be impossible due to time and memory constraints, and previously considered design matrix free algorithms do not scale well with the dimension of the parameter vector. A new design matrix free algorithm is proposed for computing the penalized maximum likelihood estimate for GLAMs, which, in particular, handles nondifferentiable penalty functions. The proposed algorithm is implemented and available via the R package \verb+glamlasso+. It combines several ideas -- previously considered separately -- to obtain sparse estimates while at the same time efficiently exploiting the GLAM structure. In this paper the convergence of the algorithm is treated and the performance of its implementation is investigated and compared to that of \verb+glmnet+ on simulated as well as real data. It is shown that the computation time for version:1
arxiv-1510-03267 | On the Robustness of Regularized Pairwise Learning Methods Based on Kernels | http://arxiv.org/abs/1510.03267 | id:1510.03267 author:Andreas Christmann, Ding-Xuan Zhou category:math.ST math.FA stat.ML stat.TH  published:2015-10-12 summary:Regularized empirical risk minimization including support vector machines plays an important role in machine learning theory. In this paper regularized pairwise learning (RPL) methods based on kernels will be investigated. One example is regularized minimization of the error entropy loss which has recently attracted quite some interest from the viewpoint of consistency and learning rates. This paper shows that such RPL methods have additionally good statistical robustness properties, if the loss function and the kernel are chosen appropriately. We treat two cases of particular interest: (i) a bounded and non-convex loss function and (ii) an unbounded convex loss function satisfying a certain Lipschitz type condition. version:1
arxiv-1510-03250 | Using Anatomical Markers for Left Ventricular Segmentation of Long Axis Ultrasound Images | http://arxiv.org/abs/1510.03250 | id:1510.03250 author:Yael Petrank, Nahum Smirin, Yossi Tsadok, Zvi Friedman, Peter Lysiansky, Dan Adam category:cs.CV  published:2015-10-12 summary:Left ventricular segmentation is essential for measuring left ventricular function indices. Segmentation of one or several images requires an initial guess of the contour. It is hypothesized here that creating an initial guess by first detecting anatomical markers, would lead to correct detection of the endocardium. The first step of the algorithm presented here includes automatic detection of the mitral valve. Next, the apex is detected in the same frame. The valve is then tracked throughout the cardiac cycle. Contours passing from the apex to each valve corner are then found using a dynamic programming algorithm. The resulting contour is used as an input to an active contour algorithm. The algorithm was tested on 21 long axis ultrasound clips and showed good agreement with manually traced contours. Thus, this study demonstrates that detection of anatomic markers leads to a reliable initial guess of the left ventricle border. version:1
arxiv-1510-03199 | Interactive multiclass segmentation using superpixel classification | http://arxiv.org/abs/1510.03199 | id:1510.03199 author:Bérengère Mathieu, Alain Crouzil, Jean-Baptiste Puel category:cs.CV  published:2015-10-12 summary:This paper adresses the problem of interactive multiclass segmentation. We propose a fast and efficient new interactive segmentation method called Superpixel Classification-based Interactive Segmentation (SCIS). From a few strokes drawn by a human user over an image, this method extracts relevant semantic objects. To get a fast calculation and an accurate segmentation, SCIS uses superpixel over-segmentation and support vector machine classification. In this paper, we demonstrate that SCIS significantly outperfoms competing algorithms by evaluating its performances on the reference benchmarks of McGuinness and Santner. version:1
arxiv-1510-03130 | On Correcting Inputs: Inverse Optimization for Online Structured Prediction | http://arxiv.org/abs/1510.03130 | id:1510.03130 author:Hal Daumé III, Samir Khuller, Manish Purohit, Gregory Sanders category:cs.LG  published:2015-10-12 summary:Algorithm designers typically assume that the input data is correct, and then proceed to find "optimal" or "sub-optimal" solutions using this input data. However this assumption of correct data does not always hold in practice, especially in the context of online learning systems where the objective is to learn appropriate feature weights given some training samples. Such scenarios necessitate the study of inverse optimization problems where one is given an input instance as well as a desired output and the task is to adjust the input data so that the given output is indeed optimal. Motivated by learning structured prediction models, in this paper we consider inverse optimization with a margin, i.e., we require the given output to be better than all other feasible outputs by a desired margin. We consider such inverse optimization problems for maximum weight matroid basis, matroid intersection, perfect matchings, minimum cost maximum flows, and shortest paths and derive the first known results for such problems with a non-zero margin. The effectiveness of these algorithmic approaches to online learning for structured prediction is also discussed. version:1
arxiv-1510-03125 | Fast detection of multiple objects in traffic scenes with a common detection framework | http://arxiv.org/abs/1510.03125 | id:1510.03125 author:Qichang Hu, Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel, Fatih Porikli category:cs.CV  published:2015-10-12 summary:Traffic scene perception (TSP) aims to real-time extract accurate on-road environment information, which in- volves three phases: detection of objects of interest, recognition of detected objects, and tracking of objects in motion. Since recognition and tracking often rely on the results from detection, the ability to detect objects of interest effectively plays a crucial role in TSP. In this paper, we focus on three important classes of objects: traffic signs, cars, and cyclists. We propose to detect all the three important objects in a single learning based detection framework. The proposed framework consists of a dense feature extractor and detectors of three important classes. Once the dense features have been extracted, these features are shared with all detectors. The advantage of using one common framework is that the detection speed is much faster, since all dense features need only to be evaluated once in the testing phase. In contrast, most previous works have designed specific detectors using different features for each of these objects. To enhance the feature robustness to noises and image deformations, we introduce spatially pooled features as a part of aggregated channel features. In order to further improve the generalization performance, we propose an object subcategorization method as a means of capturing intra-class variation of objects. We experimentally demonstrate the effectiveness and efficiency of the proposed framework in three detection applications: traffic sign detection, car detection, and cyclist detection. The proposed framework achieves the competitive performance with state-of- the-art approaches on several benchmark datasets. version:1
arxiv-1506-01490 | Rivalry of Two Families of Algorithms for Memory-Restricted Streaming PCA | http://arxiv.org/abs/1506.01490 | id:1506.01490 author:Chun-Liang Li, Hsuan-Tien Lin, Chi-Jen Lu category:stat.ML cs.LG  published:2015-06-04 summary:We study the problem of recovering the subspace spanned by the first $k$ principal components of $d$-dimensional data under the streaming setting, with a memory bound of $O(kd)$. Two families of algorithms are known for this problem. The first family is based on the framework of stochastic gradient descent. Nevertheless, the convergence rate of the family can be seriously affected by the learning rate of the descent steps and deserves more serious study. The second family is based on the power method over blocks of data, but setting the block size for its existing algorithms is not an easy task. In this paper, we analyze the convergence rate of a representative algorithm with decayed learning rate (Oja and Karhunen, 1985) in the first family for the general $k>1$ case. Moreover, we propose a novel algorithm for the second family that sets the block sizes automatically and dynamically with faster convergence rate. We then conduct empirical studies that fairly compare the two families on real-world data. The studies reveal the advantages and disadvantages of these two families. version:2
arxiv-1412-7122 | Learning Deep Object Detectors from 3D Models | http://arxiv.org/abs/1412.7122 | id:1412.7122 author:Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko category:cs.CV cs.LG cs.NE  published:2014-12-22 summary:Crowdsourced 3D CAD models are becoming easily accessible online, and can potentially generate an infinite number of training images for almost any object category.We show that augmenting the training data of contemporary Deep Convolutional Neural Net (DCNN) models with such synthetic data can be effective, especially when real training data is limited or not well matched to the target domain. Most freely available CAD models capture 3D shape but are often missing other low level cues, such as realistic object texture, pose, or background. In a detailed analysis, we use synthetic CAD-rendered images to probe the ability of DCNN to learn without these cues, with surprising findings. In particular, we show that when the DCNN is fine-tuned on the target detection task, it exhibits a large degree of invariance to missing low-level cues, but, when pretrained on generic ImageNet classification, it learns better when the low-level cues are simulated. We show that our synthetic DCNN training approach significantly outperforms previous methods on the PASCAL VOC2007 dataset when learning in the few-shot scenario and improves performance in a domain shift scenario on the Office benchmark. version:4
arxiv-1509-03808 | A Markov Jump Process for More Efficient Hamiltonian Monte Carlo | http://arxiv.org/abs/1509.03808 | id:1509.03808 author:Andrew B. Berger, Mayur Mudigonda, Michael R. DeWeese, Jascha Sohl-Dickstein category:stat.ML stat.CO  published:2015-09-13 summary:In most sampling algorithms, including Hamiltonian Monte Carlo, transition rates between states correspond to the probability of making a transition in a single time step, and are constrained to be less than or equal to 1. We derive a Hamiltonian Monte Carlo algorithm using a continuous time Markov jump process, and are thus able to escape this constraint. Transition rates in a Markov jump process need only be non-negative. We demonstrate that the new algorithm leads to improved mixing for several example problems, both by evaluating the spectral gap of the Markov operator, and by computing autocorrelation as a function of compute time. We release the algorithm as an open source Python package. version:3
arxiv-1510-03042 | ParallelPC: an R package for efficient constraint based causal exploration | http://arxiv.org/abs/1510.03042 | id:1510.03042 author:Thuc Duy Le, Tao Hoang, Jiuyong Li, Lin Liu, Shu Hu category:cs.AI stat.ML  published:2015-10-11 summary:Discovering causal relationships from data is the ultimate goal of many research areas. Constraint based causal exploration algorithms, such as PC, FCI, RFCI, PC-simple, IDA and Joint-IDA have achieved significant progress and have many applications. A common problem with these methods is the high computational complexity, which hinders their applications in real world high dimensional datasets, e.g gene expression datasets. In this paper, we present an R package, ParallelPC, that includes the parallelised versions of these causal exploration algorithms. The parallelised algorithms help speed up the procedure of experimenting big datasets and reduce the memory used when running the algorithms. The package is not only suitable for super-computers or clusters, but also convenient for researchers using personal computers with multi core CPUs. Our experiment results on real world datasets show that using the parallelised algorithms it is now practical to explore causal relationships in high dimensional datasets with thousands of variables in a single multicore computer. ParallelPC is available in CRAN repository at https://cran.rproject.org/web/packages/ParallelPC/index.html. version:1
arxiv-1210-5830 | Choice of V for V-Fold Cross-Validation in Least-Squares Density Estimation | http://arxiv.org/abs/1210.5830 | id:1210.5830 author:Sylvain Arlot, Matthieu Lerasle category:math.ST cs.LG stat.TH  published:2012-10-22 summary:This paper studies V-fold cross-validation for model selection in least-squares density estimation. The goal is to provide theoretical grounds for choosing V in order to minimize the least-squares loss of the selected estimator. We first prove a non-asymptotic oracle inequality for V-fold cross-validation and its bias-corrected version (V-fold penalization). In particular, this result implies that V-fold penalization is asymptotically optimal in the nonparametric case. Then, we compute the variance of V-fold cross-validation and related criteria, as well as the variance of key quantities for model selection performance. We show that these variances depend on V like 1+4/(V-1), at least in some particular cases, suggesting that the performance increases much from V=2 to V=5 or 10, and then is almost constant. Overall, this can explain the common advice to take V=5---at least in our setting and when the computational power is limited---, as supported by some simulation experiments. An oracle inequality and exact formulas for the variance are also proved for Monte-Carlo cross-validation, also known as repeated cross-validation, where the parameter V is replaced by the number B of random splits of the data. version:3
arxiv-1510-03021 | Textual Analysis for Studying Chinese Historical Documents and Literary Novels | http://arxiv.org/abs/1510.03021 | id:1510.03021 author:Chao-Lin Liu, Guan-Tao Jin, Hongsu Wang, Qing-Feng Liu, Wen-Huei Cheng, Wei-Yun Chiu, Richard Tzong-Han Tsai, Yu-Chun Wang category:cs.CL cs.DL  published:2015-10-11 summary:We analyzed historical and literary documents in Chinese to gain insights into research issues, and overview our studies which utilized four different sources of text materials in this paper. We investigated the history of concepts and transliterated words in China with the Database for the Study of Modern China Thought and Literature, which contains historical documents about China between 1830 and 1930. We also attempted to disambiguate names that were shared by multiple government officers who served between 618 and 1912 and were recorded in Chinese local gazetteers. To showcase the potentials and challenges of computer-assisted analysis of Chinese literatures, we explored some interesting yet non-trivial questions about two of the Four Great Classical Novels of China: (1) Which monsters attempted to consume the Buddhist monk Xuanzang in the Journey to the West (JTTW), which was published in the 16th century, (2) Which was the most powerful monster in JTTW, and (3) Which major role smiled the most in the Dream of the Red Chamber, which was published in the 18th century. Similar approaches can be applied to the analysis and study of modern documents, such as the newspaper articles published about the 228 incident that occurred in 1947 in Taiwan. version:1
arxiv-1510-02983 | OmniGraph: Rich Representation and Graph Kernel Learning | http://arxiv.org/abs/1510.02983 | id:1510.02983 author:Boyi Xie, Rebecca J. Passonneau category:cs.CL cs.LG  published:2015-10-10 summary:OmniGraph, a novel representation to support a range of NLP classification tasks, integrates lexical items, syntactic dependencies and frame semantic parses into graphs. Feature engineering is folded into the learning through convolution graph kernel learning to explore different extents of the graph. A high-dimensional space of features includes individual nodes as well as complex subgraphs. In experiments on a text-forecasting problem that predicts stock price change from news for company mentions, OmniGraph beats several benchmarks based on bag-of-words, syntactic dependencies, and semantic trees. The highly expressive features OmniGraph discovers provide insights into the semantics across distinct market sectors. To demonstrate the method's generality, we also report its high performance results on a fine-grained sentiment corpus. version:1
arxiv-1510-02975 | Optimal Piecewise Linear Function Approximation for GPU-based Applications | http://arxiv.org/abs/1510.02975 | id:1510.02975 author:Daniel Berjón, Guillermo Gallego, Carlos Cuevas, Francisco Morán, Narciso García category:math.OC cs.CV cs.DC cs.NA cs.SY  published:2015-10-10 summary:Many computer vision and human-computer interaction applications developed in recent years need evaluating complex and continuous mathematical functions as an essential step toward proper operation. However, rigorous evaluation of this kind of functions often implies a very high computational cost, unacceptable in real-time applications. To alleviate this problem, functions are commonly approximated by simpler piecewise-polynomial representations. Following this idea, we propose a novel, efficient, and practical technique to evaluate complex and continuous functions using a nearly optimal design of two types of piecewise linear approximations in the case of a large budget of evaluation subintervals. To this end, we develop a thorough error analysis that yields asymptotically tight bounds to accurately quantify the approximation performance of both representations. It provides an improvement upon previous error estimates and allows the user to control the trade-off between the approximation error and the number of evaluation subintervals. To guarantee real-time operation, the method is suitable for, but not limited to, an efficient implementation in modern Graphics Processing Units (GPUs), where it outperforms previous alternative approaches by exploiting the fixed-function interpolation routines present in their texture units. The proposed technique is a perfect match for any application requiring the evaluation of continuous functions, we have measured in detail its quality and efficiency on several functions, and, in particular, the Gaussian function because it is extensively used in many areas of computer vision and cybernetics, and it is expensive to evaluate. version:1
arxiv-1510-02969 | Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition? | http://arxiv.org/abs/1510.02969 | id:1510.02969 author:Pooya Khorrami, Tom Le Paine, Thomas S. Huang category:cs.CV cs.LG cs.NE  published:2015-10-10 summary:Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements. version:1
arxiv-1510-02949 | Spatial Semantic Regularisation for Large Scale Object Detection | http://arxiv.org/abs/1510.02949 | id:1510.02949 author:Damian Mrowca, Marcus Rohrbach, Judy Hoffman, Ronghang Hu, Kate Saenko, Trevor Darrell category:cs.CV  published:2015-10-10 summary:Large scale object detection with thousands of classes introduces the problem of many contradicting false positive detections, which have to be suppressed. Class-independent non-maximum suppression has traditionally been used for this step, but it does not scale well as the number of classes grows. Traditional non-maximum suppression does not consider label- and instance-level relationships nor does it allow an exploitation of the spatial layout of detection proposals. We propose a new multi-class spatial semantic regularisation method based on affinity propagation clustering, which simultaneously optimises across all categories and all proposed locations in the image, to improve both the localisation and categorisation of selected detection proposals. Constraints are shared across the labels through the semantic WordNet hierarchy. Our approach proves to be especially useful in large scale settings with thousands of classes, where spatial and semantic interactions are very frequent and only weakly supervised detectors can be built due to a lack of bounding box annotations. Detection experiments are conducted on the ImageNet and COCO dataset, and in settings with thousands of detected categories. Our method provides a significant precision improvement by reducing false positives, while simultaneously improving the recall. version:1
arxiv-1510-02942 | Evaluation of Joint Multi-Instance Multi-Label Learning For Breast Cancer Diagnosis | http://arxiv.org/abs/1510.02942 | id:1510.02942 author:Baris Gecer, Ozge Yalcinkaya, Onur Tasar, Selim Aksoy category:cs.CV cs.LG  published:2015-10-10 summary:Multi-instance multi-label (MIML) learning is a challenging problem in many aspects. Such learning approaches might be useful for many medical diagnosis applications including breast cancer detection and classification. In this study subset of digiPATH dataset (whole slide digital breast cancer histopathology images) are used for training and evaluation of six state-of-the-art MIML methods. At the end, performance comparison of these approaches are given by means of effective evaluation metrics. It is shown that MIML-kNN achieve the best performance that is %65.3 average precision, where most of other methods attain acceptable results as well. version:1
arxiv-1510-02930 | Fast and Accurate Poisson Denoising with Optimized Nonlinear Diffusion | http://arxiv.org/abs/1510.02930 | id:1510.02930 author:Wensen Feng, Yunjin Chen category:cs.CV  published:2015-10-10 summary:The degradation of the acquired signal by Poisson noise is a common problem for various imaging applications, such as medical imaging, night vision and microscopy. Up to now, many state-of-the-art Poisson denoising techniques mainly concentrate on achieving utmost performance, with little consideration for the computation efficiency. Therefore, in this study we aim to propose an efficient Poisson denoising model with both high computational efficiency and recovery quality. To this end, we exploit the newly-developed trainable nonlinear reaction diffusion model which has proven an extremely fast image restoration approach with performance surpassing recent state-of-the-arts. We retrain the model parameters, including the linear filters and influence functions by taking into account the Poisson noise statistics, and end up with an optimized nonlinear diffusion model specialized for Poisson denoising. The trained model provides strongly competitive results against state-of-the-art approaches, meanwhile bearing the properties of simple structure and high efficiency. Furthermore, our proposed model comes along with an additional advantage, that the diffusion process is well-suited for parallel computation on GPUs. For images of size $512 \times 512$, our GPU implementation takes less than 0.1 seconds to produce state-of-the-art Poisson denoising performance. version:1
arxiv-1510-02927 | DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations | http://arxiv.org/abs/1510.02927 | id:1510.02927 author:Srinivas S. S. Kruthiventi, Kumar Ayush, R. Venkatesh Babu category:cs.CV  published:2015-10-10 summary:Understanding and predicting the human visual attentional mechanism is an active area of research in the fields of neuroscience and computer vision. In this work, we propose DeepFix, a first-of-its-kind fully convolutional neural network for accurate saliency prediction. Unlike classical works which characterize the saliency map using various hand-crafted features, our model automatically learns features in a hierarchical fashion and predicts saliency map in an end-to-end manner. DeepFix is designed to capture semantics at multiple scales while taking global context into account using network layers with very large receptive fields. Generally, fully convolutional nets are spatially invariant which prevents them from modeling location dependent patterns (e.g. centre-bias). Our network overcomes this limitation by incorporating a novel Location Biased Convolutional layer. We evaluate our model on two challenging eye fixation datasets -- MIT300, CAT2000 and show that it outperforms other recent approaches by a significant margin. version:1
arxiv-1510-02923 | On 1-Laplacian Elliptic Equations Modeling Magnetic Resonance Image Rician Denoising | http://arxiv.org/abs/1510.02923 | id:1510.02923 author:Adrian Martin, Emanuele Schiavi, Sergio Segura de Leon category:math.AP cs.CV math.NA  published:2015-10-10 summary:Modeling magnitude Magnetic Resonance Images (MRI) rician denoising in a Bayesian or generalized Tikhonov framework using Total Variation (TV) leads naturally to the consideration of nonlinear elliptic equations. These involve the so called $1$-Laplacian operator and special care is needed to properly formulate the problem. The rician statistics of the data are introduced through a singular equation with a reaction term defined in terms of modified first order Bessel functions. An existence theory is provided here together with other qualitative properties of the solutions. Remarkably, each positive global minimum of the associated functional is one of such solutions. Moreover, we directly solve this non--smooth non--convex minimization problem using a convergent Proximal Point Algorithm. Numerical results based on synthetic and real MRI demonstrate a better performance of the proposed method when compared to previous TV based models for rician denoising which regularize or convexify the problem. Finally, an application on real Diffusion Tensor Images, a strongly affected by rician noise MRI modality, is presented and discussed. version:1
arxiv-1510-02906 | Temporal Dynamic Appearance Modeling for Online Multi-Person Tracking | http://arxiv.org/abs/1510.02906 | id:1510.02906 author:Min Yang, Yunde Jia category:cs.CV  published:2015-10-10 summary:Robust online multi-person tracking requires the correct associations of online detection responses with existing trajectories. We address this problem by developing a novel appearance modeling approach to provide accurate appearance affinities to guide data association. In contrast to most existing algorithms that only consider the spatial structure of human appearances, we exploit the temporal dynamic characteristics within temporal appearance sequences to discriminate different persons. The temporal dynamic makes a sufficient complement to the spatial structure of varying appearances in the feature space, which significantly improves the affinity measurement between trajectories and detections. We propose a feature selection algorithm to describe the appearance variations with mid-level semantic features, and demonstrate its usefulness in terms of temporal dynamic appearance modeling. Moreover, the appearance model is learned incrementally by alternatively evaluating newly-observed appearances and adjusting the model parameters to be suitable for online tracking. Reliable tracking of multiple persons in complex scenes is achieved by incorporating the learned model into an online tracking-by-detection framework. Our experiments on the challenging benchmark MOTChallenge 2015 demonstrate that our method outperforms the state-of-the-art multi-person tracking algorithms. version:1
arxiv-1510-02892 | Survey on Feature Selection | http://arxiv.org/abs/1510.02892 | id:1510.02892 author:Tarek Amr Abdallah, Beatriz de La Iglesia category:cs.LG  published:2015-10-10 summary:Feature selection plays an important role in the data mining process. It is needed to deal with the excessive number of features, which can become a computational burden on the learning algorithms. It is also necessary, even when computational resources are not scarce, since it improves the accuracy of the machine learning tasks, as we will see in the upcoming sections. In this review, we discuss the different feature selection approaches, and the relation between them and the various machine learning algorithms. version:1
arxiv-1507-01059 | Remarks on kernel Bayes' rule | http://arxiv.org/abs/1507.01059 | id:1507.01059 author:Hisashi Johno, Kazunori Nakamoto, Tatsuhiko Saigo category:stat.ML  published:2015-07-04 summary:Kernel Bayes' rule has been proposed as a nonparametric kernel-based method to realize Bayesian inference in reproducing kernel Hilbert spaces. However, we demonstrate both theoretically and experimentally that the prediction result by kernel Bayes' rule is in some cases unnatural. We consider that this phenomenon is in part due to the fact that the assumptions in kernel Bayes' rule do not hold in general. version:2
arxiv-1510-02884 | Learn to Evaluate Image Perceptual Quality Blindly from Statistics of Self-similarity | http://arxiv.org/abs/1510.02884 | id:1510.02884 author:Wufeng Xue, Xuanqin Mou, Lei Zhang category:cs.CV  published:2015-10-10 summary:Among the various image quality assessment (IQA) tasks, blind IQA (BIQA) is particularly challenging due to the absence of knowledge about the reference image and distortion type. Features based on natural scene statistics (NSS) have been successfully used in BIQA, while the quality relevance of the feature plays an essential role to the quality prediction performance. Motivated by the fact that the early processing stage in human visual system aims to remove the signal redundancies for efficient visual coding, we propose a simple but very effective BIQA method by computing the statistics of self-similarity (SOS) in an image. Specifically, we calculate the inter-scale similarity and intra-scale similarity of the distorted image, extract the SOS features from these similarities, and learn a regression model to map the SOS features to the subjective quality score. Extensive experiments demonstrate very competitive quality prediction performance and generalization ability of the proposed SOS based BIQA method. version:1
arxiv-1510-02874 | TSEB: More Efficient Thompson Sampling for Policy Learning | http://arxiv.org/abs/1510.02874 | id:1510.02874 author:P. Prasanna, Sarath Chandar, Balaraman Ravindran category:cs.LG  published:2015-10-10 summary:In model-based solution approaches to the problem of learning in an unknown environment, exploring to learn the model parameters takes a toll on the regret. The optimal performance with respect to regret or PAC bounds is achievable, if the algorithm exploits with respect to reward or explores with respect to the model parameters, respectively. In this paper, we propose TSEB, a Thompson Sampling based algorithm with adaptive exploration bonus that aims to solve the problem with tighter PAC guarantees, while being cautious on the regret as well. The proposed approach maintains distributions over the model parameters which are successively refined with more experience. At any given time, the agent solves a model sampled from this distribution, and the sampled reward distribution is skewed by an exploration bonus in order to generate more informative exploration. The policy by solving is then used for generating more experience that helps in updating the posterior over the model parameters. We provide a detailed analysis of the PAC guarantees, and convergence of the proposed approach. We show that our adaptive exploration bonus encourages the additional exploration required for better PAC bounds on the algorithm. We provide empirical analysis on two different simulated domains. version:1
arxiv-1509-08985 | Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree | http://arxiv.org/abs/1509.08985 | id:1509.08985 author:Chen-Yu Lee, Patrick W. Gallagher, Zhuowen Tu category:stat.ML cs.LG cs.NE  published:2015-09-30 summary:We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters. version:2
arxiv-1510-02866 | Wavelet Frame Based Image Restoration Using Sparsity, Nonlocal and Support Prior of Frame Coefficients | http://arxiv.org/abs/1510.02866 | id:1510.02866 author:Liangtian He, Yilun Wang category:cs.CV 90-08 I.4.4  published:2015-10-10 summary:The wavelet frame systems have been widely investigated and applied for image restoration and many other image processing problems over the past decades, attributing to their good capability of sparsely approximating piece-wise smooth functions such as images. Most wavelet frame based models exploit the $l_1$ norm of frame coefficients for a sparsity constraint in the past. The authors in \cite{ZhangY2013, Dong2013} proposed an $l_0$ minimization model, where the $l_0$ norm of wavelet frame coefficients is penalized instead, and have demonstrated that significant improvements can be achieved compared to the commonly used $l_1$ minimization model. Very recently, the authors in \cite{Chen2015} proposed $l_0$-$l_2$ minimization model, where the nonlocal prior of frame coefficients is incorporated. This model proved to outperform the single $l_0$ minimization based model in terms of better recovered image quality. In this paper, we propose a truncated $l_0$-$l_2$ minimization model which combines sparsity, nonlocal and support prior of the frame coefficients. The extensive experiments have shown that the recovery results from the proposed regularization method performs better than existing state-of-the-art wavelet frame based methods, in terms of edge enhancement and texture preserving performance. version:1
arxiv-1506-02107 | Data-Driven Learning of the Number of States in Multi-State Autoregressive Models | http://arxiv.org/abs/1506.02107 | id:1506.02107 author:Jie Ding, Mohammad Noshad, Vahid Tarokh category:stat.ML cs.LG  published:2015-06-06 summary:In this work, we consider the class of multi-state autoregressive processes that can be used to model non-stationary time-series of interest. In order to capture different autoregressive (AR) states underlying an observed time series, it is crucial to select the appropriate number of states. We propose a new model selection technique based on the Gap statistics, which uses a null reference distribution on the stable AR filters to check whether adding a new AR state significantly improves the performance of the model. To that end, we define a new distance measure between AR filters based on mean squared prediction error (MSPE), and propose an efficient method to generate random stable filters that are uniformly distributed in the coefficient space. Numerical results are provided to evaluate the performance of the proposed approach. version:3
arxiv-1510-02855 | AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery | http://arxiv.org/abs/1510.02855 | id:1510.02855 author:Izhar Wallach, Michael Dzamba, Abraham Heifets category:cs.LG cs.NE q-bio.BM stat.ML  published:2015-10-10 summary:Deep convolutional neural networks comprise a subclass of deep neural networks (DNN) with a constrained architecture that leverages the spatial and temporal structure of the domain they model. Convolutional networks achieve the best predictive performance in areas such as speech and image recognition by hierarchically composing simple local features into complex models. Although DNNs have been used in drug discovery for QSAR and ligand-based bioactivity predictions, none of these models have benefited from this powerful convolutional architecture. This paper introduces AtomNet, the first structure-based, deep convolutional neural network designed to predict the bioactivity of small molecules for drug discovery applications. We demonstrate how to apply the convolutional concepts of feature locality and hierarchical composition to the modeling of bioactivity and chemical interactions. In further contrast to existing DNN techniques, we show that AtomNet's application of local convolutional filters to structural target information successfully predicts new active molecules for targets with no previously known modulators. Finally, we show that AtomNet outperforms previous docking approaches on a diverse set of benchmarks by a large margin, achieving an AUC greater than 0.9 on 57.8% of the targets in the DUDE benchmark. version:1
arxiv-1510-01799 | Efficient Per-Example Gradient Computations | http://arxiv.org/abs/1510.01799 | id:1510.01799 author:Ian Goodfellow category:stat.ML cs.LG  published:2015-10-07 summary:This technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters. This gradient norm can be computed efficiently for every example. version:2
arxiv-1510-02833 | Earth Mover's Distance Yields Positive Definite Kernels For Certain Ground Distances | http://arxiv.org/abs/1510.02833 | id:1510.02833 author:Andrew Gardner, Christian A. Duncan, Jinko Kanno, Rastko R. Selmic category:cs.LG stat.ML  published:2015-10-09 summary:Positive definite kernels are an important tool in machine learning that enable efficient solutions to otherwise difficult or intractable problems by implicitly linearizing the problem geometry. In this paper we develop a set-theoretic interpretation of the Earth Mover's Distance (EMD) that naturally yields metric and kernel forms of EMD as generalizations of elementary set operations. In particular, EMD is generalized to sets of unequal size. We also offer the first proof of positive definite kernels based directly on EMD, and provide propositions and conjectures concerning what properties are necessary and sufficient for EMD to be conditionally negative definite. In particular, we show that three distinct positive definite kernels -- intersection, minimum, and Jaccard index -- can be derived from EMD with various ground distances. In the process we show that the Jaccard index is simply the result of a positive definite preserving transformation that can be applied to any kernel. Finally, we evaluate the proposed kernels in various computer vision tasks. version:1
arxiv-1510-02830 | p-Markov Gaussian Processes for Scalable and Expressive Online Bayesian Nonparametric Time Series Forecasting | http://arxiv.org/abs/1510.02830 | id:1510.02830 author:Yves-Laurent Kom Samo, Stephen J. Roberts category:stat.ML  published:2015-10-09 summary:In this paper we introduce a novel online time series forecasting model we refer to as the pM-GP filter. We show that our model is equivalent to Gaussian process regression, with the advantage that both online forecasting and online learning of the hyper-parameters have a constant (rather than cubic) time complexity and a constant (rather than squared) memory requirement in the number of observations, without resorting to approximations. Moreover, the proposed model is expressive in that the family of covariance functions of the implied latent process, namely the spectral Matern kernels, have recently been proven to be capable of approximating arbitrarily well any translation-invariant covariance function. The benefit of our approach compared to competing models is demonstrated using experiments on several real-life datasets. version:1
arxiv-1506-02236 | Generalized Spectral Kernels | http://arxiv.org/abs/1506.02236 | id:1506.02236 author:Yves-Laurent Kom Samo, Stephen Roberts category:stat.ML  published:2015-06-07 summary:In this paper we propose a family of tractable kernels that is dense in the family of bounded positive semi-definite functions (i.e. can approximate any bounded kernel with arbitrary precision). We start by discussing the case of stationary kernels, and propose a family of spectral kernels that extends existing approaches such as spectral mixture kernels and sparse spectrum kernels. Our extension has two primary advantages. Firstly, unlike existing spectral approaches that yield infinite differentiability, the kernels we introduce allow learning the degree of differentiability of the latent function in Gaussian process (GP) models and functions in the reproducing kernel Hilbert space (RKHS) in other kernel methods. Secondly, we show that some of the kernels we propose require fewer parameters than existing spectral kernels for the same accuracy, thereby leading to faster and more robust inference. Finally, we generalize our approach and propose a flexible and tractable family of spectral kernels that we prove can approximate any continuous bounded nonstationary kernel. version:2
arxiv-1509-07302 | Mapping Generative Models onto a Network of Digital Spiking Neurons | http://arxiv.org/abs/1509.07302 | id:1509.07302 author:Bruno U. Pedroni, Srinjoy Das, John V. Arthur, Paul A. Merolla, Bryan L. Jackson, Dharmendra S. Modha, Kenneth Kreutz-Delgado, Gert Cauwenberghs category:cs.NE q-bio.NC  published:2015-09-24 summary:Stochastic neural networks such as Restricted Boltzmann Machines (RBMs) have been successfully used in applications ranging from speech recognition to image classification. Inference and learning in these algorithms use a Markov Chain Monte Carlo procedure called Gibbs sampling, where a logistic function forms the kernel of this sampler. On the other side of the spectrum, neuromorphic systems have shown great promise for low-power and parallelized cognitive computing, but lack well-suited applications and automation procedures. In this work, we propose a systematic method for bridging the RBM algorithm and digital neuromorphic systems, with a generative pattern completion task as proof of concept. For this, we first propose a method of producing the Gibbs sampler using bio-inspired digital noisy integrate-and-fire neurons. Next, we describe the process of mapping generative RBMs trained offline onto the IBM TrueNorth neurosynaptic processor -- a low-power digital neuromorphic VLSI substrate. Mapping these algorithms onto neuromorphic hardware presents unique challenges in network connectivity and weight and bias quantization, which, in turn, require architectural and design strategies for the physical realization. Generative performance metrics are analyzed to validate the neuromorphic requirements and to best select the neuron parameters for the model. Lastly, we describe a design automation procedure which achieves optimal resource usage, accounting for the novel hardware adaptations. This work represents the first implementation of generative RBM inference on a neuromorphic VLSI substrate. version:2
arxiv-1510-02823 | Human languages order information efficiently | http://arxiv.org/abs/1510.02823 | id:1510.02823 author:Daniel Gildea, T. Florian Jaeger category:cs.CL  published:2015-10-09 summary:Most languages use the relative order between words to encode meaning relations. Languages differ, however, in what orders they use and how these orders are mapped onto different meanings. We test the hypothesis that, despite these differences, human languages might constitute different `solutions' to common pressures of language use. Using Monte Carlo simulations over data from five languages, we find that their word orders are efficient for processing in terms of both dependency length and local lexical probability. This suggests that biases originating in how the brain understands language strongly constrain how human languages change over generations. version:1
arxiv-1510-02795 | Dreaming More Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation | http://arxiv.org/abs/1510.02795 | id:1510.02795 author:Søren Hauberg, Oren Freifeld, Anders Boesen Lindbo Larsen, John W. Fisher III, Lars Kai Hansen category:cs.CV  published:2015-10-09 summary:Data augmentation is a key element in training high-dimensional models. In this approach, one synthesizes new observations by applying pre-specified transformations to the original training data; e.g. new images are formed by rotating old ones. Current augmentation schemes, however, rely on manual specification of the applied transformations, making data augmentation an implicit form of feature engineering. Working towards true end-to-end learning, we suggest to learn the applied transformations on a per-class basis. Particularly, we align image pairs within each class under the assumption that the spatial transformation between images belongs to a large class of diffeomorphisms. For each class, we then build a probabilistic generative model of the transformations in a Riemannian submanifold of the Lie group of diffeomorphisms. We demonstrate significant performance improvements in training deep neural nets over manually-specified augmentation schemes. version:1
arxiv-1510-02786 | Recovering a Hidden Community Beyond the Spectral Limit in $O(E \log^*V)$ Time | http://arxiv.org/abs/1510.02786 | id:1510.02786 author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.CC cs.SI math.PR  published:2015-10-09 summary:The stochastic block model for one community with parameters $n, K, p,$ and $q$ is considered: $K$ out of $n$ vertices are in the community; two vertices are connected by an edge with probability $p$ if they are both in the community and with probability $q$ otherwise, where $p > q > 0$ and $p/q$ is assumed to be bounded. An estimator based on observation of the graph $G=(V,E)$ is said to achieve weak recovery if the mean number of misclassified vertices is $o(K)$ as $n \to \infty$. A critical role is played by the effective signal-to-noise ratio $\lambda=K^2(p-q)^2/((n-K)q).$ In the regime $K=\Theta(n)$, a na\"{i}ve degree-thresholding algorithm achieves weak recovery in $O( E )$ time if $\lambda \to \infty$, which coincides with the information theoretic possibility of weak recovery. The main focus of the paper is on weak recovery in the sublinear regime $K=o(n)$ and $np = n^{o(1)}.$ It is shown that weak recovery is provided by a belief propagation algorithm running for $\log^\ast(n)+O(1) $ iterations, if $\lambda > 1/e,$ with the total time complexity $O( E \log^*n)$. Conversely, no local algorithm with radius $t$ of interaction satisfying $t = o(\frac{\log n}{\log(2+np)})$ can asymptotically outperform trivial random guessing if $\lambda \leq 1/e.$ By analyzing a linear message-passing algorithm that corresponds to applying power iteration to the non-backtracking matrix of the graph, we provide evidence to suggest that spectral methods fail to provide weak recovery if $\lambda \leq 1.$ version:1
arxiv-1510-02781 | Where is my puppy? Retrieving lost dogs by facial features | http://arxiv.org/abs/1510.02781 | id:1510.02781 author:Thierry Pinheiro Moreira, Mauricio Lisboa Perez, Rafael de Oliveira Werneck, Eduardo Valle category:cs.CV 68T45 I.5.4  published:2015-10-09 summary:A pet that goes missing is among many people's worst fears. A moment of distraction is enough for a dog or a cat wandering off from home. Animal management services collect stray animals and try to find their owners, but not always successfully. Some measures may improve the chances of matching lost animals to their owners; but automated visual recognition is one that -- although convenient, highly available, and low-cost -- is surprisingly overlooked. In this paper, we inaugurate that promising avenue by pursuing face recognition for dogs. We contrast three ready-to-use human facial recognizers (EigenFaces, FisherFaces and LBPH) to two original solutions based upon existing convolutional neural networks: BARK (inspired in architecture-optimized networks employed for human facial recognition) and WOOF (based upon off-the-shelf OverFeat features). Human facial recognizers perform poorly for dogs (up to 56.1% accuracy), showing that dog facial recognition is not a trivial extension of human facial recognition. The convolutional network solutions work much better, with BARK attaining up to 81.1% accuracy, and WOOF, 89.4%. The tests were conducted in two datasets: Flickr-dog, with 42 dogs of two breeds (pugs and huskies); and Snoopybook, with 18 mongrel dogs. version:1
arxiv-1510-02774 | Human Head Pose Estimation by Facial Features Location | http://arxiv.org/abs/1510.02774 | id:1510.02774 author:Eugene Borovikov category:cs.CV  published:2015-10-09 summary:We describe a method for estimating human head pose in a color image that contains enough of information to locate the head silhouette and detect non-trivial color edges of individual facial features. The method works by spotting the human head on an arbitrary background, extracting the head outline, and locating facial features necessary to describe the head orientation in the 3D space. It is robust enough to work with both color and gray-level images featuring quasi-frontal views of a human head under variable lighting conditions. version:1
arxiv-1510-02710 | Procams-Based Cybernetics | http://arxiv.org/abs/1510.02710 | id:1510.02710 author:Kosuke Sato, Daisuke Iwai, Sei Ikeda, Noriko Takemura category:cs.CV cs.GR cs.HC  published:2015-10-09 summary:Procams-based cybernetics is a unique, emerging research field, which aims at enhancing and supporting our activities by naturally connecting human and computers/machines as a cooperative integrated system via projector-camera systems (procams). It rests on various research domains such as virtual/augmented reality, computer vision, computer graphics, projection display, human computer interface, human robot interaction and so on. This laboratory presentation provides a brief history including recent achievements of our procams-based cybernetics project. version:1
arxiv-1510-02709 | Large-scale Artificial Neural Network: MapReduce-based Deep Learning | http://arxiv.org/abs/1510.02709 | id:1510.02709 author:Kairan Sun, Xu Wei, Gengtao Jia, Risheng Wang, Ruizhi Li category:cs.DC cs.LG cs.NE  published:2015-10-09 summary:Faced with continuously increasing scale of data, original back-propagation neural network based machine learning algorithm presents two non-trivial challenges: huge amount of data makes it difficult to maintain both efficiency and accuracy; redundant data aggravates the system workload. This project is mainly focused on the solution to the issues above, combining deep learning algorithm with cloud computing platform to deal with large-scale data. A MapReduce-based handwriting character recognizer will be designed in this project to verify the efficiency improvement this mechanism will achieve on training and practical large-scale data. Careful discussion and experiment will be developed to illustrate how deep learning algorithm works to train handwritten digits data, how MapReduce is implemented on deep learning neural network, and why this combination accelerates computation. Besides performance, the scalability and robustness will be mentioned in this report as well. Our system comes with two demonstration software that visually illustrates our handwritten digit recognition/encoding application. version:1
arxiv-1510-02173 | Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models | http://arxiv.org/abs/1510.02173 | id:1510.02173 author:John-Alexander M. Assael, Niklas Wahlström, Thomas B. Schön, Marc Peter Deisenroth category:cs.AI cs.CV cs.LG stat.ML  published:2015-10-08 summary:Data-efficient reinforcement learning (RL) in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems. We consider a particularly important instance of this challenge, the pixels-to-torques problem, where an RL agent learns a closed-loop control policy ("torques") from pixel information only. We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information. The key ingredient is a deep dynamical model for learning a low-dimensional feature embedding of images jointly with a predictive model in this low-dimensional feature space. Joint learning is crucial for long-term predictions, which lie at the core of the adaptive nonlinear model predictive control strategy that we use for closed-loop control. Compared to state-of-the-art RL methods for continuous states and actions, our approach learns quickly, scales to high-dimensional state spaces, is lightweight and an important step toward fully autonomous end-to-end learning from pixels to torques. version:2
arxiv-1510-02693 | Feedforward Sequential Memory Neural Networks without Recurrent Feedback | http://arxiv.org/abs/1510.02693 | id:1510.02693 author:ShiLiang Zhang, Hui Jiang, Si Wei, LiRong Dai category:cs.NE cs.CL cs.LG  published:2015-10-09 summary:We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs. version:1
arxiv-1507-01826 | Clustering Network Layers With the Strata Multilayer Stochastic Block Model | http://arxiv.org/abs/1507.01826 | id:1507.01826 author:Natalie Stanley, Saray Shai, Dane Taylor, Peter J. Mucha category:cs.SI physics.soc-ph stat.ML  published:2015-07-07 summary:Multilayer networks are a useful data structure for simultaneously capturing multiple types of relationships between a set of nodes. In such networks, each relational definition gives rise to a layer. While each layer provides its own set of information, community structure across layers can be collectively utilized to discover and quantify underlying relational patterns between nodes. To concisely extract information from a multilayer network, we propose to identify and combine sets of layers with meaningful similarities in community structure. In this paper, we describe the "strata multilayer stochastic block model'' (sMLSBM), a probabilistic model for multilayer community structure. The central extension of the model is that there exist groups of layers, called "strata'', which are defined such that all layers in a given stratum have community structure described by a common stochastic block model (SBM). That is, layers in a stratum exhibit similar node-to-community assignments and SBM probability parameters. Fitting the sMLSBM to a multilayer network provides a joint clustering that yields node-to-community and layer-to-stratum assignments, which cooperatively aid one another during inference. We describe an algorithm for separating layers into their appropriate strata and an inference technique for estimating the SBM parameters for each stratum. We demonstrate our method using synthetic networks and a multilayer network inferred from data collected in the Human Microbiome Project. version:2
arxiv-1510-02676 | Some Theory For Practical Classifier Validation | http://arxiv.org/abs/1510.02676 | id:1510.02676 author:Eric Bax, Ya Le category:stat.ML cs.LG  published:2015-10-09 summary:We compare and contrast two approaches to validating a trained classifier while using all in-sample data for training. One is simultaneous validation over an organized set of hypotheses (SVOOSH), the well-known method that began with VC theory. The other is withhold and gap (WAG). WAG withholds a validation set, trains a holdout classifier on the remaining data, uses the validation data to validate that classifier, then adds the rate of disagreement between the holdout classifier and one trained using all in-sample data, which is an upper bound on the difference in error rates. We show that complex hypothesis classes and limited training data can make WAG a favorable alternative. version:1
arxiv-1510-02674 | Technical Report of Participation in Higgs Boson Machine Learning Challenge | http://arxiv.org/abs/1510.02674 | id:1510.02674 author:S. Raza Ahmad category:cs.LG  published:2015-10-09 summary:This report entails the detailed description of the approach and methodologies taken as part of competing in the Higgs Boson Machine Learning Competition hosted by Kaggle Inc. and organized by CERN et al. It briefly describes the theoretical background of the problem and the motivation for taking part in the competition. Furthermore, the various machine learning models and algorithms analyzed and implemented during the 4 month period of participation are discussed and compared. Special attention is paid to the Deep Learning techniques and architectures implemented from scratch using Python and NumPy for this competition. version:1
arxiv-1510-02644 | Free-hand Sketch Synthesis with Deformable Stroke Models | http://arxiv.org/abs/1510.02644 | id:1510.02644 author:Yi Li, Yi-Zhe Song, Timothy Hospedales, Shaogang Gong category:cs.CV  published:2015-10-09 summary:We present a generative model which can automatically summarize the stroke composition of free-hand sketches of a given category. When our model is fit to a collection of sketches with similar poses, it discovers and learns the structure and appearance of a set of coherent parts, with each part represented by a group of strokes. It represents both consistent (topology) as well as diverse aspects (structure and appearance variations) of each sketch category. Key to the success of our model are important insights learned from a comprehensive study performed on human stroke data. By fitting this model to images, we are able to synthesize visually similar and pleasant free-hand sketches. version:1
arxiv-1502-01956 | Stochastic recursive inclusion in two timescales with an application to the Lagrangian dual problem | http://arxiv.org/abs/1502.01956 | id:1502.01956 author:Arunselvan Ramaswamy, Shalabh Bhatnagar category:cs.SY math.DS stat.ML  published:2015-02-06 summary:In this paper we present a framework to analyze the asymptotic behavior of two timescale stochastic approximation algorithms including those with set-valued mean fields. This paper builds on the works of Borkar and Perkins & Leslie. The framework presented herein is more general as compared to the synchronous two timescale framework of Perkins \& Leslie, however the assumptions involved are easily verifiable. As an application, we use this framework to analyze the two timescale stochastic approximation algorithm corresponding to the Lagrangian dual problem in optimization theory. version:2
arxiv-1510-02558 | Functional Frank-Wolfe Boosting for General Loss Functions | http://arxiv.org/abs/1510.02558 | id:1510.02558 author:Chu Wang, Yingfei Wang, Weinan E, Robert Schapire category:stat.ML cs.LG  published:2015-10-09 summary:Boosting is a generic learning method for classification and regression. Yet, as the number of base hypotheses becomes larger, boosting can lead to a deterioration of test performance. Overfitting is an important and ubiquitous phenomenon, especially in regression settings. To avoid overfitting, we consider using $l_1$ regularization. We propose a novel Frank-Wolfe type boosting algorithm (FWBoost) applied to general loss functions. By using exponential loss, the FWBoost algorithm can be rewritten as a variant of AdaBoost for binary classification. FWBoost algorithms have exactly the same form as existing boosting methods, in terms of making calls to a base learning algorithm with different weights update. This direct connection between boosting and Frank-Wolfe yields a new algorithm that is as practical as existing boosting methods but with new guarantees and rates of convergence. Experimental results show that the test performance of FWBoost is not degraded with larger rounds in boosting, which is consistent with the theoretical analysis. version:1
arxiv-1508-00792 | Fixed-point algorithms for learning determinantal point processes | http://arxiv.org/abs/1508.00792 | id:1508.00792 author:Zelda Mariet, Suvrit Sra category:cs.LG  published:2015-08-04 summary:Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique. version:2
arxiv-1510-02502 | Statistical Analysis of Persistence Intensity Functions | http://arxiv.org/abs/1510.02502 | id:1510.02502 author:Yen-Chi Chen, Daren Wang, Alessandro Rinaldo, Larry Wasserman category:stat.ME stat.ML  published:2015-10-08 summary:Persistence diagrams are two-dimensional plots that summarize the topological features of functions and are an important part of topological data analysis. A problem that has received much attention is how deal with sets of persistence diagrams. How do we summarize them, average them or cluster them? One approach -- the persistence intensity function -- was introduced informally by Edelsbrunner, Ivanov, and Karasev (2012). Here we provide a modification and formalization of this approach. Using the persistence intensity function, we can visualize multiple diagrams, perform clustering and conduct two-sample tests. version:1
arxiv-1510-02442 | Uniform Learning in a Deep Neural Network via "Oddball" Stochastic Gradient Descent | http://arxiv.org/abs/1510.02442 | id:1510.02442 author:Andrew J. R. Simpson category:cs.LG 68Txx  published:2015-10-08 summary:When training deep neural networks, it is typically assumed that the training examples are uniformly difficult to learn. Or, to restate, it is assumed that the training error will be uniformly distributed across the training examples. Based on these assumptions, each training example is used an equal number of times. However, this assumption may not be valid in many cases. "Oddball SGD" (novelty-driven stochastic gradient descent) was recently introduced to drive training probabilistically according to the error distribution - training frequency is proportional to training error magnitude. In this article, using a deep neural network to encode a video, we show that oddball SGD can be used to enforce uniform error across the training set. version:1
arxiv-1510-02437 | Distilling Model Knowledge | http://arxiv.org/abs/1510.02437 | id:1510.02437 author:George Papamakarios category:stat.ML cs.LG  published:2015-10-08 summary:Top-performing machine learning systems, such as deep neural networks, large ensembles and complex probabilistic graphical models, can be expensive to store, slow to evaluate and hard to integrate into larger systems. Ideally, we would like to replace such cumbersome models with simpler models that perform equally well. In this thesis, we study knowledge distillation, the idea of extracting the knowledge contained in a complex model and injecting it into a more convenient model. We present a general framework for knowledge distillation, whereby a convenient model of our choosing learns how to mimic a complex model, by observing the latter's behaviour and being penalized whenever it fails to reproduce it. We develop our framework within the context of three distinct machine learning applications: (a) model compression, where we compress large discriminative models, such as ensembles of neural networks, into models of much smaller size; (b) compact predictive distributions for Bayesian inference, where we distil large bags of MCMC samples into compact predictive distributions in closed form; (c) intractable generative models, where we distil unnormalizable models such as RBMs into tractable models such as NADEs. We contribute to the state of the art with novel techniques and ideas. In model compression, we describe and implement derivative matching, which allows for better distillation when data is scarce. In compact predictive distributions, we introduce online distillation, which allows for significant savings in memory. Finally, in intractable generative models, we show how to use distilled models to robustly estimate intractable quantities of the original model, such as its intractable partition function. version:1
arxiv-1510-02427 | Exact Inference Techniques for the Dynamic Analysis of Attack Graphs | http://arxiv.org/abs/1510.02427 | id:1510.02427 author:Luis Muñoz-González, Daniele Sgandurra, Martín Barrère, Emil Lupu category:cs.CR stat.AP stat.ML 62F15  published:2015-10-08 summary:Attack graphs are a powerful tool for security risk assessment by analysing network vulnerabilities and the paths attackers can use to compromise valuable network resources. The uncertainty about the attacker's behaviour and capabilities make Bayesian networks suitable to model attack graphs to perform static and dynamic analysis. Previous approaches have focused on the formalization of traditional attack graphs into a Bayesian model rather than proposing mechanisms for their analysis. In this paper we propose to use efficient algorithms to make exact inference in Bayesian attack graphs, enabling the static and dynamic network risk assessments. To support the validity of our proposed approach we have performed an extensive experimental evaluation on synthetic Bayesian attack graphs with different topologies, showing the computational advantages in terms of time and memory use of the proposed techniques when compared to existing approaches. version:1
arxiv-1510-02413 | Learning Data-driven Reflectance Priors for Intrinsic Image Decomposition | http://arxiv.org/abs/1510.02413 | id:1510.02413 author:Tinghui Zhou, Philipp Krähenbühl, Alexei A. Efros category:cs.CV  published:2015-10-08 summary:We propose a data-driven approach for intrinsic image decomposition, which is the process of inferring the confounding factors of reflectance and shading in an image. We pose this as a two-stage learning problem. First, we train a model to predict relative reflectance ordering between image patches (`brighter', `darker', `same') from large-scale human annotations, producing a data-driven reflectance prior. Second, we show how to naturally integrate this learned prior into existing energy minimization frameworks for intrinsic image decomposition. We compare our method to the state-of-the-art approach of Bell et al. on both decomposition and image relighting tasks, demonstrating the benefits of the simple relative reflectance prior, especially for scenes under challenging lighting conditions. version:1
arxiv-1503-00757 | Constrained $H^1$-regularization schemes for diffeomorphic image registration | http://arxiv.org/abs/1503.00757 | id:1503.00757 author:Andreas Mang, George Biros category:math.OC cs.CV  published:2015-03-02 summary:We propose regularization schemes for deformable registration and efficient algorithms for its numerical approximation. We treat image registration as a variational optimal control problem. The deformation map is parametrized by its velocity. Tikhonov regularization ensures well-posedness. Our scheme augments standard smoothness regularization operators based on $H^1$- and $H^2$-seminorms with a constraint on the divergence of the velocity field, which resembles variational formulations for Stokes incompressible flows. In our formulation, we invert for a stationary velocity field and a mass source map. This allows us to explicitly control the compressibility of the deformation map and by that the determinant of the deformation gradient. We also introduce a new regularization scheme that allows us to control shear. We use a globalized, preconditioned, matrix-free, reduced space Gauss-Newton-Krylov scheme for numerical optimization. We exploit variable elimination techniques to reduce the number of unknowns of our system; we only iterate on the reduced space of the velocity field. The numerical experiments demonstrate that we can control the determinant of the deformation gradient without compromising registration quality. This additional control allows us to avoid oversmoothing of the deformation map. We also demonstrate that we can promote or penalize shear whilst controlling the determinant of the deformation gradient. version:2
arxiv-1510-02387 | Mapping Unseen Words to Task-Trained Embedding Spaces | http://arxiv.org/abs/1510.02387 | id:1510.02387 author:Pranava Swaroop Madhyastha, Mohit Bansal, Kevin Gimpel, Karen Livescu category:cs.CL cs.LG  published:2015-10-08 summary:We consider the setting in which we train a supervised model that learns task-specific word representations. We assume that we have access to some initial word representations (e.g., unsupervised embeddings), and that the supervised learning procedure updates them to task-specific representations for words contained in the training data. But what about words not contained in the supervised training data? When such unseen words are encountered at test time, they are typically represented by either their initial vectors or a single unknown vector, which often leads to errors. In this paper, we address this issue by learning to map from initial representations to task-specific ones. We present a general technique that uses a neural network mapper with a weighted multiple-loss criterion. This allows us to use the same learned model parameters at test time but now with appropriate task-specific representations for unseen words. We consider the task of dependency parsing and report improvements in performance (and reductions in out-of-vocabulary rates) across multiple domains such as news, Web, and speech. We also achieve downstream improvements on the task of parsing-based sentiment analysis. version:1
arxiv-1509-08255 | Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory | http://arxiv.org/abs/1509.08255 | id:1509.08255 author:Fergal Byrne category:cs.NE cs.AI  published:2015-09-28 summary:In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM) as a model of neocortical computation, the theory and the algorithms have evolved dramatically. This paper presents a detailed description of HTM's Cortical Learning Algorithm (CLA), including for the first time a rigorous mathematical formulation of all aspects of the computations. Prediction Assisted CLA (paCLA), a refinement of the CLA is presented, which is both closer to the neuroscience and adds significantly to the computational power. Finally, we summarise the key functions of neocortex which are expressed in paCLA implementations. version:2
arxiv-1310-3003 | Distance-weighted Support Vector Machine | http://arxiv.org/abs/1310.3003 | id:1310.3003 author:Xingye Qiao, Lingsong Zhang category:stat.ML  published:2013-10-11 summary:A novel linear classification method that possesses the merits of both the Support Vector Machine (SVM) and the Distance-weighted Discrimination (DWD) is proposed in this article. The proposed Distance-weighted Support Vector Machine method can be viewed as a hybrid of SVM and DWD that finds the classification direction by minimizing mainly the DWD loss, and determines the intercept term in the SVM manner. We show that our method inheres the merit of DWD, and hence, overcomes the data-piling and overfitting issue of SVM. On the other hand, the new method is not subject to imbalanced data issue which was a main advantage of SVM over DWD. It uses an unusual loss which combines the Hinge loss (of SVM) and the DWD loss through a trick of axillary hyperplane. Several theoretical properties, including Fisher consistency and asymptotic normality of the DWSVM solution are developed. We use some simulated examples to show that the new method can compete DWD and SVM on both classification performance and interpretability. A real data application further establishes the usefulness of our approach. version:3
arxiv-1510-02364 | Texture Modelling with Nested High-order Markov-Gibbs Random Fields | http://arxiv.org/abs/1510.02364 | id:1510.02364 author:Ralph Versteegen, Georgy Gimel'farb, Patricia Riddle category:cs.CV cs.LG stat.ML  published:2015-10-08 summary:Currently, Markov-Gibbs random field (MGRF) image models which include high-order interactions are almost always built by modelling responses of a stack of local linear filters. Actual interaction structure is specified implicitly by the filter coefficients. In contrast, we learn an explicit high-order MGRF structure by considering the learning process in terms of general exponential family distributions nested over base models, so that potentials added later can build on previous ones. We relatively rapidly add new features by skipping over the costly optimisation of parameters. We introduce the use of local binary patterns as features in MGRF texture models, and generalise them by learning offsets to the surrounding pixels. These prove effective as high-order features, and are fast to compute. Several schemes for selecting high-order features by composition or search of a small subclass are compared. Additionally we present a simple modification of the maximum likelihood as a texture modelling-specific objective function which aims to improve generalisation by local windowing of statistics. The proposed method was experimentally evaluated by learning high-order MGRF models for a broad selection of complex textures and then performing texture synthesis, and succeeded on much of the continuum from stochastic through irregularly structured to near-regular textures. Learning interaction structure is very beneficial for textures with large-scale structure, although those with complex irregular structure still provide difficulties. The texture models were also quantitatively evaluated on two tasks and found to be competitive with other works: grading of synthesised textures by a panel of observers; and comparison against several recent MGRF models by evaluation on a constrained inpainting task. version:1
arxiv-1510-02354 | The Knowledge Gradient with Logistic Belief Models for Binary Classification | http://arxiv.org/abs/1510.02354 | id:1510.02354 author:Yingfei Wang, Chu Wang, Warren Powell category:stat.ML  published:2015-10-08 summary:We consider sequential decision making problems for binary classification scenario in which the learner takes an active role in repeatedly selecting samples from the action pool and receives the binary label of the selected alternatives. Our problem is motivated by applications where observations are time consuming and/or expensive, resulting in small samples. The goal is to identify the best alternative with the highest response. We use Bayesian logistic regression to predict the response of each alternative. By formulating the problem as a Markov decision process, we develop a knowledge-gradient type policy to guide the experiment by maximizing the expected value of information of labeling each alternative and provide a finite-time analysis on the estimated error. Experiments on benchmark UCI datasets demonstrate the effectiveness of the proposed method. version:1
arxiv-1508-01580 | Automata networks for memory loss effects in the formation of linguistic conventions | http://arxiv.org/abs/1508.01580 | id:1508.01580 author:Javier Vera, Eric Goles category:cs.CL physics.soc-ph  published:2015-08-07 summary:This work attempts to give new theoretical insights to the absence of intermediate stages in the evolution of language. In particular, it is developed an automata networks approach to a crucial question: how a population of language users can reach agreement on a linguistic convention? To describe the appearance of sharp transitions in the self-organization of language, it is adopted an extremely simple model of (working) memory. At each time step, language users simply loss part of their word-memories. Through computer simulations of low-dimensional lattices, it appear sharp transitions at critical values that depend on the size of the vicinities of the individuals. version:2
arxiv-1506-07656 | DeepMatching: Hierarchical Deformable Dense Matching | http://arxiv.org/abs/1506.07656 | id:1506.07656 author:Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid category:cs.CV  published:2015-06-25 summary:We introduce a novel matching algorithm, called DeepMatching, to compute dense correspondences between images. DeepMatching relies on a hierarchical, multi-layer, correlational architecture designed for matching images and was inspired by deep convolutional approaches. The proposed matching algorithm can handle non-rigid deformations and repetitive textures and efficiently determines dense correspondences in the presence of significant changes between images. We evaluate the performance of DeepMatching, in comparison with state-of-the-art matching algorithms, on the Mikolajczyk (Mikolajczyk et al 2005), the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013) datasets. DeepMatching outperforms the state-of-the-art algorithms and shows excellent results in particular for repetitive textures.We also propose a method for estimating optical flow, called DeepFlow, by integrating DeepMatching in the large displacement optical flow (LDOF) approach of Brox and Malik (2011). Compared to existing matching algorithms, additional robustness to large displacements and complex motion is obtained thanks to our matching approach. DeepFlow obtains competitive performance on public benchmarks for optical flow estimation. version:2
arxiv-1408-1292 | Scalable Greedy Algorithms for Transfer Learning | http://arxiv.org/abs/1408.1292 | id:1408.1292 author:Ilja Kuzborskij, Francesco Orabona, Barbara Caputo category:cs.CV cs.LG  published:2014-08-06 summary:In this paper we consider the binary transfer learning problem, focusing on how to select and combine sources from a large pool to yield a good performance on a target task. Constraining our scenario to real world, we do not assume the direct access to the source data, but rather we employ the source hypotheses trained from them. We propose an efficient algorithm that selects relevant source hypotheses and feature dimensions simultaneously, building on the literature on the best subset selection problem. Our algorithm achieves state-of-the-art results on three computer vision datasets, substantially outperforming both transfer learning and popular feature selection baselines in a small-sample setting. We also present a randomized variant that achieves the same results with a fraction of the computational cost. Also, we theoretically prove that, under reasonable assumptions on the source hypotheses, our algorithm can learn effectively from few examples. version:3
arxiv-1510-02267 | Reduced-Order Modeling Of Hidden Dynamics | http://arxiv.org/abs/1510.02267 | id:1510.02267 author:Patrick Héas, Cédric Herzet category:stat.ML stat.AP  published:2015-10-08 summary:The objective of this paper is to investigate how noisy and incomplete observations can be integrated in the process of building a reduced-order model. This problematic arises in many scientific domains where there exists a need for accurate low-order descriptions of highly-complex phenomena, which can not be directly and/or deterministically observed. Within this context, the paper proposes a probabilistic framework for the construction of "POD-Galerkin" reduced-order models. Assuming a hidden Markov chain, the inference integrates the uncertainty of the hidden states relying on their posterior distribution. Simulations show the benefits obtained by exploiting the proposed framework. version:1
arxiv-1510-02255 | Empirical Analysis of Sampling Based Estimators for Evaluating RBMs | http://arxiv.org/abs/1510.02255 | id:1510.02255 author:Vidyadhar Upadhya, P. S. Sastry category:cs.LG stat.ML  published:2015-10-08 summary:The Restricted Boltzmann Machines (RBM) can be used either as classifiers or as generative models. The quality of the generative RBM is measured through the average log-likelihood on test data. Due to the high computational complexity of evaluating the partition function, exact calculation of test log-likelihood is very difficult. In recent years some estimation methods are suggested for approximate computation of test log-likelihood. In this paper we present an empirical comparison of the main estimation methods, namely, the AIS algorithm for estimating the partition function, the CSL method for directly estimating the log-likelihood, and the RAISE algorithm that combines these two ideas. We use the MNIST data set to learn the RBM and then compare these methods for estimating the test log-likelihood. version:1
arxiv-1509-06658 | Attribute-Graph: A Graph based approach to Image Ranking | http://arxiv.org/abs/1509.06658 | id:1509.06658 author:Nikita Prabhu, R. Venkatesh Babu category:cs.CV  published:2015-09-22 summary:We propose a novel image representation, termed Attribute-Graph, to rank images by their semantic similarity to a given query image. An Attribute-Graph is an undirected fully connected graph, incorporating both local and global image characteristics. The graph nodes characterise objects as well as the overall scene context using mid-level semantic attributes, while the edges capture the object topology. We demonstrate the effectiveness of Attribute-Graphs by applying them to the problem of image ranking. We benchmark the performance of our algorithm on the 'rPascal' and 'rImageNet' datasets, which we have created in order to evaluate the ranking performance on complex queries containing multiple objects. Our experimental evaluation shows that modelling images as Attribute-Graphs results in improved ranking performance over existing techniques. version:2
arxiv-1510-02192 | Simultaneous Deep Transfer Across Domains and Tasks | http://arxiv.org/abs/1510.02192 | id:1510.02192 author:Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko category:cs.CV  published:2015-10-08 summary:Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simultaneously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adaptation method offers empirical performance which exceeds previously published results on two standard benchmark visual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings. version:1
arxiv-1510-02175 | Learning Summary Statistic for Approximate Bayesian Computation via Deep Neural Network | http://arxiv.org/abs/1510.02175 | id:1510.02175 author:Bai Jiang, Tung-yu Wu, Charles Zheng, Wing H. Wong category:stat.ME stat.CO stat.ML  published:2015-10-08 summary:Approximate Bayesian Computation (ABC) methods are used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Both the accuracy and computational efficiency of ABC depend on the choice of summary statistic, but outside of special cases where the optimal summary statistics are known, it is unclear which guiding principles can be used to construct effective summary statistics. In this paper we explore the possibility of automating the process of constructing summary statistics by training deep neural networks to predict the parameters from artificially generated data: the resulting summary statistics are approximately posterior means of the parameters. With minimal model-specific tuning, our method constructs summary statistics for the Ising model and the moving-average model, which match or exceed theoretically-motivated summary statistics in terms of the accuracies of the resulting posteriors. version:1
arxiv-1505-01554 | Webly Supervised Learning of Convolutional Networks | http://arxiv.org/abs/1505.01554 | id:1505.01554 author:Xinlei Chen, Abhinav Gupta category:cs.CV  published:2015-05-07 summary:We present an approach to utilize large amounts of web data for learning CNNs. Specifically inspired by curriculum learning, we present a two-step approach for CNN training. First, we use easy images to train an initial visual representation. We then use this initial CNN and adapt it to harder, more realistic images by leveraging the structure of data and categories. We demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on ImageNet on Pascal VOC 2012. We also demonstrate the strength of webly supervised learning by localizing objects in web images and training a R-CNN style detector. It achieves the best performance on VOC 2007 where no VOC training data is used. Finally, we show our approach is quite robust to noise and performs comparably even when we use image search results from March 2013 (pre-CNN image search era). version:2
arxiv-1510-02131 | DeepLogo: Hitting Logo Recognition with the Deep Neural Network Hammer | http://arxiv.org/abs/1510.02131 | id:1510.02131 author:Forrest N. Iandola, Anting Shen, Peter Gao, Kurt Keutzer category:cs.CV  published:2015-10-07 summary:Recently, there has been a flurry of industrial activity around logo recognition, such as Ditto's service for marketers to track their brands in user-generated images, and LogoGrab's mobile app platform for logo recognition. However, relatively little academic or open-source logo recognition progress has been made in the last four years. Meanwhile, deep convolutional neural networks (DCNNs) have revolutionized a broad range of object recognition applications. In this work, we apply DCNNs to logo recognition. We propose several DCNN architectures, with which we surpass published state-of-art accuracy on a popular logo recognition dataset. version:1
arxiv-1503-09113 | On the Projective Geometry of Kalman Filter | http://arxiv.org/abs/1503.09113 | id:1503.09113 author:Francesca Paola Carli, Rodolphe Sepulchre category:math.OC stat.ML  published:2015-03-31 summary:Convergence of the Kalman filter is best analyzed by studying the contraction of the Riccati map in the space of positive definite (covariance) matrices. In this paper, we explore how this contraction property relates to a more fundamental non-expansiveness property of filtering maps in the space of probability distributions endowed with the Hilbert metric. This is viewed as a preliminary step towards improving the convergence analysis of filtering algorithms over general graphical models. version:2
arxiv-1510-02078 | Leveraging Context to Support Automated Food Recognition in Restaurants | http://arxiv.org/abs/1510.02078 | id:1510.02078 author:Vinay Bettadapura, Edison Thomaz, Aman Parnami, Gregory Abowd, Irfan Essa category:cs.CV  published:2015-10-07 summary:The pervasiveness of mobile cameras has resulted in a dramatic increase in food photos, which are pictures reflecting what people eat. In this paper, we study how taking pictures of what we eat in restaurants can be used for the purpose of automating food journaling. We propose to leverage the context of where the picture was taken, with additional information about the restaurant, available online, coupled with state-of-the-art computer vision techniques to recognize the food being consumed. To this end, we demonstrate image-based recognition of foods eaten in restaurants by training a classifier with images from restaurant's online menu databases. We evaluate the performance of our system in unconstrained, real-world settings with food images taken in 10 restaurants across 5 different types of food (American, Indian, Italian, Mexican and Thai). version:1
arxiv-1510-02073 | Egocentric Field-of-View Localization Using First-Person Point-of-View Devices | http://arxiv.org/abs/1510.02073 | id:1510.02073 author:Vinay Bettadapura, Irfan Essa, Caroline Pantofaru category:cs.CV  published:2015-10-07 summary:We present a technique that uses images, videos and sensor data taken from first-person point-of-view devices to perform egocentric field-of-view (FOV) localization. We define egocentric FOV localization as capturing the visual information from a person's field-of-view in a given environment and transferring this information onto a reference corpus of images and videos of the same space, hence determining what a person is attending to. Our method matches images and video taken from the first-person perspective with the reference corpus and refines the results using the first-person's head orientation information obtained using the device sensors. We demonstrate single and multi-user egocentric FOV localization in different indoor and outdoor environments with applications in augmented reality, event understanding and studying social interactions. version:1
arxiv-1510-02071 | Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition | http://arxiv.org/abs/1510.02071 | id:1510.02071 author:Vinay Bettadapura, Grant Schindler, Thomaz Plotz, Irfan Essa category:cs.CV  published:2015-10-07 summary:We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use of randomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets. version:1
arxiv-1508-03868 | Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology | http://arxiv.org/abs/1508.03868 | id:1508.03868 author:Brendan Jou, Tao Chen, Nikolaos Pappas, Miriam Redi, Mercan Topkara, Shih-Fu Chang category:cs.MM cs.CL cs.CV cs.IR  published:2015-08-16 summary:Every culture and language is unique. Our work expressly focuses on the uniqueness of culture and language in relation to human affect, specifically sentiment and emotion semantics, and how they manifest in social multimedia. We develop sets of sentiment- and emotion-polarized visual concepts by adapting semantic structures called adjective-noun pairs, originally introduced by Borth et al. (2013), but in a multilingual context. We propose a new language-dependent method for automatic discovery of these adjective-noun constructs. We show how this pipeline can be applied on a social multimedia platform for the creation of a large-scale multilingual visual sentiment concept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our unified ontology is organized hierarchically by multilingual clusters of visually detectable nouns and subclusters of emotionally biased versions of these nouns. In addition, we present an image-based prediction task to show how generalizable language-specific models are in a multilingual context. A new, publicly available dataset of >15.6K sentiment-biased visual concepts across 12 languages with language-specific detector banks, >7.36M images and their metadata is also released. version:3
arxiv-1510-02055 | Diverse Large-Scale ITS Dataset Created from Continuous Learning for Real-Time Vehicle Detection | http://arxiv.org/abs/1510.02055 | id:1510.02055 author:Justin A. Eichel, Akshaya Mishra, Nicholas Miller, Nicholas Jankovic, Mohan A. Thomas, Tyler Abbott, Douglas Swanson, Joel Keller category:cs.CV  published:2015-10-07 summary:In traffic engineering, vehicle detectors are trained on limited datasets resulting in poor accuracy when deployed in real world applications. Annotating large-scale high quality datasets is challenging. Typically, these datasets have limited diversity; they do not reflect the real-world operating environment. There is a need for a large-scale, cloud based positive and negative mining (PNM) process and a large-scale learning and evaluation system for the application of traffic event detection. The proposed positive and negative mining process addresses the quality of crowd sourced ground truth data through machine learning review and human feedback mechanisms. The proposed learning and evaluation system uses a distributed cloud computing framework to handle data-scaling issues associated with large numbers of samples and a high-dimensional feature space. The system is trained using AdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotated video frames. The trained real-time vehicle detector achieves an accuracy of at least $95\%$ for $1/2$ and about $78\%$ for $19/20$ of the time when tested on approximately $7,500,000$ video frames. At the end of 2015, the dataset is expect to have over one billion annotated video frames. version:1
arxiv-1510-02054 | Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations | http://arxiv.org/abs/1510.02054 | id:1510.02054 author:Weiran Wang, Raman Arora, Karen Livescu, Nathan Srebro category:cs.LG  published:2015-10-07 summary:Deep CCA is a recently proposed deep neural network extension to the traditional canonical correlation analysis (CCA), and has been successful for multi-view representation learning in several domains. However, stochastic optimization of the deep CCA objective is not straightforward, because it does not decouple over training examples. Previous optimizers for deep CCA are either batch-based algorithms or stochastic optimization using large minibatches, which can have high memory consumption. In this paper, we tackle the problem of stochastic optimization for deep CCA with small minibatches, based on an iterative solution to the CCA objective, and show that we can achieve as good performance as previous optimizers and thus alleviate the memory requirement. version:1
arxiv-1510-02049 | Assisting Composition of Email Responses: a Topic Prediction Approach | http://arxiv.org/abs/1510.02049 | id:1510.02049 author:Spandana Gella, Marc Dymetman, Jean Michel Renders, Sriram Venkatapathy category:cs.CL  published:2015-10-07 summary:We propose an approach for helping agents compose email replies to customer requests. To enable that, we use LDA to extract latent topics from a collection of email exchanges. We then use these latent topics to label our data, obtaining a so-called "silver standard" topic labelling. We exploit this labelled set to train a classifier to: (i) predict the topic distribution of the entire agent's email response, based on features of the customer's email; and (ii) predict the topic distribution of the next sentence in the agent's reply, based on the customer's email features and on features of the agent's current sentence. The experimental results on a large email collection from a contact center in the tele- com domain show that the proposed ap- proach is effective in predicting the best topic of the agent's next sentence. In 80% of the cases, the correct topic is present among the top five recommended topics (out of fifty possible ones). This shows the potential of this method to be applied in an interactive setting, where the agent is presented a small list of likely topics to choose from for the next sentence. version:1
arxiv-1510-01972 | Event-based Camera Pose Tracking using a Generative Event Model | http://arxiv.org/abs/1510.01972 | id:1510.01972 author:Guillermo Gallego, Christian Forster, Elias Mueggler, Davide Scaramuzza category:cs.CV cs.RO  published:2015-10-07 summary:Event-based vision sensors mimic the operation of biological retina and they represent a major paradigm shift from traditional cameras. Instead of providing frames of intensity measurements synchronously, at artificially chosen rates, event-based cameras provide information on brightness changes asynchronously, when they occur. Such non-redundant pieces of information are called "events". These sensors overcome some of the limitations of traditional cameras (response time, bandwidth and dynamic range) but require new methods to deal with the data they output. We tackle the problem of event-based camera localization in a known environment, without additional sensing, using a probabilistic generative event model in a Bayesian filtering framework. Our main contribution is the design of the likelihood function used in the filter to process the observed events. Based on the physical characteristics of the sensor and on empirical evidence of the Gaussian-like distribution of spiked events with respect to the brightness change, we propose to use the contrast residual as a measure of how well the estimated pose of the event-based camera and the environment explain the observed events. The filter allows for localization in the general case of six degrees-of-freedom motions. version:1
arxiv-1508-00144 | Quantitative evaluation of the performance of discrete-time reservoir computers in the forecasting, filtering, and reconstruction of stochastic stationary signals | http://arxiv.org/abs/1508.00144 | id:1508.00144 author:Lyudmila Grigoryeva, Julie Henriques, Juan-Pablo Ortega category:cs.ET cs.NE math.ST stat.TH  published:2015-08-01 summary:This paper extends the notion of information processing capacity for non-independent input signals in the context of reservoir computing (RC). The presence of input autocorrelation makes worthwhile the treatment of forecasting and filtering problems for which we explicitly compute this generalized capacity as a function of the reservoir parameter values using a streamlined model. The reservoir model leading to these developments is used to show that, whenever that approximation is valid, this computational paradigm satisfies the so called separation and fading memory properties that are usually associated with good information processing performances. We show that several standard memory, forecasting, and filtering problems that appear in the parametric stochastic time series context can be readily formulated and tackled via RC which, as we show, significantly outperforms standard techniques in some instances. version:3
arxiv-1510-01949 | Hierarchical Representation of Prosody for Statistical Speech Synthesis | http://arxiv.org/abs/1510.01949 | id:1510.01949 author:Antti Suni, Daniel Aalto, Martti Vainio category:cs.CL cs.SD  published:2015-10-07 summary:Prominences and boundaries are the essential constituents of prosodic structure in speech. They provide for means to chunk the speech stream into linguistically relevant units by providing them with relative saliences and demarcating them within coherent utterance structures. Prominences and boundaries have both been widely used in both basic research on prosody as well as in text-to-speech synthesis. However, there are no representation schemes that would provide for both estimating and modelling them in a unified fashion. Here we present an unsupervised unified account for estimating and representing prosodic prominences and boundaries using a scale-space analysis based on continuous wavelet transform. The methods are evaluated and compared to earlier work using the Boston University Radio News corpus. The results show that the proposed method is comparable with the best published supervised annotation methods. version:1
arxiv-1107-0674 | "Memory foam" approach to unsupervised learning | http://arxiv.org/abs/1107.0674 | id:1107.0674 author:Natalia B. Janson, Christopher J. Marsden category:nlin.AO cs.LG 34F05  60G99  published:2011-07-04 summary:We propose an alternative approach to construct an artificial learning system, which naturally learns in an unsupervised manner. Its mathematical prototype is a dynamical system, which automatically shapes its vector field in response to the input signal. The vector field converges to a gradient of a multi-dimensional probability density distribution of the input process, taken with negative sign. The most probable patterns are represented by the stable fixed points, whose basins of attraction are formed automatically. The performance of this system is illustrated with musical signals. version:3
arxiv-1510-01942 | Helping Domain Experts Build Speech Translation Systems | http://arxiv.org/abs/1510.01942 | id:1510.01942 author:Manny Rayner, Alejandro Armando, Pierrette Bouillon, Sarah Ebling, Johanna Gerlach, Sonia Halimi, Irene Strasly, Nikos Tsourakis category:cs.HC cs.CL  published:2015-10-07 summary:We present a new platform, "Regulus Lite", which supports rapid development and web deployment of several types of phrasal speech translation systems using a minimal formalism. A distinguishing feature is that most development work can be performed directly by domain experts. We motivate the need for platforms of this type and discuss three specific cases: medical speech translation, speech-to-sign-language translation and voice questionnaires. We briefly describe initial experiences in developing practical systems. version:1
arxiv-1510-01886 | Using Ontology-Based Context in the Portuguese-English Translation of Homographs in Textual Dialogues | http://arxiv.org/abs/1510.01886 | id:1510.01886 author:Diego Moussallem, Ricardo Choren category:cs.CL  published:2015-10-07 summary:This paper introduces a novel approach to tackle the existing gap on message translations in dialogue systems. Currently, submitted messages to the dialogue systems are considered as isolated sentences. Thus, missing context information impede the disambiguation of homographs words in ambiguous sentences. Our approach solves this disambiguation problem by using concepts over existing ontologies. version:1
arxiv-1412-0767 | Learning Spatiotemporal Features with 3D Convolutional Networks | http://arxiv.org/abs/1412.0767 | id:1412.0767 author:Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri category:cs.CV  published:2014-12-02 summary:We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use. version:4
arxiv-1503-01521 | Jointly Learning Multiple Measures of Similarities from Triplet Comparisons | http://arxiv.org/abs/1503.01521 | id:1503.01521 author:Liwen Zhang, Subhransu Maji, Ryota Tomioka category:stat.ML cs.AI cs.CV cs.LG  published:2015-03-05 summary:Similarity between objects is multi-faceted and it can be easier for human annotators to measure it when the focus is on a specific aspect. We consider the problem of mapping objects into view-specific embeddings where the distance between them is consistent with the similarity comparisons of the form "from the t-th view, object A is more similar to B than to C". Our framework jointly learns view-specific embeddings exploiting correlations between views. Experiments on a number of datasets, including one of multi-view crowdsourced comparison on bird images, show the proposed method achieves lower triplet generalization error when compared to both learning embeddings independently for each view and all views pooled into one view. Our method can also be used to learn multiple measures of similarity over input features taking class labels into account and compares favorably to existing approaches for multi-task metric learning on the ISOLET dataset. version:3
arxiv-1303-3934 | A Quorum Sensing Inspired Algorithm for Dynamic Clustering | http://arxiv.org/abs/1303.3934 | id:1303.3934 author:Feng Tan, Jean-Jacques Slotine category:cs.LG  published:2013-03-16 summary:Quorum sensing is a decentralized biological process, through which a community of cells with no global awareness coordinate their functional behaviors based solely on cell-medium interactions and local decisions. This paper draws inspirations from quorum sensing and colony competition to derive a new algorithm for data clustering. The algorithm treats each data as a single cell, and uses knowledge of local connectivity to cluster cells into multiple colonies simultaneously. It simulates auto-inducers secretion in quorum sensing to tune the influence radius for each cell. At the same time, sparsely distributed core cells spread their influences to form colonies, and interactions between colonies eventually determine each cell's identity. The algorithm has the flexibility to analyze not only static but also time-varying data, which surpasses the capacity of many existing algorithms. Its stability and convergence properties are established. The algorithm is tested on several applications, including both synthetic and real benchmarks data sets, alleles clustering, community detection, image segmentation. In particular, the algorithm's distinctive capability to deal with time-varying data allows us to experiment it on novel applications such as robotic swarms grouping and switching model identification. We believe that the algorithm's promising performance would stimulate many more exciting applications. version:2
arxiv-1506-03521 | Isometric sketching of any set via the Restricted Isometry Property | http://arxiv.org/abs/1506.03521 | id:1506.03521 author:Samet Oymak, Benjamin Recht, Mahdi Soltanolkotabi category:cs.IT cs.DS math.IT math.PR math.ST stat.ML stat.TH  published:2015-06-11 summary:In this paper we show that for the purposes of dimensionality reduction certain class of structured random matrices behave similarly to random Gaussian matrices. This class includes several matrices for which matrix-vector multiply can be computed in log-linear time, providing efficient dimensionality reduction of general sets. In particular, we show that using such matrices any set from high dimensions can be embedded into lower dimensions with near optimal distortion. We obtain our results by connecting dimensionality reduction of any set to dimensionality reduction of sparse vectors via a chaining argument. version:2
arxiv-1510-01722 | Structured Transforms for Small-Footprint Deep Learning | http://arxiv.org/abs/1510.01722 | id:1510.01722 author:Vikas Sindhwani, Tara N. Sainath, Sanjiv Kumar category:stat.ML cs.CV cs.LG  published:2015-10-06 summary:We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression. version:1
arxiv-1510-01717 | Language Segmentation | http://arxiv.org/abs/1510.01717 | id:1510.01717 author:David Alfter category:cs.CL  published:2015-10-06 summary:Language segmentation consists in finding the boundaries where one language ends and another language begins in a text written in more than one language. This is important for all natural language processing tasks. The problem can be solved by training language models on language data. However, in the case of low- or no-resource languages, this is problematic. I therefore investigate whether unsupervised methods perform better than supervised methods when it is difficult or impossible to train supervised approaches. A special focus is given to difficult texts, i.e. texts that are rather short (one sentence), containing abbreviations, low-resource languages and non-standard language. I compare three approaches: supervised n-gram language models, unsupervised clustering and weakly supervised n-gram language model induction. I devised the weakly supervised approach in order to deal with difficult text specifically. In order to test the approach, I compiled a small corpus of different text types, ranging from one-sentence texts to texts of about 300 words. The weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. The results look promising, but there is room for improvement and a more thorough investigation should be undertaken. version:1
arxiv-1505-00687 | Unsupervised Learning of Visual Representations using Videos | http://arxiv.org/abs/1505.00687 | id:1505.00687 author:Xiaolong Wang, Abhinav Gupta category:cs.CV  published:2015-05-04 summary:Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to the same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation. version:2
arxiv-1510-01663 | Euclidean Auto Calibration of Camera Networks: Baseline Constraint Removes Scale Ambiguity | http://arxiv.org/abs/1510.01663 | id:1510.01663 author:Kiran Kumar Vupparaboina, Kamala Raghavan, Soumya Jana category:cs.CV  published:2015-10-06 summary:Metric auto calibration of a camera network from multiple views has been reported by several authors. Resulting 3D reconstruction recovers shape faithfully, but not scale. However, preservation of scale becomes critical in applications, such as multi-party telepresence, where multiple 3D scenes need to be fused into a single coordinate system. In this context, we propose a camera network configuration that includes a stereo pair with known baseline separation, and analytically demonstrate Euclidean auto calibration of such network under mild conditions. Further, we experimentally validate our theory using a four-camera network. Importantly, our method not only recovers scale, but also compares favorably with the well known Zhang and Pollefeys methods in terms of shape recovery. version:1
arxiv-1411-6241 | Improved Spectral Clustering via Embedded Label Propagation | http://arxiv.org/abs/1411.6241 | id:1411.6241 author:Xiaojun Chang, Feiping Nie, Yi Yang, Heng Huang category:cs.LG  published:2014-11-23 summary:Spectral clustering is a key research topic in the field of machine learning and data mining. Most of the existing spectral clustering algorithms are built upon Gaussian Laplacian matrices, which are sensitive to parameters. We propose a novel parameter free, distance consistent Locally Linear Embedding. The proposed distance consistent LLE promises that edges between closer data points have greater weight.Furthermore, we propose a novel improved spectral clustering via embedded label propagation. Our algorithm is built upon two advancements of the state of the art:1) label propagation,which propagates a node\'s labels to neighboring nodes according to their proximity; and 2) manifold learning, which has been widely used in its capacity to leverage the manifold structure of data points. First we perform standard spectral clustering on original data and assign each cluster to k nearest data points. Next, we propagate labels through dense, unlabeled data regions. Extensive experiments with various datasets validate the superiority of the proposed algorithm compared to current state of the art spectral algorithms. version:2
arxiv-1510-01648 | A Latent Source Model for Patch-Based Image Segmentation | http://arxiv.org/abs/1510.01648 | id:1510.01648 author:George Chen, Devavrat Shah, Polina Golland category:cs.CV  published:2015-10-06 summary:Despite the popularity and empirical success of patch-based nearest-neighbor and weighted majority voting approaches to medical image segmentation, there has been no theoretical development on when, why, and how well these nonparametric methods work. We bridge this gap by providing a theoretical performance guarantee for nearest-neighbor and weighted majority voting segmentation under a new probabilistic model for patch-based image segmentation. Our analysis relies on a new local property for how similar nearby patches are, and fuses existing lines of work on modeling natural imagery patches and theory for nonparametric classification. We use the model to derive a new patch-based segmentation algorithm that iterates between inferring local label patches and merging these local segmentations to produce a globally consistent image segmentation. Many existing patch-based algorithms arise as special cases of the new algorithm. version:1
arxiv-1510-01628 | Large-scale subspace clustering using sketching and validation | http://arxiv.org/abs/1510.01628 | id:1510.01628 author:Panagiotis A. Traganitis, Konstantinos Slavakis, Georgios B. Giannakis category:cs.LG cs.CV stat.ML  published:2015-10-06 summary:The nowadays massive amounts of generated and communicated data present major challenges in their processing. While capable of successfully classifying nonlinearly separable objects in various settings, subspace clustering (SC) methods incur prohibitively high computational complexity when processing large-scale data. Inspired by the random sampling and consensus (RANSAC) approach to robust regression, the present paper introduces a randomized scheme for SC, termed sketching and validation (SkeVa-)SC, tailored for large-scale data. At the heart of SkeVa-SC lies a randomized scheme for approximating the underlying probability density function of the observed data by kernel smoothing arguments. Sparsity in data representations is also exploited to reduce the computational burden of SC, while achieving high clustering accuracy. Performance analysis as well as extensive numerical tests on synthetic and real data corroborate the potential of SkeVa-SC and its competitive performance relative to state-of-the-art scalable SC approaches. Keywords: Subspace clustering, big data, kernel smoothing, randomization, sketching, validation, sparsity. version:1
arxiv-1510-01576 | Predicting Daily Activities From Egocentric Images Using Deep Learning | http://arxiv.org/abs/1510.01576 | id:1510.01576 author:Daniel Castro, Steven Hickson, Vinay Bettadapura, Edison Thomaz, Gregory Abowd, Henrik Christensen, Irfan Essa category:cs.CV I.5; J.4; J.3  published:2015-10-06 summary:We present a method to analyze images taken from a passive egocentric wearable camera along with the contextual information, such as time and day of week, to learn and predict everyday activities of an individual. We collected a dataset of 40,103 egocentric images over a 6 month period with 19 activity classes and demonstrate the benefit of state-of-the-art deep learning techniques for learning and predicting daily activities. Classification is conducted using a Convolutional Neural Network (CNN) with a classification method we introduce called a late fusion ensemble. This late fusion ensemble incorporates relevant contextual information and increases our classification accuracy. Our technique achieves an overall accuracy of 83.07% in predicting a person's activity across the 19 activity classes. We also demonstrate some promising results from two additional users by fine-tuning the classifier with one day of training data. version:1
arxiv-1510-01570 | Analyzer and generator for Pali | http://arxiv.org/abs/1510.01570 | id:1510.01570 author:David Alfter category:cs.CL  published:2015-10-06 summary:This work describes a system that performs morphological analysis and generation of Pali words. The system works with regular inflectional paradigms and a lexical database. The generator is used to build a collection of inflected and derived words, which in turn is used by the analyzer. Generating and storing morphological forms along with the corresponding morphological information allows for efficient and simple look up by the analyzer. Indeed, by looking up a word and extracting the attached morphological information, the analyzer does not have to compute this information. As we must, however, assume the lexical database to be incomplete, the system can also work without the dictionary component, using a rule-based approach. version:1
arxiv-1510-01562 | Parameterized Neural Network Language Models for Information Retrieval | http://arxiv.org/abs/1510.01562 | id:1510.01562 author:Benjamin Piwowarski, Sylvain Lamprier, Nicolas Despres category:cs.IR cs.CL H.3.3; I.2.6  published:2015-10-06 summary:Information Retrieval (IR) models need to deal with two difficult issues, vocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to the difficulty of retrieving relevant documents that do not contain exact query terms but semantically related terms. Term dependencies refers to the need of considering the relationship between the words of the query when estimating the relevance of a document. A multitude of solutions has been proposed to solve each of these two problems, but no principled model solve both. In parallel, in the last few years, language models based on neural networks have been used to cope with complex natural language processing tasks like emotion and paraphrase detection. Although they present good abilities to cope with both term dependencies and vocabulary mismatch problems, thanks to the distributed representation of words they are based upon, such models could not be used readily in IR, where the estimation of one language model per document (or query) is required. This is both computationally unfeasible and prone to over-fitting. Based on a recent work that proposed to learn a generic language model that can be modified through a set of document-specific parameters, we explore use of new neural network models that are adapted to ad-hoc IR tasks. Within the language model IR framework, we propose and study the use of a generic language model as well as a document-specific language model. Both can be used as a smoothing component, but the latter is more adapted to the document at hand and has the potential of being used as a full document language model. We experiment with such models and analyze their results on TREC-1 to 8 datasets. version:1
arxiv-1510-01113 | RAID: A Relation-Augmented Image Descriptor | http://arxiv.org/abs/1510.01113 | id:1510.01113 author:Paul Guerrero, Niloy J. Mitra, Peter Wonka category:cs.GR cs.CV I.4.8; I.4.7  published:2015-10-05 summary:As humans, we regularly interpret images based on the relations between image regions. For example, a person riding object X, or a plank bridging two objects. Current methods provide limited support to search for images based on such relations. We present RAID, a relation-augmented image descriptor that supports queries based on inter-region relations. The key idea of our descriptor is to capture the spatial distribution of simple point-to-region relationships to describe more complex relationships between two image regions. We evaluate the proposed descriptor by querying into a large subset of the Microsoft COCO database and successfully extract nontrivial images demonstrating complex inter-region relations, which are easily missed or erroneously classified by existing methods. version:2
arxiv-1510-01553 | Learning Deep Representations of Appearance and Motion for Anomalous Event Detection | http://arxiv.org/abs/1510.01553 | id:1510.01553 author:Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, Nicu Sebe category:cs.CV  published:2015-10-06 summary:We present a novel unsupervised deep learning framework for anomalous event detection in complex video scenes. While most existing works merely use hand-crafted appearance and motion features, we propose Appearance and Motion DeepNet (AMDN) which utilizes deep neural networks to automatically learn feature representations. To exploit the complementary information of both appearance and motion patterns, we introduce a novel double fusion framework, combining both the benefits of traditional early fusion and late fusion strategies. Specifically, stacked denoising autoencoders are proposed to separately learn both appearance and motion features as well as a joint representation (early fusion). Based on the learned representations, multiple one-class SVM models are used to predict the anomaly scores of each input, which are then integrated with a late fusion strategy for final anomaly detection. We evaluate the proposed method on two publicly available video surveillance datasets, showing competitive performance with respect to state of the art approaches. version:1
arxiv-1510-01544 | Active Transfer Learning with Zero-Shot Priors: Reusing Past Datasets for Future Tasks | http://arxiv.org/abs/1510.01544 | id:1510.01544 author:Efstratios Gavves, Thomas Mensink, Tatiana Tommasi, Cees G. M. Snoek, Tinne Tuytelaars category:cs.CV  published:2015-10-06 summary:How can we reuse existing knowledge, in the form of available datasets, when solving a new and apparently unrelated target task from a set of unlabeled data? In this work we make a first contribution to answer this question in the context of image classification. We frame this quest as an active learning problem and use zero-shot classifiers to guide the learning process by linking the new task to the existing classifiers. By revisiting the dual formulation of adaptive SVM, we reveal two basic conditions to choose greedily only the most relevant samples to be annotated. On this basis we propose an effective active learning algorithm which learns the best possible target classification model with minimum human labeling effort. Extensive experiments on two challenging datasets show the value of our approach compared to the state-of-the-art active learning methodologies, as well as its potential to reuse past datasets with minimal effort for future tasks. version:1
arxiv-1510-01518 | DC Decomposition of Nonconvex Polynomials with Algebraic Techniques | http://arxiv.org/abs/1510.01518 | id:1510.01518 author:Amir Ali Ahmadi, Georgina Hall category:math.OC cs.DS stat.ML  published:2015-10-06 summary:We consider the problem of decomposing a multivariate polynomial as the difference of two convex polynomials. We introduce algebraic techniques which reduce this task to linear, second order cone, and semidefinite programming. This allows us to optimize over subsets of valid difference of convex decompositions (dcds) and find ones that speed up the convex-concave procedure (CCP). We prove, however, that optimizing over the entire set of dcds is NP-hard. version:1
arxiv-1510-01495 | Quantifying Emergent Behavior of Autonomous Robots | http://arxiv.org/abs/1510.01495 | id:1510.01495 author:Georg Martius, Eckehard Olbrich category:cs.IT cs.LG cs.RO math.DS math.IT H.1.1; I.2.9  published:2015-10-06 summary:Quantifying behaviors of robots which were generated autonomously from task-independent objective functions is an important prerequisite for objective comparisons of algorithms and movements of animals. The temporal sequence of such a behavior can be considered as a time series and hence complexity measures developed for time series are natural candidates for its quantification. The predictive information and the excess entropy are such complexity measures. They measure the amount of information the past contains about the future and thus quantify the nonrandom structure in the temporal sequence. However, when using these measures for systems with continuous states one has to deal with the fact that their values will depend on the resolution with which the systems states are observed. For deterministic systems both measures will diverge with increasing resolution. We therefore propose a new decomposition of the excess entropy in resolution dependent and resolution independent parts and discuss how they depend on the dimensionality of the dynamics, correlations and the noise level. For the practical estimation we propose to use estimates based on the correlation integral instead of the direct estimation of the mutual information using the algorithm by Kraskov et al. (2004) which is based on next neighbor statistics because the latter allows less control of the scale dependencies. Using our algorithm we are able to show how autonomous learning generates behavior of increasing complexity with increasing learning duration. version:1
arxiv-1510-01490 | Directional Global Three-part Image Decomposition | http://arxiv.org/abs/1510.01490 | id:1510.01490 author:Duy Hoang Thai, Carsten Gottschlich category:cs.CV  published:2015-10-06 summary:We consider the task of image decomposition and we introduce a new model coined directional global three-part decomposition (DG3PD) for solving it. As key ingredients of the DG3PD model, we introduce a discrete multi-directional total variation norm and a discrete multi-directional G-norm. Using these novel norms, the proposed discrete DG3PD model can decompose an image into two parts or into three parts. Existing models for image decomposition by Vese and Osher, by Aujol and Chambolle, by Starck et al., and by Thai and Gottschlich are included as special cases in the new model. Decomposition of an image by DG3PD results in a cartoon image, a texture image and a residual image. Advantages of the DG3PD model over existing ones lie in the properties enforced on the cartoon and texture images. The geometric objects in the cartoon image have a very smooth surface and sharp edges. The texture image yields oscillating patterns on a defined scale which is both smooth and sparse. Moreover, the DG3PD method achieves the goal of perfect reconstruction by summation of all components better than the other considered methods. Relevant applications of DG3PD are a novel way of image compression as well as feature extraction for applications such as latent fingerprint processing and optical character recognition. version:1
arxiv-1510-01485 | Bayesian Markov Blanket Estimation | http://arxiv.org/abs/1510.01485 | id:1510.01485 author:Dinu Kaufmann, Sonali Parbhoo, Aleksander Wieczorek, Sebastian Keller, David Adametz, Volker Roth category:stat.ML cs.LG  published:2015-10-06 summary:This paper considers a Bayesian view for estimating a sub-network in a Markov random field. The sub-network corresponds to the Markov blanket of a set of query variables, where the set of potential neighbours here is big. We factorize the posterior such that the Markov blanket is conditionally independent of the network of the potential neighbours. By exploiting this blockwise decoupling, we derive analytic expressions for posterior conditionals. Subsequently, we develop an inference scheme which makes use of the factorization. As a result, estimation of a sub-network is possible without inferring an entire network. Since the resulting Gibbs sampler scales linearly with the number of variables, it can handle relatively large neighbourhoods. The proposed scheme results in faster convergence and superior mixing of the Markov chain than existing Bayesian network estimation techniques. version:1
arxiv-1510-01463 | Local Rademacher Complexity Bounds based on Covering Numbers | http://arxiv.org/abs/1510.01463 | id:1510.01463 author:Yunwen Lei, Lixin Ding, Yingzhou Bi category:cs.AI cs.LG stat.ML  published:2015-10-06 summary:This paper provides a general result on controlling local Rademacher complexities, which captures in an elegant form to relate the complexities with constraint on the expected norm to the corresponding ones with constraint on the empirical norm. This result is convenient to apply in real applications and could yield refined local Rademacher complexity bounds for function classes satisfying general entropy conditions. We demonstrate the power of our complexity bounds by applying them to derive effective generalization error bounds. version:1
arxiv-1510-01443 | A Waveform Representation Framework for High-quality Statistical Parametric Speech Synthesis | http://arxiv.org/abs/1510.01443 | id:1510.01443 author:Bo Fan, Siu Wa Lee, Xiaohai Tian, Lei Xie, Minghui Dong category:cs.SD cs.LG 68T10  published:2015-10-06 summary:State-of-the-art statistical parametric speech synthesis (SPSS) generally uses a vocoder to represent speech signals and parameterize them into features for subsequent modeling. Magnitude spectrum has been a dominant feature over the years. Although perceptual studies have shown that phase spectrum is essential to the quality of synthesized speech, it is often ignored by using a minimum phase filter during synthesis and the speech quality suffers. To bypass this bottleneck in vocoded speech, this paper proposes a phase-embedded waveform representation framework and establishes a magnitude-phase joint modeling platform for high-quality SPSS. Our experiments on waveform reconstruction show that the performance is better than that of the widely-used STRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms a leading-edge, vocoded, deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based baseline system in various objective evaluation metrics conducted. version:1
arxiv-1509-01004 | Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias | http://arxiv.org/abs/1509.01004 | id:1509.01004 author:Yohei Kondo, Kohei Hayashi, Shin-ichi Maeda category:stat.ML cs.LG  published:2015-09-03 summary:A common strategy for sparse linear regression is to introduce regularization, which eliminates irrelevant features by letting the corresponding weights be zeros. However, regularization often shrinks the estimator for relevant features, which leads to incorrect feature selection. Motivated by the above-mentioned issue, we propose Bayesian masking (BM), a sparse estimation method which imposes no regularization on the weights. The key concept of BM is to introduce binary latent variables that randomly mask features. Estimating the masking rates determines the relevance of the features automatically. We derive a variational Bayesian inference algorithm that maximizes the lower bound of the factorized information criterion (FIC), which is a recently developed asymptotic criterion for evaluating the marginal log-likelihood. In addition, we propose reparametrization to accelerate the convergence of the derived algorithm. Finally, we show that BM outperforms Lasso and automatic relevance determination (ARD) in terms of the sparsity-shrinkage trade-off. version:2
arxiv-1510-01442 | Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-encoders | http://arxiv.org/abs/1510.01442 | id:1510.01442 author:Huan Yang, Baoyuan Wang, Stephen Lin, David Wipf, Minyi Guo, Baining Guo category:cs.CV  published:2015-10-06 summary:With the growing popularity of short-form video sharing platforms such as \em{Instagram} and \em{Vine}, there has been an increasing need for techniques that automatically extract highlights from video. Whereas prior works have approached this problem with heuristic rules or supervised learning, we present an unsupervised learning approach that takes advantage of the abundance of user-edited videos on social media websites such as YouTube. Based on the idea that the most significant sub-events within a video class are commonly present among edited videos while less interesting ones appear less frequently, we identify the significant sub-events via a robust recurrent auto-encoder trained on a collection of user-edited videos queried for each particular class of interest. The auto-encoder is trained using a proposed shrinking exponential loss function that makes it robust to noise in the web-crawled training data, and is configured with bidirectional long short term memory (LSTM)~\cite{LSTM:97} cells to better model the temporal structure of highlight segments. Different from supervised techniques, our method can infer highlights using only a set of downloaded edited videos, without also needing their pre-edited counterparts which are rarely available online. Extensive experiments indicate the promise of our proposed solution in this challenging unsupervised settin version:1
arxiv-1510-01440 | Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification | http://arxiv.org/abs/1510.01440 | id:1510.01440 author:Ruobing Wu, Baoyuan Wang, Wenping Wang, Yizhou Yu category:cs.CV  published:2015-10-06 summary:Recent work on scene classification still makes use of generic CNN features in a rudimentary manner. In this ICCV 2015 paper, we present a novel pipeline built upon deep CNN features to harvest discriminative visual objects and parts for scene classification. We first use a region proposal technique to generate a set of high-quality patches potentially containing objects, and apply a pre-trained CNN to extract generic deep features from these patches. Then we perform both unsupervised and weakly supervised learning to screen these patches and discover discriminative ones representing category-specific objects and parts. We further apply discriminative clustering enhanced with local CNN fine-tuning to aggregate similar objects and parts into groups, called meta objects. A scene image representation is constructed by pooling the feature response maps of all the learned meta objects at multiple spatial scales. We have confirmed that the scene image representation obtained using this new pipeline is capable of delivering state-of-the-art performance on two popular scene benchmark datasets, MIT Indoor 67~\cite{MITIndoor67} and Sun397~\cite{Sun397} version:1
arxiv-1510-01422 | Improved Estimation of Class Prior Probabilities through Unlabeled Data | http://arxiv.org/abs/1510.01422 | id:1510.01422 author:Norman Matloff category:stat.ML cs.LG  published:2015-10-06 summary:Work in the classification literature has shown that in computing a classification function, one need not know the class membership of all observations in the training set; the unlabeled observations still provide information on the marginal distribution of the feature set, and can thus contribute to increased classification accuracy for future observations. The present paper will show that this scheme can also be used for the estimation of class prior probabilities, which would be very useful in applications in which it is difficult or expensive to determine class membership. Both parametric and nonparametric estimators are developed. Asymptotic distributions of the estimators are derived, and it is proven that the use of the unlabeled observations does reduce asymptotic variance. This methodology is also extended to the estimation of subclass probabilities. version:1
arxiv-1411-5737 | Fuzzy Adaptive Resonance Theory, Diffusion Maps and their applications to Clustering and Biclustering | http://arxiv.org/abs/1411.5737 | id:1411.5737 author:S. B. Damelin, Y. Gu, D. C. Wunsch II, R. Xu category:cs.NE cs.LG  published:2014-11-21 summary:In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive Resonance Dif- fusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theory to do clustering on high dimensional data. We describe some applications of this method and some problems for future research. version:5
arxiv-1510-01401 | On the Existence of Epipolar Matrices | http://arxiv.org/abs/1510.01401 | id:1510.01401 author:Sameer Agarwal, Hon-Leung Lee, Bernd Sturmfels, Rekha R. Thomas category:cs.CV math.AG  published:2015-10-06 summary:This paper considers the foundational question of the existence of a fundamental (resp. essential) matrix given $m$ point correspondences in two views. We present a complete answer for the existence of fundamental matrices for any value of $m$. Using examples we disprove the widely held beliefs that fundamental matrices always exist whenever $m \leq 7$. At the same time, we prove that they exist unconditionally when $m \leq 5$. Under a mild genericity condition, we show that an essential matrix always exists when $m \leq 4$. We also characterize the six and seven point configurations in two views for which all matrices satisfying the epipolar constraint have rank at most one. version:1
arxiv-1502-02734 | Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation | http://arxiv.org/abs/1502.02734 | id:1502.02734 author:George Papandreou, Liang-Chieh Chen, Kevin Murphy, Alan L. Yuille category:cs.CV  published:2015-02-09 summary:Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket.org/deeplab/deeplab-public. version:3
arxiv-1510-01378 | Batch Normalized Recurrent Neural Networks | http://arxiv.org/abs/1510.01378 | id:1510.01378 author:César Laurent, Gabriel Pereyra, Philémon Brakel, Ying Zhang, Yoshua Bengio category:stat.ML cs.LG cs.NE  published:2015-10-05 summary:Recurrent Neural Networks (RNNs) are powerful models for sequential data that have the potential to learn long-term dependencies. However, they are computationally expensive to train and difficult to parallelize. Recent work has shown that normalizing intermediate representations of neural networks can significantly improve convergence rates in feedforward neural networks . In particular, batch normalization, which uses mini-batch statistics to standardize features, was shown to significantly reduce training time. In this paper, we show that applying batch normalization to the hidden-to-hidden transitions of our RNNs doesn't help the training procedure. We also show that when applied to the input-to-hidden transitions, batch normalization can lead to a faster convergence of the training criterion but doesn't seem to improve the generalization performance on both our language modelling and speech recognition tasks. All in all, applying batch normalization to RNNs turns out to be more challenging than applying it to feedforward networks, but certain variants of it can still be beneficial. version:1
arxiv-1510-01344 | Within-Brain Classification for Brain Tumor Segmentation | http://arxiv.org/abs/1510.01344 | id:1510.01344 author:Mohammad Havaei, Hugo Larochelle, Philippe Poulin, Pierre-Marc Jodoin category:cs.CV cs.AI  published:2015-10-05 summary:Purpose: In this paper, we investigate a framework for interactive brain tumor segmentation which, at its core, treats the problem of interactive brain tumor segmentation as a machine learning problem. Methods: This method has an advantage over typical machine learning methods for this task where generalization is made across brains. The problem with these methods is that they need to deal with intensity bias correction and other MRI-specific noise. In this paper, we avoid these issues by approaching the problem as one of within brain generalization. Specifically, we propose a semi-automatic method that segments a brain tumor by training and generalizing within that brain only, based on some minimum user interaction. Conclusion: We investigate how adding spatial feature coordinates (i.e. $i$, $j$, $k$) to the intensity features can significantly improve the performance of different classification methods such as SVM, kNN and random forests. This would only be possible within an interactive framework. We also investigate the use of a more appropriate kernel and the adaptation of hyper-parameters specifically for each brain. Results: As a result of these experiments, we obtain an interactive method whose results reported on the MICCAI-BRATS 2013 dataset are the second most accurate compared to published methods, while using significantly less memory and processing power than most state-of-the-art methods. version:1
arxiv-1510-01308 | Tight Variational Bounds via Random Projections and I-Projections | http://arxiv.org/abs/1510.01308 | id:1510.01308 author:Lun-Kai Hsu, Tudor Achim, Stefano Ermon category:cs.LG  published:2015-10-05 summary:Information projections are the key building block of variational inference algorithms and are used to approximate a target probabilistic model by projecting it onto a family of tractable distributions. In general, there is no guarantee on the quality of the approximation obtained. To overcome this issue, we introduce a new class of random projections to reduce the dimensionality and hence the complexity of the original model. In the spirit of random projections, the projection preserves (with high probability) key properties of the target distribution. We show that information projections can be combined with random projections to obtain provable guarantees on the quality of the approximation obtained, regardless of the complexity of the original model. We demonstrate empirically that augmenting mean field with a random projection step dramatically improves partition function and marginal probability estimates, both on synthetic and real world data. version:1
arxiv-1510-01270 | Learning in Unlabeled Networks - An Active Learning and Inference Approach | http://arxiv.org/abs/1510.01270 | id:1510.01270 author:Tomasz Kajdanowicz, Radosław Michalski, Katarzyna Musiał, Przemysław Kazienko category:stat.ML cs.LG cs.SI  published:2015-10-05 summary:The task of determining labels of all network nodes based on the knowledge about network structure and labels of some training subset of nodes is called the within-network classification. It may happen that none of the labels of the nodes is known and additionally there is no information about number of classes to which nodes can be assigned. In such a case a subset of nodes has to be selected for initial label acquisition. The question that arises is: "labels of which nodes should be collected and used for learning in order to provide the best classification accuracy for the whole network?". Active learning and inference is a practical framework to study this problem. A set of methods for active learning and inference for within network classification is proposed and validated. The utility score calculation for each node based on network structure is the first step in the process. The scores enable to rank the nodes. Based on the ranking, a set of nodes, for which the labels are acquired, is selected (e.g. by taking top or bottom N from the ranking). The new measure-neighbour methods proposed in the paper suggest not obtaining labels of nodes from the ranking but rather acquiring labels of their neighbours. The paper examines 29 distinct formulations of utility score and selection methods reporting their impact on the results of two collective classification algorithms: Iterative Classification Algorithm and Loopy Belief Propagation. We advocate that the accuracy of presented methods depends on the structural properties of the examined network. We claim that measure-neighbour methods will work better than the regular methods for networks with higher clustering coefficient and worse than regular methods for networks with low clustering coefficient. According to our hypothesis, based on clustering coefficient we are able to recommend appropriate active learning and inference method. version:1
arxiv-1510-01257 | Efficient Object Detection for High Resolution Images | http://arxiv.org/abs/1510.01257 | id:1510.01257 author:Yongxi Lu, Tara Javidi category:cs.CV  published:2015-10-05 summary:Efficient generation of high-quality object proposals is an essential step in state-of-the-art object detection systems based on deep convolutional neural networks (DCNN) features. Current object proposal algorithms are computationally inefficient in processing high resolution images containing small objects, which makes them the bottleneck in object detection systems. In this paper we present effective methods to detect objects for high resolution images. We combine two complementary strategies. The first approach is to predict bounding boxes based on adjacent visual features. The second approach uses high level image features to guide a two-step search process that adaptively focuses on regions that are likely to contain small objects. We extract features required for the two strategies by utilizing a pre-trained DCNN model known as AlexNet. We demonstrate the effectiveness of our algorithm by showing its performance on a high-resolution image subset of the SUN 2012 object detection dataset. version:1
arxiv-1510-01225 | Bayesian Inference via Approximation of Log-likelihood for Priors in Exponential Family | http://arxiv.org/abs/1510.01225 | id:1510.01225 author:Tohid Ardeshiri, Umut Orguner, Fredrik Gustafsson category:cs.LG stat.ML  published:2015-10-05 summary:In this paper, a Bayesian inference technique based on Taylor series approximation of the logarithm of the likelihood function is presented. The proposed approximation is devised for the case, where the prior distribution belongs to the exponential family of distributions. The logarithm of the likelihood function is linearized with respect to the sufficient statistic of the prior distribution in exponential family such that the posterior obtains the same exponential family form as the prior. Similarities between the proposed method and the extended Kalman filter for nonlinear filtering are illustrated. Furthermore, an extended target measurement update for target models where the target extent is represented by a random matrix having an inverse Wishart distribution is derived. The approximate update covers the important case where the spread of measurement is due to the target extent as well as the measurement noise in the sensor. version:1
arxiv-1510-01175 | Cross-Device Tracking: Matching Devices and Cookies | http://arxiv.org/abs/1510.01175 | id:1510.01175 author:Roberto Díaz-Morales category:cs.LG cs.CY  published:2015-10-05 summary:The number of computers, tablets and smartphones is increasing rapidly, which entails the ownership and use of multiple devices to perform online tasks. As people move across devices to complete these tasks, their identities becomes fragmented. Understanding the usage and transition between those devices is essential to develop efficient applications in a multi-device world. In this paper we present a solution to deal with the cross-device identification of users based on semi-supervised machine learning methods to identify which cookies belong to an individual using a device. The method proposed in this paper scored third in the ICDM 2015 Drawbridge Cross-Device Connections challenge proving its good performance. version:1
arxiv-1510-01171 | Convergence Analysis of a Stochastic Projection-free Algorithm | http://arxiv.org/abs/1510.01171 | id:1510.01171 author:Jean Lafond, Hoi-To Wai, Eric Moulines category:stat.ML cs.LG  published:2015-10-05 summary:This paper presents and analyzes a stochastic version of the Frank-Wolfe algorithm (a.k.a. conditional gradient method or projection-free algorithm) for constrained convex optimization. We first prove that when the quality of gradient estimate improves as ${\cal O}( \sqrt{ \eta_t^{\Delta} / t } )$, where $t$ is the iteration index and $\eta_t^{\Delta}$ is an increasing sequence, then the objective value of the stochastic Frank-Wolfe algorithm converges in at least the same order. When the optimal solution lies in the interior of the constraint set, the convergence rate is accelerated to ${\cal O}(\eta_t^{\Delta} /t)$. Secondly, we study how the stochastic Frank-Wolfe algorithm can be applied to a few practical machine learning problems. Tight bounds on the gradient estimate errors for these examples are established. Numerical simulations support our findings. version:1
arxiv-1506-04828 | Significance of the levels of spectral valleys with application to front/back distinction of vowel sounds | http://arxiv.org/abs/1506.04828 | id:1506.04828 author:T. V. Ananthapadmanabha, A. G. Ramakrishnan, Shubham Sharma category:cs.CL cs.SD  published:2015-06-16 summary:An objective critical distance (OCD) has been defined as that spacing between adjacent formants, when the level of the valley between them reaches the mean spectral level. The measured OCD lies in the same range (viz., 3-3.5 bark) as the critical distance determined by subjective experiments for similar experimental conditions. The level of spectral valley serves a purpose similar to that of the spacing between the formants with an added advantage that it can be measured from the spectral envelope without an explicit knowledge of formant frequencies. Based on the relative spacing of formant frequencies, the level of the spectral valley, VI (between F1 and F2) is much higher than the level of VII (spectral valley between F2 and F3) for back vowels and vice-versa for front vowels. Classification of vowels into front/back distinction with the difference (VI-VII) as an acoustic feature, tested using TIMIT, NTIMIT, Tamil and Kannada language databases gives, on the average, an accuracy of about 95%, which is comparable to the accuracy (90.6%) obtained using a neural network classifier trained and tested using MFCC as the feature vector for TIMIT database. The acoustic feature (VI-VII) has also been tested for its robustness on the TIMIT database for additive white and babble noise and an accuracy of about 95% has been obtained for SNRs down to 25 dB for both types of noise. version:2
arxiv-1510-01130 | Bregman Iteration for Correspondence Problems: A Study of Optical Flow | http://arxiv.org/abs/1510.01130 | id:1510.01130 author:Laurent Hoeltgen, Michael Breuß category:math.OC cs.CV 65Kxx  65Nxx  published:2015-10-05 summary:Bregman iterations are known to yield excellent results for denoising, deblurring and compressed sensing tasks, but so far this technique has rarely been used for other image processing problems. In this paper we give a thorough description of the Bregman iteration, unifying thereby results of different authors within a common framework. Then we show how to adapt the split Bregman iteration, originally developed by Goldstein and Osher for image restoration purposes, to optical flow which is a fundamental correspondence problem in computer vision. We consider some classic and modern optical flow models and present detailed algorithms that exhibit the benefits of the Bregman iteration. By making use of the results of the Bregman framework, we address the issues of convergence and error estimation for the algorithms. Numerical examples complement the theoretical part. version:1
arxiv-1510-01077 | Nonlinear Spectral Analysis via One-homogeneous Functionals - Overview and Future Prospects | http://arxiv.org/abs/1510.01077 | id:1510.01077 author:Guy Gilboa, Michael Moeller, Martin Burger category:math.SP cs.CV cs.NA math.NA  published:2015-10-05 summary:We present in this paper the motivation and theory of nonlinear spectral representations, based on convex regularizing functionals. Some comparisons and analogies are drawn to the fields of signal processing, harmonic analysis and sparse representations. The basic approach, main results and initial applications are shown. A discussion of open problems and future directions concludes this work. version:1
arxiv-1510-01064 | Boosting in the presence of outliers: adaptive classification with non-convex loss functions | http://arxiv.org/abs/1510.01064 | id:1510.01064 author:Alexander Hanbo Li, Jelena Bradic category:stat.ML cs.LG math.ST stat.TH  published:2015-10-05 summary:This paper examines the role and efficiency of the non-convex loss functions for binary classification problems. In particular, we investigate how to design a simple and effective boosting algorithm that is robust to the outliers in the data. The analysis of the role of a particular non-convex loss for prediction accuracy varies depending on the diminishing tail properties of the gradient of the loss -- the ability of the loss to efficiently adapt to the outlying data, the local convex properties of the loss and the proportion of the contaminated data. In order to use these properties efficiently, we propose a new family of non-convex losses named $\gamma$-robust losses. Moreover, we present a new boosting framework, {\it Arch Boost}, designed for augmenting the existing work such that its corresponding classification algorithm is significantly more adaptable to the unknown data contamination. Along with the Arch Boosting framework, the non-convex losses lead to the new class of boosting algorithms, named adaptive, robust, boosting (ARB). Furthermore, we present theoretical examples that demonstrate the robustness properties of the proposed algorithms. In particular, we develop a new breakdown point analysis and a new influence function analysis that demonstrate gains in robustness. Moreover, we present new theoretical results, based only on local curvatures, which may be used to establish statistical and optimization properties of the proposed Arch boosting algorithms with highly non-convex loss functions. Extensive numerical calculations are used to illustrate these theoretical properties and reveal advantages over the existing boosting methods when data exhibits a number of outliers. version:1
arxiv-1503-03278 | Stochastic Texture Difference for Scale-Dependent Data Analysis | http://arxiv.org/abs/1503.03278 | id:1503.03278 author:Nicolas Brodu, Hussein Yahia category:cs.CV  published:2015-03-11 summary:This article introduces the Stochastic Texture Difference method for analyzing data at prescribed spatial and value scales. This method relies on constrained random walks around each pixel, describing how nearby image values typically evolve on each side of this pixel. Textures are represented as probability distributions of such random walks, so a texture difference operator is statistically defined as a distance between these distributions in a suitable reproducing kernel Hilbert space. The method is thus not limited to scalar pixel values: any data type for which a kernel is available may be considered, from color triplets and multispectral vector data to strings, graphs, and more. By adjusting the size of the neighborhoods that are compared, the method is implicitly scale-dependent. It is also able to focus on either small changes or large gradients. We demonstrate how it can be used to infer spatial and data value characteristic scales in measured signals and natural images. version:3
arxiv-1510-00618 | Automatic Taxonomy Extraction from Query Logs with no Additional Sources of Information | http://arxiv.org/abs/1510.00618 | id:1510.00618 author:Miguel Fernandez-Fernandez, Daniel Gayo-Avello category:cs.CL  published:2015-10-02 summary:Search engine logs store detailed information on Web users interactions. Thus, as more and more people use search engines on a daily basis, important trails of users common knowledge are being recorded in those files. Previous research has shown that it is possible to extract concept taxonomies from full text documents, while other scholars have proposed methods to obtain similar queries from query logs. We propose a mixture of both lines of research, that is, mining query logs not to find related queries nor query hierarchies, but actual term taxonomies that could be used to improve search engine effectiveness and efficiency. As a result, in this study we have developed a method that combines lexical heuristics with a supervised classification model to successfully extract hyponymy relations from specialization search patterns revealed from log missions, with no additional sources of information, and in a language independent way. version:2
arxiv-1510-01041 | GPU-Based Computation of 2D Least Median of Squares with Applications to Fast and Robust Line Detection | http://arxiv.org/abs/1510.01041 | id:1510.01041 author:Gil Shapira, Tal Hassner category:cs.CV  published:2015-10-05 summary:The 2D Least Median of Squares (LMS) is a popular tool in robust regression because of its high breakdown point: up to half of the input data can be contaminated with outliers without affecting the accuracy of the LMS estimator. The complexity of 2D LMS estimation has been shown to be $\Omega(n^2)$ where $n$ is the total number of points. This high theoretical complexity along with the availability of graphics processing units (GPU) motivates the development of a fast, parallel, GPU-based algorithm for LMS computation. We present a CUDA based algorithm for LMS computation and show it to be much faster than the optimal state of the art single threaded CPU algorithm. We begin by describing the proposed method and analyzing its performance. We then demonstrate how it can be used to modify the well-known Hough Transform (HT) in order to efficiently detect image lines in noisy images. Our method is compared with standard HT-based line detection methods and shown to overcome their shortcomings in terms of both efficiency and accuracy. version:1
arxiv-1510-01027 | Relaxed Multiple-Instance SVM with Application to Object Discovery | http://arxiv.org/abs/1510.01027 | id:1510.01027 author:Xinggang Wang, Zhuotun Zhu, Cong Yao, Xiang Bai category:cs.CV cs.LG  published:2015-10-05 summary:Multiple-instance learning (MIL) has served as an important tool for a wide range of vision applications, for instance, image classification, object detection, and visual tracking. In this paper, we propose a novel method to solve the classical MIL problem, named relaxed multiple-instance SVM (RMI-SVM). We treat the positiveness of instance as a continuous variable, use Noisy-OR model to enforce the MIL constraints, and jointly optimize the bag label and instance label in a unified framework. The optimization problem can be efficiently solved using stochastic gradient decent. The extensive experiments demonstrate that RMI-SVM consistently achieves superior performance on various benchmarks for MIL. Moreover, we simply applied RMI-SVM to a challenging vision task, common object discovery. The state-of-the-art results of object discovery on Pascal VOC datasets further confirm the advantages of the proposed method. version:1
arxiv-1510-01026 | Calculating entropy at different scales among diverse communication systems | http://arxiv.org/abs/1510.01026 | id:1510.01026 author:Gerardo Febres, Klaus Jaffe category:cs.IT cs.CL math.IT  published:2015-10-05 summary:We evaluated the impact of changing the observation scale over the entropy measures for text descriptions. MIDI coded Music, computer code and two human natural languages were studied at the scale of characters, words, and at the Fundamental Scale resulting from adjusting the symbols length used to interpret each text-description until it produced minimum entropy. The results show that the Fundamental Scale method is comparable with the use of words when measuring entropy levels in written texts. However, this method can also be used in communication systems lacking words such as music. Measuring symbolic entropy at the fundamental scale allows to calculate quantitatively, relative levels of complexity for different communication systems. The results open novel vision on differences among the structure of the communication systems studied. version:1
arxiv-1510-01025 | Quadratic Optimization with Orthogonality Constraints: Explicit Lojasiewicz Exponent and Linear Convergence of Line-Search Methods | http://arxiv.org/abs/1510.01025 | id:1510.01025 author:Huikang Liu, Weijie Wu, Anthony Man-Cho So category:math.OC cs.LG cs.NA math.NA 90C26  90C46  90C52  published:2015-10-05 summary:A fundamental class of matrix optimization problems that arise in many areas of science and engineering is that of quadratic optimization with orthogonality constraints. Such problems can be solved using line-search methods on the Stiefel manifold, which are known to converge globally under mild conditions. To determine the convergence rate of these methods, we give an explicit estimate of the exponent in a Lojasiewicz inequality for the (non-convex) set of critical points of the aforementioned class of problems. By combining such an estimate with known arguments, we are able to establish the linear convergence of a large class of line-search methods. A key step in our proof is to establish a local error bound for the set of critical points, which may be of independent interest. version:1
arxiv-1510-01018 | Single Image Dehazing through Improved Atmospheric Light Estimation | http://arxiv.org/abs/1510.01018 | id:1510.01018 author:Huimin Lu, Yujie Li, Shota Nakashima, Seiichi Serikawa category:cs.CV  published:2015-10-05 summary:Image contrast enhancement for outdoor vision is important for smart car auxiliary transport systems. The video frames captured in poor weather conditions are often characterized by poor visibility. Most image dehazing algorithms consider to use a hard threshold assumptions or user input to estimate atmospheric light. However, the brightest pixels sometimes are objects such as car lights or streetlights, especially for smart car auxiliary transport systems. Simply using a hard threshold may cause a wrong estimation. In this paper, we propose a single optimized image dehazing method that estimates atmospheric light efficiently and removes haze through the estimation of a semi-globally adaptive filter. The enhanced images are characterized with little noise and good exposure in dark regions. The textures and edges of the processed images are also enhanced significantly. version:1
arxiv-1510-01003 | Accuracy of Bayesian Latent Variable Estimation with Redundant Dimension | http://arxiv.org/abs/1510.01003 | id:1510.01003 author:Keisuke Yamazaki category:stat.ML  published:2015-10-05 summary:Hierarchical learning models such as mixture models and Bayesian networks are widely employed for unsupervised learning tasks such as clustering analysis. They consist of two variables: observable and hidden variables, which represent the given data and their hidden generation process, respectively. It has been pointed out that the conventional statistical analysis is not applicable to these models because singularities exist in the parameter space. In recent years, a method based on algebraic geometry allows us to analyze accuracy of observable variable prediction on the Bayes estimation. However, analysis for the latent variable has not been studied well though one of the main issues in unsupervised learning is how precisely the latent variable is estimated. A previous study proposed a method for the latent variable when the range of a latent variable has redundancy compared with the model generating data. The present paper extends the method to another redundancy; there are redundant latent variables instead of the variable range. We formulate two types of the error function, and derive the asymptotic forms of both types. Moreover, calculation on the error functions is demonstrated in two-layered Bayesian networks. version:1
arxiv-1408-2923 | Implicit stochastic gradient descent | http://arxiv.org/abs/1408.2923 | id:1408.2923 author:Panos Toulis, Edoardo M. Airoldi category:stat.ME stat.CO stat.ML  published:2014-08-13 summary:Stochastic optimization procedures, such as stochastic gradient descent, have gained popularity for parameter estimation from large data sets. However, standard stochastic optimization procedures cannot effectively combine numerical stability with statistical and computational efficiency. Here, we introduce an implicit stochastic gradient descent procedure, the iterates of which are implicitly defined. Intuitively, implicit iterates shrink the standard iterates. The amount of shrinkage depends on the observed Fisher information matrix, which does not need to be explicitly computed in practice, thus increasing stability without increasing the computational burden. When combined with averaging, the proposed procedure achieves statistical efficiency as well. We derive non-asymptotic bounds and characterize the asymptotic distribution of implicit procedures. Our analysis also reveals the asymptotic variance of a number of existing procedures. We demonstrate implicit stochastic gradient descent by further developing theory for generalized linear models, Cox proportional hazards, and M-estimation problems, and by carrying out extensive experiments. Our results suggest that the implicit stochastic gradient descent procedure is poised to become the workhorse of estimation with large data sets. version:5
arxiv-1508-01521 | Automatic 3D Liver Segmentation Using Sparse Representation of Global and Local Image Information via Level Set Formulation | http://arxiv.org/abs/1508.01521 | id:1508.01521 author:Saif Dawood Salman Al-Shaikhli, Michael Ying Yang, Bodo Rosenhahn category:cs.CV  published:2015-08-06 summary:In this paper, a novel framework for automated liver segmentation via a level set formulation is presented. A sparse representation of both global (region-based) and local (voxel-wise) image information is embedded in a level set formulation to innovate a new cost function. Two dictionaries are build: A region-based feature dictionary and a voxel-wise dictionary. These dictionaries are learned, using the K-SVD method, from a public database of liver segmentation challenge (MICCAI-SLiver07). The learned dictionaries provide prior knowledge to the level set formulation. For the quantitative evaluation, the proposed method is evaluated using the testing data of MICCAI-SLiver07 database. The results are evaluated using different metric scores computed by the challenge organizers. The experimental results demonstrate the superiority of the proposed framework by achieving the highest segmentation accuracy (79.6\%) in comparison to the state-of-the-art methods. version:2
arxiv-1410-3517 | Convex Modeling of Interactions with Strong Heredity | http://arxiv.org/abs/1410.3517 | id:1410.3517 author:Asad Haris, Daniela Witten, Noah Simon category:stat.ML  published:2014-10-13 summary:We consider the task of fitting a regression model involving interactions among a potentially large set of covariates, in which we wish to enforce strong heredity. We propose FAMILY, a very general framework for this task. Our proposal is a generalization of several existing methods, such as VANISH [Radchenko and James, 2010], hierNet [Bien et al., 2013], the all-pairs lasso, and the lasso using only main effects. It can be formulated as the solution to a convex optimization problem, which we solve using an efficient alternating directions method of multipliers (ADMM) algorithm. This algorithm has guaranteed convergence to the global optimum, can be easily specialized to any convex penalty function of interest, and allows for a straightforward extension to the setting of generalized linear models. We derive an unbiased estimator of the degrees of freedom of FAMILY, and explore its performance in a simulation study and on an HIV sequence data set. version:2
arxiv-1504-06375 | Holistically-Nested Edge Detection | http://arxiv.org/abs/1504.06375 | id:1504.06375 author:Saining Xie, Zhuowen Tu category:cs.CV  published:2015-04-24 summary:We develop a new edge detection algorithm that tackles two important issues in this long-standing vision problem: (1) holistic image training and prediction; and (2) multi-scale and multi-level feature learning. Our proposed method, holistically-nested edge detection (HED), performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to approach the human ability resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of .782) and the NYU Depth dataset (ODS F-score of .746), and do so with an improved speed (0.4 second per image) that is orders of magnitude faster than some recent CNN-based edge detection algorithms. version:2
arxiv-1410-0723 | A Lower Bound for the Optimization of Finite Sums | http://arxiv.org/abs/1410.0723 | id:1410.0723 author:Alekh Agarwal, Leon Bottou category:stat.ML math.OC  published:2014-10-02 summary:This paper presents a lower bound for optimizing a finite sum of $n$ functions, where each function is $L$-smooth and the sum is $\mu$-strongly convex. We show that no algorithm can reach an error $\epsilon$ in minimizing all functions from this class in fewer than $\Omega(n + \sqrt{n(\kappa-1)}\log(1/\epsilon))$ iterations, where $\kappa=L/\mu$ is a surrogate condition number. We then compare this lower bound to upper bounds for recently developed methods specializing to this setting. When the functions involved in this sum are not arbitrary, but based on i.i.d. random data, then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum. The conclusion we draw is that a lot of caution is necessary for an accurate comparison, and identify machine learning scenarios where the new methods help computationally. version:4
arxiv-1510-00889 | Background Image Generation Using Boolean Operations | http://arxiv.org/abs/1510.00889 | id:1510.00889 author:Kardi Teknomo, Proceso Fernandez category:cs.CV I.4.6  published:2015-10-04 summary:Tracking moving objects from a video sequence requires segmentation of these objects from the background image. However, getting the actual background image automatically without object detection and using only the video is difficult. In this paper, we describe a novel algorithm that generates background from real world images without foreground detection. The algorithm assumes that the background image is shown in the majority of the video. Given this simple assumption, the method described in this paper is able to accurately generate, with high probability, the background image from a video using only a small number of binary operations. version:1
arxiv-1510-00857 | Approximate Fisher Kernels of non-iid Image Models for Image Categorization | http://arxiv.org/abs/1510.00857 | id:1510.00857 author:Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid category:cs.CV cs.LG  published:2015-10-03 summary:The bag-of-words (BoW) model treats images as sets of local descriptors and represents them by visual word histograms. The Fisher vector (FV) representation extends BoW, by considering the first and second order statistics of local descriptors. In both representations local descriptors are assumed to be identically and independently distributed (iid), which is a poor assumption from a modeling perspective. It has been experimentally observed that the performance of BoW and FV representations can be improved by employing discounting transformations such as power normalization. In this paper, we introduce non-iid models by treating the model parameters as latent variables which are integrated out, rendering all local regions dependent. Using the Fisher kernel principle we encode an image by the gradient of the data log-likelihood w.r.t. the model hyper-parameters. Our models naturally generate discounting effects in the representations; suggesting that such transformations have proven successful because they closely correspond to the representations obtained for non-iid models. To enable tractable computation, we rely on variational free-energy bounds to learn the hyper-parameters and to compute approximate Fisher kernels. Our experimental evaluation results validate that our models lead to performance improvements comparable to using power normalization, as employed in state-of-the-art feature aggregation methods. version:1
arxiv-1504-07786 | Projected Iterative Soft-thresholding Algorithm for Tight Frames in Compressed Sensing Magnetic Resonance Imaging | http://arxiv.org/abs/1504.07786 | id:1504.07786 author:Yunsong Liu, Zhifang Zhan, Jian-Feng Cai, Di Guo, Zhong Chen, Xiaobo Qu category:physics.med-ph cs.CV math.OC  published:2015-04-29 summary:Compressed sensing has shown great potentials in accelerating magnetic resonance imaging. Fast image reconstruction and high image quality are two main issues faced by this new technology. It has been shown that, redundant image representations, e.g. tight frames, can significantly improve the image quality. But how to efficiently solve the reconstruction problem with these redundant representation systems is still challenging. This paper attempts to address the problem of applying iterative soft-thresholding algorithm (ISTA) to tight frames based magnetic resonance image reconstruction. By introducing the canonical dual frame to construct the orthogonal projection operator on the range of the analysis sparsity operator, we propose a projected iterative soft-thresholding algorithm (pISTA) and further accelerate it by incorporating the strategy proposed by Beck and Teboulle in 2009. We theoretically prove that pISTA converges to the minimum of a function with a balanced tight frame sparsity. Experimental results demonstrate that the proposed algorithm achieves better reconstruction than the widely used synthesis sparse model and the accelerated pISTA converges faster or comparable to the state-of-art smoothing FISTA. One major advantage of pISTA is that only one extra parameter, the step size, is introduced and the numerical solution is stable to it in terms of image reconstruction errors, thus allowing easily setting in many fast magnetic resonance imaging applications. version:2
arxiv-1510-00817 | Distributed Parameter Map-Reduce | http://arxiv.org/abs/1510.00817 | id:1510.00817 author:Qi Li category:cs.DC cs.LG stat.ML  published:2015-10-03 summary:This paper describes how to convert a machine learning problem into a series of map-reduce tasks. We study logistic regression algorithm. In logistic regression algorithm, it is assumed that samples are independent and each sample is assigned a probability. Parameters are obtained by maxmizing the product of all sample probabilities. Rapid expansion of training samples brings challenges to machine learning method. Training samples are so many that they can be only stored in distributed file system and driven by map-reduce style programs. The main step of logistic regression is inference. According to map-reduce spirit, each sample makes inference through a separate map procedure. But the premise of inference is that the map procedure holds parameters for all features in the sample. In this paper, we propose Distributed Parameter Map-Reduce, in which not only samples, but also parameters are distributed in nodes of distributed filesystem. Through a series of map-reduce tasks, we assign each sample parameters for its features, make inference for the sample and update paramters of the model. The above processes are excuted looply until convergence. We test the proposed algorithm in actual hadoop production environment. Experiments show that the acceleration of the algorithm is in linear relationship with the number of cluster nodes. version:1
arxiv-1309-0787 | Online Tensor Methods for Learning Latent Variable Models | http://arxiv.org/abs/1309.0787 | id:1309.0787 author:Furong Huang, U. N. Niranjan, Mohammad Umar Hakeem, Animashree Anandkumar category:cs.LG cs.DC cs.SI stat.ML  published:2013-09-03 summary:We introduce an online tensor decomposition based approach for two latent variable modeling problems namely, (1) community detection, in which we learn the latent communities that the social actors in social networks belong to, and (2) topic modeling, in which we infer hidden topics of text articles. We consider decomposition of moment tensors using stochastic gradient descent. We conduct optimization of multilinear operations in SGD and avoid directly forming the tensors, to save computational and storage costs. We present optimized algorithm in two platforms. Our GPU-based implementation exploits the parallelism of SIMD architectures to allow for maximum speed-up by a careful optimization of storage and data transfer, whereas our CPU-based implementation uses efficient sparse matrix computations and is suitable for large sparse datasets. For the community detection problem, we demonstrate accuracy and computational efficiency on Facebook, Yelp and DBLP datasets, and for the topic modeling problem, we also demonstrate good performance on the New York Times dataset. We compare our results to the state-of-the-art algorithms such as the variational method, and report a gain of accuracy and a gain of several orders of magnitude in the execution time. version:5
arxiv-1505-03772 | Achieving Optimal Misclassification Proportion in Stochastic Block Model | http://arxiv.org/abs/1505.03772 | id:1505.03772 author:Chao Gao, Zongming Ma, Anderson Y. Zhang, Harrison H. Zhou category:math.ST cs.SI stat.ME stat.ML stat.TH  published:2015-05-14 summary:Community detection is a fundamental statistical problem in network data analysis. Many algorithms have been proposed to tackle this problem. Most of these algorithms are not guaranteed to achieve the statistical optimality of the problem, while procedures that achieve information theoretic limits for general parameter spaces are not computationally tractable. In this paper, we present a computationally feasible two-stage method that achieves optimal statistical performance in misclassification proportion for stochastic block model under weak regularity conditions. Our two-stage procedure consists of a generic refinement step that can take a wide range of weakly consistent community detection procedures as initializer, to which the refinement stage applies and outputs a community assignment achieving optimal misclassification proportion with high probability. The practical effectiveness of the new algorithm is demonstrated by competitive numerical results. version:5
