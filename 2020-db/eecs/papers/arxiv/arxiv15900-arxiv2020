arxiv-1603-08037 | On the Detection of Mixture Distributions with applications to the Most Biased Coin Problem | http://arxiv.org/abs/1603.08037 | id:1603.08037 author:Kevin Jamieson, Daniel Haas, Ben Recht category:cs.LG  published:2016-03-25 summary:This paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. The most biased coin problem asks how many total coin flips are required to identify a "heavy" coin from an infinite bag containing both "heavy" coins with mean $\theta_1 \in (0,1)$, and "light" coins with mean $\theta_0 \in (0,\theta_1)$, where heavy coins are drawn from the bag with probability $\alpha \in (0,1/2)$. The key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. This problem has applications in crowdsourcing, anomaly detection, and radio spectrum search. Chandrasekaran et. al. (2014) recently introduced a solution to this problem but it required perfect knowledge of $\theta_0,\theta_1,\alpha$. In contrast, we derive algorithms that are adaptive to partial or absent knowledge of the problem parameters. Moreover, our techniques generalize beyond coins to more general instances of infinitely many armed bandit problems. We also prove lower bounds that show our algorithm's upper bounds are tight up to $\log$ factors, and on the way characterize the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions. As a result, these bounds have surprising implications both for solutions to the most biased coin problem and for anomaly detection when only partial information about the parameters is known. version:1
arxiv-1603-08035 | Universality of Mallows' and degeneracy of Kendall's kernels for rankings | http://arxiv.org/abs/1603.08035 | id:1603.08035 author:Horia Mania, Aaditya Ramdas, Martin J. Wainwright, Michael I. Jordan, Benjamin Recht category:stat.ML cs.DM cs.LG  published:2016-03-25 summary:Kernel methods provide an attractive framework for aggregating and learning from ranking data, and so understanding the fundamental properties of kernels over permutations is a question of broad interest. We provide a detailed analysis of the Fourier spectra of the standard Kendall and Mallows kernels, and a new class of polynomial-type kernels. We prove that the Kendall kernel has exactly two irreducible representations at which the Fourier transform is non-zero, and moreover, the associated matrices are rank one. This implies that the Kendall kernel is nearly degenerate, with limited expressive and discriminative power. In sharp contrast, we prove that the Fourier transform of the Mallows kernel is a strictly positive definite matrix at all irreducible representations. This property guarantees that the Mallows kernel is both characteristic and universal. We introduce a family of normalized polynomial kernels of degree p that interpolates between the Kendall (degree one) and Mallows (infinite degree) kernels, and show that for d-dimensional permutations, the p-th degree kernel is characteristic when p is greater or equal than d - 1, unlike the Euclidean case in which no finite-degree polynomial kernel is characteristic. version:1
arxiv-1603-08029 | Resnet in Resnet: Generalizing Residual Architectures | http://arxiv.org/abs/1603.08029 | id:1603.08029 author:Sasha Targ, Diogo Almeida, Kevin Lyman category:cs.LG cs.CV cs.NE stat.ML  published:2016-03-25 summary:Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100. version:1
arxiv-1603-08028 | On the Simultaneous Preservation of Privacy and Community Structure in Anonymized Networks | http://arxiv.org/abs/1603.08028 | id:1603.08028 author:Daniel Cullina, Kushagra Singhal, Negar Kiyavash, Prateek Mittal category:cs.LG cs.CR cs.SI  published:2016-03-25 summary:We consider the problem of performing community detection on a network, while maintaining privacy, assuming that the adversary has access to an auxiliary correlated network. We ask the question "Does there exist a regime where the network cannot be deanonymized perfectly, yet the community structure could be learned?." To answer this question, we derive information theoretic converses for the perfect deanonymization problem using the Stochastic Block Model and edge sub-sampling. We also provide an almost tight achievability result for perfect deanonymization. We also evaluate the performance of percolation based deanonymization algorithm on Stochastic Block Model data-sets that satisfy the conditions of our converse. Although our converse applies to exact deanonymization, the algorithm fails drastically when the conditions of the converse are met. Additionally, we study the effect of edge sub-sampling on the community structure of a real world dataset. Results show that the dataset falls under the purview of the idea of this paper. There results suggest that it may be possible to prove stronger partial deanonymizability converses, which would enable better privacy guarantees. version:1
arxiv-1603-08023 | How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation | http://arxiv.org/abs/1603.08023 | id:1603.08023 author:Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin, Joelle Pineau category:cs.CL cs.AI cs.LG cs.NE  published:2016-03-25 summary:We investigate evaluation metrics for end-to-end dialogue systems where supervised labels, such as task completion, are not available. Recent works in end-to-end dialogue systems have adopted metrics from machine translation and text summarization to compare a model's generated response to a single target response. We show that these metrics correlate very weakly or not at all with human judgements of the response quality in both technical and non-technical domains. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems. version:1
arxiv-1507-06970 | Perturbed Iterate Analysis for Asynchronous Stochastic Optimization | http://arxiv.org/abs/1507.06970 | id:1507.06970 author:Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, Michael I. Jordan category:stat.ML cs.DC cs.DS cs.LG math.OC  published:2015-07-24 summary:We introduce and analyze stochastic optimization methods where the input to each gradient update is perturbed by bounded noise. We show that this framework forms the basis of a unified approach to analyze asynchronous implementations of stochastic optimization algorithms.In this framework, asynchronous stochastic optimization algorithms can be thought of as serial methods operating on noisy inputs. Using our perturbed iterate framework, we provide new analyses of the Hogwild! algorithm and asynchronous stochastic coordinate descent, that are simpler than earlier analyses, remove many assumptions of previous models, and in some cases yield improved upper bounds on the convergence rates. We proceed to apply our framework to develop and analyze KroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient (SVRG) algorithm. We demonstrate experimentally on a 16-core machine that the sparse and parallel version of SVRG is in some cases more than four orders of magnitude faster than the standard SVRG algorithm. version:2
arxiv-1603-07998 | Friction from Reflectance: Deep Reflectance Codes for Predicting Physical Surface Properties from One-Shot In-Field Reflectance | http://arxiv.org/abs/1603.07998 | id:1603.07998 author:Hang Zhang, Kristin Dana, Ko Nishino category:cs.CV  published:2016-03-25 summary:Images are the standard input for vision algorithms, but one-shot infield reflectance measurements are creating new opportunities for recognition and scene understanding. In this work, we address the question of what reflectance can reveal about materials in an efficient manner. We go beyond the question of recognition and labeling and ask the question: What intrinsic physical properties of the surface can be estimated using reflectance? We introduce a framework that enables prediction of actual friction values for surfaces using one-shot reflectance measurements. This work is a first of its kind vision-based friction estimation. We develop a novel representation for reflectance disks that capture partial BRDF measurements instantaneously. Our method of deep reflectance codes combines CNN features and fisher vector pooling with optimal binary embedding to create codes that have sufficient discriminatory power and have important properties of illumination and spatial invariance. The experimental results demonstrate that reflectance can play a new role in deciphering the underlying physical properties of real-world scenes. version:1
arxiv-1603-01292 | Modular Decomposition and Analysis of Registration based Trackers | http://arxiv.org/abs/1603.01292 | id:1603.01292 author:Abhineet Singh, Ankush Roy, Xi Zhang, Martin Jagersand category:cs.CV  published:2016-03-03 summary:This paper presents a new way to study registration based trackers by decomposing them into three constituent sub modules: appearance model, state space model and search method. It is often the case that when a new tracker is introduced in literature, it only contributes to one or two of these sub modules while using existing methods for the rest. Since these are often selected arbitrarily by the authors, they may not be optimal for the new method. In such cases, our breakdown can help to experimentally find the best combination of methods for these sub modules while also providing a framework within which the contributions of the new tracker can be clearly demarcated and thus studied better. We show how existing trackers can be broken down using the suggested methodology and compare the performance of the default configuration chosen by the authors against other possible combinations to demonstrate the new insights that can be gained by such an approach. We also present an open source system that provides a convenient interface to plug in a new method for any sub module and test it against all possible combinations of methods for the other two sub modules while also serving as a fast and efficient solution for practical tracking requirements. version:2
arxiv-1603-07980 | Developing Quantum Annealer Driven Data Discovery | http://arxiv.org/abs/1603.07980 | id:1603.07980 author:Joseph Dulny III, Michael Kim category:quant-ph cs.LG  published:2016-03-25 summary:Machine learning applications are limited by computational power. In this paper, we gain novel insights into the application of quantum annealing (QA) to machine learning (ML) through experiments in natural language processing (NLP), seizure prediction, and linear separability testing. These experiments are performed on QA simulators and early-stage commercial QA hardware and compared to an unprecedented number of traditional ML techniques. We extend QBoost, an early implementation of a binary classifier that utilizes a quantum annealer, via resampling and ensembling of predicted probabilities to produce a more robust class estimator. To determine the strengths and weaknesses of this approach, resampled QBoost (RQBoost) is tested across several datasets and compared to QBoost and traditional ML. We show and explain how QBoost in combination with a commercial QA device are unable to perfectly separate binary class data which is linearly separable via logistic regression with shrinkage. We further explore the performance of RQBoost in the space of NLP and seizure prediction and find QA-enabled ML using QBoost and RQBoost is outperformed by traditional techniques. Additionally, we provide a detailed discussion of algorithmic constraints and trade-offs imposed by the use of this QA hardware. Through these experiments, we provide unique insights into the state of quantum ML via boosting and the use of quantum annealing hardware that are valuable to institutions interested in applying QA to problems in ML and beyond. version:1
arxiv-1601-05285 | Nonlinear variable selection with continuous outcome: a nonparametric incremental forward stagewise approach | http://arxiv.org/abs/1601.05285 | id:1601.05285 author:Tianwei Yu category:stat.ML  published:2016-01-20 summary:We present a method of variable selection for the situation where some predictors are nonlinearly associated with a continuous outcome variable. The method doesn't assume any specific functional form, and can select from a large number of candidates. It takes the form of incremental forward stagewise regression, in which very small steps are taken to select the variables. Given no functional form is assumed, we devised an approach termed roughening to adjust the residuals in the iterations. In simulations, we show the new method is competitive against popular machine learning approaches. We also demonstrate its performance using some real datasets. version:2
arxiv-1602-09130 | Modular Tracking Framework: A Unified Approach to Registration based Tracking | http://arxiv.org/abs/1602.09130 | id:1602.09130 author:Abhineet Singh, Martin Jagersand category:cs.CV cs.RO  published:2016-02-29 summary:This paper presents a modular, extensible and highly efficient open source framework for registration based tracking. It is implemented entirely in C++ and is designed from the ground up to easily integrate with systems that support any of several major vision and robotics libraries including OpenCV, ROS and Eigen. To establish the theoretical basis for the design of this system, we introduce a new way to study registration based trackers by decomposing them into three constituent sub modules while also extending the unifying formulation described in \cite{Baker04lucasKanade_paper} to account for several important advances in the field since its publication. In addition to being a practical solution for fast and high precision tracking, this system can also serve as a useful research tool by allowing existing and new methods for any of the aforementioned sub modules to be studied better. When a new method for one of these sub modules is introduced in literature, this breakdown can help to experimentally find the combination of methods for the other sub modules that is optimum for it while also allowing more comprehensive comparisons with existing methods to understand its contributions better. By extensive use of generic programming, the system makes it easy to plug in a new method for any of the sub modules so that it can not only be tested with existing methods for other sub modules but also become immediately available for deployment in any system that uses the framework. version:2
arxiv-1603-07965 | Unsupervised Category Discovery via Looped Deep Pseudo-Task Optimization Using a Large Scale Radiology Image Database | http://arxiv.org/abs/1603.07965 | id:1603.07965 author:Xiaosong Wang, Le Lu, Hoo-chang Shin, Lauren Kim, Isabella Nogues, Jianhua Yao, Ronald Summers category:cs.CV  published:2016-03-25 summary:Obtaining semantic labels on a large scale radiology image database (215,786 key images from 61,845 unique patients) is a prerequisite yet bottleneck to train highly effective deep convolutional neural network (CNN) models for image recognition. Nevertheless, conventional methods for collecting image labels (e.g., Google search followed by crowd-sourcing) are not applicable due to the formidable difficulties of medical annotation tasks for those who are not clinically trained. This type of image labeling task remains non-trivial even for radiologists due to uncertainty and possible drastic inter-observer variation or inconsistency. In this paper, we present a looped deep pseudo-task optimization procedure for automatic category discovery of visually coherent and clinically semantic (concept) clusters. Our system can be initialized by domain-specific (CNN trained on radiology images and text report derived labels) or generic (ImageNet based) CNN models. Afterwards, a sequence of pseudo-tasks are exploited by the looped deep image feature clustering (to refine image labels) and deep CNN training/classification using new labels (to obtain more task representative deep features). Our method is conceptually simple and based on the hypothesized "convergence" of better labels leading to better trained CNN models which in turn feed more effective deep image features to facilitate more meaningful clustering/labels. We have empirically validated the convergence and demonstrated promising quantitative and qualitative results. Category labels of significantly higher quality than those in previous work are discovered. This allows for further investigation of the hierarchical semantic nature of the given large-scale radiology image database. version:1
arxiv-1602-03534 | Unsupervised Transductive Domain Adaptation | http://arxiv.org/abs/1602.03534 | id:1602.03534 author:Ozan Sener, Hyun Oh Song, Ashutosh Saxena, Silvio Savarese category:stat.ML cs.LG  published:2016-02-10 summary:Supervised learning with large scale labeled datasets and deep layered models has made a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers generalization issues under the presence of a domain shift between the training and the test data distribution. In this regard, unsupervised domain adaptation algorithms have been proposed to directly address the domain shift problem. In this paper, we approach the problem from a transductive perspective. We incorporate the domain shift and the transductive target inference into our framework by jointly solving for an asymmetric similarity metric and the optimal transductive target label assignment. We also show that our model can easily be extended for deep feature learning in order to learn features which are discriminative in the target domain. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin. version:3
arxiv-1603-07957 | Object Recognition Based on Amounts of Unlabeled Data | http://arxiv.org/abs/1603.07957 | id:1603.07957 author:Fuqiang Liu, Fukun Bi, Liang Chen category:cs.CV  published:2016-03-25 summary:This paper proposes a novel semi-supervised method on object recognition. First, based on Boost Picking, a universal algorithm, Boost Picking Teaching (BPT), is proposed to train an effective binary-classifier just using a few labeled data and amounts of unlabeled data. Then, an ensemble strategy is detailed to synthesize multiple BPT-trained binary-classifiers to be a high-performance multi-classifier. The rationality of the strategy is also analyzed in theory. Finally, the proposed method is tested on two databases, CIFAR-10 and CIFAR-100. Using 2% labeled data and 98% unlabeled data, the accuracies of the proposed method on the two data sets are 78.39% and 50.77% respectively. version:1
arxiv-1309-7857 | Generalized system identification with stable spline kernels | http://arxiv.org/abs/1309.7857 | id:1309.7857 author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:stat.ML math.OC 62F35  65K10  published:2013-09-30 summary:Regularized least-squares approaches have been successfully applied to linear system identification. Recent approaches use quadratic penalty terms on the unknown impulse response defined by stable spline kernels, which control model space complexity by leveraging regularity and bounded-input bounded-output stability. This paper extends linear system identification to a wide class of nonsmooth stable spline estimators, where regularization functionals and data misfits can be selected from a rich set of piecewise linear quadratic penalties. This class encompasses the 1-norm, huber, and vapnik, in addition to the least-squares penalty, and the approach allows linear inequality constraints on the unknown impulse response. We develop a customized interior point solver for the entire class of proposed formulations. By representing penalties through their conjugates, we allow a simple interface that enables the user to specify any piecewise linear quadratic penalty for misfit and regularizer, together with inequality constraints on the response. The solver is locally quadratically convergent, with O(n2(m+n)) arithmetic operations per iteration, for n impulse response coefficients and m output measurements. In the system identification context, where n << m, IPsolve is competitive with available alternatives, illustrated by a comparison with TFOCS and libSVM. The modeling framework is illustrated with a range of numerical experiments, featuring robust formulations for contaminated data, relaxation systems, and nonnegativity and unimodality constraints on the impulse response. Incorporating constraints yields significant improvements in system identification. The solver used to obtain the results is distributed via an open source code repository. version:3
arxiv-1602-00417 | Transfer Learning Based on AdaBoost for Feature Selection from Multiple ConvNet Layer Features | http://arxiv.org/abs/1602.00417 | id:1602.00417 author:Jumabek Alikhanov, Myeong Hyeon Ga, Seunghyun Ko, Geun-Sik Jo category:cs.CV  published:2016-02-01 summary:Convolutional Networks (ConvNets) are powerful models that learn hierarchies of visual features, which could also be used to obtain image representations for transfer learning. The basic pipeline for transfer learning is to first train a ConvNet on a large dataset (source task) and then use feed-forward units activation of the trained ConvNet as image representation for smaller datasets (target task). Our key contribution is to demonstrate superior performance of multiple ConvNet layer features over single ConvNet layer features. Combining multiple ConvNet layer features will result in more complex feature space with some features being repetitive. This requires some form of feature selection. We use AdaBoost with single stumps to implicitly select only distinct features that are useful towards classification from concatenated ConvNet features. Experimental results show that using multiple ConvNet layer activation features instead of single ConvNet layer features consistently will produce superior performance. Improvements becomes significant as we increase the distance between source task and the target task. version:2
arxiv-1603-07886 | A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic Extraction of Semantics, Formation of Integrated Concepts and Re-selection Features for Ambiguity | http://arxiv.org/abs/1603.07886 | id:1603.07886 author:Peijie Yin, Hong Qiao, Wei Wu, Lu Qi, YinLin Li, Shanlin Zhong, Bo Zhang category:cs.CV cs.AI cs.LG  published:2016-03-25 summary:Integration between biology and information science benefits both fields. Many related models have been proposed, such as computational visual cognition models, computational motor control models, integrations of both and so on. In general, the robustness and precision of recognition is one of the key problems for object recognition models. In this paper, inspired by features of human recognition process and their biological mechanisms, a new integrated and dynamic framework is proposed to mimic the semantic extraction, concept formation and feature re-selection in human visual processing. The main contributions of the proposed model are as follows: (1) Semantic feature extraction: Local semantic features are learnt from episodic features that are extracted from raw images through a deep neural network; (2) Integrated concept formation: Concepts are formed with local semantic information and structural information learnt through network. (3) Feature re-selection: When ambiguity is detected during recognition process, distinctive features according to the difference between ambiguous candidates are re-selected for recognition. Experimental results on hand-written digits and facial shape dataset show that, compared with other methods, the new proposed model exhibits higher robustness and precision for visual recognition, especially in the condition when input samples are smantic ambiguous. Meanwhile, the introduced biological mechanisms further strengthen the interaction between neuroscience and information science. version:1
arxiv-1603-07879 | Hybridization of Expectation-Maximization and K-Means Algorithms for Better Clustering Performance | http://arxiv.org/abs/1603.07879 | id:1603.07879 author:D. Raja Kishor, N. B. Venkateswarlu category:cs.LG stat.ML  published:2016-03-25 summary:The present work proposes hybridization of Expectation-Maximization (EM) and K-Means techniques as an attempt to speed-up the clustering process. Though both K-Means and EM techniques look into different areas, K-means can be viewed as an approximate way to obtain maximum likelihood estimates for the means. Along with the proposed algorithm for hybridization, the present work also experiments with the Standard EM algorithm. Six different datasets are used for the experiments of which three are synthetic datasets. Clustering fitness and Sum of Squared Errors (SSE) are computed for measuring the clustering performance. In all the experiments it is observed that the proposed algorithm for hybridization of EM and K-Means techniques is consistently taking less execution time with acceptable Clustering Fitness value and less SSE than the standard EM algorithm. It is also observed that the proposed algorithm is producing better clustering results than the Cluster package of Purdue University. version:1
arxiv-1505-06279 | The Benefit of Multitask Representation Learning | http://arxiv.org/abs/1505.06279 | id:1505.06279 author:Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes category:stat.ML cs.LG  published:2015-05-23 summary:We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks. version:2
arxiv-1603-07871 | Exact Bayesian inference for off-line change-point detection in tree-structured graphical models | http://arxiv.org/abs/1603.07871 | id:1603.07871 author:Loïc Schwaller, Stéphane Robin category:stat.ML  published:2016-03-25 summary:We consider the problem of change-point detection in multivariate time-series. The multivariate distribution of the observations is supposed to follow a graphical model, whose graph and parameters are affected by abrupt changes throughout time. We demonstrate that it is possible to perform exact Bayesian inference whenever one considers a simple class of undirected graphs called spanning trees as possible structures. We are then able to integrate on the graph and segmentation spaces at the same time by combining classical dynamic programming with algebraic results pertaining to spanning trees. In particular, we show that quantities such as posterior distributions for change-points or posterior edge probabilities over time can efficiently be obtained. We illustrate our results on both synthetic and experimental data arising from biology and neuroscience. version:1
arxiv-1603-07866 | The Asymptotic Performance of Linear Echo State Neural Networks | http://arxiv.org/abs/1603.07866 | id:1603.07866 author:Romain Couillet, Gilles Wainrib, Harry Sevi, Hafiz Tiomoko Ali category:cs.LG cs.NE math.PR  published:2016-03-25 summary:In this article, a study of the mean-square error (MSE) performance of linear echo-state neural networks is performed, both for training and testing tasks. Considering the realistic setting of noise present at the network nodes, we derive deterministic equivalents for the aforementioned MSE in the limit where the number of input data $T$ and network size $n$ both grow large. Specializing then the network connectivity matrix to specific random settings, we further obtain simple formulas that provide new insights on the performance of such networks. version:1
arxiv-1603-07850 | Markov substitute processes : a new model for linguistics and beyond | http://arxiv.org/abs/1603.07850 | id:1603.07850 author:Olivier Catoni, Thomas Mainguy category:stat.ML math.ST stat.TH  published:2016-03-25 summary:We introduce Markov substitute processes, a new model at the crossroad of statistics and formal grammars, and prove its main property : Markov substitute processes with a given support form an exponential family. version:1
arxiv-1604-08608 | A movie genre prediction based on Multivariate Bernoulli model and genre correlations | http://arxiv.org/abs/1604.08608 | id:1604.08608 author:Eric Makita, Artem Lenskiy category:cs.IR cs.LG  published:2016-03-25 summary:Movie ratings play an important role both in determining the likelihood of a potential viewer to watch the movie and in reflecting the current viewer satisfaction with the movie. They are available in several sources like the television guide, best-selling reference books, newspaper columns, and television programs. Furthermore, movie ratings are crucial for recommendation engines that track the behavior of all users and utilize the information to suggest items they might like. Movie ratings in most cases, thus, provide information that might be more important than movie feature-based data. It is intuitively appealing that information about the viewing preferences in movie genres is sufficient for predicting a genre of an unlabeled movie. In order to predict movie genres, we treat ratings as a feature vector, apply the Bernoulli event model to estimate the likelihood of a movies given genre, and evaluate the posterior probability of the genre of a given movie using the Bayes rule. The goal of the proposed technique is to efficiently use the movie ratings for the task of predicting movie genres. In our approach we attempted to answer the question: "Given the set of users who watched a movie, is it possible to predict the genre of a movie based on its ratings?" Our simulation results with MovieLens 100k data demonstrated the efficiency and accuracy of our proposed technique, achieving 59% prediction rate for exact prediction and 69% when including correlated genres. version:1
arxiv-1603-07849 | A multinomial probabilistic model for movie genre predictions | http://arxiv.org/abs/1603.07849 | id:1603.07849 author:Eric Makita, Artem Lenskiy category:cs.IR cs.LG  published:2016-03-25 summary:This paper proposes a movie genre-prediction based on multinomial probability model. To the best of our knowledge, this problem has not been addressed yet in the field of recommender system. The prediction of a movie genre has many practical applications including complementing the items categories given by experts and providing a surprise effect in the recommendations given to a user. We employ mulitnomial event model to estimate a likelihood of a movie given genre and the Bayes rule to evaluate the posterior probability of a genre given a movie. Experiments with the MovieLens dataset validate our approach. We achieved 70% prediction rate using only 15% of the whole set for training. version:1
arxiv-1603-07846 | Deep Learning At Scale and At Ease | http://arxiv.org/abs/1603.07846 | id:1603.07846 author:Wei Wang, Gang Chen, Haibo Chen, Tien Tuan Anh Dinh, Jinyang Gao, Beng Chin Ooi, Kian-Lee Tan, Sheng Wang category:cs.LG cs.DC  published:2016-03-25 summary:Recently, deep learning techniques have enjoyed success in various multimedia applications, such as image classification and multi-modal data analysis. Large deep learning models are developed for learning rich representations of complex data. There are two challenges to overcome before deep learning can be widely adopted in multimedia and other applications. One is usability, namely the implementation of different models and training algorithms must be done by non-experts without much effort especially when the model is large and complex. The other is scalability, that is the deep learning system must be able to provision for a huge demand of computing resources for training large models with massive datasets. To address these two challenges, in this paper, we design a distributed deep learning platform called SINGA which has an intuitive programming model based on the common layer abstraction of deep learning models. Good scalability is achieved through flexible distributed training architecture and specific optimization techniques. SINGA runs on GPUs as well as on CPUs, and we show that it outperforms many other state-of-the-art deep learning systems. Our experience with developing and training deep learning models for real-life multimedia applications in SINGA shows that the platform is both usable and scalable. version:1
arxiv-1601-05911 | Orthogonal Echo State Networks and stochastic evaluations of likelihoods | http://arxiv.org/abs/1601.05911 | id:1601.05911 author:Norbert Michael Mayer, Ying-Hao Yu category:cs.NE  published:2016-01-22 summary:In this paper we report the likelihood estimates that are performed on time series using a echo state network with orthogonal recurrent connectivity. The results indicate that the optimal performance depends on the way of balancing the input strength with the recurrent activity, which also has an influence on the network with regard to the quality of the short term prediction versus prediction that accounts for influences that date back a long time in the input history. Finally, sensitivity of such networks against noise/finite accuracy of network states in the recurrent layer is investigated. In addition, a measure that bases on mutual information is introduced in order to best quantify the performance of the network with the time series. version:2
arxiv-1603-07839 | Early Detection of Combustion Instabilities using Deep Convolutional Selective Autoencoders on Hi-speed Flame Video | http://arxiv.org/abs/1603.07839 | id:1603.07839 author:Adedotun Akintayo, Kin Gwn Lore, Soumalya Sarkar, Soumik Sarkar category:cs.CV cs.LG cs.NE  published:2016-03-25 summary:This paper proposes an end-to-end convolutional selective autoencoder approach for early detection of combustion instabilities using rapidly arriving flame image frames. The instabilities arising in combustion processes cause significant deterioration and safety issues in various human-engineered systems such as land and air based gas turbine engines. These properties are described as self-sustaining, large amplitude pressure oscillations and show varying spatial scales periodic coherent vortex structure shedding. However, such instability is extremely difficult to detect before a combustion process becomes completely unstable due to its sudden (bifurcation-type) nature. In this context, an autoencoder is trained to selectively mask stable flame and allow unstable flame image frames. In that process, the model learns to identify and extract rich descriptive and explanatory flame shape features. With such a training scheme, the selective autoencoder is shown to be able to detect subtle instability features as a combustion process makes transition from stable to unstable region. As a consequence, the deep learning tool-chain can perform as an early detection framework for combustion instabilities that will have a transformative impact on the safety and performance of modern engines. version:1
arxiv-1603-07834 | An end-to-end convolutional selective autoencoder approach to Soybean Cyst Nematode eggs detection | http://arxiv.org/abs/1603.07834 | id:1603.07834 author:Adedotun Akintayo, Nigel Lee, Vikas Chawla, Mark Mullaney, Christopher Marett, Asheesh Singh, Arti Singh, Greg Tylka, Baskar Ganapathysubramaniam, Soumik Sarkar category:cs.CV cs.LG stat.ML  published:2016-03-25 summary:This paper proposes a novel selective autoencoder approach within the framework of deep convolutional networks. The crux of the idea is to train a deep convolutional autoencoder to suppress undesired parts of an image frame while allowing the desired parts resulting in efficient object detection. The efficacy of the framework is demonstrated on a critical plant science problem. In the United States, approximately $1 billion is lost per annum due to a nematode infection on soybean plants. Currently, plant-pathologists rely on labor-intensive and time-consuming identification of Soybean Cyst Nematode (SCN) eggs in soil samples via manual microscopy. The proposed framework attempts to significantly expedite the process by using a series of manually labeled microscopic images for training followed by automated high-throughput egg detection. The problem is particularly difficult due to the presence of a large population of non-egg particles (disturbances) in the image frames that are very similar to SCN eggs in shape, pose and illumination. Therefore, the selective autoencoder is trained to learn unique features related to the invariant shapes and sizes of the SCN eggs without handcrafting. After that, a composite non-maximum suppression and differencing is applied at the post-processing stage. version:1
arxiv-1603-07828 | Privacy-Preserved Big Data Analysis Based on Asymmetric Imputation Kernels | http://arxiv.org/abs/1603.07828 | id:1603.07828 author:Bo-Wei Chen category:cs.LG cs.CR  published:2016-03-25 summary:This study presents an efficient approach for incomplete data classification, where the entries of samples are missing or masked due to privacy preservation. To deal with these incomplete data, a new kernel function with asymmetric intrinsic mappings is proposed in this study. Such a new kernel uses three-side similarities for kernel matrix formation. The similarity between a test instance and a training sample relies on not only their distance but also the relation between this test sample and the centroid of the class, where the training sample belongs. This reduces biased estimation compared with typical methods when only one training sample is used for kernel matrix formation. Furthermore, the proposed kernel is capable of performing data imputation by using class-dependent averages. This enhances Fisher Discriminant Ratios and data discriminability. Experiments on two databases were carried out for evaluating the proposed method. The result indicated that the accuracy of the proposed method was higher than that of the baseline. These findings thereby demonstrate the effectiveness of the proposed idea. version:1
arxiv-1603-07823 | Training-Free Synthesized Face Sketch Recognition Using Image Quality Assessment Metrics | http://arxiv.org/abs/1603.07823 | id:1603.07823 author:Nannan Wang, Jie Li, Leiyu Sun, Bin Song, Xinbo Gao category:cs.CV  published:2016-03-25 summary:Face sketch synthesis has wide applications ranging from digital entertainments to law enforcements. Objective image quality assessment scores and face recognition accuracy are two mainly used tools to evaluate the synthesis performance. In this paper, we proposed a synthesized face sketch recognition framework based on full-reference image quality assessment metrics. Synthesized sketches generated from four state-of-the-art methods are utilized to test the performance of the proposed recognition framework. For the image quality assessment metrics, we employed the classical structured similarity index metric and other three prevalent metrics: visual information fidelity, feature similarity index metric and gradient magnitude similarity deviation. Extensive experiments compared with baseline methods illustrate the effectiveness of the proposed synthesized face sketch recognition framework. Data and implementation code in this paper are available online at www.ihitworld.com/WNN/IQA_Sketch.zip. version:1
arxiv-1501-02102 | Equitability of Dependence Measure | http://arxiv.org/abs/1501.02102 | id:1501.02102 author:Hangjin Jiang, Kan Liu, Yiming Ding category:stat.ML  published:2015-01-09 summary:A measure of dependence is said to be equitable if it gives similar scores to equally noisy relationship of different types. In practice, we do not know what kind of functional relationship is underlying two given observations, Hence the equitability of dependence measure is critical in analysis and by scoring relationships according to an equitable measure one hopes to find important patterns of any type of further examination. In this paper, we introduce our definition of equitability of a dependence measure, which is naturally from this initial description, and Further more power-equitable(weak-equitable) is introduced which is of the most practical meaning in evaluating the equitablity of a dependence measure. version:2
arxiv-1603-07810 | Disentangling Nonlinear Perceptual Embeddings With Multi-Query Triplet Networks | http://arxiv.org/abs/1603.07810 | id:1603.07810 author:Andreas Veit, Serge Belongie, Theofanis Karaletsos category:cs.CV cs.AI cs.LG  published:2016-03-25 summary:In typical perceptual tasks, higher-order concepts are inferred from visual features to assist with perceptual decision making. However, there is a multitude of visual concepts which can be inferred from a single stimulus. When learning nonlinear embeddings with siamese or triplet networks from similarities, we typically assume they are sourced from a single visual concept. In this paper, we are concerned with the hypothesis that it can be potentially harmful to ignore the heterogeneity of concepts affiliated with observed similarities when learning these embedding networks. We demonstrate empirically that this hypothesis holds and suggest an approach that deals with these shortcomings, by combining multiple notions of similarities in one compact system. We propose Multi-Query Networks (MQNs) that leverage recent advances in representation learning on factorized triplet embeddings in combination with Convolutional Networks in order to learn embeddings differentiated into semantically distinct subspaces, which are learned with a latent space attention mechanism. We show that the resulting model learns visually relevant semantic subspaces with features that do not only outperform single triplet networks, but even sets of concept specific networks. version:1
arxiv-1603-07807 | Mode-Seeking on Hypergraphs for Robust Geometric Model Fitting | http://arxiv.org/abs/1603.07807 | id:1603.07807 author:Hanzi Wang, Guobao Xiao, Yan Yan, David Suter category:cs.CV  published:2016-03-25 summary:In this paper, we propose a novel geometric model fitting method, called Mode-Seeking on Hypergraphs (MSH),to deal with multi-structure data even in the presence of severe outliers. The proposed method formulates geometric model fitting as a mode seeking problem on a hypergraph in which vertices represent model hypotheses and hyperedges denote data points. MSH intuitively detects model instances by a simple and effective mode seeking algorithm. In addition to the mode seeking algorithm, MSH includes a similarity measure between vertices on the hypergraph and a weight-aware sampling technique. The proposed method not only alleviates sensitivity to the data distribution, but also is scalable to large scale problems. Experimental results further demonstrate that the proposed method has significant superiority over the state-of-the-art fitting methods on both synthetic data and real images. version:1
arxiv-1603-04223 | Investigation of event-based memory surfaces for high-speed tracking, unsupervised feature extraction and object recognition | http://arxiv.org/abs/1603.04223 | id:1603.04223 author:Saeed Afshar, Gregory Cohen, Chetan Singh Thakur, Jonathan Tapson, Tara Julia Hamilton, Andre van Schaik category:cs.NE cs.CV  published:2016-03-14 summary:In this paper an event-based tracking, feature extraction, and classification system is presented for performing object recognition using an event-based camera. The high-speed recognition task involves detecting and classifying model airplanes that are dropped free-hand close to the camera lens so as to generate a challenging highly varied dataset of spatio-temporal event patterns. We investigate the use of time decaying memory surfaces to capture the temporal aspect of the event-based data. These surfaces are then used to perform unsupervised feature extraction, tracking and recognition. Both linear and exponentially decaying surfaces were found to result in equally high recognition accuracy. Using only twenty five event-based feature extracting neurons in series with a linear classifier, the system achieves 98.61% recognition accuracy within 156 milliseconds of the airplane entering the field of view. By comparing the linear classifier results to a high-capacity ELM classifier, we find that a small number of event-based feature extractors can effectively project the complex spatio-temporal event patterns of the data-set to a linearly separable representation in the feature space. version:2
arxiv-1603-07800 | An Effective Unconstrained Correlation Filter and Its Kernelization for Face Recognition | http://arxiv.org/abs/1603.07800 | id:1603.07800 author:Yan Yan, Hanzi Wang, Cuihua Li, Chenhui Yang, Bineng Zhong category:cs.CV  published:2016-03-25 summary:In this paper, an effective unconstrained correlation filter called Uncon- strained Optimal Origin Tradeoff Filter (UOOTF) is presented and applied to robust face recognition. Compared with the conventional correlation filters in Class-dependence Feature Analysis (CFA), UOOTF improves the overall performance for unseen patterns by removing the hard constraints on the origin correlation outputs during the filter design. To handle non-linearly separable distributions between different classes, we further develop a non- linear extension of UOOTF based on the kernel technique. The kernel ex- tension of UOOTF allows for higher flexibility of the decision boundary due to a wider range of non-linearity properties. Experimental results demon- strate the effectiveness of the proposed unconstrained correlation filter and its kernelization in the task of face recognition. version:1
arxiv-1603-07797 | Quadratic Projection Based Feature Extraction with Its Application to Biometric Recognition | http://arxiv.org/abs/1603.07797 | id:1603.07797 author:Yan Yan, Hanzi Wang, Si Chen, Xiaochun Cao, David Zhang category:cs.CV  published:2016-03-25 summary:This paper presents a novel quadratic projection based feature extraction framework, where a set of quadratic matrices is learned to distinguish each class from all other classes. We formulate quadratic matrix learning (QML) as a standard semidefinite programming (SDP) problem. However, the con- ventional interior-point SDP solvers do not scale well to the problem of QML for high-dimensional data. To solve the scalability of QML, we develop an efficient algorithm, termed DualQML, based on the Lagrange duality theory, to extract nonlinear features. To evaluate the feasibility and effectiveness of the proposed framework, we conduct extensive experiments on biometric recognition. Experimental results on three representative biometric recogni- tion tasks, including face, palmprint, and ear recognition, demonstrate the superiority of the DualQML-based feature extraction algorithm compared to the current state-of-the-art algorithms version:1
arxiv-1505-01197 | Contextual Action Recognition with R*CNN | http://arxiv.org/abs/1505.01197 | id:1505.01197 author:Georgia Gkioxari, Ross Girshick, Jitendra Malik category:cs.CV  published:2015-05-05 summary:There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R*CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R*CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R*CNN is not limited to action recognition. In particular, R*CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset. version:3
arxiv-1510-03283 | Text-Attentional Convolutional Neural Networks for Scene Text Detection | http://arxiv.org/abs/1510.03283 | id:1510.03283 author:Tong He, Weilin Huang, Yu Qiao, Jian Yao category:cs.CV  published:2015-10-12 summary:Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature computed globally from a whole image component (patch), where the cluttered background information may dominate true text features in the deep representation. This leads to less discriminative power and poorer robustness. In this work, we present a new system for scene text detection by proposing a novel Text-Attentional Convolutional Neural Network (Text-CNN) that particularly focuses on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text/nontext information. The rich supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where low-level supervised information greatly facilitates main task of text/non-text classification. In addition, a powerful low-level detector called Contrast- Enhancement Maximally Stable Extremal Regions (CE-MSERs) is developed, which extends the widely-used MSERs by enhancing intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2013 dataset, with a F-measure of 0.82, improving the state-of-the-art results substantially. version:2
arxiv-1603-07772 | Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks | http://arxiv.org/abs/1603.07772 | id:1603.07772 author:Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, Li Shen, Xiaohui Xie category:cs.CV cs.LG  published:2016-03-24 summary:Skeleton based action recognition distinguishes human actions using the trajectories of skeleton joints, which provide a very good representation for describing actions. Considering that recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) can learn feature representations and model long-term temporal dependencies automatically, we propose an end-to-end fully connected deep LSTM network for skeleton based action recognition. Inspired by the observation that the co-occurrences of the joints intrinsically characterize human actions, we take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints. To train the deep LSTM network effectively, we propose a new dropout algorithm which simultaneously operates on the gates, cells, and output responses of the LSTM neurons. Experimental results on three human action recognition datasets consistently demonstrate the effectiveness of the proposed model. version:1
arxiv-1603-07771 | Generating Text from Structured Data with Application to the Biography Domain | http://arxiv.org/abs/1603.07771 | id:1603.07771 author:Remi Lebret, David Grangier, Michael Auli category:cs.CL  published:2016-03-24 summary:This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magni- tude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocab- ulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text genera- tion. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that trans- fer sample-specific words from the in- put database to the generated output sen- tence. Our neural model significantly out- performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU. version:1
arxiv-1603-07763 | Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric Video | http://arxiv.org/abs/1603.07763 | id:1603.07763 author:Hao Jiang, Kristen Grauman category:cs.CV  published:2016-03-24 summary:Understanding the camera wearer's activity is central to egocentric vision, yet one key facet of that activity is inherently invisible to the camera--the wearer's body pose. Prior work focuses on estimating the pose of hands and arms when they come into view, but this 1) gives an incomplete view of the full body posture, and 2) prevents any pose estimate at all in many frames, since the hands are only visible in a fraction of daily life activities. We propose to infer the "invisible pose" of a person behind the egocentric camera. Given a single video, our efficient learning-based approach returns the full body 3D joint positions for each frame. Our method exploits cues from the dynamic motion signatures of the surrounding scene--which changes predictably as a function of body pose--as well as static scene structures that reveal the viewpoint (e.g., sitting vs. standing). We further introduce a novel energy minimization scheme to infer the pose sequence. It uses soft predictions of the poses per time instant together with a non-parametric model of human pose dynamics over longer windows. Our method outperforms an array of possible alternatives, including deep learning approaches for direct pose regression from images. version:1
arxiv-1506-05855 | Information-based inference in sloppy and singular models | http://arxiv.org/abs/1506.05855 | id:1506.05855 author:Colin H. LaMont, Paul A. Wiggins category:stat.ML cs.LG physics.data-an  published:2015-06-19 summary:A central problem in statistics is model selection: the choice between competing models of a stochastic process whose observables are corrupted by noise. In information-based inference, model selection is performed by maximizing the estimated predictive performance. We propose a frequen- tist information criterion (FIC) which extends the applicability of information-based inference to the analysis of singular and sloppy models. In these scenarios, the Akaike information criterion (AIC) can result in significant under or over-estimates of the predictive complexity. Two important mechanisms for this failure are examined: an implicit multiple testing problem and the presence of unidentifiable parameters. FIC rectifies this failure by applying a frequentist approximation to compute the com- plexity. For regular models in the large-sample-size limit, AIC and FIC are equal, but in general the complexity exhibits a sample-size dependent scaling. In the context of singular models, FIC can ex- hibit Bayesian information criterion-like or Hannan-Quinn-like scalings with sample size. FIC does not depend on ad hoc prior distributions or exogenous regularization and can be applied when struc- tured data complicates the use of cross-validatation. version:3
arxiv-1603-07758 | A universal tradeoff between power, precision and speed in physical communication | http://arxiv.org/abs/1603.07758 | id:1603.07758 author:Subhaneil Lahiri, Jascha Sohl-Dickstein, Surya Ganguli category:cond-mat.stat-mech cs.IT math.IT physics.bio-ph q-bio.NC stat.ML  published:2016-03-24 summary:Maximizing the speed and precision of communication while minimizing power dissipation is a fundamental engineering design goal. Also, biological systems achieve remarkable speed, precision and power efficiency using poorly understood physical design principles. Powerful theories like information theory and thermodynamics do not provide general limits on power, precision and speed. Here we go beyond these classical theories to prove that the product of precision and speed is universally bounded by power dissipation in any physical communication channel whose dynamics is faster than that of the signal. Moreover, our derivation involves a novel connection between friction and information geometry. These results may yield insight into both the engineering design of communication devices and the structure and function of biological signaling systems. version:1
arxiv-1603-06846 | New metrics for learning and inference on sets, ontologies, and functions | http://arxiv.org/abs/1603.06846 | id:1603.06846 author:Ruiyu Yang, Yuxiang Jiang, Matthew W. Hahn, Elizabeth A. Housworth, Predrag Radivojac category:stat.ML  published:2016-03-22 summary:We propose new metrics on sets, ontologies, and functions that can be used in various stages of probabilistic modeling, including exploratory data analysis, learning, inference, and result interpretation. These new functions unify and generalize some of the popular metrics on sets and functions, such as the Jaccard and bag distances on sets and Marczewski-Steinhaus distance on functions. We then introduce information-theoretic metrics on directed acyclic graphs drawn independently according to a fixed probability distribution and show how they can be used to calculate similarity between class labels for the objects with hierarchical output spaces (e.g., protein function). Finally, we provide evidence that the proposed metrics are useful by clustering species based solely on functional annotations available for subsets of their genes. The functional trees resemble evolutionary trees obtained by the phylogenetic analysis of their genomes. version:2
arxiv-1603-07749 | Pathway Lasso: Estimate and Select Sparse Mediation Pathways with High Dimensional Mediators | http://arxiv.org/abs/1603.07749 | id:1603.07749 author:Yi Zhao, Xi Luo category:stat.ML stat.AP stat.ME 62J07  62P10  90C25  published:2016-03-24 summary:In many scientific studies, it becomes increasingly important to delineate the causal pathways through a large number of mediators, such as genetic and brain mediators. Structural equation modeling (SEM) is a popular technique to estimate the pathway effects, commonly expressed as products of coefficients. However, it becomes unstable to fit such models with high dimensional mediators, especially for a general setting where all the mediators are causally dependent but the exact causal relationships between them are unknown. This paper proposes a sparse mediation model using a regularized SEM approach, where sparsity here means that a small number of mediators have nonzero mediation effects between a treatment and an outcome. To address the model selection challenge, we innovate by introducing a new penalty called Pathway Lasso. This penalty function is a convex relaxation of the non-convex product function, and it enables a computationally tractable optimization criterion to estimate and select many pathway effects simultaneously. We develop a fast ADMM-type algorithm to compute the model parameters, and we show that the iterative updates can be expressed in closed form. On both simulated data and a real fMRI dataset, the proposed approach yields higher pathway selection accuracy and lower estimation bias than other competing methods. version:1
arxiv-1603-07745 | Coarse-to-Fine Segmentation With Shape-Tailored Scale Spaces | http://arxiv.org/abs/1603.07745 | id:1603.07745 author:Ganesh Sundaramoorthi, Naeemullah Khan, Byung-Woo Hong category:cs.CV  published:2016-03-24 summary:We formulate a general energy and method for segmentation that is designed to have preference for segmenting the coarse structure over the fine structure of the data, without smoothing across boundaries of regions. The energy is formulated by considering data terms at a continuum of scales from the scale space computed from the Heat Equation within regions, and integrating these terms over all time. We show that the energy may be approximately optimized without solving for the entire scale space, but rather solving time-independent linear equations at the native scale of the image, making the method computationally feasible. We provide a multi-region scheme, and apply our method to motion segmentation. Experiments on a benchmark dataset shows that our method is less sensitive to clutter or other undesirable fine-scale structure, and leads to better performance in motion segmentation. version:1
arxiv-1603-07738 | Skill-Based Differences in Spatio-Temporal Team Behavior in Defence of The Ancients 2 | http://arxiv.org/abs/1603.07738 | id:1603.07738 author:Anders Drachen, Matthew Yancey, John Maguire, Derrek Chu, Iris Yuhui Wang, Tobias Mahlmann, Matthias Schubert, Diego Klabjan category:stat.ML  published:2016-03-24 summary:Multiplayer Online Battle Arena (MOBA) games are among the most played digital games in the world. In these games, teams of players fight against each other in arena environments, and the gameplay is focused on tactical combat. Mastering MOBAs requires extensive practice, as is exemplified in the popular MOBA Defence of the Ancients 2 (DotA 2). In this paper, we present three data-driven measures of spatio-temporal behavior in DotA 2: 1) Zone changes; 2) Distribution of team members and: 3) Time series clustering via a fuzzy approach. We present a method for obtaining accurate positional data from DotA 2. We investigate how behavior varies across these measures as a function of the skill level of teams, using four tiers from novice to professional players. Results indicate that spatio-temporal behavior of MOBA teams is related to team skill, with professional teams having smaller within-team distances and conducting more zone changes than amateur teams. The temporal distribution of the within-team distances of professional and high-skilled teams also generally follows patterns distinct from lower skill ranks. version:1
arxiv-1602-05242 | Monte Carlo Markov Chain Algorithms for Sampling Strongly Rayleigh Distributions and Determinantal Point Processes | http://arxiv.org/abs/1602.05242 | id:1602.05242 author:Nima Anari, Shayan Oveis Gharan, Alireza Rezaei category:cs.LG cs.DS math.PR  published:2016-02-16 summary:Strongly Rayleigh distributions are natural generalizations of product and determinantal probability distributions and satisfy strongest form of negative dependence properties. We show that the "natural" Monte Carlo Markov Chain (MCMC) is rapidly mixing in the support of a {\em homogeneous} strongly Rayleigh distribution. As a byproduct, our proof implies Markov chains can be used to efficiently generate approximate samples of a $k$-determinantal point process. This answers an open question raised by Deshpande and Rademacher. version:3
arxiv-1603-07704 | Probabilistic Reasoning via Deep Learning: Neural Association Models | http://arxiv.org/abs/1603.07704 | id:1603.07704 author:Quan Liu, Hui Jiang, Zhen-Hua Ling, Si Wei, Yu Hu category:cs.AI cs.LG cs.NE  published:2016-03-24 summary:In this paper, we propose a new deep learning approach, called neural association model (NAM), for probabilistic reasoning in artificial intelligence. We propose to use neural networks to model association between any two events in a domain. Neural networks take one event as input and compute a conditional probability of the other event to model how likely these two events are associated. The actual meaning of the conditional probabilities varies between applications and depends on how the models are trained. In this work, as two case studies, we have investigated two NAM structures, namely deep neural networks (DNNs) and relation modulated neural nets (RMNNs), on several probabilistic reasoning tasks in AI, including recognizing textual entailment, triple classification in multirelational knowledge bases and common-sense reasoning. Experimental results on several popular data sets derived from WordNet, FreeBase and ConceptNet have all demonstrated that both DNNs and RMNNs perform equally well and they can significantly outperform the conventional methods available for these reasoning tasks. Moreover, comparing with DNNs, RMNNs are superior in knowledge transfer, where a pre-trained model can be quickly extended to an unseen relation after observing only a few training samples. version:1
arxiv-1603-07697 | Joint Projection and Dictionary Learning using Low-rank Regularization and Graph Constraints | http://arxiv.org/abs/1603.07697 | id:1603.07697 author:Homa Foroughi, Nilanjan Ray, Hong Zhang category:cs.CV  published:2016-03-24 summary:In this paper, we aim at learning simultaneously a discriminative dictionary and a robust projection matrix from noisy data. The joint learning, makes the learned projection and dictionary a better fit for each other, so a more accurate classification can be obtained. However, current prevailing joint dimensionality reduction and dictionary learning methods, would fail when the training samples are noisy or heavily corrupted. To address this issue, we propose a joint projection and dictionary learning using low-rank regularization and graph constraints (JPDL-LR). Specifically, the discrimination of the dictionary is achieved by imposing Fisher criterion on the coding coefficients. In addition, our method explicitly encodes the local structure of data by incorporating a graph regularization term, that further improves the discriminative ability of the projection matrix. Inspired by recent advances of low-rank representation for removing outliers and noise, we enforce a low-rank constraint on sub-dictionaries of all classes to make them more compact and robust to noise. Experimental results on several benchmark datasets verify the effectiveness and robustness of our method for both dimensionality reduction and image classification, especially when the data contains considerable noise or variations. version:1
arxiv-1603-07695 | Part-of-Speech Relevance Weights for Learning Word Embeddings | http://arxiv.org/abs/1603.07695 | id:1603.07695 author:Quan Liu, Zhen-Hua Ling, Hui Jiang, Yu Hu category:cs.CL  published:2016-03-24 summary:This paper proposes a model to learn word embeddings with weighted contexts based on part-of-speech (POS) relevance weights. POS is a fundamental element in natural language. However, state-of-the-art word embedding models fail to consider it. This paper proposes to use position-dependent POS relevance weighting matrices to model the inherent syntactic relationship among words within a context window. We utilize the POS relevance weights to model each word-context pairs during the word embedding training process. The model proposed in this paper paper jointly optimizes word vectors and the POS relevance matrices. Experiments conducted on popular word analogy and word similarity tasks all demonstrated the effectiveness of the proposed method. version:1
arxiv-1603-07692 | Predictive Analytics Using Smartphone Sensors for Depressive Episodes | http://arxiv.org/abs/1603.07692 | id:1603.07692 author:Taeheon Jeong, Diego Klabjan, Justin Starren category:cs.CY cs.HC stat.ML  published:2016-03-24 summary:The behaviors of patients with depression are usually difficult to predict because the patients demonstrate the symptoms of a depressive episode without a warning at unexpected times. The goal of this research is to build algorithms that detect signals of such unusual moments so that doctors can be proactive in approaching already diagnosed patients before they fall in depression. Each patient is equipped with a smartphone with the capability to track its sensors. We first find the home location of a patient, which is then augmented with other sensor data to identify sleep patterns and select communication patterns. The algorithms require two to three weeks of training data to build standard patterns, which are considered normal behaviors; and then, the methods identify any anomalies in day-to-day data readings of sensors. Four smartphone sensors, including the accelerometer, the gyroscope, the location probe and the communication log probe are used for anomaly detection in sleeping and communication patterns. version:1
arxiv-1603-01076 | What is the right way to represent document images? | http://arxiv.org/abs/1603.01076 | id:1603.01076 author:Gabriela Csurka, Diane Larlus, Albert Gordo, Jon Almazan category:cs.CV  published:2016-03-03 summary:In this article we study the problem of document image representation based on visual features. We propose a comprehensive experimental study that compares three types of visual document image representations: (1) traditional so-called shallow features, such as the RunLength and the Fisher-Vector descriptors, (2) deep features based on Convolutional Neural Networks, and (3) features extracted from hybrid architectures that take inspiration from the two previous ones. We evaluate these features in several tasks (i.e. classification, clustering, and retrieval) and in different setups (e.g. domain transfer) using several public and in-house datasets. Our results show that deep features generally outperform other types of features when there is no domain shift and the new task is closely related to the one used to train the model. However, when a large domain or task shift is present, the Fisher-Vector shallow features generalize better and often obtain the best results. version:2
arxiv-1603-07646 | Recursive Neural Language Architecture for Tag Prediction | http://arxiv.org/abs/1603.07646 | id:1603.07646 author:Saurabh Kataria category:cs.IR cs.CL cs.LG cs.NE  published:2016-03-24 summary:We consider the problem of learning distributed representations for tags from their associated content for the task of tag recommendation. Considering tagging information is usually very sparse, effective learning from content and tag association is very crucial and challenging task. Recently, various neural representation learning models such as WSABIE and its variants show promising performance, mainly due to compact feature representations learned in a semantic space. However, their capacity is limited by a linear compositional approach for representing tags as sum of equal parts and hurt their performance. In this work, we propose a neural feedback relevance model for learning tag representations with weighted feature representations. Our experiments on two widely used datasets show significant improvement for quality of recommendations over various baselines. version:1
arxiv-1506-06081 | A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements | http://arxiv.org/abs/1506.06081 | id:1506.06081 author:Qinqing Zheng, John Lafferty category:stat.ML cs.LG  published:2015-06-19 summary:We propose a simple, scalable, and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs. With $O(r^3 \kappa^2 n \log n)$ random measurements of a positive semidefinite $n \times n$ matrix of rank $r$ and condition number $\kappa$, our method is guaranteed to converge linearly to the global optimum. version:3
arxiv-1603-07625 | Position and Vector Detection of Blind Spot motion with the Horn-Schunck Optical Flow | http://arxiv.org/abs/1603.07625 | id:1603.07625 author:Stephen Yu, Mike Wu category:cs.CV  published:2016-03-24 summary:The proposed method uses live image footage which, based on calculations of pixel motion, decides whether or not an object is in the blind-spot. If found, the driver is notified by a sensory light or noise built into the vehicle's CPU. The new technology incorporates optical vectors and flow fields rather than expensive radar-waves, creating cheaper detection systems that retain the needed accuracy while adapting to the current processor speeds. version:1
arxiv-1603-07624 | Semantic Properties of Customer Sentiment in Tweets | http://arxiv.org/abs/1603.07624 | id:1603.07624 author:Eun Hee Ko, Diego Klabjan category:cs.CL cs.IR cs.SI stat.ML  published:2016-03-24 summary:An increasing number of people are using online social networking services (SNSs), and a significant amount of information related to experiences in consumption is shared in this new media form. Text mining is an emerging technique for mining useful information from the web. We aim at discovering in particular tweets semantic patterns in consumers' discussions on social media. Specifically, the purposes of this study are twofold: 1) finding similarity and dissimilarity between two sets of textual documents that include consumers' sentiment polarities, two forms of positive vs. negative opinions and 2) driving actual content from the textual data that has a semantic trend. The considered tweets include consumers opinions on US retail companies (e.g., Amazon, Walmart). Cosine similarity and K-means clustering methods are used to achieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic modeling algorithm, is used for the latter purpose. This is the first study which discover semantic properties of textual data in consumption context beyond sentiment analysis. In addition to major findings, we apply LDA (Latent Dirichlet Allocations) to the same data and drew latent topics that represent consumers' positive opinions and negative opinions on social media. version:1
arxiv-1603-07610 | Going Out of Business: Auction House Behavior in the Massively Multi-Player Online Game | http://arxiv.org/abs/1603.07610 | id:1603.07610 author:Anders Drachen, Joseph Riley, Shawna Baskin, Diego Klabjan category:cs.CY cs.HC stat.ML  published:2016-03-24 summary:The in-game economies of massively multi-player online games (MMOGs) are complex systems that have to be carefully designed and managed. This paper presents the results of an analysis of auction house data from the MMOG Glitch, across a 14 month time period, the entire lifetime of the game. The data comprise almost 3 million data points, over 20,000 unique players and more than 650 products. Furthermore, an interactive visualization, based on Sankey flow diagrams, is presented which shows the proportion of the different clusters across each time bin, as well as the flow of players between clusters. The diagram allows evaluation of migration of players between clusters as a function of time, as well as churn analysis. The presented work provides a template analysis and visualization model for progression-based or temporal-based analysis of player behavior broadly applicable to games. version:1
arxiv-1603-07609 | Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL | http://arxiv.org/abs/1603.07609 | id:1603.07609 author:Yevgeni Berzak, Roi Reichart, Boris Katz category:cs.CL  published:2016-03-24 summary:This work examines the impact of cross-linguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low-resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition. version:1
arxiv-1603-07604 | Multi-Subregion Based Correlation Filter Bank for Robust Face Recognition | http://arxiv.org/abs/1603.07604 | id:1603.07604 author:Yan Yan, Hanzi Wang, David Suter category:cs.CV  published:2016-03-24 summary:In this paper, we propose an effective feature extraction algorithm, called Multi-Subregion based Correlation Filter Bank (MS-CFB), for robust face recognition. MS-CFB combines the benefits of global-based and local-based feature extraction algorithms, where multiple correlation filters correspond- ing to different face subregions are jointly designed to optimize the overall correlation outputs. Furthermore, we reduce the computational complexi- ty of MS-CFB by designing the correlation filter bank in the spatial domain and improve its generalization capability by capitalizing on the unconstrained form during the filter bank design process. MS-CFB not only takes the d- ifferences among face subregions into account, but also effectively exploits the discriminative information in face subregions. Experimental results on various public face databases demonstrate that the proposed algorithm pro- vides a better feature representation for classification and achieves higher recognition rates compared with several state-of-the-art algorithms. version:1
arxiv-1603-07603 | Semantic Regularities in Document Representations | http://arxiv.org/abs/1603.07603 | id:1603.07603 author:Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, Xueqi Cheng category:cs.CL  published:2016-03-24 summary:Recent work exhibited that distributed word representations are good at capturing linguistic regularities in language. This allows vector-oriented reasoning based on simple linear algebra between words. Since many different methods have been proposed for learning document representations, it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level. To answer this question, we design a new document analogy task for testing the semantic regularities in document representations, and conduct empirical evaluations over several state-of-the-art document representation models. The results reveal that neural embedding based document representations work better on this analogy task than conventional methods, and we provide some preliminary explanations over these observations. version:1
arxiv-1603-07602 | Clustering Time-Series Energy Data from Smart Meters | http://arxiv.org/abs/1603.07602 | id:1603.07602 author:Alexander Lavin, Diego Klabjan category:stat.ML  published:2016-03-24 summary:Investigations have been performed into using clustering methods in data mining time-series data from smart meters. The problem is to identify patterns and trends in energy usage profiles of commercial and industrial customers over 24-hour periods, and group similar profiles. We tested our method on energy usage data provided by several U.S. power utilities. The results show accurate grouping of accounts similar in their energy usage patterns, and potential for the method to be utilized in energy efficiency programs. version:1
arxiv-1301-0802 | Borrowing strengh in hierarchical Bayes: Posterior concentration of the Dirichlet base measure | http://arxiv.org/abs/1301.0802 | id:1301.0802 author:XuanLong Nguyen category:math.ST cs.LG math.PR stat.TH  published:2013-01-04 summary:This paper studies posterior concentration behavior of the base probability measure of a Dirichlet measure, given observations associated with the sampled Dirichlet processes, as the number of observations tends to infinity. The base measure itself is endowed with another Dirichlet prior, a construction known as the hierarchical Dirichlet processes (Teh et al. [J. Amer. Statist. Assoc. 101 (2006) 1566-1581]). Convergence rates are established in transportation distances (i.e., Wasserstein metrics) under various conditions on the geometry of the support of the true base measure. As a consequence of the theory, we demonstrate the benefit of "borrowing strength" in the inference of multiple groups of data - a powerful insight often invoked to motivate hierarchical modeling. In certain settings, the gain in efficiency due to the latent hierarchy can be dramatic, improving from a standard nonparametric rate to a parametric rate of convergence. Tools developed include transportation distances for nonparametric Bayesian hierarchies of random measures, the existence of tests for Dirichlet measures, and geometric properties of the support of Dirichlet measures. version:4
arxiv-1603-07584 | Source Localization on Graphs via l1 Recovery and Spectral Graph Theory | http://arxiv.org/abs/1603.07584 | id:1603.07584 author:Rodrigo Pena, Xavier Bresson, Pierre Vandergheynst category:cs.LG  published:2016-03-24 summary:We cast the problem of source localization on graphs as the simultaneous problem of sparse recovery and diffusion ker- nel learning. An l1 regularization term enforces the sparsity constraint while we recover the sources of diffusion from a single snapshot of the diffusion process. The diffusion ker- nel is estimated by assuming the process to be as generic as the standard heat diffusion. We show with synthetic data that we can concomitantly learn the diffusion kernel and the sources, given an estimated initialization. We validate our model with cholera mortality and atmospheric tracer diffusion data, showing also that the accuracy of the solution depends on the construction of the graph from the data points. version:1
arxiv-1604-03029 | Mapping Out Narrative Structures and Dynamics Using Networks and Textual Information | http://arxiv.org/abs/1604.03029 | id:1604.03029 author:Semi Min, Juyong Park category:cs.CL cs.SI physics.soc-ph  published:2016-03-24 summary:Human communication is often executed in the form of a narrative, an account of connected events composed of characters, actions, and settings. A coherent narrative structure is therefore a requisite for a well-formulated narrative -- be it fictional or nonfictional -- for informative and effective communication, opening up the possibility of a deeper understanding of a narrative by studying its structural properties. In this paper we present a network-based framework for modeling and analyzing the structure of a narrative, which is further expanded by incorporating methods from computational linguistics to utilize the narrative text. Modeling a narrative as a dynamically unfolding system, we characterize its progression via the growth patterns of the character network, and use sentiment analysis and topic modeling to represent the actual content of the narrative in the form of interaction maps between characters with associated sentiment values and keywords. This is a network framework advanced beyond the simple occurrence-based one most often used until now, allowing one to utilize the unique characteristics of a given narrative to a high degree. Given the ubiquity and importance of narratives, such advanced network-based representation and analysis framework may lead to a more systematic modeling and understanding of narratives for social interactions, expression of human sentiments, and communication. version:1
arxiv-1602-02964 | A Kernel Test of Goodness of Fit | http://arxiv.org/abs/1602.02964 | id:1602.02964 author:Kacper Chwialkowski, Heiko Strathmann, Arthur Gretton category:stat.ML  published:2016-02-09 summary:We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein's method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation. version:3
arxiv-1603-07485 | Weakly Supervised Semantic Labelling and Instance Segmentation | http://arxiv.org/abs/1603.07485 | id:1603.07485 author:Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, Bernt Schiele category:cs.CV  published:2016-03-24 summary:Semantic labelling and instance segmentation are two tasks that require particularly costly annotations. Starting from weak supervision in the form of bounding box detection annotations, we propose to recursively train a convnet such that outputs are improved after each iteration. We explore which aspects affect the recursive training, and which is the most suitable box-guided segmentation to use as initialisation. Our results improve significantly over previously reported ones, even when using rectangles as rough initialisation. Overall, our weak supervision approach reaches ~95% of the quality of the fully supervised model, both for semantic labelling and instance segmentation. version:1
arxiv-1603-07475 | Fine-scale Surface Normal Estimation using a Single NIR Image | http://arxiv.org/abs/1603.07475 | id:1603.07475 author:Youngjin Yoon, Gyeongmin Choe, Namil Kim, Joon-Young Lee, In So Kweon category:cs.CV  published:2016-03-24 summary:We present surface normal estimation using a single near infrared (NIR) image. We are focusing on fine-scale surface geometry captured with an uncalibrated light source. To tackle this ill-posed problem, we adopt a generative adversarial network which is effective in recovering a sharp output, which is also essential for fine-scale surface normal estimation. We incorporate angular error and integrability constraint into the objective function of the network to make estimated normals physically meaningful. We train and validate our network on a recent NIR dataset, and also evaluate the generality of our trained model by using new external datasets which are captured with a different camera under different environment. version:1
arxiv-1603-03170 | Data fluidity in DARIAH -- pushing the agenda forward | http://arxiv.org/abs/1603.03170 | id:1603.03170 author:Laurent Romary, Mike Mertens, Anne Baillot category:cs.CY cs.CL cs.DL  published:2016-03-10 summary:This paper provides both an update concerning the setting up of the European DARIAH infrastructure and a series of strong action lines related to the development of a data centred strategy for the humanities in the coming years. In particular we tackle various aspect of data management: data hosting, the setting up of a DARIAH seal of approval, the establishment of a charter between cultural heritage institutions and scholars and finally a specific view on certification mechanisms for data. version:2
arxiv-1603-07454 | Deep Extreme Feature Extraction: New MVA Method for Searching Particles in High Energy Physics | http://arxiv.org/abs/1603.07454 | id:1603.07454 author:Chao Ma, Tianchenghou, Bin Lan, Jinhui Xu, Zhenhua Zhang category:cs.LG cs.NE  published:2016-03-24 summary:In this paper, we present Deep Extreme Feature Extraction (DEFE), a new ensemble MVA method for searching $\tau^{+}\tau^{-}$ channel of Higgs bosons in high energy physics. DEFE can be viewed as a deep ensemble learning scheme that trains a strongly diverse set of neural feature learners without explicitly encouraging diversity and penalizing correlations. This is achieved by adopting an implicit neural controller (not involved in feedforward compuation) that directly controls and distributes gradient flows from higher level deep prediction network. Such model-independent controller results in that every single local feature learned are used in the feature-to-output mapping stage, avoiding the blind averaging of features. DEFE makes the ensembles 'deep' in the sense that it allows deep post-process of these features that tries to learn to select and abstract the ensemble of neural feature learners. With the application of this model, a selection regions full of signal process can be obtained through the training of a miniature collision events set. In comparison of the Classic Deep Neural Network, DEFE shows a state-of-the-art performance: the error rate has decreased by about 37\%, the accuracy has broken through 90\% for the first time, along with the discovery significance has reached a standard deviation of 6.0 $\sigma$. Experimental data shows that, DEFE is able to train an ensemble of discriminative feature learners that boosts the overperformance of final prediction. version:1
arxiv-1603-07442 | Pixel-Level Domain Transfer | http://arxiv.org/abs/1603.07442 | id:1603.07442 author:Donggeun Yoo, Namil Kim, Sunggyun Park, Anthony S. Paek, In So Kweon category:cs.CV cs.AI  published:2016-03-24 summary:We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results. version:1
arxiv-1603-04531 | Revealing the Hidden Patterns of News Photos: Analysis of Millions of News Photos Using GDELT and Deep Learning-based Vision APIs | http://arxiv.org/abs/1603.04531 | id:1603.04531 author:Haewoon Kwak, Jisun An category:cs.CY cs.CV cs.IR  published:2016-03-15 summary:In this work, we analyze more than two million news photos published in January 2016. We demonstrate i) which objects appear the most in news photos; ii) what the sentiments of news photos are; iii) whether the sentiment of news photos is aligned with the tone of the text; iv) how gender is treated; and v) how differently political candidates are portrayed. To our best knowledge, this is the first large-scale study of news photo contents using deep learning-based vision APIs. version:2
arxiv-1603-07415 | Attentive Contexts for Object Detection | http://arxiv.org/abs/1603.07415 | id:1603.07415 author:Jianan Li, Yunchao Wei, Xiaodan Liang, Jian Dong, Tingfa Xu, Jiashi Feng, Shuicheng Yan category:cs.CV  published:2016-03-24 summary:Modern deep neural network based object detection methods typically classify candidate proposals using their interior features. However, global and local surrounding contexts that are believed to be valuable for object detection are not fully exploited by existing methods yet. In this work, we take a step towards understanding what is a robust practice to extract and utilize contextual information to facilitate object detection in practice. Specifically, we consider the following two questions: "how to identify useful global contextual information for detecting a certain object?" and "how to exploit local context surrounding a proposal for better inferring its contents?". We provide preliminary answers to these questions through developing a novel Attention to Context Convolution Neural Network (AC-CNN) based object detection model. AC-CNN effectively incorporates global and local contextual information into the region-based CNN (e.g. Fast RCNN) detection model and provides better object detection performance. It consists of one attention-based global contextualized (AGC) sub-network and one multi-scale local contextualized (MLC) sub-network. To capture global context, the AGC sub-network recurrently generates an attention map for an input image to highlight useful global contextual locations, through multiple stacked Long Short-Term Memory (LSTM) layers. For capturing surrounding local context, the MLC sub-network exploits both the inside and outside contextual information of each specific proposal at multiple scales. The global and local context are then fused together for making the final decision for detection. Extensive experiments on PASCAL VOC 2007 and VOC 2012 well demonstrate the superiority of the proposed AC-CNN over well-established baselines. In particular, AC-CNN outperforms the popular Fast-RCNN by 2.0% and 2.2% on VOC 2007 and VOC 2012 in terms of mAP, respectively. version:1
arxiv-1603-07396 | A Diagram Is Worth A Dozen Images | http://arxiv.org/abs/1603.07396 | id:1603.07396 author:Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi category:cs.CV cs.AI  published:2016-03-24 summary:Diagrams are common tools for representing complex concepts, relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation and reasoning, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs. version:1
arxiv-1603-07394 | Predicting litigation likelihood and time to litigation for patents | http://arxiv.org/abs/1603.07394 | id:1603.07394 author:Papis Wongchaisuwat, Diego Klabjan, John O. McGinnis category:stat.ML  published:2016-03-23 summary:Patent lawsuits are costly and time-consuming. An ability to forecast a patent litigation and time to litigation allows companies to better allocate budget and time in managing their patent portfolios. We develop predictive models for estimating the likelihood of litigation for patents and the expected time to litigation based on both textual and non-textual features. Our work focuses on improving the state-of-the-art by relying on a different set of features and employing more sophisticated algorithms with more realistic data. The rate of patent litigations is very low, which consequently makes the problem difficult. The initial model for predicting the likelihood is further modified to capture a time-to-litigation perspective. version:1
arxiv-1603-07388 | Face Recognition Using Deep Multi-Pose Representations | http://arxiv.org/abs/1603.07388 | id:1603.07388 author:Wael AbdAlmageed, Yue Wua, Stephen Rawlsa, Shai Harel, Tal Hassner, Iacopo Masi, Jongmoo Choi, Jatuporn Toy Leksut, Jungyeon Kim, Prem Natarajan, Ram Nevatia, Gerard Medioni category:cs.CV  published:2016-03-23 summary:We introduce our method and system for face recognition using multiple pose-aware deep learning models. In our representation, a face image is processed by several pose-specific deep convolutional neural network (CNN) models to generate multiple pose-specific features. 3D rendering is used to generate multiple face poses from the input image. Sensitivity of the recognition system to pose variations is reduced since we use an ensemble of pose-specific CNN features. The paper presents extensive experimental results on the effect of landmark detection, CNN layer selection and pose model selection on the performance of the recognition pipeline. Our novel representation achieves better results than the state-of-the-art on IARPA's CS2 and NIST's IJB-A in both verification and identification (i.e. search) tasks. version:1
arxiv-1603-07341 | Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices | http://arxiv.org/abs/1603.07341 | id:1603.07341 author:Tayfun Gokmen, Yurii Vlasov category:cs.LG cs.NE stat.ML  published:2016-03-23 summary:In recent years, deep neural networks (DNN) have demonstrated significant business impact in large scale analysis and classification tasks such as speech recognition, visual object detection, pattern extraction, etc. Training of large DNNs, however, is universally considered as time consuming and computationally intensive task that demands datacenter-scale computational resources recruited for many days. Here we propose a concept of resistive processing unit (RPU) devices that can potentially accelerate DNN training by orders of magnitude while using much less power. The proposed RPU device can store and update the weight values locally thus minimizing data movement during training and allowing to fully exploit the locality and the parallelism of the training algorithm. We identify the RPU device and system specifications for implementation of an accelerator chip for DNN training in a realistic CMOS-compatible technology. For large DNNs with about 1 billion weights this massively parallel RPU architecture can achieve acceleration factors of 30,000X compared to state-of-the-art microprocessors while providing power efficiency of 84,000 GigaOps/s/W. Problems that currently require days of training on a datacenter-size cluster with thousands of machines can be addressed within hours on a single RPU accelerator. A system consisted of a cluster of RPU accelerators will be able to tackle Big Data problems with trillions of parameters that is impossible to address today like, for example, natural speech recognition and translation between all world languages, real-time analytics on large streams of business and scientific data, integration and analysis of multimodal sensory data flows from massive number of IoT (Internet of Things) sensors. version:1
arxiv-1603-07313 | CONDITOR1: Topic Maps and DITA labelling tool for textual documents with historical information | http://arxiv.org/abs/1603.07313 | id:1603.07313 author:Piedad Garrido, Jesus Tramullas, Manuel Coll category:cs.DL cs.CL cs.IR  published:2016-03-23 summary:Conditor is a software tool which works with textual documents containing historical information. The purpose of this work two-fold: firstly to show the validity of the developed engine to correctly identify and label the entities of the universe of discourse with a labelled-combined XTM-DITA model. Secondly to explain the improvements achieved in the information retrieval process thanks to the use of a object-oriented database (JPOX) as well as its integration into the Lucene-type database search process to not only accomplish more accurate searches, but to also help the future development of a recommender system. We finish with a brief demo in a 3D-graph of the results of the aforementioned search. version:1
arxiv-1508-06904 | Rapid Exact Signal Scanning with Deep Convolutional Neural Networks | http://arxiv.org/abs/1508.06904 | id:1508.06904 author:Markus Thom, Franz Gritschneder category:cs.LG cs.CV cs.NE  published:2015-08-27 summary:We introduce and analyze a rigorous formulation of the dynamics of a signal processing scheme aimed at exact dense signal scanning. Related methods proposed in the recent past lack a satisfactory analysis whether they actually fulfill any exactness constraints. We improve on this through an exact characterization of the requirements for a sound sliding window approach. The tools developed in this paper are especially beneficial if Convolutional Neural Networks are employed, but can also be used as a more general framework to validate related approaches to signal scanning. The contributed theory helps to eliminate redundant computations and renders special case treatment unnecessary, resulting in a dramatic boost in efficiency particularly on massively parallel processors. This is demonstrated both theoretically in a computational complexity analysis and empirically on modern parallel processors. version:2
arxiv-1603-07292 | Debugging Machine Learning Tasks | http://arxiv.org/abs/1603.07292 | id:1603.07292 author:Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, Deepak Vijaykeerthy category:cs.LG cs.AI cs.PL stat.ML D.2.5; I.2.3  published:2016-03-23 summary:Unlike traditional programs (such as operating systems or word processors) which have large amounts of code, machine learning tasks use programs with relatively small amounts of code (written in machine learning libraries), but voluminous amounts of data. Just like developers of traditional programs debug errors in their code, developers of machine learning tasks debug and fix errors in their data. However, algorithms and tools for debugging and fixing errors in data are less common, when compared to their counterparts for detecting and fixing errors in code. In this paper, we consider classification tasks where errors in training data lead to misclassifications in test points, and propose an automated method to find the root causes of such misclassifications. Our root cause analysis is based on Pearl's theory of causation, and uses Pearl's PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi, encodes the computation of PS as a probabilistic program, and uses recent work on probabilistic programs and transformations on probabilistic programs (along with gray-box models of machine learning algorithms) to efficiently compute PS. Psi is able to identify root causes of data errors in interesting data sets. version:1
arxiv-1603-05324 | Fast moment estimation for generalized latent Dirichlet models | http://arxiv.org/abs/1603.05324 | id:1603.05324 author:Shiwen Zhao, Barbara E. Engelhardt, Sayan Mukherjee, David B. Dunson category:math.ST cs.LG stat.AP stat.ME stat.TH  published:2016-03-17 summary:We develop a generalized method of moments (GMM) approach for fast parameter estimation in a new class of Dirichlet latent variable models with mixed data types. Parameter estimation via GMM has been demonstrated to have computational and statistical advantages over alternative methods, such as expectation maximization, variational inference, and Markov chain Monte Carlo. The key computational advan- tage of our method (MELD) is that parameter estimation does not require instantiation of the latent variables. Moreover, a representational advantage of the GMM approach is that the behavior of the model is agnostic to distributional assumptions of the observations. We derive population moment conditions after marginalizing out the sample-specific Dirichlet latent variables. The moment conditions only depend on component mean parameters. We illustrate the utility of our approach on simulated data, comparing results from MELD to alternative methods, and we show the promise of our approach through the application of MELD to several data sets. version:2
arxiv-1603-07285 | A guide to convolution arithmetic for deep learning | http://arxiv.org/abs/1603.07285 | id:1603.07285 author:Vincent Dumoulin, Francesco Visin category:stat.ML cs.LG cs.NE  published:2016-03-23 summary:We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive. version:1
arxiv-1011-6256 | Nuclear norm penalization and optimal rates for noisy low rank matrix completion | http://arxiv.org/abs/1011.6256 | id:1011.6256 author:Vladimir Koltchinskii, Alexandre B. Tsybakov, Karim Lounici category:math.ST stat.ML stat.TH  published:2010-11-29 summary:This paper deals with the trace regression model where $n$ entries or linear combinations of entries of an unknown $m_1\times m_2$ matrix $A_0$ corrupted by noise are observed. We propose a new nuclear norm penalized estimator of $A_0$ and establish a general sharp oracle inequality for this estimator for arbitrary values of $n,m_1,m_2$ under the condition of isometry in expectation. Then this method is applied to the matrix completion problem. In this case, the estimator admits a simple explicit form and we prove that it satisfies oracle inequalities with faster rates of convergence than in the previous works. They are valid, in particular, in the high-dimensional setting $m_1m_2\gg n$. We show that the obtained rates are optimal up to logarithmic factors in a minimax sense and also derive, for any fixed matrix $A_0$, a non-minimax lower bound on the rate of convergence of our estimator, which coincides with the upper bound up to a constant factor. Finally, we show that our procedure provides an exact recovery of the rank of $A_0$ with probability close to 1. We also discuss the statistical learning setting where there is no underlying model determined by $A_0$ and the aim is to find the best trace regression model approximating the data. version:4
arxiv-1603-07254 | Gaussian Process Morphable Models | http://arxiv.org/abs/1603.07254 | id:1603.07254 author:Marcel Lüthi, Christoph Jud, Thomas Gerig, Thomas Vetter category:cs.CV  published:2016-03-23 summary:Statistical shape models (SSMs) represent a class of shapes as a normal distribution of point variations, whose parameters are estimated from example shapes. Principal component analysis (PCA) is applied to obtain a low-dimensional representation of the shape variation in terms of the leading principal components. In this paper, we propose a generalization of SSMs, called Gaussian Process Morphable Models (GPMMs). We model the shape variations with a Gaussian process, which we represent using the leading components of its Karhunen-Loeve expansion. To compute the expansion, we make use of an approximation scheme based on the Nystrom method. The resulting model can be seen as a continuous analogon of an SSM. However, while for SSMs the shape variation is restricted to the span of the example data, with GPMMs we can define the shape variation using any Gaussian process. For example, we can build shape models that correspond to classical spline models, and thus do not require any example data. Furthermore, Gaussian processes make it possible to combine different models. For example, an SSM can be extended with a spline model, to obtain a model that incorporates learned shape characteristics, but is flexible enough to explain shapes that cannot be represented by the SSM. We introduce a simple algorithm for fitting a GPMM to a surface or image. This results in a non-rigid registration approach, whose regularization properties are defined by a GPMM. We show how we can obtain different registration schemes,including methods for multi-scale, spatially-varying or hybrid registration, by constructing an appropriate GPMM. As our approach strictly separates modelling from the fitting process, this is all achieved without changes to the fitting algorithm. We show the applicability and versatility of GPMMs on a clinical use case, where the goal is the model-based segmentation of 3D forearm images. version:1
arxiv-1603-07253 | Evaluating semantic models with word-sentence relatedness | http://arxiv.org/abs/1603.07253 | id:1603.07253 author:Kimberly Glasgow, Matthew Roos, Amy Haufler, Mark Chevillet, Michael Wolmetz category:cs.CL  published:2016-03-23 summary:Semantic textual similarity (STS) systems are designed to encode and evaluate the semantic similarity between words, phrases, sentences, and documents. One method for assessing the quality or authenticity of semantic information encoded in these systems is by comparison with human judgments. A data set for evaluating semantic models was developed consisting of 775 English word-sentence pairs, each annotated for semantic relatedness by human raters engaged in a Maximum Difference Scaling (MDS) task, as well as a faster alternative task. As a sample application of this relatedness data, behavior-based relatedness was compared to the relatedness computed via four off-the-shelf STS models: n-gram, Latent Semantic Analysis (LSA), Word2Vec, and UMBC Ebiquity. Some STS models captured much of the variance in the human judgments collected, but they were not sensitive to the implicatures and entailments that were processed and considered by the participants. All text stimuli and judgment data have been made freely available. version:1
arxiv-1603-07249 | A Tutorial on Deep Neural Networks for Intelligent Systems | http://arxiv.org/abs/1603.07249 | id:1603.07249 author:Juan C. Cuevas-Tello, Manuel Valenzuela-Rendon, Juan A. Nolazco-Flores category:cs.NE cs.LG J.4.6  published:2016-03-23 summary:Developing Intelligent Systems involves artificial intelligence approaches including artificial neural networks. Here, we present a tutorial of Deep Neural Networks (DNNs), and some insights about the origin of the term "deep"; references to deep learning are also given. Restricted Boltzmann Machines, which are the core of DNNs, are discussed in detail. An example of a simple two-layer network, performing unsupervised learning for unlabeled data, is shown. Deep Belief Networks (DBNs), which are used to build networks with more than two layers, are also described. Moreover, examples for supervised learning with DNNs performing simple prediction and classification tasks, are presented and explained. This tutorial includes two intelligent pattern recognition applications: hand- written digits (benchmark known as MNIST) and speech recognition. version:1
arxiv-1603-07234 | Lightweight Unsupervised Domain Adaptation by Convolutional Filter Reconstruction | http://arxiv.org/abs/1603.07234 | id:1603.07234 author:Rahaf Aljundi, Tinne Tuytelaars category:cs.CV  published:2016-03-23 summary:End-to-end learning methods have achieved impressive results in many areas of computer vision. At the same time, these methods still suffer from a degradation in performance when testing on new datasets that stem from a different distribution. This is known as the domain shift effect. Recently proposed adaptation methods focus on retraining the network parameters. However, this requires access to all (labeled) source data, a large amount of (unlabeled) target data, and plenty of computational resources. In this work, we propose a lightweight alternative, that allows adapting to the target domain based on a limited number of target samples in a matter of minutes rather than hours, days or even weeks. To this end, we first analyze the output of each convolutional layer from a domain adaptation perspective. Surprisingly, we find that already at the very first layer, domain shift effects pop up. We then propose a new domain adaptation method, where first layer convolutional filters that are badly affected by the domain shift are reconstructed based on less affected ones. This improves the performance of the deep network on various benchmark datasets. version:1
arxiv-1603-00370 | Scalable Metric Learning via Weighted Approximate Rank Component Analysis | http://arxiv.org/abs/1603.00370 | id:1603.00370 author:Cijo Jose, Francois Fleuret category:cs.CV  published:2016-03-01 summary:We are interested in the large-scale learning of Mahalanobis distances, with a particular focus on person re-identification. We propose a metric learning formulation called Weighted Approximate Rank Component Analysis (WARCA). WARCA optimizes the precision at top ranks by combining the WARP loss with a regularizer that favors orthonormal linear mappings, and avoids rank-deficient embeddings. Using this new regularizer allows us to adapt the large-scale WSABIE procedure and to leverage the Adam stochastic optimization algorithm, which results in an algorithm that scales gracefully to very large data-sets. Also, we derive a kernelized version which allows to take advantage of state-of-the-art features for re-identification when data-set size permits kernel computation. Benchmarks on recent and standard re-identification data-sets show that our method beats existing state-of-the-art techniques both in term of accuracy and speed. We also provide experimental analysis to shade lights on the properties of the regularizer we use, and how it improves performance. version:2
arxiv-1603-07195 | A Decentralized Quasi-Newton Method for Dual Formulations of Consensus Optimization | http://arxiv.org/abs/1603.07195 | id:1603.07195 author:Mark Eisen, Aryan Mokhtari, Alejandro Ribeiro category:math.OC cs.DC cs.LG  published:2016-03-23 summary:This paper considers consensus optimization problems where each node of a network has access to a different summand of an aggregate cost function. Nodes try to minimize the aggregate cost function, while they exchange information only with their neighbors. We modify the dual decomposition method to incorporate a curvature correction inspired by the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method. The resulting dual D-BFGS method is a fully decentralized algorithm in which nodes approximate curvature information of themselves and their neighbors through the satisfaction of a secant condition. Dual D-BFGS is of interest in consensus optimization problems that are not well conditioned, making first order decentralized methods ineffective, and in which second order information is not readily available, making decentralized second order methods infeasible. Asynchronous implementation is discussed and convergence of D-BFGS is established formally for both synchronous and asynchronous implementations. Performance advantages relative to alternative decentralized algorithms are shown numerically. version:1
arxiv-1603-07188 | Weakly-Supervised Semantic Segmentation using Motion Cues | http://arxiv.org/abs/1603.07188 | id:1603.07188 author:Pavel Tokmakov, Karteek Alahari, Cordelia Schmid category:cs.CV  published:2016-03-23 summary:Fully convolutional neural networks (FCNNs) trained on a large number of images with strong pixel-level annotations have become the new state of the art for the semantic segmentation task. While there have been recent attempts to learn FCNNs from image-level weak annotations, they need additional constraints, such as the size of an object, to obtain reasonable performance. To address this issue, we present motion-CNN (M-CNN), a novel FCNN framework which incorporates motion cues and is learned from video-level weak annotations. Our learning scheme to train the network uses motion segments as soft constraints, thereby handling noisy motion information. When trained on weakly-annotated videos, our method outperforms the state-of-the-art approach of Papandreou et al. by a factor of 2 on the PASCAL VOC 2012 image segmentation benchmark. We also demonstrate that the performance of M-CNN learned with 150 weak video annotations is on par with state-of-the-art weakly-supervised methods trained with thousands of images. Finally, M-CNN substantially outperforms recent approaches in a related task of video co-localization on the YouTube-Objects dataset. version:1
arxiv-1603-07185 | Enabling Cognitive Intelligence Queries in Relational Databases using Low-dimensional Word Embeddings | http://arxiv.org/abs/1603.07185 | id:1603.07185 author:Rajesh Bordawekar, Oded Shmueli category:cs.CL cs.DB  published:2016-03-23 summary:We apply distributed language embedding methods from Natural Language Processing to assign a vector to each database entity associated token (for example, a token may be a word occurring in a table row, or the name of a column). These vectors, of typical dimension 200, capture the meaning of tokens based on the contexts in which the tokens appear together. To form vectors, we apply a learning method to a token sequence derived from the database. We describe various techniques for extracting token sequences from a database. The techniques differ in complexity, in the token sequences they output and in the database information used (e.g., foreign keys). The vectors can be used to algebraically quantify semantic relationships between the tokens such as similarities and analogies. Vectors enable a dual view of the data: relational and (meaningful rather than purely syntactical) text. We introduce and explore a new class of queries called cognitive intelligence (CI) queries that extract information from the database based, in part, on the relationships encoded by vectors. We have implemented a prototype system on top of Spark to exhibit the power of CI queries. Here, CI queries are realized via SQL UDFs. This power goes far beyond text extensions to relational systems due to the information encoded in vectors. We also consider various extensions to the basic scheme, including using a collection of views derived from the database to focus on a domain of interest, utilizing vectors and/or text from external sources, maintaining vectors as the database evolves and exploring a database without utilizing its schema. For the latter, we consider minimal extensions to SQL to vastly improve query expressiveness. version:1
arxiv-1603-07150 | The Anatomy of a Search and Mining System for Digital Archives | http://arxiv.org/abs/1603.07150 | id:1603.07150 author:Martyn Harris, Mark Levene, Dell Zhang, Dan Levene category:cs.DL cs.CL cs.IR  published:2016-03-23 summary:Samtla (Search And Mining Tools with Linguistic Analysis) is a digital humanities system designed in collaboration with historians and linguists to assist them with their research work in quantifying the content of any textual corpora through approximate phrase search and document comparison. The retrieval engine uses a character-based n-gram language model rather than the conventional word-based one so as to achieve great flexibility in language agnostic query processing. The index is implemented as a space-optimised character-based suffix tree with an accompanying database of document content and metadata. A number of text mining tools are integrated into the system to allow researchers to discover textual patterns, perform comparative analysis, and find out what is currently popular in the research community. Herein we describe the system architecture, user interface, models and algorithms, and data storage of the Samtla system. We also present several case studies of its usage in practice together with an evaluation of the systems' ranking performance through crowdsourcing. version:1
arxiv-1603-07141 | BreakingNews: Article Annotation by Image and Text Processing | http://arxiv.org/abs/1603.07141 | id:1603.07141 author:Arnau Ramisa, Fei Yan, Francesc Moreno-Noguer, Krystian Mikolajczyk category:cs.CV  published:2016-03-23 summary:Building upon recent Deep Neural Network architectures, current approaches lying in the intersection of computer vision and natural language processing have achieved unprecedented breakthroughs in tasks like automatic captioning or image retrieval. Most of these learning methods, though, rely on large training sets of images associated with human annotations that specifically describe the visual content. In this paper we propose to go a step further and explore the more complex cases where textual descriptions are loosely related to the images. We focus on the particular domain of News articles in which the textual content often expresses connotative and ambiguous relations that are only suggested but not directly inferred from images. We introduce new deep learning methods that address source detection, popularity prediction, article illustration and geolocation of articles. An adaptive CNN architecture is proposed, that shares most of the structure for all the tasks, and is suitable for multitask and transfer learning. Deep Canonical Correlation Analysis is deployed for article illustration, and a new loss function based on Great Circle Distance is proposed for geolocation. Furthermore, we present BreakingNews, a novel dataset with approximately 100K news articles including images, text and captions, and enriched with heterogeneous meta-data (such as GPS coordinates and popularity metrics). We show this dataset to be appropriate to explore all aforementioned problems, for which we provide a baseline performance using various Deep Learning architectures, and different representations of the textual and visual features. We report very promising results and bring to light several limitations of current state-of-the-art in this kind of domain, which we hope will help spur progress in the field. version:1
arxiv-1503-08650 | Comparison of Bayesian predictive methods for model selection | http://arxiv.org/abs/1503.08650 | id:1503.08650 author:Juho Piironen, Aki Vehtari category:stat.ME cs.LG  published:2015-03-30 summary:The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation (CV) score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. This can also lead to substantial selection induced bias and optimism in the performance evaluation for the selected model. From a predictive viewpoint, best results are obtained by accounting for model uncertainty by forming the full encompassing model, such as the Bayesian model averaging solution over the candidate models. If the encompassing model is too complex, it can be robustly simplified by the projection method, in which the information of the full model is projected onto the submodels. This approach is substantially less prone to overfitting than selection based on CV-score. Overall, the projection method appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model. version:4
arxiv-1603-07123 | Robust cDNA microarray image segmentation and analysis technique based on Hough circle transform | http://arxiv.org/abs/1603.07123 | id:1603.07123 author:R. M. Farouk, M. A. SayedElahl category:cs.CV  published:2016-03-23 summary:One of the most challenging tasks in microarray image analysis is spot segmentation. A solution to this problem is to provide an algorithm than can be used to find any spot within the microarray image. Circular Hough Transformation (CHT) is a powerful feature extraction technique used in image analysis, computer vision, and digital image processing. CHT algorithm is applied on the cDNA microarray images to develop the accuracy and the efficiency of the spots localization, addressing and segmentation process. The purpose of the applied technique is to find imperfect instances of spots within a certain class of circles by applying a voting procedure on the cDNA microarray images for spots localization, addressing and characterizing the pixels of each spot into foreground pixels and background simultaneously. Intensive experiments on the University of North Carolina (UNC) microarray database indicate that the proposed method is superior to the K-means method and the Support vector machine (SVM). Keywords: Hough circle transformation, cDNA microarray image analysis, cDNA microarray image segmentation, spots localization and addressing, spots segmentation version:1
arxiv-1603-07120 | Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos | http://arxiv.org/abs/1603.07120 | id:1603.07120 author:Amir Shahroudy, Tian-Tsong Ng, Yihong Gong, Gang Wang category:cs.CV  published:2016-03-23 summary:Single modality action recognition on RGB or depth sequences has been extensively explored recently. It is generally accepted that each of these two modalities has different strengths and limitations for the task of action recognition. Therefore, analysis of the RGB+D videos can help us to better study the complementary properties of these two types of modalities and achieve higher levels of performance. In this paper, we propose a new deep autoencoder based shared-specific feature factorization network to separate input multimodal signals into a hierarchy of components. Further, based on the structure of the features, a structured sparsity learning machine is proposed which utilizes mixed norms to apply regularization within components and group selection between them for better classification performance. Our experimental results show the effectiveness of our cross-modality feature analysis framework by achieving state-of-the-art accuracy for action classification on four challenging benchmark datasets, for which we reduce the error rate by more than 40% in three datasets and saturating the benchmark with perfect accuracy for the other one. version:1
arxiv-1603-07094 | Predicting Glaucoma Visual Field Loss by Hierarchically Aggregating Clustering-based Predictors | http://arxiv.org/abs/1603.07094 | id:1603.07094 author:Motohide Higaki, Kai Morino, Hiroshi Murata, Ryo Asaoka, Kenji Yamanishi category:stat.ML cs.LG  published:2016-03-23 summary:This study addresses the issue of predicting the glaucomatous visual field loss from patient disease datasets. Our goal is to accurately predict the progress of the disease in individual patients. As very few measurements are available for each patient, it is difficult to produce good predictors for individuals. A recently proposed clustering-based method enhances the power of prediction using patient data with similar spatiotemporal patterns. Each patient is categorized into a cluster of patients, and a predictive model is constructed using all of the data in the class. Predictions are highly dependent on the quality of clustering, but it is difficult to identify the best clustering method. Thus, we propose a method for aggregating cluster-based predictors to obtain better prediction accuracy than from a single cluster-based prediction. Further, the method shows very high performances by hierarchically aggregating experts generated from several cluster-based methods. We use real datasets to demonstrate that our method performs significantly better than conventional clustering-based and patient-wise regression methods, because the hierarchical aggregating strategy has a mechanism whereby good predictors in a small community can thrive. version:1
arxiv-1503-08248 | Socializing the Semantic Gap: A Comparative Survey on Image Tag Assignment, Refinement and Retrieval | http://arxiv.org/abs/1503.08248 | id:1503.08248 author:Xirong Li, Tiberio Uricchio, Lamberto Ballan, Marco Bertini, Cees G. M. Snoek, Alberto Del Bimbo category:cs.IR cs.CV cs.MM cs.SI H.3.1; H.3.3  published:2015-03-28 summary:Where previous reviews on content-based image retrieval emphasize on what can be seen in an image to bridge the semantic gap, this survey considers what people tag about an image. A comprehensive treatise of three closely linked problems, i.e., image tag assignment, refinement, and tag-based image retrieval is presented. While existing works vary in terms of their targeted tasks and methodology, they rely on the key functionality of tag relevance, i.e. estimating the relevance of a specific tag with respect to the visual content of a given image and its social context. By analyzing what information a specific method exploits to construct its tag relevance function and how such information is exploited, this paper introduces a taxonomy to structure the growing literature, understand the ingredients of the main works, clarify their connections and difference, and recognize their merits and limitations. For a head-to-head comparison between the state-of-the-art, a new experimental protocol is presented, with training sets containing 10k, 100k and 1m images and an evaluation on three test sets, contributed by various research groups. Eleven representative works are implemented and evaluated. Putting all this together, the survey aims to provide an overview of the past and foster progress for the near future. version:3
arxiv-1505-06821 | Deep Ranking for Person Re-identification via Joint Representation Learning | http://arxiv.org/abs/1505.06821 | id:1505.06821 author:Shi-Zhe Chen, Chun-Chao Guo, Jian-Huang Lai category:cs.CV  published:2015-05-26 summary:This paper proposes a novel approach to person re-identification, a fundamental task in distributed multi-camera surveillance systems. Although a variety of powerful algorithms have been presented in the past few years, most of them usually focus on designing hand-crafted features and learning metrics either individually or sequentially. Different from previous works, we formulate a unified deep ranking framework that jointly tackles both of these key components to maximize their strengths. We start from the principle that the correct match of the probe image should be positioned in the top rank within the whole gallery set. An effective learning-to-rank algorithm is proposed to minimize the cost corresponding to the ranking disorders of the gallery. The ranking model is solved with a deep convolutional neural network (CNN) that builds the relation between input image pairs and their similarity scores through joint representation learning directly from raw image pixels. The proposed framework allows us to get rid of feature engineering and does not rely on any assumption. An extensive comparative evaluation is given, demonstrating that our approach significantly outperforms all state-of-the-art approaches, including both traditional and CNN-based methods on the challenging VIPeR, CUHK-01 and CAVIAR4REID datasets. Additionally, our approach has better ability to generalize across datasets without fine-tuning. version:2
arxiv-1603-07063 | Semantic Object Parsing with Graph LSTM | http://arxiv.org/abs/1603.07063 | id:1603.07063 author:Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, Shuicheng Yan category:cs.CV  published:2016-03-23 summary:By taking the semantic object parsing task as an exemplar application scenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network, which is the generalization of LSTM from sequential data or multi-dimensional data to general graph-structured data. Particularly, instead of evenly and fixedly dividing an image to pixels or patches in existing multi-dimensional LSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each arbitrary-shaped superpixel as a semantically consistent node, and adaptively construct an undirected graph for each image, where the spatial relations of the superpixels are naturally used as edges. Constructed on such an adaptive graph topology, the Graph LSTM is more naturally aligned with the visual patterns in the image (e.g., object boundaries or appearance similarities) and provides a more economical information propagation route. Furthermore, for each optimization step over Graph LSTM, we propose to use a confidence-driven scheme to update the hidden and memory states of nodes progressively till all nodes are updated. In addition, for each node, the forgets gates are adaptively learned to capture different degrees of semantic correlation with neighboring nodes. Comprehensive evaluations on four diverse semantic object parsing datasets well demonstrate the significant superiority of our Graph LSTM over other state-of-the-art solutions. version:1
arxiv-1603-06201 | A Survey on Object Detection in Optical Remote Sensing Images | http://arxiv.org/abs/1603.06201 | id:1603.06201 author:Gong Cheng, Junwei Han category:cs.CV  published:2016-03-20 summary:Object detection in optical remote sensing images, being a fundamental but challenging problem in the field of aerial and satellite image analysis, plays an important role for a wide range of applications and is receiving significant attention in recent years. While enormous methods exist, a deep review of the literature concerning generic object detection is still lacking. This paper aims to provide a review of the recent progress in this field. Different from several previously published surveys that focus on a specific object class such as building and road, we concentrate on more generic object categories including, but are not limited to, road, building, tree, vehicle, ship, airport, urban-area. Covering about 270 publications we survey 1) template matching-based object detection methods, 2) knowledge-based object detection methods, 3) object-based image analysis (OBIA)-based object detection methods, 4) machine learning-based object detection methods, and 5) five publicly available datasets and three standard evaluation metrics. We also discuss the challenges of current studies and propose two promising research directions, namely deep learning-based feature representation and weakly supervised learning-based geospatial object detection. It is our hope that this survey will be beneficial for the researchers to have better understanding of this research field. version:2
arxiv-1603-07051 | Cosolver2B: An Efficient Local Search Heuristic for the Travelling Thief Problem | http://arxiv.org/abs/1603.07051 | id:1603.07051 author:Mohamed El Yafrani, Belaïd Ahiod category:cs.AI cs.DS cs.NE  published:2016-03-23 summary:Real-world problems are very difficult to optimize. However, many researchers have been solving benchmark problems that have been extensively investigated for the last decades even if they have very few direct applications. The Traveling Thief Problem (TTP) is a NP-hard optimization problem that aims to provide a more realistic model. TTP targets particularly routing problem under packing/loading constraints which can be found in supply chain management and transportation. In this paper, TTP is presented and formulated mathematically. A combined local search algorithm is proposed and compared with Random Local Search (RLS) and Evolutionary Algorithm (EA). The obtained results are quite promising since new better solutions were found. version:1
arxiv-1603-07044 | Recurrent Neural Network Encoder with Attention for Community Question Answering | http://arxiv.org/abs/1603.07044 | id:1603.07044 author:Wei-Ning Hsu, Yu Zhang, James Glass category:cs.CL cs.LG cs.NE  published:2016-03-23 summary:We apply a general recurrent neural network (RNN) encoder framework to community question answering (cQA) tasks. Our approach does not rely on any linguistic processing, and can be applied to different languages or domains. Further improvements are observed when we extend the RNN encoders with a neural attention mechanism that encourages reasoning over entire sequences. To deal with practical issues such as data sparsity and imbalanced labels, we apply various techniques such as transfer learning and multitask learning. Our experiments on the SemEval-2016 cQA task show 10% improvement on a MAP score compared to an information retrieval-based approach, and achieve comparable performance to a strong handcrafted feature-based method. version:1
arxiv-1603-07027 | MOON: A Mixed Objective Optimization Network for the Recognition of Facial Attributes | http://arxiv.org/abs/1603.07027 | id:1603.07027 author:Ethan Rudd, Manuel Günther, Terrance Boult category:cs.CV  published:2016-03-22 summary:Multi-task vision problems can often be decomposed into separate tasks and stages, e.g., separating feature extraction and model building, or training independent models for each task. Joint optimization has been shown to improve performance, but can be difficult to apply to deep convolution neural networks (DCNN), especially with unbalanced data. This paper introduces a novel mixed objective optimization network (MOON), with a loss function which mixes errors from multiple tasks and supports domain adaptation when label frequencies differ between training and operational testing. Experiments demonstrate that not only does MOON advance the state of the art in facial attribute recognition, but it also outperforms independently trained DCNNs using the same data. version:1
arxiv-1603-07022 | Active Detection and Localization of Textureless Objects in Cluttered Environments | http://arxiv.org/abs/1603.07022 | id:1603.07022 author:Marco Imperoli, Alberto Pretto category:cs.CV cs.RO  published:2016-03-22 summary:This paper introduces an active object detection and localization framework that combines a robust untextured object detection and 3D pose estimation algorithm with a novel next-best-view selection strategy. We address the detection and localization problems by proposing an edge-based registration algorithm that refines the object position by minimizing a cost directly extracted from a 3D image tensor that encodes the minimum distance to an edge point in a joint direction/location space. We face the next-best-view problem by exploiting a sequential decision process that, for each step, selects the next camera position which maximizes the mutual information between the state and the next observations. We solve the intrinsic intractability of this solution by generating observations that represent scene realizations, i.e. combination samples of object hypothesis provided by the object detector, while modeling the state by means of a set of constantly resampled particles. Experiments performed on different real world, challenging datasets confirm the effectiveness of the proposed methods. version:1
arxiv-1509-05789 | BLC: Private Matrix Factorization Recommenders via Automatic Group Learning | http://arxiv.org/abs/1509.05789 | id:1509.05789 author:Alessandro Checco, Giuseppe Bianchi, Doug Leith category:cs.LG stat.ML  published:2015-09-18 summary:We propose a privacy-enhanced matrix factorization recommender that exploits the fact that users can often be grouped together by interest. This allows a form of "hiding in the crowd" privacy. We introduce a novel matrix factorization approach suited to making recommendations in a shared group (or nym) setting and the BLC algorithm for carrying out this matrix factorization in a privacy-enhanced manner. We demonstrate that the increased privacy does not come at the cost of reduced recommendation accuracy. version:2
arxiv-1603-07012 | Word Sense Disambiguation with Neural Language Models | http://arxiv.org/abs/1603.07012 | id:1603.07012 author:Dayu Yuan, Ryan Doherty, Julian Richardson, Colin Evans, Eric Altendorf category:cs.CL  published:2016-03-22 summary:Determining the intended sense of words in text -- word sense disambiguation (WSD) -- is a long-standing problem in natural language processing. In this paper, we present WSD algorithms which use neural network language models to achieve state-of-the-art precision. Each of these methods learns to disambiguate word senses using only a set of word senses, a few example sentences for each sense taken from a licensed lexicon, and a large unlabeled text corpus. We classify based on cosine similarity of vectors derived from the contexts in unlabeled query and labeled example sentences. We demonstrate state-of-the-art results when using the WordNet sense inventory, and significantly better than baseline performance using the New Oxford American Dictionary inventory. The best performance was achieved by combining an LSTM language model with graph label propagation. version:1
arxiv-1603-06987 | Knowledge Transfer for Scene-specific Motion Prediction | http://arxiv.org/abs/1603.06987 | id:1603.06987 author:Lamberto Ballan, Francesco Castaldo, Alexandre Alahi, Francesco Palmieri, Silvio Savarese category:cs.CV  published:2016-03-22 summary:When given a single frame of the video, humans can not only interpret the content of the scene, but also they are able to forecast the near future. This ability is mostly driven by their rich prior knowledge about the visual world, both in terms of (\emph{i}) the dynamics of moving agents, as well as (\emph{ii}) the semantic of the scene. In this work we exploit the interplay between these two key elements to predict scene-specific motion patterns. First, we extract patch descriptors encoding the probability of moving to the adjacent patches, and the probability of being in that particular patch or changing behavior. Then, we introduce a Dynamic Bayesian Network which exploits this scene specific knowledge for trajectory prediction. Experimental results demonstrate that our method is able to accurately predict trajectories and transfer predictions to a novel scene characterized by similar elements. version:1
arxiv-1601-00372 | Mutual Information and Diverse Decoding Improve Neural Machine Translation | http://arxiv.org/abs/1601.00372 | id:1601.00372 author:Jiwei Li, Dan Jurafsky category:cs.CL cs.AI  published:2016-01-04 summary:Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., $p(y x)$, an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, the proposed models offers a consistent performance boost on both standard LSTM and attention-based neural MT architectures. version:2
arxiv-1602-02995 | Segmental Spatio-Temporal CNNs for Fine-grained Action Segmentation and Classification | http://arxiv.org/abs/1602.02995 | id:1602.02995 author:Colin Lea, Austin Reiter, Rene Vidal, Gregory D. Hager category:cs.CV cs.RO  published:2016-02-09 summary:Joint segmentation and classification of fine-grained actions is important for applications in human-robot interaction, video surveillance, and human skill evaluation. However, despite substantial recent progress in large scale action classification, the performance of state-of-the-art fine-grained action recognition approaches remains low. In this paper, we propose a new spatio-temporal CNN model for fine-grained action classification and segmentation, which combines (1) a spatial CNN to represent objects in the scene and their spatial relationships; (2) a temporal CNN that captures how object relationships within an action change over time; and (3) a semi-Markov model that captures transitions from one action to another. In addition, we introduce an efficient segmental inference algorithm for joint segmentation and classification of actions that is orders of magnitude faster than state-of-the-art approaches. We highlight the effectiveness of our approach on cooking and surgical action datasets for which we observe substantially improved performance relative to recent baseline methods. version:2
arxiv-1603-06937 | Stacked Hourglass Networks for Human Pose Estimation | http://arxiv.org/abs/1603.06937 | id:1603.06937 author:Alejandro Newell, Kaiyu Yang, Jia Deng category:cs.CV  published:2016-03-22 summary:This work introduces a novel Convolutional Network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a 'stacked hourglass' network based on the successive steps of pooling and upsampling that are done to produce a final set of estimates. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods. version:1
arxiv-1603-06923 | Inference via Message Passing on Partially Labeled Stochastic Block Models | http://arxiv.org/abs/1603.06923 | id:1603.06923 author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST stat.ML stat.TH  published:2016-03-22 summary:We study the community detection and recovery problem in partially-labeled stochastic block models (SBM). We develop a fast linearized message-passing algorithm to reconstruct labels for SBM (with $n$ nodes, $k$ blocks, $p,q$ intra and inter block connectivity) when $\delta$ proportion of node labels are revealed. The signal-to-noise ratio ${\sf SNR}(n,k,p,q,\delta)$ is shown to characterize the fundamental limitations of inference via local algorithms. On the one hand, when ${\sf SNR}>1$, the linearized message-passing algorithm provides the statistical inference guarantee with mis-classification rate at most $\exp(-({\sf SNR}-1)/2)$, thus interpolating smoothly between strong and weak consistency. This exponential dependence improves upon the known error rate $({\sf SNR}-1)^{-1}$ in the literature on weak recovery. On the other hand, when ${\sf SNR}<1$ (for $k=2$) and ${\sf SNR}<1/4$ (for general growing $k$), we prove that local algorithms suffer an error rate at least $\frac{1}{2} - \sqrt{\delta \cdot {\sf SNR}}$, which is only slightly better than random guess for small $\delta$. version:1
arxiv-1603-06915 | Completely random measures for modeling power laws in sparse graphs | http://arxiv.org/abs/1603.06915 | id:1603.06915 author:Diana Cai, Tamara Broderick category:stat.ML math.ST stat.ME stat.TH  published:2016-03-22 summary:Network data appear in a number of applications, such as online social networks and biological networks, and there is growing interest in both developing models for networks as well as studying the properties of such data. Since individual network datasets continue to grow in size, it is necessary to develop models that accurately represent the real-life scaling properties of networks. One behavior of interest is having a power law in the degree distribution. However, other types of power laws that have been observed empirically and considered for applications such as clustering and feature allocation models have not been studied as frequently in models for graph data. In this paper, we enumerate desirable asymptotic behavior that may be of interest for modeling graph data, including sparsity and several types of power laws. We outline a general framework for graph generative models using completely random measures; by contrast to the pioneering work of Caron and Fox (2015), we consider instantiating more of the existing atoms of the random measure as the dataset size increases rather than adding new atoms to the measure. We see that these two models can be complementary; they respectively yield interpretations as (1) time passing among existing members of a network and (2) new individuals joining a network. We detail a particular instance of this framework and show simulated results that suggest this model exhibits some desirable asymptotic power-law behavior. version:1
arxiv-1603-06898 | Edge-exchangeable graphs and sparsity | http://arxiv.org/abs/1603.06898 | id:1603.06898 author:Tamara Broderick, Diana Cai category:math.ST stat.ME stat.ML stat.TH  published:2016-03-22 summary:A known failing of many popular random graph models is that the Aldous-Hoover Theorem guarantees these graphs are dense with probability one; that is, the number of edges grows quadratically with the number of nodes. This behavior is considered unrealistic in observed graphs. We define a notion of edge exchangeability for random graphs in contrast to the established notion of infinite exchangeability for random graphs --- which has traditionally relied on exchangeability of nodes (rather than edges) in a graph. We show that, unlike node exchangeability, edge exchangeability encompasses models that are known to provide a projective sequence of random graphs that circumvent the Aldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the number of edges with the number of nodes. We show how edge-exchangeability of graphs relates naturally to existing notions of exchangeability from clustering (a.k.a. partitions) and other familiar combinatorial structures. version:1
arxiv-1603-06881 | Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of Pairwise Comparisons | http://arxiv.org/abs/1603.06881 | id:1603.06881 author:Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright category:cs.LG cs.AI cs.IT math.IT stat.ML  published:2016-03-22 summary:We study methods for aggregating pairwise comparison data in order to estimate outcome probabilities for future comparisons among a collection of n items. Working within a flexible framework that imposes only a form of strong stochastic transitivity (SST), we introduce an adaptivity index defined by the indifference sets of the pairwise comparison probabilities. In addition to measuring the usual worst-case risk of an estimator, this adaptivity index also captures the extent to which the estimator adapts to instance-specific difficulty relative to an oracle estimator. We prove three main results that involve this adaptivity index and different algorithms. First, we propose a three-step estimator termed Count-Randomize-Least squares (CRL), and show that it has adaptivity index upper bounded as $\sqrt{n}$ up to logarithmic factors. We then show that that conditional on the hardness of planted clique, no computationally efficient estimator can achieve an adaptivity index smaller than $\sqrt{n}$. Second, we show that a regularized least squares estimator can achieve a poly-logarithmic adaptivity index, thereby demonstrating a $\sqrt{n}$-gap between optimal and computationally achievable adaptivity. Finally, we prove that the standard least squares estimator, which is known to be optimally adaptive in several closely related problems, fails to adapt in the context of estimating pairwise probabilities. version:1
arxiv-1505-05114 | Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems | http://arxiv.org/abs/1505.05114 | id:1505.05114 author:Yuxin Chen, Emmanuel J. Candes category:cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH  published:2015-05-19 summary:We consider the fundamental problem of solving quadratic systems of equations in $n$ variables, where $y_i = \langle \boldsymbol{a}_i, \boldsymbol{x} \rangle ^2$, $i = 1, \ldots, m$ and $\boldsymbol{x} \in \mathbb{R}^n$ is unknown. We propose a novel method, which starting with an initial guess computed by means of a spectral method, proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach. There are several key distinguishing features, most notably, a distinct objective functional and novel update rules, which operate in an adaptive fashion and drop terms bearing too much influence on the search direction. These careful selection rules provide a tighter initial guess, better descent directions, and thus enhanced practical performance. On the theoretical side, we prove that for certain unstructured models of quadratic systems, our algorithms return the correct solution in linear time, i.e. in time proportional to reading the data $\{\boldsymbol{a}_i\}$ and $\{y_i\}$ as soon as the ratio $m/n$ between the number of equations and unknowns exceeds a fixed numerical constant. We extend the theory to deal with noisy systems in which we only have $y_i \approx \langle \boldsymbol{a}_i, \boldsymbol{x} \rangle ^2$ and prove that our algorithms achieve a statistical accuracy, which is nearly un-improvable. We complement our theoretical study with numerical examples showing that solving random quadratic systems is both computationally and statistically not much harder than solving linear systems of the same size---hence the title of this paper. For instance, we demonstrate empirically that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size. version:2
arxiv-1511-07827 | Stopping criteria for boosting automatic experimental design using real-time fMRI with Bayesian optimization | http://arxiv.org/abs/1511.07827 | id:1511.07827 author:Romy Lorenz, Ricardo P Monti, Ines R Violante, Aldo A Faisal, Christoforos Anagnostopoulos, Robert Leech, Giovanni Montana category:q-bio.NC stat.ML  published:2015-11-24 summary:Bayesian optimization has been proposed as a practical and efficient tool through which to tune parameters in many difficult settings. Recently, such techniques have been combined with real-time fMRI to propose a novel framework which turns on its head the conventional functional neuroimaging approach. This closed-loop method automatically designs the optimal experiment to evoke a desired target brain pattern. One of the challenges associated with extending such methods to real-time brain imaging is the need for adequate stopping criteria, an aspect of Bayesian optimization which has received limited attention. In light of high scanning costs and limited attentional capacities of subjects an accurate and reliable stopping criteria is essential. In order to address this issue we propose and empirically study the performance of two stopping criteria. version:2
arxiv-1603-06861 | Trading-off variance and complexity in stochastic gradient descent | http://arxiv.org/abs/1603.06861 | id:1603.06861 author:Vatsal Shah, Megasthenis Asteris, Anastasios Kyrillidis, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT math.OC  published:2016-03-22 summary:Stochastic gradient descent is the method of choice for large-scale machine learning problems, by virtue of its light complexity per iteration. However, it lags behind its non-stochastic counterparts with respect to the convergence rate, due to high variance introduced by the stochastic updates. The popular Stochastic Variance-Reduced Gradient (SVRG) method mitigates this shortcoming, introducing a new update rule which requires infrequent passes over the entire input dataset to compute the full-gradient. In this work, we propose CheapSVRG, a stochastic variance-reduction optimization scheme. Our algorithm is similar to SVRG but instead of the full gradient, it uses a surrogate which can be efficiently computed on a small subset of the input data. It achieves a linear convergence rate ---up to some error level, depending on the nature of the optimization problem---and features a trade-off between the computational complexity and the convergence rate. Empirical evaluation shows that CheapSVRG performs at least competitively compared to the state of the art. version:1
arxiv-1603-06859 | Enhanced perceptrons using contrastive biclusters | http://arxiv.org/abs/1603.06859 | id:1603.06859 author:André L. V. Coelho, Fabrício O. de França category:cs.NE cs.LG stat.ML  published:2016-03-22 summary:Perceptrons are neuronal devices capable of fully discriminating linearly separable classes. Although straightforward to implement and train, their applicability is usually hindered by non-trivial requirements imposed by real-world classification problems. Therefore, several approaches, such as kernel perceptrons, have been conceived to counteract such difficulties. In this paper, we investigate an enhanced perceptron model based on the notion of contrastive biclusters. From this perspective, a good discriminative bicluster comprises a subset of data instances belonging to one class that show high coherence across a subset of features and high differentiation from nearest instances of the other class under the same features (referred to as its contrastive bicluster). Upon each local subspace associated with a pair of contrastive biclusters a perceptron is trained and the model with highest area under the receiver operating characteristic curve (AUC) value is selected as the final classifier. Experiments conducted on a range of data sets, including those related to a difficult biosignal classification problem, show that the proposed variant can be indeed very useful, prevailing in most of the cases upon standard and kernel perceptrons in terms of accuracy and AUC measures. version:1
arxiv-1504-00548 | Learning to Understand Phrases by Embedding the Dictionary | http://arxiv.org/abs/1504.00548 | id:1504.00548 author:Felix Hill, Kyunghyun Cho, Anna Korhonen, Yoshua Bengio category:cs.CL  published:2015-04-02 summary:Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions. We present two applications of these architectures: "reverse dictionaries" that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing models that understand phrases and sentences. version:4
arxiv-1603-06829 | Multi-velocity neural networks for gesture recognition in videos | http://arxiv.org/abs/1603.06829 | id:1603.06829 author:Otkrist Gupta, Dan Raviv, Ramesh Raskar category:cs.CV cs.LG  published:2016-03-22 summary:We present a new action recognition deep neural network which adaptively learns the best action velocities in addition to the classification. While deep neural networks have reached maturity for image understanding tasks, we are still exploring network topologies and features to handle the richer environment of video clips. Here, we tackle the problem of multiple velocities in action recognition, and provide state-of-the-art results for gesture recognition, on known and new collected datasets. We further provide the training steps for our semi-supervised network, suited to learn from huge unlabeled datasets with only a fraction of labeled examples. version:1
arxiv-1602-05221 | Patterns of Scalable Bayesian Inference | http://arxiv.org/abs/1602.05221 | id:1602.05221 author:Elaine Angelino, Matthew James Johnson, Ryan P. Adams category:stat.ML  published:2016-02-16 summary:Datasets are growing not just in size but in complexity, creating a demand for rich models and quantification of uncertainty. Bayesian methods are an excellent fit for this demand, but scaling Bayesian inference is a challenge. In response to this challenge, there has been considerable recent work based on varying assumptions about model structure, underlying computational resources, and the importance of asymptotic correctness. As a result, there is a zoo of ideas with few clear overarching principles. In this paper, we seek to identify unifying principles, patterns, and intuitions for scaling Bayesian inference. We review existing work on utilizing modern computing resources with both MCMC and variational approximation techniques. From this taxonomy of ideas, we characterize the general principles that have proven successful for designing scalable inference procedures and comment on the path forward. version:2
arxiv-1603-06805 | Using real-time cluster configurations of streaming asynchronous features as online state descriptors in financial markets | http://arxiv.org/abs/1603.06805 | id:1603.06805 author:Dieter Hendricks category:q-fin.TR cs.LG q-fin.CP  published:2016-03-22 summary:We present a scheme for online, unsupervised state discovery and detection from streaming, multi-featured, asynchronous data in high-frequency financial markets. Online feature correlations are computed using an unbiased, lossless Fourier estimator. A high-speed maximum likelihood clustering algorithm is then used to find the feature cluster configuration which best explains the structure in the correlation matrix. We conjecture that this feature configuration is a candidate descriptor for the temporal state of the system. Using a simple cluster configuration similarity metric, we are able to enumerate the state space based on prevailing feature configurations. The proposed state representation removes the need for human-driven data pre-processing for state attribute specification, allowing a learning agent to find structure in streaming data, discern changes in the system, enumerate its perceived state space and learn suitable action-selection policies. version:1
arxiv-1603-06788 | Adaptive Parameter Selection in Evolutionary Algorithms by Reinforcement Learning with Dynamic Discretization of Parameter Range | http://arxiv.org/abs/1603.06788 | id:1603.06788 author:Arkady Rost, Irina Petrova, Arina Buzdalova category:cs.NE 68T05 G.1.6; I.2.6  published:2016-03-22 summary:Online parameter controllers for evolutionary algorithms adjust values of parameters during the run of an evolutionary algorithm. Recently a new efficient parameter controller based on reinforcement learning was proposed by Karafotias et al. In this method ranges of parameters are discretized into several intervals before the run. However, performing adaptive discretization during the run may increase efficiency of an evolutionary algorithm. Aleti et al. proposed another efficient controller with adaptive discretization. In the present paper we propose a parameter controller based on reinforcement learning with adaptive discretization. The proposed controller is compared with the existing parameter adjusting methods on several test problems using different configurations of an evolutionary algorithm. For the test problems, we consider four continuous functions, namely the sphere function, the Rosenbrock function, the Levi function and the Rastrigin function. Results show that the new controller outperforms the other controllers on most of the considered test problems. version:1
arxiv-1603-06785 | Multi-domain machine translation enhancements by parallel data extraction from comparable corpora | http://arxiv.org/abs/1603.06785 | id:1603.06785 author:Krzysztof Wołk, Emilia Rejmund, Krzysztof Marasek category:cs.CL stat.ML  published:2016-03-22 summary:Parallel texts are a relatively rare language resource, however, they constitute a very useful research material with a wide range of applications. This study presents and analyses new methodologies we developed for obtaining such data from previously built comparable corpora. The methodologies are automatic and unsupervised which makes them good for large scale research. The task is highly practical as non-parallel multilingual data occur much more frequently than parallel corpora and accessing them is easy, although parallel sentences are a considerably more useful resource. In this study, we propose a method of automatic web crawling in order to build topic-aligned comparable corpora, e.g. based on the Wikipedia or Euronews.com. We also developed new methods of obtaining parallel sentences from comparable data and proposed methods of filtration of corpora capable of selecting inconsistent or only partially equivalent translations. Our methods are easily scalable to other languages. Evaluation of the quality of the created corpora was performed by analysing the impact of their use on statistical machine translation systems. Experiments were presented on the basis of the Polish-English language pair for texts from different domains, i.e. lectures, phrasebooks, film dialogues, European Parliament proceedings and texts contained medicines leaflets. We also tested a second method of creating parallel corpora based on data from comparable corpora which allows for automatically expanding the existing corpus of sentences about a given domain on the basis of analogies found between them. It does not require, therefore, having past parallel resources in order to train a classifier. version:1
arxiv-1603-06782 | Doubly Random Parallel Stochastic Methods for Large Scale Learning | http://arxiv.org/abs/1603.06782 | id:1603.06782 author:Aryan Mokhtari, Alec Koppel, Alejandro Ribeiro category:cs.LG math.OC  published:2016-03-22 summary:We consider learning problems over training sets in which both, the number of training examples and the dimension of the feature vectors, are large. To solve these problems we propose the random parallel stochastic algorithm (RAPSA). We call the algorithm random parallel because it utilizes multiple processors to operate in a randomly chosen subset of blocks of the feature vector. We call the algorithm parallel stochastic because processors choose elements of the training set randomly and independently. Algorithms that are parallel in either of these dimensions exist, but RAPSA is the first attempt at a methodology that is parallel in both, the selection of blocks and the selection of elements of the training set. In RAPSA, processors utilize the randomly chosen functions to compute the stochastic gradient component associated with a randomly chosen block. The technical contribution of this paper is to show that this minimally coordinated algorithm converges to the optimal classifier when the training objective is convex. In particular, we show that: (i) When using decreasing stepsizes, RAPSA converges almost surely over the random choice of blocks and functions. (ii) When using constant stepsizes, convergence is to a neighborhood of optimality with a rate that is linear in expectation. RAPSA is numerically evaluated on the MNIST digit recognition problem. version:1
arxiv-1603-06777 | Energy-Efficient ConvNets Through Approximate Computing | http://arxiv.org/abs/1603.06777 | id:1603.06777 author:Bert Moons, Bert De Brabandere, Luc Van Gool, Marian Verhelst category:cs.CV  published:2016-03-22 summary:Recently ConvNets or convolutional neural networks (CNN) have come up as state-of-the-art classification and detection algorithms, achieving near-human performance in visual detection. However, ConvNet algorithms are typically very computation and memory intensive. In order to be able to embed ConvNet-based classification into wearable platforms and embedded systems such as smartphones or ubiquitous electronics for the internet-of-things, their energy consumption should be reduced drastically. This paper proposes methods based on approximate computing to reduce energy consumption in state-of-the-art ConvNet accelerators. By combining techniques both at the system- and circuit level, we can gain energy in the systems arithmetic: up to 30x without losing classification accuracy and more than 100x at 99% classification accuracy, compared to the commonly used 16-bit fixed point number format. version:1
arxiv-1603-06759 | Convolution in Convolution for Network in Network | http://arxiv.org/abs/1603.06759 | id:1603.06759 author:Yanwei Pang, Manli Sun, Xiaoheng Jiang, Xuelong Li category:cs.CV  published:2016-03-22 summary:Network in Netwrok (NiN) is an effective instance and an important extension of Convolutional Neural Network (CNN) consisting of alternating convolutional layers and pooling layers. Instead of using a linear filter for convolution, NiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, to replace the linear filter. Because of the powerfulness of MLP and $ 1\times 1 $ convolutions in spatial domain, NiN has stronger ability of feature representation and hence results in better recognition rate. However, MLP itself consists of fully connected layers which give rise to a large number of parameters. In this paper, we propose to replace dense shallow MLP with sparse shallow MLP. One or more layers of the sparse shallow MLP are sparely connected in the channel dimension or channel-spatial domain. The proposed method is implemented by applying unshared convolution across the channel dimension and applying shared convolution across the spatial dimension in some computational layers. The proposed method is called CiC. Experimental results on the CIFAR10 dataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate the effectiveness of the proposed CiC method. version:1
arxiv-1511-09231 | Design of Kernels in Convolutional Neural Networks for Image Classification | http://arxiv.org/abs/1511.09231 | id:1511.09231 author:Zhun Sun, Mete Ozay, Takayuki Okatani category:cs.CV  published:2015-11-30 summary:Despite the effectiveness of Convolutional Neural Networks (CNNs) for image classification, our understanding of the relationship between shape of convolution kernels and learned representations is limited. In this work, we explore and employ the relationship between shape of kernels which define Receptive Fields (RFs) in CNNs for learning of feature representations and image classification. For this purpose, we first propose a feature visualization method for visualization of pixel-wise classification score maps of learned features. Motivated by our experimental results, and observations reported in the literature for modeling of visual systems, we propose a novel design of shape of kernels for learning of representations in CNNs. In the experimental results, we achieved a state-of-the-art classification performance compared to a base CNN model [28] by reducing the number of parameters and computational time of the model using the ILSVRC-2012 dataset [24]. The proposed models also outperform the state-of-the-art models employed on the CIFAR-10/100 datasets [12] for image classification. Additionally, we analyzed the robustness of the proposed method to occlusion for classification of partially occluded images compared with the state-of-the-art methods. Our results indicate the effectiveness of the proposed approach. version:2
arxiv-1512-01881 | Recognition from Hand Cameras | http://arxiv.org/abs/1512.01881 | id:1512.01881 author:Cheng-Sheng Chan, Shou-Zhong Chen, Pei-Xuan Xie, Chiung-Chih Chang, Min Sun category:cs.CV  published:2015-12-07 summary:We revisit the study of a wrist-mounted camera system (referred to as HandCam) for recognizing activities of hands. HandCam has two unique properties as compared to egocentric systems (referred to as HeadCam): (1) it avoids the need to detect hands; (2) it more consistently observes the activities of hands. By taking advantage of these properties, we propose a deep-learning-based method to recognize hand states (free v.s. active hands, hand gestures, object categories), and discover object categories. Moreover, we propose a novel two-streams deep network to further take advantage of both HandCam and HeadCam. We have collected a new synchronized HandCam and HeadCam dataset with 20 videos captured in three scenes for hand states recognition. Experiments show that our HandCam system consistently outperforms a deep-learning-based HeadCam method (with estimated manipulation regions) and a dense-trajectory-based HeadCam method in all tasks. We also show that HandCam videos captured by different users can be easily aligned to improve free v.s. active recognition accuracy (3.3% improvement) in across-scenes use case. Moreover, we observe that finetuning Convolutional Neural Network consistently improves accuracy. Finally, our novel two-streams deep network combining HandCam and HeadCam features achieves the best performance in four out of five tasks. With more data, we believe a joint HandCam and HeadCam system can robustly log hand states in daily life. version:3
arxiv-1603-06680 | Image Super-Resolution Based on Sparsity Prior via Smoothed $l_0$ Norm | http://arxiv.org/abs/1603.06680 | id:1603.06680 author:Mohammad Rostami, Zhou Wang category:cs.CV  published:2016-03-22 summary:In this paper we aim to tackle the problem of reconstructing a high-resolution image from a single low-resolution input image, known as single image super-resolution. In the literature, sparse representation has been used to address this problem, where it is assumed that both low-resolution and high-resolution images share the same sparse representation over a pair of coupled jointly trained dictionaries. This assumption enables us to use the compressed sensing theory to find the jointly sparse representation via the low-resolution image and then use it to recover the high-resolution image. However, sparse representation of a signal over a known dictionary is an ill-posed, combinatorial optimization problem. Here we propose an algorithm that adopts the smoothed $l_0$-norm (SL0) approach to find the jointly sparse representation. Improved quality of the reconstructed image is obtained for most images in terms of both peak signal-to-noise-ratio (PSNR) and structural similarity (SSIM) measures. version:1
arxiv-1603-06678 | Stitching Stabilizer: Two-frame-stitching Video Stabilization for Embedded Systems | http://arxiv.org/abs/1603.06678 | id:1603.06678 author:Masaki Satoh category:cs.CV  published:2016-03-22 summary:In conventional electronic video stabilization, the stabilized frame is obtained by cropping the input frame to cancel camera shake. While a small cropping size results in strong stabilization, it does not provide us satisfactory results from the viewpoint of image quality, because it narrows the angle of view. By fusing several frames, we can effectively expand the area of input frames, and achieve strong stabilization even with a large cropping size. Several methods for doing so have been studied. However, their computational costs are too high for embedded systems such as smartphones. We propose a simple, yet surprisingly effective algorithm, called the stitching stabilizer. It stitches only two frames together with a minimal computational cost. It can achieve real-time processes in embedded systems, for Full HD and 30 FPS videos. To clearly show the effect, we apply it to hyperlapse. Using several clips, we show it produces more strongly stabilized and natural results than the existing solutions from Microsoft and Instagram. version:1
arxiv-1603-06677 | Learning Executable Semantic Parsers for Natural Language Understanding | http://arxiv.org/abs/1603.06677 | id:1603.06677 author:Percy Liang category:cs.CL cs.AI  published:2016-03-22 summary:For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are interested in learning semantic parsers from data, which introduces a new layer of statistical and computational issues. This article lays out the components of a statistical semantic parser, highlighting the key challenges. We will see that semantic parsing is a rich fusion of the logical and the statistical world, and that this fusion will play an integral role in the future of natural language understanding systems. version:1
arxiv-1603-06669 | Implementation of a FPGA-Based Feature Detection and Networking System for Real-time Traffic Monitoring | http://arxiv.org/abs/1603.06669 | id:1603.06669 author:Jieshi Chen, Benjamin Carrion Schafer, Ivan Wang-Hei Ho category:cs.CV  published:2016-03-22 summary:With the growing demand of real-time traffic monitoring nowadays, software-based image processing can hardly meet the real-time data processing requirement due to the serial data processing nature. In this paper, the implementation of a hardware-based feature detection and networking system prototype for real-time traffic monitoring as well as data transmission is presented. The hardware architecture of the proposed system is mainly composed of three parts: data collection, feature detection, and data transmission. Overall, the presented prototype can tolerate a high data rate of about 60 frames per second. By integrating the feature detection and data transmission functions, the presented system can be further developed for various VANET application scenarios to improve road safety and traffic efficiency. For example, detection of vehicles that violate traffic rules, parking enforcement, etc. version:1
arxiv-1603-06668 | Learning Representations for Automatic Colorization | http://arxiv.org/abs/1603.06668 | id:1603.06668 author:Gustav Larsson, Michael Maire, Gregory Shakhnarovich category:cs.CV  published:2016-03-22 summary:We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations during colorization. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation; our experiments consider both scenarios. On both fully and partially automatic colorization tasks, our system significantly outperforms all existing methods. version:1
arxiv-1603-06665 | Information Processing by Nonlinear Phase Dynamics in Locally Connected Arrays | http://arxiv.org/abs/1603.06665 | id:1603.06665 author:Richard A. Kiehl category:cs.NE cs.ET  published:2016-03-22 summary:Research toward powerful information processing systems that circumvent the interconnect bottleneck by exploiting the nonlinear evolution of multiple phase dynamics in locally connected arrays is discussed. We focus on a scheme in which logic states are defined by the electrical phase of a dynamic process and information processing is realized through interactions between the elements in the array. Simulation results are given for networks comprised of neuron-like integrate-and-fire elements, which could potentially be implemented by ultra-small tunnel junctions, molecules and other types of nanoscale elements. This approach could lead to powerful information processing systems due to massive parallelism in simple, highly scalable nano-architectures. The rational for this approach, its advantages, simulation results, critical issues, and future research directions are discussed. version:1
arxiv-1603-06655 | Input Aggregated Network for Face Video Representation | http://arxiv.org/abs/1603.06655 | id:1603.06655 author:Zhen Dong, Su Jia, Chi Zhang, Mingtao Pei category:cs.CV  published:2016-03-22 summary:Recently, deep neural network has shown promising performance in face image recognition. The inputs of most networks are face images, and there is hardly any work reported in literature on network with face videos as input. To sufficiently discover the useful information contained in face videos, we present a novel network architecture called input aggregated network which is able to learn fixed-length representations for variable-length face videos. To accomplish this goal, an aggregation unit is designed to model a face video with various frames as a point on a Riemannian manifold, and the mapping unit aims at mapping the point into high-dimensional space where face videos belonging to the same subject are close-by and others are distant. These two units together with the frame representation unit build an end-to-end learning system which can learn representations of face videos for the specific tasks. Experiments on two public face video datasets demonstrate the effectiveness of the proposed network. version:1
arxiv-1603-06653 | Information Theoretic-Learning Auto-Encoder | http://arxiv.org/abs/1603.06653 | id:1603.06653 author:Eder Santana, Matthew Emigh, Jose C Principe category:cs.LG  published:2016-03-22 summary:We propose Information Theoretic-Learning (ITL) divergence measures for variational regularization of neural networks. We also explore ITL-regularized autoencoders as an alternative to variational autoencoding bayes, adversarial autoencoders and generative adversarial networks for randomly generating sample data without explicitly defining a partition function. This paper also formalizes, generative moment matching networks under the ITL framework. version:1
arxiv-1512-06110 | Morphological Inflection Generation Using Character Sequence to Sequence Learning | http://arxiv.org/abs/1512.06110 | id:1512.06110 author:Manaal Faruqui, Yulia Tsvetkov, Graham Neubig, Chris Dyer category:cs.CL  published:2015-12-18 summary:Morphological inflection generation is the task of generating the inflected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inflection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inflection generation. version:3
arxiv-1603-06169 | Towards Automatic Wild Animal Monitoring: Identification of Animal Species in Camera-trap Images using Very Deep Convolutional Neural Networks | http://arxiv.org/abs/1603.06169 | id:1603.06169 author:Alexander Gomez, Augusto Salazar, Francisco Vargas category:cs.CV  published:2016-03-20 summary:Non intrusive monitoring of animals in the wild is possible using camera trapping framework, which uses cameras triggered by sensors to take a burst of images of animals in their habitat. However camera trapping framework produces a high volume of data (in the order on thousands or millions of images), which must be analyzed by a human expert. In this work, a method for animal species identification in the wild using very deep convolutional neural networks is presented. Multiple versions of the Snapshot Serengeti dataset were used in order to probe the ability of the method to cope with different challenges that camera-trap images demand. The method reached 88.9% of accuracy in Top-1 and 98.1% in Top-5 in the evaluation set using a residual network topology. Also, the results show that the proposed method outperforms previous approximations and proves that recognition in camera-trap images can be automated. version:2
arxiv-1603-06182 | Modelling Temporal Information Using Discrete Fourier Transform for Video Classification | http://arxiv.org/abs/1603.06182 | id:1603.06182 author:Haimin Zhang, Min Xu, Changsheng Xu, Ramesh Jain category:cs.CV  published:2016-03-20 summary:Recently, video classification attracts intensive research efforts. However, most existing works are based on framelevel visual features, which might fail to model the temporal information, e.g. characteristics accumulated along time. In order to capture video temporal information, we propose to analyse features in frequency domain transformed by discrete Fourier transform (DFT features). Frame-level features are firstly extract by a pre-trained deep convolutional neural network (CNN). Then, time domain features are transformed and interpolated into DFT features. CNN and DFT features are further encoded by using different pooling methods and fused for video classification. In this way, static image features extracted from a pre-trained deep CNN and temporal information represented by DFT features are jointly considered for video classification. We test our method for video emotion classification and action recognition. Experimental results demonstrate that combining DFT features can effectively capture temporal information and therefore improve the performance of both video emotion classification and action recognition. Our approach has achieved a state-of-the-art performance on the largest video emotion dataset (VideoEmotion-8 dataset) and competitive results on UCF-101. version:2
arxiv-1603-06624 | Variational Autoencoders for Feature Detection of Magnetic Resonance Imaging Data | http://arxiv.org/abs/1603.06624 | id:1603.06624 author:R. Devon Hjelm, Sergey M. Plis, Vince C. Calhoun category:cs.LG cs.NE stat.ML  published:2016-03-21 summary:Independent component analysis (ICA), as an approach to the blind source-separation (BSS) problem, has become the de-facto standard in many medical imaging settings. Despite successes and a large ongoing research effort, the limitation of ICA to square linear transformations have not been overcome, so that general INFOMAX is still far from being realized. As an alternative, we present feature analysis in medical imaging as a problem solved by Helmholtz machines, which include dimensionality reduction and reconstruction of the raw data under the same objective, and which recently have overcome major difficulties in inference and learning with deep and nonlinear configurations. We demonstrate one approach to training Helmholtz machines, variational auto-encoders (VAE), as a viable approach toward feature extraction with magnetic resonance imaging (MRI) data. version:1
arxiv-1511-01473 | How Robust are Reconstruction Thresholds for Community Detection? | http://arxiv.org/abs/1511.01473 | id:1511.01473 author:Ankur Moitra, William Perry, Alexander S. Wein category:cs.DS cs.IT cs.LG math.IT math.PR stat.ML  published:2015-11-04 summary:The stochastic block model is one of the oldest and most ubiquitous models for studying clustering and community detection. In an exciting sequence of developments, motivated by deep but non-rigorous ideas from statistical physics, Decelle et al. conjectured a sharp threshold for when community detection is possible in the sparse regime. Mossel, Neeman and Sly and Massoulie proved the conjecture and gave matching algorithms and lower bounds. Here we revisit the stochastic block model from the perspective of semirandom models where we allow an adversary to make `helpful' changes that strengthen ties within each community and break ties between them. We show a surprising result that these `helpful' changes can shift the information-theoretic threshold, making the community detection problem strictly harder. We complement this by showing that an algorithm based on semidefinite programming (which was known to get close to the threshold) continues to work in the semirandom model (even for partial recovery). This suggests that algorithms based on semidefinite programming are robust in ways that any algorithm meeting the information-theoretic threshold cannot be. These results point to an interesting new direction: Can we find robust, semirandom analogues to some of the classical, average-case thresholds in statistics? We also explore this question in the broadcast tree model, and we show that the viewpoint of semirandom models can help explain why some algorithms are preferred to others in practice, in spite of the gaps in their statistical performance on random models. version:2
arxiv-1603-08296 | The SVM Classifier Based on the Modified Particle Swarm Optimization | http://arxiv.org/abs/1603.08296 | id:1603.08296 author:L. Demidova, E. Nikulchev, Yu. Sokolova category:cs.LG cs.NE  published:2016-03-21 summary:The problem of development of the SVM classifier based on the modified particle swarm optimization has been considered. This algorithm carries out the simultaneous search of the kernel function type, values of the kernel function parameters and value of the regularization parameter for the SVM classifier. Such SVM classifier provides the high quality of data classification. The idea of particles' {\guillemotleft}regeneration{\guillemotright} is put on the basis of the modified particle swarm optimization algorithm. At the realization of this idea, some particles change their kernel function type to the one which corresponds to the particle with the best value of the classification accuracy. The offered particle swarm optimization algorithm allows reducing the time expenditures for development of the SVM classifier. The results of experimental studies confirm the efficiency of this algorithm. version:1
arxiv-1511-04031 | Facial Landmark Detection with Tweaked Convolutional Neural Networks | http://arxiv.org/abs/1511.04031 | id:1511.04031 author:Yue Wu, Tal Hassner, KangGeon Kim, Gerard Medioni, Prem Natarajan category:cs.CV  published:2015-11-12 summary:We present a novel convolutional neural network (CNN) design for facial landmark coordinate regression. We examine the intermediate features of a standard CNN trained for landmark detection and show that features extracted from later, more specialized layers capture rough landmark locations. This provides a natural means of applying differential treatment midway through the network, tweaking processing based on facial alignment. The resulting Tweaked CNN model (TCNN) harnesses the robustness of CNNs for landmark detection, in an appearance-sensitive manner without training multi-part or multi-scale models. Our results on standard face landmark detection and face verification benchmarks show TCNN to surpasses previously published performances by wide margins. version:2
arxiv-1603-06560 | Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits | http://arxiv.org/abs/1603.06560 | id:1603.06560 author:Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar category:cs.LG stat.ML  published:2016-03-21 summary:Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where allocation of additional resources to an arm corresponds to training a configuration on larger subsets of the data. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. We compare Hyperband with state-of-the-art Bayesian optimization methods and a random search baseline on a comprehensive benchmark including 117 datasets. Our results on this benchmark demonstrate that while Bayesian optimization methods do not outperform random search trained for twice as long, Hyperband in favorable settings offers valuable speedups. version:1
arxiv-1603-06554 | Action-Affect Classification and Morphing using Multi-Task Representation Learning | http://arxiv.org/abs/1603.06554 | id:1603.06554 author:Timothy J. Shields, Mohamed R. Amer, Max Ehrlich, Amir Tamrakar category:cs.CV cs.AI cs.HC cs.LG  published:2016-03-21 summary:Most recent work focused on affect from facial expressions, and not as much on body. This work focuses on body affect analysis. Affect does not occur in isolation. Humans usually couple affect with an action in natural interactions; for example, a person could be talking and smiling. Recognizing body affect in sequences requires efficient algorithms to capture both the micro movements that differentiate between happy and sad and the macro variations between different actions. We depart from traditional approaches for time-series data analytics by proposing a multi-task learning model that learns a shared representation that is well-suited for action-affect classification as well as generation. For this paper we choose Conditional Restricted Boltzmann Machines to be our building block. We propose a new model that enhances the CRBM model with a factored multi-task component to become Multi-Task Conditional Restricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two publicly available datasets, the Body Affect dataset and the Tower Game dataset, and show superior classification performance improvement over the state-of-the-art, as well as the generative abilities of our model. version:1
arxiv-1503-09105 | Two Timescale Stochastic Approximation with Controlled Markov noise and Off-policy temporal difference learning | http://arxiv.org/abs/1503.09105 | id:1503.09105 author:Prasenjit Karmakar, Shalabh Bhatnagar category:math.DS cs.AI stat.ML  published:2015-03-31 summary:We present for the first time an asymptotic convergence analysis of two time-scale stochastic approximation driven by `controlled' Markov noise. In particular, both the faster and slower recursions have non-additive controlled Markov noise components in addition to martingale difference noise. We analyze the asymptotic behavior of our framework by relating it to limiting differential inclusions in both time-scales that are defined in terms of the ergodic occupation measures associated with the controlled Markov processes. Finally, we present a solution to the off-policy convergence problem for temporal difference learning with linear function approximation, using our results. version:11
arxiv-1603-06541 | A Comparison Study of Nonlinear Kernels | http://arxiv.org/abs/1603.06541 | id:1603.06541 author:Ping Li category:stat.ML cs.LG  published:2016-03-21 summary:In this paper, we compare 5 different nonlinear kernels: min-max, RBF, fRBF (folded RBF), acos, and acos-$\chi^2$, on a wide range of publicly available datasets. The proposed fRBF kernel performs very similarly to the RBF kernel. Both RBF and fRBF kernels require an important tuning parameter ($\gamma$). Interestingly, for a significant portion of the datasets, the min-max kernel outperforms the best-tuned RBF/fRBF kernels. The acos kernel and acos-$\chi^2$ kernel also perform well in general and in some datasets achieve the best accuracies. One crucial issue with the use of nonlinear kernels is the excessive computational and memory cost. These days, one increasingly popular strategy is to linearize the kernels through various randomization algorithms. In our study, the randomization method for the min-max kernel demonstrates excellent performance compared to the randomization methods for other types of nonlinear kernels, measured in terms of the number of nonzero terms in the transformed dataset. Our study provides evidence for supporting the use of the min-max kernel and the corresponding randomized linearization method (i.e., the so-called "0-bit CWS"). Furthermore, the results motivate at least two directions for future research: (i) To develop new (and linearizable) nonlinear kernels for better accuracies; and (ii) To develop better linearization algorithms for improving the current linearization methods for the RBF kernel, the acos kernel, and the acos-$\chi^2$ kernel. One attempt is to combine the min-max kernel with the acos kernel or the acos-$\chi^2$ kernel. The advantages of these two new and tuning-free nonlinear kernels are demonstrated vias our extensive experiments. version:1
arxiv-1603-06531 | Deep video gesture recognition using illumination invariants | http://arxiv.org/abs/1603.06531 | id:1603.06531 author:Otkrist Gupta, Dan Raviv, Ramesh Raskar category:cs.CV cs.LG  published:2016-03-21 summary:In this paper we present architectures based on deep neural nets for gesture recognition in videos, which are invariant to local scaling. We amalgamate autoencoder and predictor architectures using an adaptive weighting scheme coping with a reduced size labeled dataset, while enriching our models from enormous unlabeled sets. We further improve robustness to lighting conditions by introducing a new adaptive filer based on temporal local scale normalization. We provide superior results over known methods, including recent reported approaches based on neural nets. version:1
arxiv-1402-1298 | Phase transitions and sample complexity in Bayes-optimal matrix factorization | http://arxiv.org/abs/1402.1298 | id:1402.1298 author:Yoshiyuki Kabashima, Florent Krzakala, Marc Mézard, Ayaka Sakata, Lenka Zdeborová category:cs.NA cond-mat.stat-mech cs.IT cs.LG math.IT stat.ML  published:2014-02-06 summary:We analyse the matrix factorization problem. Given a noisy measurement of a product of two matrices, the problem is to estimate back the original matrices. It arises in many applications such as dictionary learning, blind matrix calibration, sparse principal component analysis, blind source separation, low rank matrix completion, robust principal component analysis or factor analysis. It is also important in machine learning: unsupervised representation learning can often be studied through matrix factorization. We use the tools of statistical mechanics - the cavity and replica methods - to analyze the achievability and computational tractability of the inference problems in the setting of Bayes-optimal inference, which amounts to assuming that the two matrices have random independent elements generated from some known distribution, and this information is available to the inference algorithm. In this setting, we compute the minimal mean-squared-error achievable in principle in any computational time, and the error that can be achieved by an efficient approximate message passing algorithm. The computation is based on the asymptotic state-evolution analysis of the algorithm. The performance that our analysis predicts, both in terms of the achieved mean-squared-error, and in terms of sample complexity, is extremely promising and motivating for a further development of the algorithm. version:3
arxiv-1603-06503 | Static and Dynamic Feature Selection in Morphosyntactic Analyzers | http://arxiv.org/abs/1603.06503 | id:1603.06503 author:Bernd Bohnet, Miguel Ballesteros, Ryan McDonald, Joakim Nivre category:cs.CL  published:2016-03-21 summary:We study the use of greedy feature selection methods for morphosyntactic tagging under a number of different conditions. We compare a static ordering of features to a dynamic ordering based on mutual information statistics, and we apply the techniques to standalone taggers as well as joint systems for tagging and parsing. Experiments on five languages show that feature selection can result in more compact models as well as higher accuracy under all conditions, but also that a dynamic ordering works better than a static ordering and that joint systems benefit more than standalone taggers. We also show that the same techniques can be used to select which morphosyntactic categories to predict in order to maximize syntactic accuracy in a joint system. Our final results represent a substantial improvement of the state of the art for several languages, while at the same time reducing both the number of features and the running time by up to 80% in some cases. version:1
arxiv-1603-06496 | Instance Influence Estimation for Hyperspectral Target Signature Characterization using Extended Functions of Multiple Instances | http://arxiv.org/abs/1603.06496 | id:1603.06496 author:Sheng Zou, Alina Zare category:cs.CV  published:2016-03-21 summary:The Extended Functions of Multiple Instances (eFUMI) algorithm is a generalization of Multiple Instance Learning (MIL). In eFUMI, only bag level (i.e. set level) labels are needed to estimate target signatures from mixed data. The training bags in eFUMI are labeled positive if any data point in a bag contains or represents any proportion of the target signature and are labeled as a negative bag if all data points in the bag do not represent any target. From these imprecise labels, eFUMI has been shown to be effective at estimating target signatures in hyperspectral subpixel target detection problems. One motivating scenario for the use of eFUMI is where an analyst circles objects/regions of interest in a hyperspectral scene such that the target signatures of these objects can be estimated and be used to determine whether other instances of the object appear elsewhere in the image collection. The regions highlighted by the analyst serve as the imprecise labels for eFUMI. Often, an analyst may want to iteratively refine their imprecise labels. In this paper, we present an approach for estimating the influence on the estimated target signature if the label for a particular input data point is modified. This "instance influence estimation" guides an analyst to focus on (re-)labeling the data points that provide the largest change in the resulting estimated target signature and, thus, reduce the amount of time an analyst needs to spend refining the labels for a hyperspectral scene. Results are shown on real hyperspectral sub-pixel target detection data sets. version:1
arxiv-1603-06485 | A System for Probabilistic Linking of Thesauri and Classification Systems | http://arxiv.org/abs/1603.06485 | id:1603.06485 author:Lisa Posch, Philipp Schaer, Arnim Bleier, Markus Strohmaier category:cs.AI cs.CL cs.DL  published:2016-03-21 summary:This paper presents a system which creates and visualizes probabilistic semantic links between concepts in a thesaurus and classes in a classification system. For creating the links, we build on the Polylingual Labeled Topic Model (PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in the classification system by using information from the natural language text of documents, their assigned thesaurus descriptors and their designated classes. The links are then presented to users of the system in an interactive visualization, providing them with an automatically generated overview of the relations between the thesaurus and the classification system. version:1
arxiv-1603-06478 | Hard-Clustering with Gaussian Mixture Models | http://arxiv.org/abs/1603.06478 | id:1603.06478 author:Johannes Blömer, Sascha Brauer, Kathrin Bujna category:cs.LG cs.DS  published:2016-03-21 summary:Training the parameters of statistical models to describe a given data set is a central task in the field of data mining and machine learning. A very popular and powerful way of parameter estimation is the method of maximum likelihood estimation (MLE). Among the most widely used families of statistical models are mixture models, especially, mixtures of Gaussian distributions. A popular hard-clustering variant of the MLE problem is the so-called complete-data maximum likelihood estimation (CMLE) method. The standard approach to solve the CMLE problem is the Classification-Expectation-Maximization (CEM) algorithm. Unfortunately, it is only guaranteed that the algorithm converges to some (possibly arbitrarily poor) stationary point of the objective function. In this paper, we present two algorithms for a restricted version of the CMLE problem. That is, our algorithms approximate reasonable solutions to the CMLE problem which satisfy certain natural properties. Moreover, they compute solutions whose cost (i.e. complete-data log-likelihood values) are at most a factor $(1+\epsilon)$ worse than the cost of the solutions that we search for. Note the CMLE problem in its most general, i.e. unrestricted, form is not well defined and allows for trivial optimal solutions that can be thought of as degenerated solutions. version:1
arxiv-1603-06470 | Frankenstein: Learning Deep Face Representations using Small Data | http://arxiv.org/abs/1603.06470 | id:1603.06470 author:Guosheng Hu, Xiaojiang Peng, Yongxin Yang, Timothy Hospedales, Jakob Verbeek category:cs.CV  published:2016-03-21 summary:Deep convolutional neural networks have recently proven extremely effective for difficult face recognition problems in uncontrolled settings. To train such networks very large training sets are needed with millions of labeled images. For some applications, such as near-infrared (NIR) face recognition, such larger training datasets are, however, not publicly available and very difficult to collect. We propose a method to generate very large training datasets of synthetic images by compositing real face images in a given dataset. We show that this method enables to learn models from as few as 10,000 training images, which perform on par with models trained from 500,000 images. Using our approach we also improve the state-of-the-art results on the CASIA NIR-VIS heterogeneous face recognition dataset. version:1
arxiv-1603-06433 | Illumination-invariant image mosaic calculation based on logarithmic search | http://arxiv.org/abs/1603.06433 | id:1603.06433 author:Wolfgang Konen category:cs.CV  published:2016-03-21 summary:This technical report describes an improved image mosaicking algorithm. It is based on Jain's logarithmic search algorithm [Jain 1981] which is coupled to the method of Kourogi (1999} for matching images in a video sequence. Logarithmic search has a better invariance against illumination changes than the original optical-flow-based method of Kourogi. version:1
arxiv-1603-06432 | Beyond Sharing Weights for Deep Domain Adaptation | http://arxiv.org/abs/1603.06432 | id:1603.06432 author:Artem Rozantsev, Mathieu Salzmann, Pascal Fua category:cs.CV  published:2016-03-21 summary:Deep Neural Networks have demonstrated outstanding performance in many Computer Vision tasks but typically require large amounts of labeled training data to achieve it. This is a serious limitation when such data is difficult to obtain. In traditional Machine Learning, Domain Adaptation is an approach to overcoming this problem by leveraging annotated data from a source domain, in which it is abundant, to train a classifier to operate in a target domain, in which labeled data is either sparse or even lacking altogether. In the Deep Learning case, most existing methods use the same architecture with the same weights for both source and target data, which essentially amounts to learning domain invariant features. Here, we show that it is more effective to explicitly model the shift from one domain to the other. To this end, we introduce a two-stream architecture, one of which operates in the source domain and the other in the target domain. In contrast to other approaches, the weights in corresponding layers are related but not shared to account for differences between the two domains. We demonstrate that this both yields higher accuracy than state-of-the-art methods on several object recognition and detection tasks and consistently outperforms networks with shared weights in both supervised and unsupervised settings. version:1
arxiv-1603-05073 | Descriptor transition tables for object retrieval using unconstrained cluttered video acquired using a consumer level handheld mobile device | http://arxiv.org/abs/1603.05073 | id:1603.05073 author:Warren Rieutort-Louis, Ognjen Arandjelovic category:cs.CV  published:2016-03-16 summary:Visual recognition and vision based retrieval of objects from large databases are tasks with a wide spectrum of potential applications. In this paper we propose a novel recognition method from video sequences suitable for retrieval from databases acquired in highly unconstrained conditions e.g. using a mobile consumer-level device such as a phone. On the lowest level, we represent each sequence as a 3D mesh of densely packed local appearance descriptors. While image plane geometry is captured implicitly by a large overlap of neighbouring regions from which the descriptors are extracted, 3D information is extracted by means of a descriptor transition table, learnt from a single sequence for each known gallery object. These allow us to connect local descriptors along the 3rd dimension (which corresponds to viewpoint changes), thus resulting in a set of variable length Markov chains for each video. The matching of two sets of such chains is formulated as a statistical hypothesis test, whereby a subset of each is chosen to maximize the likelihood that the corresponding video sequences show the same object. The effectiveness of the proposed algorithm is empirically evaluated on the Amsterdam Library of Object Images and a new highly challenging video data set acquired using a mobile phone. On both data sets our method is shown to be successful in recognition in the presence of background clutter and large viewpoint changes. version:2
arxiv-1603-06398 | Appearance Harmonization for Single Image Shadow Removal | http://arxiv.org/abs/1603.06398 | id:1603.06398 author:Liqian Ma, Jue Wang, Eli Shechtman, Kalyan Sunkavalli, Shimin Hu category:cs.CV  published:2016-03-21 summary:Shadows often create unwanted artifacts in photographs, and removing them can be very challenging. Previous shadow removal methods often produce de-shadowed regions that are visually inconsistent with the rest of the image. In this work we propose a fully automatic shadow region harmonization approach that improves the appearance compatibility of the de-shadowed region as typically produced by previous methods. It is based on a shadow-guided patch-based image synthesis approach that reconstructs the shadow region using patches sampled from non-shadowed regions. The result is then refined based on the reconstruction confidence to handle unique image patterns. Many shadow removal results and comparisons are show the effectiveness of our improvement. Quantitative evaluation on a benchmark dataset suggests that our automatic shadow harmonization approach effectively improves upon the state-of-the-art. version:1
arxiv-1503-00164 | Analysis of Crowdsourced Sampling Strategies for HodgeRank with Sparse Random Graphs | http://arxiv.org/abs/1503.00164 | id:1503.00164 author:Braxton Osting, Jiechao Xiong, Qianqian Xu, Yuan Yao category:stat.ML cs.LG  published:2015-02-28 summary:Crowdsourcing platforms are now extensively used for conducting subjective pairwise comparison studies. In this setting, a pairwise comparison dataset is typically gathered via random sampling, either \emph{with} or \emph{without} replacement. In this paper, we use tools from random graph theory to analyze these two random sampling methods for the HodgeRank estimator. Using the Fiedler value of the graph as a measurement for estimator stability (informativeness), we provide a new estimate of the Fiedler value for these two random graph models. In the asymptotic limit as the number of vertices tends to infinity, we prove the validity of the estimate. Based on our findings, for a small number of items to be compared, we recommend a two-stage sampling strategy where a greedy sampling method is used initially and random sampling \emph{without} replacement is used in the second stage. When a large number of items is to be compared, we recommend random sampling with replacement as this is computationally inexpensive and trivially parallelizable. Experiments on synthetic and real-world datasets support our analysis. version:2
arxiv-1402-5874 | Predictive Interval Models for Non-parametric Regression | http://arxiv.org/abs/1402.5874 | id:1402.5874 author:Mohammad Ghasemi Hamed, Mathieu Serrurier, Nicolas Durand category:cs.LG stat.ML  published:2014-02-24 summary:Having a regression model, we are interested in finding two-sided intervals that are guaranteed to contain at least a desired proportion of the conditional distribution of the response variable given a specific combination of predictors. We name such intervals predictive intervals. This work presents a new method to find two-sided predictive intervals for non-parametric least squares regression without the homoscedasticity assumption. Our predictive intervals are built by using tolerance intervals on prediction errors in the query point's neighborhood. We proposed a predictive interval model test and we also used it as a constraint in our hyper-parameter tuning algorithm. This gives an algorithm that finds the smallest reliable predictive intervals for a given dataset. We also introduce a measure for comparing different interval prediction methods yielding intervals having different size and coverage. These experiments show that our methods are more reliable, effective and precise than other interval prediction methods. version:2
arxiv-1509-05936 | STDP as presynaptic activity times rate of change of postsynaptic activity | http://arxiv.org/abs/1509.05936 | id:1509.05936 author:Yoshua Bengio, Thomas Mesnard, Asja Fischer, Saizheng Zhang, Yuhuai Wu category:cs.NE cs.LG q-bio.NC  published:2015-09-19 summary:We introduce a weight update formula that is expressed only in terms of firing rates and their derivatives and that results in changes consistent with those associated with spike-timing dependent plasticity (STDP) rules and biological observations, even though the explicit timing of spikes is not needed. The new rule changes a synaptic weight in proportion to the product of the presynaptic firing rate and the temporal rate of change of activity on the postsynaptic side. These quantities are interesting for studying theoretical explanation for synaptic changes from a machine learning perspective. In particular, if neural dynamics moved neural activity towards reducing some objective function, then this STDP rule would correspond to stochastic gradient descent on that objective function. version:2
arxiv-1603-06374 | Analyzing coevolutionary games with dynamic fitness landscapes | http://arxiv.org/abs/1603.06374 | id:1603.06374 author:Hendrik Richter category:q-bio.PE cs.GT cs.NE  published:2016-03-21 summary:Coevolutionary games cast players that may change their strategies as well as their networks of interaction. In this paper a framework is introduced for describing coevolutionary game dynamics by landscape models. It is shown that coevolutionary games invoke dynamic landscapes. Numerical experiments are shown for a prisoner's dilemma (PD) and a snow drift (SD) game that both use either birth-death (BD) or death-birth (DB) strategy updating. The resulting landscapes are analyzed with respect to modality and ruggedness version:1
arxiv-1603-06359 | Unified Depth Prediction and Intrinsic Image Decomposition from a Single Image via Joint Convolutional Neural Fields | http://arxiv.org/abs/1603.06359 | id:1603.06359 author:Seungryong Kim, Kihong Park, Kwanghoon Sohn, Stephen Lin category:cs.CV  published:2016-03-21 summary:We present a method for jointly predicting a depth map and intrinsic images from single-image input. The two tasks are formulated in a synergistic manner through a joint conditional random field (CRF) that is solved using a novel convolutional neural network (CNN) architecture, called the joint convolutional neural field (JCNF) model. Tailored to our joint estimation problem, JCNF differs from previous CNNs in its sharing of convolutional activations and layers between networks for each task, its inference in the gradient domain where there exists greater correlation between depth and intrinsic images, and the incorporation of a gradient scale network that learns the confidence of estimated gradients in order to effectively balance them in the solution. This approach is shown to surpass state-of-the-art methods both on single-image depth estimation and on intrinsic image decomposition. version:1
arxiv-1603-06353 | A Discontinuous Neural Network for Non-Negative Sparse Approximation | http://arxiv.org/abs/1603.06353 | id:1603.06353 author:Martijn Arts, Marius Cordts, Monika Gorin, Marc Spehr, Rudolf Mathar category:cs.NE math.OC q-bio.NC  published:2016-03-21 summary:This paper investigates a discontinuous neural network which is used as a model of the mammalian olfactory system and can more generally be applied to solve non-negative sparse approximation problems. By inherently limiting the systems integrators to having non-negative outputs, the system function becomes discontinuous since the integrators switch between being inactive and being active. It is shown that the presented network converges to equilibrium points which are solutions to general non-negative least squares optimization problems. We specify a Caratheodory solution and prove that the network is stable, provided that the system matrix has full column-rank. Under a mild condition on the equilibrium point, we show that the network converges to its equilibrium within a finite number of switches. Two applications of the neural network are shown. Firstly, we apply the network as a model of the olfactory system and show that in principle it may be capable of performing complex sparse signal recovery tasks. Secondly, we generalize the application to include non-negative sparse approximation problems and compare the recovery performance to a classical non-negative basis pursuit denoising algorithm. We conclude that the recovery performance differs only marginally from the classical algorithm, while the neural network has the advantage that no performance critical regularization parameter has to be chosen prior to recovery. version:1
arxiv-1603-06348 | Learning Dexterous Manipulation for a Soft Robotic Hand from Human Demonstration | http://arxiv.org/abs/1603.06348 | id:1603.06348 author:Abhishek Gupta, Clemens Eppner, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO  published:2016-03-21 summary:Dexterous multi-fingered hands can accomplish fine manipulation behaviors that are infeasible with simple robotic grippers. However, sophisticated multi-fingered hands are often expensive and fragile. Low-cost soft hands offer an appealing alternative to more conventional devices, but present considerable challenges in sensing and actuation, making them difficult to apply to more complex manipulation tasks. In this paper, we describe an approach to learning from demonstration that can be used to train soft robotic hands to perform dexterous manipulation tasks. Our method uses object-centric demonstrations, where a human demonstrates the desired motion of manipulated objects with their own hands, and the robot autonomously learns to imitate these demonstrations using reinforcement learning. We propose a novel algorithm that allows us to blend and select a subset of the most feasible demonstrations to learn to imitate on the hardware, which we use with an extension of the guided policy search framework to use multiple demonstrations to learn generalizable neural network policies. We demonstrate our approach on the RBO Hand 2, with learned motor skills for turning a valve, manipulating an abacus, and grasping. version:1
arxiv-1603-06340 | Data Augmentation via Levy Processes | http://arxiv.org/abs/1603.06340 | id:1603.06340 author:Stefan Wager, William Fithian, Percy Liang category:stat.ML  published:2016-03-21 summary:If a document is about travel, we may expect that short snippets of the document should also be about travel. We introduce a general framework for incorporating these types of invariances into a discriminative classifier. The framework imagines data as being drawn from a slice of a Levy process. If we slice the Levy process at an earlier point in time, we obtain additional pseudo-examples, which can be used to train the classifier. We show that this scheme has two desirable properties: it preserves the Bayes decision boundary, and it is equivalent to fitting a generative model in the limit where we rewind time back to 0. Our construction captures popular schemes such as Gaussian feature noising and dropout training, as well as admitting new generalizations. version:1
arxiv-1603-04922 | DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding | http://arxiv.org/abs/1603.04922 | id:1603.04922 author:Yinda Zhang, Mingru Bai, Pushmeet Kohli, Shahram Izadi, Jianxiong Xiao category:cs.CV  published:2016-03-16 summary:While deep neural networks have led to human-level performance on computer vision tasks, they have yet to demonstrate similar gains for holistic scene understanding. In particular, 3D context has been shown to be an extremely important cue for scene understanding - yet very little research has been done on integrating context information with deep models. This paper presents an approach to embed 3D context into the topology of a neural network trained to perform holistic scene understanding. Given a depth image depicting a 3D scene, our network aligns the observed scene with a predefined 3D scene template, and then reasons about the existence and location of each object within the scene template. In doing so, our model recognizes multiple objects in a single forward pass of a 3D convolutional neural network, capturing both global scene and local object information simultaneously. To create training data for this 3D network, we generate partly hallucinated depth images which are rendered by replacing real objects with a repository of CAD models of the same object category. Extensive experiments demonstrate the effectiveness of our algorithm compared to the state-of-the-arts. Source code and data will be available. version:2
arxiv-1603-06327 | Deep Self-Convolutional Activations Descriptor for Dense Cross-Modal Correspondence | http://arxiv.org/abs/1603.06327 | id:1603.06327 author:Seungryong Kim, Dongbo Min, Stephen Lin, Kwanghoon Sohn category:cs.CV  published:2016-03-21 summary:We present a novel descriptor, called deep self-convolutional activations (DeSCA), designed for establishing dense correspondences between images taken under different imaging modalities, such as different spectral ranges or lighting conditions. Motivated by descriptors based on local self-similarity (LSS), we formulate a novel descriptor by leveraging LSS in a deep architecture, leading to better discriminative power and greater robustness to non-rigid image deformations than state-of-the-art cross-modality descriptors. The DeSCA first computes self-convolutions over a local support window for randomly sampled patches, and then builds self-convolution activations by performing an average pooling through a hierarchical formulation within a deep convolutional architecture. Finally, the feature responses on the self-convolution activations are encoded through a spatial pyramid pooling in a circular configuration. In contrast to existing convolutional neural networks (CNNs) based descriptors, the DeSCA is training-free (i.e., randomly sampled patches are utilized as the convolution kernels), is robust to cross-modal imaging, and can be densely computed in an efficient manner that significantly reduces computational redundancy. The state-of-the-art performance of DeSCA on challenging cases of cross-modal image pairs is demonstrated through extensive experiments. version:1
arxiv-1603-06288 | Multi-fidelity Gaussian Process Bandit Optimisation | http://arxiv.org/abs/1603.06288 | id:1603.06288 author:Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff Schneider, Barnabas Poczos category:stat.ML cs.AI cs.LG  published:2016-03-20 summary:In many scientific and engineering applications, we are tasked with the optimisation of an expensive to evaluate black box function $f$. Traditional methods for this problem assume just the availability of this single function. However, in many cases, cheap approximations to $f$ may be obtainable. For example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation. We can use these approximations to eliminate low function value regions and use the expensive evaluations to $f$ in a small promising region and speedily identify the optimum. We formalise this task as a \emph{multi-fidelity} bandit problem where the target function and its approximations are sampled from a Gaussian process. We develop a method based on upper confidence bound techniques and prove that it exhibits precisely the above behaviour, hence achieving better regret than strategies which ignore multi-fidelity information. Our method outperforms such naive strategies on several synthetic and real experiments. version:1
arxiv-1603-06270 | Multi-Task Cross-Lingual Sequence Tagging from Scratch | http://arxiv.org/abs/1603.06270 | id:1603.06270 author:Zhilin Yang, Ruslan Salakhutdinov, William Cohen category:cs.CL cs.LG  published:2016-03-20 summary:We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and cross-lingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases. version:1
arxiv-1412-4679 | Bayesian multi-tensor factorization | http://arxiv.org/abs/1412.4679 | id:1412.4679 author:Suleiman A. Khan, Eemeli Leppäaho, Samuel Kaski category:stat.ML  published:2014-12-15 summary:We introduce Bayesian multi-tensor factorization, a model that is the first Bayesian formulation for joint factorization of multiple matrices and tensors. The research problem generalizes the joint matrix-tensor factorization problem to arbitrary sets of tensors of any depth, including matrices, can be interpreted as unsupervised multi-view learning from multiple data tensors, and can be generalized to relax the usual trilinear tensor factorization assumptions. The result is a factorization of the set of tensors into factors shared by any subsets of the tensors, and factors private to individual tensors. We demonstrate the performance against existing baselines in multiple tensor factorization tasks in structural toxicogenomics and functional neuroimaging. version:3
arxiv-1603-06220 | Flow of Information in Feed-Forward Deep Neural Networks | http://arxiv.org/abs/1603.06220 | id:1603.06220 author:Pejman Khadivi, Ravi Tandon, Naren Ramakrishnan category:cs.IT cs.LG math.IT  published:2016-03-20 summary:Feed-forward deep neural networks have been used extensively in various machine learning applications. Developing a precise understanding of the underling behavior of neural networks is crucial for their efficient deployment. In this paper, we use an information theoretic approach to study the flow of information in a neural network and to determine how entropy of information changes between consecutive layers. Moreover, using the Information Bottleneck principle, we develop a constrained optimization problem that can be used in the training process of a deep neural network. Furthermore, we determine a lower bound for the level of data representation that can be achieved in a deep neural network with an acceptable level of distortion. version:1
arxiv-1603-06212 | Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science | http://arxiv.org/abs/1603.06212 | id:1603.06212 author:Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, Jason H. Moore category:cs.NE cs.AI cs.LG  published:2016-03-20 summary:As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning---pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design. version:1
arxiv-1602-03861 | Statistical Foundation of Spectral Graph Theory | http://arxiv.org/abs/1602.03861 | id:1602.03861 author:Subhadeep Mukhopadhyay category:math.ST stat.ME stat.ML stat.TH  published:2016-02-11 summary:Spectral graph theory is undoubtedly the most favored graph data analysis technique, both in theory and practice. It has emerged as a versatile tool for a wide variety of applications including data mining, web search, quantum computing, computer vision, image segmentation, and among others. However, the way in which spectral graph theory is currently taught and practiced is rather mechanical, consisting of a series of matrix calculations that at first glance seem to have very little to do with statistics, thus posing a serious limitation to our understanding of graph problems from a statistical perspective. Our work is motivated by the following question: How can we develop a general statistical foundation of "spectral heuristics" that avoids the cookbook mechanical approach? A unified method is proposed that permits frequency analysis of graphs from a nonparametric perspective by viewing it as function estimation problem. We show that the proposed formalism incorporates seemingly unrelated spectral modeling tools (e.g., Laplacian, modularity, regularized Laplacian, etc.) under a single general method, thus providing better fundamental understanding. It is the purpose of this paper to bridge the gap between two spectral graph modeling cultures: Statistical theory (based on nonparametric function approximation and smoothing methods) and Algorithmic computing (based on matrix theory and numerical linear algebra based techniques) to provide transparent and complementary insight into graph problems. version:3
arxiv-1603-06208 | RotationNet: Learning Object Classification Using Unsupervised Viewpoint Estimation | http://arxiv.org/abs/1603.06208 | id:1603.06208 author:Asako Kanezaki category:cs.CV  published:2016-03-20 summary:The recent popularization of depth sensors and the availability of large-scale 3D model databases such as ShapeNet have drawn increased attention to 3D object recognition. Despite the convenience of using 3D models captured offline, we are allowed to observe only a single view of an object at once, with the exception of the use of special environments such as multi-camera studios. This impedes the recognition of diverse objects in a real environment. If a mechanical system (or a robot) has access to multi-view models of objects and is able to estimate the viewpoint of a currently observed object, it can rotate the object to a better view for classification. In this paper, we propose a novel method to learn a deep convolutional neural network that both classifies an object and estimates the rotation path to its best view under the predicted object category. We conduct experiments on a 3D model database as well as a real image dataset to demonstrate that our system can achieve an effective strategy of object rotation for category classification. version:1
arxiv-1603-06202 | Extracting Predictive Information from Heterogeneous Data Streams using Gaussian Processes | http://arxiv.org/abs/1603.06202 | id:1603.06202 author:Sid Ghoshal, Stephen Roberts category:q-fin.ST stat.ML  published:2016-03-20 summary:Financial markets are notoriously complex environments, presenting vast amounts of noisy, yet potentially informative data. We consider the problem of forecasting financial time series from a wide range of information sources using online Gaussian Processes with Automatic Relevance Determination (ARD) kernels. We measure the performance gain, quantified in terms of Normalised Root Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearson correlation, from fusing each of four separate data domains: time series technicals, sentiment analysis, options market data and broker recommendations. We show evidence that ARD kernels produce meaningful feature rankings that help retain salient inputs and reduce input dimensionality, providing a framework for sifting through financial complexity. We measure the performance gain from fusing each domain's heterogeneous data streams into a single probabilistic model. In particular our findings highlight the critical value of options data in mapping out the curvature of price space and inspire an intuitive, novel direction for research in financial prediction. version:1
arxiv-1510-01315 | Stochastic model for phonemes uncovers an author-dependency of their usage | http://arxiv.org/abs/1510.01315 | id:1510.01315 author:Weibing Deng, Armen E. Allahverdyan category:cs.CL nlin.AO  published:2015-10-05 summary:We study rank-frequency relations for phonemes, the minimal units that still relate to linguistic meaning. We show that these relations can be described by the Dirichlet distribution, a direct analogue of the ideal-gas model in statistical mechanics. This description allows us to demonstrate that the rank-frequency relations for phonemes of a text do depend on its author. The author-dependency effect is not caused by the author's vocabulary (common words used in different texts), and is confirmed by several alternative means. This suggests that it can be directly related to phonemes. These features contrast to rank-frequency relations for words, which are both author and text independent and are governed by the Zipf's law. version:2
arxiv-1603-06568 | Modelling Temporal Information Using Discrete Fourier Transform for Recognizing Emotions in User-generated Videos | http://arxiv.org/abs/1603.06568 | id:1603.06568 author:Haimin Zhang, Min Xu category:cs.CV  published:2016-03-20 summary:With the widespread of user-generated Internet videos, emotion recognition in those videos attracts increasing research efforts. However, most existing works are based on framelevel visual features and/or audio features, which might fail to model the temporal information, e.g. characteristics accumulated along time. In order to capture video temporal information, in this paper, we propose to analyse features in frequency domain transformed by discrete Fourier transform (DFT features). Frame-level features are firstly extract by a pre-trained deep convolutional neural network (CNN). Then, time domain features are transferred and interpolated into DFT features. CNN and DFT features are further encoded and fused for emotion classification. By this way, static image features extracted from a pre-trained deep CNN and temporal information represented by DFT features are jointly considered for video emotion recognition. Experimental results demonstrate that combining DFT features can effectively capture temporal information and therefore improve emotion recognition performance. Our approach has achieved a state-of-the-art performance on the largest video emotion dataset (VideoEmotion-8 dataset), improving accuracy from 51.1% to 62.6%. version:1
arxiv-1603-06180 | Segmentation from Natural Language Expressions | http://arxiv.org/abs/1603.06180 | id:1603.06180 author:Ronghang Hu, Marcus Rohrbach, Trevor Darrell category:cs.CV  published:2016-03-20 summary:In this paper we approach the novel problem of segmenting an image based on a natural language expression. This is different from traditional semantic segmentation over a predefined set of semantic classes, as e.g., the phrase "two men sitting on the right bench" requires segmenting only the two people on the right bench and no one standing or sitting on another bench. Previous approaches suitable for this task were limited to a fixed set of categories and/or rectangular regions. To produce pixelwise segmentation for the language expression, we propose an end-to-end trainable recurrent and convolutional network model that jointly learns to process visual and linguistic information. In our model, a recurrent LSTM network is used to encode the referential expression into a vector representation, and a fully convolutional network is used to a extract a spatial feature map from the image and output a spatial response map for the target object. We demonstrate on a benchmark dataset that our model can produce quality segmentation output from the natural language expression, and outperforms baseline methods by a large margin. version:1
arxiv-1603-06170 | Joint Stochastic Approximation learning of Helmholtz Machines | http://arxiv.org/abs/1603.06170 | id:1603.06170 author:Haotian Xu, Zhijian Ou category:cs.LG stat.ML  published:2016-03-20 summary:Though with progress, model learning and performing posterior inference still remains a common challenge for using deep generative models, especially for handling discrete hidden variables. This paper is mainly concerned with algorithms for learning Helmholz machines, which is characterized by pairing the generative model with an auxiliary inference model. A common drawback of previous learning algorithms is that they indirectly optimize some bounds of the targeted marginal log-likelihood. In contrast, we successfully develop a new class of algorithms, based on stochastic approximation (SA) theory of the Robbins-Monro type, to directly optimize the marginal log-likelihood and simultaneously minimize the inclusive KL-divergence. The resulting learning algorithm is thus called joint SA (JSA). Moreover, we construct an effective MCMC operator for JSA. Our results on the MNIST datasets demonstrate that the JSA's performance is consistently superior to that of competing algorithms like RWS, for learning a range of difficult models. version:1
arxiv-1511-05946 | ACDC: A Structured Efficient Linear Layer | http://arxiv.org/abs/1511.05946 | id:1511.05946 author:Marcin Moczulski, Misha Denil, Jeremy Appleyard, Nando de Freitas category:cs.LG cs.NE  published:2015-11-18 summary:The linear layer is one of the most pervasive modules in deep learning representations. However, it requires $O(N^2)$ parameters and $O(N^2)$ operations. These costs can be prohibitive in mobile applications or prevent scaling in many domains. Here, we introduce a deep, differentiable, fully-connected neural network module composed of diagonal matrices of parameters, $\mathbf{A}$ and $\mathbf{D}$, and the discrete cosine transform $\mathbf{C}$. The core module, structured as $\mathbf{ACDC^{-1}}$, has $O(N)$ parameters and incurs $O(N log N )$ operations. We present theoretical results showing how deep cascades of ACDC layers approximate linear layers. ACDC is, however, a stand-alone module and can be used in combination with any other types of module. In our experiments, we show that it can indeed be successfully interleaved with ReLU modules in convolutional neural networks for image recognition. Our experiments also study critical factors in the training of these structured modules, including initialization and depth. Finally, this paper also provides a connection between structured linear transforms used in deep learning and the field of Fourier optics, illustrating how ACDC could in principle be implemented with lenses and diffractive elements. version:5
arxiv-1603-06159 | Fast Incremental Method for Nonconvex Optimization | http://arxiv.org/abs/1603.06159 | id:1603.06159 author:Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola category:math.OC cs.LG stat.ML  published:2016-03-19 summary:We analyze a fast incremental aggregated gradient method for optimizing nonconvex problems of the form $\min_x \sum_i f_i(x)$. Specifically, we analyze the SAGA algorithm within an Incremental First-order Oracle framework, and show that it converges to a stationary point provably faster than both gradient descent and stochastic gradient descent. We also discuss a Polyak's special class of nonconvex problems for which SAGA converges at a linear rate to the global optimum. Finally, we analyze the practically valuable regularized and minibatch variants of SAGA. To our knowledge, this paper presents the first analysis of fast convergence for an incremental aggregated gradient method for nonconvex problems. version:1
arxiv-1603-06141 | Evolving Shepherding Behavior with Genetic Programming Algorithms | http://arxiv.org/abs/1603.06141 | id:1603.06141 author:Joshua Brulé, Kevin Engel, Nick Fung, Isaac Julien category:cs.AI cs.NE  published:2016-03-19 summary:We apply genetic programming techniques to the `shepherding' problem, in which a group of one type of animal (sheep dogs) attempts to control the movements of a second group of animals (sheep) obeying flocking behavior. Our genetic programming algorithm evolves an expression tree that governs the movements of each dog. The operands of the tree are hand-selected features of the simulation environment that may allow the dogs to herd the sheep effectively. The algorithm uses tournament-style selection, crossover reproduction, and a point mutation. We find that the evolved solutions generalize well and outperform a (naive) human-designed algorithm. version:1
arxiv-1603-06129 | Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks | http://arxiv.org/abs/1603.06129 | id:1603.06129 author:Sahil Bhatia, Rishabh Singh category:cs.PL cs.AI cs.LG cs.SE  published:2016-03-19 summary:We present a method for automatically generating repair feedback for syntax errors for introductory programming problems. Syntax errors constitute one of the largest classes of errors (34%) in our dataset of student submissions obtained from a MOOC course on edX. The previous techniques for generating automated feed- back on programming assignments have focused on functional correctness and style considerations of student programs. These techniques analyze the program AST of the program and then perform some dynamic and symbolic analyses to compute repair feedback. Unfortunately, it is not possible to generate ASTs for student pro- grams with syntax errors and therefore the previous feedback techniques are not applicable in repairing syntax errors. We present a technique for providing feedback on syntax errors that uses Recurrent neural networks (RNNs) to model syntactically valid token sequences. Our approach is inspired from the recent work on learning language models from Big Code (large code corpus). For a given programming assignment, we first learn an RNN to model all valid token sequences using the set of syntactically correct student submissions. Then, for a student submission with syntax errors, we query the learnt RNN model with the prefix to- ken sequence to predict token sequences that can fix the error by either replacing or inserting the predicted token sequence at the error location. We evaluate our technique on over 14, 000 student submissions with syntax errors. Our technique can completely re- pair 31.69% (4501/14203) of submissions with syntax errors and in addition partially correct 6.39% (908/14203) of the submissions. version:1
arxiv-1603-06125 | The Computational Power of Dynamic Bayesian Networks | http://arxiv.org/abs/1603.06125 | id:1603.06125 author:Joshua Brulé category:cs.AI stat.ML  published:2016-03-19 summary:This paper considers the computational power of constant size, dynamic Bayesian networks. Although discrete dynamic Bayesian networks are no more powerful than hidden Markov models, dynamic Bayesian networks with continuous random variables and discrete children of continuous parents are capable of performing Turing-complete computation. With modified versions of existing algorithms for belief propagation, such a simulation can be carried out in real time. This result suggests that dynamic Bayesian networks may be more powerful than previously considered. Relationships to causal models and recurrent neural networks are also discussed. version:1
arxiv-1603-06121 | Buried object detection using handheld WEMI with task-driven extended functions of multiple instances | http://arxiv.org/abs/1603.06121 | id:1603.06121 author:Matthew Cook, Alina Zare, Dominic Ho category:cs.CV  published:2016-03-19 summary:Many effective supervised discriminative dictionary learning methods have been developed in the literature. However, when training these algorithms, precise ground-truth of the training data is required to provide very accurate point-wise labels. Yet, in many applications, accurate labels are not always feasible. This is especially true in the case of buried object detection in which the size of the objects are not consistent. In this paper, a new multiple instance dictionary learning algorithm for detecting buried objects using a handheld WEMI sensor is detailed. The new algorithm, Task Driven Extended Functions of Multiple Instances, can overcome data that does not have very precise point-wise labels and still learn a highly discriminative dictionary. Results are presented and discussed on measured WEMI data. version:1
arxiv-1510-04862 | You-Do, I-Learn: Unsupervised Multi-User egocentric Approach Towards Video-Based Guidance | http://arxiv.org/abs/1510.04862 | id:1510.04862 author:Dima Damen, Teesid Leelasawassuk, Walterio Mayol-Cuevas category:cs.CV  published:2015-10-16 summary:This paper presents an unsupervised approach towards automatically extracting video-based guidance on object usage, from egocentric video and wearable gaze tracking, collected from multiple users while performing tasks. The approach i) discovers task relevant objects, ii) builds a model for each, iii) distinguishes different ways in which each discovered object has been used and iv) discovers the dependencies between object interactions. The work investigates using appearance, position, motion and attention, and presents results using each and a combination of relevant features. Moreover, an online scalable approach is presented and is compared to offline results. The paper proposes a method for selecting a suitable video guide to be displayed to a novice user indicating how to use an object, purely triggered by the user's gaze. The potential assistive mode can also recommend an object to be used next based on the learnt sequence of object interactions. The approach was tested on a variety of daily tasks such as initialising a printer, preparing a coffee and setting up a gym machine. version:2
arxiv-1511-05942 | Doctor AI: Predicting Clinical Events via Recurrent Neural Networks | http://arxiv.org/abs/1511.05942 | id:1511.05942 author:Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, Jimeng Sun category:cs.LG  published:2015-11-18 summary:Large amount of Electronic Health Record (EHR) data have been collected over millions of patients over multiple years. The rich longitudinal EHR data documented the collective experiences of physicians including diagnosis, medication prescription and procedures. We argue it is possible now to leverage the EHR data to model how physicians behave, and we call our model Doctor AI. Towards this direction of modeling clinical behavior of physicians, we develop a successful application of Recurrent Neural Networks (RNN) to jointly forecast the future disease diagnosis and medication prescription along with their timing. Unlike traditional classification models where a single target is of interest, our model can assess the entire history of patients and make continuous and multilabel predictions based on patients' historical data. We evaluate the performance of the proposed method on a large real-world EHR data over 260K patients over 8 years. We observed Doctor AI can perform differential diagnosis with similar accuracy to physicians. In particular, Doctor AI achieves up to 79% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by applying the resulting models on data from a completely different medication institution achieving comparable performance. version:8
arxiv-1603-06111 | How Transferable are Neural Networks in NLP Applications? | http://arxiv.org/abs/1603.06111 | id:1603.06111 author:Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin category:cs.CL cs.LG cs.NE  published:2016-03-19 summary:Transfer learning is aimed to make use of valuable knowledge in a source domain to help the model performance in a target domain. It is particularly important to neural networks because neural models are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct a series of empirical studies and provide an illuminating picture on the transferability of neural networks in NLP. version:1
arxiv-1603-06098 | Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation | http://arxiv.org/abs/1603.06098 | id:1603.06098 author:Alexander Kolesnikov, Christoph H. Lampert category:cs.CV  published:2016-03-19 summary:We introduce a new loss function for the weakly-supervised training of semantic image segmentation models based on three guiding principles: to seed with weak location cues, to expand objects based on the information about which classes can occur, and to constrain the segmentations to coincide with image boundaries. We show experimentally that training a deep convolutional neural network using the proposed loss function leads to substantially better segmentations than previous state-of-the-art methods on the challenging PASCAL VOC 2012 dataset. We furthermore give insight into the working mechanism of our method by a detailed experimental study that illustrates how the segmentation quality is affected by each term of the proposed loss function as well as their combinations. version:1
arxiv-1603-04259 | Item2Vec: Neural Item Embedding for Collaborative Filtering | http://arxiv.org/abs/1603.04259 | id:1603.04259 author:Oren Barkan, Noam Koenigstein category:cs.LG cs.AI cs.IR  published:2016-03-14 summary:Many Collaborative Filtering (CF) algorithms are item-based in the sense that they analyze item-item relations in order to produce item similarities. Recently, several works in the field of Natural Language Processing suggested to learn a latent representation of words using neural embedding algorithms. Among them, the Skip-gram with Negative Sampling (SGNS), also known as Word2Vec, was shown to provide state-of-the-art results on various linguistics tasks. In this paper, we show that item-based CF can be cast in the same framework of neural word embedding. Inspired by SGNS, we describe a method we name Item2Vec for item-based CF that produces embedding for items in a latent space. The method is capable of inferring item-to-item relations even when user information is not available. We present experimental results on large scale datasets that demonstrate the effectiveness of the Item2Vec method and show it is competitive with SVD. version:2
arxiv-1603-06093 | Large scale near-duplicate image retrieval using Triples of Adjacent Ranked Features (TARF) with embedded geometric information | http://arxiv.org/abs/1603.06093 | id:1603.06093 author:Sergei Fedorov, Olga Kacher category:cs.CV  published:2016-03-19 summary:Most approaches to large-scale image retrieval are based on the construction of the inverted index of local image descriptors or visual words. A search in such an index usually results in a large number of candidates. This list of candidates is then re-ranked with the help of a geometric verification, using a RANSAC algorithm, for example. In this paper we propose a feature representation, which is built as a combination of three local descriptors. It allows one to significantly decrease the number of false matches and to shorten the list of candidates after the initial search in the inverted index. This combination of local descriptors is both reproducible and highly discriminative, and thus can be efficiently used for large-scale near-duplicate image retrieval. version:1
arxiv-1603-06078 | Deep Shading: Convolutional Neural Networks for Screen-Space Shading | http://arxiv.org/abs/1603.06078 | id:1603.06078 author:Oliver Nalbach, Elena Arabadzhiyska, Dushyant Mehta, Hans-Peter Seidel, Tobias Ritschel category:cs.GR cs.LG I.3.7; I.2.6  published:2016-03-19 summary:In computer vision, Convolutional Neural Networks (CNNs) have recently achieved new levels of performance for several inverse problems where RGB pixel appearance is mapped to attributes such as positions, normals or reflectance. In computer graphics, screen-space shading has recently increased the visual quality in interactive image synthesis, where per-pixel attributes such as positions, normals or reflectance of a virtual 3D scene are converted into RGB pixel appearance, enabling effects like ambient occlusion, indirect light, scattering, depth-of-field, motion blur, or anti-aliasing. In this paper we consider the diagonal problem: synthesizing appearance from given per-pixel attributes using a CNN. The resulting Deep Shading simulates all screen-space effects as well as arbitrary combinations thereof at competitive quality and speed while not being programmed by human experts but learned from example images. version:1
arxiv-1603-06060 | DASA: Domain Adaptation in Stacked Autoencoders using Systematic Dropout | http://arxiv.org/abs/1603.06060 | id:1603.06060 author:Abhijit Guha Roy, Debdoot Sheet category:cs.CV cs.LG  published:2016-03-19 summary:Domain adaptation deals with adapting behaviour of machine learning based systems trained using samples in source domain to their deployment in target domain where the statistics of samples in both domains are dissimilar. The task of directly training or adapting a learner in the target domain is challenged by lack of abundant labeled samples. In this paper we propose a technique for domain adaptation in stacked autoencoder (SAE) based deep neural networks (DNN) performed in two stages: (i) unsupervised weight adaptation using systematic dropouts in mini-batch training, (ii) supervised fine-tuning with limited number of labeled samples in target domain. We experimentally evaluate performance in the problem of retinal vessel segmentation where the SAE-DNN is trained using large number of labeled samples in the source domain (DRIVE dataset) and adapted using less number of labeled samples in target domain (STARE dataset). The performance of SAE-DNN measured using $logloss$ in source domain is $0.19$, without and with adaptation are $0.40$ and $0.18$, and $0.39$ when trained exclusively with limited samples in target domain. The area under ROC curve is observed respectively as $0.90$, $0.86$, $0.92$ and $0.87$. The high efficiency of vessel segmentation with DASA strongly substantiates our claim. version:1
arxiv-1603-06038 | Tensor Methods and Recommender Systems | http://arxiv.org/abs/1603.06038 | id:1603.06038 author:Evgeny Frolov, Ivan Oseledets category:cs.LG cs.IR stat.ML  published:2016-03-19 summary:A substantial progress in development of new and efficient tensor factorization techniques has led to an extensive research of their applicability in recommender systems field. Tensor-based recommender models push the boundaries of traditional collaborative filtering techniques by taking into account a multifaceted nature of real environments, which allows to produce more accurate, situational (e.g. context-aware, criteria-driven) recommendations. Despite the promising results, tensor-based methods are poorly covered in existing recommender systems surveys. This survey aims to complement previous works and provide a comprehensive overview on the subject. To the best of our knowledge, this is the first attempt to consolidate studies from various application domains in an easily readable, digestible format, which helps to get a notion of the current state of the field. We also provide a high level discussion of the future perspectives and directions for further improvement of tensor-based recommendation systems. version:1
arxiv-1603-06036 | A Fractal-based CNN for Detecting Complicated Curves in AFM Images | http://arxiv.org/abs/1603.06036 | id:1603.06036 author:Hongteng Xu, Junchi Yan, Nils Persson, Hongyuan Zha category:cs.CV  published:2016-03-19 summary:Convolutional neural networks (CNNs) have been widely used in computer vision, including low-and middle-level vision problems such as contour detection and image reconstruction. In this paper, we propose a novel fractal-based CNN model for the problem of complicated curve detection, providing a geometric interpretation of CNN-based image model leveraging local fractal analysis. Utilizing the notion of local self-similarity, we develop a local fractal model for images. A curve detector is designed based on the model, consisting of an orientation-adaptive filtering process to enhance the local response along a certain orientation. This is followed by a post-processing step to preserve local invariance of the fractal dimension of image. We show the iterative framework of such an adaptive filtering process can be re-instantiated approximately via a CNN model, the nonlinear processing layer of which preserves fractal dimension approximately and the convolution layer achieves orientation enhancement. We demonstrate our fractal-based CNN model on the challenging task for detecting complicated curves from the texture-like images depicting microstructures of materials obtained by atomic force microscopy (AFM). version:1
arxiv-1511-05234 | Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering | http://arxiv.org/abs/1511.05234 | id:1511.05234 author:Huijuan Xu, Kate Saenko category:cs.CV cs.AI cs.CL cs.NE  published:2015-11-17 summary:We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single "hop" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA [2], and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer [3]. version:2
arxiv-1603-06035 | L0-norm Sparse Graph-regularized SVD for Biclustering | http://arxiv.org/abs/1603.06035 | id:1603.06035 author:Wenwen Min, Juan Liu, Shihua Zhang category:cs.LG stat.ML I.5.1  I.5.3  H.2.8  published:2016-03-19 summary:Learning the "blocking" structure is a central challenge for high dimensional data (e.g., gene expression data). Recently, a sparse singular value decomposition (SVD) has been used as a biclustering tool to achieve this goal. However, this model ignores the structural information between variables (e.g., gene interaction graph). Although typical graph-regularized norm can incorporate such prior graph information to get accurate discovery and better interpretability, it fails to consider the opposite effect of variables with different signs. Motivated by the development of sparse coding and graph-regularized norm, we propose a novel sparse graph-regularized SVD as a powerful biclustering tool for analyzing high-dimensional data. The key of this method is to impose two penalties including a novel graph-regularized norm ($ \pmb{u} \pmb{L} \pmb{u} $) and $L_0$-norm ($\ \pmb{u}\ _0$) on singular vectors to induce structural sparsity and enhance interpretability. We design an efficient Alternating Iterative Sparse Projection (AISP) algorithm to solve it. Finally, we apply our method and related ones to simulated and real data to show its efficiency in capturing natural blocking structures. version:1
arxiv-1505-05914 | A Multi-scale Multiple Instance Video Description Network | http://arxiv.org/abs/1505.05914 | id:1505.05914 author:Huijuan Xu, Subhashini Venugopalan, Vasili Ramanishka, Marcus Rohrbach, Kate Saenko category:cs.CV  published:2015-05-21 summary:Generating natural language descriptions for in-the-wild videos is a challenging task. Most state-of-the-art methods for solving this problem borrow existing deep convolutional neural network (CNN) architectures (AlexNet, GoogLeNet) to extract a visual representation of the input video. However, these deep CNN architectures are designed for single-label centered-positioned object classification. While they generate strong semantic features, they have no inherent structure allowing them to detect multiple objects of different sizes and locations in the frame. Our paper tries to solve this problem by integrating the base CNN into several fully convolutional neural networks (FCNs) to form a multi-scale network that handles multiple receptive field sizes in the original image. FCNs, previously applied to image segmentation, can generate class heat-maps efficiently compared to sliding window mechanisms, and can easily handle multiple scales. To further handle the ambiguity over multiple objects and locations, we incorporate the Multiple Instance Learning mechanism (MIL) to consider objects in different positions and at different scales simultaneously. We integrate our multi-scale multi-instance architecture with a sequence-to-sequence recurrent neural network to generate sentence descriptions based on the visual representation. Ours is the first end-to-end trainable architecture that is capable of multi-scale region processing. Evaluation on a Youtube video dataset shows the advantage of our approach compared to the original single-scale whole frame CNN model. Our flexible and efficient architecture can potentially be extended to support other video processing tasks. version:3
arxiv-1510-02533 | New Optimisation Methods for Machine Learning | http://arxiv.org/abs/1510.02533 | id:1510.02533 author:Aaron Defazio category:cs.LG stat.ML  published:2015-10-09 summary:A thesis submitted for the degree of Doctor of Philosophy of The Australian National University. In this work we introduce several new optimisation methods for problems in machine learning. Our algorithms broadly fall into two categories: optimisation of finite sums and of graph structured objectives. The finite sum problem is simply the minimisation of objective functions that are naturally expressed as a summation over a large number of terms, where each term has a similar or identical weight. Such objectives most often appear in machine learning in the empirical risk minimisation framework in the non-online learning setting. The second category, that of graph structured objectives, consists of objectives that result from applying maximum likelihood to Markov random field models. Unlike the finite sum case, all the non-linearity is contained within a partition function term, which does not readily decompose into a summation. For the finite sum problem, we introduce the Finito and SAGA algorithms, as well as variants of each. For graph-structured problems, we take three complementary approaches. We look at learning the parameters for a fixed structure, learning the structure independently, and learning both simultaneously. Specifically, for the combined approach, we introduce a new method for encouraging graph structures with the "scale-free" property. For the structure learning problem, we establish SHORTCUT, a O(n^{2.5}) expected time approximate structure learning method for Gaussian graphical models. For problems where the structure is known but the parameters unknown, we introduce an approximate maximum likelihood learning algorithm that is capable of learning a useful subclass of Gaussian graphical models. version:2
arxiv-1603-06028 | A Survey of Stealth Malware: Attacks, Mitigation Measures, and Steps Toward Autonomous Open World Solutions | http://arxiv.org/abs/1603.06028 | id:1603.06028 author:Ethan Rudd, Andras Rozsa, Manuel Gunther, Terrance Boult category:cs.CR cs.CV  published:2016-03-19 summary:Development of generic and autonomous anti-malware solutions is becoming increasingly vital as the deployment of stealth malware continues to increase at an alarming rate. In this paper, we survey malicious stealth technologies as well as existing autonomous countermeasures. Our findings suggest that while machine learning offers promising potential for generic and autonomous solutions, both at the network level and at the host level, several flawed assumptions inherent to most recognition algorithms prevent a direct mapping between the stealth malware recognition problem and a machine learning solution. The most notable of these flawed assumptions is the closed world assumption: that no sample belonging to a class outside of a static training set will appear at query time. We present a formalized adaptive open world framework for stealth malware recognition, relating it mathematically to research from other machine learning domains. version:1
arxiv-1603-06015 | A Comprehensive Performance Evaluation of Deformable Face Tracking "In-the-Wild" | http://arxiv.org/abs/1603.06015 | id:1603.06015 author:Grigorios G. Chrysos, Epameinondas Antonakos, Patrick Snape, Akshay Asthana, Stefanos Zafeiriou category:cs.CV cs.AI  published:2016-03-18 summary:Recently, technologies such as face detection, facial landmark localisation and face recognition and verification have matured enough to provide effective and efficient solutions for imagery captured under arbitrary conditions (referred to as "in-the-wild"). This is partially attributed to the fact that comprehensive "in-the-wild" benchmarks have been developed for face detection, landmark localisation and recognition/verification. A very important technology that has not been thoroughly evaluated yet is deformable face tracking "in-the-wild". Until now, the performance has mainly been assessed qualitatively by visually assessing the result of a deformable face tracking technology on short videos. In this paper, we perform the first, to the best of our knowledge, thorough evaluation of state-of-the-art deformable face tracking pipelines using the recently introduced 300VW benchmark. We evaluate many different architectures focusing mainly on the task of on-line deformable face tracking. In particular, we compare the following general strategies: (a) generic face detection plus generic facial landmark localisation, (b) generic model free tracking plus generic facial landmark localisation, as well as (c) hybrid approaches using state-of-the-art face detection, model free tracking and facial landmark localisation technologies. Our evaluation reveals future avenues for further research on the topic. version:1
arxiv-1603-06009 | Readability-based Sentence Ranking for Evaluating Text Simplification | http://arxiv.org/abs/1603.06009 | id:1603.06009 author:Sowmya Vajjala, Detmar Meurers category:cs.CL  published:2016-03-18 summary:We propose a new method for evaluating the readability of simplified sentences through pair-wise ranking. The validity of the method is established through in-corpus and cross-corpus evaluation experiments. The approach correctly identifies the ranking of simplified and unsimplified sentences in terms of their reading level with an accuracy of over 80%, significantly outperforming previous results. To gain qualitative insights into the nature of simplification at the sentence level, we studied the impact of specific linguistic features. We empirically confirm that both word-level and syntactic features play a role in comparing the degree of simplification of authentic data. To carry out this research, we created a new sentence-aligned corpus from professionally simplified news articles. The new corpus resource enriches the empirical basis of sentence-level simplification research, which so far relied on a single resource. Most importantly, it facilitates cross-corpus evaluation for simplification, a key step towards generalizable results. version:1
arxiv-1603-06002 | A Message Passing Algorithm for the Problem of Path Packing in Graphs | http://arxiv.org/abs/1603.06002 | id:1603.06002 author:Patrick Eschenfeldt, David Gamarnik category:cs.DS stat.ML  published:2016-03-18 summary:We consider the problem of packing node-disjoint directed paths in a directed graph. We consider a variant of this problem where each path starts within a fixed subset of root nodes, subject to a given bound on the length of paths. This problem is motivated by the so-called kidney exchange problem, but has potential other applications and is interesting in its own right. We propose a new algorithm for this problem based on the message passing/belief propagation technique. A priori this problem does not have an associated graphical model, so in order to apply a belief propagation algorithm we provide a novel representation of the problem as a graphical model. Standard belief propagation on this model has poor scaling behavior, so we provide an efficient implementation that significantly decreases the complexity. We provide numerical results comparing the performance of our algorithm on both artificially created graphs and real world networks to several alternative algorithms, including algorithms based on integer programming (IP) techniques. These comparisons show that our algorithm scales better to large instances than IP-based algorithms and often finds better solutions than a simple algorithm that greedily selects the longest path from each root node. In some cases it also finds better solutions than the ones found by IP-based algorithms even when the latter are allowed to run significantly longer than our algorithm. version:1
arxiv-1406-0873 | Linear Dimensionality Reduction: Survey, Insights, and Generalizations | http://arxiv.org/abs/1406.0873 | id:1406.0873 author:John P. Cunningham, Zoubin Ghahramani category:stat.ML  published:2014-06-03 summary:Linear dimensionality reduction methods are a cornerstone of analyzing high dimensional data, due to their simple geometric interpretations and typically attractive computational properties. These methods capture many data features of interest, such as covariance, dynamical structure, correlation between data sets, input-output relationships, and margin between data classes. Methods have been developed with a variety of names and motivations in many fields, and perhaps as a result the connections between all these methods have not been highlighted. Here we survey methods from this disparate literature as optimization programs over matrix manifolds. We discuss principal component analysis, factor analysis, linear multidimensional scaling, Fisher's linear discriminant analysis, canonical correlations analysis, maximum autocorrelation factors, slow feature analysis, sufficient dimensionality reduction, undercomplete independent component analysis, linear regression, distance metric learning, and more. This optimization framework gives insight to some rarely discussed shortcomings of well-known methods, such as the suboptimality of certain eigenvector solutions. Modern techniques for optimization over matrix manifolds enable a generic linear dimensionality reduction solver, which accepts as input data and an objective to be optimized, and returns, as output, an optimal low-dimensional projection of the data. This simple optimization framework further allows straightforward generalizations and novel variants of classical methods, which we demonstrate here by creating an orthogonal-projection canonical correlations analysis. More broadly, this survey and generic solver suggest that linear dimensionality reduction can move toward becoming a blackbox, objective-agnostic numerical technology. version:2
arxiv-1603-02752 | Best-of-K Bandits | http://arxiv.org/abs/1603.02752 | id:1603.02752 author:Max Simchowitz, Kevin Jamieson, Benjamin Recht category:cs.LG stat.ML  published:2016-03-09 summary:This paper studies the Best-of-K Bandit game: At each time the player chooses a subset S among all N-choose-K possible options and observes reward max(X(i) : i in S) where X is a random vector drawn from a joint distribution. The objective is to identify the subset that achieves the highest expected reward with high probability using as few queries as possible. We present distribution-dependent lower bounds based on a particular construction which force a learner to consider all N-choose-K subsets, and match naive extensions of known upper bounds in the bandit setting obtained by treating each subset as a separate arm. Nevertheless, we present evidence that exhaustive search may be avoided for certain, favorable distributions because the influence of high-order order correlations may be dominated by lower order statistics. Finally, we present an algorithm and analysis for independent arms, which mitigates the surprising non-trivial information occlusion that occurs due to only observing the max in the subset. This may inform strategies for more general dependent measures, and we complement these result with independent-arm lower bounds. version:2
arxiv-1603-05962 | Document Neural Autoregressive Distribution Estimation | http://arxiv.org/abs/1603.05962 | id:1603.05962 author:Stanislas Lauly, Yin Zheng, Alexandre Allauzen, Hugo Larochelle category:cs.LG cs.CL  published:2016-03-18 summary:We present an approach based on feed-forward neural networks for learning the distribution of textual documents. This approach is inspired by the Neural Autoregressive Distribution Estimator(NADE) model, which has been shown to be a good estimator of the distribution of discrete-valued igh-dimensional vectors. In this paper, we present how NADE can successfully be adapted to the case of textual data, retaining from NADE the property that sampling or computing the probability of observations can be done exactly and efficiently. The approach can also be used to learn deep representations of documents that are competitive to those learned by the alternative topic modeling approaches. Finally, we describe how the approach can be combined with a regular neural network N-gram model and substantially improve its performance, by making its learned representation sensitive to the larger, document-specific context. version:1
arxiv-1603-05955 | Transferring Learned Microcalcification Group Detection from 2D Mammography to 3D Digital Breast Tomosynthesis Using a Hierarchical Model and Scope-based Normalization Features | http://arxiv.org/abs/1603.05955 | id:1603.05955 author:Yin Yin, Sergei V. Fotin, Hrishikesh Haldankar, Jeffrey W. Hoffmeister, Senthil Periaswamy category:cs.CV  published:2016-03-18 summary:A novel hierarchical model is introduced to solve a general problem of detecting groups of similar objects. Under this model, detection of groups is performed in hierarchically organized layers while each layer represents a scope for target objects. The processing of these layers involves sequential extraction of appearance features for an individual object, consistency measurement features for nearby objects, and finally the distribution features for all objects within the group. Using the concept of scope-based normalization, the extracted features not only enhance local contrast of an individual object, but also provide consistent characterization for all related objects. As an example, a microcalcification group detection system for 2D mammography was developed, and then the learned model was transferred to 3D digital breast tomosynthesis without any retraining or fine-tuning. The detection system demonstrated state-of-the-art performance and detected 96% of cancerous lesions at the rate of 1.2 false positives per volume as measured on an independent tomosynthesis test set. version:1
arxiv-1603-05933 | Distributed Iterative Learning Control for a Team of Quadrotors | http://arxiv.org/abs/1603.05933 | id:1603.05933 author:Andreas Hock, Angela P. Schoellig category:cs.RO cs.LG cs.MA  published:2016-03-18 summary:The goal of this work is to enable a team of quadrotors to learn how to accurately track a desired trajectory while holding a given formation. We solve this problem in a distributed manner, where each vehicle has only access to the information of its neighbors. The desired trajectory is only available to one (or few) vehicles. We present a distributed iterative learning control (ILC) approach where each vehicle learns from the experience of its own and its neighbors' previous task repetitions and adapts its feedforward input to improve performance. Existing algorithms are extended in theory to make them more applicable for real-world experiments. In particular, we prove stability for any causal learning function with gains chosen according to a simple scalar condition. Previous proofs were restricted to a specific learning function, which only depends on the tracking error derivative (D-type ILC). This extension provides more degrees of freedom in the ILC design and, as a result, better performance can be achieved. We also show that stability is not affected by a linear dynamic coupling between neighbors. This allows us to use an additional consensus feedback controller to compensate for non-repetitive disturbances. Experiments with two quadrotors attest the practical applicability of the proposed distributed multi-agent ILC approach. This is the first work to show distributed ILC in experiment. version:1
arxiv-1603-05930 | Geometric Hypergraph Learning for Visual Tracking | http://arxiv.org/abs/1603.05930 | id:1603.05930 author:Dawei Du, Honggang Qi, Longyin Wen, Qi Tian, Qingming Huang, Siwei Lyu category:cs.CV  published:2016-03-18 summary:Graph based representation is widely used in visual tracking field by finding correct correspondences between target parts in consecutive frames. However, most graph based trackers consider pairwise geometric relations between local parts. They do not make full use of the target's intrinsic structure, thereby making the representation easily disturbed by errors in pairwise affinities when large deformation and occlusion occur. In this paper, we propose a geometric hypergraph learning based tracking method, which fully exploits high-order geometric relations among multiple correspondences of parts in consecutive frames. Then visual tracking is formulated as the mode-seeking problem on the hypergraph in which vertices represent correspondence hypotheses and hyperedges describe high-order geometric relations. Besides, a confidence-aware sampling method is developed to select representative vertices and hyperedges to construct the geometric hypergraph for more robustness and scalability. The experiments are carried out on two challenging datasets (VOT2014 and Deform-SOT) to demonstrate that the proposed method performs favorable against other existing trackers. version:1
arxiv-1506-02169 | Approximating Likelihood Ratios with Calibrated Discriminative Classifiers | http://arxiv.org/abs/1506.02169 | id:1506.02169 author:Kyle Cranmer, Juan Pavez, Gilles Louppe category:stat.AP physics.data-an stat.ML 62P35  62F99  62H30  published:2015-06-06 summary:In many fields of science, generalized likelihood ratio tests are established tools for statistical inference. At the same time, it has become increasingly common that a simulator (or generative model) is used to describe complex processes that tie parameters $\theta$ of an underlying theory and measurement apparatus to high-dimensional observations $\mathbf{x}\in \mathbb{R}^p$. However, simulator often do not provide a way to evaluate the likelihood function for a given observation $\mathbf{x}$, which motivates a new class of likelihood-free inference algorithms. In this paper, we show that likelihood ratios are invariant under a specific class of dimensionality reduction maps $\mathbb{R}^p \mapsto \mathbb{R}$. As a direct consequence, we show that discriminative classifiers can be used to approximate the generalized likelihood ratio statistic when only a generative model for the data is available. This leads to a new machine learning-based approach to likelihood-free inference that is complementary to Approximate Bayesian Computation, and which does not require a prior on the model parameters. Experimental results on artificial problems with known exact likelihoods illustrate the potential of the proposed method. version:2
arxiv-1508-03865 | Predicting Grades | http://arxiv.org/abs/1508.03865 | id:1508.03865 author:Yannick Meier, Jie Xu, Onur Atan, Mihaela van der Schaar category:cs.LG  published:2015-08-16 summary:To increase efficacy in traditional classroom courses as well as in Massive Open Online Courses (MOOCs), automated systems supporting the instructor are needed. One important problem is to automatically detect students that are going to do poorly in a course early enough to be able to take remedial actions. Existing grade prediction systems focus on maximizing the accuracy of the prediction while overseeing the importance of issuing timely and personalized predictions. This paper proposes an algorithm that predicts the final grade of each student in a class. It issues a prediction for each student individually, when the expected accuracy of the prediction is sufficient. The algorithm learns online what is the optimal prediction and time to issue a prediction based on past history of students' performance in a course. We derive a confidence estimate for the prediction accuracy and demonstrate the performance of our algorithm on a dataset obtained based on the performance of approximately 700 UCLA undergraduate students who have taken an introductory digital signal processing over the past 7 years. We demonstrate that for 85% of the students we can predict with 76% accuracy whether they are going do well or poorly in the class after the 4th course week. Using data obtained from a pilot course, our methodology suggests that it is effective to perform early in-class assessments such as quizzes, which result in timely performance prediction for each student, thereby enabling timely interventions by the instructor (at the student or class level) when necessary. version:2
arxiv-1509-08082 | Multivariate Median Filters and Partial Differential Equations | http://arxiv.org/abs/1509.08082 | id:1509.08082 author:Martin Welk category:cs.CV I.4.3; G.1.8  published:2015-09-27 summary:Multivariate median filters have been proposed as generalisations of the well-established median filter for grey-value images to multi-channel images. As multivariate median, most of the recent approaches use the $L^1$ median, i.e.\ the minimiser of an objective function that is the sum of distances to all input points. Many properties of univariate median filters generalise to such a filter. However, the famous result by Guichard and Morel about approximation of the mean curvature motion PDE by median filtering does not have a comparably simple counterpart for $L^1$ multivariate median filtering. We discuss the affine equivariant Oja median and the affine equivariant transformation--retransformation $L^1$ median as alternatives to $L^1$ median filtering. We analyse multivariate median filters in a space-continuous setting, including the formulation of a space-continuous version of the transformation--retransformation $L^1$ median, and derive PDEs approximated by these filters in the cases of bivariate planar images, three-channel volume images and three-channel planar images. The PDEs for the affine equivariant filters can be interpreted geometrically as combinations of a diffusion and a principal-component-wise curvature motion contribution with a cross-effect term based on torsions of principal components. Numerical experiments are presented that demonstrate the validity of the approximation results. version:2
arxiv-1502-02476 | An Infinite Restricted Boltzmann Machine | http://arxiv.org/abs/1502.02476 | id:1502.02476 author:Marc-Alexandre Côté, Hugo Larochelle category:cs.LG  published:2015-02-09 summary:We present a mathematical construction for the restricted Boltzmann machine (RBM) that doesn't require specifying the number of hidden units. In fact, the hidden layer size is adaptive and can grow during training. This is obtained by first extending the RBM to be sensitive to the ordering of its hidden units. Then, thanks to a carefully chosen definition of the energy function, we show that the limit of infinitely many hidden units is well defined. As with RBM, approximate maximum likelihood training can be performed, resulting in an algorithm that naturally and adaptively adds trained hidden units during learning. We empirically study the behaviour of this infinite RBM, showing that its performance is competitive to that of the RBM, while not requiring the tuning of a hidden layer size. version:4
arxiv-1603-05876 | Generalized support vector regression: duality and tensor-kernel representation | http://arxiv.org/abs/1603.05876 | id:1603.05876 author:Saverio Salzo, Johan A. K. Suykens category:math.OC math.FA stat.ML  published:2016-03-18 summary:In this paper we study the variational problem associated to support vector regression in Banach function spaces. Using the Fenchel-Rockafellar duality theory, we give explicit formulation of the dual problem as well as of the related optimality conditions. Moreover, we provide a new computational framework for solving the problem which relies on a tensor-kernel representation. This analysis overcomes the typical difficulties connected to learning in Banach spaces. We finally present a large class of tensor-kernels to which our theory fully applies: power series tensor kernels. This type of kernels describe Banach spaces of analytic functions and include generalizations of the exponential and polynomial kernels as well as, in the complex case, generalizations of the Szeg\"o and Bergman kernels. version:1
arxiv-1603-05875 | Approximated Robust Principal Component Analysis for Improved General Scene Background Subtraction | http://arxiv.org/abs/1603.05875 | id:1603.05875 author:Salehe Erfanian Ebadi, Valia Guerra Ones, Ebroul Izquierdo category:cs.CV stat.AP  published:2016-03-18 summary:The research reported in this paper addresses the fundamental task of separation of locally moving or deforming image areas from a static or globally moving background. It builds on the latest developments in the field of robust principal component analysis, specifically, the recently reported practical solutions for the long-standing problem of recovering the low-rank and sparse parts of a large matrix made up of the sum of these two components. This article addresses a few critical issues including: embedding global motion parameters in the matrix decomposition model, i.e., estimation of global motion parameters simultaneously with the foreground/background separation task, considering matrix block-sparsity rather than generic matrix sparsity as natural feature in video processing applications, attenuating background ghosting effects when foreground is subtracted, and more critically providing an extremely efficient algorithm to solve the low-rank/sparse matrix decomposition task. The first aspect is important for background/foreground separation in generic video sequences where the background usually obeys global displacements originated by the camera motion in the capturing process. The second aspect exploits the fact that in video processing applications the sparse matrix has a very particular structure, where the non-zero matrix entries are not randomly distributed but they build small blocks within the sparse matrix. The next feature of the proposed approach addresses removal of ghosting effects originated from foreground silhouettes and the lack of information in the occluded background regions of the image. Finally, the proposed model also tackles algorithmic complexity by introducing an extremely efficient "SVD-free" technique that can be applied in most background/foreground separation tasks for conventional video processing. version:1
arxiv-1603-05850 | N-ary Error Correcting Coding Scheme | http://arxiv.org/abs/1603.05850 | id:1603.05850 author:Joey Tianyi Zhou, Ivor W. Tsang, Shen-Shyang Ho, Klaus-Robert Muller category:cs.LG  published:2016-03-18 summary:The coding matrix design plays a fundamental role in the prediction performance of the error correcting output codes (ECOC)-based multi-class task. {In many-class classification problems, e.g., fine-grained categorization, it is difficult to distinguish subtle between-class differences under existing coding schemes due to a limited choices of coding values.} In this paper, we investigate whether one can relax existing binary and ternary code design to $N$-ary code design to achieve better classification performance. {In particular, we present a novel $N$-ary coding scheme that decomposes the original multi-class problem into simpler multi-class subproblems, which is similar to applying a divide-and-conquer method.} The two main advantages of such a coding scheme are as follows: (i) the ability to construct more discriminative codes and (ii) the flexibility for the user to select the best $N$ for ECOC-based classification. We show empirically that the optimal $N$ (based on classification performance) lies in $[3, 10]$ with some trade-off in computational cost. Moreover, we provide theoretical insights on the dependency of the generalization error bound of an $N$-ary ECOC on the average base classifier generalization error and the minimum distance between any two codes constructed. Extensive experimental results on benchmark multi-class datasets show that the proposed coding scheme achieves superior prediction performance over the state-of-the-art coding methods. version:1
arxiv-1603-05835 | A Flexible Primal-Dual Toolbox | http://arxiv.org/abs/1603.05835 | id:1603.05835 author:Hendrik Dirks category:math.OC cs.CV cs.MS I.4  G.1.6  G.4  published:2016-03-18 summary:\textbf{FlexBox} is a flexible MATLAB toolbox for finite dimensional convex variational problems in image processing and beyond. Such problems often consist of non-differentiable parts and involve linear operators. The toolbox uses a primal-dual scheme to avoid (computationally) inefficient operator inversion and to get reliable error estimates. From the user-side, \textbf{FlexBox} expects the primal formulation of the problem, automatically decouples operators and dualizes the problem. For large-scale problems, \textbf{FlexBox} also comes with a \cpp-module, which can be used stand-alone or together with MATLAB via MEX-interfaces. Besides various pre-implemented data-fidelities and regularization-terms, \textbf{FlexBox} is able to handle arbitrary operators while being easily extendable, due to its object-oriented design. The toolbox is available at \href{http://www.flexbox.im}{http://www.flexbox.im} version:1
arxiv-1603-05824 | Comparing Time and Frequency Domain for Audio Event Recognition Using Deep Learning | http://arxiv.org/abs/1603.05824 | id:1603.05824 author:Lars Hertel, Huy Phan, Alfred Mertins category:cs.NE cs.LG cs.SD  published:2016-03-18 summary:Recognizing acoustic events is an intricate problem for a machine and an emerging field of research. Deep neural networks achieve convincing results and are currently the state-of-the-art approach for many tasks. One advantage is their implicit feature learning, opposite to an explicit feature extraction of the input signal. In this work, we analyzed whether more discriminative features can be learned from either the time-domain or the frequency-domain representation of the audio signal. For this purpose, we trained multiple deep networks with different architectures on the Freiburg-106 and ESC-10 datasets. Our results show that feature learning from the frequency domain is superior to the time domain. Moreover, additionally using convolution and pooling layers, to explore local structures of the audio signal, significantly improves the recognition performance and achieves state-of-the-art results. version:1
arxiv-1509-08745 | Compression of Deep Neural Networks on the Fly | http://arxiv.org/abs/1509.08745 | id:1509.08745 author:Guillaume Soulié, Vincent Gripon, Maëlys Robert category:cs.LG cs.CV cs.NE  published:2015-09-29 summary:Thanks to their state-of-the-art performance, deep neural networks are increasingly used for object recognition. To achieve these results, they use millions of parameters to be trained. However, when targeting embedded applications the size of these models becomes problematic. As a consequence, their usage on smartphones or other resource limited devices is prohibited. In this paper we introduce a novel compression method for deep neural networks that is performed during the learning phase. It consists in adding an extra regularization term to the cost function of fully-connected layers. We combine this method with Product Quantization (PQ) of the trained weights for higher savings in storage consumption. We evaluate our method on two data sets (MNIST and CIFAR10), on which we achieve significantly larger compression rates than state-of-the-art methods. version:5
arxiv-1510-09041 | Postprocessing of Compressed Images via Sequential Denoising | http://arxiv.org/abs/1510.09041 | id:1510.09041 author:Yehuda Dar, Alfred M. Bruckstein, Michael Elad, Raja Giryes category:cs.CV  published:2015-10-30 summary:In this work we propose a novel postprocessing technique for compression-artifact reduction. Our approach is based on posing this task as an inverse problem, with a regularization that leverages on existing state-of-the-art image denoising algorithms. We rely on the recently proposed Plug-and-Play Prior framework, suggesting the solution of general inverse problems via Alternating Direction Method of Multipliers (ADMM), leading to a sequence of Gaussian denoising steps. A key feature in our scheme is a linearization of the compression-decompression process, so as to get a formulation that can be optimized. In addition, we supply a thorough analysis of this linear approximation for several basic compression procedures. The proposed method is suitable for diverse compression techniques that rely on transform coding. Specifically, we demonstrate impressive gains in image quality for several leading compression methods - JPEG, JPEG2000, and HEVC. version:2
arxiv-1603-05800 | A Comparison between Deep Neural Nets and Kernel Acoustic Models for Speech Recognition | http://arxiv.org/abs/1603.05800 | id:1603.05800 author:Zhiyun Lu, Dong Guo, Alireza Bagheri Garakani, Kuan Liu, Avner May, Aurelien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael Picheny, Fei Sha category:cs.LG stat.ML  published:2016-03-18 summary:We study large-scale kernel methods for acoustic modeling and compare to DNNs on performance metrics related to both acoustic modeling and recognition. Measuring perplexity and frame-level classification accuracy, kernel-based acoustic models are as effective as their DNN counterparts. However, on token-error-rates DNN models can be significantly better. We have discovered that this might be attributed to DNN's unique strength in reducing both the perplexity and the entropy of the predicted posterior probabilities. Motivated by our findings, we propose a new technique, entropy regularized perplexity, for model selection. This technique can noticeably improve the recognition performance of both types of models, and reduces the gap between them. While effective on Broadcast News, this technique could be also applicable to other tasks. version:1
arxiv-1603-05782 | Unsupervised Cross-Media Hashing with Structure Preservation | http://arxiv.org/abs/1603.05782 | id:1603.05782 author:Xiangyu Wang, Alex Yong-Sang Chia category:cs.CV cs.IR H.3.3  published:2016-03-18 summary:Recent years have seen the exponential growth of heterogeneous multimedia data. The need for effective and accurate data retrieval from heterogeneous data sources has attracted much research interest in cross-media retrieval. Here, given a query of any media type, cross-media retrieval seeks to find relevant results of different media types from heterogeneous data sources. To facilitate large-scale cross-media retrieval, we propose a novel unsupervised cross-media hashing method. Our method incorporates local affinity and distance repulsion constraints into a matrix factorization framework. Correspondingly, the proposed method learns hash functions that generates unified hash codes from different media types, while ensuring intrinsic geometric structure of the data distribution is preserved. These hash codes empower the similarity between data of different media types to be evaluated directly. Experimental results on two large-scale multimedia datasets demonstrate the effectiveness of the proposed method, where we outperform the state-of-the-art methods. version:1
arxiv-1603-05772 | Learning to Navigate the Energy Landscape | http://arxiv.org/abs/1603.05772 | id:1603.05772 author:Julien Valentin, Angela Dai, Matthias Nießner, Pushmeet Kohli, Philip Torr, Shahram Izadi, Cem Keskin category:cs.CV  published:2016-03-18 summary:In this paper, we present a novel and efficient architecture for addressing computer vision problems that use `Analysis by Synthesis'. Analysis by synthesis involves the minimization of the reconstruction error which is typically a non-convex function of the latent target variables. State-of-the-art methods adopt a hybrid scheme where discriminatively trained predictors like Random Forests or Convolutional Neural Networks are used to initialize local search algorithms. While these methods have been shown to produce promising results, they often get stuck in local optima. Our method goes beyond the conventional hybrid architecture by not only proposing multiple accurate initial solutions but by also defining a navigational structure over the solution space that can be used for extremely efficient gradient-free local search. We demonstrate the efficacy of our approach on the challenging problem of RGB Camera Relocalization. To make the RGB camera relocalization problem particularly challenging, we introduce a new dataset of 3D environments which are significantly larger than those found in other publicly-available datasets. Our experiments reveal that the proposed method is able to achieve state-of-the-art camera relocalization results. We also demonstrate the generalizability of our approach on Hand Pose Estimation and Image Retrieval tasks. version:1
arxiv-1603-05544 | Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent | http://arxiv.org/abs/1603.05544 | id:1603.05544 author:Linnan Wang, Yi Yang, Martin Renqiang Min, Srimat Chakradhar category:cs.LG cs.DC  published:2016-03-17 summary:SGD is the widely adopted method to train CNN. Conceptually it approximates the population with a randomly sampled batch; then it evenly trains batches by conducting a gradient update on every batch in an epoch. In this paper, we demonstrate Sampling Bias, Intrinsic Image Difference and Fixed Cycle Pseudo Random Sampling differentiate batches in training, which then affect learning speeds on them. Because of this, the unbiased treatment of batches involved in SGD creates improper load balancing. To address this issue, we present Inconsistent Stochastic Gradient Descent (ISGD) to dynamically vary training effort according to learning statuses on batches. Specifically ISGD leverages techniques in Statistical Process Control to identify a undertrained batch. Once a batch is undertrained, ISGD solves a new subproblem, a chasing logic plus a conservative constraint, to accelerate the training on the batch while avoid drastic parameter changes. Extensive experiments on a variety of datasets demonstrate ISGD converges faster than SGD. In training AlexNet, ISGD is 21.05\% faster than SGD to reach 56\% top1 accuracy under the exactly same experiment setup. We also extend ISGD to work on multiGPU or heterogeneous distributed system based on data parallelism, enabling the batch size to be the key to scalability. Then we present the study of ISGD batch size to the learning rate, parallelism, synchronization cost, system saturation and scalability. We conclude the optimal ISGD batch size is machine dependent. Various experiments on a multiGPU system validate our claim. In particular, ISGD trains AlexNet to 56.3% top1 and 80.1% top5 accuracy in 11.5 hours with 4 NVIDIA TITAN X at the batch size of 1536. version:2
arxiv-1603-05770 | A Probabilistic Machine Learning Approach to Detect Industrial Plant Faults | http://arxiv.org/abs/1603.05770 | id:1603.05770 author:Wei Xiao category:stat.ML stat.AP  published:2016-03-18 summary:Fault detection in industrial plants is a hot research area as more and more sensor data are being collected throughout the industrial process. Automatic data-driven approaches are widely needed and seen as a promising area of investment. This paper proposes an effective machine learning algorithm to predict industrial plant faults based on classification methods such as penalized logistic regression, random forest and gradient boosted tree. A fault's start time and end time are predicted sequentially in two steps by formulating the original prediction problems as classification problems. The algorithms described in this paper won first place in the Prognostics and Health Management Society 2015 Data Challenge. version:1
arxiv-1603-05763 | From line segments to more organized Gestalts | http://arxiv.org/abs/1603.05763 | id:1603.05763 author:Boshra Rajaei, Rafael Grompone von Gioi, Jean-Michel Morel category:cs.CV  published:2016-03-18 summary:In this paper, we reconsider the early computer vision bottom-up program, according to which higher level features (geometric structures) in an image could be built up recursively from elementary features by simple grouping principles coming from Gestalt theory. Taking advantage of the (recent) advances in reliable line segment detectors, we propose three feature detectors that constitute one step up in this bottom up pyramid. For any digital image, our unsupervised algorithm computes three classic Gestalts from the set of predetected line segments: good continuations, nonlocal alignments, and bars. The methodology is based on a common stochastic {\it a contrario model} yielding three simple detection formulas, characterized by their number of false alarms. This detection algorithm is illustrated on several digital images. version:1
arxiv-1511-03745 | Grounding of Textual Phrases in Images by Reconstruction | http://arxiv.org/abs/1511.03745 | id:1511.03745 author:Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, Bernt Schiele category:cs.CV cs.CL cs.LG  published:2015-11-12 summary:Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution. Few datasets provide the ground truth spatial localization of phrases, thus it is desirable to learn from data with no or little grounding supervision. We propose a novel approach which learns grounding by reconstructing a given phrase using an attention mechanism, which can be either latent or optimized directly. During training our model encodes the phrase using a recurrent network language model and then learns to attend to the relevant image region in order to reconstruct the input phrase. At test time, the correct attention, i.e., the grounding, is evaluated. If grounding supervision is available it can be directly applied via a loss over the attention mechanism. We demonstrate the effectiveness of our approach on the Flickr 30k Entities and ReferItGame datasets with different levels of supervision, ranging from no supervision over partial supervision to full supervision. Our supervised variant improves by a large margin over the state-of-the-art on both datasets. version:3
arxiv-1512-00927 | Mean-Field Inference in Gaussian Restricted Boltzmann Machine | http://arxiv.org/abs/1512.00927 | id:1512.00927 author:Chako Takahashi, Muneki Yasuda category:stat.ML physics.data-an  published:2015-12-03 summary:A Gaussian restricted Boltzmann machine (GRBM) is a Boltzmann machine defined on a bipartite graph and is an extension of usual restricted Boltzmann machines. A GRBM consists of two different layers: a visible layer composed of continuous visible variables and a hidden layer composed of discrete hidden variables. In this paper, we derive two different inference algorithms for GRBMs based on the naive mean-field approximation (NMFA). One is an inference algorithm for whole variables in a GRBM, and the other is an inference algorithm for partial variables in a GBRBM. We compare the two methods analytically and numerically and show that the latter method is better. version:2
arxiv-1311-2645 | Program Evaluation and Causal Inference with High-Dimensional Data | http://arxiv.org/abs/1311.2645 | id:1311.2645 author:Alexandre Belloni, Victor Chernozhukov, Ivan Fernández-Val, Chris Hansen category:math.ST stat.ME stat.ML stat.TH  published:2013-11-11 summary:In this paper, we provide efficient estimators and honest confidence bands for a variety of treatment effects including local average (LATE) and local quantile treatment effects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment effects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces efficient estimators and honest bands for (functional) average treatment effects (ATE) and quantile treatment effects (QTE). To make informative inference possible, we assume that key reduced form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide-range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced form functional parameters. We illustrate the use of the proposed methods with an application to estimating the effect of 401(k) eligibility and participation on accumulated assets. version:6
arxiv-1603-05739 | A Readability Analysis of Campaign Speeches from the 2016 US Presidential Campaign | http://arxiv.org/abs/1603.05739 | id:1603.05739 author:Elliot Schumacher, Maxine Eskenazi category:cs.CL  published:2016-03-18 summary:Readability is defined as the reading level of the speech from grade 1 to grade 12. It results from the use of the REAP readability analysis (vocabulary - Collins-Thompson and Callan, 2004; syntax - Heilman et al ,2006, 2007), which use the lexical contents and grammatical structure of the sentences in a document to predict the reading level. After analysis, results were grouped into the average readability of each candidate, the evolution of the candidate's speeches' readability over time and the standard deviation, or how much each candidate varied their speech from one venue to another. For comparison, one speech from four past presidents and the Gettysburg Address were also analyzed. version:1
arxiv-1603-05673 | Predicting health inspection results from online restaurant reviews | http://arxiv.org/abs/1603.05673 | id:1603.05673 author:Samantha Wong, Hamidreza Chinaei, Frank Rudzicz category:cs.CL cs.LG  published:2016-03-17 summary:Informatics around public health are increasingly shifting from the professional to the public spheres. In this work, we apply linguistic analytics to restaurant reviews, from Yelp, in order to automatically predict official health inspection reports. We consider two types of feature sets, i.e., keyword detection and topic model features, and use these in several classification methods. Our empirical analysis shows that these extracted features can predict public health inspection reports with over 90% accuracy using simple support vector machines. version:1
arxiv-1603-05670 | Bank distress in the news: Describing events through deep learning | http://arxiv.org/abs/1603.05670 | id:1603.05670 author:Samuel Rönnqvist, Peter Sarlin category:cs.CL cs.AI cs.IR cs.NE q-fin.CP  published:2016-03-17 summary:While many models are purposed for detecting the occurrence of events in complex systems, the task of providing qualitative detail on the developments is not usually as well automated. We present a deep learning approach for detecting relevant discussion in text and extracting natural language descriptions of events. Supervised by only a small set of event information, the model is leveraged by unsupervised learning of semantic vector representations on extensive text data. We demonstrate applicability to the study of financial risk based on news (6.6M articles), particularly bank distress and government interventions (243 events), where indices can signal the level of bank-stress-related reporting at the entity level, or aggregated at country or European level, while being coupled with explanations. Thus, we exemplify how text, as timely and widely available data, can serve as a useful complementary source of information for financial risk analytics. version:1
arxiv-1603-05643 | Variance Reduction for Faster Non-Convex Optimization | http://arxiv.org/abs/1603.05643 | id:1603.05643 author:Zeyuan Allen-Zhu, Elad Hazan category:math.OC cs.DS cs.LG cs.NE stat.ML  published:2016-03-17 summary:We consider the fundamental problem in non-convex optimization of efficiently reaching a stationary point. In contrast to the convex case, in the long history of this basic problem, the only known theoretical results on first-order non-convex optimization remain to be full gradient descent that converges in $O(1/\varepsilon)$ iterations for smooth objectives, and stochastic gradient descent that converges in $O(1/\varepsilon^2)$ iterations for objectives that are sum of smooth functions. We provide the first improvement in this line of research. Our result is based on the variance reduction trick recently introduced to convex optimization, as well as a brand new analysis of variance reduction that is suitable for non-convex optimization. For objectives that are sum of smooth functions, our first-order minibatch stochastic method converges with an $O(1/\varepsilon)$ rate, and is faster than full gradient descent by $\Omega(n^{1/3})$. We demonstrate the effectiveness of our methods on empirical risk minimizations with non-convex loss functions and training neural nets. version:1
arxiv-1603-05631 | Generative Image Modeling using Style and Structure Adversarial Networks | http://arxiv.org/abs/1603.05631 | id:1603.05631 author:Xiaolong Wang, Abhinav Gupta category:cs.CV  published:2016-03-17 summary:Current generative frameworks use end-to-end learning and generate images by sampling from uniform noise distribution. However, these approaches ignore the most basic principle of image formation: images are product of: (a) Structure: the underlying 3D model; (b) Style: the texture mapped onto structure. In this paper, we factorize the image generation process and propose Style and Structure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has two components: the Structure-GAN generates a surface normal map; the Style-GAN takes the surface normal map as input and generates the 2D image. Apart from a real vs. generated loss function, we use an additional loss with computed surface normals from generated images. The two GANs are first trained independently, and then merged together via joint learning. We show our S^2-GAN model is interpretable, generates more realistic images and can be used to learn unsupervised RGBD representations. version:1
arxiv-1603-05614 | Streaming Algorithms for News and Scientific Literature Recommendation: Submodular Maximization with a $d$-Knapsack Constraint | http://arxiv.org/abs/1603.05614 | id:1603.05614 author:Qilian Yu, Easton Li Xu, Shuguang Cui category:cs.LG cs.DS  published:2016-03-17 summary:Submodular maximization problems belong to the family of combinatorial optimization problems and enjoy wide applications. In this paper, we focus on the problem of maximizing a monotone submodular function subject to a $d$-knapsack constraint, for which we propose a streaming algorithm that achieves a $\left(\frac{1}{1+d}-\epsilon\right)$-approximation of the optimal value, while it only needs one single pass through the dataset without storing all the data in the memory. In our experiments, we extensively evaluate the effectiveness of our proposed algorithm via two applications: news recommendation and scientific literature recommendation. It is observed that the proposed streaming algorithm achieves both execution speedup and memory saving by several orders of magnitude, compared with existing approaches. version:1
arxiv-1603-05600 | "What happens if..." Learning to Predict the Effect of Forces in Images | http://arxiv.org/abs/1603.05600 | id:1603.05600 author:Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, Ali Farhadi category:cs.CV  published:2016-03-17 summary:What happens if one pushes a cup sitting on a table toward the edge of the table? How about pushing a desk against a wall? In this paper, we study the problem of understanding the movements of objects as a result of applying external forces to them. For a given force vector applied to a specific location in an image, our goal is to predict long-term sequential movements caused by that force. Doing so entails reasoning about scene geometry, objects, their attributes, and the physical rules that govern the movements of objects. We design a deep neural network model that learns long-term sequential dependencies of object movements while taking into account the geometry and appearance of the scene by combining Convolutional and Recurrent Neural Networks. Training our model requires a large-scale dataset of object movements caused by external forces. To build a dataset of forces in scenes, we reconstructed all images in SUN RGB-D dataset in a physics simulator to estimate the physical movements of objects caused by external forces applied to them. Our Forces in Scenes (ForScene) dataset contains 10,335 images in which a variety of external forces are applied to different types of objects resulting in more than 65,000 object movements represented in 3D. Our experimental evaluations show that the challenging task of predicting long-term movements of objects as their reaction to external forces is possible from a single image. version:1
arxiv-1603-05594 | Mapping Temporal Variables into the NeuCube for Improved Pattern Recognition, Predictive Modelling and Understanding of Stream Data | http://arxiv.org/abs/1603.05594 | id:1603.05594 author:Enmei Tu, Nikola Kasabov, Jie Yang category:cs.NE cs.AI stat.ML  published:2016-03-17 summary:This paper proposes a new method for an optimized mapping of temporal variables, describing a temporal stream data, into the recently proposed NeuCube spiking neural network architecture. This optimized mapping extends the use of the NeuCube, which was initially designed for spatiotemporal brain data, to work on arbitrary stream data and to achieve a better accuracy of temporal pattern recognition, a better and earlier event prediction and a better understanding of complex temporal stream data through visualization of the NeuCube connectivity. The effect of the new mapping is demonstrated on three bench mark problems. The first one is early prediction of patient sleep stage event from temporal physiological data. The second one is pattern recognition of dynamic temporal patterns of traffic in the Bay Area of California and the last one is the Challenge 2012 contest data set. In all cases the use of the proposed mapping leads to an improved accuracy of pattern recognition and event prediction and a better understanding of the data when compared to traditional machine learning techniques or spiking neural network reservoirs with arbitrary mapping of the variables. version:1
arxiv-1511-06421 | Deep Manifold Traversal: Changing Labels with Convolutional Features | http://arxiv.org/abs/1511.06421 | id:1511.06421 author:Jacob R. Gardner, Paul Upchurch, Matt J. Kusner, Yixuan Li, Kilian Q. Weinberger, Kavita Bala, John E. Hopcroft category:cs.LG cs.CV stat.ML  published:2015-11-19 summary:Many tasks in computer vision can be cast as a "label changing" problem, where the goal is to make a semantic change to the appearance of an image or some subject in an image in order to alter the class membership. Although successful task-specific methods have been developed for some label changing applications, to date no general purpose method exists. Motivated by this we propose deep manifold traversal, a method that addresses the problem in its most general form: it first approximates the manifold of natural images then morphs a test image along a traversal path away from a source class and towards a target class while staying near the manifold throughout. The resulting algorithm is surprisingly effective and versatile. It is completely data driven, requiring only an example set of images from the desired source and target domains. We demonstrate deep manifold traversal on highly diverse label changing tasks: changing an individual's appearance (age and hair color), changing the season of an outdoor image, and transforming a city skyline towards nighttime. version:3
arxiv-1511-06409 | Learning to Generate Images with Perceptual Similarity Metrics | http://arxiv.org/abs/1511.06409 | id:1511.06409 author:Karl Ridgeway, Jake Snell, Brett D. Roads, Richard S. Zemel, Michael C. Mozer category:cs.LG cs.CV  published:2015-11-19 summary:Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions and reconstructing an input image from a compact representation. Supervised training of image-synthesis networks typically uses a pixel-wise loss (PL) to indicate the mismatch between a generated image and its corresponding target image. We propose instead to use a loss function that is better calibrated to human perceptual judgments of image quality: the multiscale structural-similarity score (MS-SSIM). Because MS-SSIM is differentiable, it is easily incorporated into gradient-descent learning. We compare the consequences of using MS-SSIM versus PL loss on training deterministic and stochastic autoencoders. For three different architectures, we collected human judgments of the quality of image reconstructions. Observers reliably prefer images synthesized by MS-SSIM-optimized models over those synthesized by PL-optimized models, for two distinct PL measures ($\ell_1$ and $\ell_2$ distances). We also explore the effect of training objective on image encoding and analyze conditions under which perceptually-optimized representations yield better performance on image classification. Just as computer vision has advanced through the use of convolutional architectures that mimic the structure of the mammalian visual system, we argue that significant additional advances can be made in modeling images through the use of training objectives that are well aligned to characteristics of human perception. version:2
arxiv-1603-05570 | Predicate Gradual Logic and Linguistics | http://arxiv.org/abs/1603.05570 | id:1603.05570 author:Ryuta Arisaka category:cs.CL  published:2016-03-17 summary:There are several major proposals for treating donkey anaphora such as discourse representation theory and the likes, or E-Type theories and the likes. Every one of them works well for a set of specific examples that they use to demonstrate validity of their approaches. As I show in this paper, however, they are not very generalisable and do not account for essentially the same problem that they remedy when it manifests in other examples. I propose another logical approach. I develoop logic that extends a recent, propositional gradual logic, and show that it can treat donkey anaphora generally. I also identify and address a problem around the modern convention on existential import. Furthermore, I show that Aristotle's syllogisms and conversion are realisable in this logic. version:1
arxiv-1507-04717 | Less is More: Nyström Computational Regularization | http://arxiv.org/abs/1507.04717 | id:1507.04717 author:Alessandro Rudi, Raffaello Camoriano, Lorenzo Rosasco category:stat.ML cs.LG  published:2015-07-16 summary:We study Nystr\"om type subsampling approaches to large scale kernel methods, and prove learning bounds in the statistical learning setting, where random sampling and high probability estimates are considered. In particular, we prove that these approaches can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple incremental variant of Nystr\"om Kernel Regularized Least Squares, where the subsampling level implements a form of computational regularization, in the sense that it controls at the same time regularization and computations. Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets. version:6
arxiv-1603-05522 | Tracking multiple moving objects in images using Markov Chain Monte Carlo | http://arxiv.org/abs/1603.05522 | id:1603.05522 author:Lan Jiang, Sumeetpal S. Singh category:stat.AP cs.CV stat.CO  published:2016-03-17 summary:A new Bayesian state and parameter learning algorithm for multiple target tracking (MTT) models with image observations is proposed. Specifically, a Markov chain Monte Carlo algorithm is designed to sample from the posterior distribution of the unknown number of targets, their birth and death times, states and model parameters, which constitutes the complete solution to the tracking problem. The conventional approach is to pre-process the images to extract point observations and then perform tracking. We model the image generation process directly to avoid potential loss of information when extracting point observations. Numerical examples show that our algorithm has improved tracking performance over commonly used techniques, for both synthetic examples and real florescent microscopy data, especially in the case of dim targets with overlapping illuminated regions. version:1
arxiv-1310-2880 | Feature Selection with Annealing for Computer Vision and Big Data Learning | http://arxiv.org/abs/1310.2880 | id:1310.2880 author:Adrian Barbu, Yiyuan She, Liangjing Ding, Gary Gramajo category:stat.ML cs.CV cs.LG math.ST stat.TH  published:2013-10-10 summary:Many computer vision and medical imaging problems are faced with learning from large-scale datasets, with millions of observations and features. In this paper we propose a novel efficient learning scheme that tightens a sparsity constraint by gradually removing variables based on a criterion and a schedule. The attractive fact that the problem size keeps dropping throughout the iterations makes it particularly suitable for big data learning. Our approach applies generically to the optimization of any differentiable loss function, and finds applications in regression, classification and ranking. The resultant algorithms build variable screening into estimation and are extremely simple to implement. We provide theoretical guarantees of convergence and selection consistency. In addition, one dimensional piecewise linear response functions are used to account for nonlinearity and a second order prior is imposed on these functions to avoid overfitting. Experiments on real and synthetic data show that the proposed method compares very well with other state of the art methods in regression, classification and ranking while being computationally very efficient and scalable. version:7
arxiv-1602-02830 | Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 | http://arxiv.org/abs/1602.02830 | id:1602.02830 author:Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio category:cs.LG  published:2016-02-09 summary:We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line. version:3
arxiv-1603-05486 | A flexible state space model for learning nonlinear dynamical systems | http://arxiv.org/abs/1603.05486 | id:1603.05486 author:Andreas Svensson, Thomas B. Schön category:stat.CO cs.SY stat.ML  published:2016-03-17 summary:We consider a nonlinear state space model with the state transition and observation functions expressed as basis function expansions. We learn the coefficients in the basis function expansions from data, and with a connection to Gaussian processes we also develop priors on them for tuning the model flexibility and to prevent overfitting to data, akin to a Gaussian process state space model. The priors can alternatively be seen as a regularization, and helps the model in generalizing the data without sacrificing the richness offered by the basis function expansion. To learn the coefficients and other unknown parameters efficiently, we tailor an algorithm for this model using state-of-the-art sequential Monte Carlo methods, which comes with theoretical guarantees on the learning. Our approach indicates promising results when evaluated on a classical benchmark as well as real data. version:1
arxiv-1412-0614 | Classification and Reconstruction of High-Dimensional Signals from Low-Dimensional Features in the Presence of Side Information | http://arxiv.org/abs/1412.0614 | id:1412.0614 author:Francesco Renna, Liming Wang, Xin Yuan, Jianbo Yang, Galen Reeves, Robert Calderbank, Lawrence Carin, Miguel R. D. Rodrigues category:cs.IT cs.CV math.IT math.ST stat.ML stat.TH  published:2014-12-01 summary:This paper offers a characterization of fundamental limits on the classification and reconstruction of high-dimensional signals from low-dimensional features, in the presence of side information. We consider a scenario where a decoder has access both to linear features of the signal of interest and to linear features of the side information signal; while the side information may be in a compressed form, the objective is recovery or classification of the primary signal, not the side information. The signal of interest and the side information are each assumed to have (distinct) latent discrete labels; conditioned on these two labels, the signal of interest and side information are drawn from a multivariate Gaussian distribution. With joint probabilities on the latent labels, the overall signal-(side information) representation is defined by a Gaussian mixture model. We then provide sharp sufficient and/or necessary conditions for these quantities to approach zero when the covariance matrices of the Gaussians are nearly low-rank. These conditions, which are reminiscent of the well-known Slepian-Wolf and Wyner-Ziv conditions, are a function of the number of linear features extracted from the signal of interest, the number of linear features extracted from the side information signal, and the geometry of these signals and their interplay. Moreover, on assuming that the signal of interest and the side information obey such an approximately low-rank model, we derive expansions of the reconstruction error as a function of the deviation from an exactly low-rank model; such expansions also allow identification of operational regimes where the impact of side information on signal reconstruction is most relevant. Our framework, which offers a principled mechanism to integrate side information in high-dimensional data problems, is also tested in the context of imaging applications. version:2
arxiv-1603-05474 | Neural Aggregation Network for Video Face Recognition | http://arxiv.org/abs/1603.05474 | id:1603.05474 author:Jiaolong Yang, Peiran Ren, Dong Chen, Fang Wen, Hongdong Li, Gang Hua category:cs.CV cs.AI  published:2016-03-17 summary:In this paper, we present a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with variable number of face frames as its input, and produces a compact and fixed-dimension visual representation of that person. The whole network is composed of two modules. The feature embedding module is a CNN which maps each face frame into a feature representation. The neural aggregation module is composed of two content based attention blocks which is driven by a memory storing all the features extracted from the face video through the feature embedding module. The output of the first attention block adapts the second, whose output is adopted as the aggregated representation of the video faces. Due to the attention mechanism, this representation is invariant to the order of the face frames. The experiments show that the proposed NAN consistently outperforms hand-crafted aggregations such as average pooling, and achieves state-of-the-art accuracy on three video face recognition datasets: the YouTube Face, IJB-A and Celebrity-1000 datasets. version:1
arxiv-1603-05414 | Variable-Length Hashing | http://arxiv.org/abs/1603.05414 | id:1603.05414 author:Honghai Yu, Pierre Moulin, Hong Wei Ng, Xiaoli Li category:cs.CV cs.IR  published:2016-03-17 summary:Hashing has emerged as a popular technique for large-scale similarity search. Most learning-based hashing methods generate compact yet correlated hash codes. However, this redundancy is storage-inefficient. Hence we propose a lossless variable-length hashing (VLH) method that is both storage- and search-efficient. Storage efficiency is achieved by converting the fixed-length hash code into a variable-length code. Search efficiency is obtained by using a multiple hash table structure. With VLH, we are able to deliberately add redundancy into hash codes to improve retrieval performance with little sacrifice in storage efficiency or search complexity. In particular, we propose a block K-means hashing (B-KMH) method to obtain significantly improved retrieval performance with no increase in storage and marginal increase in computational cost. version:1
arxiv-1603-05412 | Online semi-parametric learning for inverse dynamics modeling | http://arxiv.org/abs/1603.05412 | id:1603.05412 author:Diego Romeres, Mattia Zorzi, Alessandro Chiuso category:math.OC cs.LG stat.ML  published:2016-03-17 summary:This paper presents a semi-parametric algorithm for online learning of a robot inverse dynamics model. It combines the strength of the parametric and non-parametric modeling. The former exploits the rigid body dynamics equation, while the latter exploits a suitable kernel function. We provide an extensive comparison with other methods from the literature using real data from the iCub humanoid robot. In doing so we also compare two different techniques, namely cross validation and marginal likelihood optimization, for estimating the hyperparameters of the kernel function. version:1
arxiv-1508-06073 | Cooking in the kitchen: Recognizing and Segmenting Human Activities in Videos | http://arxiv.org/abs/1508.06073 | id:1508.06073 author:Hilde Kuehne, Juergen Gall, Thomas Serre category:cs.CV  published:2015-08-25 summary:As research on action recognition matures, the focus is shifting away from categorizing basic task-oriented actions using hand-segmented video datasets to understanding complex goal-oriented daily human activities in real-world settings. Temporally structured models would seem obvious to tackle this set of problems, but so far, cases where these models have outperformed simpler unstructured bag-of-word types of models are scarce. With the increasing availability of large human activity datasets, combined with the development of novel feature coding techniques that yield more compact representations, it is time to revisit structured generative approaches. Here, we describe an end-to-end generative approach from the encoding of features to the structural modeling of complex human activities by applying Fisher vectors and temporal models for the analysis of video sequences. We systematically evaluate the proposed approach on several available datasets (ADL, MPIICooking, and Breakfast datasets) using a variety of performance metrics. Through extensive system evaluations, we demonstrate that combining compact video representations based on Fisher Vectors with HMM-based modeling yields very significant gains in accuracy and when properly trained with sufficient training samples, structured temporal models outperform unstructured bag-of-word types of models by a large margin on the tested performance metric. version:2
arxiv-1509-01947 | An end-to-end generative framework for video segmentation and recognition | http://arxiv.org/abs/1509.01947 | id:1509.01947 author:Hilde Kuehne, Juergen Gall, Thomas Serre category:cs.CV  published:2015-09-07 summary:We describe an end-to-end generative approach for the segmentation and recognition of human activities. In this approach, a visual representation based on reduced Fisher Vectors is combined with a structured temporal model for recognition. We show that the statistical properties of Fisher Vectors make them an especially suitable front-end for generative models such as Gaussian mixtures. The system is evaluated for both the recognition of complex activities as well as their parsing into action units. Using a variety of video datasets ranging from human cooking activities to animal behaviors, our experiments demonstrate that the resulting architecture outperforms state-of-the-art approaches for larger datasets, i.e. when sufficient amount of data is available for training structured generative models. version:2
arxiv-1603-05359 | Cascading Bandits for Large-Scale Recommendation Problems | http://arxiv.org/abs/1603.05359 | id:1603.05359 author:Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng Wen, Branislav Kveton category:cs.LG stat.ML  published:2016-03-17 summary:Most recommender systems recommend a list of items. The user examines the list, from the first item to the last, and often chooses the first attractive item and does not examine the rest. This type of user behavior can be modeled by the cascade model. In this work, we study cascading bandits, an online learning variant of the cascade model where the goal is to recommend $K$ most attractive items from a large set of $L$ candidate items. We propose two algorithms for solving this problem, which are based on the idea of linear generalization. The key idea in our solutions is that we learn a predictor of the attraction probabilities of items from their features, as opposing to learning the attraction probability of each item independently as in the existing work. This results in practical learning algorithms whose regret does not depend on the number of items $L$. We bound the regret of one algorithm and comprehensively evaluate the other on a range of recommendation problems. The algorithm performs well and outperforms all baselines. version:1
arxiv-1410-2500 | Validation of k-Nearest Neighbor Classifiers Using Inclusion and Exclusion | http://arxiv.org/abs/1410.2500 | id:1410.2500 author:Eric Bax, Lingjie Weng, Xu Tian category:cs.LG cs.IT math.IT stat.ML  published:2014-10-09 summary:This paper presents a series of PAC exponential error bounds for $k$-nearest neighbors classifiers, with O($n^{-\frac{r}{2r+1}}\sqrt{k \ln n}$) error bound range for each integer $r>0$, where $n$ is the number of in-sample examples. This shows that $k$-nn classifiers, in spite of their famously fractured decision boundaries, come close to having Gaussian-style exponential error bounds with O($n^{-\frac{1}{2}}$) bound ranges. version:3
arxiv-1603-01025 | Convolutional Neural Networks using Logarithmic Data Representation | http://arxiv.org/abs/1603.01025 | id:1603.01025 author:Daisuke Miyashita, Edward H. Lee, Boris Murmann category:cs.NE cs.LG  published:2016-03-03 summary:Recent advances in convolutional neural networks have considered model complexity and hardware efficiency to enable deployment onto embedded systems and mobile devices. For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance. However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance. In this paper we propose a new data representation that enables state-of-the-art networks to be encoded to 3 bits with negligible loss in classification performance. To perform this, we take advantage of the fact that the weights and activations in a trained network naturally have non-uniform distributions. Using non-uniform, base-2 logarithmic representation to encode weights, communicate activations, and perform dot-products enables networks to 1) achieve higher classification accuracies than fixed-point at the same resolution and 2) eliminate bulky digital multipliers. Finally, we propose an end-to-end training procedure that uses log representation at 5-bits, which achieves higher final test accuracy than linear at 5-bits. version:2
arxiv-1512-07158 | Feature Selection for Classification under Anonymity Constraint | http://arxiv.org/abs/1512.07158 | id:1512.07158 author:Baichuan Zhang, Vachik Dave, Noman Mohammed, Mohammad Al Hasan category:cs.LG cs.CR  published:2015-12-22 summary:Over the last decade, proliferation of various online platforms and their increasing adoption by billions of users have heightened the privacy risk of a user enormously. In fact, security researchers have shown that sparse microdata containing information about online activities of a user although anonymous, can still be used to disclose the identity of the user by cross-referencing the data with other data sources. To preserve the privacy of a user, in existing works several methods (k-anonymity, l-diversity, differential privacy) are proposed that ensure a dataset which is meant to share or publish bears small identity disclosure risk. However, the majority of these methods modify the data in isolation, without considering their utility in subsequent knowledge discovery tasks, which makes these datasets less informative. In this work, we consider labeled data that are generally used for classification, and propose two methods for feature selection considering two goals: first, on the reduced feature set the data has small disclosure risk, and second, the utility of the data is preserved for performing a classification task. Experimental results on various real-world datasets show that the method is effective and useful in practice. version:4
arxiv-1603-05335 | Saliency Detection with Spaces of Background-based Distribution | http://arxiv.org/abs/1603.05335 | id:1603.05335 author:Tong Zhao, Lin Li, Xinghao Ding, Yue Huang, Delu Zeng category:cs.CV  published:2016-03-17 summary:In this letter, an effective image saliency detection method is proposed by constructing some novel spaces to model the background and redefine the distance of the salient patches away from the background. Concretely, given the backgroundness prior, eigendecomposition is utilized to create four spaces of background-based distribution (SBD) to model the background, in which a more appropriate metric (Mahalanobis distance) is quoted to delicately measure the saliency of every image patch away from the background. After that, a coarse saliency map is obtained by integrating the four adjusted Mahalanobis distance maps, each of which is formed by the distances between all the patches and background in the corresponding SBD. To be more discriminative, the coarse saliency map is further enhanced into the posterior probability map within Bayesian perspective. Finally, the final saliency map is generated by properly refining the posterior probability map with geodesic distance. Experimental results on two usual datasets show that the proposed method is effective compared with the state-of-the-art algorithms. version:1
arxiv-1511-02917 | Detecting events and key actors in multi-person videos | http://arxiv.org/abs/1511.02917 | id:1511.02917 author:Vignesh Ramanathan, Jonathan Huang, Sami Abu-El-Haija, Alexander Gorban, Kevin Murphy, Li Fei-Fei category:cs.CV cs.AI  published:2015-11-09 summary:Multi-person event recognition is a challenging task, often with many people active in the scene but only a small subset contributing to an actual event. In this paper, we propose a model which learns to detect events in such videos while automatically "attending" to the people responsible for the event. Our model does not use explicit annotations regarding who or where those people are during training and testing. In particular, we track people in videos and use a recurrent neural network (RNN) to represent the track features. We learn time-varying attention weights to combine these features at each time-instant. The attended features are then processed using another RNN for event detection/classification. Since most video datasets with multiple people are restricted to a small number of videos, we also collected a new basketball dataset comprising 257 basketball games with 14K event annotations corresponding to 11 event classes. Our model outperforms state-of-the-art methods for both event classification and detection on this new dataset. Additionally, we show that the attention mechanism is able to consistently localize the relevant players. version:2
arxiv-1410-7632 | On the Covariance of ICP-based Scan-matching Techniques | http://arxiv.org/abs/1410.7632 | id:1410.7632 author:Silvère Bonnabel, Martin Barczyk, François Goulette category:cs.CV cs.RO cs.SY  published:2014-10-16 summary:This paper considers the problem of estimating the covariance of roto-translations computed by the Iterative Closest Point (ICP) algorithm. The problem is relevant for localization of mobile robots and vehicles equipped with depth-sensing cameras (e.g., Kinect) or Lidar (e.g., Velodyne). The closed-form formulas for covariance proposed in previous literature generally build upon the fact that the solution to ICP is obtained by minimizing a linear least-squares problem. In this paper, we show this approach needs caution because the rematching step of the algorithm is not explicitly accounted for, and applying it to the point-to-point version of ICP leads to completely erroneous covariances. We then provide a formal mathematical proof why the approach is valid in the point-to-plane version of ICP, which validates the intuition and experimental results of practitioners. version:3
arxiv-1603-05310 | Persistent Homology of Attractors For Action Recognition | http://arxiv.org/abs/1603.05310 | id:1603.05310 author:Vinay Venkataraman, Karthikeyan Natesan Ramamurthy, Pavan Turaga category:cs.CG cs.CV  published:2016-03-16 summary:In this paper, we propose a novel framework for dynamical analysis of human actions from 3D motion capture data using topological data analysis. We model human actions using the topological features of the attractor of the dynamical system. We reconstruct the phase-space of time series corresponding to actions using time-delay embedding, and compute the persistent homology of the phase-space reconstruction. In order to better represent the topological properties of the phase-space, we incorporate the temporal adjacency information when computing the homology groups. The persistence of these homology groups encoded using persistence diagrams are used as features for the actions. Our experiments with action recognition using these features demonstrate that the proposed approach outperforms other baseline methods. version:1
arxiv-1603-05305 | Near-Optimal Stochastic Approximation for Online Principal Component Estimation | http://arxiv.org/abs/1603.05305 | id:1603.05305 author:Chris J. Li, Mengdi Wang, Han Liu, Tong Zhang category:math.OC stat.ML  published:2016-03-16 summary:Principal component analysis (PCA) has been a prominent tool for high-dimensional data analysis. Online algorithms that estimate the principal component by processing streaming data are of tremendous practical and theoretical interests. Despite its rich applications, theoretical convergence analysis remains largely open. In this paper, we cast online PCA into a stochastic nonconvex optimization problem, and we analyze the online PCA algorithm as a stochastic approximation iteration. The stochastic approximation iteration processes data points incrementally and maintains a running estimate of the principal component. We prove for the first time a nearly optimal convergence rate result for the online PCA algorithm. We show that the finite-sample error closely matches the minimax information lower bound. In addition, we characterize the convergence process using ordinary and stochastic differential equation approximations. version:1
arxiv-1511-08855 | Semantic Folding Theory And its Application in Semantic Fingerprinting | http://arxiv.org/abs/1511.08855 | id:1511.08855 author:Francisco De Sousa Webber category:cs.AI cs.CL q-bio.NC  published:2015-11-28 summary:Human language is recognized as a very complex domain since decades. No computer system has been able to reach human levels of performance so far. The only known computational system capable of proper language processing is the human brain. While we gather more and more data about the brain, its fundamental computational processes still remain obscure. The lack of a sound computational brain theory also prevents the fundamental understanding of Natural Language Processing. As always when science lacks a theoretical foundation, statistical modeling is applied to accommodate as many sampled real-world data as possible. An unsolved fundamental issue is the actual representation of language (data) within the brain, denoted as the Representational Problem. Starting with Jeff Hawkins' Hierarchical Temporal Memory (HTM) theory, a consistent computational theory of the human cortex, we have developed a corresponding theory of language data representation: The Semantic Folding Theory. The process of encoding words, by using a topographic semantic space as distributional reference frame into a sparse binary representational vector is called Semantic Folding and is the central topic of this document. Semantic Folding describes a method of converting language from its symbolic representation (text) into an explicit, semantically grounded representation that can be generically processed by Hawkins' HTM networks. As it turned out, this change in representation, by itself, can solve many complex NLP problems by applying Boolean operators and a generic similarity function like the Euclidian Distance. Many practical problems of statistical NLP systems, like the high cost of computation, the fundamental incongruity of precision and recall , the complex tuning procedures etc., can be elegantly overcome by applying Semantic Folding. version:2
arxiv-1603-05285 | Image Labeling by Assignment | http://arxiv.org/abs/1603.05285 | id:1603.05285 author:Freddie Åström, Stefania Petra, Bernhard Schmitzer, Christoph Schnörr category:cs.CV math.OC  published:2016-03-16 summary:We introduce a novel geometric approach to the image labeling problem. Abstracting from specific labeling applications, a general objective function is defined on a manifold of stochastic matrices, whose elements assign prior data that are given in any metric space, to observed image measurements. The corresponding Riemannian gradient flow entails a set of replicator equations, one for each data point, that are spatially coupled by geometric averaging on the manifold. Starting from uniform assignments at the barycenter as natural initialization, the flow terminates at some global maximum, each of which corresponds to an image labeling that uniquely assigns the prior data. Our geometric variational approach constitutes a smooth non-convex inner approximation of the general image labeling problem, implemented with sparse interior-point numerics in terms of parallel multiplicative updates that converge efficiently. version:1
arxiv-1510-05613 | PERCH: Perception via Search for Multi-Object Recognition and Localization | http://arxiv.org/abs/1510.05613 | id:1510.05613 author:Venkatraman Narayanan, Maxim Likhachev category:cs.CV cs.AI cs.RO  published:2015-10-19 summary:In many robotic domains such as flexible automated manufacturing or personal assistance, a fundamental perception task is that of identifying and localizing objects whose 3D models are known. Canonical approaches to this problem include discriminative methods that find correspondences between feature descriptors computed over the model and observed data. While these methods have been employed successfully, they can be unreliable when the feature descriptors fail to capture variations in observed data; a classic cause being occlusion. As a step towards deliberative reasoning, we present PERCH: PErception via SeaRCH, an algorithm that seeks to find the best explanation of the observed sensor data by hypothesizing possible scenes in a generative fashion. Our contributions are: i) formulating the multi-object recognition and localization task as an optimization problem over the space of hypothesized scenes, ii) exploiting structure in the optimization to cast it as a combinatorial search problem on what we call the Monotone Scene Generation Tree, and iii) leveraging parallelization and recent advances in multi-heuristic search in making combinatorial search tractable. We prove that our system can guaranteedly produce the best explanation of the scene under the chosen cost function, and validate our claims on real world RGB-D test data. Our experimental results show that we can identify and localize objects under heavy occlusion--cases where state-of-the-art methods struggle. version:2
arxiv-1603-05201 | Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units | http://arxiv.org/abs/1603.05201 | id:1603.05201 author:Wenling Shang, Kihyuk Sohn, Diogo Almeida, Honglak Lee category:cs.LG cs.CV  published:2016-03-16 summary:Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification. version:1
arxiv-1603-05191 | Distributed Inexact Damped Newton Method: Data Partitioning and Load-Balancing | http://arxiv.org/abs/1603.05191 | id:1603.05191 author:Chenxin Ma, Martin Takáč category:cs.LG math.OC  published:2016-03-16 summary:In this paper we study inexact dumped Newton method implemented in a distributed environment. We start with an original DiSCO algorithm [Communication-Efficient Distributed Optimization of Self-Concordant Empirical Loss, Yuchen Zhang and Lin Xiao, 2015]. We will show that this algorithm may not scale well and propose an algorithmic modifications which will lead to less communications, better load-balancing and more efficient computation. We perform numerical experiments with an regularized empirical loss minimization instance described by a 273GB dataset. version:1
arxiv-1603-05189 | Applying Artifical Neural Networks To Predict Nominal Vehicle Performance | http://arxiv.org/abs/1603.05189 | id:1603.05189 author:Adam J. Last category:cs.NE cs.SY  published:2016-03-16 summary:This paper investigates the use of artificial neural networks (ANNs) to replace traditional algorithms and manual review for identifying anomalies in vehicle run data. The specific data used for this study is from undersea vehicle qualification tests. Such data is highly non-linear, therefore traditional algorithms are not adequate and manual review is time consuming. By using ANNs to predict nominal vehicle performance based solely on information available pre-run, vehicle deviation from expected performance can be automatically identified in the post-run data. Such capability is only now becoming available due to the rapid increase in understanding of ANN framework and available computing power in the past decade. The ANN trained for the purpose of this investigation is relatively simple, to keep the computing requirements within the parameters of a modern desktop PC. This ANN showed potential in predicting vehicle performance, particularly during transient events within the run data. However, there were also several performance cases, such as steady state operation and cases which did not have sufficient training data, where the ANN showed deficiencies. It is expected that as computational power becomes more readily available, ANN understanding matures, and more training data is acquired from real world tests, the performance predictions of the ANN will surpass traditional algorithms and manual human review. version:1
arxiv-1603-04467 | TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems | http://arxiv.org/abs/1603.04467 | id:1603.04467 author:Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng category:cs.DC cs.LG  published:2016-03-14 summary:TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org. version:2
arxiv-1603-05154 | 2D Discrete Fourier Transform with Simultaneous Edge Artifact Removal for Real-Time Applications | http://arxiv.org/abs/1603.05154 | id:1603.05154 author:Faisal Mahmood, Märt Toots, Lars-Göran Öfverstedt, Ulf Skoglund category:cs.CV cs.AR  published:2016-03-16 summary:Two-Dimensional (2D) Discrete Fourier Transform (DFT) is a basic and computationally intensive algorithm, with a vast variety of applications. 2D images are, in general, non-periodic, but are assumed to be periodic while calculating their DFTs. This leads to cross-shaped artifacts in the frequency domain due to spectral leakage. These artifacts can have critical consequences if the DFTs are being used for further processing. In this paper we present a novel FPGA-based design to calculate high-throughput 2D DFTs with simultaneous edge artifact removal. Standard approaches for removing these artifacts using apodization functions or mirroring, either involve removing critical frequencies or a surge in computation by increasing image size. We use a periodic-plus-smooth decomposition based artifact removal algorithm optimized for FPGA implementation, while still achieving real-time ($\ge$23 frames per second) performance for a 512$\times$512 size image stream. Our optimization approach leads to a significant decrease in external memory utilization thereby avoiding memory conflicts and simplifies the design. We have tested our design on a PXIe based Xilinx Kintex 7 FPGA system communicating with a host PC which gives us the advantage to further expand the design for industrial applications. version:1
arxiv-1603-05152 | Feature Selection as a Multiagent Coordination Problem | http://arxiv.org/abs/1603.05152 | id:1603.05152 author:Kleanthis Malialis, Jun Wang, Gary Brooks, George Frangou category:cs.LG stat.ML  published:2016-03-16 summary:Datasets with hundreds to tens of thousands features is the new norm. Feature selection constitutes a central problem in machine learning, where the aim is to derive a representative set of features from which to construct a classification (or prediction) model for a specific task. Our experimental study involves microarray gene expression datasets, these are high-dimensional and noisy datasets that contain genetic data typically used for distinguishing between benign or malicious tissues or classifying different types of cancer. In this paper, we formulate feature selection as a multiagent coordination problem and propose a novel feature selection method using multiagent reinforcement learning. The central idea of the proposed approach is to "assign" a reinforcement learning agent to each feature where each agent learns to control a single feature, we refer to this approach as MARL. Applying this to microarray datasets creates an enormous multiagent coordination problem between thousands of learning agents. To address the scalability challenge we apply a form of reward shaping called CLEAN rewards. We compare in total nine feature selection methods, including state-of-the-art methods, and show that the proposed method using CLEAN rewards can significantly scale-up, thus outperforming the rest of learning-based methods. We further show that a hybrid variant of MARL achieves the best overall performance. version:1
arxiv-1603-05145 | Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions | http://arxiv.org/abs/1603.05145 | id:1603.05145 author:Qiyang Zhao, Lewis D Griffin category:cs.CV cs.AI cs.LG  published:2016-03-16 summary:Many deep Convolutional Neural Networks (CNN) make incorrect predictions on adversarial samples obtained by imperceptible perturbations of clean samples. We hypothesize that this is caused by a failure to suppress unusual signals within network layers. As remedy we propose the use of Symmetric Activation Functions (SAF) in non-linear signal transducer units. These units suppress signals of exceptional magnitude. We prove that SAF networks can perform classification tasks to arbitrary precision in a simplified situation. In practice, rather than use SAFs alone, we add them into CNNs to improve their robustness. The modified CNNs can be easily trained using popular strategies with the moderate training load. Our experiments on MNIST and CIFAR-10 show that the modified CNNs perform similarly to plain ones on clean samples, and are remarkably more robust against adversarial and nonsense samples. version:1
arxiv-1603-05118 | Recurrent Dropout without Memory Loss | http://arxiv.org/abs/1603.05118 | id:1603.05118 author:Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth category:cs.CL  published:2016-03-16 summary:This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feed-forward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout. version:1
arxiv-1406-5647 | On semidefinite relaxations for the block model | http://arxiv.org/abs/1406.5647 | id:1406.5647 author:Arash A. Amini, Elizaveta Levina category:cs.LG cs.SI stat.ML  published:2014-06-21 summary:The stochastic block model (SBM) is a popular tool for community detection in networks, but fitting it by maximum likelihood (MLE) involves a computationally infeasible optimization problem. We propose a new semidefinite programming (SDP) solution to the problem of fitting the SBM, derived as a relaxation of the MLE. We put ours and previously proposed SDPs in a unified framework, as relaxations of the MLE over various sub-classes of the SBM, revealing a connection to sparse PCA. Our main relaxation, which we call SDP-1, is tighter than other recently proposed SDP relaxations, and thus previously established theoretical guarantees carry over. However, we show that SDP-1 exactly recovers true communities over a wider class of SBMs than those covered by current results. In particular, the assumption of strong assortativity of the SBM, implicit in consistency conditions for previously proposed SDPs, can be relaxed to weak assortativity for our approach, thus significantly broadening the class of SBMs covered by the consistency results. We also show that strong assortativity is indeed a necessary condition for exact recovery for previously proposed SDP approaches and not an artifact of the proofs. Our analysis of SDPs is based on primal-dual witness constructions, which provides some insight into the nature of the solutions of various SDPs. We show how to combine features from SDP-1 and already available SDPs to achieve the most flexibility in terms of both assortativity and block-size constraints, as our relaxation has the tendency to produce communities of similar sizes. This tendency makes it the ideal tool for fitting network histograms, a method gaining popularity in the graphon estimation literature, as we illustrate on an example of a social networks of dolphins. We also provide empirical evidence that SDPs outperform spectral methods for fitting SBMs with a large number of blocks. version:3
arxiv-1603-05060 | Short-term time series prediction using Hilbert space embeddings of autoregressive processes | http://arxiv.org/abs/1603.05060 | id:1603.05060 author:Edgar A. Valencia, Mauricio A. Álvarez category:stat.ML  published:2016-03-16 summary:Linear autoregressive models serve as basic representations of discrete time stochastic processes. Different attempts have been made to provide non-linear versions of the basic autoregressive process, including different versions based on kernel methods. Motivated by the powerful framework of Hilbert space embeddings of distributions, in this paper we apply this methodology for the kernel embedding of an autoregressive process of order $p$. By doing so, we provide a non-linear version of an autoregressive process, that shows increased performance over the linear model in highly complex time series. We use the method proposed for one-step ahead forecasting of different time-series, and compare its performance against other non-linear methods. version:1
arxiv-1603-05015 | Non-linear Dimensionality Regularizer for Solving Inverse Problems | http://arxiv.org/abs/1603.05015 | id:1603.05015 author:Ravi Garg, Anders Eriksson, Ian Reid category:cs.CV  published:2016-03-16 summary:Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (un- known) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regulariza- tion technique for solving such problems without pre-training. We re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy. To the best of our knowledge, ours is the first at- tempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered fac- tors directly in the implicit feature space to create their low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise. We demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise. version:1
arxiv-1603-04992 | Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue | http://arxiv.org/abs/1603.04992 | id:1603.04992 author:Ravi Garg, Vijay Kumar BG, Ian Reid category:cs.CV  published:2016-03-16 summary:A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manu- ally labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth predic- tion, without requiring a pre-training stage or annotated ground truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement, to reconstruct the source image; the photomet- ric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset (without any further augmentation) gives com- parable performance to that of the state of art supervised methods for single view depth estimation. version:1
arxiv-1603-04989 | Scaled stochastic gradient descent for low-rank matrix completion | http://arxiv.org/abs/1603.04989 | id:1603.04989 author:Bamdev Mishra, Rodolphe Sepulchre category:cs.LG math.OC  published:2016-03-16 summary:The paper looks at a scaled variant of the stochastic gradient descent algorithm for the matrix completion problem. Specifically, we propose a novel matrix-scaling of the partial derivatives that acts as an efficient preconditioning for the standard stochastic gradient descent algorithm. This proposed matrix-scaling provides a trade-off between local and global second order information. It also resolves the issue of scale invariance that exists in matrix factorization models. The overall computational complexity is linear with the number of known entries, thereby extending to a large-scale setup. Numerical comparisons show that the proposed algorithm competes favorably with state-of-the-art algorithms on various different benchmarks. version:1
arxiv-1508-07569 | Spherical Conformal Parameterization of Genus-0 Point Clouds for Meshing | http://arxiv.org/abs/1508.07569 | id:1508.07569 author:Gary Pui-Tung Choi, Kin Tat Ho, Lok Ming Lui category:cs.CG cs.CV cs.GR math.DG  published:2015-08-30 summary:Point cloud is the most fundamental representation of 3D geometric objects. Analyzing and processing point cloud surfaces is important in computer graphics and computer vision. However, most of the existing algorithms for surface analysis require connectivity information. Therefore, it is desirable to develop a mesh structure on point clouds. This task can be simplified with the aid of a parameterization. In particular, conformal parameterizations are advantageous in preserving the geometric information of the point cloud data. In this paper, we extend a state-of-the-art spherical conformal parameterization algorithm for genus-0 closed meshes to the case of point clouds, using an improved approximation of the Laplace-Beltrami operator on data points. Then, we propose an iterative scheme called the North-South reiteration for achieving a spherical conformal parameterization. A balancing scheme is introduced to enhance the distribution of the spherical parameterization. High quality triangulations and quadrangulations can then be built on the point clouds with the aid of the parameterizations. Also, the meshes generated are guaranteed to be genus-0 closed meshes. Moreover, using our proposed spherical conformal parameterization, multilevel representations of point clouds can be easily constructed. Experimental results demonstrate the effectiveness of our proposed framework. version:3
arxiv-1603-04981 | Regret-optimal Strategies for Playing Repeated Games with Discounted Losses | http://arxiv.org/abs/1603.04981 | id:1603.04981 author:Vijay Kamble, Patrick Loiseau, Jean Walrand category:cs.GT cs.DS cs.LG stat.ML  published:2016-03-16 summary:The regret-minimization paradigm has emerged as a powerful technique for designing algorithms for online decision-making in adversarial environments. But so far, designing exact minmax-optimal algorithms for minimizing the worst-case regret has proven to be a difficult task in general, with only a few known results in specific settings. In this paper, we present a novel set-valued dynamic programming approach for designing such exact regret-optimal policies for playing repeated games with discounted losses. Our approach first draws the connection between regret minimization, and determining minimal achievable guarantees in repeated games with vector-valued losses. We then characterize the set of these minimal guarantees as the fixed point of a dynamic programming operator defined on the space of Pareto frontiers of convex and compact sets. This approach simultaneously results in the characterization of the optimal strategies that achieve these minimal guarantees, and hence of regret-optimal strategies in the original repeated game. As an illustration of our approach, we design a simple near-optimal strategy for prediction using expert advice for the case of 2 experts. version:1
arxiv-1603-04954 | Online Optimization in Dynamic Environments: Improved Regret Rates for Strongly Convex Problems | http://arxiv.org/abs/1603.04954 | id:1603.04954 author:Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, Alejandro Ribeiro category:cs.LG math.OC  published:2016-03-16 summary:In this paper, we address tracking of a time-varying parameter with unknown dynamics. We formalize the problem as an instance of online optimization in a dynamic setting. Using online gradient descent, we propose a method that sequentially predicts the value of the parameter and in turn suffers a loss. The objective is to minimize the accumulation of losses over the time horizon, a notion that is termed dynamic regret. While existing methods focus on convex loss functions, we consider strongly convex functions so as to provide better guarantees of performance. We derive a regret bound that captures the path-length of the time-varying parameter, defined in terms of the distance between its consecutive values. In other words, the bound represents the natural connection of tracking quality to the rate of change of the parameter. We provide numerical experiments to complement our theoretical findings. version:1
arxiv-1603-04779 | Revisiting Batch Normalization For Practical Domain Adaptation | http://arxiv.org/abs/1603.04779 | id:1603.04779 author:Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, Xiaodi Hou category:cs.CV cs.LG  published:2016-03-15 summary:Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common (yet inconvenient) practice to prepare at least tens of thousands of labeled image to fine-tune a network on every task before the model is ready to use. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization(AdaBN), to increase the generalization ability of a DNN. Our approach is based on the well-known Batch Normalization technique which has become a standard component in modern deep learning. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance. version:2
arxiv-1301-3195 | Audio Classical Composer Identification by Deep Neural Network | http://arxiv.org/abs/1301.3195 | id:1301.3195 author:Zhen Hu, Kun Fu, Changshui Zhang category:cs.NE cs.IR  published:2013-01-15 summary:Audio Classical Composer Identification (ACC) is an important problem in Music Information Retrieval (MIR) which aims at identifying the composer for audio classical music clips. The famous annual competition, Music Information Retrieval Evaluation eXchange (MIREX), also takes it as one of the four training&testing tasks. We built a hybrid model based on Deep Belief Network (DBN) and Stacked Denoising Autoencoder (SDA) to identify the composer from audio signal. As a matter of copyright, sponsors of MIREX cannot publish their data set. We built a comparable data set to test our model. We got an accuracy of 76.26% in our data set which is better than some pure models and shallow models. We think our method is promising even though we test it in a different data set, since our data set is comparable to that in MIREX by size. We also found that samples from different classes become farther away from each other when transformed by more layers in our model. version:7
arxiv-1603-04947 | On the Complexity of One-class SVM for Multiple Instance Learning | http://arxiv.org/abs/1603.04947 | id:1603.04947 author:Zhen Hu, Zhuyin Xue category:cs.LG  published:2016-03-16 summary:In traditional multiple instance learning (MIL), both positive and negative bags are required to learn a prediction function. However, a high human cost is needed to know the label of each bag---positive or negative. Only positive bags contain our focus (positive instances) while negative bags consist of noise or background (negative instances). So we do not expect to spend too much to label the negative bags. Contrary to our expectation, nearly all existing MIL methods require enough negative bags besides positive ones. In this paper we propose an algorithm called "Positive Multiple Instance" (PMI), which learns a classifier given only a set of positive bags. So the annotation of negative bags becomes unnecessary in our method. PMI is constructed based on the assumption that the unknown positive instances in positive bags be similar each other and constitute one compact cluster in feature space and the negative instances locate outside this cluster. The experimental results demonstrate that PMI achieves the performances close to or a little worse than those of the traditional MIL algorithms on benchmark and real data sets. However, the number of training bags in PMI is reduced significantly compared with traditional MIL algorithms. version:1
arxiv-1603-04930 | Deep Fully-Connected Networks for Video Compressive Sensing | http://arxiv.org/abs/1603.04930 | id:1603.04930 author:Michael Iliadis, Leonidas Spinoulas, Aggelos K. Katsaggelos category:cs.CV cs.LG cs.MM  published:2016-03-16 summary:In this work we present a deep learning framework for video compressive sensing. The proposed formulation enables recovery of video frames in a few seconds at significantly improved reconstruction quality compared to previous approaches. Our investigation starts by learning a linear mapping between video sequences and corresponding measured frames which turns out to provide promising results. We then extend the linear formulation to deep fully-connected networks and explore the performance gains using deeper architectures. Our analysis is always driven by the applicability of the proposed framework on existing compressive video architectures. Extensive simulations on several video sequences document the superiority of our approach both quantitatively and qualitatively. Finally, our analysis offers insights into understanding how dataset sizes and number of layers affect reconstruction performance while raising a few points for future investigation. version:1
arxiv-1603-04918 | Data Clustering and Graph Partitioning via Simulated Mixing | http://arxiv.org/abs/1603.04918 | id:1603.04918 author:Shahzad Bhatti, Carolyn Beck, Angelia Nedic category:cs.LG stat.ML  published:2016-03-15 summary:Spectral clustering approaches have led to well-accepted algorithms for finding accurate clusters in a given dataset. However, their application to large-scale datasets has been hindered by computational complexity of eigenvalue decompositions. Several algorithms have been proposed in the recent past to accelerate spectral clustering, however they compromise on the accuracy of the spectral clustering to achieve faster speed. In this paper, we propose a novel spectral clustering algorithm based on a mixing process on a graph. Unlike the existing spectral clustering algorithms, our algorithm does not require computing eigenvectors. Specifically, it finds the equivalent of a linear combination of eigenvectors of the normalized similarity matrix weighted with corresponding eigenvalues. This linear combination is then used to partition the dataset into meaningful clusters. Simulations on real datasets show that partitioning datasets based on such linear combinations of eigenvectors achieves better accuracy than standard spectral clustering methods as the number of clusters increase. Our algorithm can easily be implemented in a distributed setting. version:1
arxiv-1603-04908 | First Person Action-Object Detection with EgoNet | http://arxiv.org/abs/1603.04908 | id:1603.04908 author:Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi category:cs.CV  published:2016-03-15 summary:Objects afford visual sensation and motor actions. A first person camera, placed at the person's head, captures unscripted moments of our visual sensorimotor object interactions. Can a single first person image tell us about our momentary visual attention and motor action with objects, without a gaze tracking device or tactile sensors? To study the holistic correlation of visual attention with motor action, we introduce the concept of action-objects---objects associated with seeing and touching actions, which exhibit characteristic 3D spatial distance and orientation with respect to the person. A predictive action-object model is designed to re-organize the space of interactions in terms of visual and tactile sensations, which is realized by our proposed EgoNet network. EgoNet is composed of two convolutional neural networks: 1) Semantic Gaze Pathway that learns 2D appearance cues with first person coordinate embedding, and 2) 3D Spatial Pathway that focuses on 3D depth and height measurements relative to the person with brightness reflectance attached. Retaining two distinct pathways enables effective learning from a limited number of examples, diversified prediction from complementary visual signals, and flexible architecture that is functional with RGB image without depth information. We show that our model correctly predicts action-objects in a first person image where we outperform the existing approaches across different datasets. version:1
arxiv-1603-04904 | Turing learning: a metric-free approach to inferring behavior and its application to swarms | http://arxiv.org/abs/1603.04904 | id:1603.04904 author:Wei Li, Melvin Gauci, Roderich Gross category:stat.ML cs.LG cs.NE  published:2016-03-15 summary:We propose Turing Learning, a novel system identification method for inferring behavior. Turing Learning simultaneously optimizes models and classifiers. The classifiers are provided with data samples from both an agent and models under observation, and are rewarded for discriminating between them. Conversely, the models are rewarded for 'tricking' the classifiers into categorizing them as the agent. Unlike other methods for system identification, Turing Learning does not require predefined metrics to quantify the difference between the agent and models. We present two case studies with swarms of simulated robots that show that Turing Learning outperforms a metric-based system identification method in terms of model accuracy. The classifiers perform well collectively and could be used to detect abnormal behavior in the swarm. Moreover, we show that Turing Learning also successfully infers the behavior of physical robot swarms. The results show that collective behaviors can be directly inferred from motion trajectories of a single agent in the swarm, which may have significant implications for the study of animal collectives. version:1
arxiv-1603-04882 | Bias Correction for Regularized Regression and its Application in Learning with Streaming Data | http://arxiv.org/abs/1603.04882 | id:1603.04882 author:Qiang Wu category:stat.ML cs.LG  published:2016-03-15 summary:We propose an approach to reduce the bias of ridge regression and regularization kernel network. When applied to a single data set the new algorithms have comparable learning performance with the original ones. When applied to incremental learning with block wise streaming data the new algorithms are more efficient due to bias reduction. Both theoretical characterizations and simulation studies are used to verify the effectiveness of these new algorithms. version:1
arxiv-1603-04871 | Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation | http://arxiv.org/abs/1603.04871 | id:1603.04871 author:Zhicheng Yan, Hao Zhang, Yangqing Jia, Thomas Breuel, Yizhou Yu category:cs.CV  published:2016-03-15 summary:State-of-the-art results of semantic segmentation are established by Fully Convolutional neural Networks (FCNs). FCNs rely on cascaded convolutional and pooling layers to gradually enlarge the receptive fields of neurons, resulting in an indirect way of modeling the distant contextual dependence. In this work, we advocate the use of spatially recurrent layers (i.e. ReNet layers) which directly capture global contexts and lead to improved feature representations. We demonstrate the effectiveness of ReNet layers by building a Naive deep ReNet (N-ReNet), which achieves competitive performance on Stanford Background dataset. Furthermore, we integrate ReNet layers with FCNs, and develop a novel Hybrid deep ReNet (H-ReNet). It enjoys a few remarkable properties, including full-image receptive fields, end-to-end training, and efficient network execution. On the PASCAL VOC 2012 benchmark, the H-ReNet improves the results of state-of-the-art approaches Piecewise, CRFasRNN and DeepParsing by 3.6%, 2.3% and 0.2%, respectively, and achieves the highest IoUs for 13 out of the 20 object classes. version:1
arxiv-1603-04868 | Efficient Globally Optimal Point Cloud Alignment using Bayesian Nonparametric Mixtures | http://arxiv.org/abs/1603.04868 | id:1603.04868 author:Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III category:cs.CV  published:2016-03-15 summary:Point cloud alignment is a common problem in computer vision and robotics, with applications ranging from object recognition to reconstruction. We propose a novel approach to the alignment problem that utilizes Bayesian nonparametrics to describe the point cloud and surface normal densities, and the branch and bound (BB) paradigm to recover the optimal relative transformation. BB relies on a novel, refinable, approximately-uniform tessellation of the rotation space using 4D tetrahedra which leads to more efficient BB operation in comparison to the common axis-angle tessellation. For this novel tessellation, we provide upper and lower objective function bounds, and prove convergence and optimality of the BB approach under mild assumptions. Finally, we empirically demonstrate the efficiency of the proposed approach as well as its robustness to suboptimal real-world conditions such as missing data and partial overlap. version:1
arxiv-1603-04833 | Ensemble of Deep Convolutional Neural Networks for Learning to Detect Retinal Vessels in Fundus Images | http://arxiv.org/abs/1603.04833 | id:1603.04833 author:Debapriya Maji, Anirban Santara, Pabitra Mitra, Debdoot Sheet category:cs.LG cs.CV stat.ML  published:2016-03-15 summary:Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. In this work we present a computational imaging framework using deep and ensemble learning for reliable detection of blood vessels in fundus color images. An ensemble of deep convolutional neural networks is trained to segment vessel and non-vessel areas of a color fundus image. During inference, the responses of the individual ConvNets of the ensemble are averaged to form the final segmentation. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with maximum average accuracy of 94.7\% and area under ROC curve of 0.9283. version:1
arxiv-1602-03943 | Second Order Stochastic Optimization in Linear Time | http://arxiv.org/abs/1602.03943 | id:1602.03943 author:Naman Agarwal, Brian Bullins, Elad Hazan category:stat.ML cs.LG  published:2016-02-12 summary:Stochastic optimization and, in particular, first-order stochastic methods are a cornerstone of modern machine learning due to their extremely efficient per-iteration computational cost. Second-order methods, while able to provide faster per-iteration convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop a second-order stochastic method for optimization problems arising in machine learning based on novel matrix randomization techniques that match the per-iteration cost of gradient descent, yet enjoy the linear-convergence properties of second-order optimization. We also consider the special case of self-concordant functions where we show that a first order method can achieve linear convergence with guarantees independent of the condition number. We demonstrate significant speedups for training linear classifiers over several convex benchmarks. version:3
arxiv-1603-04350 | An optimal algorithm for bandit convex optimization | http://arxiv.org/abs/1603.04350 | id:1603.04350 author:Elad Hazan, Yuanzhi Li category:cs.LG cs.DS G.1.6  published:2016-03-14 summary:We consider the problem of online convex optimization against an arbitrary adversary with bandit feedback, known as bandit convex optimization. We give the first $\tilde{O}(\sqrt{T})$-regret algorithm for this setting based on a novel application of the ellipsoid method to online learning. This bound is known to be tight up to logarithmic factors. Our analysis introduces new tools in discrete convex geometry. version:2
arxiv-1603-04771 | A Neural Approach to Blind Motion Deblurring | http://arxiv.org/abs/1603.04771 | id:1603.04771 author:Ayan Chakrabarti category:cs.CV  published:2016-03-15 summary:We present a new method for blind motion deblurring that uses a neural network trained to compute estimates of sharp image patches from observations that are blurred by an unknown motion kernel. Instead of regressing directly to patch intensities, this network learns to predict the complex Fourier coefficients of a deconvolution filter to be applied to the input patch for restoration. For inference, we apply the network independently to all overlapping patches in the observed image, and average its outputs to form an initial estimate of the sharp image. We then explicitly estimate a single global blur kernel by relating this estimate to the observed image, and finally perform non-blind deconvolution with this kernel. Our method exhibits accuracy and robustness close to state-of-the-art iterative methods, while being much faster when parallelized on GPU hardware. version:1
arxiv-1603-04767 | Evaluating the word-expert approach for Named-Entity Disambiguation | http://arxiv.org/abs/1603.04767 | id:1603.04767 author:Angel X. Chang, Valentin I. Spitkovsky, Christopher D. Manning, Eneko Agirre category:cs.CL  published:2016-03-15 summary:Named Entity Disambiguation (NED) is the task of linking a named-entity mention to an instance in a knowledge-base, typically Wikipedia. This task is closely related to word-sense disambiguation (WSD), where the supervised word-expert approach has prevailed. In this work we present the results of the word-expert approach to NED, where one classifier is built for each target entity mention string. The resources necessary to build the system, a dictionary and a set of training instances, have been automatically derived from Wikipedia. We provide empirical evidence of the value of this approach, as well as a study of the differences between WSD and NED, including ambiguity and synonymy statistics. version:1
arxiv-1603-04747 | Topic Modeling Using Distributed Word Embeddings | http://arxiv.org/abs/1603.04747 | id:1603.04747 author:Ramandeep S Randhawa, Parag Jain, Gagan Madan category:cs.CL  published:2016-03-15 summary:We propose a new algorithm for topic modeling, Vec2Topic, that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings. Our technique is unsupervised and generates a list of topics ranked with respect to importance. We find that it works better than existing topic modeling techniques such as Latent Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats, etc., where topics are diffused across the corpus. We also find that Vec2Topic works equally well for non-user generated content, such as papers, reports, etc., and for small corpora such as a single-document. version:1
arxiv-1506-09016 | Online Learning to Sample | http://arxiv.org/abs/1506.09016 | id:1506.09016 author:Guillaume Bouchard, Théo Trouillon, Julien Perez, Adrien Gaidon category:cs.LG cs.CV cs.NA math.OC stat.ML  published:2015-06-30 summary:Stochastic Gradient Descent (SGD) is one of the most widely used techniques for online optimization in machine learning. In this work, we accelerate SGD by adaptively learning how to sample the most useful training examples at each time step. First, we show that SGD can be used to learn the best possible sampling distribution of an importance sampling estimator. Second, we show that the sampling distribution of an SGD algorithm can be estimated online by incrementally minimizing the variance of the gradient. The resulting algorithm - called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to optimize, as well as a set of parameters to sample learning examples. We show that AWSGD yields faster convergence in three different applications: (i) image classification with deep features, where the sampling of images depends on their labels, (ii) matrix factorization, where rows and columns are not sampled uniformly, and (iii) reinforcement learning, where the optimized and exploration policies are estimated at the same time, where our approach corresponds to an off-policy gradient algorithm. version:2
arxiv-1603-04713 | Modeling Time Series Similarity with Siamese Recurrent Networks | http://arxiv.org/abs/1603.04713 | id:1603.04713 author:Wenjie Pei, David M. J. Tax, Laurens van der Maaten category:cs.CV  published:2016-03-15 summary:Traditional techniques for measuring similarities between time series are based on handcrafted similarity measures, whereas more recent learning-based approaches cannot exploit external supervision. We combine ideas from time-series modeling and metric learning, and study siamese recurrent networks (SRNs) that minimize a classification loss to learn a good similarity measure between time series. Specifically, our approach learns a vectorial representation for each time series in such a way that similar time series are modeled by similar representations, and dissimilar time series by dissimilar representations. Because it is a similarity prediction models, SRNs are particularly well-suited to challenging scenarios such as signature recognition, in which each person is a separate class and very few examples per class are available. We demonstrate the potential merits of SRNs in within-domain and out-of-domain classification experiments and in one-shot learning experiments on tasks such as signature, voice, and sign language recognition. version:1
