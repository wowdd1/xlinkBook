arxiv-1402-3631 | Privately Solving Linear Programs | http://arxiv.org/abs/1402.3631 | id:1402.3631 author:Justin Hsu, Aaron Roth, Tim Roughgarden, Jonathan Ullman category:cs.DS cs.CR cs.LG  published:2014-02-15 summary:In this paper, we initiate the systematic study of solving linear programs under differential privacy. The first step is simply to define the problem: to this end, we introduce several natural classes of private linear programs that capture different ways sensitive data can be incorporated into a linear program. For each class of linear programs we give an efficient, differentially private solver based on the multiplicative weights framework, or we give an impossibility result. version:2
arxiv-1311-3576 | Reproducing kernel Hilbert space based estimation of systems of ordinary differential equations | http://arxiv.org/abs/1311.3576 | id:1311.3576 author:Javier González, Ivan Vujačić, Ernst Wit category:stat.ME stat.ML  published:2013-11-14 summary:Non-linear systems of differential equations have attracted the interest in fields like system biology, ecology or biochemistry, due to their flexibility and their ability to describe dynamical systems. Despite the importance of such models in many branches of science they have not been the focus of systematic statistical analysis until recently. In this work we propose a general approach to estimate the parameters of systems of differential equations measured with noise. Our methodology is based on the maximization of the penalized likelihood where the system of differential equations is used as a penalty. To do so, we use a Reproducing Kernel Hilbert Space approach that allows to formulate the estimation problem as an unconstrained numeric maximization problem easy to solve. The proposed method is tested with synthetically simulated data and it is used to estimate the unobserved transcription factor CdaR in Steptomyes coelicolor using gene expression data of the genes it regulates. version:2
arxiv-1405-1967 | Image Resolution and Contrast Enhancement of Satellite Geographical Images with Removal of Noise using Wavelet Transforms | http://arxiv.org/abs/1405.1967 | id:1405.1967 author:Prajakta P. Khairnar, C. A. Manjare category:cs.CV  published:2014-05-08 summary:In this paper the technique for resolution and contrast enhancement of satellite geographical images based on discrete wavelet transform (DWT), stationary wavelet transform (SWT) and singular value decomposition (SVD) has been proposed. In this, the noise is added in the input low resolution and low contrast image. The median filter is used remove noise from the input image. This low resolution, low contrast image without noise is decomposed into four sub-bands by using DWT and SWT. The resolution enhancement technique is based on the interpolation of high frequency components obtained by DWT and input image. SWT is used to enhance input image. DWT is used to decompose an image into four frequency sub bands and these four sub-bands are interpolated using bicubic interpolation technique. All these sub-bands are reconstructed as high resolution image by using inverse DWT (IDWT). To increase the contrast the proposed technique uses DWT and SVD. GHE is used to equalize an image. The equalized image is decomposed into four sub-bands using DWT and new LL sub-band is reconstructed using SVD. All sub-bands are reconstructed using IDWT to generate high resolution and contrast image over conventional techniques. The experimental result shows superiority of the proposed technique over conventional techniques. Key words: Discrete wavelet transform (DWT), General histogram equalization (GHE), Median filter, Singular value decomposition (SVD), Stationary wavelet transform (SWT). version:1
arxiv-1405-1924 | An Expert System for Automatic Reading of A Text Written in Standard Arabic | http://arxiv.org/abs/1405.1924 | id:1405.1924 author:Tebbi Hanane, Azzoune Hamid category:cs.CL  published:2014-05-08 summary:In this work we present our expert system of Automatic reading or speech synthesis based on a text written in Standard Arabic, our work is carried out in two great stages: the creation of the sound data base, and the transformation of the written text into speech (Text To Speech TTS). This transformation is done firstly by a Phonetic Orthographical Transcription (POT) of any written Standard Arabic text with the aim of transforming it into his corresponding phonetics sequence, and secondly by the generation of the voice signal which corresponds to the chain transcribed. We spread out the different of conception of the system, as well as the results obtained compared to others works studied to realize TTS based on Standard Arabic. version:1
arxiv-1405-1815 | Implementation And Performance Evaluation Of Background Subtraction Algorithms | http://arxiv.org/abs/1405.1815 | id:1405.1815 author:Deepjoy Das, Dr. Sarat Saharia category:cs.CV  published:2014-05-08 summary:The study evaluates three background subtraction techniques. The techniques ranges from very basic algorithm to state of the art published techniques categorized based on speed, memory requirements and accuracy. Such a review can effectively guide the designer to select the most suitable method for a given application in a principled way. The algorithms used in the study ranges from varying levels of accuracy and computational complexity. Few of them can also deal with real time challenges like rain, snow, hails, swaying branches, objects overlapping, varying light intensity or slow moving objects. version:1
arxiv-1405-0133 | Geodesic Distance Function Learning via Heat Flow on Vector Fields | http://arxiv.org/abs/1405.0133 | id:1405.0133 author:Binbin Lin, Ji Yang, Xiaofei He, Jieping Ye category:cs.LG math.DG stat.ML  published:2014-05-01 summary:Learning a distance function or metric on a given data manifold is of great importance in machine learning and pattern recognition. Many of the previous works first embed the manifold to Euclidean space and then learn the distance function. However, such a scheme might not faithfully preserve the distance function if the original manifold is not Euclidean. Note that the distance function on a manifold can always be well-defined. In this paper, we propose to learn the distance function directly on the manifold without embedding. We first provide a theoretical characterization of the distance function by its gradient field. Based on our theoretical analysis, we propose to first learn the gradient field of the distance function and then learn the distance function itself. Specifically, we set the gradient field of a local distance function as an initial vector field. Then we transport it to the whole manifold via heat flow on vector fields. Finally, the geodesic distance function can be obtained by requiring its gradient field to be close to the normalized vector field. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm. version:2
arxiv-1303-0551 | Sparse PCA through Low-rank Approximations | http://arxiv.org/abs/1303.0551 | id:1303.0551 author:Dimitris S. Papailiopoulos, Alexandros G. Dimakis, Stavros Korokythakis category:stat.ML cs.IT cs.LG math.IT  published:2013-03-03 summary:We introduce a novel algorithm that computes the $k$-sparse principal component of a positive semidefinite matrix $A$. Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of $A$. We obtain provable approximation guarantees that depend on the spectral decay profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation. For example, if the eigenvalues of $A$ follow a power-law decay, we obtain a polynomial-time approximation algorithm for any desired accuracy. A key algorithmic component of our scheme is a combinatorial feature elimination step that is provably safe and in practice significantly reduces the running complexity of our algorithm. We implement our algorithm and test it on multiple artificial and real data sets. Due to the feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes. Our experimental evaluation shows that our scheme is nearly optimal while finding very sparse vectors. We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms in all tested data sets. version:2
arxiv-1405-1773 | On Tensor Completion via Nuclear Norm Minimization | http://arxiv.org/abs/1405.1773 | id:1405.1773 author:Ming Yuan, Cun-Hui Zhang category:stat.ML cs.IT math.IT math.NA math.OC math.PR  published:2014-05-07 summary:Many problems can be formulated as recovering a low-rank tensor. Although an increasingly common task, tensor recovery remains a challenging problem because of the delicacy associated with the decomposition of higher order tensors. To overcome these difficulties, existing approaches often proceed by unfolding tensors into matrices and then apply techniques for matrix completion. We show here that such matricization fails to exploit the tensor structure and may lead to suboptimal procedure. More specifically, we investigate a convex optimization approach to tensor completion by directly minimizing a tensor nuclear norm and prove that this leads to an improved sample size requirement. To establish our results, we develop a series of algebraic and probabilistic techniques such as characterization of subdifferetial for tensor nuclear norm and concentration inequalities for tensor martingales, which may be of independent interests and could be useful in other tensor related problems. version:1
arxiv-1405-2048 | Learning Alternative Name Spellings | http://arxiv.org/abs/1405.2048 | id:1405.2048 author:Jeffrey Sukharev, Leonid Zhukov, Alexandrin Popescul category:cs.IR cs.CL  published:2014-05-07 summary:Name matching is a key component of systems for entity resolution or record linkage. Alternative spellings of the same names are a com- mon occurrence in many applications. We use the largest collection of genealogy person records in the world together with user search query logs to build name matching models. The procedure for building a crowd-sourced training set is outlined together with the presentation of our method. We cast the problem of learning alternative spellings as a machine translation problem at the character level. We use in- formation retrieval evaluation methodology to show that this method substantially outperforms on our data a number of standard well known phonetic and string similarity methods in terms of precision and re- call. Additionally, we rigorously compare the performance of standard methods when compared with each other. Our result can lead to a significant practical impact in entity resolution applications. version:1
arxiv-1405-1717 | Entropy Based Cartoon Texture Separation | http://arxiv.org/abs/1405.1717 | id:1405.1717 author:Kutlu Emre Yilmaz category:cs.CV  published:2014-05-07 summary:Separating an image into cartoon and texture components comes useful in image processing applications, such as image compression, image segmentation, image inpainting. Yves Meyer's influential cartoon texture decomposition model involves deriving an energy functional by choosing appropriate spaces and functionals. Minimizers of the derived energy functional are cartoon and texture components of an image. In this study, cartoon part of an image is separated, by reconstructing it from pixels of multi scale Total-Variation filtered versions of the original image which is sought to be decomposed into cartoon and texture parts. An information theoretic pixel by pixel selection criteria is employed to choose the contributing pixels and their scales. version:1
arxiv-1405-1605 | DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News | http://arxiv.org/abs/1405.1605 | id:1405.1605 author:Jacopo Staiano, Marco Guerini category:cs.CL cs.CY  published:2014-05-07 summary:While many lexica annotated with words polarity are available for sentiment analysis, very few tackle the harder task of emotion analysis and are usually quite limited in coverage. In this paper, we present a novel approach for extracting - in a totally automated way - a high-coverage and high-precision lexicon of roughly 37 thousand terms annotated with emotion scores, called DepecheMood. Our approach exploits in an original way 'crowd-sourced' affective annotation implicitly provided by readers of news articles from rappler.com. By providing new state-of-the-art performances in unsupervised settings for regression and classification tasks, even using a na\"{\i}ve approach, our experiments show the beneficial impact of harvesting social media data for affective lexicon building. version:1
arxiv-1405-1580 | PAC-Bayes Mini-tutorial: A Continuous Union Bound | http://arxiv.org/abs/1405.1580 | id:1405.1580 author:Tim van Erven category:stat.ML  published:2014-05-07 summary:When I first encountered PAC-Bayesian concentration inequalities they seemed to me to be rather disconnected from good old-fashioned results like Hoeffding's and Bernstein's inequalities. But, at least for one flavour of the PAC-Bayesian bounds, there is actually a very close relation, and the main innovation is a continuous version of the union bound, along with some ingenious applications. Here's the gist of what's going on, presented from a machine learning perspective. version:1
arxiv-1405-1535 | Learning Boolean Halfspaces with Small Weights from Membership Queries | http://arxiv.org/abs/1405.1535 | id:1405.1535 author:Hasan Abasi, Ali Z. Abdi, Nader H. Bshouty category:cs.LG  published:2014-05-07 summary:We consider the problem of proper learning a Boolean Halfspace with integer weights $\{0,1,\ldots,t\}$ from membership queries only. The best known algorithm for this problem is an adaptive algorithm that asks $n^{O(t^5)}$ membership queries where the best lower bound for the number of membership queries is $n^t$ [Learning Threshold Functions with Small Weights Using Membership Queries. COLT 1999] In this paper we close this gap and give an adaptive proper learning algorithm with two rounds that asks $n^{O(t)}$ membership queries. We also give a non-adaptive proper learning algorithm that asks $n^{O(t^3)}$ membership queries. version:1
arxiv-1405-1513 | A Mathematical Theory of Learning | http://arxiv.org/abs/1405.1513 | id:1405.1513 author:Ibrahim Alabdulmohsin category:cs.LG cs.AI cs.IT math.IT  published:2014-05-07 summary:In this paper, a mathematical theory of learning is proposed that has many parallels with information theory. We consider Vapnik's General Setting of Learning in which the learning process is defined to be the act of selecting a hypothesis in response to a given training set. Such hypothesis can, for example, be a decision boundary in classification, a set of centroids in clustering, or a set of frequent item-sets in association rule mining. Depending on the hypothesis space and how the final hypothesis is selected, we show that a learning process can be assigned a numeric score, called learning capacity, which is analogous to Shannon's channel capacity and satisfies similar interesting properties as well such as the data-processing inequality and the information-cannot-hurt inequality. In addition, learning capacity provides the tightest possible bound on the difference between true risk and empirical risk of the learning process for all loss functions that are parametrized by the chosen hypothesis. It is also shown that the notion of learning capacity equivalently quantifies how sensitive the choice of the final hypothesis is to a small perturbation in the training set. Consequently, algorithmic stability is both necessary and sufficient for generalization. While the theory does not rely on concentration inequalities, we finally show that analogs to classical results in learning theory using the Probably Approximately Correct (PAC) model can be immediately deduced using this theory, and conclude with information-theoretic bounds to learning capacity. version:1
arxiv-1405-1445 | Pulling back error to the hidden-node parameter technology: Single-hidden-layer feedforward network without output weight | http://arxiv.org/abs/1405.1445 | id:1405.1445 author:Yimin Yang, Q. M. Jonathan Wu, Guangbin Huang, Yaonan Wang category:cs.NE 68Txx F.1.1  published:2014-05-06 summary:According to conventional neural network theories, the feature of single-hidden-layer feedforward neural networks(SLFNs) resorts to parameters of the weighted connections and hidden nodes. SLFNs are universal approximators when at least the parameters of the networks including hidden-node parameter and output weight are exist. Unlike above neural network theories, this paper indicates that in order to let SLFNs work as universal approximators, one may simply calculate the hidden node parameter only and the output weight is not needed at all. In other words, this proposed neural network architecture can be considered as a standard SLFNs with fixing output weight equal to an unit vector. Further more, this paper presents experiments which show that the proposed learning method tends to extremely reduce network output error to a very small number with only 1 hidden node. Simulation results demonstrate that the proposed method can provide several to thousands of times faster than other learning algorithm including BP, SVM/SVR and other ELM methods. version:1
arxiv-1405-1444 | Understanding Protein Dynamics with L1-Regularized Reversible Hidden Markov Models | http://arxiv.org/abs/1405.1444 | id:1405.1444 author:Robert T. McGibbon, Bharath Ramsundar, Mohammad M. Sultan, Gert Kiss, Vijay S. Pande category:q-bio.BM stat.AP stat.ML  published:2014-05-06 summary:We present a machine learning framework for modeling protein dynamics. Our approach uses L1-regularized, reversible hidden Markov models to understand large protein datasets generated via molecular dynamics simulations. Our model is motivated by three design principles: (1) the requirement of massive scalability; (2) the need to adhere to relevant physical law; and (3) the necessity of providing accessible interpretations, critical for both cellular biology and rational drug design. We present an EM algorithm for learning and introduce a model selection criteria based on the physical notion of convergence in relaxation timescales. We contrast our model with standard methods in biophysics and demonstrate improved robustness. We implement our algorithm on GPUs and apply the method to two large protein simulation datasets generated respectively on the NCSA Bluewaters supercomputer and the Folding@Home distributed computing network. Our analysis identifies the conformational dynamics of the ubiquitin protein critical to cellular signaling, and elucidates the stepwise activation mechanism of the c-Src kinase protein. version:1
arxiv-1405-1438 | The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter | http://arxiv.org/abs/1405.1438 | id:1405.1438 author:Chenhao Tan, Lillian Lee, Bo Pang category:cs.SI cs.CL physics.soc-ph  published:2014-05-06 summary:Consider a person trying to spread an important message on a social network. He/she can spend hours trying to craft the message. Does it actually matter? While there has been extensive prior work looking into predicting popularity of social-media content, the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic. To control for these confounding factors, we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording. Given such pairs, we ask: which version attracts more retweets? This turns out to be a more difficult task than predicting popular topics. Still, humans can answer this question better than chance (but far from perfectly), and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data. version:1
arxiv-1405-1436 | Training Restricted Boltzmann Machine by Perturbation | http://arxiv.org/abs/1405.1436 | id:1405.1436 author:Siamak Ravanbakhsh, Russell Greiner, Brendan Frey category:cs.NE cs.LG stat.ML  published:2014-05-06 summary:A new approach to maximum likelihood learning of discrete graphical models and RBM in particular is introduced. Our method, Perturb and Descend (PD) is inspired by two ideas (I) perturb and MAP method for sampling (II) learning by Contrastive Divergence minimization. In contrast to perturb and MAP, PD leverages training data to learn the models that do not allow efficient MAP estimation. During the learning, to produce a sample from the current model, we start from a training data and descend in the energy landscape of the "perturbed model", for a fixed number of steps, or until a local optima is reached. For RBM, this involves linear calculations and thresholding which can be very fast. Furthermore we show that the amount of perturbation is closely related to the temperature parameter and it can regularize the model by producing robust features resulting in sparse hidden layer activation. version:1
arxiv-1405-1429 | How Community Feedback Shapes User Behavior | http://arxiv.org/abs/1405.1429 | id:1405.1429 author:Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8  published:2014-05-06 summary:Social media systems rely on user feedback and rating mechanisms for personalization, ranking, and content filtering. However, when users evaluate content contributed by fellow users (e.g., by liking a post or voting on a comment), these evaluations create complex social feedback effects. This paper investigates how ratings on a piece of content affect its author's future behavior. By studying four large comment-based news communities, we find that negative feedback leads to significant behavioral changes that are detrimental to the community. Not only do authors of negatively-evaluated content contribute more, but also their future posts are of lower quality, and are perceived by the community as such. Moreover, these authors are more likely to subsequently evaluate their fellow users negatively, percolating these effects through the community. In contrast, positive feedback does not carry similar effects, and neither encourages rewarded authors to write more, nor improves the quality of their posts. Interestingly, the authors that receive no feedback are most likely to leave a community. Furthermore, a structural analysis of the voter network reveals that evaluations polarize the community the most when positive and negative votes are equally split. version:1
arxiv-1405-1406 | D-Bees: A Novel Method Inspired by Bee Colony Optimization for Solving Word Sense Disambiguation | http://arxiv.org/abs/1405.1406 | id:1405.1406 author:Sallam Abualhaija, Karl-Heinz Zimmermann category:cs.CL  published:2014-05-06 summary:Word sense disambiguation (WSD) is a problem in the field of computational linguistics given as finding the intended sense of a word (or a set of words) when it is activated within a certain context. WSD was recently addressed as a combinatorial optimization problem in which the goal is to find a sequence of senses that maximize the semantic relatedness among the target words. In this article, a novel algorithm for solving the WSD problem called D-Bees is proposed which is inspired by bee colony optimization (BCO)where artificial bee agents collaborate to solve the problem. The D-Bees algorithm is evaluated on a standard dataset (SemEval 2007 coarse-grained English all-words task corpus)and is compared to simulated annealing, genetic algorithms, and two ant colony optimization techniques (ACO). It will be observed that the BCO and ACO approaches are on par. version:1
arxiv-1405-0049 | Exemplar Dynamics Models of the Stability of Phonological Categories | http://arxiv.org/abs/1405.0049 | id:1405.0049 author:P. F. Tupper category:cs.CL cs.SD 91F20  published:2014-04-30 summary:We develop a model for the stability and maintenance of phonological categories. Examples of phonological categories are vowel sounds such as "i" and "e". We model such categories as consisting of collections of labeled exemplars that language users store in their memory. Each exemplar is a detailed memory of an instance of the linguistic entity in question. Starting from an exemplar-level model we derive integro-differential equations for the long-term evolution of the density of exemplars in different portions of phonetic space. Using these latter equations we investigate under what conditions two phonological categories merge or not. Our main conclusion is that for the preservation of distinct phonological categories, it is necessary that anomalous speech tokens of a given category are discarded, and not merely stored in memory as an exemplar of another category. version:2
arxiv-1405-1346 | Automatic Method Of Domain Ontology Construction based on Characteristics of Corpora POS-Analysis | http://arxiv.org/abs/1405.1346 | id:1405.1346 author:Olena Orobinska category:cs.CL  published:2014-05-06 summary:It is now widely recognized that ontologies, are one of the fundamental cornerstones of knowledge-based systems. What is lacking, however, is a currently accepted strategy of how to build ontology; what kinds of the resources and techniques are indispensables to optimize the expenses and the time on the one hand and the amplitude, the completeness, the robustness of en ontology on the other hand. The paper offers a semi-automatic ontology construction method from text corpora in the domain of radiological protection. This method is composed from next steps: 1) text annotation with part-of-speech tags; 2) revelation of the significant linguistic structures and forming the templates; 3) search of text fragments corresponding to these templates; 4) basic ontology instantiation process version:1
arxiv-1405-0586 | On Lipschitz Continuity and Smoothness of Loss Functions in Learning to Rank | http://arxiv.org/abs/1405.0586 | id:1405.0586 author:Ambuj Tewari, Sougata Chaudhuri category:cs.LG stat.ML  published:2014-05-03 summary:In binary classification and regression problems, it is well understood that Lipschitz continuity and smoothness of the loss function play key roles in governing generalization error bounds for empirical risk minimization algorithms. In this paper, we show how these two properties affect generalization error bounds in the learning to rank problem. The learning to rank problem involves vector valued predictions and therefore the choice of the norm with respect to which Lipschitz continuity and smoothness are defined becomes crucial. Choosing the $\ell_\infty$ norm in our definition of Lipschitz continuity allows us to improve existing bounds. Furthermore, under smoothness assumptions, our choice enables us to prove rates that interpolate between $1/\sqrt{n}$ and $1/n$ rates. Application of our results to ListNet, a popular learning to rank method, gives state-of-the-art performance guarantees. version:2
arxiv-1405-1207 | Nuclear Norm based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes | http://arxiv.org/abs/1405.1207 | id:1405.1207 author:Jian Yang, Jianjun Qian, Lei Luo, Fanlong Zhang, Yicheng Gao category:cs.CV  published:2014-05-06 summary:Recently regression analysis becomes a popular tool for face recognition. The existing regression methods all use the one-dimensional pixel-based error model, which characterizes the representation error pixel by pixel individually and thus neglects the whole structure of the error image. We observe that occlusion and illumination changes generally lead to a low-rank error image. To make use of this low-rank structural information, this paper presents a two-dimensional image matrix based error model, i.e. matrix regression, for face representation and classification. Our model uses the minimal nuclear norm of representation error image as a criterion, and the alternating direction method of multipliers method to calculate the regression coefficients. Compared with the current regression methods, the proposed Nuclear Norm based Matrix Regression (NMR) model is more robust for alleviating the effect of illumination, and more intuitive and powerful for removing the structural noise caused by occlusion. We experiment using four popular face image databases, the Extended Yale B database, the AR database, the Multi-PIE and the FRGC database. Experimental results demonstrate the performance advantage of NMR over the state-of-the-art regression based face recognition methods. version:1
arxiv-1311-5591 | PANDA: Pose Aligned Networks for Deep Attribute Modeling | http://arxiv.org/abs/1311.5591 | id:1311.5591 author:Ning Zhang, Manohar Paluri, Marc'Aurelio Ranzato, Trevor Darrell, Lubomir Bourdev category:cs.CV  published:2013-11-21 summary:We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the effects of pose and viewpoint. Discounting for pose variation would require training on very large labeled datasets which are not presently available. Part-based models, such as poselets and DPM have been shown to perform well for this problem but they are limited by shallow low-level features. We propose a new method which combines part-based models and deep learning by training pose-normalized CNNs. We show substantial improvement vs. state-of-the-art methods on challenging attribute classification tasks in unconstrained settings. Experiments confirm that our method outperforms both the best part-based methods on this problem and conventional CNNs trained on the full bounding box of the person. version:2
arxiv-1405-0947 | Learning Bilingual Word Representations by Marginalizing Alignments | http://arxiv.org/abs/1405.0947 | id:1405.0947 author:Tomáš Kočiský, Karl Moritz Hermann, Phil Blunsom category:cs.CL  published:2014-05-05 summary:We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data. By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments. The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art. version:1
arxiv-1405-0941 | Towards a Benchmark of Natural Language Arguments | http://arxiv.org/abs/1405.0941 | id:1405.0941 author:Elena Cabrio, Serena Villata category:cs.AI cs.CL  published:2014-05-05 summary:The connections among natural language processing and argumentation theory are becoming stronger in the latest years, with a growing amount of works going in this direction, in different scenarios and applying heterogeneous techniques. In this paper, we present two datasets we built to cope with the combination of the Textual Entailment framework and bipolar abstract argumentation. In our approach, such datasets are used to automatically identify through a Textual Entailment system the relations among the arguments (i.e., attack, support), and then the resulting bipolar argumentation graphs are analyzed to compute the accepted arguments. version:1
arxiv-1405-1379 | Design and Optimization of a Speech Recognition Front-End for Distant-Talking Control of a Music Playback Device | http://arxiv.org/abs/1405.1379 | id:1405.1379 author:Ramin Pichevar, Jason Wung, Daniele Giacobello, Joshua Atkins category:cs.SD cs.CL  published:2014-05-05 summary:This paper addresses the challenging scenario for the distant-talking control of a music playback device, a common portable speaker with four small loudspeakers in close proximity to one microphone. The user controls the device through voice, where the speech-to-music ratio can be as low as -30 dB during music playback. We propose a speech enhancement front-end that relies on known robust methods for echo cancellation, double-talk detection, and noise suppression, as well as a novel adaptive quasi-binary mask that is well suited for speech recognition. The optimization of the system is then formulated as a large scale nonlinear programming problem where the recognition rate is maximized and the optimal values for the system parameters are found through a genetic algorithm. We validate our methodology by testing over the TIMIT database for different music playback levels and noise types. Finally, we show that the proposed front-end allows a natural interaction with the device for limited-vocabulary voice commands. version:1
arxiv-1310-3892 | Ridge Fusion in Statistical Learning | http://arxiv.org/abs/1310.3892 | id:1310.3892 author:Bradley S. Price, Charles J. Geyer, Adam J. Rothman category:stat.ML cs.LG stat.CO  published:2013-10-15 summary:We propose a penalized likelihood method to jointly estimate multiple precision matrices for use in quadratic discriminant analysis and model based clustering. A ridge penalty and a ridge fusion penalty are used to introduce shrinkage and promote similarity between precision matrix estimates. Block-wise coordinate descent is used for optimization, and validation likelihood is used for tuning parameter selection. Our method is applied in quadratic discriminant analysis and semi-supervised model based clustering. version:3
arxiv-1405-1027 | K-NS: Section-Based Outlier Detection in High Dimensional Space | http://arxiv.org/abs/1405.1027 | id:1405.1027 author:Zhana Bao category:cs.AI cs.LG stat.ML  published:2014-05-05 summary:Finding rare information hidden in a huge amount of data from the Internet is a necessary but complex issue. Many researchers have studied this issue and have found effective methods to detect anomaly data in low dimensional space. However, as the dimension increases, most of these existing methods perform poorly in detecting outliers because of "high dimensional curse". Even though some approaches aim to solve this problem in high dimensional space, they can only detect some anomaly data appearing in low dimensional space and cannot detect all of anomaly data which appear differently in high dimensional space. To cope with this problem, we propose a new k-nearest section-based method (k-NS) in a section-based space. Our proposed approach not only detects outliers in low dimensional space with section-density ratio but also detects outliers in high dimensional space with the ratio of k-nearest section against average value. After taking a series of experiments with the dimension from 10 to 10000, the experiment results show that our proposed method achieves 100% precision and 100% recall result in the case of extremely high dimensional space, and better improvement in low dimensional space compared to our previously proposed method. version:1
arxiv-1405-0869 | Robust Subspace Outlier Detection in High Dimensional Space | http://arxiv.org/abs/1405.0869 | id:1405.0869 author:Zhana Bao category:cs.AI cs.LG stat.ML  published:2014-05-05 summary:Rare data in a large-scale database are called outliers that reveal significant information in the real world. The subspace-based outlier detection is regarded as a feasible approach in very high dimensional space. However, the outliers found in subspaces are only part of the true outliers in high dimensional space, indeed. The outliers hidden in normal-clustered points are sometimes neglected in the projected dimensional subspace. In this paper, we propose a robust subspace method for detecting such inner outliers in a given dataset, which uses two dimensional-projections: detecting outliers in subspaces with local density ratio in the first projected dimensions; finding outliers by comparing neighbor's positions in the second projected dimensions. Each point's weight is calculated by summing up all related values got in the two steps projected dimensions, and then the points scoring the largest weight values are taken as outliers. By taking a series of experiments with the number of dimensions from 10 to 10000, the results show that our proposed method achieves high precision in the case of extremely high dimensional space, and works well in low dimensional space. version:1
arxiv-1405-0833 | Generalized Risk-Aversion in Stochastic Multi-Armed Bandits | http://arxiv.org/abs/1405.0833 | id:1405.0833 author:Alexander Zimin, Rasmus Ibsen-Jensen, Krishnendu Chatterjee category:cs.LG stat.ML  published:2014-05-05 summary:We consider the problem of minimizing the regret in stochastic multi-armed bandit, when the measure of goodness of an arm is not the mean return, but some general function of the mean and the variance.We characterize the conditions under which learning is possible and present examples for which no natural algorithm can achieve sublinear regret. version:1
arxiv-1405-0792 | On Exact Learning Monotone DNF from Membership Queries | http://arxiv.org/abs/1405.0792 | id:1405.0792 author:Hasan Abasi, Nader H. Bshouty, Hanna Mazzawi category:cs.LG  published:2014-05-05 summary:In this paper, we study the problem of learning a monotone DNF with at most $s$ terms of size (number of variables in each term) at most $r$ ($s$ term $r$-MDNF) from membership queries. This problem is equivalent to the problem of learning a general hypergraph using hyperedge-detecting queries, a problem motivated by applications arising in chemical reactions and genome sequencing. We first present new lower bounds for this problem and then present deterministic and randomized adaptive algorithms with query complexities that are almost optimal. All the algorithms we present in this paper run in time linear in the query complexity and the number of variables $n$. In addition, all of the algorithms we present in this paper are asymptotically tight for fixed $r$ and/or $s$. version:1
arxiv-1405-0110 | A Structural Approach to Coordinate-Free Statistics | http://arxiv.org/abs/1405.0110 | id:1405.0110 author:Tom LaGatta, P. Richard Hahn category:math.PR math.FA math.ST stat.ML stat.TH  published:2014-05-01 summary:We consider the question of learning in general topological vector spaces. By exploiting known (or parametrized) covariance structures, our Main Theorem demonstrates that any continuous linear map corresponds to a certain isomorphism of embedded Hilbert spaces. By inverting this isomorphism and extending continuously, we construct a version of the Ordinary Least Squares estimator in absolute generality. Our Gauss-Markov theorem demonstrates that OLS is a "best linear unbiased estimator", extending the classical result. We construct a stochastic version of the OLS estimator, which is a continuous disintegration exactly for the class of "uncorrelated implies independent" (UII) measures. As a consequence, Gaussian measures always exhibit continuous disintegrations through continuous linear maps, extending a theorem of the first author. Applying this framework to some problems in machine learning, we prove a useful representation theorem for covariance tensors, and show that OLS defines a good kriging predictor for vector-valued arrays on general index spaces. We also construct a support-vector machine classifier in this setting. We hope that our article shines light on some deeper connections between probability theory, statistics and machine learning, and may serve as a point of intersection for these three communities. version:2
arxiv-1405-0701 | "Translation can't change a name": Using Multilingual Data for Named Entity Recognition | http://arxiv.org/abs/1405.0701 | id:1405.0701 author:Manaal Faruqui category:cs.CL  published:2014-05-04 summary:Named Entities (NEs) are often written with no orthographic changes across different languages that share a common alphabet. We show that this can be leveraged so as to improve named entity recognition (NER) by using unsupervised word clusters from secondary languages as features in state-of-the-art discriminative NER systems. We observe significant increases in performance, finding that person and location identification is particularly improved, and that phylogenetically close languages provide more valuable features than more distant languages. version:1
arxiv-1209-6491 | Review of Statistical Shape Spaces for 3D Data with Comparative Analysis for Human Faces | http://arxiv.org/abs/1209.6491 | id:1209.6491 author:Alan Brunton, Augusto Salazar, Timo Bolkart, Stefanie Wuhrer category:cs.CV cs.GR  published:2012-09-28 summary:With systems for acquiring 3D surface data being evermore commonplace, it has become important to reliably extract specific shapes from the acquired data. In the presence of noise and occlusions, this can be done through the use of statistical shape models, which are learned from databases of clean examples of the shape in question. In this paper, we review, analyze and compare different statistical models: from those that analyze the variation in geometry globally to those that analyze the variation in geometry locally. We first review how different types of models have been used in the literature, then proceed to define the models and analyze them theoretically, in terms of both their statistical and computational aspects. We then perform extensive experimental comparison on the task of model fitting, and give intuition about which type of model is better for a few applications. Due to the wide availability of databases of high-quality data, we use the human face as the specific shape we wish to extract from corrupted data. version:3
arxiv-1405-0632 | Rule of Three for Superresolution of Still Images with Applications to Compression and Denoising | http://arxiv.org/abs/1405.0632 | id:1405.0632 author:Mario Mastriani category:cs.CV  published:2014-05-04 summary:We describe a new method for superresolution of still images (in the wavelet domain) based on the reconstruction of missing details subbands pixels at a given ith level via Rule of Three (Ro3) between pixels of approximation subband of such level, and pixels of approximation and detail subbands of (i+1)th level. The histogramic profiles demonstrate that Ro3 is the appropriate mechanism to recover missing detail subband pixels in these cases. Besides, with the elimination of the details subbands pixels (in an eventual compression scheme), we obtain a bigger compression rate. Experimental results demonstrate that our approach compares favorably to more typical methods of denoising and compression in wavelet domain. Our method does not compress, but facilitates the action of the real compressor, in our case, Joint Photographic Experts Group (JPEG) and JPEg2000, that is, Ro3 acts as a catalyst compression version:1
arxiv-1405-0616 | Automated Attribution and Intertextual Analysis | http://arxiv.org/abs/1405.0616 | id:1405.0616 author:James Brofos, Ajay Kannan, Rui Shu category:cs.CL cs.DL stat.ML  published:2014-05-03 summary:In this work, we employ quantitative methods from the realm of statistics and machine learning to develop novel methodologies for author attribution and textual analysis. In particular, we develop techniques and software suitable for applications to Classical study, and we illustrate the efficacy of our approach in several interesting open questions in the field. We apply our numerical analysis techniques to questions of authorship attribution in the case of the Greek tragedian Euripides, to instances of intertextuality and influence in the poetry of the Roman statesman Seneca the Younger, and to cases of "interpolated" text with respect to the histories of Livy. version:1
arxiv-1405-0603 | Extracting Family Relationship Networks from Novels | http://arxiv.org/abs/1405.0603 | id:1405.0603 author:Aibek Makazhanov, Denilson Barbosa, Grzegorz Kondrak category:cs.CL  published:2014-05-03 summary:We present an approach to the extraction of family relations from literary narrative, which incorporates a technique for utterance attribution proposed recently by Elson and McKeown (2010). In our work this technique is used in combination with the detection of vocatives - the explicit forms of address used by the characters in a novel. We take advantage of the fact that certain vocatives indicate family relations between speakers. The extracted relations are then propagated using a set of rules. We report the results of the application of our method to Jane Austen's Pride and Prejudice. version:1
arxiv-1405-0602 | Why (and When and How) Contrastive Divergence Works | http://arxiv.org/abs/1405.0602 | id:1405.0602 author:Ian E Fellows category:stat.ML stat.ME  published:2014-05-03 summary:Contrastive divergence (CD) is a promising method of inference in high dimensional distributions with intractable normalizing constants, however, the theoretical foundations justifying its use are somewhat shaky. This document proposes a framework for understanding CD inference, how/when it works, and provides multiple justifications for the CD moment conditions, including framing them as a variational approximation. Algorithms for performing inference are discussed and are applied to social network data using an exponential-family random graph models (ERGM). The framework also provides guidance about how to construct MCMC kernels providing good CD inference, which turn out to be quite different from those used typically to provide fast global mixing. version:1
arxiv-1405-0601 | Supervised Descent Method for Solving Nonlinear Least Squares Problems in Computer Vision | http://arxiv.org/abs/1405.0601 | id:1405.0601 author:Xuehan Xiong, Fernando De la Torre category:cs.CV  published:2014-05-03 summary:Many computer vision problems (e.g., camera calibration, image alignment, structure from motion) are solved with nonlinear optimization methods. It is generally accepted that second order descent methods are the most robust, fast, and reliable approaches for nonlinear optimization of a general smooth function. However, in the context of computer vision, second order descent methods have two main drawbacks: (1) the function might not be analytically differentiable and numerical approximations are impractical, and (2) the Hessian may be large and not positive definite. To address these issues, this paper proposes generic descent maps, which are average "descent directions" and rescaling factors learned in a supervised fashion. Using generic descent maps, we derive a practical algorithm - Supervised Descent Method (SDM) - for minimizing Nonlinear Least Squares (NLS) problems. During training, SDM learns a sequence of decent maps that minimize the NLS. In testing, SDM minimizes the NLS objective using the learned descent maps without computing the Jacobian or the Hessian. We prove the conditions under which the SDM is guaranteed to converge. We illustrate the effectiveness and accuracy of SDM in three computer vision problems: rigid image alignment, non-rigid image alignment, and 3D pose estimation. In particular, we show how SDM achieves state-of-the-art performance in the problem of facial feature detection. The code has been made available at www.humansensing.cs.cmu.edu/intraface. version:1
arxiv-1405-0591 | Perceptron-like Algorithms and Generalization Bounds for Learning to Rank | http://arxiv.org/abs/1405.0591 | id:1405.0591 author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG stat.ML  published:2014-05-03 summary:Learning to rank is a supervised learning problem where the output space is the space of rankings but the supervision space is the space of relevance scores. We make theoretical contributions to the learning to rank problem both in the online and batch settings. First, we propose a perceptron-like algorithm for learning a ranking function in an online setting. Our algorithm is an extension of the classic perceptron algorithm for the classification problem. Second, in the setting of batch learning, we introduce a sufficient condition for convex ranking surrogates to ensure a generalization bound that is independent of number of objects per query. Our bound holds when linear ranking functions are used: a common practice in many learning to rank algorithms. En route to developing the online algorithm and generalization bound, we propose a novel family of listwise large margin ranking surrogates. Our novel surrogate family is obtained by modifying a well-known pairwise large margin ranking surrogate and is distinct from the listwise large margin surrogates developed using the structured prediction framework. Using the proposed family, we provide a guaranteed upper bound on the cumulative NDCG (or MAP) induced loss under the perceptron-like algorithm. We also show that the novel surrogates satisfy the generalization bound condition. version:1
arxiv-1405-1304 | Application of Machine Learning Techniques in Aquaculture | http://arxiv.org/abs/1405.1304 | id:1405.1304 author:Akhlaqur Rahman, Sumaira Tasnim category:cs.CE cs.LG  published:2014-05-03 summary:In this paper we present applications of different machine learning algorithms in aquaculture. Machine learning algorithms learn models from historical data. In aquaculture historical data are obtained from farm practices, yields, and environmental data sources. Associations between these different variables can be obtained by applying machine learning algorithms to historical data. In this paper we present applications of different machine learning algorithms in aquaculture applications. version:1
arxiv-1405-0573 | Spatial Neural Networks and their Functional Samples: Similarities and Differences | http://arxiv.org/abs/1405.0573 | id:1405.0573 author:Lucas Antiqueira, Liang Zhao category:cs.NE q-bio.NC  published:2014-05-03 summary:Models of neural networks have proven their utility in the development of learning algorithms in computer science and in the theoretical study of brain dynamics in computational neuroscience. We propose in this paper a spatial neural network model to analyze the important class of functional networks, which are commonly employed in computational studies of clinical brain imaging time series. We developed a simulation framework inspired by multichannel brain surface recordings (more specifically, EEG -- electroencephalogram) in order to link the mesoscopic network dynamics (represented by sampled functional networks) and the microscopic network structure (represented by an integrate-and-fire neural network located in a 3D space -- hence the term spatial neural network). Functional networks are obtained by computing pairwise correlations between time-series of mesoscopic electric potential dynamics, which allows the construction of a graph where each node represents one time-series. The spatial neural network model is central in this study in the sense that it allowed us to characterize sampled functional networks in terms of what features they are able to reproduce from the underlying spatial network. Our modeling approach shows that, in specific conditions of sample size and edge density, it is possible to precisely estimate several network measurements of spatial networks by just observing functional samples. version:1
arxiv-1405-0549 | Classification of Diabetes Mellitus using Modified Particle Swarm Optimization and Least Squares Support Vector Machine | http://arxiv.org/abs/1405.0549 | id:1405.0549 author:Omar S. Soliman, Eman AboElhamd category:cs.CE cs.NE  published:2014-05-03 summary:Diabetes Mellitus is a major health problem all over the world. Many classification algorithms have been applied for its diagnoses and treatment. In this paper, a hybrid algorithm of Modified-Particle Swarm Optimization and Least Squares- Support Vector Machine is proposed for the classification of type II DM patients. LS-SVM algorithm is used for classification by finding optimal hyper-plane which separates various classes. Since LS-SVM is so sensitive to the changes of its parameter values, Modified-PSO algorithm is used as an optimization technique for LS-SVM parameters. This will Guarantee the robustness of the hybrid algorithm by searching for the optimal values for LS-SVM parameters. The pro-posed Algorithm is implemented and evaluated using Pima Indians Diabetes Data set from UCI repository of machine learning databases. It is also compared with different classifier algorithms which were applied on the same database. The experimental results showed the superiority of the proposed algorithm which could achieve an average classification accuracy of 97.833%. version:1
arxiv-1405-1404 | A Network Intrusions Detection System based on a Quantum Bio Inspired Algorithm | http://arxiv.org/abs/1405.1404 | id:1405.1404 author:Omar S. Soliman, Aliaa Rassem category:cs.NE cs.CR  published:2014-05-03 summary:Network intrusion detection systems (NIDSs) have a role of identifying malicious activities by monitoring the behavior of networks. Due to the currently high volume of networks trafic in addition to the increased number of attacks and their dynamic properties, NIDSs have the challenge of improving their classification performance. Bio-Inspired Optimization Algorithms (BIOs) are used to automatically extract the the discrimination rules of normal or abnormal behavior to improve the classification accuracy and the detection ability of NIDS. A quantum vaccined immune clonal algorithm with the estimation of distribution algorithm (QVICA-with EDA) is proposed in this paper to build a new NIDS. The proposed algorithm is used as classification algorithm of the new NIDS where it is trained and tested using the KDD data set. Also, the new NIDS is compared with another detection system based on particle swarm optimization (PSO). Results shows the ability of the proposed algorithm of achieving high intrusions classification accuracy where the highest obtained accuracy is 94.8 %. version:1
arxiv-1405-0545 | Optimal measurement of visual motion across spatial and temporal scales | http://arxiv.org/abs/1405.0545 | id:1405.0545 author:Sergei Gepshtein, Ivan Tyukin category:cs.CV q-bio.NC  published:2014-05-03 summary:Sensory systems use limited resources to mediate the perception of a great variety of objects and events. Here a normative framework is presented for exploring how the problem of efficient allocation of resources can be solved in visual perception. Starting with a basic property of every measurement, captured by Gabor's uncertainty relation about the location and frequency content of signals, prescriptions are developed for optimal allocation of sensors for reliable perception of visual motion. This study reveals that a large-scale characteristic of human vision (the spatiotemporal contrast sensitivity function) is similar to the optimal prescription, and it suggests that some previously puzzling phenomena of visual sensitivity, adaptation, and perceptual organization have simple principled explanations. version:1
arxiv-1402-0108 | Markov Blanket Ranking using Kernel-based Conditional Dependence Measures | http://arxiv.org/abs/1402.0108 | id:1402.0108 author:Eric V. Strobl, Shyam Visweswaran category:stat.ML cs.LG  published:2014-02-01 summary:Developing feature selection algorithms that move beyond a pure correlational to a more causal analysis of observational data is an important problem in the sciences. Several algorithms attempt to do so by discovering the Markov blanket of a target, but they all contain a forward selection step which variables must pass in order to be included in the conditioning set. As a result, these algorithms may not consider all possible conditional multivariate combinations. We improve on this limitation by proposing a backward elimination method that uses a kernel-based conditional dependence measure to identify the Markov blanket in a fully multivariate fashion. The algorithm is easy to implement and compares favorably to other methods on synthetic and real datasets. version:3
arxiv-1007-0210 | Uncertainty of visual measurement and efficient allocation of sensory resources | http://arxiv.org/abs/1007.0210 | id:1007.0210 author:Sergei Gepshtein, Ivan Tyukin category:q-bio.NC cs.CV cs.IT math.IT  published:2010-07-01 summary:We review the reasoning underlying two approaches to combination of sensory uncertainties. First approach is noncommittal, making no assumptions about properties of uncertainty or parameters of stimulation. Then we explain the relationship between this approach and the one commonly used in modeling "higher level" aspects of sensory systems, such as in visual cue integration, where assumptions are made about properties of stimulation. The two approaches follow similar logic, except in one case maximal uncertainty is minimized, and in the other minimal certainty is maximized. Then we demonstrate how optimal solutions are found to the problem of resource allocation under uncertainty. version:2
arxiv-1405-0530 | A Rank-SVM Approach to Anomaly Detection | http://arxiv.org/abs/1405.0530 | id:1405.0530 author:Jing Qian, Jonathan Root, Venkatesh Saligrama, Yuting Chen category:stat.ML  published:2014-05-02 summary:We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on rank-SVM. Data points are first ranked based on scores derived from nearest neighbor graphs on n-point nominal data. We then train a rank-SVM using this ranked data. A test-point is declared as an anomaly at alpha-false alarm level if the predicted score is in the alpha-percentile. The resulting anomaly detector is shown to be asymptotically optimal and adaptive in that for any false alarm rate alpha, its decision region converges to the alpha-percentile level set of the unknown underlying density. In addition we illustrate through a number of synthetic and real-data experiments both the statistical performance and computational efficiency of our anomaly detector. version:1
arxiv-1405-0501 | Exchangeable Variable Models | http://arxiv.org/abs/1405.0501 | id:1405.0501 author:Mathias Niepert, Pedro Domingos category:cs.LG cs.AI  published:2014-05-02 summary:A sequence of random variables is exchangeable if its joint distribution is invariant under variable permutations. We introduce exchangeable variable models (EVMs) as a novel class of probabilistic models whose basic building blocks are partially exchangeable sequences, a generalization of exchangeable sequences. We prove that a family of tractable EVMs is optimal under zero-one loss for a large class of functions, including parity and threshold functions, and strictly subsumes existing tractable independence-based model families. Extensive experiments show that EVMs outperform state of the art classifiers such as SVMs and probabilistic models which are solely based on independence assumptions. version:1
arxiv-1210-6738 | Nested Hierarchical Dirichlet Processes | http://arxiv.org/abs/1210.6738 | id:1210.6738 author:John Paisley, Chong Wang, David M. Blei, Michael I. Jordan category:stat.ML cs.LG  published:2012-10-25 summary:We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP is a generalization of the nested Chinese restaurant process (nCRP) that allows each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. This alleviates the rigid, single-path formulation of the nCRP, allowing a document to more easily express thematic borrowings as a random effect. We derive a stochastic variational inference algorithm for the model, in addition to a greedy subtree selection method for each document, which allows for efficient inference using massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 3.3 million documents from Wikipedia. version:4
arxiv-1305-1809 | Cover Tree Bayesian Reinforcement Learning | http://arxiv.org/abs/1305.1809 | id:1305.1809 author:Nikolaos Tziortziotis, Christos Dimitrakakis, Konstantinos Blekas category:stat.ML cs.LG  published:2013-05-08 summary:This paper proposes an online tree-based Bayesian approach for reinforcement learning. For inference, we employ a generalised context tree model. This defines a distribution on multivariate Gaussian piecewise-linear models, which can be updated in closed form. The tree structure itself is constructed using the cover tree method, which remains efficient in high dimensional spaces. We combine the model with Thompson sampling and approximate dynamic programming to obtain effective exploration policies in unknown environments. The flexibility and computational simplicity of the model render it suitable for many reinforcement learning problems in continuous state spaces. We demonstrate this in an experimental comparison with least squares policy iteration. version:2
arxiv-1405-0234 | Retrieval in Long Surveillance Videos using User Described Motion and Object Attributes | http://arxiv.org/abs/1405.0234 | id:1405.0234 author:Greg Castanon, Mohamed Elgharib, Venkatesh Saligrama, Pierre-Marc Jodoin category:cs.CV  published:2014-05-01 summary:We present a content-based retrieval method for long surveillance videos both for wide-area (Airborne) as well as near-field imagery (CCTV). Our goal is to retrieve video segments, with a focus on detecting objects moving on routes, that match user-defined events of interest. The sheer size and remote locations where surveillance videos are acquired, necessitates highly compressed representations that are also meaningful for supporting user-defined queries. To address these challenges we archive long-surveillance video through lightweight processing based on low-level local spatio-temporal extraction of motion and object features. These are then hashed into an inverted index using locality-sensitive hashing (LSH). This local approach allows for query flexibility as well as leads to significant gains in compression. Our second task is to extract partial matches to the user-created query and assembles them into full matches using Dynamic Programming (DP). DP exploits causality to assemble the indexed low level features into a video segment which matches the query route. We examine CCTV and Airborne footage, whose low contrast makes motion extraction more difficult. We generate robust motion estimates for Airborne data using a tracklets generation algorithm while we use Horn and Schunck approach to generate motion estimates for CCTV. Our approach handles long routes, low contrasts and occlusion. We derive bounds on the rate of false positives and demonstrate the effectiveness of the approach for counting, motion pattern recognition and abandoned object applications. version:1
arxiv-1211-4657 | Forest Sparsity for Multi-channel Compressive Sensing | http://arxiv.org/abs/1211.4657 | id:1211.4657 author:Chen Chen, Yeqing Li, Junzhou Huang category:cs.LG cs.CV cs.IT math.IT stat.ML  published:2012-11-20 summary:In this paper, we investigate a new compressive sensing model for multi-channel sparse data where each channel can be represented as a hierarchical tree and different channels are highly correlated. Therefore, the full data could follow the forest structure and we call this property as \emph{forest sparsity}. It exploits both intra- and inter- channel correlations and enriches the family of existing model-based compressive sensing theories. The proposed theory indicates that only $\mathcal{O}(Tk+\log(N/k))$ measurements are required for multi-channel data with forest sparsity, where $T$ is the number of channels, $N$ and $k$ are the length and sparsity number of each channel respectively. This result is much better than $\mathcal{O}(Tk+T\log(N/k))$ of tree sparsity, $\mathcal{O}(Tk+k\log(N/k))$ of joint sparsity, and far better than $\mathcal{O}(Tk+Tk\log(N/k))$ of standard sparsity. In addition, we extend the forest sparsity theory to the multiple measurement vectors problem, where the measurement matrix is a block-diagonal matrix. The result shows that the required measurement bound can be the same as that for dense random measurement matrix, when the data shares equal energy in each channel. A new algorithm is developed and applied on four example applications to validate the benefit of the proposed model. Extensive experiments demonstrate the effectiveness and efficiency of the proposed theory and algorithm. version:2
arxiv-1405-0174 | VSCAN: An Enhanced Video Summarization using Density-based Spatial Clustering | http://arxiv.org/abs/1405.0174 | id:1405.0174 author:Karim M. Mohamed, Mohamed A. Ismail, Nagia M. Ghanem category:cs.CV cs.MM  published:2014-05-01 summary:In this paper, we present VSCAN, a novel approach for generating static video summaries. This approach is based on a modified DBSCAN clustering algorithm to summarize the video content utilizing both color and texture features of the video frames. The paper also introduces an enhanced evaluation method that depends on color and texture features. Video Summaries generated by VSCAN are compared with summaries generated by other approaches found in the literature and those created by users. Experimental results indicate that the video summaries generated by VSCAN have a higher quality than those generated by other approaches. version:1
arxiv-1405-0145 | Contextual Semantic Parsing using Crowdsourced Spatial Descriptions | http://arxiv.org/abs/1405.0145 | id:1405.0145 author:Kais Dukes category:cs.CL  published:2014-05-01 summary:We describe a contextual parser for the Robot Commands Treebank, a new crowdsourced resource. In contrast to previous semantic parsers that select the most-probable parse, we consider the different problem of parsing using additional situational context to disambiguate between different readings of a sentence. We show that multiple semantic analyses can be searched using dynamic programming via interaction with a spatial planner, to guide the parsing process. We are able to parse sentences in near linear-time by ruling out analyses early on that are incompatible with spatial context. We report a 34% upper bound on accuracy, as our planner correctly processes spatial context for 3,394 out of 10,000 sentences. However, our parser achieves a 96.53% exact-match score for parsing within the subset of sentences recognized by the planner, compared to 82.14% for a non-contextual parser. version:1
arxiv-1405-0099 | Fast MLE Computation for the Dirichlet Multinomial | http://arxiv.org/abs/1405.0099 | id:1405.0099 author:Max Sklar category:stat.ML cs.LG  published:2014-05-01 summary:Given a collection of categorical data, we want to find the parameters of a Dirichlet distribution which maximizes the likelihood of that data. Newton's method is typically used for this purpose but current implementations require reading through the entire dataset on each iteration. In this paper, we propose a modification which requires only a single pass through the dataset and substantially decreases running time. Furthermore we analyze both theoretically and empirically the performance of the proposed algorithm, and provide an open source implementation. version:1
arxiv-1403-1430 | Sparse Principal Component Analysis via Rotation and Truncation | http://arxiv.org/abs/1403.1430 | id:1403.1430 author:Zhenfang Hu, Gang Pan, Yueming Wang, Zhaohui Wu category:cs.LG cs.CV stat.ML  published:2014-03-06 summary:Sparse principal component analysis (sparse PCA) aims at finding a sparse basis to improve the interpretability over the dense basis of PCA, meanwhile the sparse basis should cover the data subspace as much as possible. In contrast to most of existing work which deal with the problem by adding some sparsity penalties on various objectives of PCA, in this paper, we propose a new method SPCArt, whose motivation is to find a rotation matrix and a sparse basis such that the sparse basis approximates the basis of PCA after the rotation. The algorithm of SPCArt consists of three alternating steps: rotate PCA basis, truncate small entries, and update the rotation matrix. Its performance bounds are also given. SPCArt is efficient, with each iteration scaling linearly with the data dimension. It is easy to choose parameters in SPCArt, due to its explicit physical explanations. Besides, we give a unified view to several existing sparse PCA methods and discuss the connection with SPCArt. Some ideas in SPCArt are extended to GPower, a popular sparse PCA algorithm, to overcome its drawback. Experimental results demonstrate that SPCArt achieves the state-of-the-art performance. It also achieves a good tradeoff among various criteria, including sparsity, explained variance, orthogonality, balance of sparsity among loadings, and computational speed. version:2
arxiv-1405-0085 | Relative Facial Action Unit Detection | http://arxiv.org/abs/1405.0085 | id:1405.0085 author:Mahmoud Khademi, Louis-Philippe Morency category:cs.CV  published:2014-05-01 summary:This paper presents a subject-independent facial action unit (AU) detection method by introducing the concept of relative AU detection, for scenarios where the neutral face is not provided. We propose a new classification objective function which analyzes the temporal neighborhood of the current frame to decide if the expression recently increased, decreased or showed no change. This approach is a significant change from the conventional absolute method which decides about AU classification using the current frame, without an explicit comparison with its neighboring frames. Our proposed method improves robustness to individual differences such as face scale and shape, age-related wrinkles, and transitions among expressions (e.g., lower intensity of expressions). Our experiments on three publicly available datasets (Extended Cohn-Kanade (CK+), Bosphorus, and DISFA databases) show significant improvement of our approach over conventional absolute techniques. Keywords: facial action coding system (FACS); relative facial action unit detection; temporal information; version:1
arxiv-1302-2671 | Latent Self-Exciting Point Process Model for Spatial-Temporal Networks | http://arxiv.org/abs/1302.2671 | id:1302.2671 author:Yoon-Sik Cho, Aram Galstyan, P. Jeffrey Brantingham, George Tita category:cs.SI cs.LG stat.ML  published:2013-02-12 summary:We propose a latent self-exciting point process model that describes geographically distributed interactions between pairs of entities. In contrast to most existing approaches that assume fully observable interactions, here we consider a scenario where certain interaction events lack information about participants. Instead, this information needs to be inferred from the available observations. We develop an efficient approximate algorithm based on variational expectation-maximization to infer unknown participants in an event given the location and the time of the event. We validate the model on synthetic as well as real-world data, and obtain very promising results on the identity-inference task. We also use our model to predict the timing and participants of future events, and demonstrate that it compares favorably with baseline approaches. version:3
arxiv-1312-6974 | Piecewise regression mixture for simultaneous functional data clustering and optimal segmentation | http://arxiv.org/abs/1312.6974 | id:1312.6974 author:Faicel Chamroukhi category:stat.ME cs.LG math.ST stat.ML stat.TH  published:2013-12-25 summary:This paper introduces a novel mixture model-based approach for simultaneous clustering and optimal segmentation of functional data which are curves presenting regime changes. The proposed model consists in a finite mixture of piecewise polynomial regression models. Each piecewise polynomial regression model is associated with a cluster, and within each cluster, each piecewise polynomial component is associated with a regime (i.e., a segment). We derive two approaches for learning the model parameters. The former is an estimation approach and consists in maximizing the observed-data likelihood via a dedicated expectation-maximization (EM) algorithm. A fuzzy partition of the curves in K clusters is then obtained at convergence by maximizing the posterior cluster probabilities. The latter however is a classification approach and optimizes a specific classification likelihood criterion through a dedicated classification expectation-maximization (CEM) algorithm. The optimal curve segmentation is performed by using dynamic programming. In the classification approach, both the curve clustering and the optimal segmentation are performed simultaneously as the CEM learning proceeds. We show that the classification approach is the probabilistic version that generalizes the deterministic K-means-like algorithm proposed in H\'ebrail et al. (2010). The proposed approach is evaluated using simulated curves and real-world curves. Comparisons with alternatives including regression mixture models and the K-means like algorithm for piecewise regression demonstrate the effectiveness of the proposed approach. version:2
arxiv-1404-7073 | Probably Approximately Correct MDP Learning and Control With Temporal Logic Constraints | http://arxiv.org/abs/1404.7073 | id:1404.7073 author:Jie Fu, Ufuk Topcu category:cs.SY cs.LG cs.LO cs.RO 93E35 I.2.8; G.3  published:2014-04-28 summary:We consider synthesis of control policies that maximize the probability of satisfying given temporal logic specifications in unknown, stochastic environments. We model the interaction between the system and its environment as a Markov decision process (MDP) with initially unknown transition probabilities. The solution we develop builds on the so-called model-based probably approximately correct Markov decision process (PAC-MDP) methodology. The algorithm attains an $\varepsilon$-approximately optimal policy with probability $1-\delta$ using samples (i.e. observations), time and space that grow polynomially with the size of the MDP, the size of the automaton expressing the temporal logic specification, $\frac{1}{\varepsilon}$, $\frac{1}{\delta}$ and a finite time horizon. In this approach, the system maintains a model of the initially unknown MDP, and constructs a product MDP based on its learned model and the specification automaton that expresses the temporal logic constraints. During execution, the policy is iteratively updated using observation of the transitions taken by the system. The iteration terminates in finitely many steps. With high probability, the resulting policy is such that, for any state, the difference between the probability of satisfying the specification under this policy and the optimal one is within a predefined bound. version:2
arxiv-1404-7789 | Phase transitions in semisupervised clustering of sparse networks | http://arxiv.org/abs/1404.7789 | id:1404.7789 author:Pan Zhang, Cristopher Moore, Lenka Zdeborová category:cs.SI cond-mat.stat-mech physics.soc-ph stat.ML  published:2014-04-30 summary:Predicting labels of nodes in a network, such as community memberships or demographic variables, is an important problem with applications in social and biological networks. A recently-discovered phase transition puts fundamental limits on the accuracy of these predictions if we have access only to the network topology. However, if we know the correct labels of some fraction $\alpha$ of the nodes, we can do better. We study the phase diagram of this "semisupervised" learning problem for networks generated by the stochastic block model. We use the cavity method and the associated belief propagation algorithm to study what accuracy can be achieved as a function of $\alpha$. For $k = 2$ groups, we find that the detectability transition disappears for any $\alpha > 0$, in agreement with previous work. For larger $k$ where a hard but detectable regime exists, we find that the easy/hard transition (the point at which efficient algorithms can do better than chance) becomes a line of transitions where the accuracy jumps discontinuously at a critical value of $\alpha$. This line ends in a critical point with a second-order transition, beyond which the accuracy is a continuous function of $\alpha$. We demonstrate qualitatively similar transitions in two real-world networks. version:1
arxiv-1405-0006 | Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction | http://arxiv.org/abs/1405.0006 | id:1405.0006 author:Moritz Kassner, William Patera, Andreas Bulling category:cs.CV cs.HC  published:2014-04-30 summary:Commercial head-mounted eye trackers provide useful features to customers in industry and research but are expensive and rely on closed source hardware and software. This limits the application areas and use of mobile eye tracking to expert users and inhibits user-driven development, customisation, and extension. In this paper we present Pupil -- an accessible, affordable, and extensible open source platform for mobile eye tracking and gaze-based interaction. Pupil comprises 1) a light-weight headset with high-resolution cameras, 2) an open source software framework for mobile eye tracking, as well as 3) a graphical user interface (GUI) to playback and visualize video and gaze data. Pupil features high-resolution scene and eye cameras for monocular and binocular gaze estimation. The software and GUI are platform-independent and include state-of-the-art algorithms for real-time pupil detection and tracking, calibration, and accurate gaze estimation. Results of a performance evaluation show that Pupil can provide an average gaze estimation accuracy of 0.6 degree of visual angle (0.08 degree precision) with a latency of the processing pipeline of only 0.045 seconds. version:1
arxiv-1404-7748 | A graph-based mathematical morphology reader | http://arxiv.org/abs/1404.7748 | id:1404.7748 author:Laurent Najman, Jean Cousty category:cs.CV  published:2014-04-30 summary:This survey paper aims at providing a "literary" anthology of mathematical morphology on graphs. It describes in the English language many ideas stemming from a large number of different papers, hence providing a unified view of an active and diverse field of research. version:1
arxiv-1307-5697 | Dimension Reduction via Colour Refinement | http://arxiv.org/abs/1307.5697 | id:1307.5697 author:Martin Grohe, Kristian Kersting, Martin Mladenov, Erkal Selman category:cs.DS cs.DM cs.LG math.OC  published:2013-07-22 summary:Colour refinement is a basic algorithmic routine for graph isomorphism testing, appearing as a subroutine in almost all practical isomorphism solvers. It partitions the vertices of a graph into "colour classes" in such a way that all vertices in the same colour class have the same number of neighbours in every colour class. Tinhofer (Disc. App. Math., 1991), Ramana, Scheinerman, and Ullman (Disc. Math., 1994) and Godsil (Lin. Alg. and its App., 1997) established a tight correspondence between colour refinement and fractional isomorphisms of graphs, which are solutions to the LP relaxation of a natural ILP formulation of graph isomorphism. We introduce a version of colour refinement for matrices and extend existing quasilinear algorithms for computing the colour classes. Then we generalise the correspondence between colour refinement and fractional automorphisms and develop a theory of fractional automorphisms and isomorphisms of matrices. We apply our results to reduce the dimensions of systems of linear equations and linear programs. Specifically, we show that any given LP L can efficiently be transformed into a (potentially) smaller LP L' whose number of variables and constraints is the number of colour classes of the colour refinement algorithm, applied to a matrix associated with the LP. The transformation is such that we can easily (by a linear mapping) map both feasible and optimal solutions back and forth between the two LPs. We demonstrate empirically that colour refinement can indeed greatly reduce the cost of solving linear programs. version:2
arxiv-1404-3538 | Proceedings of The 38th Annual Workshop of the Austrian Association for Pattern Recognition (ÖAGM), 2014 | http://arxiv.org/abs/1404.3538 | id:1404.3538 author:Vladimir Kolmogorov, Christoph Lampert, Emilie Morvant, Rustem Takhanov category:cs.CV  published:2014-04-14 summary:The 38th Annual Workshop of the Austrian Association for Pattern Recognition (\"OAGM) will be held at IST Austria, on May 22-23, 2014. The workshop provides a platform for researchers and industry to discuss traditional and new areas of computer vision. This year the main topic is: Pattern Recognition: interdisciplinary challenges and opportunities. version:2
arxiv-1405-0921 | Gabor Filter and Rough Clustering Based Edge Detection | http://arxiv.org/abs/1405.0921 | id:1405.0921 author:Chandranath Adak category:cs.CV cs.AI  published:2014-04-30 summary:This paper introduces an efficient edge detection method based on Gabor filter and rough clustering. The input image is smoothed by Gabor function, and the concept of rough clustering is used to focus on edge detection with soft computational approach. Hysteresis thresholding is used to get the actual output, i.e. edges of the input image. To show the effectiveness, the proposed technique is compared with some other edge detection methods. version:1
arxiv-1404-7594 | Selecting a Small Set of Optimal Gestures from an Extensive Lexicon | http://arxiv.org/abs/1404.7594 | id:1404.7594 author:Jacob Grosek, J. Nathan Kutz category:cs.CV  published:2014-04-30 summary:Finding the best set of gestures to use for a given computer recognition problem is an essential part of optimizing the recognition performance while being mindful to those who may articulate the gestures. An objective function, called the ellipsoidal distance ratio metric (EDRM), for determining the best gestures from a larger lexicon library is presented, along with a numerical method for incorporating subjective preferences. In particular, we demonstrate an efficient algorithm that chooses the best $n$ gestures from a lexicon of $m$ gestures where typically $n \ll m$ using a weighting of both subjective and objective measures. version:1
arxiv-1404-7592 | Dynamic Mode Decomposition for Real-Time Background/Foreground Separation in Video | http://arxiv.org/abs/1404.7592 | id:1404.7592 author:Jacob Grosek, J. Nathan Kutz category:cs.CV  published:2014-04-30 summary:This paper introduces the method of dynamic mode decomposition (DMD) for robustly separating video frames into background (low-rank) and foreground (sparse) components in real-time. The method is a novel application of a technique used for characterizing nonlinear dynamical systems in an equation-free manner by decomposing the state of the system into low-rank terms whose Fourier components in time are known. DMD terms with Fourier frequencies near the origin (zero-modes) are interpreted as background (low-rank) portions of the given video frames, and the terms with Fourier frequencies bounded away from the origin are their sparse counterparts. An approximate low-rank/sparse separation is achieved at the computational cost of just one singular value decomposition and one linear equation solve, thus producing results orders of magnitude faster than a leading separation method, namely robust principal component analysis (RPCA). The DMD method that is developed here is demonstrated to work robustly in real-time with personal laptop-class computing power and without any parameter tuning, which is a transformative improvement in performance that is ideal for video surveillance and recognition applications. version:1
arxiv-1404-7566 | Image Compressive Sensing Recovery Using Adaptively Learned Sparsifying Basis via L0 Minimization | http://arxiv.org/abs/1404.7566 | id:1404.7566 author:Jian Zhang, Chen Zhao, Debin Zhao, Wen Gao category:cs.CV  published:2014-04-30 summary:From many fewer acquired measurements than suggested by the Nyquist sampling theory, compressive sensing (CS) theory demonstrates that, a signal can be reconstructed with high probability when it exhibits sparsity in some domain. Most of the conventional CS recovery approaches, however, exploited a set of fixed bases (e.g. DCT, wavelet and gradient domain) for the entirety of a signal, which are irrespective of the non-stationarity of natural signals and cannot achieve high enough degree of sparsity, thus resulting in poor CS recovery performance. In this paper, we propose a new framework for image compressive sensing recovery using adaptively learned sparsifying basis via L0 minimization. The intrinsic sparsity of natural images is enforced substantially by sparsely representing overlapped image patches using the adaptively learned sparsifying basis in the form of L0 norm, greatly reducing blocking artifacts and confining the CS solution space. To make our proposed scheme tractable and robust, a split Bregman iteration based technique is developed to solve the non-convex L0 minimization problem efficiently. Experimental results on a wide range of natural images for CS recovery have shown that our proposed algorithm achieves significant performance improvements over many current state-of-the-art schemes and exhibits good convergence property. version:1
arxiv-1305-5029 | Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates | http://arxiv.org/abs/1305.5029 | id:1305.5029 author:Yuchen Zhang, John C. Duchi, Martin J. Wainwright category:math.ST cs.LG stat.ML stat.TH  published:2013-05-22 summary:We establish optimal convergence rates for a decomposition-based scalable approach to kernel ridge regression. The method is simple to describe: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent kernel ridge regression estimator for each subset, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all N samples. Our two main theorems establish that despite the computational speed-up, statistical optimality is retained: as long as m is not too large, the partition-based estimator achieves the statistical minimax rate over all estimators using the set of N samples. As concrete examples, our theory guarantees that the number of processors m may grow nearly linearly for finite-rank kernels and Gaussian kernels and polynomially in N for Sobolev spaces, which in turn allows for substantial reductions in computational cost. We conclude with experiments on both simulated data and a music-prediction task that complement our theoretical results, exhibiting the computational and statistical benefits of our approach. version:2
arxiv-1310-7780 | The Information Geometry of Mirror Descent | http://arxiv.org/abs/1310.7780 | id:1310.7780 author:Garvesh Raskutti, Sayan Mukherjee category:stat.ML cs.LG  published:2013-10-29 summary:Information geometry applies concepts in differential geometry to probability and statistics and is especially useful for parameter estimation in exponential families where parameters are known to lie on a Riemannian manifold. Connections between the geometric properties of the induced manifold and statistical properties of the estimation problem are well-established. However developing first-order methods that scale to larger problems has been less of a focus in the information geometry community. The best known algorithm that incorporates manifold structure is the second-order natural gradient descent algorithm introduced by Amari. On the other hand, stochastic approximation methods have led to the development of first-order methods for optimizing noisy objective functions. A recent generalization of the Robbins-Monro algorithm known as mirror descent, developed by Nemirovski and Yudin is a first order method that induces non-Euclidean geometries. However current analysis of mirror descent does not precisely characterize the induced non-Euclidean geometry nor does it consider performance in terms of statistical relative efficiency. In this paper, we prove that mirror descent induced by Bregman divergences is equivalent to the natural gradient descent algorithm on the dual Riemannian manifold. Using this equivalence, it follows that (1) mirror descent is the steepest descent direction along the Riemannian manifold of the exponential family; (2) mirror descent with log-likelihood loss applied to parameter estimation in exponential families asymptotically achieves the classical Cram\'er-Rao lower bound and (3) natural gradient descent for manifolds corresponding to exponential families can be implemented as a first-order method through mirror descent. version:2
arxiv-1404-7472 | Implementing spectral methods for hidden Markov models with real-valued emissions | http://arxiv.org/abs/1404.7472 | id:1404.7472 author:Carl Mattfeld category:cs.LG  published:2014-04-29 summary:Hidden Markov models (HMMs) are widely used statistical models for modeling sequential data. The parameter estimation for HMMs from time series data is an important learning problem. The predominant methods for parameter estimation are based on local search heuristics, most notably the expectation-maximization (EM) algorithm. These methods are prone to local optima and oftentimes suffer from high computational and sample complexity. Recent years saw the emergence of spectral methods for the parameter estimation of HMMs, based on a method of moments approach. Two spectral learning algorithms as proposed by Hsu, Kakade and Zhang 2012 (arXiv:0811.4413) and Anandkumar, Hsu and Kakade 2012 (arXiv:1203.0683) are assessed in this work. Using experiments with synthetic data, the algorithms are compared with each other. Furthermore, the spectral methods are compared to the Baum-Welch algorithm, a well-established method applying the EM algorithm to HMMs. The spectral algorithms are found to have a much more favorable computational and sample complexity. Even though the algorithms readily handle high dimensional observation spaces, instability issues are encountered in this regime. In view of learning from real-world experimental data, the representation of real-valued observations for the use in spectral methods is discussed, presenting possible methods to represent data for the use in the learning algorithms. version:1
arxiv-1402-5731 | Information-Theoretic Bounds for Adaptive Sparse Recovery | http://arxiv.org/abs/1402.5731 | id:1402.5731 author:Cem Aksoylar, Venkatesh Saligrama category:cs.IT cs.LG math.IT math.ST stat.TH  published:2014-02-24 summary:We derive an information-theoretic lower bound for sample complexity in sparse recovery problems where inputs can be chosen sequentially and adaptively. This lower bound is in terms of a simple mutual information expression and unifies many different linear and nonlinear observation models. Using this formula we derive bounds for adaptive compressive sensing (CS), group testing and 1-bit CS problems. We show that adaptivity cannot decrease sample complexity in group testing, 1-bit CS and CS with linear sparsity. In contrast, we show there might be mild performance gains for CS in the sublinear regime. Our unified analysis also allows characterization of gains due to adaptivity from a wider perspective on sparse problems. version:2
arxiv-1312-7734 | Identification of structural features in chemicals associated with cancer drug response: A systematic data-driven analysis | http://arxiv.org/abs/1312.7734 | id:1312.7734 author:Suleiman A Khan, Seppo Virtanen, Olli P Kallioniemi, Krister Wennerberg, Antti Poso, Samuel Kaski category:stat.ML q-bio.GN stat.AP  published:2013-12-30 summary:Motivation: Analysis of relationships of drug structure to biological response is key to understanding off-target and unexpected drug effects, and for developing hypotheses on how to tailor drug thera-pies. New methods are required for integrated analyses of a large number of chemical features of drugs against the corresponding genome-wide responses of multiple cell models. Results: In this paper, we present the first comprehensive multi-set analysis on how the chemical structure of drugs impacts on ge-nome-wide gene expression across several cancer cell lines (CMap database). The task is formulated as searching for drug response components across multiple cancers to reveal shared effects of drugs and the chemical features that may be responsible. The com-ponents can be computed with an extension of a very recent ap-proach called Group Factor Analysis (GFA). We identify 11 compo-nents that link the structural descriptors of drugs with specific gene expression responses observed in the three cell lines, and identify structural groups that may be responsible for the responses. Our method quantitatively outperforms the limited earlier studies on CMap and identifies both the previously reported associations and several interesting novel findings, by taking into account multiple cell lines and advanced 3D structural descriptors. The novel observations include: previously unknown similarities in the effects induced by 15-delta prostaglandin J2 and HSP90 inhibitors, which are linked to the 3D descriptors of the drugs; and the induction by simvastatin of leukemia-specific anti-inflammatory response, resem-bling the effects of corticosteroids. version:2
arxiv-1306-6239 | Near-Optimal Adaptive Compressed Sensing | http://arxiv.org/abs/1306.6239 | id:1306.6239 author:Matthew L. Malloy, Robert D. Nowak category:cs.IT math.IT stat.ML  published:2013-06-26 summary:This paper proposes a simple adaptive sensing and group testing algorithm for sparse signal recovery. The algorithm, termed Compressive Adaptive Sense and Search (CASS), is shown to be near-optimal in that it succeeds at the lowest possible signal-to-noise-ratio (SNR) levels, improving on previous work in adaptive compressed sensing. Like traditional compressed sensing based on random non-adaptive design matrices, the CASS algorithm requires only k log n measurements to recover a k-sparse signal of dimension n. However, CASS succeeds at SNR levels that are a factor log n less than required by standard compressed sensing. From the point of view of constructing and implementing the sensing operation as well as computing the reconstruction, the proposed algorithm is substantially less computationally intensive than standard compressed sensing. CASS is also demonstrated to perform considerably better in practice through simulation. To the best of our knowledge, this is the first demonstration of an adaptive compressed sensing algorithm with near-optimal theoretical guarantees and excellent practical performance. This paper also shows that methods like compressed sensing, group testing, and pooling have an advantage beyond simply reducing the number of measurements or tests -- adaptive versions of such methods can also improve detection and estimation performance when compared to non-adaptive direct (uncompressed) sensing. version:2
arxiv-1404-7362 | Concise comparative summaries (CCS) of large text corpora with a human experiment | http://arxiv.org/abs/1404.7362 | id:1404.7362 author:Jinzhu Jia, Luke Miratrix, Bin Yu, Brian Gawalt, Laurent El Ghaoui, Luke Barnesmoore, Sophie Clavier category:cs.CL stat.AP  published:2014-04-29 summary:In this paper we propose a general framework for topic-specific summarization of large text corpora and illustrate how it can be used for the analysis of news databases. Our framework, concise comparative summarization (CCS), is built on sparse classification methods. CCS is a lightweight and flexible tool that offers a compromise between simple word frequency based methods currently in wide use and more heavyweight, model-intensive methods such as latent Dirichlet allocation (LDA). We argue that sparse methods have much to offer for text analysis and hope CCS opens the door for a new branch of research in this important field. For a particular topic of interest (e.g., China or energy), CSS automatically labels documents as being either on- or off-topic (usually via keyword search), and then uses sparse classification methods to predict these labels with the high-dimensional counts of all the other words and phrases in the documents. The resulting small set of phrases found as predictive are then harvested as the summary. To validate our tool, we, using news articles from the New York Times international section, designed and conducted a human survey to compare the different summarizers with human understanding. We demonstrate our approach with two case studies, a media analysis of the framing of "Egypt" in the New York Times throughout the Arab Spring and an informal comparison of the New York Times' and Wall Street Journal's coverage of "energy." Overall, we find that the Lasso with $L^2$ normalization can be effectively and usefully used to summarize large corpora, regardless of document size. version:1
arxiv-1404-7306 | Generalized Nonconvex Nonsmooth Low-Rank Minimization | http://arxiv.org/abs/1404.7306 | id:1404.7306 author:Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin category:cs.CV cs.LG stat.ML  published:2014-04-29 summary:As surrogate functions of $L_0$-norm, many nonconvex penalty functions have been proposed to enhance the sparse vector recovery. It is easy to extend these nonconvex penalty functions on singular values of a matrix to enhance low-rank matrix recovery. However, different from convex optimization, solving the nonconvex low-rank minimization problem is much more challenging than the nonconvex sparse minimization problem. We observe that all the existing nonconvex penalty functions are concave and monotonically increasing on $[0,\infty)$. Thus their gradients are decreasing functions. Based on this property, we propose an Iteratively Reweighted Nuclear Norm (IRNN) algorithm to solve the nonconvex nonsmooth low-rank minimization problem. IRNN iteratively solves a Weighted Singular Value Thresholding (WSVT) problem. By setting the weight vector as the gradient of the concave penalty function, the WSVT problem has a closed form solution. In theory, we prove that IRNN decreases the objective function value monotonically, and any limit point is a stationary point. Extensive experiments on both synthetic data and real images demonstrate that IRNN enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms. version:1
arxiv-1403-6274 | Arguments for Nested Patterns in Neural Ensembles | http://arxiv.org/abs/1403.6274 | id:1403.6274 author:Kieran Greer category:cs.NE q-bio.NC  published:2014-03-25 summary:This paper describes a relatively simple way of allowing a brain model to self-organise its concept patterns through nested structures. Time is a key element and a simulator would be able to show how patterns may form and then fire in sequence, as part of a search or thought process. It uses a very simple equation to show how the inhibitors in particular, can switch off certain areas, to allow other areas to become the prominent ones and thereby define the current brain state. This allows for a small amount of control over what appears to be a chaotic structure inside of the brain. It is attractive because it is still mostly mechanical and therefore can be added as an automatic process, or the modelling of that. The paper also describes how the nested pattern structure can be used as a basic counting mechanism. version:2
arxiv-1404-7298 | Code Minimization for Fringe Projection Based 3D Stereo Sensors by Calibration Improvement | http://arxiv.org/abs/1404.7298 | id:1404.7298 author:Christian Bräuer-Burchardt, Peter Kühmstedt, Gunther Notni category:math.MG cs.CV  published:2014-04-29 summary:Code minimization provides a speed-up of the processing time of fringe projection based stereo sensors and possibly makes them real-time applicable. This paper reports a methodology which enables such sensors to completely omit Gray code or other additional code. Only a sequence of sinusoidal images is necessary. The code reduction is achieved by involvement of the projection unit into the measurement, double triangulation, and a precise projector calibration or significant projector calibration improvement, respectively. version:1
arxiv-1404-7296 | A Deep Architecture for Semantic Parsing | http://arxiv.org/abs/1404.7296 | id:1404.7296 author:Edward Grefenstette, Phil Blunsom, Nando de Freitas, Karl Moritz Hermann category:cs.CL  published:2014-04-29 summary:Many successful approaches to semantic parsing build on top of the syntactic analysis of text, and make use of distributional representations or statistical models to match parses to ontology-specific queries. This paper presents a novel deep learning architecture which provides a semantic parsing system through the union of two neural models of language semantics. It allows for the generation of ontology-specific queries from natural language statements and questions without the need for parsing, which makes it especially suitable to grammatically malformed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resource-poor languages. version:1
arxiv-1404-7059 | Stereo on a budget | http://arxiv.org/abs/1404.7059 | id:1404.7059 author:Dana Menaker, Shai Avidan category:cs.CV  published:2014-04-28 summary:We propose an algorithm for recovering depth using less than two images. Instead of having both cameras send their entire image to the host computer, the left camera sends its image to the host while the right camera sends only a fraction $\epsilon$ of its image. The key aspect is that the cameras send the information without communicating at all. Hence, the required communication bandwidth is significantly reduced. While standard image compression techniques can reduce the communication bandwidth, this requires additional computational resources on the part of the encoder (camera). We aim at designing a light weight encoder that only touches a fraction of the pixels. The burden of decoding is placed on the decoder (host). We show that it is enough for the encoder to transmit a sparse set of pixels. Using only $1+\epsilon$ images, with $\epsilon$ as little as 2% of the image, the decoder can compute a depth map. The depth map's accuracy is comparable to traditional stereo matching algorithms that require both images as input. Using the depth map and the left image, the right image can be synthesized. No computations are required at the encoder, and the decoder's runtime is linear in the images' size. version:2
arxiv-1305-4987 | Robust Logistic Regression using Shift Parameters (Long Version) | http://arxiv.org/abs/1305.4987 | id:1305.4987 author:Julie Tibshirani, Christopher D. Manning category:cs.AI cs.LG stat.ML  published:2013-05-21 summary:Annotation errors can significantly hurt classifier performance, yet datasets are only growing noisier with the increased use of Amazon Mechanical Turk and techniques like distant supervision that automatically generate labels. In this paper, we present a robust extension of logistic regression that incorporates the possibility of mislabelling directly into the objective. Our model can be trained through nearly the same means as logistic regression, and retains its efficiency on high-dimensional datasets. Through named entity recognition experiments, we demonstrate that our approach can provide a significant improvement over the standard model when annotation errors are present. version:2
arxiv-1404-7255 | Meteorological time series forecasting based on MLP modelling using heterogeneous transfer functions | http://arxiv.org/abs/1404.7255 | id:1404.7255 author:Cyril Voyant, Marie Laure Nivet, Christophe Paoli, Marc Muselli, Gilles Notton category:cs.LG  published:2014-04-29 summary:In this paper, we propose to study four meteorological and seasonal time series coupled with a multi-layer perceptron (MLP) modeling. We chose to combine two transfer functions for the nodes of the hidden layer, and to use a temporal indicator (time index as input) in order to take into account the seasonal aspect of the studied time series. The results of the prediction concern two years of measurements and the learning step, eight independent years. We show that this methodology can improve the accuracy of meteorological data estimation compared to a classical MLP modelling with a homogenous transfer function. version:1
arxiv-1404-7236 | High Dimensional Semiparametric Latent Graphical Model for Mixed Data | http://arxiv.org/abs/1404.7236 | id:1404.7236 author:Jianqing Fan, Han Liu, Yang Ning, Hui Zou category:stat.ML  published:2014-04-29 summary:Graphical models are commonly used tools for modeling multivariate random variables. While there exist many convenient multivariate distributions such as Gaussian distribution for continuous data, mixed data with the presence of discrete variables or a combination of both continuous and discrete variables poses new challenges in statistical modeling. In this paper, we propose a semiparametric model named latent Gaussian copula model for binary and mixed data. The observed binary data are assumed to be obtained by dichotomizing a latent variable satisfying the Gaussian copula distribution or the nonparanormal distribution. The latent Gaussian model with the assumption that the latent variables are multivariate Gaussian is a special case of the proposed model. A novel rank-based approach is proposed for both latent graph estimation and latent principal component analysis. Theoretically, the proposed methods achieve the same rates of convergence for both precision matrix estimation and eigenvector estimation, as if the latent variables were observed. Under similar conditions, the consistency of graph structure recovery and feature selection for leading eigenvectors is established. The performance of the proposed methods is numerically assessed through simulation studies, and the usage of our methods is illustrated by a genetic dataset. version:1
arxiv-1404-7212 | Structural Group Sparse Representation for Image Compressive Sensing Recovery | http://arxiv.org/abs/1404.7212 | id:1404.7212 author:Jian Zhang, Debin Zhao, Feng Jiang, Wen Gao category:cs.CV  published:2014-04-29 summary:Compressive Sensing (CS) theory shows that a signal can be decoded from many fewer measurements than suggested by the Nyquist sampling theory, when the signal is sparse in some domain. Most of conventional CS recovery approaches, however, exploited a set of fixed bases (e.g. DCT, wavelet, contourlet and gradient domain) for the entirety of a signal, which are irrespective of the nonstationarity of natural signals and cannot achieve high enough degree of sparsity, thus resulting in poor rate-distortion performance. In this paper, we propose a new framework for image compressive sensing recovery via structural group sparse representation (SGSR) modeling, which enforces image sparsity and self-similarity simultaneously under a unified framework in an adaptive group domain, thus greatly confining the CS solution space. In addition, an efficient iterative shrinkage/thresholding algorithm based technique is developed to solve the above optimization problem. Experimental results demonstrate that the novel CS recovery strategy achieves significant performance improvements over the current state-of-the-art schemes and exhibits nice convergence. version:1
arxiv-1404-7211 | Spatially Directional Predictive Coding for Block-based Compressive Sensing of Natural Images | http://arxiv.org/abs/1404.7211 | id:1404.7211 author:Jian Zhang, Debin Zhao, Feng Jiang category:cs.CV  published:2014-04-29 summary:A novel coding strategy for block-based compressive sens-ing named spatially directional predictive coding (SDPC) is proposed, which efficiently utilizes the intrinsic spatial cor-relation of natural images. At the encoder, for each block of compressive sensing (CS) measurements, the optimal pre-diction is selected from a set of prediction candidates that are generated by four designed directional predictive modes. Then, the resulting residual is processed by scalar quantiza-tion (SQ). At the decoder, the same prediction is added onto the de-quantized residuals to produce the quantized CS measurements, which is exploited for CS reconstruction. Experimental results substantiate significant improvements achieved by SDPC-plus-SQ in rate distortion performance as compared with SQ alone and DPCM-plus-SQ. version:1
arxiv-1404-7203 | Randomized Sketches of Convex Programs with Sharp Guarantees | http://arxiv.org/abs/1404.7203 | id:1404.7203 author:Mert Pilanci, Martin J. Wainwright category:cs.IT cs.DS math.IT math.OC stat.ML  published:2014-04-29 summary:Random projection (RP) is a classical technique for reducing storage and computational costs. We analyze RP-based approximations of convex programs, in which the original optimization problem is approximated by the solution of a lower-dimensional problem. Such dimensionality reduction is essential in computation-limited settings, since the complexity of general convex programming can be quite high (e.g., cubic for quadratic programs, and substantially higher for semidefinite programs). In addition to computational savings, random projection is also useful for reducing memory usage, and has useful properties for privacy-sensitive optimization. We prove that the approximation ratio of this procedure can be bounded in terms of the geometry of constraint set. For a broad class of random projections, including those based on various sub-Gaussian distributions as well as randomized Hadamard and Fourier transforms, the data matrix defining the cost function can be projected down to the statistical dimension of the tangent cone of the constraints at the original solution, which is often substantially smaller than the original dimension. We illustrate consequences of our theory for various cases, including unconstrained and $\ell_1$-constrained least squares, support vector machines, low-rank matrix estimation, and discuss implications on privacy-sensitive optimization and some connections with de-noising and compressed sensing. version:1
arxiv-1404-7195 | Fast Approximation of Rotations and Hessians matrices | http://arxiv.org/abs/1404.7195 | id:1404.7195 author:Michael Mathieu, Yann LeCun category:cs.LG  published:2014-04-29 summary:A new method to represent and approximate rotation matrices is introduced. The method represents approximations of a rotation matrix $Q$ with linearithmic complexity, i.e. with $\frac{1}{2}n\lg(n)$ rotations over pairs of coordinates, arranged in an FFT-like fashion. The approximation is "learned" using gradient descent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ is a diagonal matrix. It can be used to approximate covariance matrix of Gaussian models in order to speed up inference, or to estimate and track the inverse Hessian of an objective function by relating changes in parameters to changes in gradient along the trajectory followed by the optimization procedure. Experiments were conducted to approximate synthetic matrices, covariance matrices of real data, and Hessian matrices of objective functions involved in machine learning problems. version:1
arxiv-1312-2578 | Kernel-based Distance Metric Learning in the Output Space | http://arxiv.org/abs/1312.2578 | id:1312.2578 author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG  published:2013-12-09 summary:In this paper we present two related, kernel-based Distance Metric Learning (DML) methods. Their respective models non-linearly map data from their original space to an output space, and subsequent distance measurements are performed in the output space via a Mahalanobis metric. The dimensionality of the output space can be directly controlled to facilitate the learning of a low-rank metric. Both methods allow for simultaneous inference of the associated metric and the mapping to the output space, which can be used to visualize the data, when the output space is 2- or 3-dimensional. Experimental results for a collection of classification tasks illustrate the advantages of the proposed methods over other traditional and kernel-based DML approaches. version:2
arxiv-1404-7456 | Automatic Differentiation of Algorithms for Machine Learning | http://arxiv.org/abs/1404.7456 | id:1404.7456 author:Atilim Gunes Baydin, Barak A. Pearlmutter category:cs.LG cs.SC stat.ML 68W30  65D25  68T05 G.1.4; I.2.6  published:2014-04-28 summary:Automatic differentiation---the mechanical transformation of numeric computer programs to calculate derivatives efficiently and accurately---dates to the origin of the computer age. Reverse mode automatic differentiation both antedates and generalizes the method of backwards propagation of errors used in machine learning. Despite this, practitioners in a variety of fields, including machine learning, have been little influenced by automatic differentiation, and make scant use of available tools. Here we review the technique of automatic differentiation, describe its two main modes, and explain how it can benefit machine learning practitioners. To reach the widest possible audience our treatment assumes only elementary differential calculus, and does not assume any knowledge of linear algebra. version:1
arxiv-1206-0338 | Poisson noise reduction with non-local PCA | http://arxiv.org/abs/1206.0338 | id:1206.0338 author:Joseph Salmon, Zachary Harmany, Charles-Alban Deledalle, Rebecca Willett category:cs.CV cs.LG stat.CO  published:2012-06-02 summary:Photon-limited imaging arises when the number of photons collected by a sensor array is small relative to the number of detector elements. Photon limitations are an important concern for many applications such as spectral imaging, night vision, nuclear medicine, and astronomy. Typically a Poisson distribution is used to model these observations, and the inherent heteroscedasticity of the data combined with standard noise removal methods yields significant artifacts. This paper introduces a novel denoising algorithm for photon-limited images which combines elements of dictionary learning and sparse patch-based representations of images. The method employs both an adaptation of Principal Component Analysis (PCA) for Poisson noise and recently developed sparsity-regularized convex optimization algorithms for photon-limited images. A comprehensive empirical evaluation of the proposed method helps characterize the performance of this approach relative to other state-of-the-art denoising methods. The results reveal that, despite its conceptual simplicity, Poisson PCA-based denoising appears to be highly competitive in very low light regimes. version:4
arxiv-1404-6876 | Conditional Density Estimation with Dimensionality Reduction via Squared-Loss Conditional Entropy Minimization | http://arxiv.org/abs/1404.6876 | id:1404.6876 author:Voot Tangkaratt, Ning Xie, Masashi Sugiyama category:cs.LG stat.ML  published:2014-04-28 summary:Regression aims at estimating the conditional mean of output given input. However, regression is not informative enough if the conditional density is multimodal, heteroscedastic, and asymmetric. In such a case, estimating the conditional density itself is preferable, but conditional density estimation (CDE) is challenging in high-dimensional space. A naive approach to coping with high-dimensionality is to first perform dimensionality reduction (DR) and then execute CDE. However, such a two-step process does not perform well in practice because the error incurred in the first DR step can be magnified in the second CDE step. In this paper, we propose a novel single-shot procedure that performs CDE and DR simultaneously in an integrated way. Our key idea is to formulate DR as the problem of minimizing a squared-loss variant of conditional entropy, and this is solved via CDE. Thus, an additional CDE step is not needed after DR. We demonstrate the usefulness of the proposed method through extensive experiments on various datasets including humanoid robot transition and computer art. version:1
arxiv-1404-6871 | Proximal Iteratively Reweighted Algorithm with Multiple Splitting for Nonconvex Sparsity Optimization | http://arxiv.org/abs/1404.6871 | id:1404.6871 author:Canyi Lu, Yunchao Wei, Zhouchen Lin, Shuicheng Yan category:cs.NA cs.CV  published:2014-04-28 summary:This paper proposes the Proximal Iteratively REweighted (PIRE) algorithm for solving a general problem, which involves a large body of nonconvex sparse and structured sparse related problems. Comparing with previous iterative solvers for nonconvex sparse problem, PIRE is much more general and efficient. The computational cost of PIRE in each iteration is usually as low as the state-of-the-art convex solvers. We further propose the PIRE algorithm with Parallel Splitting (PIRE-PS) and PIRE algorithm with Alternative Updating (PIRE-AU) to handle the multi-variable problems. In theory, we prove that our proposed methods converge and any limit solution is a stationary point. Extensive experiments on both synthesis and real data sets demonstrate that our methods achieve comparative learning performance, but are much more efficient, by comparing with previous nonconvex solvers. version:1
arxiv-1404-3366 | Learning Deep Convolutional Features for MRI Based Alzheimer's Disease Classification | http://arxiv.org/abs/1404.3366 | id:1404.3366 author:Fayao Liu, Chunhua Shen category:cs.CV  published:2014-04-13 summary:Effective and accurate diagnosis of Alzheimer's disease (AD) or mild cognitive impairment (MCI) can be critical for early treatment and thus has attracted more and more attention nowadays. Since first introduced, machine learning methods have been gaining increasing popularity for AD related research. Among the various identified biomarkers, magnetic resonance imaging (MRI) are widely used for the prediction of AD or MCI. However, before a machine learning algorithm can be applied, image features need to be extracted to represent the MRI images. While good representations can be pivotal to the classification performance, almost all the previous studies typically rely on human labelling to find the regions of interest (ROI) which may be correlated to AD, such as hippocampus, amygdala, precuneus, etc. This procedure requires domain knowledge and is costly and tedious. Instead of relying on extraction of ROI features, it is more promising to remove manual ROI labelling from the pipeline and directly work on the raw MRI images. In other words, we can let the machine learning methods to figure out these informative and discriminative image structures for AD classification. In this work, we propose to learn deep convolutional image features using unsupervised and supervised learning. Deep learning has emerged as a powerful tool in the machine learning community and has been successfully applied to various tasks. We thus propose to exploit deep features of MRI images based on a pre-trained large convolutional neural network (CNN) for AD and MCI classification, which spares the effort of manual ROI annotation process. version:2
arxiv-1404-6818 | Subspace clustering of dimensionality-reduced data | http://arxiv.org/abs/1404.6818 | id:1404.6818 author:Reinhard Heckel, Michael Tschannen, Helmut Bölcskei category:cs.IT math.IT stat.ML  published:2014-04-27 summary:Subspace clustering refers to the problem of clustering unlabeled high-dimensional data points into a union of low-dimensional linear subspaces, assumed unknown. In practice one may have access to dimensionality-reduced observations of the data only, resulting, e.g., from "undersampling" due to complexity and speed constraints on the acquisition device. More pertinently, even if one has access to the high-dimensional data set it is often desirable to first project the data points into a lower-dimensional space and to perform the clustering task there; this reduces storage requirements and computational cost. The purpose of this paper is to quantify the impact of dimensionality-reduction through random projection on the performance of the sparse subspace clustering (SSC) and the thresholding based subspace clustering (TSC) algorithms. We find that for both algorithms dimensionality reduction down to the order of the subspace dimensions is possible without incurring significant performance degradation. The mathematical engine behind our theorems is a result quantifying how the affinities between subspaces change under random dimensionality reducing projections. version:1
arxiv-1404-6163 | Overlapping Trace Norms in Multi-View Learning | http://arxiv.org/abs/1404.6163 | id:1404.6163 author:Behrouz Behmardi, Cedric Archambeau, Guillaume Bouchard category:cs.LG  published:2014-04-24 summary:Multi-view learning leverages correlations between different sources of data to make predictions in one view based on observations in another view. A popular approach is to assume that, both, the correlations between the views and the view-specific covariances have a low-rank structure, leading to inter-battery factor analysis, a model closely related to canonical correlation analysis. We propose a convex relaxation of this model using structured norm regularization. Further, we extend the convex formulation to a robust version by adding an l1-penalized matrix to our estimator, similarly to convex robust PCA. We develop and compare scalable algorithms for several convex multi-view models. We show experimentally that the view-specific correlations are improving data imputation performances, as well as labeling accuracy in real-world multi-label prediction tasks. version:2
arxiv-1404-6736 | Robust and Efficient Subspace Segmentation via Least Squares Regression | http://arxiv.org/abs/1404.6736 | id:1404.6736 author:Can-Yi Lu, Hai Min, Zhong-Qiu Zhao, Lin Zhu, De-Shuang Huang, Shuicheng Yan category:cs.CV  published:2014-04-27 summary:This paper studies the subspace segmentation problem which aims to segment data drawn from a union of multiple linear subspaces. Recent works by using sparse representation, low rank representation and their extensions attract much attention. If the subspaces from which the data drawn are independent or orthogonal, they are able to obtain a block diagonal affinity matrix, which usually leads to a correct segmentation. The main differences among them are their objective functions. We theoretically show that if the objective function satisfies some conditions, and the data are sufficiently drawn from independent subspaces, the obtained affinity matrix is always block diagonal. Furthermore, the data sampling can be insufficient if the subspaces are orthogonal. Some existing methods are all special cases. Then we present the Least Squares Regression (LSR) method for subspace segmentation. It takes advantage of data correlation, which is common in real data. LSR encourages a grouping effect which tends to group highly correlated data together. Experimental results on the Hopkins 155 database and Extended Yale Database B show that our method significantly outperforms state-of-the-art methods. Beyond segmentation accuracy, all experiments demonstrate that LSR is much more efficient. version:1
arxiv-1404-3174 | Model Based Clustering of High-Dimensional Binary Data | http://arxiv.org/abs/1404.3174 | id:1404.3174 author:Yang Tang, Ryan P. Browne, Paul D. McNicholas category:stat.ME stat.CO stat.ML  published:2014-04-11 summary:We propose a mixture of latent trait models with common slope parameters (MCLT) for model-based clustering of high-dimensional binary data, a data type for which few established methods exist. Recent work on clustering of binary data, based on a $d$-dimensional Gaussian latent variable, is extended by incorporating common factor analyzers. Accordingly, our approach facilitates a low-dimensional visual representation of the clusters. We extend the model further by the incorporation of random block effects. The dependencies in each block are taken into account through block-specific parameters that are considered to be random variables. A variational approximation to the likelihood is exploited to derive a fast algorithm for determining the model parameters. Our approach is demonstrated on real and simulated data. version:2
arxiv-1404-6702 | A Constrained Matrix-Variate Gaussian Process for Transposable Data | http://arxiv.org/abs/1404.6702 | id:1404.6702 author:Oluwasanmi Koyejo, Cheng Lee, Joydeep Ghosh category:stat.ML cs.LG  published:2014-04-27 summary:Transposable data represents interactions among two sets of entities, and are typically represented as a matrix containing the known interaction values. Additional side information may consist of feature vectors specific to entities corresponding to the rows and/or columns of such a matrix. Further information may also be available in the form of interactions or hierarchies among entities along the same mode (axis). We propose a novel approach for modeling transposable data with missing interactions given additional side information. The interactions are modeled as noisy observations from a latent noise free matrix generated from a matrix-variate Gaussian process. The construction of row and column covariances using side information provides a flexible mechanism for specifying a-priori knowledge of the row and column correlations in the data. Further, the use of such a prior combined with the side information enables predictions for new rows and columns not observed in the training data. In this work, we combine the matrix-variate Gaussian process model with low rank constraints. The constrained Gaussian process approach is applied to the prediction of hidden associations between genes and diseases using a small set of observed associations as well as prior covariances induced by gene-gene interaction networks and disease ontologies. The proposed approach is also applied to recommender systems data which involves predicting the item ratings of users using known associations as well as prior covariances induced by social networks. We present experimental results that highlight the performance of constrained matrix-variate Gaussian process as compared to state of the art approaches in each domain. version:1
arxiv-1404-5997 | One weird trick for parallelizing convolutional neural networks | http://arxiv.org/abs/1404.5997 | id:1404.5997 author:Alex Krizhevsky category:cs.NE cs.DC cs.LG  published:2014-04-23 summary:I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks. version:2
arxiv-1404-6691 | Sinogram constrained TV-minimization for metal artifact reduction in CT | http://arxiv.org/abs/1404.6691 | id:1404.6691 author:Clemens Schiffer, Kristian Bredies category:math.NA cs.CV physics.med-ph  published:2014-04-26 summary:A new method for reducing metal artifacts in X-ray computed tomography (CT) images is presented. It bases on the solution of a convex optimization problem with inequality constraints on the sinogram, and total variation regularization for the reconstructed image. The Chambolle-Pock algorithm is used to numerically solve the discretized version of the optimization problem. As proof of concept we present and discuss numerical results for synthetic data. version:1
arxiv-1404-6674 | A Comparison of First-order Algorithms for Machine Learning | http://arxiv.org/abs/1404.6674 | id:1404.6674 author:Yu Wei, Pock Thomas category:cs.LG  published:2014-04-26 summary:Using an optimization algorithm to solve a machine learning problem is one of mainstreams in the field of science. In this work, we demonstrate a comprehensive comparison of some state-of-the-art first-order optimization algorithms for convex optimization problems in machine learning. We concentrate on several smooth and non-smooth machine learning problems with a loss function plus a regularizer. The overall experimental results show the superiority of primal-dual algorithms in solving a machine learning problem from the perspectives of the ease to construct, running time and accuracy. version:1
arxiv-1404-6640 | Estimation of positive definite M-matrices and structure learning for attractive Gaussian Markov Random fields | http://arxiv.org/abs/1404.6640 | id:1404.6640 author:Martin Slawski, Matthias Hein category:math.ST stat.ML stat.TH  published:2014-04-26 summary:Consider a random vector with finite second moments. If its precision matrix is an M-matrix, then all partial correlations are non-negative. If that random vector is additionally Gaussian, the corresponding Markov random field (GMRF) is called attractive. We study estimation of M-matrices taking the role of inverse second moment or precision matrices using sign-constrained log-determinant divergence minimization. We also treat the high-dimensional case with the number of variables exceeding the sample size. The additional sign-constraints turn out to greatly simplify the estimation problem: we provide evidence that explicit regularization is no longer required. To solve the resulting convex optimization problem, we propose an algorithm based on block coordinate descent, in which each sub-problem can be recast as non-negative least squares problem. Illustrations on both simulated and real world data are provided. version:1
arxiv-1404-6538 | On Quadratization of Pseudo-Boolean Functions | http://arxiv.org/abs/1404.6538 | id:1404.6538 author:Endre Boros, Aritanan Gruber category:math.OC cs.CV math.CO  published:2014-04-25 summary:We survey current term-wise techniques for quadratizing high-degree pseudo-Boolean functions and introduce a new one, which allows multiple splits of terms. We also introduce the first aggregative approach, which splits a collection of terms based on their common parts. version:1
arxiv-1404-6535 | Quadratization of Symmetric Pseudo-Boolean Functions | http://arxiv.org/abs/1404.6535 | id:1404.6535 author:Martin Anthony, Endre Boros, Yves Crama, Aritanan Gruber category:math.OC cs.CC cs.CV math.CO 06E30  90C09  90C20  published:2014-04-25 summary:A pseudo-Boolean function is a real-valued function $f(x)=f(x_1,x_2,\ldots,x_n)$ of $n$ binary variables; that is, a mapping from $\{0,1\}^n$ to $\mathbb{R}$. For a pseudo-Boolean function $f(x)$ on $\{0,1\}^n$, we say that $g(x,y)$ is a quadratization of $f$ if $g(x,y)$ is a quadratic polynomial depending on $x$ and on $m$ auxiliary binary variables $y_1,y_2,\ldots,y_m$ such that $f(x)= \min \{g(x,y) : y \in \{0,1\}^m \}$ for all $x \in \{0,1\}^n$. By means of quadratizations, minimization of $f$ is reduced to minimization (over its extended set of variables) of the quadratic function $g(x,y)$. This is of some practical interest because minimization of quadratic functions has been thoroughly studied for the last few decades, and much progress has been made in solving such problems exactly or heuristically. A related paper \cite{ABCG} initiated a systematic study of the minimum number of auxiliary $y$-variables required in a quadratization of an arbitrary function $f$ (a natural question, since the complexity of minimizing the quadratic function $g(x,y)$ depends, among other factors, on the number of binary variables). In this paper, we determine more precisely the number of auxiliary variables required by quadratizations of symmetric pseudo-Boolean functions $f(x)$, those functions whose value depends only on the Hamming weight of the input $x$ (the number of variables equal to $1$). version:1
arxiv-1405-2294 | Nonparametric Detection of Anomalous Data via Kernel Mean Embedding | http://arxiv.org/abs/1405.2294 | id:1405.2294 author:Shaofeng Zou, Yingbin Liang, H. Vincent Poor, Xinghua Shi category:cs.LG stat.ML  published:2014-04-25 summary:An anomaly detection problem is investigated, in which there are totally n sequences with s anomalous sequences to be detected. Each normal sequence contains m independent and identically distributed (i.i.d.) samples drawn from a distribution p, whereas each anomalous sequence contains m i.i.d. samples drawn from a distribution q that is distinct from p. The distributions p and q are assumed to be unknown a priori. Two scenarios, respectively with and without a reference sequence generated by p, are studied. Distribution-free tests are constructed using maximum mean discrepancy (MMD) as the metric, which is based on mean embeddings of distributions into a reproducing kernel Hilbert space (RKHS). For both scenarios, it is shown that as the number n of sequences goes to infinity, if the value of s is known, then the number m of samples in each sequence should be at the order O(log n) or larger in order for the developed tests to consistently detect s anomalous sequences. If the value of s is unknown, then m should be at the order strictly larger than O(log n). Computational complexity of all developed tests is shown to be polynomial. Numerical results demonstrate that our tests outperform (or perform as well as) the tests based on other competitive traditional statistical approaches and kernel-based approaches under various cases. Consistency of the proposed test is also demonstrated on a real data set. version:1
arxiv-1312-5198 | Learning Semantic Script Knowledge with Event Embeddings | http://arxiv.org/abs/1312.5198 | id:1312.5198 author:Ashutosh Modi, Ivan Titov category:cs.LG cs.CL stat.ML I.2.6; I.2.7  published:2013-12-18 summary:Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods. version:4
arxiv-1404-6413 | Indoor Activity Detection and Recognition for Sport Games Analysis | http://arxiv.org/abs/1404.6413 | id:1404.6413 author:Georg Waltner, Thomas Mauthner, Horst Bischof category:cs.CV  published:2014-04-25 summary:Activity recognition in sport is an attractive field for computer vision research. Game, player and team analysis are of great interest and research topics within this field emerge with the goal of automated analysis. The very specific underlying rules of sports can be used as prior knowledge for the recognition task and present a constrained environment for evaluation. This paper describes recognition of single player activities in sport with special emphasis on volleyball. Starting from a per-frame player-centered activity recognition, we incorporate geometry and contextual information via an activity context descriptor that collects information about all player's activities over a certain timespan relative to the investigated player. The benefit of this context information on single player activity recognition is evaluated on our new real-life dataset presenting a total amount of almost 36k annotated frames containing 7 activity classes within 6 videos of professional volleyball games. Our incorporation of the contextual information improves the average player-centered classification performance of 77.56% by up to 18.35% on specific classes, proving that spatio-temporal context is an important clue for activity recognition. version:1
arxiv-1405-4894 | Optimization of OFDM radar waveforms using genetic algorithms | http://arxiv.org/abs/1405.4894 | id:1405.4894 author:Gabriel Lellouch, Amit Kumar Mishra category:cs.NE  published:2014-04-25 summary:In this paper, we present our investigations on the use of single objective and multiobjective genetic algorithms based optimisation algorithms to improve the design of OFDM pulses for radar. We discuss these optimization procedures in the scope of a waveform design intended for two different radar processing solutions. Lastly, we show how the encoding solution is suited to permit the optimizations of waveform for OFDM radar related challenges such as enhanced detection. version:1
arxiv-1303-1152 | An Equivalence between the Lasso and Support Vector Machines | http://arxiv.org/abs/1303.1152 | id:1303.1152 author:Martin Jaggi category:cs.LG stat.ML 65C60  90C25  68T05 F.2.2; I.5.1  published:2013-03-05 summary:We investigate the relation of two fundamental tools in machine learning and signal processing, that is the support vector machine (SVM) for classification, and the Lasso technique used in regression. We show that the resulting optimization problems are equivalent, in the following sense. Given any instance of an $\ell_2$-loss soft-margin (or hard-margin) SVM, we construct a Lasso instance having the same optimal solutions, and vice versa. As a consequence, many existing optimization algorithms for both SVMs and Lasso can also be applied to the respective other problem instances. Also, the equivalence allows for many known theoretical insights for SVM and Lasso to be translated between the two settings. One such implication gives a simple kernelized version of the Lasso, analogous to the kernels used in the SVM setting. Another consequence is that the sparsity of a Lasso solution is equal to the number of support vectors for the corresponding SVM instance, and that one can use screening rules to prune the set of support vectors. Furthermore, we can relate sublinear time algorithms for the two problems, and give a new such algorithm variant for the Lasso. We also study the regularization paths for both methods. version:2
arxiv-1404-6369 | Applying machine learning to the problem of choosing a heuristic to select the variable ordering for cylindrical algebraic decomposition | http://arxiv.org/abs/1404.6369 | id:1404.6369 author:Zongyan Huang, Matthew England, David Wilson, James H. Davenport, Lawrence C. Paulson, James Bridge category:cs.SC cs.LG 68W30  68T05  O3C10 I.2.6  published:2014-04-25 summary:Cylindrical algebraic decomposition(CAD) is a key tool in computational algebraic geometry, particularly for quantifier elimination over real-closed fields. When using CAD, there is often a choice for the ordering placed on the variables. This can be important, with some problems infeasible with one variable ordering but easy with another. Machine learning is the process of fitting a computer model to a complex function based on properties learned from measured data. In this paper we use machine learning (specifically a support vector machine) to select between heuristics for choosing a variable ordering, outperforming each of the separate heuristics. version:1
arxiv-1404-6351 | Improving weather radar by fusion and classification | http://arxiv.org/abs/1404.6351 | id:1404.6351 author:Harald Ganster, Martina Uray, Sylwia Steginska, Gerardus Croonen, Rudolf Kaltenböck, Karin Hennermann category:cs.CV  published:2014-04-25 summary:In air traffic management (ATM) all necessary operations (tactical planing, sector configuration, required staffing, runway configuration, routing of approaching aircrafts) rely on accurate measurements and predictions of the current weather situation. An essential basis of information is delivered by weather radar images (WXR), which, unfortunately, exhibit a vast amount of disturbances. Thus, the improvement of these datasets is the key factor for more accurate predictions of weather phenomena and weather conditions. Image processing methods based on texture analysis and geometric operators allow to identify regions including artefacts as well as zones of missing information. Correction of these zones is implemented by exploiting multi-spectral satellite data (Meteosat Second Generation). Results prove that the proposed system for artefact detection and data correction significantly improves the quality of WXR data and, thus, enables more reliable weather now- and forecast leading to increased ATM safety. version:1
arxiv-1307-2982 | Fast Exact Search in Hamming Space with Multi-Index Hashing | http://arxiv.org/abs/1307.2982 | id:1307.2982 author:Mohammad Norouzi, Ali Punjani, David J. Fleet category:cs.CV cs.AI cs.DS cs.IR  published:2013-07-11 summary:There is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search. Although binary codes are motivated by their use as direct indices (addresses) into a hash table, codes longer than 32 bits are not being used as such, as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in Hamming space. The approach is storage efficient and straightforward to implement. Theoretical analysis shows that the algorithm exhibits sub-linear run-time behavior for uniformly distributed codes. Empirical results show dramatic speedups over a linear scan baseline for datasets of up to one billion codes of 64, 128, or 256 bits. version:3
arxiv-1404-6289 | Solution Path Clustering with Adaptive Concave Penalty | http://arxiv.org/abs/1404.6289 | id:1404.6289 author:Yuliya Marchetti, Qing Zhou category:stat.ME stat.ML  published:2014-04-24 summary:Fast accumulation of large amounts of complex data has created a need for more sophisticated statistical methodologies to discover interesting patterns and better extract information from these data. The large scale of the data often results in challenging high-dimensional estimation problems where only a minority of the data shows specific grouping patterns. To address these emerging challenges, we develop a new clustering methodology that introduces the idea of a regularization path into unsupervised learning. A regularization path for a clustering problem is created by varying the degree of sparsity constraint that is imposed on the differences between objects via the minimax concave penalty with adaptive tuning parameters. Instead of providing a single solution represented by a cluster assignment for each object, the method produces a short sequence of solutions that determines not only the cluster assignment but also a corresponding number of clusters for each solution. The optimization of the penalized loss function is carried out through an MM algorithm with block coordinate descent. The advantages of this clustering algorithm compared to other existing methods are as follows: it does not require the input of the number of clusters; it is capable of simultaneously separating irrelevant or noisy observations that show no grouping pattern, which can greatly improve data interpretation; it is a general methodology that can be applied to many clustering problems. We test this method on various simulated datasets and on gene expression data, where it shows better or competitive performance compared against several clustering methods. version:1
arxiv-1404-6283 | Automated adaptive inference of coarse-grained dynamical models in systems biology | http://arxiv.org/abs/1404.6283 | id:1404.6283 author:Bryan C. Daniels, Ilya Nemenman category:q-bio.QM physics.data-an stat.ML  published:2014-04-24 summary:Cellular regulatory dynamics is driven by large and intricate networks of interactions at the molecular scale, whose sheer size obfuscates understanding. In light of limited experimental data, many parameters of such dynamics are unknown, and thus models built on the detailed, mechanistic viewpoint overfit and are not predictive. At the other extreme, simple ad hoc models of complex processes often miss defining features of the underlying systems. Here we propose an approach that instead constructs phenomenological, coarse-grained models of network dynamics that automatically adapt their complexity to the amount of available data. Such adaptive models lead to accurate predictions even when microscopic details of the studied systems are unknown due to insufficient data. The approach is computationally tractable, even for a relatively large number of dynamical variables, allowing its software realization, named Sir Isaac, to make successful predictions even when important dynamic variables are unobserved. For example, it matches the known phase space structure for simulated planetary motion data, avoids overfitting in a complex biological signaling system, and produces accurate predictions for a yeast glycolysis model with only tens of data points and over half of the interacting species unobserved. version:1
arxiv-1404-6272 | Scalable Similarity Learning using Large Margin Neighborhood Embedding | http://arxiv.org/abs/1404.6272 | id:1404.6272 author:Zhaowen Wang, Jianchao Yang, Zhe Lin, Jonathan Brandt, Shiyu Chang, Thomas Huang category:cs.CV cs.LG  published:2014-04-24 summary:Classifying large-scale image data into object categories is an important problem that has received increasing research attention. Given the huge amount of data, non-parametric approaches such as nearest neighbor classifiers have shown promising results, especially when they are underpinned by a learned distance or similarity measurement. Although metric learning has been well studied in the past decades, most existing algorithms are impractical to handle large-scale data sets. In this paper, we present an image similarity learning method that can scale well in both the number of images and the dimensionality of image descriptors. To this end, similarity comparison is restricted to each sample's local neighbors and a discriminative similarity measure is induced from large margin neighborhood embedding. We also exploit the ensemble of projections so that high-dimensional features can be processed in a set of lower-dimensional subspaces in parallel without much performance compromise. The similarity function is learned online using a stochastic gradient descent algorithm in which the triplet sampling strategy is customized for quick convergence of classification performance. The effectiveness of our proposed model is validated on several data sets with scales varying from tens of thousands to one million images. Recognition accuracies competitive with the state-of-the-art performance are achieved with much higher efficiency and scalability. version:1
arxiv-1404-6216 | CoRE Kernels | http://arxiv.org/abs/1404.6216 | id:1404.6216 author:Ping Li category:stat.ML cs.DS cs.LG stat.ME  published:2014-04-24 summary:The term "CoRE kernel" stands for correlation-resemblance kernel. In many applications (e.g., vision), the data are often high-dimensional, sparse, and non-binary. We propose two types of (nonlinear) CoRE kernels for non-binary sparse data and demonstrate the effectiveness of the new kernels through a classification experiment. CoRE kernels are simple with no tuning parameters. However, training nonlinear kernel SVM can be (very) costly in time and memory and may not be suitable for truly large-scale industrial applications (e.g. search). In order to make the proposed CoRE kernels more practical, we develop basic probabilistic hashing algorithms which transform nonlinear kernels into linear kernels. version:1
arxiv-1312-6026 | How to Construct Deep Recurrent Neural Networks | http://arxiv.org/abs/1312.6026 | id:1312.6026 author:Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio category:cs.NE cs.LG stat.ML  published:2013-12-20 summary:In this paper, we explore different ways to extend a recurrent neural network (RNN) to a \textit{deep} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs. version:5
arxiv-1402-4304 | Automatic Construction and Natural-Language Description of Nonparametric Regression Models | http://arxiv.org/abs/1402.4304 | id:1402.4304 author:James Robert Lloyd, David Duvenaud, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani category:stat.ML cs.LG  published:2014-02-18 summary:This paper presents the beginnings of an automatic statistician, focusing on regression problems. Our system explores an open-ended space of statistical models to discover a good explanation of a data set, and then produces a detailed report with figures and natural-language text. Our approach treats unknown regression functions nonparametrically using Gaussian processes, which has two important consequences. First, Gaussian processes can model functions in terms of high-level properties (e.g. smoothness, trends, periodicity, changepoints). Taken together with the compositional structure of our language of models this allows us to automatically describe functions in simple terms. Second, the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state-of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains. version:3
arxiv-1404-6075 | Unsupervised Text Extraction from G-Maps | http://arxiv.org/abs/1404.6075 | id:1404.6075 author:Chandranath Adak category:cs.CV cs.AI  published:2014-04-24 summary:This paper represents an text extraction method from Google maps, GIS maps/images. Due to an unsupervised approach there is no requirement of any prior knowledge or training set about the textual and non-textual parts. Fuzzy CMeans clustering technique is used for image segmentation and Prewitt method is used to detect the edges. Connected component analysis and gridding technique enhance the correctness of the results. The proposed method reaches 98.5% accuracy level on the basis of experimental data sets. version:1
arxiv-1404-6074 | Classifying pairs with trees for supervised biological network inference | http://arxiv.org/abs/1404.6074 | id:1404.6074 author:Marie Schrynemackers, Louis Wehenkel, M. Madan Babu, Pierre Geurts category:cs.LG stat.ML  published:2014-04-24 summary:Networks are ubiquitous in biology and computational approaches have been largely investigated for their inference. In particular, supervised machine learning methods can be used to complete a partially known network by integrating various measurements. Two main supervised frameworks have been proposed: the local approach, which trains a separate model for each network node, and the global approach, which trains a single model over pairs of nodes. Here, we systematically investigate, theoretically and empirically, the exploitation of tree-based ensemble methods in the context of these two approaches for biological network inference. We first formalize the problem of network inference as classification of pairs, unifying in the process homogeneous and bipartite graphs and discussing two main sampling schemes. We then present the global and the local approaches, extending the later for the prediction of interactions between two unseen network nodes, and discuss their specializations to tree-based ensemble methods, highlighting their interpretability and drawing links with clustering techniques. Extensive computational experiments are carried out with these methods on various biological networks that clearly highlight that these methods are competitive with existing methods. version:1
arxiv-1404-6071 | Rough Clustering Based Unsupervised Image Change Detection | http://arxiv.org/abs/1404.6071 | id:1404.6071 author:Chandranath Adak category:cs.CV cs.AI  published:2014-04-24 summary:This paper introduces an unsupervised technique to detect the changed region of multitemporal images on a same reference plane with the help of rough clustering. The proposed technique is a soft-computing approach, based on the concept of rough set with rough clustering and Pawlak's accuracy. It is less noisy and avoids pre-deterministic knowledge about the distribution of the changed and unchanged regions. To show the effectiveness, the proposed technique is compared with some other approaches. version:1
arxiv-1206-2459 | Rényi Divergence and Kullback-Leibler Divergence | http://arxiv.org/abs/1206.2459 | id:1206.2459 author:Tim van Erven, Peter Harremoës category:cs.IT math.IT math.ST stat.ML stat.TH  published:2012-06-12 summary:R\'enyi divergence is related to R\'enyi entropy much like Kullback-Leibler divergence is related to Shannon's entropy, and comes up in many settings. It was introduced by R\'enyi as a measure of information that satisfies almost the same axioms as Kullback-Leibler divergence, and depends on a parameter that is called its order. In particular, the R\'enyi divergence of order 1 equals the Kullback-Leibler divergence. We review and extend the most important properties of R\'enyi divergence and Kullback-Leibler divergence, including convexity, continuity, limits of $\sigma$-algebras and the relation of the special order 0 to the Gaussian dichotomy and contiguity. We also show how to generalize the Pythagorean inequality to orders different from 1, and we extend the known equivalence between channel capacity and minimax redundancy to continuous channel inputs (for all orders) and present several other minimax results. version:2
arxiv-1403-3080 | Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling | http://arxiv.org/abs/1403.3080 | id:1403.3080 author:Xi Chen, Qihang Lin, Dengyong Zhou category:cs.LG math.OC stat.ML  published:2014-03-12 summary:In crowd labeling, a large amount of unlabeled data instances are outsourced to a crowd of workers. Workers will be paid for each label they provide, but the labeling requester usually has only a limited amount of the budget. Since data instances have different levels of labeling difficulty and workers have different reliability, it is desirable to have an optimal policy to allocate the budget among all instance-worker pairs such that the overall labeling accuracy is maximized. We consider categorical labeling tasks and formulate the budget allocation problem as a Bayesian Markov decision process (MDP), which simultaneously conducts learning and decision making. Using the dynamic programming (DP) recurrence, one can obtain the optimal allocation policy. However, DP quickly becomes computationally intractable when the size of the problem increases. To solve this challenge, we propose a computationally efficient approximate policy, called optimistic knowledge gradient policy. Our MDP is a quite general framework, which applies to both pull crowdsourcing marketplaces with homogeneous workers and push marketplaces with heterogeneous workers. It can also incorporate the contextual information of instances when they are available. The experiments on both simulated and real data show that the proposed policy achieves a higher labeling accuracy than other existing policies at the same budget level. version:2
arxiv-1404-6039 | The fshape framework for the variability analysis of functional shapes | http://arxiv.org/abs/1404.6039 | id:1404.6039 author:Benjamin Charlier, Nicolas Charon, Alain Trouvé category:cs.CG cs.CV math.DG  published:2014-04-24 summary:This article introduces a full mathematical and numerical framework for treating functional shapes (or fshapes) following the landmarks of shape spaces and shape analysis. Functional shapes can be described as signal functions supported on varying geometrical supports. Analysing variability of fshapes' ensembles require the modelling and quantification of joint variations in geometry and signal, which have been treated separately in previous approaches. Instead, building on the ideas of shape spaces for purely geometrical objects, we propose the extended concept of fshape bundles and define Riemannian metrics for fshape metamorphoses to model geometrico-functional transformations within these bundles. We also generalize previous works on data attachment terms based on the notion of varifolds and demonstrate the utility of these distances. Based on these, we propose variational formulations of the atlas estimation problem on populations of fshapes and prove existence of solutions for the different models. The second part of the article examines the numerical implementation of the models by detailing discrete expressions for the metrics and gradients and proposing an optimization scheme for the atlas estimation problem. We present a few results of the methodology on a synthetic dataset as well as on a population of retinal membranes with thickness maps. version:1
arxiv-1404-6031 | Maximum Margin Vector Correlation Filter | http://arxiv.org/abs/1404.6031 | id:1404.6031 author:Vishnu Naresh Boddeti, B. V. K. Vijaya Kumar category:cs.CV  published:2014-04-24 summary:Correlation Filters (CFs) are a class of classifiers which are designed for accurate pattern localization. Traditionally CFs have been used with scalar features only, which limits their ability to be used with vector feature representations like Gabor filter banks, SIFT, HOG, etc. In this paper we present a new CF named Maximum Margin Vector Correlation Filter (MMVCF) which extends the traditional CF designs to vector features. MMVCF further combines the generalization capability of large margin based classifiers like Support Vector Machines (SVMs) and the localization properties of CFs for better robustness to outliers. We demonstrate the efficacy of MMVCF for object detection and landmark localization on a variety of databases and demonstrate that MMVCF consistently shows improved pattern localization capability in comparison to SVMs. version:1
arxiv-1211-2958 | Study design in causal models | http://arxiv.org/abs/1211.2958 | id:1211.2958 author:Juha Karvanen category:stat.ME stat.AP stat.ML G.3; G.2.2  published:2012-11-13 summary:The causal assumptions, the study design and the data are the elements required for scientific inference in empirical research. The research is adequately communicated only if all of these elements and their relations are described precisely. Causal models with design describe the study design and the missing data mechanism together with the causal structure and allow the direct application of causal calculus in the estimation of the causal effects. The flow of the study is visualized by ordering the nodes of the causal diagram in two dimensions by their causal order and the time of the observation. Conclusions whether a causal or observational relationship can be estimated from the collected incomplete data can be made directly from the graph. Causal models with design offer a systematic and unifying view scientific inference and increase the clarity and speed of communication. Examples on the causal models for a case-control study, a nested case-control study, a clinical trial and a two-stage case-cohort study are presented. version:4
arxiv-1405-6164 | Generating Natural Language Descriptions from OWL Ontologies: the NaturalOWL System | http://arxiv.org/abs/1405.6164 | id:1405.6164 author:Ion Androutsopoulos, Gerasimos Lampouras, Dimitrios Galanis category:cs.CL cs.AI  published:2014-04-24 summary:We present NaturalOWL, a natural language generation system that produces texts describing individuals or classes of OWL ontologies. Unlike simpler OWL verbalizers, which typically express a single axiom at a time in controlled, often not entirely fluent natural language primarily for the benefit of domain experts, we aim to generate fluent and coherent multi-sentence texts for end-users. With a system like NaturalOWL, one can publish information in OWL on the Web, along with automatically produced corresponding texts in multiple languages, making the information accessible not only to computer programs and domain experts, but also end-users. We discuss the processing stages of NaturalOWL, the optional domain-dependent linguistic resources that the system can use at each stage, and why they are useful. We also present trials showing that when the domain-dependent llinguistic resources are available, NaturalOWL produces significantly better texts compared to a simpler verbalizer, and that the resources can be created with relatively light effort. version:1
arxiv-1405-5488 | On Learning Where To Look | http://arxiv.org/abs/1405.5488 | id:1405.5488 author:Marc'Aurelio Ranzato category:cs.CV cs.LG  published:2014-04-24 summary:Current automatic vision systems face two major challenges: scalability and extreme variability of appearance. First, the computational time required to process an image typically scales linearly with the number of pixels in the image, therefore limiting the resolution of input images to thumbnail size. Second, variability in appearance and pose of the objects constitute a major hurdle for robust recognition and detection. In this work, we propose a model that makes baby steps towards addressing these challenges. We describe a learning based method that recognizes objects through a series of glimpses. This system performs an amount of computation that scales with the complexity of the input rather than its number of pixels. Moreover, the proposed method is potentially more robust to changes in appearance since its parameters are learned in a data driven manner. Preliminary experiments on a handwritten dataset of digits demonstrate the computational advantages of this approach. version:1
arxiv-1404-6491 | An Account of Opinion Implicatures | http://arxiv.org/abs/1404.6491 | id:1404.6491 author:Janyce Wiebe, Lingjia Deng category:cs.CL cs.IR  published:2014-04-23 summary:While previous sentiment analysis research has concentrated on the interpretation of explicitly stated opinions and attitudes, this work initiates the computational study of a type of opinion implicature (i.e., opinion-oriented inference) in text. This paper described a rule-based framework for representing and analyzing opinion implicatures which we hope will contribute to deeper automatic interpretation of subjective language. In the course of understanding implicatures, the system recognizes implicit sentiments (and beliefs) toward various events and entities in the sentence, often attributed to different sources (holders) and of mixed polarities; thus, it produces a richer interpretation than is typical in opinion analysis. version:1
arxiv-1404-6955 | Probabilistic graphs using coupled random variables | http://arxiv.org/abs/1404.6955 | id:1404.6955 author:Kenric P. Nelson, Madalina Barbu, Brian J. Scannell category:cs.LG cs.IT cs.NE math.IT  published:2014-04-23 summary:Neural network design has utilized flexible nonlinear processes which can mimic biological systems, but has suffered from a lack of traceability in the resulting network. Graphical probabilistic models ground network design in probabilistic reasoning, but the restrictions reduce the expressive capability of each node making network designs complex. The ability to model coupled random variables using the calculus of nonextensive statistical mechanics provides a neural node design incorporating nonlinear coupling between input states while maintaining the rigor of probabilistic reasoning. A generalization of Bayes rule using the coupled product enables a single node to model correlation between hundreds of random variables. A coupled Markov random field is designed for the inferencing and classification of UCI's MLR 'Multiple Features Data Set' such that thousands of linear correlation parameters can be replaced with a single coupling parameter with just a (3%, 4%) percent reduction in (classification, inference) performance. version:1
arxiv-1312-7302 | Learning Human Pose Estimation Features with Convolutional Networks | http://arxiv.org/abs/1312.7302 | id:1312.7302 author:Arjun Jain, Jonathan Tompson, Mykhaylo Andriluka, Graham W. Taylor, Christoph Bregler category:cs.CV cs.LG cs.NE  published:2013-12-27 summary:This paper introduces a new architecture for human pose estimation using a multi- layer convolutional network architecture and a modified learning technique that learns low-level features and higher-level weak spatial models. Unconstrained human pose estimation is one of the hardest problems in computer vision, and our new architecture and learning schema shows significant improvement over the current state-of-the-art results. The main contribution of this paper is showing, for the first time, that a specific variation of deep learning is able to outperform all existing traditional architectures on this task. The paper also discusses several lessons learned while researching alternatives, most notably, that it is possible to learn strong low-level feature detectors on features that might even just cover a few pixels in the image. Higher-level spatial models improve somewhat the overall result, but to a much lesser extent then expected. Many researchers previously argued that the kinematic structure and top-down information is crucial for this domain, but with our purely bottom up, and weak spatial model, we could improve other more complicated architectures that currently produce the best results. This mirrors what many other researchers, like those in the speech recognition, object recognition, and other domains have experienced. version:6
arxiv-1404-5903 | Most Correlated Arms Identification | http://arxiv.org/abs/1404.5903 | id:1404.5903 author:Che-Yu Liu, Sébastien Bubeck category:stat.ML cs.LG  published:2014-04-23 summary:We study the problem of finding the most mutually correlated arms among many arms. We show that adaptive arms sampling strategies can have significant advantages over the non-adaptive uniform sampling strategy. Our proposed algorithms rely on a novel correlation estimator. The use of this accurate estimator allows us to get improved results for a wide range of problem instances. version:1
arxiv-1206-5538 | Representation Learning: A Review and New Perspectives | http://arxiv.org/abs/1206.5538 | id:1206.5538 author:Yoshua Bengio, Aaron Courville, Pascal Vincent category:cs.LG  published:2012-06-24 summary:The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning. version:3
arxiv-1305-4076 | Contractive De-noising Auto-encoder | http://arxiv.org/abs/1305.4076 | id:1305.4076 author:Fu-qiang Chen, Yan Wu, Guo-dong Zhao, Jun-ming Zhang, Ming Zhu, Jing Bai category:cs.LG  published:2013-05-17 summary:Auto-encoder is a special kind of neural network based on reconstruction. De-noising auto-encoder (DAE) is an improved auto-encoder which is robust to the input by corrupting the original data first and then reconstructing the original input by minimizing the reconstruction error function. And contractive auto-encoder (CAE) is another kind of improved auto-encoder to learn robust feature by introducing the Frobenius norm of the Jacobean matrix of the learned feature with respect to the original input. In this paper, we combine de-noising auto-encoder and contractive auto- encoder, and propose another improved auto-encoder, contractive de-noising auto- encoder (CDAE), which is robust to both the original input and the learned feature. We stack CDAE to extract more abstract features and apply SVM for classification. The experiment result on benchmark dataset MNIST shows that our proposed CDAE performed better than both DAE and CAE, proving the effective of our method. version:5
arxiv-1404-5767 | Codynamic Fitness Landscapes of Coevolutionary Minimal Substrates | http://arxiv.org/abs/1404.5767 | id:1404.5767 author:Hendrik Richter category:cs.NE  published:2014-04-23 summary:Coevolutionary minimal substrates are simple and abstract models that allow studying the relationships and codynamics between objective and subjective fitness. Using these models an approach is presented for defining and analyzing fitness landscapes of coevolutionary problems. We devise similarity measures of codynamic fitness landscapes and experimentally study minimal substrates of test--based and compositional problems for both cooperative and competitive interaction. version:1
arxiv-1404-5765 | Find my mug: Efficient object search with a mobile robot using semantic segmentation | http://arxiv.org/abs/1404.5765 | id:1404.5765 author:Daniel Wolf, Markus Bajones, Johann Prankl, Markus Vincze category:cs.CV cs.RO  published:2014-04-23 summary:In this paper, we propose an efficient semantic segmentation framework for indoor scenes, tailored to the application on a mobile robot. Semantic segmentation can help robots to gain a reasonable understanding of their environment, but to reach this goal, the algorithms not only need to be accurate, but also fast and robust. Therefore, we developed an optimized 3D point cloud processing framework based on a Randomized Decision Forest, achieving competitive results at sufficiently high frame rates. We evaluate the capabilities of our method on the popular NYU depth dataset and our own data and demonstrate its feasibility by deploying it on a mobile service robot, for which we could optimize an object search procedure using our results. version:1
arxiv-1106-2729 | Nested Graph Words for Object Recognition | http://arxiv.org/abs/1106.2729 | id:1106.2729 author:Svebor Karaman, Jenny Benois-Pineau, Rémi Mégret category:cs.MM cs.CV  published:2011-06-14 summary:In this paper, we propose a new, scalable approach for the task of object based image search or object recognition. Despite the very large literature existing on the scalability issues in CBIR in the sense of retrieval approaches, the scalability of media and scalability of features remain an issue. In our work we tackle the problem of scalability and structural organization of features. The proposed features are nested local graphs built upon sets of SURF feature points with Delaunay triangulation. A Bag-of-Visual-Words (BoVW) framework is applied on these graphs, giving birth to a Bag-of-Graph-Words representation. The nested nature of the descriptors consists in scaling from trivial Delaunay graphs - isolated feature points - by increasing the number of nodes layer by layer up to graphs with maximal number of nodes. For each layer of graphs its proper visual dictionary is built. The experiments conducted on the SIVAL data set reveal that the graph features at different layers exhibit complementary performances on the same content. The nested approach, the combination of all existing layers, yields significant improvement of the object recognition performance compared to single level approaches. version:2
arxiv-1404-1425 | Density Estimation via Adaptive Partition and Discrepancy Control | http://arxiv.org/abs/1404.1425 | id:1404.1425 author:Kun Yang, Wing Hung Wong category:stat.ML  published:2014-04-05 summary:Given iid samples from some unknown continuous density on hyper-rectangle $[0, 1]^d$, we attempt to learn a piecewise constant function that approximates this underlying density nonparametrically. Our density estimate is defined on a binary split of $[0, 1]^d$ and built up sequentially according to discrepancy criteria; the key ingredient is to control the discrepancy adaptively in each sub-rectangle to achieve overall bound. We prove that the estimate, even though simple as it appears, preserves most of the estimation power. By exploiting its structure, it can be directly applied to some important pattern recognition tasks such as mode seeking and density landscape exploration, we demonstrate its applicability through simulations and examples. version:3
arxiv-1303-2132 | Heuristic Ternary Error-Correcting Output Codes Via Weight Optimization and Layered Clustering-Based Approach | http://arxiv.org/abs/1303.2132 | id:1303.2132 author:Xiao-Lei Zhang category:cs.LG  published:2013-03-08 summary:One important classifier ensemble for multiclass classification problems is Error-Correcting Output Codes (ECOCs). It bridges multiclass problems and binary-class classifiers by decomposing multiclass problems to a serial binary-class problems. In this paper, we present a heuristic ternary code, named Weight Optimization and Layered Clustering-based ECOC (WOLC-ECOC). It starts with an arbitrary valid ECOC and iterates the following two steps until the training risk converges. The first step, named Layered Clustering based ECOC (LC-ECOC), constructs multiple strong classifiers on the most confusing binary-class problem. The second step adds the new classifiers to ECOC by a novel Optimized Weighted (OW) decoding algorithm, where the optimization problem of the decoding is solved by the cutting plane algorithm. Technically, LC-ECOC makes the heuristic training process not blocked by some difficult binary-class problem. OW decoding guarantees the non-increase of the training risk for ensuring a small code length. Results on 14 UCI datasets and a music genre classification problem demonstrate the effectiveness of WOLC-ECOC. version:2
arxiv-1404-5588 | Large Margin Image Set Representation and Classification | http://arxiv.org/abs/1404.5588 | id:1404.5588 author:Jim Jing-Yan Wang, Majed Alzahrani, Xin Gao category:cs.CV  published:2014-04-22 summary:In this paper, we propose a novel image set representation and classification method by maximizing the margin of image sets. The margin of an image set is defined as the difference of the distance to its nearest image set from different classes and the distance to its nearest image set of the same class. By modeling the image sets by using both their image samples and their affine hull models, and maximizing the margins of the images sets, the image set representation parameter learning problem is formulated as an minimization problem, which is further optimized by an expectation -maximization (EM) strategy with accelerated proximal gradient (APG) optimization in an iterative algorithm. To classify a given test image set, we assign it to the class which could provide the largest margin. Experiments on two applications of video-sequence-based face recognition demonstrate that the proposed method significantly outperforms state-of-the-art image set classification methods in terms of both effectiveness and efficiency. version:1
arxiv-1404-5585 | A Structural Query System for Han Characters | http://arxiv.org/abs/1404.5585 | id:1404.5585 author:Matthew Skala category:cs.CL cs.DB H.3.1  published:2014-04-22 summary:The IDSgrep structural query system for Han character dictionaries is presented. This system includes a data model and syntax for describing the spatial structure of Han characters using Extended Ideographic Description Sequences (EIDSes) based on the Unicode IDS syntax; a language for querying EIDS databases, designed to suit the needs of font developers and foreign language learners; a bit vector index inspired by Bloom filters for faster query operations; a freely available implementation; and format translation from popular third-party IDS and XML character databases. Experimental results are included, with a comparison to other software used for similar applications. version:1
arxiv-1301-3529 | Discrete Restricted Boltzmann Machines | http://arxiv.org/abs/1301.3529 | id:1301.3529 author:Guido Montufar, Jason Morton category:stat.ML math.AG math.PR G.3  published:2013-01-15 summary:We describe discrete restricted Boltzmann machines: probabilistic graphical models with bipartite interactions between visible and hidden discrete variables. Examples are binary restricted Boltzmann machines and discrete naive Bayes models. We detail the inference functions and distributed representations arising in these models in terms of configurations of projected products of simplices and normal fans of products of simplices. We bound the number of hidden variables, depending on the cardinalities of their state spaces, for which these models can approximate any probability distribution on their visible states to any given accuracy. In addition, we use algebraic methods and coding theory to compute their dimension. version:4
arxiv-1209-2082 | Blind Image Deblurring by Spectral Properties of Convolution Operators | http://arxiv.org/abs/1209.2082 | id:1209.2082 author:Guangcan Liu, Shiyu Chang, Yi Ma category:cs.CV  published:2012-09-10 summary:In this paper, we study the problem of recovering a sharp version of a given blurry image when the blur kernel is unknown. Previous methods often introduce an image-independent regularizer (such as Gaussian or sparse priors) on the desired blur kernel. We shall show that the blurry image itself encodes rich information about the blur kernel. Such information can be found through analyzing and comparing how the spectrum of an image as a convolution operator changes before and after blurring. Our analysis leads to an effective convex regularizer on the blur kernel which depends only on the given blurry image. We show that the minimizer of this regularizer guarantees to give good approximation to the blur kernel if the original image is sharp enough. By combining this powerful regularizer with conventional image deblurring techniques, we show how we could significantly improve the deblurring results through simulations and experiments on real images. In addition, our analysis and experiments help explaining a widely accepted doctrine; that is, the edges are good features for deblurring. version:3
arxiv-1307-2965 | Semantic Context Forests for Learning-Based Knee Cartilage Segmentation in 3D MR Images | http://arxiv.org/abs/1307.2965 | id:1307.2965 author:Quan Wang, Dijia Wu, Le Lu, Meizhu Liu, Kim L. Boyer, Shaohua Kevin Zhou category:cs.CV  published:2013-07-11 summary:The automatic segmentation of human knee cartilage from 3D MR images is a useful yet challenging task due to the thin sheet structure of the cartilage with diffuse boundaries and inhomogeneous intensities. In this paper, we present an iterative multi-class learning method to segment the femoral, tibial and patellar cartilage simultaneously, which effectively exploits the spatial contextual constraints between bone and cartilage, and also between different cartilages. First, based on the fact that the cartilage grows in only certain area of the corresponding bone surface, we extract the distance features of not only to the surface of the bone, but more informatively, to the densely registered anatomical landmarks on the bone surface. Second, we introduce a set of iterative discriminative classifiers that at each iteration, probability comparison features are constructed from the class confidence maps derived by previously learned classifiers. These features automatically embed the semantic context information between different cartilages of interest. Validated on a total of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, the proposed approach demonstrates high robustness and accuracy of segmentation in comparison with existing state-of-the-art MR cartilage segmentation methods. version:2
arxiv-1404-5521 | Together we stand, Together we fall, Together we win: Dynamic Team Formation in Massive Open Online Courses | http://arxiv.org/abs/1404.5521 | id:1404.5521 author:Tanmay Sinha category:cs.SI cs.CY cs.LG cs.MA  published:2014-04-22 summary:Massive Open Online Courses (MOOCs) offer a new scalable paradigm for e-learning by providing students with global exposure and opportunities for connecting and interacting with millions of people all around the world. Very often, students work as teams to effectively accomplish course related tasks. However, due to lack of face to face interaction, it becomes difficult for MOOC students to collaborate. Additionally, the instructor also faces challenges in manually organizing students into teams because students flock to these MOOCs in huge numbers. Thus, the proposed research is aimed at developing a robust methodology for dynamic team formation in MOOCs, the theoretical framework for which is grounded at the confluence of organizational team theory, social network analysis and machine learning. A prerequisite for such an undertaking is that we understand the fact that, each and every informal tie established among students offers the opportunities to influence and be influenced. Therefore, we aim to extract value from the inherent connectedness of students in the MOOC. These connections carry with them radical implications for the way students understand each other in the networked learning community. Our approach will enable course instructors to automatically group students in teams that have fairly balanced social connections with their peers, well defined in terms of appropriately selected qualitative and quantitative network metrics. version:1
arxiv-1404-5443 | Approximate Inference for Nonstationary Heteroscedastic Gaussian process Regression | http://arxiv.org/abs/1404.5443 | id:1404.5443 author:Ville Tolvanen, Pasi Jylänki, Aki Vehtari category:stat.ML  published:2014-04-22 summary:This paper presents a novel approach for approximate integration over the uncertainty of noise and signal variances in Gaussian process (GP) regression. Our efficient and straightforward approach can also be applied to integration over input dependent noise variance (heteroscedasticity) and input dependent signal variance (nonstationarity) by setting independent GP priors for the noise and signal variances. We use expectation propagation (EP) for inference and compare results to Markov chain Monte Carlo in two simulated data sets and three empirical examples. The results show that EP produces comparable results with less computational burden. version:1
arxiv-1303-3154 | Mixed Strategy May Outperform Pure Strategy: An Initial Study | http://arxiv.org/abs/1303.3154 | id:1303.3154 author:Jun He, Wei Hou, Hongbin Dong, Feidun He category:cs.NE cs.GT  published:2013-03-13 summary:In pure strategy meta-heuristics, only one search strategy is applied for all time. In mixed strategy meta-heuristics, each time one search strategy is chosen from a strategy pool with a probability and then is applied. An example is classical genetic algorithms, where either a mutation or crossover operator is chosen with a probability each time. The aim of this paper is to compare the performance between mixed strategy and pure strategy meta-heuristic algorithms. First an experimental study is implemented and results demonstrate that mixed strategy evolutionary algorithms may outperform pure strategy evolutionary algorithms on the 0-1 knapsack problem in up to 77.8% instances. Then Complementary Strategy Theorem is rigorously proven for applying mixed strategy at the population level. The theorem asserts that given two meta-heuristic algorithms where one uses pure strategy 1 and another uses pure strategy 2, the condition of pure strategy 2 being complementary to pure strategy 1 is sufficient and necessary if there exists a mixed strategy meta-heuristics derived from these two pure strategies and its expected number of generations to find an optimal solution is no more than that of using pure strategy 1 for any initial population, and less than that of using pure strategy 1 for some initial population. version:3
arxiv-1202-1708 | A Polynomial Time Approximation Scheme for a Single Machine Scheduling Problem Using a Hybrid Evolutionary Algorithm | http://arxiv.org/abs/1202.1708 | id:1202.1708 author:Boris Mitavskiy, Jun He category:cs.NE  published:2012-02-08 summary:Nowadays hybrid evolutionary algorithms, i.e, heuristic search algorithms combining several mutation operators some of which are meant to implement stochastically a well known technique designed for the specific problem in question while some others playing the role of random search, have become rather popular for tackling various NP-hard optimization problems. While empirical studies demonstrate that hybrid evolutionary algorithms are frequently successful at finding solutions having fitness sufficiently close to the optimal, many fewer articles address the computational complexity in a mathematically rigorous fashion. This paper is devoted to a mathematically motivated design and analysis of a parameterized family of evolutionary algorithms which provides a polynomial time approximation scheme for one of the well-known NP-hard combinatorial optimization problems, namely the "single machine scheduling problem without precedence constraints". The authors hope that the techniques and ideas developed in this article may be applied in many other situations. version:2
arxiv-1404-5421 | Concurrent bandits and cognitive radio networks | http://arxiv.org/abs/1404.5421 | id:1404.5421 author:Orly Avner, Shie Mannor category:cs.LG cs.MA  published:2014-04-22 summary:We consider the problem of multiple users targeting the arms of a single multi-armed stochastic bandit. The motivation for this problem comes from cognitive radio networks, where selfish users need to coexist without any side communication between them, implicit cooperation or common control. Even the number of users may be unknown and can vary as users join or leave the network. We propose an algorithm that combines an $\epsilon$-greedy learning rule with a collision avoidance mechanism. We analyze its regret with respect to the system-wide optimum and show that sub-linear regret can be obtained in this setting. Experiments show dramatic improvement compared to other algorithms for this setting. version:1
arxiv-1404-5417 | Attractor Metadynamics in Adapting Neural Networks | http://arxiv.org/abs/1404.5417 | id:1404.5417 author:Claudius Gros, Mathias Linkerhand, Valentin Walther category:q-bio.NC cond-mat.dis-nn cs.NE  published:2014-04-22 summary:Slow adaption processes, like synaptic and intrinsic plasticity, abound in the brain and shape the landscape for the neural dynamics occurring on substantially faster timescales. At any given time the network is characterized by a set of internal parameters, which are adapting continuously, albeit slowly. This set of parameters defines the number and the location of the respective adiabatic attractors. The slow evolution of network parameters hence induces an evolving attractor landscape, a process which we term attractor metadynamics. We study the nature of the metadynamics of the attractor landscape for several continuous-time autonomous model networks. We find both first- and second-order changes in the location of adiabatic attractors and argue that the study of the continuously evolving attractor landscape constitutes a powerful tool for understanding the overall development of the neural dynamics. version:1
arxiv-1404-5165 | GP-Localize: Persistent Mobile Robot Localization using Online Sparse Gaussian Process Observation Model | http://arxiv.org/abs/1404.5165 | id:1404.5165 author:Nuo Xu, Kian Hsiang Low, Jie Chen, Keng Kiat Lim, Etkin Baris Ozgul category:cs.RO cs.LG stat.ML  published:2014-04-21 summary:Central to robot exploration and mapping is the task of persistent localization in environmental fields characterized by spatially correlated measurements. This paper presents a Gaussian process localization (GP-Localize) algorithm that, in contrast to existing works, can exploit the spatially correlated field measurements taken during a robot's exploration (instead of relying on prior training data) for efficiently and scalably learning the GP observation model online through our proposed novel online sparse GP. As a result, GP-Localize is capable of achieving constant time and memory (i.e., independent of the size of the data) per filtering step, which demonstrates the practical feasibility of using GPs for persistent robot localization and autonomy. Empirical evaluation via simulated experiments with real-world datasets and a real robot experiment shows that GP-Localize outperforms existing GP localization algorithms. version:2
arxiv-1404-5899 | A Comparison of Clustering and Missing Data Methods for Health Sciences | http://arxiv.org/abs/1404.5899 | id:1404.5899 author:Ran Zhao, Deanna Needell, Christopher Johansen, Jerry L. Grenard category:math.NA cs.LG 62H30  91C20  94A08  published:2014-04-22 summary:In this paper, we compare and analyze clustering methods with missing data in health behavior research. In particular, we propose and analyze the use of compressive sensing's matrix completion along with spectral clustering to cluster health related data. The empirical tests and real data results show that these methods can outperform standard methods like LPA and FIML, in terms of lower misclassification rates in clustering and better matrix completion performance in missing data problems. According to our examination, a possible explanation of these improvements is that spectral clustering takes advantage of high data dimension and compressive sensing methods utilize the near-to-low-rank property of health data. version:1
arxiv-1404-5372 | Linking Geographic Vocabularies through WordNet | http://arxiv.org/abs/1404.5372 | id:1404.5372 author:Andrea Ballatore, Michela Bertolotto, David C. Wilson category:cs.IR cs.CL  published:2014-04-22 summary:The linked open data (LOD) paradigm has emerged as a promising approach to structuring and sharing geospatial information. One of the major obstacles to this vision lies in the difficulties found in the automatic integration between heterogeneous vocabularies and ontologies that provides the semantic backbone of the growing constellation of open geo-knowledge bases. In this article, we show how to utilize WordNet as a semantic hub to increase the integration of LOD. With this purpose in mind, we devise Voc2WordNet, an unsupervised mapping technique between a given vocabulary and WordNet, combining intensional and extensional aspects of the geographic terms. Voc2WordNet is evaluated against a sample of human-generated alignments with the OpenStreetMap (OSM) Semantic Network, a crowdsourced geospatial resource, and the GeoNames ontology, the vocabulary of a large digital gazetteer. These empirical results indicate that the approach can obtain high precision and recall. version:1
arxiv-1404-2728 | Real-time Decolorization using Dominant Colors | http://arxiv.org/abs/1404.2728 | id:1404.2728 author:Wei Hu, Wei Li, Fan Zhang, Qian Du category:cs.GR cs.CV  published:2014-04-10 summary:Decolorization is the process to convert a color image or video to its grayscale version, and it has received great attention in recent years. An ideal decolorization algorithm should preserve the original color contrast as much as possible. Meanwhile, it should provide the final decolorized result as fast as possible. However, most of the current methods are suffering from either unsatisfied color information preservation or high computational cost, limiting their application value. In this paper, a simple but effective technique is proposed for real-time decolorization. Based on the typical rgb2gray() color conversion model, which produces a grayscale image by linearly combining R, G, and B channels, we propose a dominant color hypothesis and a corresponding distance measurement metric to evaluate the quality of grayscale conversion. The local optimum scheme provides several "good" candidates in a confidence interval, from which the "best" result can be extracted. Experimental results demonstrate that remarkable simplicity of the proposed method facilitates the process of high resolution images and videos in real-time using a common CPU. version:2
arxiv-1404-5367 | Lexicon Infused Phrase Embeddings for Named Entity Resolution | http://arxiv.org/abs/1404.5367 | id:1404.5367 author:Alexandre Passos, Vineet Kumar, Andrew McCallum category:cs.CL  published:2014-04-22 summary:Most state-of-the-art approaches for named-entity recognition (NER) use semi supervised information in the form of word clusters and lexicons. Recently neural network-based language models have been explored, as they as a byproduct generate highly informative vector representations for words, known as word embeddings. In this paper we present two contributions: a new form of learning word embeddings that can leverage information from relevant lexicons to improve the representations, and the first system to use neural word embeddings to achieve state-of-the-art results on named-entity recognition in both CoNLL and Ontonotes NER. Our system achieves an F1 score of 90.90 on the test set for CoNLL 2003---significantly better than any previous system trained on public data, and matching a system employing massive private industrial query-log data. version:1
arxiv-1404-5357 | Morphological Analysis of the Bishnupriya Manipuri Language using Finite State Transducers | http://arxiv.org/abs/1404.5357 | id:1404.5357 author:Nayan Jyoti Kalita, Navanath Saharia, Smriti Kumar Sinha category:cs.CL  published:2014-04-22 summary:In this work we present a morphological analysis of Bishnupriya Manipuri language, an Indo-Aryan language spoken in the north eastern India. As of now, there is no computational work available for the language. Finite state morphology is one of the successful approaches applied in a wide variety of languages over the year. Therefore we adapted the finite state approach to analyse morphology of the Bishnupriya Manipuri language. version:1
arxiv-1404-5351 | Fast Approximate Matching of Cell-Phone Videos for Robust Background Subtraction | http://arxiv.org/abs/1404.5351 | id:1404.5351 author:Raffay Hamid, Atish Das Sarma, Dennis DeCoste, Neel Sundaresan category:cs.CV  published:2014-04-22 summary:We identify a novel instance of the background subtraction problem that focuses on extracting near-field foreground objects captured using handheld cameras. Given two user-generated videos of a scene, one with and the other without the foreground object(s), our goal is to efficiently generate an output video with only the foreground object(s) present in it. We cast this challenge as a spatio-temporal frame matching problem, and propose an efficient solution for it that exploits the temporal smoothness of the video sequences. We present theoretical analyses for the error bounds of our approach, and validate our findings using a detailed set of simulation experiments. Finally, we present the results of our approach tested on multiple real videos captured using handheld cameras, and compare them to several alternate foreground extraction approaches. version:1
arxiv-1404-5278 | The Frobenius anatomy of word meanings I: subject and object relative pronouns | http://arxiv.org/abs/1404.5278 | id:1404.5278 author:Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke category:cs.CL  published:2014-04-21 summary:This paper develops a compositional vector-based semantics of subject and object relative pronouns within a categorical framework. Frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the modified noun phrase, as well as copying, combining, and discarding parts of the relative clause. We develop two instantiations of the abstract semantics, one based on a truth-theoretic approach and one based on corpus statistics. version:1
arxiv-1404-5214 | Graph Kernels via Functional Embedding | http://arxiv.org/abs/1404.5214 | id:1404.5214 author:Anshumali Shrivastava, Ping Li category:cs.LG cs.AI stat.ML  published:2014-04-21 summary:We propose a representation of graph as a functional object derived from the power iteration of the underlying adjacency matrix. The proposed functional representation is a graph invariant, i.e., the functional remains unchanged under any reordering of the vertices. This property eliminates the difficulty of handling exponentially many isomorphic forms. Bhattacharyya kernel constructed between these functionals significantly outperforms the state-of-the-art graph kernels on 3 out of the 4 standard benchmark graph classification datasets, demonstrating the superiority of our approach. The proposed methodology is simple and runs in time linear in the number of edges, which makes our kernel more efficient and scalable compared to many widely adopted graph kernels with running time cubic in the number of vertices. version:1
arxiv-1312-2244 | Time-dependent Hierarchical Dirichlet Model for Timeline Generation | http://arxiv.org/abs/1312.2244 | id:1312.2244 author:Tao Wang category:cs.CL cs.IR  published:2013-12-08 summary:Timeline Generation aims at summarizing news from different epochs and telling readers how an event evolves. It is a new challenge that combines salience ranking with novelty detection. For long-term public events, the main topic usually includes various aspects across different epochs and each aspect has its own evolving pattern. Existing approaches neglect such hierarchical topic structure involved in the news corpus in timeline generation. In this paper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) for timeline generation. Our model can aptly detect different levels of topic information across corpus and such structure is further used for sentence selection. Based on the topic mined fro HDM, sentences are selected by considering different aspects such as relevance, coherence and coverage. We develop experimental systems to evaluate 8 long-term events that public concern. Performance comparison between different systems demonstrates the effectiveness of our model in terms of ROUGE metrics. version:2
arxiv-1312-4637 | Constraint Reduction using Marginal Polytope Diagrams for MAP LP Relaxations | http://arxiv.org/abs/1312.4637 | id:1312.4637 author:Zhen Zhang, Qinfeng Shi, Yanning Zhang, Chunhua Shen, Anton van den Hengel category:cs.CV cs.AI  published:2013-12-17 summary:LP relaxation-based message passing algorithms provide an effective tool for MAP inference over Probabilistic Graphical Models. However, different LP relaxations often have different objective functions and variables of differing dimensions, which presents a barrier to effective comparison and analysis. In addition, the computational complexity of LP relaxation-based methods grows quickly with the number of constraints. Reducing the number of constraints without sacrificing the quality of the solutions is thus desirable. We propose a unified formulation under which existing MAP LP relaxations may be compared and analysed. Furthermore, we propose a new tool called Marginal Polytope Diagrams. Some properties of Marginal Polytope Diagrams are exploited such as node redundancy and edge equivalence. We show that using Marginal Polytope Diagrams allows the number of constraints to be reduced without loosening the LP relaxations. Then, using Marginal Polytope Diagrams and constraint reduction, we develop three novel message passing algorithms, and demonstrate that two of these show a significant improvement in speed over state-of-art algorithms while delivering a competitive, and sometimes higher, quality of solution. version:2
arxiv-1404-5144 | Influence of the learning method in the performance of feedforward neural networks when the activity of neurons is modified | http://arxiv.org/abs/1404.5144 | id:1404.5144 author:M. Konomi, G. M. Sacha category:cs.NE  published:2014-04-21 summary:A method that allows us to give a different treatment to any neuron inside feedforward neural networks is presented. The algorithm has been implemented with two very different learning methods: a standard Back-propagation (BP) procedure and an evolutionary algorithm. First, we have demonstrated that the EA training method converges faster and gives more accurate results than BP. Then we have made a full analysis of the effects of turning off different combinations of neurons after the training phase. We demonstrate that EA is much more robust than BP for all the cases under study. Even in the case when two hidden neurons are lost, EA training is still able to give good average results. This difference implies that we must be very careful when pruning or redundancy effects are being studied since the network performance when losing neurons strongly depends on the training method. Moreover, the influence of the individual inputs will also depend on the training algorithm. Since EA keeps a good classification performance when units are lost, this method could be a good way to simulate biological learning systems since they must be robust against deficient neuron performance. Although biological systems are much more complex than the simulations shown in this article, we propose that a smart training strategy such as the one shown here could be considered as a first protection against the losing of a certain number of neurons. version:1
arxiv-1404-5520 | A Computationally Efficient Limited Memory CMA-ES for Large Scale Optimization | http://arxiv.org/abs/1404.5520 | id:1404.5520 author:Ilya Loshchilov category:cs.NE  published:2014-04-21 summary:We propose a computationally efficient limited memory Covariance Matrix Adaptation Evolution Strategy for large scale optimization, which we call the LM-CMA-ES. The LM-CMA-ES is a stochastic, derivative-free algorithm for numerical optimization of non-linear, non-convex optimization problems in continuous domain. Inspired by the limited memory BFGS method of Liu and Nocedal (1989), the LM-CMA-ES samples candidate solutions according to a covariance matrix reproduced from $m$ direction vectors selected during the optimization process. The decomposition of the covariance matrix into Cholesky factors allows to reduce the time and memory complexity of the sampling to $O(mn)$, where $n$ is the number of decision variables. When $n$ is large (e.g., $n$ > 1000), even relatively small values of $m$ (e.g., $m=20,30$) are sufficient to efficiently solve fully non-separable problems and to reduce the overall run-time. version:1
arxiv-1311-3995 | Compressed Sensing for Energy-Efficient Wireless Telemonitoring: Challenges and Opportunities | http://arxiv.org/abs/1311.3995 | id:1311.3995 author:Zhilin Zhang, Bhaskar D. Rao, Tzyy-Ping Jung category:cs.IT math.IT stat.ML  published:2013-11-15 summary:As a lossy compression framework, compressed sensing has drawn much attention in wireless telemonitoring of biosignals due to its ability to reduce energy consumption and make possible the design of low-power devices. However, the non-sparseness of biosignals presents a major challenge to compressed sensing. This study proposes and evaluates a spatio-temporal sparse Bayesian learning algorithm, which has the desired ability to recover such non-sparse biosignals. It exploits both temporal correlation in each individual biosignal and inter-channel correlation among biosignals from different channels. The proposed algorithm was used for compressed sensing of multichannel electroencephalographic (EEG) signals for estimating vehicle drivers' drowsiness. Results showed that the drowsiness estimation was almost unaffected even if raw EEG signals (containing various artifacts) were compressed by 90%. version:2
arxiv-1401-6623 | Near-Ideal Behavior of Compressed Sensing Algorithms | http://arxiv.org/abs/1401.6623 | id:1401.6623 author:Mehmet Eren Ahsen, Mathukumalli Vidyasagar category:stat.ML 62J07  published:2014-01-26 summary:In a recent paper, it is shown that the LASSO algorithm exhibits "near-ideal behavior," in the following sense: Suppose $y = Az + \eta$ where $A$ satisfies the restricted isometry property (RIP) with a sufficiently small constant, and $\Vert \eta \Vert_2 \leq \epsilon$. Then minimizing $\Vert z \Vert_1$ subject to $\Vert y - Az \Vert_2 \leq \epsilon$ leads to an estimate $\hat{x}$ whose error $\Vert \hat{x} - x \Vert_2$ is bounded by a universal constant times the error achieved by an "oracle" that knows the location of the nonzero components of $x$. In the world of optimization, the LASSO algorithm has been generalized in several directions such as the group LASSO, the sparse group LASSO, either without or with tree-structured overlapping groups, and most recently, the sorted LASSO. In this paper, it is shown that {\it any algorithm\/} exhibits near-ideal behavior in the above sense, provided only that (i) the norm used to define the sparsity index is "decomposable," (ii) the penalty norm that is minimized in an effort to enforce sparsity is "$\gamma$-decomposable," and (iii) a "compressibility condition" in terms of a group restricted isometry property is satisfied. Specifically, the group LASSO, and the sparse group LASSO (with some permissible overlap in the groups), as well as the sorted $\ell_1$-norm minimization all exhibit near-ideal behavior. Explicit bounds on the residual error are derived that contain previously known results as special cases. version:3
arxiv-1404-5065 | Multi-Target Regression via Random Linear Target Combinations | http://arxiv.org/abs/1404.5065 | id:1404.5065 author:Grigorios Tsoumakas, Eleftherios Spyromitros-Xioufis, Aikaterini Vrekou, Ioannis Vlahavas category:cs.LG  published:2014-04-20 summary:Multi-target regression is concerned with the simultaneous prediction of multiple continuous target variables based on the same set of input variables. It arises in several interesting industrial and environmental application domains, such as ecological modelling and energy forecasting. This paper presents an ensemble method for multi-target regression that constructs new target variables via random linear combinations of existing targets. We discuss the connection of our approach with multi-label classification algorithms, in particular RA$k$EL, which originally inspired this work, and a family of recent multi-label classification algorithms that involve output coding. Experimental results on 12 multi-target datasets show that it performs significantly better than a strong baseline that learns a single model for each target using gradient boosting and compares favourably to multi-objective random forest approach, which is a state-of-the-art approach. The experiments further show that our approach improves more when stronger unconditional dependencies exist among the targets. version:1
arxiv-1109-0069 | Inter-rater Agreement on Sentence Formality | http://arxiv.org/abs/1109.0069 | id:1109.0069 author:Shibamouli Lahiri, Xiaofei Lu category:cs.CL H.3.1; I.2.7  published:2011-09-01 summary:Formality is one of the most important dimensions of writing style variation. In this study we conducted an inter-rater reliability experiment for assessing sentence formality on a five-point Likert scale, and obtained good agreement results as well as different rating distributions for different sentence categories. We also performed a difficulty analysis to identify the bottlenecks of our rating procedure. Our main objective is to design an automatic scoring mechanism for sentence-level formality, and this study is important for that purpose. version:2
arxiv-1403-8067 | Robust Subspace Recovery via Bi-Sparsity Pursuit | http://arxiv.org/abs/1403.8067 | id:1403.8067 author:Xiao Bian, Hamid Krim category:cs.CV  published:2014-03-31 summary:Successful applications of sparse models in computer vision and machine learning imply that in many real-world applications, high dimensional data is distributed in a union of low dimensional subspaces. Nevertheless, the underlying structure may be affected by sparse errors and/or outliers. In this paper, we propose a bi-sparse model as a framework to analyze this problem and provide a novel algorithm to recover the union of subspaces in presence of sparse corruptions. We further show the effectiveness of our method by experiments on both synthetic data and real-world vision data. version:2
arxiv-1404-5028 | Clustering via Mode Seeking by Direct Estimation of the Gradient of a Log-Density | http://arxiv.org/abs/1404.5028 | id:1404.5028 author:Hiroaki Sasaki, Aapo Hyvärinen, Masashi Sugiyama category:stat.ML  published:2014-04-20 summary:Mean shift clustering finds the modes of the data probability density by identifying the zero points of the density gradient. Since it does not require to fix the number of clusters in advance, the mean shift has been a popular clustering algorithm in various application fields. A typical implementation of the mean shift is to first estimate the density by kernel density estimation and then compute its gradient. However, since good density estimation does not necessarily imply accurate estimation of the density gradient, such an indirect two-step approach is not reliable. In this paper, we propose a method to directly estimate the gradient of the log-density without going through density estimation. The proposed method gives the global solution analytically and thus is computationally efficient. We then develop a mean-shift-like fixed-point algorithm to find the modes of the density for clustering. As in the mean shift, one does not need to set the number of clusters in advance. We empirically show that the proposed clustering method works much better than the mean shift especially for high-dimensional data. Experimental results further indicate that the proposed method outperforms existing clustering methods. version:1
arxiv-1006-3787 | Complete Complementary Results Report of the MARF's NLP Approach to the DEFT 2010 Competition | http://arxiv.org/abs/1006.3787 | id:1006.3787 author:Serguei A. Mokhov category:cs.CL 68T50  68T10  68T37 I.2.7; I.5  published:2010-06-18 summary:This companion paper complements the main DEFT'10 article describing the MARF approach (arXiv:0905.1235) to the DEFT'10 NLP challenge (described at http://www.groupes.polymtl.ca/taln2010/deft.php in French). This paper is aimed to present the complete result sets of all the conducted experiments and their settings in the resulting tables highlighting the approach and the best results, but also showing the worse and the worst and their subsequent analysis. This particular work focuses on application of the MARF's classical and NLP pipelines to identification tasks within various francophone corpora to identify decades when certain articles were published for the first track (Piste 1) and place of origin of a publication (Piste 2), such as the journal and location (France vs. Quebec). This is the sixth iteration of the release of the results. version:7
arxiv-1312-6034 | Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps | http://arxiv.org/abs/1312.6034 | id:1312.6034 author:Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV  published:2013-12-20 summary:This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]. version:2
arxiv-1404-4942 | Geometric Abstraction from Noisy Image-Based 3D Reconstructions | http://arxiv.org/abs/1404.4942 | id:1404.4942 author:Thomas Holzmann, Christof Hoppe, Stefan Kluckner, Horst Bischof category:cs.CV  published:2014-04-19 summary:Creating geometric abstracted models from image-based scene reconstructions is difficult due to noise and irregularities in the reconstructed model. In this paper, we present a geometric modeling method for noisy reconstructions dominated by planar horizontal and orthogonal vertical structures. We partition the scene into horizontal slices and create an inside/outside labeling represented by a floor plan for each slice by solving an energy minimization problem. Consecutively, we create an irregular discretization of the volume according to the individual floor plans and again label each cell as inside/outside by minimizing an energy function. By adjusting the smoothness parameter, we introduce different levels of detail. In our experiments, we show results with varying regularization levels using synthetically generated and real-world data. version:1
arxiv-1404-4935 | Opinion Mining In Hindi Language: A Survey | http://arxiv.org/abs/1404.4935 | id:1404.4935 author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.IR cs.CL  published:2014-04-19 summary:Opinions are very important in the life of human beings. These Opinions helped the humans to carry out the decisions. As the impact of the Web is increasing day by day, Web documents can be seen as a new source of opinion for human beings. Web contains a huge amount of information generated by the users through blogs, forum entries, and social networking websites and so on To analyze this large amount of information it is required to develop a method that automatically classifies the information available on the Web. This domain is called Sentiment Analysis and Opinion Mining. Opinion Mining or Sentiment Analysis is a natural language processing task that mine information from various text forms such as reviews, news, and blogs and classify them on the basis of their polarity as positive, negative or neutral. But, from the last few years, enormous increase has been seen in Hindi language on the Web. Research in opinion mining mostly carried out in English language but it is very important to perform the opinion mining in Hindi language also as large amount of information in Hindi is also available on the Web. This paper gives an overview of the work that has been done Hindi language. version:1
arxiv-1404-4893 | CTBNCToolkit: Continuous Time Bayesian Network Classifier Toolkit | http://arxiv.org/abs/1404.4893 | id:1404.4893 author:Daniele Codecasa, Fabio Stella category:cs.AI cs.LG cs.MS  published:2014-04-18 summary:Continuous time Bayesian network classifiers are designed for temporal classification of multivariate streaming data when time duration of events matters and the class does not change over time. This paper introduces the CTBNCToolkit: an open source Java toolkit which provides a stand-alone application for temporal classification and a library for continuous time Bayesian network classifiers. CTBNCToolkit implements the inference algorithm, the parameter learning algorithm, and the structural learning algorithm for continuous time Bayesian network classifiers. The structural learning algorithm is based on scoring functions: the marginal log-likelihood score and the conditional log-likelihood score are provided. CTBNCToolkit provides also an implementation of the expectation maximization algorithm for clustering purpose. The paper introduces continuous time Bayesian network classifiers. How to use the CTBNToolkit from the command line is described in a specific section. Tutorial examples are included to facilitate users to understand how the toolkit must be used. A section dedicate to the Java library is proposed to help further code extensions. version:1
arxiv-1312-6661 | Rapid and deterministic estimation of probability densities using scale-free field theories | http://arxiv.org/abs/1312.6661 | id:1312.6661 author:Justin B. Kinney category:physics.data-an cs.LG math.ST q-bio.QM stat.ML stat.TH  published:2013-12-23 summary:The question of how best to estimate a continuous probability density from finite data is an intriguing open problem at the interface of statistics and physics. Previous work has argued that this problem can be addressed in a natural way using methods from statistical field theory. Here I describe new results that allow this field-theoretic approach to be rapidly and deterministically computed in low dimensions, making it practical for use in day-to-day data analysis. Importantly, this approach does not impose a privileged length scale for smoothness of the inferred probability density, but rather learns a natural length scale from the data due to the tradeoff between goodness-of-fit and an Occam factor. Open source software implementing this method in one and two dimensions is provided. version:3
arxiv-1404-5511 | Coactive Learning for Locally Optimal Problem Solving | http://arxiv.org/abs/1404.5511 | id:1404.5511 author:Robby Goetschalckx, Alan Fern, Prasad Tadepalli category:cs.LG  published:2014-04-18 summary:Coactive learning is an online problem solving setting where the solutions provided by a solver are interactively improved by a domain expert, which in turn drives learning. In this paper we extend the study of coactive learning to problems where obtaining a globally optimal or near-optimal solution may be intractable or where an expert can only be expected to make small, local improvements to a candidate solution. The goal of learning in this new setting is to minimize the cost as measured by the expert effort over time. We first establish theoretical bounds on the average cost of the existing coactive Perceptron algorithm. In addition, we consider new online algorithms that use cost-sensitive and Passive-Aggressive (PA) updates, showing similar or improved theoretical bounds. We provide an empirical evaluation of the learners in various domains, which show that the Perceptron based algorithms are quite effective and that unlike the case for online classification, the PA algorithms do not yield significant performance gains. version:1
arxiv-1404-4880 | Bias Correction and Modified Profile Likelihood under the Wishart Complex Distribution | http://arxiv.org/abs/1404.4880 | id:1404.4880 author:Abraão D. C. Nascimento, Alejandro C. Frery, Renato J. Cintra category:cs.CV stat.ME  published:2014-04-18 summary:This paper proposes improved methods for the maximum likelihood (ML) estimation of the equivalent number of looks $L$. This parameter has a meaningful interpretation in the context of polarimetric synthetic aperture radar (PolSAR) images. Due to the presence of coherent illumination in their processing, PolSAR systems generate images which present a granular noise called speckle. As a potential solution for reducing such interference, the parameter $L$ controls the signal-noise ratio. Thus, the proposal of efficient estimation methodologies for $L$ has been sought. To that end, we consider firstly that a PolSAR image is well described by the scaled complex Wishart distribution. In recent years, Anfinsen et al. derived and analyzed estimation methods based on the ML and on trace statistical moments for obtaining the parameter $L$ of the unscaled version of such probability law. This paper generalizes that approach. We present the second-order bias expression proposed by Cox and Snell for the ML estimator of this parameter. Moreover, the formula of the profile likelihood modified by Barndorff-Nielsen in terms of $L$ is discussed. Such derivations yield two new ML estimators for the parameter $L$, which are compared to the estimators proposed by Anfinsen et al. The performance of these estimators is assessed by means of Monte Carlo experiments, adopting three statistical measures as comparison criterion: the mean square error, the bias, and the coefficient of variation. Equivalently to the simulation study, an application to actual PolSAR data concludes that the proposed estimators outperform all the others in homogeneous scenarios. version:1
arxiv-1404-4805 | iPiano: Inertial Proximal Algorithm for Non-Convex Optimization | http://arxiv.org/abs/1404.4805 | id:1404.4805 author:Peter Ochs, Yunjin Chen, Thomas Brox, Thomas Pock category:cs.CV math.OC  published:2014-04-18 summary:In this paper we study an algorithm for solving a minimization problem composed of a differentiable (possibly non-convex) and a convex (possibly non-differentiable) function. The algorithm iPiano combines forward-backward splitting with an inertial force. It can be seen as a non-smooth split version of the Heavy-ball method from Polyak. A rigorous analysis of the algorithm for the proposed class of problems yields global convergence of the function values and the arguments. This makes the algorithm robust for usage on non-convex problems. The convergence result is obtained based on the \KL inequality. This is a very weak restriction, which was used to prove convergence for several other gradient methods. First, an abstract convergence theorem for a generic algorithm is proved, and, then iPiano is shown to satisfy the requirements of this theorem. Furthermore, a convergence rate is established for the general problem class. We demonstrate iPiano on computer vision problems: image denoising with learned priors and diffusion based image compression. version:1
arxiv-1306-6042 | OptShrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage | http://arxiv.org/abs/1306.6042 | id:1306.6042 author:Raj Rao Nadakuditi category:math.ST cs.IT math.IT stat.ML stat.TH  published:2013-06-25 summary:The truncated singular value decomposition (SVD) of the measurement matrix is the optimal solution to the_representation_ problem of how to best approximate a noisy measurement matrix using a low-rank matrix. Here, we consider the (unobservable)_denoising_ problem of how to best approximate a low-rank signal matrix buried in noise by optimal (re)weighting of the singular vectors of the measurement matrix. We exploit recent results from random matrix theory to exactly characterize the large matrix limit of the optimal weighting coefficients and show that they can be computed directly from data for a large class of noise models that includes the i.i.d. Gaussian noise case. Our analysis brings into sharp focus the shrinkage-and-thresholding form of the optimal weights, the non-convex nature of the associated shrinkage function (on the singular values) and explains why matrix regularization via singular value thresholding with convex penalty functions (such as the nuclear norm) will always be suboptimal. We validate our theoretical predictions with numerical simulations, develop an implementable algorithm (OptShrink) that realizes the predicted performance gains and show how our methods can be used to improve estimation in the setting where the measured matrix has missing entries. version:4
arxiv-1404-4780 | Robust Face Recognition via Adaptive Sparse Representation | http://arxiv.org/abs/1404.4780 | id:1404.4780 author:Jing Wang, Canyi Lu, Meng Wang, Peipei Li, Shuicheng Yan, Xuegang Hu category:cs.CV  published:2014-04-18 summary:Sparse Representation (or coding) based Classification (SRC) has gained great success in face recognition in recent years. However, SRC emphasizes the sparsity too much and overlooks the correlation information which has been demonstrated to be critical in real-world face recognition problems. Besides, some work considers the correlation but overlooks the discriminative ability of sparsity. Different from these existing techniques, in this paper, we propose a framework called Adaptive Sparse Representation based Classification (ASRC) in which sparsity and correlation are jointly considered. Specifically, when the samples are of low correlation, ASRC selects the most discriminative samples for representation, like SRC; when the training samples are highly correlated, ASRC selects most of the correlated and discriminative samples for representation, rather than choosing some related samples randomly. In general, the representation model is adaptive to the correlation structure, which benefits from both $\ell_1$-norm and $\ell_2$-norm. Extensive experiments conducted on publicly available data sets verify the effectiveness and robustness of the proposed algorithm by comparing it with state-of-the-art methods. version:1
arxiv-1404-4740 | Challenges in Persian Electronic Text Analysis | http://arxiv.org/abs/1404.4740 | id:1404.4740 author:Behrang QasemiZadeh, Saeed Rahimi, Mehdi Safaee Ghalati category:cs.CL 68T50 I.2.7  published:2014-04-18 summary:Farsi, also known as Persian, is the official language of Iran and Tajikistan and one of the two main languages spoken in Afghanistan. Farsi enjoys a unified Arabic script as its writing system. In this paper we briefly introduce the writing standards of Farsi and highlight problems one would face when analyzing Farsi electronic texts, especially during development of Farsi corpora regarding to transcription and encoding of Farsi e-texts. The pointes mentioned may sounds easy but they are crucial when developing and processing written corpora of Farsi. version:1
arxiv-1404-4714 | Radical-Enhanced Chinese Character Embedding | http://arxiv.org/abs/1404.4714 | id:1404.4714 author:Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, Xiaolong Wang category:cs.CL  published:2014-04-18 summary:We present a method to leverage radical for learning Chinese character embedding. Radical is a semantic and phonetic component of Chinese character. It plays an important role as characters with the same radical usually have similar semantic meaning and grammatical usage. However, existing Chinese processing algorithms typically regard word or character as the basic unit but ignore the crucial radical information. In this paper, we fill this gap by leveraging radical for learning continuous representation of Chinese character. We develop a dedicated neural architecture to effectively learn character embedding and apply it on Chinese character similarity judgement and Chinese word segmentation. Experiment results show that our radical-enhanced method outperforms existing embedding learning algorithms on both tasks. version:1
arxiv-1404-4702 | Nearly Tight Bounds on $\ell_1$ Approximation of Self-Bounding Functions | http://arxiv.org/abs/1404.4702 | id:1404.4702 author:Vitaly Feldman, Pravesh Kothari, Jan Vondrák category:cs.LG cs.DS  published:2014-04-18 summary:We study the complexity of learning and approximation of self-bounding functions over the uniform distribution on the Boolean hypercube ${0,1}^n$. Informally, a function $f:{0,1}^n \rightarrow \mathbb{R}$ is self-bounding if for every $x \in {0,1}^n$, $f(x)$ upper bounds the sum of all the $n$ marginal decreases in the value of the function at $x$. Self-bounding functions include such well-known classes of functions as submodular and fractionally-subadditive (XOS) functions. They were introduced by Boucheron et al in the context of concentration of measure inequalities. Our main result is a nearly tight $\ell_1$-approximation of self-bounding functions by low-degree juntas. Specifically, all self-bounding functions can be $\epsilon$-approximated in $\ell_1$ by a polynomial of degree $\tilde{O}(1/\epsilon)$ over $2^{\tilde{O}(1/\epsilon)}$ variables. Both the degree and junta-size are optimal up to logarithmic terms. Previously, the best known bound was $O(1/\epsilon^{2})$ on the degree and $2^{O(1/\epsilon^2)}$ on the number of variables (Feldman and Vondr \'{a}k 2013). These results lead to improved and in several cases almost tight bounds for PAC and agnostic learning of submodular, XOS and self-bounding functions. In particular, assuming hardness of learning juntas, we show that PAC and agnostic learning of self-bounding functions have complexity of $n^{\tilde{\Theta}(1/\epsilon)}$. version:1
arxiv-1310-6719 | Two Dimensional Array Imaging with Beam Steered Data | http://arxiv.org/abs/1310.6719 | id:1310.6719 author:Sujeet Patole, Murat Torlak category:cs.CV cs.IT math.IT stat.AP  published:2013-10-24 summary:This paper discusses different approaches used for millimeter wave imaging of two-dimensional objects. Imaging of a two dimensional object requires reflected wave data to be collected across two distinct dimensions. In this paper, we propose a reconstruction method that uses narrowband waveforms along with two dimensional beam steering. The beam is steered in azimuthal and elevation direction, which forms the two distinct dimensions required for the reconstruction. The Reconstruction technique uses inverse Fourier transform along with amplitude and phase correction factors. In addition, this reconstruction technique does not require interpolation of the data in either wavenumber or spatial domain. Use of the two dimensional beam steering offers better performance in the presence of noise compared with the existing methods, such as switched array imaging system. Effects of RF impairments such as quantization of the phase of beam steering weights and timing jitter which add to phase noise, are analyzed. version:2
arxiv-1404-4667 | Subspace Learning and Imputation for Streaming Big Data Matrices and Tensors | http://arxiv.org/abs/1404.4667 | id:1404.4667 author:Morteza Mardani, Gonzalo Mateos, Georgios B. Giannakis category:stat.ML cs.IT cs.LG math.IT  published:2014-04-17 summary:Extracting latent low-dimensional structure from high-dimensional data is of paramount importance in timely inference tasks encountered with `Big Data' analytics. However, increasingly noisy, heterogeneous, and incomplete datasets as well as the need for {\em real-time} processing of streaming data pose major challenges to this end. In this context, the present paper permeates benefits from rank minimization to scalable imputation of missing data, via tracking low-dimensional subspaces and unraveling latent (possibly multi-way) structure from \emph{incomplete streaming} data. For low-rank matrix data, a subspace estimator is proposed based on an exponentially-weighted least-squares criterion regularized with the nuclear norm. After recasting the non-separable nuclear norm into a form amenable to online optimization, real-time algorithms with complementary strengths are developed and their convergence is established under simplifying technical assumptions. In a stationary setting, the asymptotic estimates obtained offer the well-documented performance guarantees of the {\em batch} nuclear-norm regularized estimator. Under the same unifying framework, a novel online (adaptive) algorithm is developed to obtain multi-way decompositions of \emph{low-rank tensors} with missing entries, and perform imputation as a byproduct. Simulated tests with both synthetic as well as real Internet and cardiac magnetic resonance imagery (MRI) data confirm the efficacy of the proposed algorithms, and their superior performance relative to state-of-the-art alternatives. version:1
arxiv-1404-4661 | Learning Fine-grained Image Similarity with Deep Ranking | http://arxiv.org/abs/1404.4661 | id:1404.4661 author:Jiang Wang, Yang song, Thomas Leung, Chuck Rosenberg, Jinbin Wang, James Philbin, Bo Chen, Ying Wu category:cs.CV  published:2014-04-17 summary:Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn similarity metric directly from images.It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sampling algorithm is proposed to learn the model with distributed asynchronized stochastic gradient. Extensive experiments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models. version:1
arxiv-1404-4655 | Hierarchical Quasi-Clustering Methods for Asymmetric Networks | http://arxiv.org/abs/1404.4655 | id:1404.4655 author:Gunnar Carlsson, Facundo Mémoli, Alejandro Ribeiro, Santiago Segarra category:cs.LG stat.ML  published:2014-04-17 summary:This paper introduces hierarchical quasi-clustering methods, a generalization of hierarchical clustering for asymmetric networks where the output structure preserves the asymmetry of the input data. We show that this output structure is equivalent to a finite quasi-ultrametric space and study admissibility with respect to two desirable properties. We prove that a modified version of single linkage is the only admissible quasi-clustering method. Moreover, we show stability of the proposed method and we establish invariance properties fulfilled by it. Algorithms are further developed and the value of quasi-clustering analysis is illustrated with a study of internal migration within United States. version:1
arxiv-1404-4644 | A New Space for Comparing Graphs | http://arxiv.org/abs/1404.4644 | id:1404.4644 author:Anshumali Shrivastava, Ping Li category:stat.ME cs.IR cs.LG stat.ML  published:2014-04-17 summary:Finding a new mathematical representations for graph, which allows direct comparison between different graph structures, is an open-ended research direction. Having such a representation is the first prerequisite for a variety of machine learning algorithms like classification, clustering, etc., over graph datasets. In this paper, we propose a symmetric positive semidefinite matrix with the $(i,j)$-{th} entry equal to the covariance between normalized vectors $A^ie$ and $A^je$ ($e$ being vector of all ones) as a representation for graph with adjacency matrix $A$. We show that the proposed matrix representation encodes the spectrum of the underlying adjacency matrix and it also contains information about the counts of small sub-structures present in the graph such as triangles and small paths. In addition, we show that this matrix is a \emph{"graph invariant"}. All these properties make the proposed matrix a suitable object for representing graphs. The representation, being a covariance matrix in a fixed dimensional metric space, gives a mathematical embedding for graphs. This naturally leads to a measure of similarity on graph objects. We define similarity between two given graphs as a Bhattacharya similarity measure between their corresponding covariance matrix representations. As shown in our experimental study on the task of social network classification, such a similarity measure outperforms other widely used state-of-the-art methodologies. Our proposed method is also computationally efficient. The computation of both the matrix representation and the similarity value can be performed in operations linear in the number of edges. This makes our method scalable in practice. We believe our theoretical and empirical results provide evidence for studying truncated power iterations, of the adjacency matrix, to characterize social networks. version:1
arxiv-1404-4641 | Multilingual Models for Compositional Distributed Semantics | http://arxiv.org/abs/1404.4641 | id:1404.4641 author:Karl Moritz Hermann, Phil Blunsom category:cs.CL  published:2014-04-17 summary:We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data. version:1
arxiv-1404-0789 | The Least Wrong Model Is Not in the Data | http://arxiv.org/abs/1404.0789 | id:1404.0789 author:Oscar Stiffelman category:cs.LG  published:2014-04-03 summary:The true process that generated data cannot be determined when multiple explanations are possible. Prediction requires a model of the probability that a process, chosen randomly from the set of candidate explanations, generates some future observation. The best model includes all of the information contained in the minimal description of the data that is not contained in the data. It is closely related to the Halting Problem and is logarithmic in the size of the data. Prediction is difficult because the ideal model is not computable, and the best computable model is not "findable." However, the error from any approximation can be bounded by the size of the description using the model. version:3
arxiv-1211-6085 | Random Projections for Linear Support Vector Machines | http://arxiv.org/abs/1211.6085 | id:1211.6085 author:Saurabh Paul, Christos Boutsidis, Malik Magdon-Ismail, Petros Drineas category:cs.LG stat.ML  published:2012-11-26 summary:Let X be a data matrix of rank \rho, whose rows represent n points in d-dimensional space. The linear support vector machine constructs a hyperplane separator that maximizes the 1-norm soft margin. We develop a new oblivious dimension reduction technique which is precomputed and can be applied to any input matrix X. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within \epsilon-relative error, ensuring comparable generalization as in the original space in the case of classification. For regression, we show that the margin is preserved to \epsilon-relative error with high probability. We present extensive experiments with real and synthetic data to support our theory. version:5
arxiv-1404-4572 | The First Parallel Multilingual Corpus of Persian: Toward a Persian BLARK | http://arxiv.org/abs/1404.4572 | id:1404.4572 author:Behrang Qasemizadeh, Saeed Rahimi, Behrooz Mahmoodi Bakhtiari category:cs.CL 68T50 I.2.7  published:2014-04-17 summary:In this article, we have introduced the first parallel corpus of Persian with more than 10 other European languages. This article describes primary steps toward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now, we have proposed morphosyntactic specification of Persian based on EAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The article introduces Persian Language, with emphasis on its orthography and morphosyntactic features, then a new Part-of-Speech categorization and orthography for Persian in digital environments is proposed. Finally, the corpus and related statistic will be analyzed. version:1
arxiv-1404-4038 | Discovering and Exploiting Entailment Relationships in Multi-Label Learning | http://arxiv.org/abs/1404.4038 | id:1404.4038 author:Christina Papagiannopoulou, Grigorios Tsoumakas, Ioannis Tsamardinos category:cs.LG  published:2014-04-15 summary:This work presents a sound probabilistic method for enforcing adherence of the marginal probabilities of a multi-label model to automatically discovered deterministic relationships among labels. In particular we focus on discovering two kinds of relationships among the labels. The first one concerns pairwise positive entailement: pairs of labels, where the presence of one implies the presence of the other in all instances of a dataset. The second concerns exclusion: sets of labels that do not coexist in the same instances of the dataset. These relationships are represented with a Bayesian network. Marginal probabilities are entered as soft evidence in the network and adjusted through probabilistic inference. Our approach offers robust improvements in mean average precision compared to the standard binary relavance approach across all 12 datasets involved in our experiments. The discovery process helps interesting implicit knowledge to emerge, which could be useful in itself. version:2
arxiv-1312-7381 | Rate-Distortion Auto-Encoders | http://arxiv.org/abs/1312.7381 | id:1312.7381 author:Luis G. Sanchez Giraldo, Jose C. Principe category:cs.LG  published:2013-12-28 summary:A rekindled the interest in auto-encoder algorithms has been spurred by recent work on deep learning. Current efforts have been directed towards effective training of auto-encoder architectures with a large number of coding units. Here, we propose a learning algorithm for auto-encoders based on a rate-distortion objective that minimizes the mutual information between the inputs and the outputs of the auto-encoder subject to a fidelity constraint. The goal is to learn a representation that is minimally committed to the input data, but that is rich enough to reconstruct the inputs up to certain level of distortion. Minimizing the mutual information acts as a regularization term whereas the fidelity constraint can be understood as a risk functional in the conventional statistical learning setting. The proposed algorithm uses a recently introduced measure of entropy based on infinitely divisible matrices that avoids the plug in estimation of densities. Experiments using over-complete bases show that the rate-distortion auto-encoders can learn a regularized input-output mapping in an implicit manner. version:2
arxiv-1312-4617 | A Survey of Data Mining Techniques for Social Media Analysis | http://arxiv.org/abs/1312.4617 | id:1312.4617 author:Mariam Adedoyin-Olowe, Mohamed Medhat Gaber, Frederic Stahl category:cs.SI cs.CL  published:2013-12-17 summary:Social network has gained remarkable attention in the last decade. Accessing social network sites such as Twitter, Facebook LinkedIn and Google+ through the internet and the web 2.0 technologies has become more affordable. People are becoming more interested in and relying on social network for information, news and opinion of other users on diverse subject matters. The heavy reliance on social network sites causes them to generate massive data characterised by three computational issues namely; size, noise and dynamism. These issues often make social network data very complex to analyse manually, resulting in the pertinent use of computational means of analysing them. Data mining provides a wide range of techniques for detecting useful knowledge from massive datasets like trends, patterns and rules [44]. Data mining techniques are used for information retrieval, statistical modelling and machine learning. These techniques employ data pre-processing, data analysis, and data interpretation processes in the course of data analysis. This survey discusses different data mining techniques used in mining diverse aspects of the social network over decades going from the historical techniques to the up-to-date models, including our novel technique named TRCM. All the techniques covered in this survey are listed in the Table.1 including the tools employed as well as names of their authors. version:2
arxiv-1405-1965 | Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes using High-Resolution Neural EM Data | http://arxiv.org/abs/1405.1965 | id:1405.1965 author:Ayushi Sinha, William Gray Roncal, Narayanan Kasthuri, Jeff W. Lichtman, Randal Burns, Michael Kazhdan category:cs.CV  published:2014-04-16 summary:Accurately estimating the wiring diagram of a brain, known as a connectome, at an ultrastructure level is an open research problem. Specifically, precisely tracking neural processes is difficult, especially across many image slices. Here, we propose a novel method to automatically identify and annotate small subcellular structures present in axons, known as axoplasmic reticula, through a 3D volume of high-resolution neural electron microscopy data. Our method produces high precision annotations, which can help improve automatic segmentation by using our results as seeds for segmentation, and as cues to aid segment merging. version:1
arxiv-1404-4800 | Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes | http://arxiv.org/abs/1404.4800 | id:1404.4800 author:Ayushi Sinha, William Gray Roncal, Narayanan Kasthuri, Ming Chuang, Priya Manavalan, Dean M. Kleissas, Joshua T. Vogelstein, R. Jacob Vogelstein, Randal Burns, Jeff W. Lichtman, Michael Kazhdan category:cs.CV  published:2014-04-16 summary:In this paper, we present a new pipeline which automatically identifies and annotates axoplasmic reticula, which are small subcellular structures present only in axons. We run our algorithm on the Kasthuri11 dataset, which was color corrected using gradient-domain techniques to adjust contrast. We use a bilateral filter to smooth out the noise in this data while preserving edges, which highlights axoplasmic reticula. These axoplasmic reticula are then annotated using a morphological region growing algorithm. Additionally, we perform Laplacian sharpening on the bilaterally filtered data to enhance edges, and repeat the morphological region growing algorithm to annotate more axoplasmic reticula. We track our annotations through the slices to improve precision, and to create long objects to aid in segment merging. This method annotates axoplasmic reticula with high precision. Our algorithm can easily be adapted to annotate axoplasmic reticula in different sets of brain data by changing a few thresholds. The contribution of this work is the introduction of a straightforward and robust pipeline which annotates axoplasmic reticula with high precision, contributing towards advancements in automatic feature annotations in neural EM data. version:1
arxiv-1404-4351 | Stable Graphical Models | http://arxiv.org/abs/1404.4351 | id:1404.4351 author:Navodit Misra, Ercan E. Kuruoglu category:cs.LG stat.ML  published:2014-04-16 summary:Stable random variables are motivated by the central limit theorem for densities with (potentially) unbounded variance and can be thought of as natural generalizations of the Gaussian distribution to skewed and heavy-tailed phenomenon. In this paper, we introduce stable graphical (SG) models, a class of multivariate stable densities that can also be represented as Bayesian networks whose edges encode linear dependencies between random variables. One major hurdle to the extensive use of stable distributions is the lack of a closed-form analytical expression for their densities. This makes penalized maximum-likelihood based learning computationally demanding. We establish theoretically that the Bayesian information criterion (BIC) can asymptotically be reduced to the computationally more tractable minimum dispersion criterion (MDC) and develop StabLe, a structure learning algorithm based on MDC. We use simulated datasets for five benchmark network topologies to empirically demonstrate how StabLe improves upon ordinary least squares (OLS) regression. We also apply StabLe to microarray gene expression data for lymphoblastoid cells from 727 individuals belonging to eight global population groups. We establish that StabLe improves test set performance relative to OLS via ten-fold cross-validation. Finally, we develop SGEX, a method for quantifying differential expression of genes between different population groups. version:1
arxiv-1404-1377 | Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion | http://arxiv.org/abs/1404.1377 | id:1404.1377 author:Zheng Wang, Ming-Jun Lai, Zhaosong Lu, Wei Fan, Hasan Davulcu, Jieping Ye category:cs.LG math.NA stat.ML  published:2014-04-04 summary:In this paper, we propose an efficient and scalable low rank matrix completion algorithm. The key idea is to extend orthogonal matching pursuit method from the vector case to the matrix case. We further propose an economic version of our algorithm by introducing a novel weight updating rule to reduce the time and storage complexity. Both versions are computationally inexpensive for each matrix pursuit iteration, and find satisfactory results in a few iterations. Another advantage of our proposed algorithm is that it has only one tunable parameter, which is the rank. It is easy to understand and to use by the user. This becomes especially important in large-scale learning problems. In addition, we rigorously show that both versions achieve a linear convergence rate, which is significantly better than the previous known results. We also empirically compare the proposed algorithms with several state-of-the-art matrix completion algorithms on many real-world datasets, including the large-scale recommendation dataset Netflix as well as the MovieLens datasets. Numerical results show that our proposed algorithm is more efficient than competing algorithms while achieving similar or better prediction performance. version:2
arxiv-1404-4326 | Open Question Answering with Weakly Supervised Embedding Models | http://arxiv.org/abs/1404.4326 | id:1404.4326 author:Antoine Bordes, Jason Weston, Nicolas Usunier category:cs.CL cs.LG  published:2014-04-16 summary:Building computers able to answer questions on any subject is a long standing goal of artificial intelligence. Promising progress has recently been achieved by methods that learn to map questions to logical forms or database queries. Such approaches can be effective but at the cost of either large amounts of human-labeled data or by defining lexicons and grammars tailored by practitioners. In this paper, we instead take the radical approach of learning to map questions to vectorial feature representations. By mapping answers into the same space one can query any knowledge base independent of its schema, without requiring any grammar or lexicon. Our method is trained with a new optimization procedure combining stochastic gradient descent followed by a fine-tuning step using the weak supervision provided by blending automatically and collaboratively generated resources. We empirically demonstrate that our model can capture meaningful signals from its noisy supervision leading to major improvements over paralex, the only existing method able to be trained on similar weakly labeled data. version:1
arxiv-1404-4316 | Generic Object Detection With Dense Neural Patterns and Regionlets | http://arxiv.org/abs/1404.4316 | id:1404.4316 author:Will Y. Zou, Xiaoyu Wang, Miao Sun, Yuanqing Lin category:cs.CV  published:2014-04-16 summary:This paper addresses the challenge of establishing a bridge between deep convolutional neural networks and conventional object detection frameworks for accurate and efficient generic object detection. We introduce Dense Neural Patterns, short for DNPs, which are dense local features derived from discriminatively trained deep convolutional neural networks. DNPs can be easily plugged into conventional detection frameworks in the same way as other dense local features(like HOG or LBP). The effectiveness of the proposed approach is demonstrated with the Regionlets object detection framework. It achieved 46.1% mean average precision on the PASCAL VOC 2007 dataset, and 44.1% on the PASCAL VOC 2010 dataset, which dramatically improves the original Regionlets approach without DNPs. version:1
arxiv-1404-4314 | An Empirical Comparison of Parsing Methods for Stanford Dependencies | http://arxiv.org/abs/1404.4314 | id:1404.4314 author:Lingpeng Kong, Noah A. Smith category:cs.CL  published:2014-04-16 summary:Stanford typed dependencies are a widely desired representation of natural language sentences, but parsing is one of the major computational bottlenecks in text analysis systems. In light of the evolving definition of the Stanford dependencies and developments in statistical dependency parsing algorithms, this paper revisits the question of Cer et al. (2010): what is the tradeoff between accuracy and speed in obtaining Stanford dependencies in particular? We also explore the effects of input representations on this tradeoff: part-of-speech tags, the novel use of an alternative dependency representation as input, and distributional representaions of words. We find that direct dependency parsing is a more viable solution than it was found to be in the past. An accompanying software release can be found at: http://www.ark.cs.cmu.edu/TBSD version:1
arxiv-1403-4640 | Communication Communities in MOOCs | http://arxiv.org/abs/1403.4640 | id:1403.4640 author:Nabeel Gillani, Rebecca Eynon, Michael Osborne, Isis Hjorth, Stephen Roberts category:cs.CY cs.SI stat.ML  published:2014-03-18 summary:Massive Open Online Courses (MOOCs) bring together thousands of people from different geographies and demographic backgrounds -- but to date, little is known about how they learn or communicate. We introduce a new content-analysed MOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to extract communities of learners based on the nature of their online forum posts. We see that BNMF yields a superior probabilistic generative model for online discussions when compared to other models, and that the communities it learns are differentiated by their composite students' demographic and course performance indicators. These findings suggest that computationally efficient probabilistic generative modelling of MOOCs can reveal important insights for educational researchers and practitioners and help to develop more intelligent and responsive online learning environments. version:2
arxiv-1404-4175 | MEG Decoding Across Subjects | http://arxiv.org/abs/1404.4175 | id:1404.4175 author:Emanuele Olivetti, Seyed Mostafa Kia, Paolo Avesani category:stat.ML cs.LG q-bio.NC  published:2014-04-16 summary:Brain decoding is a data analysis paradigm for neuroimaging experiments that is based on predicting the stimulus presented to the subject from the concurrent brain activity. In order to make inference at the group level, a straightforward but sometimes unsuccessful approach is to train a classifier on the trials of a group of subjects and then to test it on unseen trials from new subjects. The extreme difficulty is related to the structural and functional variability across the subjects. We call this approach "decoding across subjects". In this work, we address the problem of decoding across subjects for magnetoencephalographic (MEG) experiments and we provide the following contributions: first, we formally describe the problem and show that it belongs to a machine learning sub-field called transductive transfer learning (TTL). Second, we propose to use a simple TTL technique that accounts for the differences between train data and test data. Third, we propose the use of ensemble learning, and specifically of stacked generalization, to address the variability across subjects within train data, with the aim of producing more stable classifiers. On a face vs. scramble task MEG dataset of 16 subjects, we compare the standard approach of not modelling the differences across subjects, to the proposed one of combining TTL and ensemble learning. We show that the proposed approach is consistently more accurate than the standard one. version:1
arxiv-1404-4171 | Dropout Training for Support Vector Machines | http://arxiv.org/abs/1404.4171 | id:1404.4171 author:Ning Chen, Jun Zhu, Jianfei Chen, Bo Zhang category:cs.LG  published:2014-04-16 summary:Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for linear SVMs. To deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights have closed-form solutions. The similar ideas are applied to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of linear SVMs. version:1
arxiv-1404-3543 | Recover Canonical-View Faces in the Wild with Deep Neural Networks | http://arxiv.org/abs/1404.3543 | id:1404.3543 author:Zhenyao Zhu, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV  published:2014-04-14 summary:Face images in the wild undergo large intra-personal variations, such as poses, illuminations, occlusions, and low resolutions, which cause great challenges to face-related applications. This paper addresses this challenge by proposing a new deep learning framework that can recover the canonical view of face images. It dramatically reduces the intra-person variances, while maintaining the inter-person discriminativeness. Unlike the existing face reconstruction methods that were either evaluated in controlled 2D environment or employed 3D information, our approach directly learns the transformation from the face images with a complex set of variations to their canonical views. At the training stage, to avoid the costly process of labeling canonical-view images from the training set by hand, we have devised a new measurement to automatically select or synthesize a canonical-view image for each identity. As an application, this face recovery approach is used for face verification. Facial features are learned from the recovered canonical-view face images by using a facial component-based convolutional neural network. Our approach achieves the state-of-the-art performance on the LFW dataset. version:2
arxiv-1312-2164 | Budgeted Influence Maximization for Multiple Products | http://arxiv.org/abs/1312.2164 | id:1312.2164 author:Nan Du, Yingyu Liang, Maria Florina Balcan, Le Song category:cs.LG cs.SI stat.ML  published:2013-12-08 summary:The typical algorithmic problem in viral marketing aims to identify a set of influential users in a social network, who, when convinced to adopt a product, shall influence other users in the network and trigger a large cascade of adoptions. However, the host (the owner of an online social platform) often faces more constraints than a single product, endless user attentions, unlimited budget and unbounded time; in reality, multiple products need to be advertised, each user can tolerate only a small number of recommendations, influencing user has a cost and advertisers have only limited budgets, and the adoptions need to be maximized within a short time window. Given theses myriads of user, monetary, and timing constraints, it is extremely challenging for the host to design principled and efficient viral market algorithms with provable guarantees. In this paper, we provide a novel solution by formulating the problem as a submodular maximization in a continuous-time diffusion model under an intersection of a matroid and multiple knapsack constraints. We also propose an adaptive threshold greedy algorithm which can be faster than the traditional greedy algorithm with lazy evaluation, and scalable to networks with million of nodes. Furthermore, our mathematical formulation allows us to prove that the algorithm can achieve an approximation factor of $k_a/(2+2 k)$ when $k_a$ out of the $k$ knapsack constraints are active, which also improves over previous guarantees from combinatorial optimization literature. In the case when influencing each user has uniform cost, the approximation becomes even better to a factor of $1/3$. Extensive synthetic and real world experiments demonstrate that our budgeted influence maximization algorithm achieves the-state-of-the-art in terms of both effectiveness and scalability, often beating the next best by significant margins. version:2
arxiv-1404-3989 | Bayesian Neural Networks for Genetic Association Studies of Complex Disease | http://arxiv.org/abs/1404.3989 | id:1404.3989 author:Andrew L. Beam, Alison Motsinger-Reif, Jon Doyle category:q-bio.GN stat.AP stat.ML  published:2014-04-15 summary:Discovering causal genetic variants from large genetic association studies poses many difficult challenges. Assessing which genetic markers are involved in determining trait status is a computationally demanding task, especially in the presence of gene-gene interactions. A non-parametric Bayesian approach in the form of a Bayesian neural network is proposed for use in analyzing genetic association studies. Demonstrations on synthetic and real data reveal they are able to efficiently and accurately determine which variants are involved in determining case-control status. Using graphics processing units (GPUs) the time needed to build these models is decreased by several orders of magnitude. In comparison with commonly used approaches for detecting genetic interactions, Bayesian neural networks perform very well across a broad spectrum of possible genetic relationships while having the computational efficiency needed to handle large datasets. version:2
arxiv-1404-4105 | Sparse Compositional Metric Learning | http://arxiv.org/abs/1404.4105 | id:1404.4105 author:Yuan Shi, Aurélien Bellet, Fei Sha category:cs.LG stat.ML  published:2014-04-15 summary:We propose a new approach for metric learning by framing it as learning a sparse combination of locally discriminative metrics that are inexpensive to generate from the training data. This flexible framework allows us to naturally derive formulations for global, multi-task and local metric learning. The resulting algorithms have several advantages over existing methods in the literature: a much smaller number of parameters to be estimated and a principled way to generalize learned metrics to new testing data points. To analyze the approach theoretically, we derive a generalization bound that justifies the sparse combination. Empirically, we evaluate our algorithms on several datasets against state-of-the-art metric learning methods. The results are consistent with our theoretical findings and demonstrate the superiority of our approach in terms of classification performance and scalability. version:1
arxiv-1404-4104 | Sparse Bilinear Logistic Regression | http://arxiv.org/abs/1404.4104 | id:1404.4104 author:Jianing V. Shi, Yangyang Xu, Richard G. Baraniuk category:math.OC cs.CV cs.LG 65K10  68W40  68Q32  published:2014-04-15 summary:In this paper, we introduce the concept of sparse bilinear logistic regression for decision problems involving explanatory variables that are two-dimensional matrices. Such problems are common in computer vision, brain-computer interfaces, style/content factorization, and parallel factor analysis. The underlying optimization problem is bi-convex; we study its solution and develop an efficient algorithm based on block coordinate descent. We provide a theoretical guarantee for global convergence and estimate the asymptotical convergence rate using the Kurdyka-{\L}ojasiewicz inequality. A range of experiments with simulated and real data demonstrate that sparse bilinear logistic regression outperforms current techniques in several important applications. version:1
arxiv-1404-4088 | Ensemble Classifiers and Their Applications: A Review | http://arxiv.org/abs/1404.4088 | id:1404.4088 author:Akhlaqur Rahman, Sumaira Tasnim category:cs.LG  published:2014-04-15 summary:Ensemble classifier refers to a group of individual classifiers that are cooperatively trained on data set in a supervised classification problem. In this paper we present a review of commonly used ensemble classifiers in the literature. Some ensemble classifiers are also developed targeting specific applications. We also present some application driven ensemble classifiers in this paper. version:1
arxiv-1311-1838 | Efficient Regularization of Squared Curvature | http://arxiv.org/abs/1311.1838 | id:1311.1838 author:Claudia Nieuwenhuis, Eno Toeppe, Lena Gorelick, Olga Veksler, Yuri Boykov category:cs.CV  published:2013-11-07 summary:Curvature has received increased attention as an important alternative to length based regularization in computer vision. In contrast to length, it preserves elongated structures and fine details. Existing approaches are either inefficient, or have low angular resolution and yield results with strong block artifacts. We derive a new model for computing squared curvature based on integral geometry. The model counts responses of straight line triple cliques. The corresponding energy decomposes into submodular and supermodular pairwise potentials. We show that this energy can be efficiently minimized even for high angular resolutions using the trust region framework. Our results confirm that we obtain accurate and visually pleasing solutions without strong artifacts at reasonable run times. version:2
arxiv-1311-1856 | Submodularization for Quadratic Pseudo-Boolean Optimization | http://arxiv.org/abs/1311.1856 | id:1311.1856 author:Lena Gorelick, Yuri Boykov, Olga Veksler, Ismail Ben Ayed, Andrew Delong category:cs.CV  published:2013-11-08 summary:Many computer vision problems require optimization of binary non-submodular energies. We propose a general optimization framework based on local submodular approximations (LSA). Unlike standard LP relaxation methods that linearize the whole energy globally, our approach iteratively approximates the energies locally. On the other hand, unlike standard local optimization methods (e.g. gradient descent or projection techniques) we use non-linear submodular approximations and optimize them without leaving the domain of integer solutions. We discuss two specific LSA algorithms based on "trust region" and "auxiliary function" principles, LSA-TR and LSA-AUX. These methods obtain state-of-the-art results on a wide range of applications outperforming many standard techniques such as LBP, QPBO, and TRWS. While our paper is focused on pairwise energies, our ideas extend to higher-order problems. The code is available online (http://vision.csd.uwo.ca/code/). version:2
arxiv-1309-0489 | Relative Comparison Kernel Learning with Auxiliary Kernels | http://arxiv.org/abs/1309.0489 | id:1309.0489 author:Eric Heim, Hamed Valizadegan, Milos Hauskrecht category:cs.LG  published:2013-09-02 summary:In this work we consider the problem of learning a positive semidefinite kernel matrix from relative comparisons of the form: "object A is more similar to object B than it is to C", where comparisons are given by humans. Existing solutions to this problem assume many comparisons are provided to learn a high quality kernel. However, this can be considered unrealistic for many real-world tasks since relative assessments require human input, which is often costly or difficult to obtain. Because of this, only a limited number of these comparisons may be provided. In this work, we explore methods for aiding the process of learning a kernel with the help of auxiliary kernels built from more easily extractable information regarding the relationships among objects. We propose a new kernel learning approach in which the target kernel is defined as a conic combination of auxiliary kernels and a kernel whose elements are learned directly. We formulate a convex optimization to solve for this target kernel that adds only minor overhead to methods that use no auxiliary information. Empirical results show that in the presence of few training relative comparisons, our method can learn kernels that generalize to more out-of-sample comparisons than methods that do not utilize auxiliary information, as well as similar methods that learn metrics over objects. version:3
arxiv-1404-4067 | An effective AHP-based metaheuristic approach to solve supplier selection problem | http://arxiv.org/abs/1404.4067 | id:1404.4067 author:Tamal Ghosh, Tanmoy Chakraborty, Pranab K Dan category:cs.NE  published:2014-04-15 summary:The supplier selection problem is based on electing the best supplier from a group of pre-specified candidates, is identified as a Multi Criteria Decision Making (MCDM), is proportionately significant in terms of qualitative and quantitative attributes. It is a fundamental issue to achieve a trade-off between such quantifiable and unquantifiable attributes with an aim to accomplish the best solution to the abovementioned problem. This article portrays a metaheuristic based optimization model to solve this NP-Complete problem. Initially the Analytic Hierarchy Process (AHP) is implemented to generate an initial feasible solution of the problem. Thereafter a Simulated Annealing (SA) algorithm is exploited to improve the quality of the obtained solution. The Taguchi robust design method is exploited to solve the critical issues on the subject of the parameter selection of the SA technique. In order to verify the proposed methodology the numerical results are demonstrated based on tangible industry data. version:1
arxiv-1404-3591 | Hybrid Conditional Gradient - Smoothing Algorithms with Applications to Sparse and Low Rank Regularization | http://arxiv.org/abs/1404.3591 | id:1404.3591 author:Andreas Argyriou, Marco Signoretto, Johan Suykens category:math.OC cs.LG stat.ML  published:2014-04-14 summary:We study a hybrid conditional gradient - smoothing algorithm (HCGS) for solving composite convex optimization problems which contain several terms over a bounded set. Examples of these include regularization problems with several norms as penalties and a norm constraint. HCGS extends conditional gradient methods to cases with multiple nonsmooth terms, in which standard conditional gradient methods may be difficult to apply. The HCGS algorithm borrows techniques from smoothing proximal methods and requires first-order computations (subgradients and proximity operations). Unlike proximal methods, HCGS benefits from the advantages of conditional gradient methods, which render it more efficient on certain large scale optimization problems. We demonstrate these advantages with simulations on two matrix optimization problems: regularization of matrices with combined $\ell_1$ and trace norm penalties; and a convex relaxation of sparse PCA. version:2
arxiv-1404-3415 | Generalized version of the support vector machine for binary classification problems: supporting hyperplane machine | http://arxiv.org/abs/1404.3415 | id:1404.3415 author:E. G. Abramov, A. B. Komissarov, D. A. Kornyakov category:cs.LG stat.ML F.1.1; I.5.1  published:2014-04-13 summary:In this paper there is proposed a generalized version of the SVM for binary classification problems in the case of using an arbitrary transformation x -> y. An approach similar to the classic SVM method is used. The problem is widely explained. Various formulations of primal and dual problems are proposed. For one of the most important cases the formulae are derived in detail. A simple computational example is demonstrated. The algorithm and its implementation is presented in Octave language. version:2
arxiv-1404-3992 | Assessing the Quality of MT Systems for Hindi to English Translation | http://arxiv.org/abs/1404.3992 | id:1404.3992 author:Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, Ajai Kumar category:cs.CL  published:2014-04-15 summary:Evaluation plays a vital role in checking the quality of MT output. It is done either manually or automatically. Manual evaluation is very time consuming and subjective, hence use of automatic metrics is done most of the times. This paper evaluates the translation quality of different MT Engines for Hindi-English (Hindi data is provided as input and English is obtained as output) using various automatic metrics like BLEU, METEOR etc. Further the comparison automatic evaluation results with Human ranking have also been given. version:1
arxiv-1404-3991 | Spiralet Sparse Representation | http://arxiv.org/abs/1404.3991 | id:1404.3991 author:Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.CV  published:2014-04-15 summary:This is the first report on Working Paper WP-RFM-14-01. The potential and capability of sparse representations is well-known. However, their (multivariate variable) vectorial form, which is completely fine in many fields and disciplines, results in removal and filtering of important "spatial" relations that are implicitly carried by two-dimensional [or multi-dimensional] objects, such as images. In this paper, a new approach, called spiralet sparse representation, is proposed in order to develop an augmented representation and therefore a modified sparse representation and theory, which is capable to preserve the data associated to the spatial relations. version:1
arxiv-1405-1966 | Texture Based Image Segmentation of Chili Pepper X-Ray Images Using Gabor Filter | http://arxiv.org/abs/1405.1966 | id:1405.1966 author:M. Rajalakshmi, Dr. P. Subashini category:cs.CV cs.LG  published:2014-04-15 summary:Texture segmentation is the process of partitioning an image into regions with different textures containing a similar group of pixels. Detecting the discontinuity of the filter's output and their statistical properties help in segmenting and classifying a given image with different texture regions. In this proposed paper, chili x-ray image texture segmentation is performed by using Gabor filter. The texture segmented result obtained from Gabor filter fed into three texture filters, namely Entropy, Standard Deviation and Range filter. After performing texture analysis, features can be extracted by using Statistical methods. In this paper Gray Level Co-occurrence Matrices and First order statistics are used as feature extraction methods. Features extracted from statistical methods are given to Support Vector Machine (SVM) classifier. Using this methodology, it is found that texture segmentation is followed by the Gray Level Co-occurrence Matrix feature extraction method gives a higher accuracy rate of 84% when compared with First order feature extraction method. Key Words: Texture segmentation, Texture filter, Gabor filter, Feature extraction methods, SVM classifier. version:1
arxiv-1404-3959 | Is it morally acceptable for a system to lie to persuade me? | http://arxiv.org/abs/1404.3959 | id:1404.3959 author:Marco Guerini, Fabio Pianesi, Oliviero Stock category:cs.CY cs.CL  published:2014-04-15 summary:Given the fast rise of increasingly autonomous artificial agents and robots, a key acceptability criterion will be the possible moral implications of their actions. In particular, intelligent persuasive systems (systems designed to influence humans via communication) constitute a highly sensitive topic because of their intrinsically social nature. Still, ethical studies in this area are rare and tend to focus on the output of the required action. Instead, this work focuses on the persuasive acts themselves (e.g. "is it morally acceptable that a machine lies or appeals to the emotions of a person to persuade her, even if for a good end?"). Exploiting a behavioral approach, based on human assessment of moral dilemmas -- i.e. without any prior assumption of underlying ethical theories -- this paper reports on a set of experiments. These experiments address the type of persuader (human or machine), the strategies adopted (purely argumentative, appeal to positive emotions, appeal to negative emotions, lie) and the circumstances. Findings display no differences due to the agent, mild acceptability for persuasion and reveal that truth-conditional reasoning (i.e. argument validity) is a significant dimension affecting subjects' judgment. Some implications for the design of intelligent persuasive systems are discussed. version:1
arxiv-1404-3933 | Scalable Matting: A Sub-linear Approach | http://arxiv.org/abs/1404.3933 | id:1404.3933 author:Philip G. Lee, Ying Wu category:cs.CV  published:2014-04-15 summary:Natural image matting, which separates foreground from background, is a very important intermediate step in recent computer vision algorithms. However, it is severely underconstrained and difficult to solve. State-of-the-art approaches include matting by graph Laplacian, which significantly improves the underconstrained nature by reducing the solution space. However, matting by graph Laplacian is still very difficult to solve and gets much harder as the image size grows: current iterative methods slow down as $\mathcal{O}\left(n^2 \right)$ in the resolution $n$. This creates uncomfortable practical limits on the resolution of images that we can matte. Current literature mitigates the problem, but they all remain super-linear in complexity. We expose properties of the problem that remain heretofore unexploited, demonstrating that an optimization technique originally intended to solve PDEs can be adapted to take advantage of this knowledge to solve the matting problem, not heuristically, but exactly and with sub-linear complexity. This makes ours the most efficient matting solver currently known by a very wide margin and allows matting finally to be practical and scalable in the future as consumer photos exceed many dozens of megapixels, and also relieves matting from being a bottleneck for vision algorithms that depend on it. version:1
arxiv-1404-1521 | Exploring the power of GPU's for training Polyglot language models | http://arxiv.org/abs/1404.1521 | id:1404.1521 author:Vivek Kulkarni, Rami Al-Rfou', Bryan Perozzi, Steven Skiena category:cs.LG cs.CL  published:2014-04-05 summary:One of the major research trends currently is the evolution of heterogeneous parallel computing. GP-GPU computing is being widely used and several applications have been designed to exploit the massive parallelism that GP-GPU's have to offer. While GPU's have always been widely used in areas of computer vision for image processing, little has been done to investigate whether the massive parallelism provided by GP-GPU's can be utilized effectively for Natural Language Processing(NLP) tasks. In this work, we investigate and explore the power of GP-GPU's in the task of learning language models. More specifically, we investigate the performance of training Polyglot language models using deep belief neural networks. We evaluate the performance of training the model on the GPU and present optimizations that boost the performance on the GPU.One of the key optimizations, we propose increases the performance of a function involved in calculating and updating the gradient by approximately 50 times on the GPU for sufficiently large batch sizes. We show that with the above optimizations, the GP-GPU's performance on the task increases by factor of approximately 3-4. The optimizations we made are generic Theano optimizations and hence potentially boost the performance of other models which rely on these operations.We also show that these optimizations result in the GPU's performance at this task being now comparable to that on the CPU. We conclude by presenting a thorough evaluation of the applicability of GP-GPU's for this task and highlight the factors limiting the performance of training a Polyglot model on the GPU. version:3
arxiv-1305-2490 | Combining Drift Analysis and Generalized Schema Theory to Design Efficient Hybrid and/or Mixed Strategy EAs | http://arxiv.org/abs/1305.2490 | id:1305.2490 author:Boris Mitavskiy, Jun He category:cs.NE  published:2013-05-11 summary:Hybrid and mixed strategy EAs have become rather popular for tackling various complex and NP-hard optimization problems. While empirical evidence suggests that such algorithms are successful in practice, rather little theoretical support for their success is available, not mentioning a solid mathematical foundation that would provide guidance towards an efficient design of this type of EAs. In the current paper we develop a rigorous mathematical framework that suggests such designs based on generalized schema theory, fitness levels and drift analysis. An example-application for tackling one of the classical NP-hard problems, the "single-machine scheduling problem" is presented. version:2
arxiv-1404-3759 | Meta-evaluation of comparability metrics using parallel corpora | http://arxiv.org/abs/1404.3759 | id:1404.3759 author:Bogdan Babych, Anthony Hartley category:cs.CL  published:2014-04-14 summary:Metrics for measuring the comparability of corpora or texts need to be developed and evaluated systematically. Applications based on a corpus, such as training Statistical MT systems in specialised narrow domains, require finding a reasonable balance between the size of the corpus and its consistency, with controlled and benchmarked levels of comparability for any newly added sections. In this article we propose a method that can meta-evaluate comparability metrics by calculating monolingual comparability scores separately on the 'source' and 'target' sides of parallel corpora. The range of scores on the source side is then correlated (using Pearson's r coefficient) with the range of 'target' scores; the higher the correlation - the more reliable is the metric. The intuition is that a good metric should yield the same distance between different domains in different languages. Our method gives consistent results for the same metrics on different data sets, which indicates that it is reliable and can be used for metric comparison or for optimising settings of parametrised metrics. version:1
arxiv-1112-1517 | Pure Strategy or Mixed Strategy? | http://arxiv.org/abs/1112.1517 | id:1112.1517 author:Jun He, Feidun He, Hongbin Dong category:cs.NE  published:2011-12-07 summary:Mixed strategy EAs aim to integrate several mutation operators into a single algorithm. However few theoretical analysis has been made to answer the question whether and when the performance of mixed strategy EAs is better than that of pure strategy EAs. In theory, the performance of EAs can be measured by asymptotic convergence rate and asymptotic hitting time. In this paper, it is proven that given a mixed strategy (1+1) EAs consisting of several mutation operators, its performance (asymptotic convergence rate and asymptotic hitting time)is not worse than that of the worst pure strategy (1+1) EA using one mutation operator; if these mutation operators are mutually complementary, then it is possible to design a mixed strategy (1+1) EA whose performance is better than that of any pure strategy (1+1) EA using one mutation operator. version:4
arxiv-1312-4894 | Deep Convolutional Ranking for Multilabel Image Annotation | http://arxiv.org/abs/1312.4894 | id:1312.4894 author:Yunchao Gong, Yangqing Jia, Thomas Leung, Alexander Toshev, Sergey Ioffe category:cs.CV  published:2013-12-17 summary:Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, features based on Deep Neural Networks have shown potential to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with approximate top-$k$ ranking objectives, as thye naturally fit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset outperforms the conventional visual features by about 10%, obtaining the best reported performance in the literature. version:2
arxiv-1404-3656 | Methods for Ordinal Peer Grading | http://arxiv.org/abs/1404.3656 | id:1404.3656 author:Karthik Raman, Thorsten Joachims category:cs.LG cs.IR H.4  published:2014-04-14 summary:MOOCs have the potential to revolutionize higher education with their wide outreach and accessibility, but they require instructors to come up with scalable alternates to traditional student evaluation. Peer grading -- having students assess each other -- is a promising approach to tackling the problem of evaluation at scale, since the number of "graders" naturally scales with the number of students. However, students are not trained in grading, which means that one cannot expect the same level of grading skills as in traditional settings. Drawing on broad evidence that ordinal feedback is easier to provide and more reliable than cardinal feedback, it is therefore desirable to allow peer graders to make ordinal statements (e.g. "project X is better than project Y") and not require them to make cardinal statements (e.g. "project X is a B-"). Thus, in this paper we study the problem of automatically inferring student grades from ordinal peer feedback, as opposed to existing methods that require cardinal peer feedback. We formulate the ordinal peer grading problem as a type of rank aggregation problem, and explore several probabilistic models under which to estimate student grades and grader reliability. We study the applicability of these methods using peer grading data collected from a real class -- with instructor and TA grades as a baseline -- and demonstrate the efficacy of ordinal feedback techniques in comparison to existing cardinal peer grading methods. Finally, we compare these peer-grading techniques to traditional evaluation techniques. version:1
arxiv-1308-2867 | Composite Self-Concordant Minimization | http://arxiv.org/abs/1308.2867 | id:1308.2867 author:Quoc Tran-Dinh, Anastasios Kyrillidis, Volkan Cevher category:stat.ML cs.LG math.OC  published:2013-08-13 summary:We propose a variable metric framework for minimizing the sum of a self-concordant function and a possibly non-smooth convex function, endowed with an easily computable proximal operator. We theoretically establish the convergence of our framework without relying on the usual Lipschitz gradient assumption on the smooth part. An important highlight of our work is a new set of analytic step-size selection and correction procedures based on the structure of the problem. We describe concrete algorithmic instances of our framework for several interesting applications and demonstrate them numerically on both synthetic and real data. version:2
arxiv-1404-3520 | A Theoretical Assessment of Solution Quality in Evolutionary Algorithms for the Knapsack Problem | http://arxiv.org/abs/1404.3520 | id:1404.3520 author:Jun He, Boris Mitavskiy, Yuren Zhou category:cs.NE  published:2014-04-14 summary:Evolutionary algorithms are well suited for solving the knapsack problem. Some empirical studies claim that evolutionary algorithms can produce good solutions to the 0-1 knapsack problem. Nonetheless, few rigorous investigations address the quality of solutions that evolutionary algorithms may produce for the knapsack problem. The current paper focuses on a theoretical investigation of three types of (N+1) evolutionary algorithms that exploit bitwise mutation, truncation selection, plus different repair methods for the 0-1 knapsack problem. It assesses the solution quality in terms of the approximation ratio. Our work indicates that the solution produced by pure strategy and mixed strategy evolutionary algorithms is arbitrarily bad. Nevertheless, the evolutionary algorithm using helper objectives may produce 1/2-approximation solutions to the 0-1 knapsack problem. version:1
arxiv-1308-4017 | A Study on Stroke Rehabilitation through Task-Oriented Control of a Haptic Device via Near-Infrared Spectroscopy-Based BCI | http://arxiv.org/abs/1308.4017 | id:1308.4017 author:Berdakh Abibullaev, Jinung An, Seung-Hyun Lee, Jeon-Il Moon category:stat.ML cs.HC q-bio.NC  published:2013-08-19 summary:This paper presents a study in task-oriented approach to stroke rehabilitation by controlling a haptic device via near-infrared spectroscopy-based brain-computer interface (BCI). The task is to command the haptic device to move in opposing directions of leftward and rightward movement. Our study consists of data acquisition, signal preprocessing, and classification. In data acquisition, we conduct experiments based on two different mental tasks: one on pure motor imagery, and another on combined motor imagery and action observation. The experiments were conducted in both offline and online modes. In the signal preprocessing, we use localization method to eliminate channels that are irrelevant to the mental task, as well as perform feature extraction for subsequent classification. We propose multiple support vector machine classifiers with a majority-voting scheme for improved classification results. And lastly, we present test results to demonstrate the efficacy of our proposed approach to possible stroke rehabilitation practice. version:3
arxiv-1401-1605 | Fast nonparametric clustering of structured time-series | http://arxiv.org/abs/1401.1605 | id:1401.1605 author:James Hensman, Magnus Rattray, Neil D. Lawrence category:cs.LG cs.CV stat.ML  published:2014-01-08 summary:In this publication, we combine two Bayesian non-parametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variationala pproximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a twofold speed-up over EM-based variational inference. version:2
arxiv-1312-6082 | Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks | http://arxiv.org/abs/1312.6082 | id:1312.6082 author:Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet category:cs.CV  published:2013-12-20 summary:Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. We employ the DistBelief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over $96\%$ accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving $97.84\%$ accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over $90\%$ accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. We report a $99.8\%$ accuracy on the hardest category of reCAPTCHA. Our evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators. version:4
arxiv-1404-3439 | Anytime Hierarchical Clustering | http://arxiv.org/abs/1404.3439 | id:1404.3439 author:Omur Arslan, Daniel E. Koditschek category:stat.ML cs.IR cs.LG H.3.3; I.5.3  published:2014-04-13 summary:We propose a new anytime hierarchical clustering method that iteratively transforms an arbitrary initial hierarchy on the configuration of measurements along a sequence of trees we prove for a fixed data set must terminate in a chain of nested partitions that satisfies a natural homogeneity requirement. Each recursive step re-edits the tree so as to improve a local measure of cluster homogeneity that is compatible with a number of commonly used (e.g., single, average, complete) linkage functions. As an alternative to the standard batch algorithms, we present numerical evidence to suggest that appropriate adaptations of this method can yield decentralized, scalable algorithms suitable for distributed/parallel computation of clustering hierarchies and online tracking of clustering trees applicable to large, dynamically changing databases and anomaly detection. version:1
arxiv-1404-3418 | Active Learning for Undirected Graphical Model Selection | http://arxiv.org/abs/1404.3418 | id:1404.3418 author:Divyanshu Vats, Robert D. Nowak, Richard G. Baraniuk category:stat.ML cs.IT math.IT math.ST stat.TH  published:2014-04-13 summary:This paper studies graphical model selection, i.e., the problem of estimating a graph of statistical relationships among a collection of random variables. Conventional graphical model selection algorithms are passive, i.e., they require all the measurements to have been collected before processing begins. We propose an active learning algorithm that uses junction tree representations to adapt future measurements based on the information gathered from prior measurements. We prove that, under certain conditions, our active learning algorithm requires fewer scalar measurements than any passive algorithm to reliably estimate a graph. A range of numerical results validate our theory and demonstrates the benefits of active learning. version:1
arxiv-1311-2626 | Second-order Shape Optimization for Geometric Inverse Problems in Vision | http://arxiv.org/abs/1311.2626 | id:1311.2626 author:J. Balzer, S. Soatto category:cs.CV  published:2013-11-11 summary:We develop a method for optimization in shape spaces, i.e., sets of surfaces modulo re-parametrization. Unlike previously proposed gradient flows, we achieve superlinear convergence rates through a subtle approximation of the shape Hessian, which is generally hard to compute and suffers from a series of degeneracies. Our analysis highlights the role of mean curvature motion in comparison with first-order schemes: instead of surface area, our approach penalizes deformation, either by its Dirichlet energy or total variation. Latter regularizer sparks the development of an alternating direction method of multipliers on triangular meshes. Therein, a conjugate-gradients solver enables us to bypass formation of the Gaussian normal equations appearing in the course of the overall optimization. We combine all of the aforementioned ideas in a versatile geometric variation-regularized Levenberg-Marquardt-type method applicable to a variety of shape functionals, depending on intrinsic properties of the surface such as normal field and curvature as well as its embedding into space. Promising experimental results are reported. version:5
arxiv-1404-3377 | A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing | http://arxiv.org/abs/1404.3377 | id:1404.3377 author:Rene Pickhardt, Thomas Gottron, Martin Körner, Paul Georg Wagner, Till Speicher, Steffen Staab category:cs.CL  published:2014-04-13 summary:We introduce a novel approach for building language models based on a systematic, recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing. Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case. In this paper we motivate, formalize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1% and 12.7% in comparison to traditional language models using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity. version:1
arxiv-1403-0284 | Bayes Merging of Multiple Vocabularies for Scalable Image Retrieval | http://arxiv.org/abs/1403.0284 | id:1403.0284 author:Liang Zheng, Shengjin Wang, Wengang Zhou, Qi Tian category:cs.CV  published:2014-03-03 summary:The Bag-of-Words (BoW) representation is well applied to recent state-of-the-art image retrieval works. Typically, multiple vocabularies are generated to correct quantization artifacts and improve recall. However, this routine is corrupted by vocabulary correlation, i.e., overlapping among different vocabularies. Vocabulary correlation leads to an over-counting of the indexed features in the overlapped area, or the intersection set, thus compromising the retrieval accuracy. In order to address the correlation problem while preserve the benefit of high recall, this paper proposes a Bayes merging approach to down-weight the indexed features in the intersection set. Through explicitly modeling the correlation problem in a probabilistic view, a joint similarity on both image- and feature-level is estimated for the indexed features in the intersection set. We evaluate our method through extensive experiments on three benchmark datasets. Albeit simple, Bayes merging can be well applied in various merging tasks, and consistently improves the baselines on multi-vocabulary merging. Moreover, Bayes merging is efficient in terms of both time and memory cost, and yields competitive performance compared with the state-of-the-art methods. version:2
arxiv-1402-2681 | Packing and Padding: Coupled Multi-index for Accurate Image Retrieval | http://arxiv.org/abs/1402.2681 | id:1402.2681 author:Liang Zheng, Shengjin Wang, Ziqiong Liu, Qi Tian category:cs.CV  published:2014-02-11 summary:In Bag-of-Words (BoW) based image retrieval, the SIFT visual word has a low discriminative power, so false positive matches occur prevalently. Apart from the information loss during quantization, another cause is that the SIFT feature only describes the local gradient distribution. To address this problem, this paper proposes a coupled Multi-Index (c-MI) framework to perform feature fusion at indexing level. Basically, complementary features are coupled into a multi-dimensional inverted index. Each dimension of c-MI corresponds to one kind of feature, and the retrieval process votes for images similar in both SIFT and other feature spaces. Specifically, we exploit the fusion of local color feature into c-MI. While the precision of visual match is greatly enhanced, we adopt Multiple Assignment to improve recall. The joint cooperation of SIFT and color features significantly reduces the impact of false positive matches. Extensive experiments on several benchmark datasets demonstrate that c-MI improves the retrieval accuracy significantly, while consuming only half of the query time compared to the baseline. Importantly, we show that c-MI is well complementary to many prior techniques. Assembling these methods, we have obtained an mAP of 85.8% and N-S score of 3.85 on Holidays and Ukbench datasets, respectively, which compare favorably with the state-of-the-arts. version:2
arxiv-1310-3863 | Estimating Time-varying Brain Connectivity Networks from Functional MRI Time Series | http://arxiv.org/abs/1310.3863 | id:1310.3863 author:Ricardo Pio Monti, Peter Hellyer, David Sharp, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana category:stat.ML stat.AP  published:2013-10-14 summary:Understanding the functional architecture of the brain in terms of networks is becoming increasingly common. In most fMRI applications functional networks are assumed to be stationary, resulting in a single network estimated for the entire time course. However recent results suggest that the connectivity between brain regions is highly non-stationary even at rest. As a result, there is a need for new brain imaging methodologies that comprehensively account for the dynamic (i.e., non-stationary) nature of the fMRI data. In this work we propose the Smooth Incremental Graphical Lasso Estimation (SINGLE) algorithm which estimates dynamic brain networks from fMRI data. We apply the SINGLE algorithm to functional MRI data from 24 healthy patients performing a choice-response task to demonstrate the dynamic changes in network structure that accompany a simple but attentionally demanding cognitive task. Using graph theoretic measures we show that the Right Inferior Frontal Gyrus, frequently reported as playing an important role in cognitive control, dynamically changes with the task. Our results suggest that the Right Inferior Frontal Gyrus plays a fundamental role in the attention and executive function during cognitively demanding tasks and may play a key role in regulating the balance between other brain regions. version:2
arxiv-1404-3312 | Shrinkage Optimized Directed Information using Pictorial Structures for Action Recognition | http://arxiv.org/abs/1404.3312 | id:1404.3312 author:Xu Chen, Alfred Hero, Silvio Savarese category:cs.CV  published:2014-04-12 summary:In this paper, we propose a novel action recognition framework. The method uses pictorial structures and shrinkage optimized directed information assessment (SODA) coupled with Markov Random Fields called SODA+MRF to model the directional temporal dependency and bidirectional spatial dependency. As a variant of mutual information, directional information captures the directional information flow and temporal structure of video sequences across frames. Meanwhile, within each frame, Markov random fields are utilized to model the spatial relations among different parts of a human body and the body parts of different people. The proposed SODA+MRF model is robust to view point transformations and detect complex interactions accurately. We compare the proposed method against several baseline methods to highlight the effectiveness of the SODA+MRF model. We demonstrate that our algorithm has superior action recognition performance on the UCF action recognition dataset, the Olympic sports dataset and the collective activity dataset over several state-of-the-art methods. version:1
arxiv-1404-3291 | Cost-Effective HITs for Relative Similarity Comparisons | http://arxiv.org/abs/1404.3291 | id:1404.3291 author:Michael J. Wilber, Iljung S. Kwak, Serge J. Belongie category:cs.CV cs.LG  published:2014-04-12 summary:Similarity comparisons of the form "Is object a more similar to b than to c?" are useful for computer vision and machine learning applications. Unfortunately, an embedding of $n$ points is specified by $n^3$ triplets, making collecting every triplet an expensive task. In noticing this difficulty, other researchers have investigated more intelligent triplet sampling techniques, but they do not study their effectiveness or their potential drawbacks. Although it is important to reduce the number of collected triplets, it is also important to understand how best to display a triplet collection task to a user. In this work we explore an alternative display for collecting triplets and analyze the monetary cost and speed of the display. We propose best practices for creating cost effective human intelligence tasks for collecting triplets. We show that rather than changing the sampling algorithm, simple changes to the crowdsourcing UI can lead to much higher quality embeddings. We also provide a dataset as well as the labels collected from crowd workers. version:1
arxiv-1404-3290 | Motion-Compensated Coding and Frame-Rate Up-Conversion: Models and Analysis | http://arxiv.org/abs/1404.3290 | id:1404.3290 author:Yehuda Dar, Alfred M. Bruckstein category:cs.MM cs.CV  published:2014-04-12 summary:Block-based motion estimation (ME) and compensation (MC) techniques are widely used in modern video processing algorithms and compression systems. The great variety of video applications and devices results in numerous compression specifications. Specifically, there is a diversity of frame-rates and bit-rates. In this paper, we study the effect of frame-rate and compression bit-rate on block-based ME and MC as commonly utilized in inter-frame coding and frame-rate up conversion (FRUC). This joint examination yields a comprehensive foundation for comparing MC procedures in coding and FRUC. First, the video signal is modeled as a noisy translational motion of an image. Then, we theoretically model the motion-compensated prediction of an available and absent frames as in coding and FRUC applications, respectively. The theoretic MC-prediction error is further analyzed and its autocorrelation function is calculated for coding and FRUC applications. We show a linear relation between the variance of the MC-prediction error and temporal-distance. While the affecting distance in MC-coding is between the predicted and reference frames, MC-FRUC is affected by the distance between the available frames used for the interpolation. Moreover, the dependency in temporal-distance implies an inverse effect of the frame-rate. FRUC performance analysis considers the prediction error variance, since it equals to the mean-squared-error of the interpolation. However, MC-coding analysis requires the entire autocorrelation function of the error; hence, analytic simplicity is beneficial. Therefore, we propose two constructions of a separable autocorrelation function for prediction error in MC-coding. We conclude by comparing our estimations with experimental results. version:1
arxiv-1404-3233 | Pagination: It's what you say, not how long it takes to say it | http://arxiv.org/abs/1404.3233 | id:1404.3233 author:Joshua Hailpern, Niranjan Damera Venkata, Marina Danilevsky category:cs.CL cs.IR I.7.2; I.7.4  published:2014-04-11 summary:Pagination - the process of determining where to break an article across pages in a multi-article layout is a common layout challenge for most commercially printed newspapers and magazines. To date, no one has created an algorithm that determines a minimal pagination break point based on the content of the article. Existing approaches for automatic multi-article layout focus exclusively on maximizing content (number of articles) and optimizing aesthetic presentation (e.g., spacing between articles). However, disregarding the semantic information within the article can lead to overly aggressive cutting, thereby eliminating key content and potentially confusing the reader, or setting too generous of a break point, thereby leaving in superfluous content and making automatic layout more difficult. This is one of the remaining challenges on the path from manual layouts to fully automated processes that still ensure article content quality. In this work, we present a new approach to calculating a document minimal break point for the task of pagination. Our approach uses a statistical language model to predict minimal break points based on the semantic content of an article. We then compare 4 novel candidate approaches, and 4 baselines (currently in use by layout algorithms). Results from this experiment show that one of our approaches strongly outperforms the baselines and alternatives. Results from a second study suggest that humans are not able to agree on a single "best" break point. Therefore, this work shows that a semantic-based lower bound break point prediction is necessary for ideal automated document synthesis within a real-world context. version:1
arxiv-1404-3219 | Estimating nonlinear regression errors without doing regression | http://arxiv.org/abs/1404.3219 | id:1404.3219 author:Hong Pi, Carsten Peterson category:stat.ML nlin.CD q-fin.ST  published:2014-04-11 summary:A method for estimating nonlinear regression errors and their distributions without performing regression is presented. Assuming continuity of the modeling function the variance is given in terms of conditional probabilities extracted from the data. For N data points the computational demand is N2. Comparing the predicted residual errors with those derived from a linear model assumption provides a signal for nonlinearity. The method is successfully illustrated with data generated by the Ikeda and Lorenz maps augmented with noise. As a by-product the embedding dimensions of these maps are also extracted. version:1
arxiv-1404-3203 | Compressive classification and the rare eclipse problem | http://arxiv.org/abs/1404.3203 | id:1404.3203 author:Afonso S. Bandeira, Dustin G. Mixon, Benjamin Recht category:cs.LG cs.IT math.IT math.ST stat.TH  published:2014-04-11 summary:This paper addresses the fundamental question of when convex sets remain disjoint after random projection. We provide an analysis using ideas from high-dimensional convex geometry. For ellipsoids, we provide a bound in terms of the distance between these ellipsoids and simple functions of their polynomial coefficients. As an application, this theorem provides bounds for compressive classification of convex sets. Rather than assuming that the data to be classified is sparse, our results show that the data can be acquired via very few measurements yet will remain linearly separable. We demonstrate the feasibility of this approach in the context of hyperspectral imaging. version:1
arxiv-1404-3190 | Pareto-Path Multi-Task Multiple Kernel Learning | http://arxiv.org/abs/1404.3190 | id:1404.3190 author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG  published:2014-04-11 summary:A traditional and intuitively appealing Multi-Task Multiple Kernel Learning (MT-MKL) method is to optimize the sum (thus, the average) of objective functions with (partially) shared kernel function, which allows information sharing amongst tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO) problem, which considers the concurrent optimization of all task objectives involved in the Multi-Task Learning (MTL) problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel Support Vector Machine (SVM) MT-MKL framework, that considers an implicitly-defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of achieving better classification performance, when compared to other similar MTL approaches. version:1
arxiv-1404-3184 | Decreasing Weighted Sorted $\ell_1$ Regularization | http://arxiv.org/abs/1404.3184 | id:1404.3184 author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT cs.LG math.IT  published:2014-04-11 summary:We consider a new family of regularizers, termed {\it weighted sorted $\ell_1$ norms} (WSL1), which generalizes the recently introduced {\it octagonal shrinkage and clustering algorithm for regression} (OSCAR) and also contains the $\ell_1$ and $\ell_{\infty}$ norms as particular instances. We focus on a special case of the WSL1, the {\sl decreasing WSL1} (DWSL1), where the elements of the argument vector are sorted in non-increasing order and the weights are also non-increasing. In this paper, after showing that the DWSL1 is indeed a norm, we derive two key tools for its use as a regularizer: the dual norm and the Moreau proximity operator. version:1
arxiv-1401-6333 | The Sampling-and-Learning Framework: A Statistical View of Evolutionary Algorithms | http://arxiv.org/abs/1401.6333 | id:1401.6333 author:Yang Yu, Hong Qian category:cs.NE cs.LG  published:2014-01-24 summary:Evolutionary algorithms (EAs), a large class of general purpose optimization algorithms inspired from the natural phenomena, are widely used in various industrial optimizations and often show excellent performance. This paper presents an attempt towards revealing their general power from a statistical view of EAs. By summarizing a large range of EAs into the sampling-and-learning framework, we show that the framework directly admits a general analysis on the probable-absolute-approximate (PAA) query complexity. We particularly focus on the framework with the learning subroutine being restricted as a binary classification, which results in the sampling-and-classification (SAC) algorithms. With the help of the learning theory, we obtain a general upper bound on the PAA query complexity of SAC algorithms. We further compare SAC algorithms with the uniform search in different situations. Under the error-target independence condition, we show that SAC algorithms can achieve polynomial speedup to the uniform search, but not super-polynomial speedup. Under the one-side-error condition, we show that super-polynomial speedup can be achieved. This work only touches the surface of the framework. Its power under other conditions is still open. version:2
arxiv-1404-3026 | On the Ground Validation of Online Diagnosis with Twitter and Medical Records | http://arxiv.org/abs/1404.3026 | id:1404.3026 author:Todd Bodnar, Victoria C Barclay, Nilam Ram, Conrad S Tucker, Marcel Salathé category:cs.SI cs.CL cs.LG I.2.1  published:2014-04-11 summary:Social media has been considered as a data source for tracking disease. However, most analyses are based on models that prioritize strong correlation with population-level disease rates over determining whether or not specific individual users are actually sick. Taking a different approach, we develop a novel system for social-media based disease detection at the individual level using a sample of professionally diagnosed individuals. Specifically, we develop a system for making an accurate influenza diagnosis based on an individual's publicly available Twitter data. We find that about half (17/35 = 48.57%) of the users in our sample that were sick explicitly discuss their disease on Twitter. By developing a meta classifier that combines text analysis, anomaly detection, and social network analysis, we are able to diagnose an individual with greater than 99% accuracy even if she does not discuss her health. version:1
arxiv-1404-2999 | A Reverse Hierarchy Model for Predicting Eye Fixations | http://arxiv.org/abs/1404.2999 | id:1404.2999 author:Tianlin Shi, Liang Ming, Xiaolin Hu category:cs.CV  published:2014-04-11 summary:A number of psychological and physiological evidences suggest that early visual attention works in a coarse-to-fine way, which lays a basis for the reverse hierarchy theory (RHT). This theory states that attention propagates from the top level of the visual hierarchy that processes gist and abstract information of input, to the bottom level that processes local details. Inspired by the theory, we develop a computational model for saliency detection in images. First, the original image is downsampled to different scales to constitute a pyramid. Then, saliency on each layer is obtained by image super-resolution reconstruction from the layer above, which is defined as unpredictability from this coarse-to-fine reconstruction. Finally, saliency on each layer of the pyramid is fused into stochastic fixations through a probabilistic model, where attention initiates from the top layer and propagates downward through the pyramid. Extensive experiments on two standard eye-tracking datasets show that the proposed method can achieve competitive results with state-of-the-art models. version:1
arxiv-1404-2997 | Automatic Detection of Reuses and Citations in Literary Texts | http://arxiv.org/abs/1404.2997 | id:1404.2997 author:Jean-Gabriel Ganascia, Pierre Glaudes, Andrea Del Lungo category:cs.CL cs.DL  published:2014-04-11 summary:For more than forty years now, modern theories of literature (Compagnon, 1979) insist on the role of paraphrases, rewritings, citations, reciprocal borrowings and mutual contributions of any kinds. The notions of intertextuality, transtextuality, hypertextuality/hypotextuality, were introduced in the seventies and eighties to approach these phenomena. The careful analysis of these references is of particular interest in evaluating the distance that the creator voluntarily introduces with his/her masters. Phoebus is collaborative project that makes computer scientists from the University Pierre and Marie Curie (LIP6-UPMC) collaborate with the literary teams of Paris-Sorbonne University with the aim to develop efficient tools for literary studies that take advantage of modern computer science techniques. In this context, we have developed a piece of software that automatically detects and explores networks of textual reuses in classical literature. This paper describes the principles on which is based this program, the significant results that have already been obtained and the perspectives for the near future. version:1
arxiv-1306-6677 | Supersparse Linear Integer Models for Interpretable Classification | http://arxiv.org/abs/1306.6677 | id:1306.6677 author:Berk Ustun, Stefano Tracà, Cynthia Rudin category:stat.ML stat.AP  published:2013-06-27 summary:Scoring systems are classification models that only require users to add, subtract and multiply a few meaningful numbers to make a prediction. These models are often used because they are practical and interpretable. In this paper, we introduce an off-the-shelf tool to create scoring systems that both accurate and interpretable, known as a Supersparse Linear Integer Model (SLIM). SLIM is a discrete optimization problem that minimizes the 0-1 loss to encourage a high level of accuracy, regularizes the L0-norm to encourage a high level of sparsity, and constrains coefficients to a set of interpretable values. We illustrate the practical and interpretable nature of SLIM scoring systems through applications in medicine and criminology, and show that they are are accurate and sparse in comparison to state-of-the-art classification models using numerical experiments. version:6
arxiv-1404-2986 | A Tutorial on Independent Component Analysis | http://arxiv.org/abs/1404.2986 | id:1404.2986 author:Jonathon Shlens category:cs.LG stat.ML  published:2014-04-11 summary:Independent component analysis (ICA) has become a standard data analysis technique applied to an array of problems in signal processing and machine learning. This tutorial provides an introduction to ICA based on linear algebra formulating an intuition for ICA from first principles. The goal of this tutorial is to provide a solid foundation on this advanced topic so that one might learn the motivation behind ICA, learn why and when to apply this technique and in the process gain an introduction to this exciting field of active research. version:1
arxiv-1404-3610 | Targeting HIV-related Medication Side Effects and Sentiment Using Twitter Data | http://arxiv.org/abs/1404.3610 | id:1404.3610 author:Cosme Adrover, Todd Bodnar, Marcel Salathe category:cs.SI cs.CL cs.IR  published:2014-04-11 summary:We present a descriptive analysis of Twitter data. Our study focuses on extracting the main side effects associated with HIV treatments. The crux of our work was the identification of personal tweets referring to HIV. We summarize our results in an infographic aimed at the general public. In addition, we present a measure of user sentiment based on hand-rated tweets. version:1
arxiv-1403-6508 | Multi-agent Inverse Reinforcement Learning for Zero-sum Games | http://arxiv.org/abs/1403.6508 | id:1403.6508 author:Xiaomin Lin, Peter A. Beling, Randy Cogill category:cs.GT cs.AI cs.LG  published:2014-03-25 summary:In this paper we introduce a Bayesian framework for solving a class of problems termed Multi-agent Inverse Reinforcement Learning (MIRL). Compared to the well-known Inverse Reinforcement Learning (IRL) problem, MIRL is formalized in the context of a stochastic game rather than a Markov decision process (MDP). Games bring two primary challenges: First, the concept of optimality, central to MDPs, loses its meaning and must be replaced with a more general solution concept, such as the Nash equilibrium. Second, the non-uniqueness of equilibria means that in MIRL, in addition to multiple reasonable solutions for a given inversion model, there may be multiple inversion models that are all equally sensible approaches to solving the problem. We establish a theoretical foundation for competitive two-agent MIRL problems and propose a Bayesian optimization algorithm to solve the problem. We focus on the case of two-person zero-sum stochastic games, developing a generative model for the likelihood of unknown rewards of agents given observed game play assuming that the two agents follow a minimax bipolicy. As a numerical illustration, we apply our method in the context of an abstract soccer game. For the soccer game, we investigate relationships between the extent of prior information and the quality of learned rewards. Results suggest that covariance structure is more important than mean value in reward priors. version:2
arxiv-1404-2948 | Gradient-based Laplacian Feature Selection | http://arxiv.org/abs/1404.2948 | id:1404.2948 author:Bo Wang, Anna Goldenberg category:cs.LG  published:2014-04-10 summary:Analysis of high dimensional noisy data is of essence across a variety of research fields. Feature selection techniques are designed to find the relevant feature subset that can facilitate classification or pattern detection. Traditional (supervised) feature selection methods utilize label information to guide the identification of relevant feature subsets. In this paper, however, we consider the unsupervised feature selection problem. Without the label information, it is particularly difficult to identify a small set of relevant features due to the noisy nature of real-world data which corrupts the intrinsic structure of the data. Our Gradient-based Laplacian Feature Selection (GLFS) selects important features by minimizing the variance of the Laplacian regularized least squares regression model. With $\ell_1$ relaxation, GLFS can find a sparse subset of features that is relevant to the Laplacian manifolds. Extensive experiments on simulated, three real-world object recognition and two computational biology datasets, have illustrated the power and superior performance of our approach over multiple state-of-the-art unsupervised feature selection methods. Additionally, we show that GLFS selects a sparser set of more relevant features in a supervised setting outperforming the popular elastic net methodology. version:1
arxiv-1311-5401 | Clustering and Relational Ambiguity: from Text Data to Natural Data | http://arxiv.org/abs/1311.5401 | id:1311.5401 author:Nicolas Turenne category:cs.CL cs.IR  published:2013-11-21 summary:Text data is often seen as "take-away" materials with little noise and easy to process information. Main questions are how to get data and transform them into a good document format. But data can be sensitive to noise oftenly called ambiguities. Ambiguities are aware from a long time, mainly because polysemy is obvious in language and context is required to remove uncertainty. I claim in this paper that syntactic context is not suffisant to improve interpretation. In this paper I try to explain that firstly noise can come from natural data themselves, even involving high technology, secondly texts, seen as verified but meaningless, can spoil content of a corpus; it may lead to contradictions and background noise. version:2
arxiv-1404-2878 | Overview of Stemming Algorithms for Indian and Non-Indian Languages | http://arxiv.org/abs/1404.2878 | id:1404.2878 author:Dalwadi Bijal, Suthar Sanket category:cs.CL  published:2014-04-10 summary:Stemming is a pre-processing step in Text Mining applications as well as a very common requirement of Natural Language processing functions. Stemming is the process for reducing inflected words to their stem. The main purpose of stemming is to reduce different grammatical forms / word forms of a word like its noun, adjective, verb, adverb etc. to its root form. Stemming is widely uses in Information Retrieval system and reduces the size of index files. We can say that the goal of stemming is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. In this paper we have discussed different stemming algorithm for non-Indian and Indian language, methods of stemming, accuracy and errors. version:1
arxiv-1312-6114 | Auto-Encoding Variational Bayes | http://arxiv.org/abs/1312.6114 | id:1312.6114 author:Diederik P Kingma, Max Welling category:stat.ML cs.LG  published:2013-12-20 summary:How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results. version:10
arxiv-1405-6166 | Real Time Speckle Image De-Noising | http://arxiv.org/abs/1405.6166 | id:1405.6166 author:D. Sachin Kumar, P. R. Seshadri, N. Vaishnav, Dr. Saraswathi Janaki category:cs.CV  published:2014-04-10 summary:The paper presents real time speckle de-noising based on activity computation algorithm and wavelet transform. Speckles arise in an image when laser light is reflected from an illuminated surface. The process involves detection of speckles in an image by obtaining a number of frames of the same object under different illumination or angle and comparing the frames for the granular computation and de-noising the same on presence of greater activity index. The project can be implemented in FPGA (Field Programmable Gate Array) technology. The results can be shown that the used activity computation algorithm and wavelet transform has better accuracy in the process of speckle detection and de-noising. version:1
arxiv-1404-2772 | A New Clustering Approach for Anomaly Intrusion Detection | http://arxiv.org/abs/1404.2772 | id:1404.2772 author:Ravi Ranjan, G. Sahoo category:cs.DC cs.CR cs.LG  published:2014-04-10 summary:Recent advances in technology have made our work easier compare to earlier times. Computer network is growing day by day but while discussing about the security of computers and networks it has always been a major concerns for organizations varying from smaller to larger enterprises. It is true that organizations are aware of the possible threats and attacks so they always prepare for the safer side but due to some loopholes attackers are able to make attacks. Intrusion detection is one of the major fields of research and researchers are trying to find new algorithms for detecting intrusions. Clustering techniques of data mining is an interested area of research for detecting possible intrusions and attacks. This paper presents a new clustering approach for anomaly intrusion detection by using the approach of K-medoids method of clustering and its certain modifications. The proposed algorithm is able to achieve high detection rate and overcomes the disadvantages of K-means algorithm. version:1
arxiv-1402-2331 | Computational Limits for Matrix Completion | http://arxiv.org/abs/1402.2331 | id:1402.2331 author:Moritz Hardt, Raghu Meka, Prasad Raghavendra, Benjamin Weitz category:cs.CC cs.LG  published:2014-02-10 summary:Matrix Completion is the problem of recovering an unknown real-valued low-rank matrix from a subsample of its entries. Important recent results show that the problem can be solved efficiently under the assumption that the unknown matrix is incoherent and the subsample is drawn uniformly at random. Are these assumptions necessary? It is well known that Matrix Completion in its full generality is NP-hard. However, little is known if make additional assumptions such as incoherence and permit the algorithm to output a matrix of slightly higher rank. In this paper we prove that Matrix Completion remains computationally intractable even if the unknown matrix has rank $4$ but we are allowed to output any constant rank matrix, and even if additionally we assume that the unknown matrix is incoherent and are shown $90%$ of the entries. This result relies on the conjectured hardness of the $4$-Coloring problem. We also consider the positive semidefinite Matrix Completion problem. Here we show a similar hardness result under the standard assumption that $\mathrm{P}\ne \mathrm{NP}.$ Our results greatly narrow the gap between existing feasibility results and computational lower bounds. In particular, we believe that our results give the first complexity-theoretic justification for why distributional assumptions are needed beyond the incoherence assumption in order to obtain positive results. On the technical side, we contribute several new ideas on how to encode hard combinatorial problems in low-rank optimization problems. We hope that these techniques will be helpful in further understanding the computational limits of Matrix Completion and related problems. version:2
arxiv-1404-2655 | Open problem: Tightness of maximum likelihood semidefinite relaxations | http://arxiv.org/abs/1404.2655 | id:1404.2655 author:Afonso S. Bandeira, Yuehaw Khoo, Amit Singer category:math.OC cs.LG stat.ML  published:2014-04-10 summary:We have observed an interesting, yet unexplained, phenomenon: Semidefinite programming (SDP) based relaxations of maximum likelihood estimators (MLE) tend to be tight in recovery problems with noisy data, even when MLE cannot exactly recover the ground truth. Several results establish tightness of SDP based relaxations in the regime where exact recovery from MLE is possible. However, to the best of our knowledge, their tightness is not understood beyond this regime. As an illustrative example, we focus on the generalized Procrustes problem. version:1
arxiv-1303-2607 | Joint optimization of fitting & matching in multi-view reconstruction | http://arxiv.org/abs/1303.2607 | id:1303.2607 author:Hossam Isack, Yuri Boykov category:cs.CV  published:2013-03-11 summary:Many standard approaches for geometric model fitting are based on pre-matched image features. Typically, such pre-matching uses only feature appearances (e.g. SIFT) and a large number of non-unique features must be discarded in order to control the false positive rate. In contrast, we solve feature matching and multi-model fitting problems in a joint optimization framework. This paper proposes several fit-&-match energy formulations based on a generalization of the assignment problem. We developed an efficient solver based on min-cost-max-flow algorithm that finds near optimal solutions. Our approach significantly increases the number of detected matches. In practice, energy-based joint fitting & matching allows to increase the distance between view-points previously restricted by robustness of local SIFT-matching and to improve the model fitting accuracy when compared to state-of-the-art multi-model fitting techniques. version:2
arxiv-1311-6887 | Modeling Radiometric Uncertainty for Vision with Tone-mapped Color Images | http://arxiv.org/abs/1311.6887 | id:1311.6887 author:Ayan Chakrabarti, Ying Xiong, Baochen Sun, Trevor Darrell, Daniel Scharstein, Todd Zickler, Kate Saenko category:cs.CV  published:2013-11-27 summary:To produce images that are suitable for display, tone-mapping is widely used in digital cameras to map linear color measurements into narrow gamuts with limited dynamic range. This introduces non-linear distortion that must be undone, through a radiometric calibration process, before computer vision systems can analyze such photographs radiometrically. This paper considers the inherent uncertainty of undoing the effects of tone-mapping. We observe that this uncertainty varies substantially across color space, making some pixels more reliable than others. We introduce a model for this uncertainty and a method for fitting it to a given camera or imaging pipeline. Once fit, the model provides for each pixel in a tone-mapped digital photograph a probability distribution over linear scene colors that could have induced it. We demonstrate how these distributions can be useful for visual inference by incorporating them into estimation algorithms for a representative set of vision tasks. version:2
arxiv-1404-2571 | RANCOR: Non-Linear Image Registration with Total Variation Regularization | http://arxiv.org/abs/1404.2571 | id:1404.2571 author:Martin Rajchl, John S. H. Baxter, Wu Qiu, Ali R. Khan, Aaron Fenster, Terry M. Peters, Jing Yuan category:cs.CV  published:2014-04-09 summary:Optimization techniques have been widely used in deformable registration, allowing for the incorporation of similarity metrics with regularization mechanisms. These regularization mechanisms are designed to mitigate the effects of trivial solutions to ill-posed registration problems and to otherwise ensure the resulting deformation fields are well-behaved. This paper introduces a novel deformable registration algorithm, RANCOR, which uses iterative convexification to address deformable registration problems under total-variation regularization. Initial comparative results against four state-of-the-art registration algorithms are presented using the Internet Brain Segmentation Repository (IBSR) database. version:1
arxiv-1404-2553 | Noisy Optimization: Convergence with a Fixed Number of Resamplings | http://arxiv.org/abs/1404.2553 | id:1404.2553 author:Marie-Liesse Cauwet category:math.OC stat.ML  published:2014-04-09 summary:It is known that evolution strategies in continuous domains might not converge in the presence of noise. It is also known that, under mild assumptions, and using an increasing number of resamplings, one can mitigate the effect of additive noise and recover convergence. We show new sufficient conditions for the convergence of an evolutionary algorithm with constant number of resamplings; in particular, we get fast rates (log-linear convergence) provided that the variance decreases around the optimum slightly faster than in the so-called multiplicative noise model. Keywords: Noisy optimization, evolutionary algorithm, theory. version:1
arxiv-1404-2268 | A Compact Linear Programming Relaxation for Binary Sub-modular MRF | http://arxiv.org/abs/1404.2268 | id:1404.2268 author:Junyan Wang, Sai-Kit Yeung category:cs.CV  published:2014-04-09 summary:We propose a novel compact linear programming (LP) relaxation for binary sub-modular MRF in the context of object segmentation. Our model is obtained by linearizing an $l_1^+$-norm derived from the quadratic programming (QP) form of the MRF energy. The resultant LP model contains significantly fewer variables and constraints compared to the conventional LP relaxation of the MRF energy. In addition, unlike QP which can produce ambiguous labels, our model can be viewed as a quasi-total-variation minimization problem, and it can therefore preserve the discontinuities in the labels. We further establish a relaxation bound between our LP model and the conventional LP model. In the experiments, we demonstrate our method for the task of interactive object segmentation. Our LP model outperforms QP when converting the continuous labels to binary labels using different threshold values on the entire Oxford interactive segmentation dataset. The computational complexity of our LP is of the same order as that of the QP, and it is significantly lower than the conventional LP relaxation. version:1
arxiv-1403-3371 | Spectral Correlation Hub Screening of Multivariate Time Series | http://arxiv.org/abs/1403.3371 | id:1403.3371 author:Hamed Firouzi, Dennis Wei, Alfred O. Hero III category:stat.OT cs.LG stat.AP  published:2014-03-13 summary:This chapter discusses correlation analysis of stationary multivariate Gaussian time series in the spectral or Fourier domain. The goal is to identify the hub time series, i.e., those that are highly correlated with a specified number of other time series. We show that Fourier components of the time series at different frequencies are asymptotically statistically independent. This property permits independent correlation analysis at each frequency, alleviating the computational and statistical challenges of high-dimensional time series. To detect correlation hubs at each frequency, an existing correlation screening method is extended to the complex numbers to accommodate complex-valued Fourier components. We characterize the number of hub discoveries at specified correlation and degree thresholds in the regime of increasing dimension and fixed sample size. The theory specifies appropriate thresholds to apply to sample correlation matrices to detect hubs and also allows statistical significance to be attributed to hub discoveries. Numerical results illustrate the accuracy of the theory and the usefulness of the proposed spectral framework. version:2
arxiv-1402-2447 | A comparison of linear and non-linear calibrations for speaker recognition | http://arxiv.org/abs/1402.2447 | id:1402.2447 author:Niko Brümmer, Albert Swart, David van Leeuwen category:stat.ML cs.LG  published:2014-02-11 summary:In recent work on both generative and discriminative score to log-likelihood-ratio calibration, it was shown that linear transforms give good accuracy only for a limited range of operating points. Moreover, these methods required tailoring of the calibration training objective functions in order to target the desired region of best accuracy. Here, we generalize the linear recipes to non-linear ones. We experiment with a non-linear, non-parametric, discriminative PAV solution, as well as parametric, generative, maximum-likelihood solutions that use Gaussian, Student's T and normal-inverse-Gaussian score distributions. Experiments on NIST SRE'12 scores suggest that the non-linear methods provide wider ranges of optimal accuracy and can be trained without having to resort to objective function tailoring. version:2
arxiv-1312-6062 | Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error | http://arxiv.org/abs/1312.6062 | id:1312.6062 author:David Buchaca, Enrique Romero, Ferran Mazzanti, Jordi Delgado category:cs.LG  published:2013-12-20 summary:Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning. version:2
arxiv-0911-2974 | A Dynamic Near-Optimal Algorithm for Online Linear Programming | http://arxiv.org/abs/0911.2974 | id:0911.2974 author:Shipra Agrawal, Zizhuo Wang, Yinyu Ye category:cs.DS cs.LG  published:2009-11-16 summary:A natural optimization model that formulates many online resource allocation and revenue management problems is the online linear program (LP) in which the constraint matrix is revealed column by column along with the corresponding objective coefficient. In such a model, a decision variable has to be set each time a column is revealed without observing the future inputs and the goal is to maximize the overall objective function. In this paper, we provide a near-optimal algorithm for this general class of online problems under the assumption of random order of arrival and some mild conditions on the size of the LP right-hand-side input. Specifically, our learning-based algorithm works by dynamically updating a threshold price vector at geometric time intervals, where the dual prices learned from the revealed columns in the previous period are used to determine the sequential decisions in the current period. Due to the feature of dynamic learning, the competitiveness of our algorithm improves over the past study of the same problem. We also present a worst-case example showing that the performance of our algorithm is near-optimal. version:3
arxiv-1404-2353 | Power System Parameters Forecasting Using Hilbert-Huang Transform and Machine Learning | http://arxiv.org/abs/1404.2353 | id:1404.2353 author:Victor Kurbatsky, Nikita Tomin, Vadim Spiryaev, Paul Leahy, Denis Sidorov, Alexei Zhukov category:cs.LG stat.ML 62M10  91B84  published:2014-04-09 summary:A novel hybrid data-driven approach is developed for forecasting power system parameters with the goal of increasing the efficiency of short-term forecasting studies for non-stationary time-series. The proposed approach is based on mode decomposition and a feature analysis of initial retrospective data using the Hilbert-Huang transform and machine learning algorithms. The random forests and gradient boosting trees learning techniques were examined. The decision tree techniques were used to rank the importance of variables employed in the forecasting models. The Mean Decrease Gini index is employed as an impurity function. The resulting hybrid forecasting models employ the radial basis function neural network and support vector regression. Apart from introduction and references the paper is organized as follows. The section 2 presents the background and the review of several approaches for short-term forecasting of power system parameters. In the third section a hybrid machine learning-based algorithm using Hilbert-Huang transform is developed for short-term forecasting of power system parameters. Fourth section describes the decision tree learning algorithms used for the issue of variables importance. Finally in section six the experimental results in the following electric power problems are presented: active power flow forecasting, electricity price forecasting and for the wind speed and direction forecasting. version:1
arxiv-1404-2885 | A Networks and Machine Learning Approach to Determine the Best College Coaches of the 20th-21st Centuries | http://arxiv.org/abs/1404.2885 | id:1404.2885 author:Tian-Shun Jiang, Zachary Polizzi, Christopher Yuan category:stat.AP cs.LG cs.SI  published:2014-04-08 summary:Our objective is to find the five best college sports coaches of past century for three different sports. We decided to look at men's basketball, football, and baseball. We wanted to use an approach that could definitively determine team skill from the games played, and then use a machine-learning algorithm to calculate the correct coach skills for each team in a given year. We created a networks-based model to calculate team skill from historical game data. A digraph was created for each year in each sport. Nodes represented teams, and edges represented a game played between two teams. The arrowhead pointed towards the losing team. We calculated the team skill of each graph using a right-hand eigenvector centrality measure. This way, teams that beat good teams will be ranked higher than teams that beat mediocre teams. The eigenvector centrality rankings for most years were well correlated with tournament performance and poll-based rankings. We assumed that the relationship between coach skill $C_s$, player skill $P_s$, and team skill $T_s$ was $C_s \cdot P_s = T_s$. We then created a function to describe the probability that a given score difference would occur based on player skill and coach skill. We multiplied the probabilities of all edges in the network together to find the probability that the correct network would occur with any given player skill and coach skill matrix. We was able to determine player skill as a function of team skill and coach skill, eliminating the need to optimize two unknown matrices. The top five coaches in each year were noted, and the top coach of all time was calculated by dividing the number of times that coach ranked in the yearly top five by the years said coach had been active. version:1
arxiv-1203-0550 | Algorithms for Learning Kernels Based on Centered Alignment | http://arxiv.org/abs/1203.0550 | id:1203.0550 author:Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh category:cs.LG cs.AI  published:2012-03-02 summary:This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difficult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classification and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efficient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classification and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classification and regression. version:2
arxiv-1404-2189 | Data mining for censored time-to-event data: A Bayesian network model for predicting cardiovascular risk from electronic health record data | http://arxiv.org/abs/1404.2189 | id:1404.2189 author:Sunayan Bandyopadhyay, Julian Wolfson, David M. Vock, Gabriela Vazquez-Benitez, Gediminas Adomavicius, Mohamed Elidrisi, Paul E. Johnson, Patrick J. O'Connor category:stat.ML stat.AP  published:2014-04-08 summary:Models for predicting the risk of cardiovascular events based on individual patient characteristics are important tools for managing patient care. Most current and commonly used risk prediction models have been built from carefully selected epidemiological cohorts. However, the homogeneity and limited size of such cohorts restricts the predictive power and generalizability of these risk models to other populations. Electronic health data (EHD) from large health care systems provide access to data on large, heterogeneous, and contemporaneous patient populations. The unique features and challenges of EHD, including missing risk factor information, non-linear relationships between risk factors and cardiovascular event outcomes, and differing effects from different patient subgroups, demand novel machine learning approaches to risk model development. In this paper, we present a machine learning approach based on Bayesian networks trained on EHD to predict the probability of having a cardiovascular event within five years. In such data, event status may be unknown for some individuals as the event time is right-censored due to disenrollment and incomplete follow-up. Since many traditional data mining methods are not well-suited for such data, we describe how to modify both modelling and assessment techniques to account for censored observation times. We show that our approach can lead to better predictive performance than the Cox proportional hazards model (i.e., a regression-based approach commonly used for censored, time-to-event data) or a Bayesian network with {\em{ad hoc}} approaches to right-censoring. Our techniques are motivated by and illustrated on data from a large U.S. Midwestern health care system. version:1
arxiv-1404-2188 | A Convolutional Neural Network for Modelling Sentences | http://arxiv.org/abs/1404.2188 | id:1404.2188 author:Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom category:cs.CL  published:2014-04-08 summary:The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline. version:1
arxiv-1311-4291 | Minimum $n$-Rank Approximation via Iterative Hard Thresholding | http://arxiv.org/abs/1311.4291 | id:1311.4291 author:Min Zhang, Lei Yang, Zheng-Hai Huang category:math.OC stat.ML  published:2013-11-18 summary:The problem of recovering a low $n$-rank tensor is an extension of sparse recovery problem from the low dimensional space (matrix space) to the high dimensional space (tensor space) and has many applications in computer vision and graphics such as image inpainting and video inpainting. In this paper, we consider a new tensor recovery model, named as minimum $n$-rank approximation (MnRA), and propose an appropriate iterative hard thresholding algorithm with giving the upper bound of the $n$-rank in advance. The convergence analysis of the proposed algorithm is also presented. Particularly, we show that for the noiseless case, the linear convergence with rate $\frac{1}{2}$ can be obtained for the proposed algorithm under proper conditions. Additionally, combining an effective heuristic for determining $n$-rank, we can also apply the proposed algorithm to solve MnRA when $n$-rank is unknown in advance. Some preliminary numerical results on randomly generated and real low $n$-rank tensor completion problems are reported, which show the efficiency of the proposed algorithms. version:2
arxiv-1404-2124 | A Naive Bayes machine learning approach to risk prediction using censored, time-to-event data | http://arxiv.org/abs/1404.2124 | id:1404.2124 author:Julian Wolfson, Sunayan Bandyopadhyay, Mohamed Elidrisi, Gabriela Vazquez-Benitez, Donald Musgrove, Gediminas Adomavicius, Paul Johnson, Patrick O'Connor category:stat.ML  published:2014-04-08 summary:Predicting an individual's risk of experiencing a future clinical outcome is a statistical task with important consequences for both practicing clinicians and public health experts. Modern observational databases such as electronic health records (EHRs) provide an alternative to the longitudinal cohort studies traditionally used to construct risk models, bringing with them both opportunities and challenges. Large sample sizes and detailed covariate histories enable the use of sophisticated machine learning techniques to uncover complex associations and interactions, but observational databases are often ``messy,'' with high levels of missing data and incomplete patient follow-up. In this paper, we propose an adaptation of the well-known Naive Bayes (NB) machine learning approach for classification to time-to-event outcomes subject to censoring. We compare the predictive performance of our method to the Cox proportional hazards model which is commonly used for risk prediction in healthcare populations, and illustrate its application to prediction of cardiovascular risk using an EHR dataset from a large Midwest integrated healthcare system. version:1
arxiv-1404-2083 | Efficiency of conformalized ridge regression | http://arxiv.org/abs/1404.2083 | id:1404.2083 author:Evgeny Burnaev, Vladimir Vovk category:cs.LG stat.ML  published:2014-04-08 summary:Conformal prediction is a method of producing prediction sets that can be applied on top of a wide range of prediction algorithms. The method has a guaranteed coverage probability under the standard IID assumption regardless of whether the assumptions (often considerably more restrictive) of the underlying algorithm are satisfied. However, for the method to be really useful it is desirable that in the case where the assumptions of the underlying algorithm are satisfied, the conformal predictor loses little in efficiency as compared with the underlying algorithm (whereas being a conformal predictor, it has the stronger guarantee of validity). In this paper we explore the degree to which this additional requirement of efficiency is satisfied in the case of Bayesian ridge regression; we find that asymptotically conformal prediction sets differ little from ridge regression prediction intervals when the standard Bayesian assumptions are satisfied. version:1
arxiv-1404-2071 | Extracting a bilingual semantic grammar from FrameNet-annotated corpora | http://arxiv.org/abs/1404.2071 | id:1404.2071 author:Dana Dannélls, Normunds Grūzītis category:cs.CL  published:2014-04-08 summary:We present the creation of an English-Swedish FrameNet-based grammar in Grammatical Framework. The aim of this research is to make existing framenets computationally accessible for multilingual natural language applications via a common semantic grammar API, and to facilitate the porting of such grammar to other languages. In this paper, we describe the abstract syntax of the semantic grammar while focusing on its automatic extraction possibilities. We have extracted a shared abstract syntax from ~58,500 annotated sentences in Berkeley FrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). The abstract syntax defines 769 frame-specific valence patterns that cover 77.8% examples in BFN and 74.9% in SweFN belonging to the shared set of 471 frames. As a side result, we provide a unified method for comparing semantic and syntactic valence patterns across framenets. version:1
arxiv-1404-2014 | Entropy Computation of Document Images in Run-Length Compressed Domain | http://arxiv.org/abs/1404.2014 | id:1404.2014 author:P. Nagabhushan, Mohammed Javed, B. B. Chaudhuri category:cs.CV  published:2014-04-08 summary:Compression of documents, images, audios and videos have been traditionally practiced to increase the efficiency of data storage and transfer. However, in order to process or carry out any analytical computations, decompression has become an unavoidable pre-requisite. In this research work, we have attempted to compute the entropy, which is an important document analytic directly from the compressed documents. We use Conventional Entropy Quantifier (CEQ) and Spatial Entropy Quantifiers (SEQ) for entropy computations [1]. The entropies obtained are useful in applications like establishing equivalence, word spotting and document retrieval. Experiments have been performed with all the data sets of [1], at character, word and line levels taking compressed documents in run-length compressed domain. The algorithms developed are computational and space efficient, and results obtained match 100% with the results reported in [1]. version:1
arxiv-1404-2007 | A Permutation Approach for Selecting the Penalty Parameter in Penalized Model Selection | http://arxiv.org/abs/1404.2007 | id:1404.2007 author:Jeremy Sabourin, William Valdar, Andrew Nobel category:stat.ML  published:2014-04-08 summary:We describe a simple, efficient, permutation based procedure for selecting the penalty parameter in the LASSO. The procedure, which is intended for applications where variable selection is the primary focus, can be applied in a variety of structural settings, including generalized linear models. We briefly discuss connections between permutation selection and existing theory for the LASSO. In addition, we present a simulation study and an analysis of three real data sets in which permutation selection is compared with cross-validation (CV), the Bayesian information criterion (BIC), and a selection method based on recently developed testing procedures for the LASSO. version:1
arxiv-1404-2005 | Automatic Tracker Selection w.r.t Object Detection Performance | http://arxiv.org/abs/1404.2005 | id:1404.2005 author:Duc Phu Chau, François Bremond, Monique Thonnat, Slawomir Bak category:cs.CV  published:2014-04-08 summary:The tracking algorithm performance depends on video content. This paper presents a new multi-object tracking approach which is able to cope with video content variations. First the object detection is improved using Kanade- Lucas-Tomasi (KLT) feature tracking. Second, for each mobile object, an appropriate tracker is selected among a KLT-based tracker and a discriminative appearance-based tracker. This selection is supported by an online tracking evaluation. The approach has been experimented on three public video datasets. The experimental results show a better performance of the proposed approach compared to recent state of the art trackers. version:1
arxiv-1404-1999 | Notes on Generalized Linear Models of Neurons | http://arxiv.org/abs/1404.1999 | id:1404.1999 author:Jonathon Shlens category:cs.NE cs.LG q-bio.NC  published:2014-04-08 summary:Experimental neuroscience increasingly requires tractable models for analyzing and predicting the behavior of neurons and networks. The generalized linear model (GLM) is an increasingly popular statistical framework for analyzing neural data that is flexible, exhibits rich dynamic behavior and is computationally tractable (Paninski, 2004; Pillow et al., 2008; Truccolo et al., 2005). What follows is a brief summary of the primary equations governing the application of GLM's to spike trains with a few sentences linking this work to the larger statistical literature. Latter sections include extensions of a basic GLM to model spatio-temporal receptive fields as well as network activity in an arbitrary numbers of neurons. version:1
arxiv-1405-6168 | Human Face as human single identity | http://arxiv.org/abs/1405.6168 | id:1405.6168 author:Spits Warnars category:cs.CV  published:2014-04-08 summary:Human face as a physical human recognition can be used as a unique identity for computer to recognize human by transforming human face with face algorithm as simple text number which can be primary key for human. Human face as single identity for human will be done by making a huge and large world centre human face database, where the human face around the world will be recorded from time to time and from generation to generation. Architecture database will be divided become human face image database which will save human face images and human face output code which will save human face output code as a transformation human face image with face algorithm. As an improvement the slightly and simple human face output code database will make human face searching process become more fast. Transaction with human face as a transaction without card can make human no need their card for the transaction and office automation and banking system as an example for implementation architecture. As an addition suspect human face database can be extended for fighting crime and terrorism by doing surveillance and searching suspect human face around the world. version:1
arxiv-1204-0170 | A New Approach to Speeding Up Topic Modeling | http://arxiv.org/abs/1204.0170 | id:1204.0170 author:Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao category:cs.LG cs.IR  published:2012-04-01 summary:Latent Dirichlet allocation (LDA) is a widely-used probabilistic topic modeling paradigm, and recently finds many applications in computer vision and computational biology. In this paper, we propose a fast and accurate batch algorithm, active belief propagation (ABP), for training LDA. Usually batch LDA algorithms require repeated scanning of the entire corpus and searching the complete topic space. To process massive corpora having a large number of topics, the training iteration of batch LDA algorithms is often inefficient and time-consuming. To accelerate the training speed, ABP actively scans the subset of corpus and searches the subset of topic space for topic modeling, therefore saves enormous training time in each iteration. To ensure accuracy, ABP selects only those documents and topics that contribute to the largest residuals within the residual belief propagation (RBP) framework. On four real-world corpora, ABP performs around $10$ to $100$ times faster than state-of-the-art batch LDA algorithms with a comparable topic modeling accuracy. version:2
arxiv-1404-1982 | Aspect-Based Opinion Extraction from Customer reviews | http://arxiv.org/abs/1404.1982 | id:1404.1982 author:Amani K Samha, Yuefeng Li, Jinglan Zhang category:cs.CL cs.IR  published:2014-04-08 summary:Text is the main method of communicating information in the digital age. Messages, blogs, news articles, reviews, and opinionated information abound on the Internet. People commonly purchase products online and post their opinions about purchased items. This feedback is displayed publicly to assist others with their purchasing decisions, creating the need for a mechanism with which to extract and summarize useful information for enhancing the decision-making process. Our contribution is to improve the accuracy of extraction by combining different techniques from three major areas, named Data Mining, Natural Language Processing techniques and Ontologies. The proposed framework sequentially mines products aspects and users opinions, groups representative aspects by similarity, and generates an output summary. This paper focuses on the task of extracting product aspects and users opinions by extracting all possible aspects and opinions from reviews using natural language, ontology, and frequent (tag) sets. The proposed framework, when compared with an existing baseline model, yielded promising results. version:1
arxiv-1405-6223 | Coupled Item-based Matrix Factorization | http://arxiv.org/abs/1405.6223 | id:1405.6223 author:Fangfang Li, Guandong Xu, Longbing Cao category:cs.LG cs.IR  published:2014-04-08 summary:The essence of the challenges cold start and sparsity in Recommender Systems (RS) is that the extant techniques, such as Collaborative Filtering (CF) and Matrix Factorization (MF), mainly rely on the user-item rating matrix, which sometimes is not informative enough for predicting recommendations. To solve these challenges, the objective item attributes are incorporated as complementary information. However, most of the existing methods for inferring the relationships between items assume that the attributes are "independently and identically distributed (iid)", which does not always hold in reality. In fact, the attributes are more or less coupled with each other by some implicit relationships. Therefore, in this pa-per we propose an attribute-based coupled similarity measure to capture the implicit relationships between items. We then integrate the implicit item coupling into MF to form the Coupled Item-based Matrix Factorization (CIMF) model. Experimental results on two open data sets demonstrate that CIMF outperforms the benchmark methods. version:1
arxiv-1404-1890 | Polish and English wordnets -- statistical analysis of interconnected networks | http://arxiv.org/abs/1404.1890 | id:1404.1890 author:Maksymilian Bujok, Piotr Fronczak, Agata Fronczak category:cs.CL physics.soc-ph  published:2014-04-07 summary:Wordnets are semantic networks containing nouns, verbs, adjectives, and adverbs organized according to linguistic principles, by means of semantic relations. In this work, we adopt a complex network perspective to perform a comparative analysis of the English and Polish wordnets. We determine their similarities and show that the networks exhibit some of the typical characteristics observed in other real-world networks. We analyse interlingual relations between both wordnets and deliberate over the problem of mapping the Polish lexicon onto the English one. version:1
arxiv-1310-2916 | From Shading to Local Shape | http://arxiv.org/abs/1310.2916 | id:1310.2916 author:Ying Xiong, Ayan Chakrabarti, Ronen Basri, Steven J. Gortler, David W. Jacobs, Todd Zickler category:cs.CV  published:2013-10-10 summary:We develop a framework for extracting a concise representation of the shape information available from diffuse shading in a small image patch. This produces a mid-level scene descriptor, comprised of local shape distributions that are inferred separately at every image patch across multiple scales. The framework is based on a quadratic representation of local shape that, in the absence of noise, has guarantees on recovering accurate local shape and lighting. And when noise is present, the inferred local shape distributions provide useful shape information without over-committing to any particular image explanation. These local shape distributions naturally encode the fact that some smooth diffuse regions are more informative than others, and they enable efficient and robust reconstruction of object-scale shape. Experimental results show that this approach to surface reconstruction compares well against the state-of-art on both synthetic images and captured photographs. version:2
arxiv-1403-4698 | A Hierarchical Graphical Model for Big Inverse Covariance Estimation with an Application to fMRI | http://arxiv.org/abs/1403.4698 | id:1403.4698 author:Xi Luo category:stat.ME stat.AP stat.ML  published:2014-03-19 summary:Brain networks has attracted the interests of many neuroscientists. From functional MRI (fMRI) data, statistical tools have been developed to recover brain networks. However, the dimensionality of whole-brain fMRI, usually in hundreds of thousands, challenges the applicability of these methods. We develop a hierarchical graphical model (HGM) to remediate this difficulty. This model introduces a hidden layer of networks based on sparse Gaussian graphical models, and the observed data are sampled from individual network nodes. In fMRI, the network layer models the underlying signals of different brain functional units, and how these units directly interact with each other. The introduction of this hierarchical structure not only provides a formal and interpretable approach, but also enables efficient computation for inferring big networks with hundreds of thousands of nodes. Based on the conditional convexity of our formulation, we develop an alternating update algorithm to compute the HGM model parameters simultaneously. The effectiveness of this approach is demonstrated on simulated data and a real dataset from a stop/go fMRI experiment. version:2
arxiv-1404-1872 | Intégration des données d'un lexique syntaxique dans un analyseur syntaxique probabiliste | http://arxiv.org/abs/1404.1872 | id:1404.1872 author:Anthony Sigogne, Matthieu Constant, Eric Laporte category:cs.CL  published:2014-04-07 summary:This article reports the evaluation of the integration of data from a syntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntactic parser. We show that by changing the set of labels for verbs and predicational nouns, we can improve the performance on French of a non-lexicalized probabilistic parser. version:1
arxiv-1404-1869 | DenseNet: Implementing Efficient ConvNet Descriptor Pyramids | http://arxiv.org/abs/1404.1869 | id:1404.1869 author:Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, Kurt Keutzer category:cs.CV  published:2014-04-07 summary:Convolutional Neural Networks (CNNs) can provide accurate object classification. They can be extended to perform object detection by iterating over dense or selected proposed object regions. However, the runtime of such detectors scales as the total number and/or area of regions to examine per image, and training such detectors may be prohibitively slow. However, for some CNN classifier topologies, it is possible to share significant work among overlapping regions to be classified. This paper presents DenseNet, an open source system that computes dense, multiscale features from the convolutional layers of a CNN based object classifier. Future work will involve training efficient object detectors with DenseNet feature descriptors. version:1
arxiv-1404-1847 | Evaluation and Ranking of Machine Translated Output in Hindi Language using Precision and Recall Oriented Metrics | http://arxiv.org/abs/1404.1847 | id:1404.1847 author:Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, Ajai Kumar, Hemant Darbari category:cs.CL  published:2014-04-07 summary:Evaluation plays a crucial role in development of Machine translation systems. In order to judge the quality of an existing MT system i.e. if the translated output is of human translation quality or not, various automatic metrics exist. We here present the implementation results of different metrics when used on Hindi language along with their comparisons, illustrating how effective are these metrics on languages like Hindi (free word order language). version:1
