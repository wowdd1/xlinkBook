arxiv-1406-6425 | Compressive Imaging and Characterization of Sparse Light Deflection Maps | http://arxiv.org/abs/1406.6425 | id:1406.6425 author:Prasad Sudhakar, Laurent Jacques, Xavier Dubois, Philippe Antoine, Luc Joannes category:cs.CV  published:2014-06-25 summary:Light rays incident on a transparent object of uniform refractive index undergo deflections, which uniquely characterize the surface geometry of the object. Associated with each point on the surface is a deflection map (or spectrum) which describes the pattern of deflections in various directions. This article presents a novel method to efficiently acquire and reconstruct sparse deflection spectra induced by smooth object surfaces. To this end, we leverage the framework of Compressed Sensing (CS) in a particular implementation of a schlieren deflectometer, i.e., an optical system providing linear measurements of deflection spectra with programmable spatial light modulation patterns. We design those modulation patterns on the principle of spread spectrum CS for reducing the number of observations. The ability of our device to simultaneously observe the deflection spectra on a dense discretization of the object surface is related to a Multiple Measurement Vector (MMV) model. This scheme allows us to estimate both the noise power and the instrumental point spread function. We formulate the spectrum reconstruction task as the solving of a linear inverse problem regularized by an analysis sparsity prior using a translation invariant wavelet frame. Our results demonstrate the capability and advantages of using a CS based approach for deflectometric imaging both on simulated data and experimental deflectometric data. Finally, the paper presents an extension of our method showing how we can extract the main deflection direction in each point of the object surface from a few compressive measurements, without needing any costly reconstruction procedures. This compressive characterization is then confirmed with experimental results on simple plano-convex and multifocal intra-ocular lenses studying the evolution of the main deflection as a function of the object point location. version:2
arxiv-1111-4470 | Efficient Regression in Metric Spaces via Approximate Lipschitz Extension | http://arxiv.org/abs/1111.4470 | id:1111.4470 author:Lee-Ad Gottlieb, Aryeh Kontorovich, Robert Krauthgamer category:cs.LG  published:2011-11-18 summary:We present a framework for performing efficient regression in general metric spaces. Roughly speaking, our regressor predicts the value at a new point by computing a Lipschitz extension --- the smoothest function consistent with the observed data --- after performing structural risk minimization to avoid overfitting. We obtain finite-sample risk bounds with minimal structural and noise assumptions, and a natural speed-precision tradeoff. The offline (learning) and online (prediction) stages can be solved by convex programming, but this naive approach has runtime complexity $O(n^3)$, which is prohibitive for large datasets. We design instead a regression algorithm whose speed and generalization performance depend on the intrinsic dimension of the data, to which the algorithm adapts. While our main innovation is algorithmic, the statistical results may also be of independent interest. version:2
arxiv-1311-6547 | Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis | http://arxiv.org/abs/1311.6547 | id:1311.6547 author:Katya Scheinberg, Xiaocheng Tang category:cs.LG math.OC stat.ML  published:2013-11-26 summary:Recently several methods were proposed for sparse optimization which make careful use of second-order information [10, 28, 16, 3] to improve local convergence rates. These methods construct a composite quadratic approximation using Hessian information, optimize this approximation using a first-order method, such as coordinate descent and employ a line search to ensure sufficient descent. Here we propose a general framework, which includes slightly modified versions of existing algorithms and also a new algorithm, which uses limited memory BFGS Hessian approximations, and provide a novel global convergence rate analysis, which covers methods that solve subproblems via coordinate descent. version:4
arxiv-1507-03887 | An SVM-like Approach for Expectile Regression | http://arxiv.org/abs/1507.03887 | id:1507.03887 author:Muhammad Farooq, Ingo Steinwart category:stat.CO stat.ML  published:2015-07-14 summary:Expectile regression is a nice tool for investigating conditional distributions beyond the conditional mean. It is well-known that expectiles can be described with the help of the asymmetric least square loss function, and this link makes it possible to estimate expectiles in a non-parametric framework by a support vector machine like approach. In this work we develop an efficient sequential-minimal-optimization-based solver for the underlying optimization problem. The behavior of the solver is investigated by conducting various experiments and the results are compared with the recent R-package ER-Boost. version:1
arxiv-1507-03867 | Rich Component Analysis | http://arxiv.org/abs/1507.03867 | id:1507.03867 author:Rong Ge, James Zou category:cs.LG stat.ML  published:2015-07-14 summary:In many settings, we have multiple data sets (also called views) that capture different and overlapping aspects of the same phenomenon. We are often interested in finding patterns that are unique to one or to a subset of the views. For example, we might have one set of molecular observations and one set of physiological observations on the same group of individuals, and we want to quantify molecular patterns that are uncorrelated with physiology. Despite being a common problem, this is highly challenging when the correlations come from complex distributions. In this paper, we develop the general framework of Rich Component Analysis (RCA) to model settings where the observations from different views are driven by different sets of latent components, and each component can be a complex, high-dimensional distribution. We introduce algorithms based on cumulant extraction that provably learn each of the components without having to model the other components. We show how to integrate RCA with stochastic gradient descent into a meta-algorithm for learning general models, and demonstrate substantial improvement in accuracy on several synthetic and real datasets in both supervised and unsupervised tasks. Our method makes it possible to learn latent variable models when we don't have samples from the true model but only samples after complex perturbations. version:1
arxiv-1507-03811 | Ensemble of Hankel Matrices for Face Emotion Recognition | http://arxiv.org/abs/1507.03811 | id:1507.03811 author:Liliana Lo Presti, Marco La Cascia category:cs.CV cs.HC cs.RO  published:2015-07-14 summary:In this paper, a face emotion is considered as the result of the composition of multiple concurrent signals, each corresponding to the movements of a specific facial muscle. These concurrent signals are represented by means of a set of multi-scale appearance features that might be correlated with one or more concurrent signals. The extraction of these appearance features from a sequence of face images yields to a set of time series. This paper proposes to use the dynamics regulating each appearance feature time series to recognize among different face emotions. To this purpose, an ensemble of Hankel matrices corresponding to the extracted time series is used for emotion classification within a framework that combines nearest neighbor and a majority vote schema. Experimental results on a public available dataset shows that the adopted representation is promising and yields state-of-the-art accuracy in emotion classification. version:1
arxiv-1408-4002 | The Filament Sensor for Near Real-Time Detection of Cytoskeletal Fiber Structures | http://arxiv.org/abs/1408.4002 | id:1408.4002 author:Benjamin Eltzner, Carina Wollnik, Carsten Gottschlich, Stephan Huckemann, Florian Rehfeldt category:cs.CV I.4.3; I.4.6  published:2014-08-18 summary:A reliable extraction of filament data from microscopic images is of high interest in the analysis of acto-myosin structures as early morphological markers in mechanically guided differentiation of human mesenchymal stem cells and the understanding of the underlying fiber arrangement processes. In this paper, we propose the filament sensor (FS), a fast and robust processing sequence which detects and records location, orientation, length and width for each single filament of an image, and thus allows for the above described analysis. The extraction of these features has previously not been possible with existing methods. We evaluate the performance of the proposed FS in terms of accuracy and speed in comparison to three existing methods with respect to their limited output. Further, we provide a benchmark dataset of real cell images along with filaments manually marked by a human expert as well as simulated benchmark images. The FS clearly outperforms existing methods in terms of computational runtime and filament extraction accuracy. The implementation of the FS and the benchmark database are available as open source. version:3
arxiv-1507-03751 | Closed Curves and Elementary Visual Object Identification | http://arxiv.org/abs/1507.03751 | id:1507.03751 author:Manfred Harringer category:cs.CV cs.LG q-bio.NC  published:2015-07-14 summary:For two closed curves on a plane (discrete version) and local criteria for similarity of points on the curves one gets a potential, which describes the similarity between curve points. This is the base for a global similarity measure of closed curves (Fr\'echet distance). I use borderlines of handwritten digits to demonstrate an area of application. I imagine, measuring the similarity of closed curves is an essential and elementary task performed by a visual system. This approach to similarity measures may be used by visual systems. version:1
arxiv-1507-03734 | Splitting the Smoothed Primal-Dual Gap: Optimal Alternating Direction Methods | http://arxiv.org/abs/1507.03734 | id:1507.03734 author:Quoc Tran-Dinh, Volkan Cevher category:math.OC stat.ML  published:2015-07-14 summary:We develop rigorous alternating direction optimization methods for a prototype constrained convex optimization template, which has broad applications in computational sciences. We build upon our earlier work on the model-based gap reduction (MGR) technique, which revolves around a smoothed estimate of the primal-dual gap. MGR allows us to simultaneously update a sequence of primal and dual variables as well as primal and dual smoothness parameters so that the smoothed gap function converges to the true gap, which in turn converges to zero -- both at optimal rates. In contrast, this paper introduces a new split-gap reduction (SGR) technique as a natural counterpart of MGR in order to take advantage of additional splitting structures present in the prototype template. We illustrate SGR technique using the forward-backward and Douglas-Rachford splittings on the smoothed gap function and derive new alternating direction methods. The new methods obtain optimal convergence rates without heuristics and eliminate the infamous penalty parameter tuning issue in the existing alternating direction methods. Finally, we verify the performance of our methods in comparison to the existing state-of-the-art and the new theoretical performance bounds via numerical examples. version:1
arxiv-1507-03719 | A New Framework for Distributed Submodular Maximization | http://arxiv.org/abs/1507.03719 | id:1507.03719 author:Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, Justin Ward category:cs.DS cs.AI cs.DC cs.LG  published:2015-07-14 summary:A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. A lot of recent effort has been devoted to developing distributed algorithms for these problems. However, these results suffer from high number of rounds, suboptimal approximation ratios, or both. We develop a framework for bringing existing algorithms in the sequential setting to the distributed setting, achieving near optimal approximation ratios for many settings in only a constant number of MapReduce rounds. Our techniques also give a fast sequential algorithm for non-monotone maximization subject to a matroid constraint. version:1
arxiv-1507-03707 | Projected Wirtinger Gradient Descent for Low-Rank Hankel Matrix Completion in Spectral Compressed Sensing | http://arxiv.org/abs/1507.03707 | id:1507.03707 author:Jian-Feng Cai, Suhui Liu, Weiyu Xu category:cs.IT cs.LG math.IT math.OC  published:2015-07-14 summary:This paper considers reconstructing a spectrally sparse signal from a small number of randomly observed time-domain samples. The signal of interest is a linear combination of complex sinusoids at $R$ distinct frequencies. The frequencies can assume any continuous values in the normalized frequency domain $[0,1)$. After converting the spectrally sparse signal recovery into a low rank structured matrix completion problem, we propose an efficient feasible point approach, named projected Wirtinger gradient descent (PWGD) algorithm, to efficiently solve this structured matrix completion problem. We further accelerate our proposed algorithm by a scheme inspired by FISTA. We give the convergence analysis of our proposed algorithms. Extensive numerical experiments are provided to illustrate the efficiency of our proposed algorithm. Different from earlier approaches, our algorithm can solve problems of very large dimensions very efficiently. version:1
arxiv-1509-03208 | Towards Understanding Egyptian Arabic Dialogues | http://arxiv.org/abs/1509.03208 | id:1509.03208 author:Abdelrahim A Elmadany, Sherif M Abdou, Mervat Gheith category:cs.CL  published:2015-07-14 summary:Labelling of user's utterances to understanding his attends which called Dialogue Act (DA) classification, it is considered the key player for dialogue language understanding layer in automatic dialogue systems. In this paper, we proposed a novel approach to user's utterances labeling for Egyptian spontaneous dialogues and Instant Messages using Machine Learning (ML) approach without relying on any special lexicons, cues, or rules. Due to the lack of Egyptian dialect dialogue corpus, the system evaluated by multi-genre corpus includes 4725 utterances for three domains, which are collected and annotated manually from Egyptian call-centers. The system achieves F1 scores of 70. 36% overall domains. version:1
arxiv-1412-7091 | Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets | http://arxiv.org/abs/1412.7091 | id:1412.7091 author:Pascal Vincent, Alexandre de Brébisson, Xavier Bouthillier category:cs.NE cs.CL cs.LG  published:2014-12-22 summary:An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D x d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d^2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture. version:3
arxiv-1507-03641 | Neural CRF Parsing | http://arxiv.org/abs/1507.03641 | id:1507.03641 author:Greg Durrett, Dan Klein category:cs.CL cs.NE  published:2015-07-13 summary:This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. version:1
arxiv-1507-03538 | Classifying X-ray Binaries: A Probabilistic Approach | http://arxiv.org/abs/1507.03538 | id:1507.03538 author:Giri Gopalan, Saeqa Dil Vrtilek, Luke Bornn category:astro-ph.HE stat.AP stat.ML  published:2015-07-13 summary:In X-ray binary star systems consisting of a compact object that accretes material from an orbiting secondary star, there is no straightforward means to decide if the compact object is a black hole or a neutron star. To assist this process we develop a Bayesian statistical model which makes use of the fact that X-ray binary systems appear to cluster based on their compact object type when viewed from a 3-dimensional coordinate system derived from X-ray spectral data, where the first coordinate is the ratio of counts in mid to low energy band (color 1), the second coordinate is the ratio of counts in high to low energy band (color 2), and the third coordinate is the sum of counts in all three bands. Precisely, we use this model to estimate the probabilities that an X-ray binary system contains a black hole, non-pulsing neutron star or pulsing neutron star. In particular we utilize a latent variable model in which the latent variables follow a Gaussian process prior distribution, and hence we are able to induce the spatial correlation we believe exists between systems of the same type. The utility of this approach is evidenced by the accurate prediction of system types using Rossi X-ray Timing Explorer All Sky Monitor data, but it is not flawless. In particular, non-pulsing neutron systems containing "bursters" which are close to the boundary demarcating systems containing black holes tend to be classified as black hole systems. As a byproduct of our analyses, we provide the astronomer with public R code that can be used to predict the compact object type of X-ray binaries given training data. version:1
arxiv-1506-06422 | Beyond Hartigan Consistency: Merge Distortion Metric for Hierarchical Clustering | http://arxiv.org/abs/1506.06422 | id:1506.06422 author:Justin Eldridge, Mikhail Belkin, Yusu Wang category:stat.ML math.ST stat.TH  published:2015-06-21 summary:Hierarchical clustering is a popular method for analyzing data which associates a tree to a dataset. Hartigan consistency has been used extensively as a framework to analyze such clustering algorithms from a statistical point of view. Still, as we show in the paper, a tree which is Hartigan consistent with a given density can look very different than the correct limit tree. Specifically, Hartigan consistency permits two types of undesirable configurations which we term over-segmentation and improper nesting. Moreover, Hartigan consistency is a limit property and does not directly quantify difference between trees. In this paper we identify two limit properties, separation and minimality, which address both over-segmentation and improper nesting and together imply (but are not implied by) Hartigan consistency. We proceed to introduce a merge distortion metric between hierarchical clusterings and show that convergence in our distance implies both separation and minimality. We also prove that uniform separation and minimality imply convergence in the merge distortion metric. Furthermore, we show that our merge distortion metric is stable under perturbations of the density. Finally, we demonstrate applicability of these concepts by proving convergence results for two clustering algorithms. First, we show convergence (and hence separation and minimality) of the recent robust single linkage algorithm of Chaudhuri and Dasgupta (2010). Second, we provide convergence results on manifolds for topological split tree clustering. version:2
arxiv-1404-3331 | Priors for Random Count Matrices Derived from a Family of Negative Binomial Processes | http://arxiv.org/abs/1404.3331 | id:1404.3331 author:Mingyuan Zhou, Oscar Hernan Madrid Padilla, James G. Scott category:stat.ME stat.ML  published:2014-04-12 summary:We define a family of probability distributions for random count matrices with a potentially unbounded number of rows and columns. The three distributions we consider are derived from the gamma-Poisson, gamma-negative binomial, and beta-negative binomial processes. Because the models lead to closed-form Gibbs sampling update equations, they are natural candidates for nonparametric Bayesian priors over count matrices. A key aspect of our analysis is the recognition that, although the random count matrices within the family are defined by a row-wise construction, their columns can be shown to be i.i.d. This fact is used to derive explicit formulas for drawing all the columns at once. Moreover, by analyzing these matrices' combinatorial structure, we describe how to sequentially construct a column-i.i.d. random count matrix one row at a time, and derive the predictive distribution of a new row count vector with previously unseen features. We describe the similarities and differences between the three priors, and argue that the greater flexibility of the gamma- and beta- negative binomial processes, especially their ability to model over-dispersed, heavy-tailed count data, makes these well suited to a wide variety of real-world applications. As an example of our framework, we construct a naive-Bayes text classifier to categorize a count vector to one of several existing random count matrices of different categories. The classifier supports an unbounded number of features, and unlike most existing methods, it does not require a predefined finite vocabulary to be shared by all the categories, and needs neither feature selection nor parameter tuning. Both the gamma- and beta- negative binomial processes are shown to significantly outperform the gamma-Poisson process for document categorization, with comparable performance to other state-of-the-art supervised text classification algorithms. version:3
arxiv-1502-05700 | Scalable Bayesian Optimization Using Deep Neural Networks | http://arxiv.org/abs/1502.05700 | id:1502.05700 author:Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, Ryan P. Adams category:stat.ML  published:2015-02-19 summary:Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models. version:2
arxiv-1507-03496 | The mRMR variable selection method: a comparative study for functional data | http://arxiv.org/abs/1507.03496 | id:1507.03496 author:José R. Berrendero, Antonio Cuevas, José L. Torrecilla category:stat.ME stat.ML  published:2015-07-13 summary:The use of variable selection methods is particularly appealing in statistical problems with functional data. The obvious general criterion for variable selection is to choose the `most representative' or `most relevant' variables. However, it is also clear that a purely relevance-oriented criterion could lead to select many redundant variables. The mRMR (minimum Redundance Maximum Relevance) procedure, proposed by Ding and Peng (2005) and Peng et al. (2005) is an algorithm to systematically perform variable selection, achieving a reasonable trade-off between relevance and redundancy. In its original form, this procedure is based on the use of the so-called mutual information criterion to assess relevance and redundancy. Keeping the focus on functional data problems, we propose here a modified version of the mRMR method, obtained by replacing the mutual information by the new association measure (called distance correlation) suggested by Sz\'ekely et al. (2007). We have also performed an extensive simulation study, including 1600 functional experiments (100 functional models $\times$ 4 sample sizes $\times$ 4 classifiers) and three real-data examples aimed at comparing the different versions of the mRMR methodology. The results are quite conclusive in favor of the new proposed alternative. version:1
arxiv-1507-03482 | Individual performance calibration using physiological stress signals | http://arxiv.org/abs/1507.03482 | id:1507.03482 author:Francisco Hernando-Gallego, Antonio Artés-Rodríguez category:cs.HC cs.CY stat.ML  published:2015-07-13 summary:The relation between performance and stress is described by the Yerkes-Dodson Law but varies significantly between individuals. This paper describes a method for determining the individual optimal performance as a function of physiological signals. The method is based on attention and reasoning tests of increasing complexity under monitoring of three physiological signals: Galvanic Skin Response (GSR), Heart Rate (HR), and Electromyogram (EMG). Based on the test results with 15 different individuals, we first show that two of the signals, GSR and HR, have enough discriminative power to distinguish between relax and stress periods. We then show a positive correlation between the complexity level of the tests and the GSR and HR signals, and we finally determine the optimal performance point as the signal level just before a performance decrease. We also discuss the differences among signals depending on the type of test. version:1
arxiv-1507-03471 | Incremental LSTM-based Dialog State Tracker | http://arxiv.org/abs/1507.03471 | id:1507.03471 author:Lukas Zilka, Filip Jurcicek category:cs.CL  published:2015-07-13 summary:A dialog state tracker is an important component in modern spoken dialog systems. We present an incremental dialog state tracker, based on LSTM networks. It directly uses automatic speech recognition hypotheses to track the state. We also present the key non-standard aspects of the model that bring its performance close to the state-of-the-art and experimentally analyze their contribution: including the ASR confidence scores, abstracting scarcely represented values, including transcriptions in the training data, and model averaging. version:1
arxiv-1507-03462 | Supervised Hierarchical Classification for Student Answer Scoring | http://arxiv.org/abs/1507.03462 | id:1507.03462 author:Itziar Aldabe, Oier Lopez de Lacalle, Iñigo Lopez-Gazpio, Montse Maritxalar category:cs.CL  published:2015-07-13 summary:This paper describes a hierarchical system that predicts one label at a time for automated student response analysis. For the task, we build a classification binary tree that delays more easily confused labels to later stages using hierarchical processes. In particular, the paper describes how the hierarchical classifier has been built and how the classification task has been broken down into binary subtasks. It finally discusses the motivations and fundamentals of such an approach. version:1
arxiv-1407-8322 | Zipf's law for word frequencies: word forms versus lemmas in long texts | http://arxiv.org/abs/1407.8322 | id:1407.8322 author:Alvaro Corral, Gemma Boleda, Ramon Ferrer-i-Cancho category:physics.soc-ph cs.CL physics.data-an  published:2014-07-31 summary:Zipf's law is a fundamental paradigm in the statistics of written and spoken natural language as well as in other communication systems. We raise the question of the elementary units for which Zipf's law should hold in the most natural way, studying its validity for plain word forms and for the corresponding lemma forms. In order to have as homogeneous sources as possible, we analyze some of the longest literary texts ever written, comprising four different languages, with different levels of morphological complexity. In all cases Zipf's law is fulfilled, in the sense that a power-law distribution of word or lemma frequencies is valid for several orders of magnitude. We investigate the extent to which the word-lemma transformation preserves two parameters of Zipf's law: the exponent and the low-frequency cut-off. We are not able to demonstrate a strict invariance of the tail, as for a few texts both exponents deviate significantly, but we conclude that the exponents are very similar, despite the remarkable transformation that going from words to lemmas represents, considerably affecting all ranges of frequencies. In contrast, the low-frequency cut-offs are less stable. version:2
arxiv-1505-06163 | Direct Variational Perspective Shape from Shading with Cartesian Depth Parametrisation | http://arxiv.org/abs/1505.06163 | id:1505.06163 author:Yong Chul Ju, Daniel Maurer, Michael Breuß, Andrés Bruhn category:cs.CV  published:2015-05-22 summary:Most of today's state-of-the-art methods for perspective shape from shading are modelled in terms of partial differential equations (PDEs) of Hamilton-Jacobi type. To improve the robustness of such methods w.r.t. noise and missing data, first approaches have recently been proposed that seek to embed the underlying PDE into a variational framework with data and smoothness term. So far, however, such methods either make use of a radial depth parametrisation that makes the regularisation hard to interpret from a geometrical viewpoint or they consider indirect smoothness terms that require additional consistency constraints to provide valid solutions. Moreover the minimisation of such frameworks is an intricate task, since the underlying energy is typically non-convex. In our paper we address all three of the aforementioned issues. First, we propose a novel variational model that operates directly on the Cartesian depth. In this context, we also point out a common mistake in the derivation of the surface normal. Moreover, we employ a direct second-order regulariser with edge-preservation property. This direct regulariser yields by construction valid solutions without requiring additional consistency constraints. Finally, we also propose a novel coarse-to-fine minimisation framework based on an alternating explicit scheme. This framework allows us to avoid local minima during the minimisation and thus to improve the accuracy of the reconstruction. Experiments show the good quality of our model as well as the usefulness of the proposed numerical scheme. version:2
arxiv-1507-03360 | Sparsity assisted solution to the twin image problem in phase retrieval | http://arxiv.org/abs/1507.03360 | id:1507.03360 author:Charu Gaur, Baranidharan Mohan, Kedar Khare category:cs.CV physics.optics  published:2015-07-13 summary:The iterative phase retrieval problem for complex-valued objects from Fourier transform magnitude data is known to suffer from the twin image problem. In particular, when the object support is centro-symmetric, the iterative solution often stagnates such that the resultant complex image contains the features of both the desired solution and its inverted and complex-conjugated replica. The conventional approach to address the twin image problem is to modify the object support during initial iterations which can possibly lead to elimination of one of the twin images. However, at present there seems to be no deterministic procedure to make sure that the twin image will always be very weak or absent. In this work we make an important observation that the ideal solution without the twin image is typically more sparse (in some suitable transform domain) as compared to the stagnated solution containing the twin image. We further show that introducing a sparsity enhancing step in the iterative algorithm can address the twin image problem without the need to change the object support throughout the iterative process even when the object support is centro-symmetric. In a simulation study, we use binary and gray-scale pure phase objects and illustrate the effectiveness of the sparsity assisted phase recovery in the context of the twin image problem. The results have important implications for a wide range of topics in Physics where the phase retrieval problem plays a central role. version:1
arxiv-1311-6359 | Score-based Causal Learning in Additive Noise Models | http://arxiv.org/abs/1311.6359 | id:1311.6359 author:Christopher Nowzohour, Peter Bühlmann category:stat.ML  published:2013-11-25 summary:Given data sampled from a number of variables, one is often interested in the underlying causal relationships in the form of a directed acyclic graph. In the general case, without interventions on some of the variables it is only possible to identify the graph up to its Markov equivalence class. However, in some situations one can find the true causal graph just from observational data, for example in structural equation models with additive noise and nonlinear edge functions. Most current methods for achieving this rely on nonparametric independence tests. One of the problems there is that the null hypothesis is independence, which is what one would like to get evidence for. We take a different approach in our work by using a penalized likelihood as a score for model selection. This is practically feasible in many settings and has the advantage of yielding a natural ranking of the candidate models. When making smoothness assumptions on the probability density space, we prove consistency of the penalized maximum likelihood estimator. We also present empirical results for simulated scenarios and real two-dimensional data sets (cause-effect pairs) where we obtain similar results as other state-of-the-art methods. version:3
arxiv-1507-03340 | Quantitative Evaluation of Performance and Validity Indices for Clustering the Web Navigational Sessions | http://arxiv.org/abs/1507.03340 | id:1507.03340 author:Zahid Ansari, M. F. Azeem, Waseem Ahmed, A. Vinaya Babu category:cs.LG cs.SI  published:2015-07-13 summary:Clustering techniques are widely used in Web Usage Mining to capture similar interests and trends among users accessing a Web site. For this purpose, web access logs generated at a particular web site are preprocessed to discover the user navigational sessions. Clustering techniques are then applied to group the user session data into user session clusters, where intercluster similarities are minimized while the intra cluster similarities are maximized. Since the application of different clustering algorithms generally results in different sets of cluster formation, it is important to evaluate the performance of these methods in terms of accuracy and validity of the clusters, and also the time required to generate them, using appropriate performance measures. This paper describes various validity and accuracy measures including Dunn's Index, Davies Bouldin Index, C Index, Rand Index, Jaccard Index, Silhouette Index, Fowlkes Mallows and Sum of the Squared Error (SSE). We conducted the performance evaluation of the following clustering techniques: k-Means, k-Medoids, Leader, Single Link Agglomerative Hierarchical and DBSCAN. These techniques are implemented and tested against the Web user navigational data. Finally their performance results are presented and compared. version:1
arxiv-1401-0362 | EigenGP: Gaussian Process Models with Adaptive Eigenfunctions | http://arxiv.org/abs/1401.0362 | id:1401.0362 author:Hao Peng, Yuan Qi category:cs.LG  published:2014-01-02 summary:Gaussian processes (GPs) provide a nonparametric representation of functions. However, classical GP inference suffers from high computational cost for big data. In this paper, we propose a new Bayesian approach, EigenGP, that learns both basis dictionary elements--eigenfunctions of a GP prior--and prior precisions in a sparse finite model. It is well known that, among all orthogonal basis functions, eigenfunctions can provide the most compact representation. Unlike other sparse Bayesian finite models where the basis function has a fixed form, our eigenfunctions live in a reproducing kernel Hilbert space as a finite linear combination of kernel functions. We learn the dictionary elements--eigenfunctions--and the prior precisions over these elements as well as all the other hyperparameters from data by maximizing the model marginal likelihood. We explore computational linear algebra to simplify the gradient computation significantly. Our experimental results demonstrate improved predictive performance of EigenGP over alternative sparse GP methods as well as relevance vector machine. version:3
arxiv-1507-02145 | Learning to Mine Chinese Coordinate Terms Using the Web | http://arxiv.org/abs/1507.02145 | id:1507.02145 author:Xiaojiang Huang, Xiaojun Wan, Jianguo Xiao category:cs.CL  published:2015-07-08 summary:Coordinate relation refers to the relation between instances of a concept and the relation between the directly hyponyms of a concept. In this paper, we focus on the task of extracting terms which are coordinate with a user given seed term in Chinese, and grouping the terms which belong to different concepts if the seed term has several meanings. We propose a semi-supervised method that integrates manually defined linguistic patterns and automatically learned semi-structural patterns to extract coordinate terms in Chinese from web search results. In addition, terms are grouped into different concepts based on their co-occurring terms and contexts. We further calculate the saliency scores of extracted terms and rank them accordingly. Experimental results demonstrate that our proposed method generates results with high quality and wide coverage. version:2
arxiv-1507-03285 | Scatter Matrix Concordance: A Diagnostic for Regressions on Subsets of Data | http://arxiv.org/abs/1507.03285 | id:1507.03285 author:Michael J. Kane, Bryan Lewis, Sekhar Tatikonda, Simon Urbanek category:stat.ML  published:2015-07-12 summary:Linear regression models depend directly on the design matrix and its properties. Techniques that efficiently estimate model coefficients by partitioning rows of the design matrix are increasingly popular for large-scale problems because they fit well with modern parallel computing architectures. We propose a simple measure of {\em concordance} between a design matrix and a subset of its rows that estimates how well a subset captures the variance-covariance structure of a larger data set. We illustrate the use of this measure in a heuristic method for selecting row partition sizes that balance statistical and computational efficiency goals in real-world problems. version:1
arxiv-1507-03269 | Tensor principal component analysis via sum-of-squares proofs | http://arxiv.org/abs/1507.03269 | id:1507.03269 author:Samuel B. Hopkins, Jonathan Shi, David Steurer category:cs.LG cs.CC cs.DS stat.ML  published:2015-07-12 summary:We study a statistical model for the tensor principal component analysis problem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ of the form $T = \tau \cdot v_0^{\otimes 3} + A$, where $\tau \geq 0$ is a signal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noise tensor, the goal is to recover the planted vector $v_0$. For the case that $A$ has iid standard Gaussian entries, we give an efficient algorithm to recover $v_0$ whenever $\tau \geq \omega(n^{3/4} \log(n)^{1/4})$, and certify that the recovered vector is close to a maximum likelihood estimator, all with high probability over the random choice of $A$. The previous best algorithms with provable guarantees required $\tau \geq \Omega(n)$. In the regime $\tau \leq o(n)$, natural tensor-unfolding-based spectral relaxations for the underlying optimization problem break down (in the sense that their integrality gap is large). To go beyond this barrier, we use convex relaxations based on the sum-of-squares method. Our recovery algorithm proceeds by rounding a degree-$4$ sum-of-squares relaxations of the maximum-likelihood-estimation problem for the statistical model. To complement our algorithmic results, we show that degree-$4$ sum-of-squares relaxations break down for $\tau \leq O(n^{3/4}/\log(n)^{1/4})$, which demonstrates that improving our current guarantees (by more than logarithmic factors) would require new techniques or might even be intractable. Finally, we show how to exploit additional problem structure in order to solve our sum-of-squares relaxations, up to some approximation, very efficiently. Our fastest algorithm runs in nearly-linear time using shifted (matrix) power iteration and has similar guarantees as above. The analysis of this algorithm also confirms a variant of a conjecture of Montanari and Richard about singular vectors of tensor unfoldings. version:1
arxiv-1507-03229 | Homotopy Continuation Approaches for Robust SV Classification and Regression | http://arxiv.org/abs/1507.03229 | id:1507.03229 author:Shinya Suzumura, Kohei Ogawa, Masashi Sugiyama, Masayuki Karasuyama, Ichiro Takeuchi category:stat.ML cs.LG  published:2015-07-12 summary:In support vector machine (SVM) applications with unreliable data that contains a portion of outliers, non-robustness of SVMs often causes considerable performance deterioration. Although many approaches for improving the robustness of SVMs have been studied, two major challenges remain in robust SVM learning. First, robust learning algorithms are essentially formulated as non-convex optimization problems. It is thus important to develop a non-convex optimization method for robust SVM that can find a good local optimal solution. The second practical issue is how one can tune the hyperparameter that controls the balance between robustness and efficiency. Unfortunately, due to the non-convexity, robust SVM solutions with slightly different hyper-parameter values can be significantly different, which makes model selection highly unstable. In this paper, we address these two issues simultaneously by introducing a novel homotopy approach to non-convex robust SVM learning. Our basic idea is to introduce parametrized formulations of robust SVM which bridge the standard SVM and fully robust SVM via the parameter that represents the influence of outliers. We characterize the necessary and sufficient conditions of the local optimal solutions of robust SVM, and develop an algorithm that can trace a path of local optimal solutions when the influence of outliers is gradually decreased. An advantage of our homotopy approach is that it can be interpreted as simulated annealing, a common approach for finding a good local optimal solution in non-convex optimization problems. In addition, our homotopy method allows stable and efficient model selection based on the path of local optimal solutions. Empirical performances of the proposed approach are demonstrated through intensive numerical experiments both on robust classification and regression problems. version:1
arxiv-1507-03228 | Scalable Bayesian Inference for Excitatory Point Process Networks | http://arxiv.org/abs/1507.03228 | id:1507.03228 author:Scott W. Linderman, Ryan P. Adams category:stat.ML  published:2015-07-12 summary:Networks capture our intuition about relationships in the world. They describe the friendships between Facebook users, interactions in financial markets, and synapses connecting neurons in the brain. These networks are richly structured with cliques of friends, sectors of stocks, and a smorgasbord of cell types that govern how neurons connect. Some networks, like social network friendships, can be directly observed, but in many cases we only have an indirect view of the network through the actions of its constituents and an understanding of how the network mediates that activity. In this work, we focus on the problem of latent network discovery in the case where the observable activity takes the form of a mutually-excitatory point process known as a Hawkes process. We build on previous work that has taken a Bayesian approach to this problem, specifying prior distributions over the latent network structure and a likelihood of observed activity given this network. We extend this work by proposing a discrete-time formulation and developing a computationally efficient stochastic variational inference (SVI) algorithm that allows us to scale the approach to long sequences of observations. We demonstrate our algorithm on the calcium imaging data used in the Chalearn neural connectomics challenge. version:1
arxiv-1507-03223 | Classifier-Based Text Simplification for Improved Machine Translation | http://arxiv.org/abs/1507.03223 | id:1507.03223 author:Shruti Tyagi, Deepti Chopra, Iti Mathur, Nisheeth Joshi category:cs.CL  published:2015-07-12 summary:Machine Translation is one of the research fields of Computational Linguistics. The objective of many MT Researchers is to develop an MT System that produce good quality and high accuracy output translations and which also covers maximum language pairs. As internet and Globalization is increasing day by day, we need a way that improves the quality of translation. For this reason, we have developed a Classifier based Text Simplification Model for English-Hindi Machine Translation Systems. We have used support vector machines and Na\"ive Bayes Classifier to develop this model. We have also evaluated the performance of these classifiers. version:1
arxiv-1306-2347 | Auditing: Active Learning with Outcome-Dependent Query Costs | http://arxiv.org/abs/1306.2347 | id:1306.2347 author:Sivan Sabato, Anand D. Sarwate, Nathan Srebro category:cs.LG  published:2013-06-10 summary:We propose a learning setting in which unlabeled data is free, and the cost of a label depends on its value, which is not known in advance. We study binary classification in an extreme case, where the algorithm only pays for negative labels. Our motivation are applications such as fraud detection, in which investigating an honest transaction should be avoided if possible. We term the setting auditing, and consider the auditing complexity of an algorithm: the number of negative labels the algorithm requires in order to learn a hypothesis with low relative error. We design auditing algorithms for simple hypothesis classes (thresholds and rectangles), and show that with these algorithms, the auditing complexity can be significantly lower than the active label complexity. We also discuss a general competitive approach for auditing and possible modifications to the framework. version:4
arxiv-1507-03196 | DeepFont: Identify Your Font from An Image | http://arxiv.org/abs/1507.03196 | id:1507.03196 author:Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, Thomas S. Huang category:cs.CV  published:2015-07-12 summary:As font is one of the core design concepts, automatic font identification and similar font suggestion from an image or photo has been on the wish list of many designers. We study the Visual Font Recognition (VFR) problem, and advance the state-of-the-art remarkably by developing the DeepFont system. First of all, we build up the first available large-scale VFR dataset, named AdobeVFR, consisting of both labeled synthetic data and partially labeled real-world data. Next, to combat the domain mismatch between available training and testing data, we introduce a Convolutional Neural Network (CNN) decomposition approach, using a domain adaptation technique based on a Stacked Convolutional Auto-Encoder (SCAE) that exploits a large corpus of unlabeled real-world text images combined with synthetic data preprocessed in a specific way. Moreover, we study a novel learning-based model compression approach, in order to reduce the DeepFont model size without sacrificing its performance. The DeepFont system achieves an accuracy of higher than 80% (top-5) on our collected dataset, and also produces a good font similarity measure for font selection and suggestion. We also achieve around 6 times compression of the model without any visible loss of recognition accuracy. version:1
arxiv-1306-1066 | Differential Privacy in a Bayesian setting through posterior sampling | http://arxiv.org/abs/1306.1066 | id:1306.1066 author:Christos Dimitrakakis, Blaine Nelson, and Zuhe Zhang, Aikaterini Mitrokotsa, Benjamin Rubinstein category:stat.ML cs.LG  published:2013-06-05 summary:We examine the robustness and privacy properties of Bayesian inference, under assumptions on the prior. With no modifications to the Bayesian framework, we show that a simple posterior sampling algorithm results in uniform utility and privacy guarantees. In more detail, we generalise the concept of differential privacy to arbitrary dataset distances, outcome spaces and distribution families. We then prove bounds on the robustness of the posterior, introduce a posterior sampling mechanism, show that it is differentially private and provide finite sample bounds for distinguishability-based privacy under a strong adversarial model. Finally, we give examples satisfying our assumptions. version:4
arxiv-1507-03176 | Dependent Indian Buffet Process-based Sparse Nonparametric Nonnegative Matrix Factorization | http://arxiv.org/abs/1507.03176 | id:1507.03176 author:Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo category:stat.ML  published:2015-07-12 summary:Nonnegative Matrix Factorization (NMF) aims to factorize a matrix into two optimized nonnegative matrices appropriate for the intended applications. The method has been widely used for unsupervised learning tasks, including recommender systems (rating matrix of users by items) and document clustering (weighting matrix of papers by keywords). However, traditional NMF methods typically assume the number of latent factors (i.e., dimensionality of the loading matrices) to be fixed. This assumption makes them inflexible for many applications. In this paper, we propose a nonparametric NMF framework to mitigate this issue by using dependent Indian Buffet Processes (dIBP). In a nutshell, we apply a correlation function for the generation of two stick weights associated with each pair of columns of loading matrices, while still maintaining their respective marginal distribution specified by IBP. As a consequence, the generation of two loading matrices will be column-wise (indirectly) correlated. Under this same framework, two classes of correlation function are proposed (1) using Bivariate beta distribution and (2) using Copula function. Both methods allow us to adopt our work for various applications by flexibly choosing an appropriate parameter settings. Compared with the other state-of-the art approaches in this area, such as using Gaussian Process (GP)-based dIBP, our work is seen to be much more flexible in terms of allowing the two corresponding binary matrix columns to have greater variations in their non-zero entries. Our experiments on the real-world and synthetic datasets show that three proposed models perform well on the document clustering task comparing standard NMF without predefining the dimension for the factor matrices, and the Bivariate beta distribution-based and Copula-based models have better flexibility than the GP-based model. version:1
arxiv-1507-03133 | Best Subset Selection via a Modern Optimization Lens | http://arxiv.org/abs/1507.03133 | id:1507.03133 author:Dimitris Bertsimas, Angela King, Rahul Mazumder category:stat.ME math.OC stat.CO stat.ML  published:2015-07-11 summary:In the last twenty-five years (1990-2014), algorithmic advances in integer optimization combined with hardware improvements have resulted in an astonishing 200 billion factor speedup in solving Mixed Integer Optimization (MIO) problems. We present a MIO approach for solving the classical best subset selection problem of choosing $k$ out of $p$ features in linear regression given $n$ observations. We develop a discrete extension of modern first order continuous optimization methods to find high quality feasible solutions that we use as warm starts to a MIO solver that finds provably optimal solutions. The resulting algorithm (a) provides a solution with a guarantee on its suboptimality even if we terminate the algorithm early, (b) can accommodate side constraints on the coefficients of the linear regression and (c) extends to finding best subset solutions for the least absolute deviation loss function. Using a wide variety of synthetic and real datasets, we demonstrate that our approach solves problems with $n$ in the 1000s and $p$ in the 100s in minutes to provable optimality, and finds near optimal solutions for $n$ in the 100s and $p$ in the 1000s in minutes. We also establish via numerical experiments that the MIO approach performs better than {\texttt {Lasso}} and other popularly used sparse learning procedures, in terms of achieving sparse solutions with good predictive power. version:1
arxiv-1507-03130 | Joint estimation of quantile planes over arbitrary predictor spaces | http://arxiv.org/abs/1507.03130 | id:1507.03130 author:Yun Yang, Surya Tokdar category:stat.ME stat.CO stat.ML  published:2015-07-11 summary:In spite of the recent surge of interest in quantile regression, joint estimation of linear quantile planes remains a great challenge in statistics and econometrics. We propose a novel parametrization that characterizes any collection of non-crossing quantile planes over arbitrarily shaped convex predictor domains in any dimension by means of unconstrained scalar, vector and function valued parameters. Statistical models based on this parametrization inherit a fast computation of the likelihood function, enabling penalized likelihood or Bayesian approaches to model fitting. We introduce a complete Bayesian methodology by using Gaussian process prior distributions on the function valued parameters and develop a robust and efficient Markov chain Monte Carlo parameter estimation. The resulting method is shown to offer posterior consistency under mild tail and regularity conditions. We present several illustrative examples where the new method is compared against existing approaches and is found to offer better accuracy, coverage and model fit. version:1
arxiv-1507-03125 | A new boosting algorithm based on dual averaging scheme | http://arxiv.org/abs/1507.03125 | id:1507.03125 author:Nan Wang category:cs.LG  published:2015-07-11 summary:The fields of machine learning and mathematical optimization increasingly intertwined. The special topic on supervised learning and convex optimization examines this interplay. The training part of most supervised learning algorithms can usually be reduced to an optimization problem that minimizes a loss between model predictions and training data. While most optimization techniques focus on accuracy and speed of convergence, the qualities of good optimization algorithm from the machine learning perspective can be quite different since machine learning is more than fitting the data. Better optimization algorithms that minimize the training loss can possibly give very poor generalization performance. In this paper, we examine a particular kind of machine learning algorithm, boosting, whose training process can be viewed as functional coordinate descent on the exponential loss. We study the relation between optimization techniques and machine learning by implementing a new boosting algorithm. DABoost, based on dual-averaging scheme and study its generalization performance. We show that DABoost, although slower in reducing the training error, in general enjoys a better generalization error than AdaBoost. version:1
arxiv-1507-03092 | On the Use of Harrell's C for Node Splitting in Random Survival Forests | http://arxiv.org/abs/1507.03092 | id:1507.03092 author:Matthias Schmid, Marvin Wright, Andreas Ziegler category:stat.ML  published:2015-07-11 summary:Random forests are one of the most successful methods for statistical learning and prediction. Here we consider random survival forests (RSF), which are an extension of the original random forest method to right-censored outcome variables. RSF use the log-rank split criterion to form an ensemble of survival trees, the prediction accuracy of the ensemble estimate is subsequently evaluated by the concordance index for survival data ("Harrell's C"). Conceptually, this strategy means that the split criterion in RSF is different from the evaluation criterion of interest. In view of this discrepancy, we analyze the theoretical relationship between the two criteria and investigate whether a unified strategy that uses Harrell's C for both node splitting and evaluation is able to improve the performance of RSF. Based on simulation studies and the analysis of real-world data, we show that substantial performance gains are possible if the log-rank statistic is replaced by Harrell's C for node splitting in RSF. Our results also show that C-based splitting is not superior to log-rank splitting if the percentage of noise variables is high, a result which can be attributed to the more unbalanced splits that are generated by the log-rank statistic. version:1
arxiv-1507-03045 | Markov Logic Networks for Natural Language Question Answering | http://arxiv.org/abs/1507.03045 | id:1507.03045 author:Tushar Khot, Niranjan Balasubramanian, Eric Gribkoff, Ashish Sabharwal, Peter Clark, Oren Etzioni category:cs.AI cs.CL  published:2015-07-10 summary:Our goal is to answer elementary-level science questions using knowledge extracted automatically from science textbooks, expressed in a subset of first-order logic. Given the incomplete and noisy nature of these automatically extracted rules, Markov Logic Networks (MLNs) seem a natural model to use, but the exact way of leveraging MLNs is by no means obvious. We investigate three ways of applying MLNs to our task. In the first, we simply use the extracted science rules directly as MLN clauses. Unlike typical MLN applications, our domain has long and complex rules, leading to an unmanageable number of groundings. We exploit the structure present in hard constraints to improve tractability, but the formulation remains ineffective. In the second approach, we instead interpret science rules as describing prototypical entities, thus mapping rules directly to grounded MLN assertions, whose constants are then clustered using existing entity resolution methods. This drastically simplifies the network, but still suffers from brittleness. Finally, our third approach, called Praline, uses MLNs to align the lexical elements as well as define and control how inference should be performed in this task. Our experiments, demonstrating a 15\% accuracy boost and a 10x reduction in runtime, suggest that the flexibility and different inference semantics of Praline are a better fit for the natural language question answering task. version:1
arxiv-1406-1102 | Linear Convergence of Variance-Reduced Stochastic Gradient without Strong Convexity | http://arxiv.org/abs/1406.1102 | id:1406.1102 author:Pinghua Gong, Jieping Ye category:cs.NA cs.LG stat.CO stat.ML  published:2014-06-04 summary:Stochastic gradient algorithms estimate the gradient based on only one or a few samples and enjoy low computational cost per iteration. They have been widely used in large-scale optimization problems. However, stochastic gradient algorithms are usually slow to converge and achieve sub-linear convergence rates, due to the inherent variance in the gradient computation. To accelerate the convergence, some variance-reduced stochastic gradient algorithms, e.g., proximal stochastic variance-reduced gradient (Prox-SVRG) algorithm, have recently been proposed to solve strongly convex problems. Under the strongly convex condition, these variance-reduced stochastic gradient algorithms achieve a linear convergence rate. However, many machine learning problems are convex but not strongly convex. In this paper, we introduce Prox-SVRG and its projected variant called Variance-Reduced Projected Stochastic Gradient (VRPSG) to solve a class of non-strongly convex optimization problems widely used in machine learning. As the main technical contribution of this paper, we show that both VRPSG and Prox-SVRG achieve a linear convergence rate without strong convexity. A key ingredient in our proof is a Semi-Strongly Convex (SSC) inequality which is the first to be rigorously proved for a class of non-strongly convex problems in both constrained and regularized settings. Moreover, the SSC inequality is independent of algorithms and may be applied to analyze other stochastic gradient algorithms besides VRPSG and Prox-SVRG, which may be of independent interest. To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. version:2
arxiv-1507-02907 | Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach | http://arxiv.org/abs/1507.02907 | id:1507.02907 author:Luís Marujo, Ricardo Ribeiro, David Martins de Matos, João P. Neto, Anatole Gershman, Jaime Carbonell category:cs.IR cs.CL  published:2015-07-10 summary:The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results. version:1
arxiv-1507-02879 | Deep Perceptual Mapping for Thermal to Visible Face Recognition | http://arxiv.org/abs/1507.02879 | id:1507.02879 author:M. Saquib Sarfraz, Rainer Stiefelhagen category:cs.CV  published:2015-07-10 summary:Cross modal face matching between the thermal and visible spectrum is a much de- sired capability for night-time surveillance and security applications. Due to a very large modality gap, thermal-to-visible face recognition is one of the most challenging face matching problem. In this paper, we present an approach to bridge this modality gap by a significant margin. Our approach captures the highly non-linear relationship be- tween the two modalities by using a deep neural network. Our model attempts to learn a non-linear mapping from visible to thermal spectrum while preserving the identity in- formation. We show substantive performance improvement on a difficult thermal-visible face dataset. The presented approach improves the state-of-the-art by more than 10% in terms of Rank-1 identification and bridge the drop in performance due to the modality gap by more than 40%. version:1
arxiv-1507-02835 | A Trainable Neuromorphic Integrated Circuit that Exploits Device Mismatch | http://arxiv.org/abs/1507.02835 | id:1507.02835 author:Chetan Singh Thakur, Runchun Wang, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik category:cs.NE  published:2015-07-10 summary:Random device mismatch that arises as a result of scaling of the CMOS (complementary metal-oxide semi-conductor) technology into the deep submicron regime degrades the accuracy of analogue circuits. Methods to combat this increase the complexity of design. We have developed a novel neuromorphic system called a Trainable Analogue Block (TAB), which exploits device mismatch as a means for random projections of the input to a higher dimensional space. The TAB framework is inspired by the principles of neural population coding operating in the biological nervous system. Three neuronal layers, namely input, hidden, and output, constitute the TAB framework, with the number of hidden layer neurons far exceeding the input layer neurons. Here, we present measurement results of the first prototype TAB chip built using a 65nm process technology and show its learning capability for various regression tasks. Our TAB chip exploits inherent randomness and variability arising due to the fabrication process to perform various learning tasks. Additionally, we characterise each neuron and discuss the statistical variability of its tuning curve that arises due to random device mismatch, a desirable property for the learning capability of the TAB. We also discuss the effect of the number of hidden neurons and the resolution of output weights on the accuracy of the learning capability of the TAB. version:1
arxiv-1507-02779 | Robust Performance-driven 3D Face Tracking in Long Range Depth Scenes | http://arxiv.org/abs/1507.02779 | id:1507.02779 author:Hai X. Pham, Chongyu Chen, Luc N. Dao, Vladimir Pavlovic, Jianfei Cai, Tat-jen Cham category:cs.CV  published:2015-07-10 summary:We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker, capable of handling a wide range of target scene depths, beyond those that can be afforded by traditional depth or RGB face trackers. Lastly, since the blendshape is not able to accurately recover the real facial shape, we use the tracked 3D face model as a prior in a novel filtering process to further refine the depth map for use in other tasks, such as 3D reconstruction. version:1
arxiv-1507-02743 | Locally Non-linear Embeddings for Extreme Multi-label Learning | http://arxiv.org/abs/1507.02743 | id:1507.02743 author:Kush Bhatia, Himanshu Jain, Purushottam Kar, Prateek Jain, Manik Varma category:cs.LG cs.IR math.OC stat.ML  published:2015-07-09 summary:The objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set. Embedding based approaches make training and prediction tractable by assuming that the training label matrix is low-rank and hence the effective number of labels can be reduced by projecting the high dimensional label vectors onto a low dimensional linear subspace. Still, leading embedding approaches have been unable to deliver high prediction accuracies or scale to large problems as the low rank assumption is violated in most real world applications. This paper develops the X-One classifier to address both limitations. The main technical contribution in X-One is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring (tail) labels. This allows X-One to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors. We conducted extensive experiments on several real-world as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification. Experiments reveal that X-One can make significantly more accurate predictions then the state-of-the-art methods including both embeddings (by as much as 35%) as well as trees (by as much as 6%). X-One can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods. version:1
arxiv-1507-02703 | Robot In a Room: Toward Perfect Object Recognition in Closed Environments | http://arxiv.org/abs/1507.02703 | id:1507.02703 author:Shuran Song, Linguang Zhang, Jianxiong Xiao category:cs.CV  published:2015-07-09 summary:While general object recognition is still far from being solved, this paper proposes a way for a robot to recognize every object at an almost human-level accuracy. Our key observation is that many robots will stay in a relatively closed environment (e.g. a house or an office). By constraining a robot to stay in a limited territory, we can ensure that the robot has seen most objects before and the speed of introducing a new object is slow. Furthermore, we can build a 3D map of the environment to reliably subtract the background to make recognition easier. We propose extremely robust algorithms to obtain a 3D map and enable humans to collectively annotate objects. During testing time, our algorithm can recognize all objects very reliably, and query humans from crowd sourcing platform if confidence is low or new objects are identified. This paper explains design decisions in building such a system, and constructs a benchmark for extensive evaluation. Experiments suggest that making robot vision appear to be working from an end user's perspective is a reachable goal today, as long as the robot stays in a closed environment. By formulating this task, we hope to lay the foundation of a new direction in vision for robotics. Code and data will be available upon acceptance. version:1
arxiv-1507-02642 | Quantum Inspired Training for Boltzmann Machines | http://arxiv.org/abs/1507.02642 | id:1507.02642 author:Nathan Wiebe, Ashish Kapoor, Christopher Granade, Krysta M Svore category:cs.LG quant-ph  published:2015-07-09 summary:We present an efficient classical algorithm for training deep Boltzmann machines (DBMs) that uses rejection sampling in concert with variational approximations to estimate the gradients of the training objective function. Our algorithm is inspired by a recent quantum algorithm for training DBMs. We obtain rigorous bounds on the errors in the approximate gradients; in turn, we find that choosing the instrumental distribution to minimize the alpha=2 divergence with the Gibbs state minimizes the asymptotic algorithmic complexity. Our rejection sampling approach can yield more accurate gradients than low-order contrastive divergence training and the costs incurred in finding increasingly accurate gradients can be easily parallelized. Finally our algorithm can train full Boltzmann machines and scales more favorably with the number of layers in a DBM than greedy contrastive divergence training. version:1
arxiv-1411-6836 | Deep convolutional filter banks for texture recognition and segmentation | http://arxiv.org/abs/1411.6836 | id:1411.6836 author:Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi category:cs.CV  published:2014-11-25 summary:Research in texture recognition often concentrates on the problem of material recognition in uncluttered conditions, an assumption rarely met by applications. In this work we conduct a first study of material and describable texture at- tributes recognition in clutter, using a new dataset derived from the OpenSurface texture repository. Motivated by the challenge posed by this problem, we propose a new texture descriptor, D-CNN, obtained by Fisher Vector pooling of a Convolutional Neural Network (CNN) filter bank. D-CNN substantially improves the state-of-the-art in texture, mate- rial and scene recognition. Our approach achieves 82.3% accuracy on Flickr material dataset and 81.1% accuracy on MIT indoor scenes, providing absolute gains of more than 10% over existing approaches. D-CNN easily trans- fers across domains without requiring feature adaptation as for methods that build on the fully-connected layers of CNNs. Furthermore, D-CNN can seamlessly incorporate multi-scale information and describe regions of arbitrary shapes and sizes. Our approach is particularly suited at lo- calizing stuff categories and obtains state-of-the-art re- sults on MSRC segmentation dataset, as well as promising results on recognizing materials and surface attributes in clutter on the OpenSurfaces dataset. version:2
arxiv-1507-02628 | FAQ-based Question Answering via Word Alignment | http://arxiv.org/abs/1507.02628 | id:1507.02628 author:Zhiguo Wang, Abraham Ittycheriah category:cs.CL  published:2015-07-09 summary:In this paper, we propose a novel word-alignment-based method to solve the FAQ-based question answering task. First, we employ a neural network model to calculate question similarity, where the word alignment between two questions is used for extracting features. Second, we design a bootstrap-based feature extraction method to extract a small set of effective lexical features. Third, we propose a learning-to-rank algorithm to train parameters more suitable for the ranking tasks. Experimental results, conducted on three languages (English, Spanish and Japanese), demonstrate that the question similarity model is more effective than baseline systems, the sparse features bring 5% improvements on top-1 accuracy, and the learning-to-rank algorithm works significantly better than the traditional method. We further evaluate our method on the answer sentence selection task. Our method outperforms all the previous systems on the standard TREC data set. version:1
arxiv-1507-02564 | Sampling from a log-concave distribution with Projected Langevin Monte Carlo | http://arxiv.org/abs/1507.02564 | id:1507.02564 author:Sébastien Bubeck, Ronen Eldan, Joseph Lehec category:math.PR cs.DS cs.LG  published:2015-07-09 summary:We extend the Langevin Monte Carlo (LMC) algorithm to compactly supported measures via a projection step, akin to projected Stochastic Gradient Descent (SGD). We show that (projected) LMC allows to sample in polynomial time from a log-concave distribution with smooth potential. This gives a new Markov chain to sample from a log-concave distribution. Our main result shows in particular that when the target distribution is uniform, LMC mixes in $\tilde{O}(n^7)$ steps (where $n$ is the dimension). We also provide preliminary experimental evidence that LMC performs at least as well as hit-and-run, for which a better mixing time of $\tilde{O}(n^4)$ was proved by Lov{\'a}sz and Vempala. version:1
arxiv-1507-02492 | Adaptive Chemical Reaction Optimization for Global Numerical Optimization | http://arxiv.org/abs/1507.02492 | id:1507.02492 author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE  published:2015-07-09 summary:A newly proposed chemical-reaction-inspired metaheurisic, Chemical Reaction Optimization (CRO), has been applied to many optimization problems in both discrete and continuous domains. To alleviate the effort in tuning parameters, this paper reduces the number of optimization parameters in canonical CRO and develops an adaptive scheme to evolve them. Our proposed Adaptive CRO (ACRO) adapts better to different optimization problems. We perform simulations with ACRO on a widely-used benchmark of continuous problems. The simulation results show that ACRO has superior performance over canonical CRO. version:1
arxiv-1507-02491 | Parameter Sensitivity Analysis of Social Spider Algorithm | http://arxiv.org/abs/1507.02491 | id:1507.02491 author:James J. Q. Yu, Victor O. K. Li category:cs.NE  published:2015-07-09 summary:Social Spider Algorithm (SSA) is a recently proposed general-purpose real-parameter metaheuristic designed to solve global numerical optimization problems. This work systematically benchmarks SSA on a suite of 11 functions with different control parameters. We conduct parameter sensitivity analysis of SSA using advanced non-parametric statistical tests to generate statistically significant conclusion on the best performing parameter settings. The conclusion can be adopted in future work to reduce the effort in parameter tuning. In addition, we perform a success rate test to reveal the impact of the control parameters on the convergence speed of the algorithm. version:1
arxiv-1507-00996 | A New Approach to Probabilistic Programming Inference | http://arxiv.org/abs/1507.00996 | id:1507.00996 author:Frank Wood, Jan Willem van de Meent, Vikash Mansinghka category:stat.ML cs.AI cs.PL  published:2015-07-03 summary:We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is simple to implement and easy to parallelize. It applies to Turing-complete probabilistic programming languages and supports accurate inference in models that make use of complex control flow, including stochastic recursion. It also includes primitives from Bayesian nonparametric statistics. Our experiments show that this approach can be more efficient than previously introduced single-site Metropolis-Hastings methods. version:2
arxiv-1507-02447 | Data Mining of Causal Relations from Text: Analysing Maritime Accident Investigation Reports | http://arxiv.org/abs/1507.02447 | id:1507.02447 author:Santosh Tirunagari category:cs.IR cs.CL  published:2015-07-09 summary:Text mining is a process of extracting information of interest from text. Such a method includes techniques from various areas such as Information Retrieval (IR), Natural Language Processing (NLP), and Information Extraction (IE). In this study, text mining methods are applied to extract causal relations from maritime accident investigation reports collected from the Marine Accident Investigation Branch (MAIB). These causal relations provide information on various mechanisms behind accidents, including human and organizational factors relating to the accident. The objective of this study is to facilitate the analysis of the maritime accident investigation reports, by means of extracting contributory causes with more feasibility. A careful investigation of contributory causes from the reports provide opportunity to improve safety in future. Two methods have been employed in this study to extract the causal relations. They are 1) Pattern classification method and 2) Connectives method. The earlier one uses naive Bayes and Support Vector Machines (SVM) as classifiers. The latter simply searches for the words connecting cause and effect in sentences. The causal patterns extracted using these two methods are compared to the manual (human expert) extraction. The pattern classification method showed a fair and sensible performance with F-measure(average) = 65% when compared to connectives method with F-measure(average) = 58%. This study is an evidence, that text mining methods could be employed in extracting causal relations from marine accident investigation reports. version:1
arxiv-1507-02438 | Generalized Video Deblurring for Dynamic Scenes | http://arxiv.org/abs/1507.02438 | id:1507.02438 author:Tae Hyun Kim, Kyoung Mu Lee category:cs.CV  published:2015-07-09 summary:Several state-of-the-art video deblurring methods are based on a strong assumption that the captured scenes are static. These methods fail to deblur blurry videos in dynamic scenes. We propose a video deblurring method to deal with general blurs inherent in dynamic scenes, contrary to other methods. To handle locally varying and general blurs caused by various sources, such as camera shake, moving objects, and depth variation in a scene, we approximate pixel-wise kernel with bidirectional optical flows. Therefore, we propose a single energy model that simultaneously estimates optical flows and latent frames to solve our deblurring problem. We also provide a framework and efficient solvers to optimize the energy model. By minimizing the proposed energy function, we achieve significant improvements in removing blurs and estimating accurate optical flows in blurry frames. Extensive experimental results demonstrate the superiority of the proposed method in real and challenging videos that state-of-the-art methods fail in either deblurring or optical flow estimation. version:1
arxiv-1502-06256 | Spaced seeds improve k-mer-based metagenomic classification | http://arxiv.org/abs/1502.06256 | id:1502.06256 author:Karel Brinda, Maciej Sykulski, Gregory Kucherov category:q-bio.GN cs.CE cs.LG  published:2015-02-22 summary:Metagenomics is a powerful approach to study genetic content of environmental samples that has been strongly promoted by NGS technologies. To cope with massive data involved in modern metagenomic projects, recent tools [4, 39] rely on the analysis of k-mers shared between the read to be classified and sampled reference genomes. Within this general framework, we show in this work that spaced seeds provide a significant improvement of classification accuracy as opposed to traditional contiguous k-mers. We support this thesis through a series a different computational experiments, including simulations of large-scale metagenomic projects. Scripts and programs used in this study, as well as supplementary material, are available from http://github.com/gregorykucherov/spaced-seeds-for-metagenomics. version:3
arxiv-1507-02380 | Learning Structured Ordinal Measures for Video based Face Recognition | http://arxiv.org/abs/1507.02380 | id:1507.02380 author:Ran He, Tieniu Tan, Larry Davis, Zhenan Sun category:cs.CV  published:2015-07-09 summary:This paper presents a structured ordinal measure method for video-based face recognition that simultaneously learns ordinal filters and structured ordinal features. The problem is posed as a non-convex integer program problem that includes two parts. The first part learns stable ordinal filters to project video data into a large-margin ordinal space. The second seeks self-correcting and discrete codes by balancing the projected data and a rank-one ordinal matrix in a structured low-rank way. Unsupervised and supervised structures are considered for the ordinal matrix. In addition, as a complement to hierarchical structures, deep feature representations are integrated into our method to enhance coding stability. An alternating minimization method is employed to handle the discrete and low-rank constraints, yielding high-quality codes that capture prior structures well. Experimental results on three commonly used face video databases show that our method with a simple voting classifier can achieve state-of-the-art recognition rates using fewer features and samples. version:1
arxiv-1507-02356 | Intrinsic Non-stationary Covariance Function for Climate Modeling | http://arxiv.org/abs/1507.02356 | id:1507.02356 author:Chintan A. Dalal, Vladimir Pavlovic, Robert E. Kopp category:stat.ML cs.LG  published:2015-07-09 summary:Designing a covariance function that represents the underlying correlation is a crucial step in modeling complex natural systems, such as climate models. Geospatial datasets at a global scale usually suffer from non-stationarity and non-uniformly smooth spatial boundaries. A Gaussian process regression using a non-stationary covariance function has shown promise for this task, as this covariance function adapts to the variable correlation structure of the underlying distribution. In this paper, we generalize the non-stationary covariance function to address the aforementioned global scale geospatial issues. We define this generalized covariance function as an intrinsic non-stationary covariance function, because it uses intrinsic statistics of the symmetric positive definite matrices to represent the characteristic length scale and, thereby, models the local stochastic process. Experiments on a synthetic and real dataset of relative sea level changes across the world demonstrate improvements in the error metrics for the regression estimates using our newly proposed approach. version:1
arxiv-1507-02355 | The Shadows of a Cycle Cannot All Be Paths | http://arxiv.org/abs/1507.02355 | id:1507.02355 author:Prosenjit Bose, Jean-Lou De Carufel, Michael G. Dobbins, Heuna Kim, Giovanni Viglietta category:cs.CG cs.CV math.MG  published:2015-07-09 summary:A "shadow" of a subset $S$ of Euclidean space is an orthogonal projection of $S$ into one of the coordinate hyperplanes. In this paper we show that it is not possible for all three shadows of a cycle (i.e., a simple closed curve) in $\mathbb R^3$ to be paths (i.e., simple open curves). We also show two contrasting results: the three shadows of a path in $\mathbb R^3$ can all be cycles (although not all convex) and, for every $d\geq 1$, there exists a $d$-sphere embedded in $\mathbb R^{d+2}$ whose $d+2$ shadows have no holes (i.e., they deformation-retract onto a point). version:1
arxiv-1507-02347 | Achieving Synergy in Cognitive Behavior of Humanoids via Deep Learning of Dynamic Visuo-Motor-Attentional Coordination | http://arxiv.org/abs/1507.02347 | id:1507.02347 author:Jungsik Hwang, Minju Jung, Naveen Madapana, Jinhyung Kim, Minkyu Choi, Jun Tani category:cs.AI cs.LG cs.RO  published:2015-07-09 summary:The current study examines how adequate coordination among different cognitive processes including visual recognition, attention switching, action preparation and generation can be developed via learning of robots by introducing a novel model, the Visuo-Motor Deep Dynamic Neural Network (VMDNN). The proposed model is built on coupling of a dynamic vision network, a motor generation network, and a higher level network allocated on top of these two. The simulation experiments using the iCub simulator were conducted for cognitive tasks including visual object manipulation responding to human gestures. The results showed that synergetic coordination can be developed via iterative learning through the whole network when spatio-temporal hierarchy and temporal one can be self-organized in the visual pathway and in the motor pathway, respectively, such that the higher level can manipulate them with abstraction. version:1
arxiv-1507-02346 | Neural Network Classifiers for Natural Food Products | http://arxiv.org/abs/1507.02346 | id:1507.02346 author:Jaderick P. Pabico, Alona V. De Grano, Alan L. Zarsuela category:cs.CV  published:2015-07-09 summary:Two cheap, off-the-shelf machine vision systems (MVS), each using an artificial neural network (ANN) as classifier, were developed, improved and evaluated to automate the classification of tomato ripeness and acceptability of eggs, respectively. Six thousand color images of human-graded tomatoes and 750 images of human-graded eggs were used to train, test, and validate several multi-layered ANNs. The ANNs output the corresponding grade of the produce by accepting as inputs the spectral patterns of the background-less image. In both MVS, the ANN with the highest validation rate was automatically chosen by a heuristic and its performance compared to that of the human graders'. Using the validation set, the MVS correctly graded 97.00\% and 86.00\% of the tomato and egg data, respectively. The human grader's, however, were measured to perform at a daily average of 92.65\% and 72.67\% for tomato and egg grading, respectively. This results show that an ANN-based MVS is a potential alternative to manual grading. version:1
arxiv-1301-6308 | An Extragradient-Based Alternating Direction Method for Convex Minimization | http://arxiv.org/abs/1301.6308 | id:1301.6308 author:Tianyi Lin, Shiqian Ma, Shuzhong Zhang category:math.OC stat.ML  published:2013-01-27 summary:In this paper, we consider the problem of minimizing the sum of two convex functions subject to linear linking constraints. The classical alternating direction type methods usually assume that the two convex functions have relatively easy proximal mappings. However, many problems arising from statistics, image processing and other fields have the structure that while one of the two functions has easy proximal mapping, the other function is smoothly convex but does not have an easy proximal mapping. Therefore, the classical alternating direction methods cannot be applied. To deal with the difficulty, we propose in this paper an alternating direction method based on extragradients. Under the assumption that the smooth function has a Lipschitz continuous gradient, we prove that the proposed method returns an $\epsilon$-optimal solution within $O(1/\epsilon)$ iterations. We apply the proposed method to solve a new statistical model called fused logistic regression. Our numerical experiments show that the proposed method performs very well when solving the test problems. We also test the performance of the proposed method through solving the lasso problem arising from statistics and compare the result with several existing efficient solvers for this problem; the results are very encouraging indeed. version:3
arxiv-1507-02323 | Multisection in the Stochastic Block Model using Semidefinite Programming | http://arxiv.org/abs/1507.02323 | id:1507.02323 author:Naman Agarwal, Afonso S. Bandeira, Konstantinos Koiliaris, Alexandra Kolla category:cs.DS math.PR stat.ML  published:2015-07-08 summary:We consider the problem of identifying underlying community-like structures in graphs. Towards this end we study the Stochastic Block Model (SBM) on $k$-clusters: a random model on $n=km$ vertices, partitioned in $k$ equal sized clusters, with edges sampled independently across clusters with probability $q$ and within clusters with probability $p$, $p>q$. The goal is to recover the initial "hidden" partition of $[n]$. We study semidefinite programming (SDP) based algorithms in this context. In the regime $p = \frac{\alpha \log(m)}{m}$ and $q = \frac{\beta \log(m)}{m}$ we show that a certain natural SDP based algorithm solves the problem of {\em exact recovery} in the $k$-community SBM, with high probability, whenever $\sqrt{\alpha} - \sqrt{\beta} > \sqrt{1}$, as long as $k=o(\log n)$. This threshold is known to be the information theoretically optimal. We also study the case when $k=\theta(\log(n))$. In this case however we achieve recovery guarantees that no longer match the optimal condition $\sqrt{\alpha} - \sqrt{\beta} > \sqrt{1}$, thus leaving achieving optimality for this range an open question. version:1
arxiv-1506-04878 | End-to-end people detection in crowded scenes | http://arxiv.org/abs/1506.04878 | id:1506.04878 author:Russell Stewart, Mykhaylo Andriluka category:cs.CV  published:2015-06-16 summary:Current people detectors operate either by scanning an image in a sliding window fashion or by classifying a discrete set of proposals. We propose a model that is based on decoding an image into a set of people detections. Our system takes an image as input and directly outputs a set of distinct detection hypotheses. Because we generate predictions jointly, common post-processing steps such as non-maximum suppression are unnecessary. We use a recurrent LSTM layer for sequence generation and train our model end-to-end with a new loss function that operates on sets of detections. We demonstrate the effectiveness of our approach on the challenging task of detecting people in crowded scenes. version:3
arxiv-1507-02313 | Feature Representation in Convolutional Neural Networks | http://arxiv.org/abs/1507.02313 | id:1507.02313 author:Ben Athiwaratkun, Keegan Kang category:cs.CV  published:2015-07-08 summary:Convolutional Neural Networks (CNNs) are powerful models that achieve impressive results for image classification. In addition, pre-trained CNNs are also useful for other computer vision tasks as generic feature extractors. This paper aims to gain insight into the feature aspect of CNN and demonstrate other uses of CNN features. Our results show that CNN feature maps can be used with Random Forests and SVM to yield classification results that outperforms the original CNN. A CNN that is less than optimal (e.g. not fully trained or overfitting) can also extract features for Random Forest/SVM that yield competitive classification accuracy. In contrast to the literature which uses the top-layer activations as feature representation of images for other tasks, using lower-layer features can yield better results for classification. version:1
arxiv-1507-02221 | A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion | http://arxiv.org/abs/1507.02221 | id:1507.02221 author:Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob G. Simonsen, Jian-Yun Nie category:cs.NE cs.IR  published:2015-07-08 summary:Users may strive to formulate an adequate textual query for their information need. Search engines assist the users by presenting query suggestions. To preserve the original search intent, suggestions should be context-aware and account for the previous queries issued by the user. Achieving context awareness is challenging due to data sparsity. We present a probabilistic suggestion model that is able to account for sequences of previous queries of arbitrary lengths. Our novel hierarchical recurrent encoder-decoder architecture allows the model to be sensitive to the order of queries in the context while avoiding data sparsity. Additionally, our model can suggest for rare, or long-tail, queries. The produced suggestions are synthetic and are sampled one word at a time, using computationally cheap decoding techniques. This is in contrast to current synthetic suggestion models relying upon machine learning pipelines and hand-engineered feature sets. Results show that it outperforms existing context-aware approaches in a next query prediction setting. In addition to query suggestion, our model is general enough to be used in a variety of other applications. version:1
arxiv-1507-02189 | Intersecting Faces: Non-negative Matrix Factorization With New Guarantees | http://arxiv.org/abs/1507.02189 | id:1507.02189 author:Rong Ge, James Zou category:cs.LG stat.ML  published:2015-07-08 summary:Non-negative matrix factorization (NMF) is a natural model of admixture and is widely used in science and engineering. A plethora of algorithms have been developed to tackle NMF, but due to the non-convex nature of the problem, there is little guarantee on how well these methods work. Recently a surge of research have focused on a very restricted class of NMFs, called separable NMF, where provably correct algorithms have been developed. In this paper, we propose the notion of subset-separable NMF, which substantially generalizes the property of separability. We show that subset-separability is a natural necessary condition for the factorization to be unique or to have minimum volume. We developed the Face-Intersect algorithm which provably and efficiently solves subset-separable NMF under natural conditions, and we prove that our algorithm is robust to small noise. We explored the performance of Face-Intersect on simulations and discuss settings where it empirically outperformed the state-of-art methods. Our work is a step towards finding provably correct algorithms that solve large classes of NMF problems. version:1
arxiv-1507-02188 | AutoCompete: A Framework for Machine Learning Competition | http://arxiv.org/abs/1507.02188 | id:1507.02188 author:Abhishek Thakur, Artus Krohn-Grimberghe category:stat.ML cs.LG  published:2015-07-08 summary:In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data types, choosing a machine learn- ing model, tuning hyper-parameters, avoiding over-fitting and optimization for a provided evaluation metric. We also observe that the proposed system produces better (or comparable) results with less runtime as compared to other approaches. version:1
arxiv-1507-02177 | Iris Recognition Using Scattering Transform and Textural Features | http://arxiv.org/abs/1507.02177 | id:1507.02177 author:Shervin Minaee, AmirAli Abdolrashidi, Yao Wang category:cs.CV  published:2015-07-08 summary:Iris recognition has drawn a lot of attention since the mid-twentieth century. Among all biometric features, iris is known to possess a rich set of features. Different features have been used to perform iris recognition in the past. In this paper, two powerful sets of features are introduced to be used for iris recognition: scattering transform-based features and textural features. PCA is also applied on the extracted features to reduce the dimensionality of the feature vector while preserving most of the information of its initial value. Minimum distance classifier is used to perform template matching for each new test sample. The proposed scheme is tested on a well-known iris database, and showed promising results with the best accuracy rate of 99.2%. version:1
arxiv-1507-02159 | Towards Good Practices for Very Deep Two-Stream ConvNets | http://arxiv.org/abs/1507.02159 | id:1507.02159 author:Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao category:cs.CV  published:2015-07-08 summary:Deep convolutional networks have achieved great success for object recognition in still images. However, for action recognition in videos, the improvement of deep convolutional networks is not so evident. We argue that there are two reasons that could probably explain this result. First the current network architectures (e.g. Two-stream ConvNets) are relatively shallow compared with those very deep models in image domain (e.g. VGGNet, GoogLeNet), and therefore their modeling capacity is constrained by their depth. Second, probably more importantly, the training dataset of action recognition is extremely small compared with the ImageNet dataset, and thus it will be easy to over-fit on the training dataset. To address these issues, this report presents very deep two-stream ConvNets for action recognition, by adapting recent very deep architectures into video domain. However, this extension is not easy as the size of action recognition is quite small. We design several good practices for the training of very deep two-stream ConvNets, namely (i) pre-training for both spatial and temporal nets, (ii) smaller learning rates, (iii) more data augmentation techniques, (iv) high drop out ratio. Meanwhile, we extend the Caffe toolbox into Multi-GPU implementation with high computational efficiency and low memory consumption. We verify the performance of very deep two-stream ConvNets on the dataset of UCF101 and it achieves the recognition accuracy of $91.4\%$. version:1
arxiv-1507-02158 | An Empirical Study on Budget-Aware Online Kernel Algorithms for Streams of Graphs | http://arxiv.org/abs/1507.02158 | id:1507.02158 author:Giovanni Da San Martino, Nicolò Navarin, Alessandro Sperduti category:cs.LG  published:2015-07-08 summary:Kernel methods are considered an effective technique for on-line learning. Many approaches have been developed for compactly representing the dual solution of a kernel method when the problem imposes memory constraints. However, in literature no work is specifically tailored to streams of graphs. Motivated by the fact that the size of the feature space representation of many state-of-the-art graph kernels is relatively small and thus it is explicitly computable, we study whether executing kernel algorithms in the feature space can be more effective than the classical dual approach. We propose three different algorithms and various strategies for managing the budget. Efficiency and efficacy of the proposed approaches are experimentally assessed on relatively large graph streams exhibiting concept drift. It turns out that, when strict memory budget constraints have to be enforced, working in feature space, given the current state of the art on graph kernels, is more than a viable alternative to dual approaches, both in terms of speed and classification performance. version:1
arxiv-1507-02154 | Double-Base Asymmetric AdaBoost | http://arxiv.org/abs/1507.02154 | id:1507.02154 author:Iago Landesa-Vázquez, José Luis Alba-Castro category:cs.CV cs.AI cs.LG  published:2015-07-08 summary:Based on the use of different exponential bases to define class-dependent error bounds, a new and highly efficient asymmetric boosting scheme, coined as AdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical derivation procedure, unlike most of the other approaches in the literature, our algorithm preserves all the formal guarantees and properties of original (cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive AdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel derivation scheme enables an extremely efficient conditional search procedure, dramatically improving and simplifying the training phase of the algorithm. Experiments, both over synthetic and real datasets, reveal that AdaBoostDB is able to save over 99% training time with regard to Cost-Sensitive AdaBoost, providing the same cost-sensitive results. This computational advantage of AdaBoostDB can make a difference in problems managing huge pools of weak classifiers in which boosting techniques are commonly used. version:1
arxiv-1507-02150 | SAR Imaging of Moving Target based on Knowledge-aided Two-dimensional Autofocus | http://arxiv.org/abs/1507.02150 | id:1507.02150 author:Xinhua Mao category:cs.IT cs.CV math.IT  published:2015-07-08 summary:Due to uncertainty on target's motion, the range cell migration (RCM) and azimuth phase error (APE) of moving targets can't be completely compensated in synthetic aperture radar (SAR) processing. Therefore, moving targets often appear two-dimensional (2-D) defocused in SAR images. In this paper, a 2-D autofocus method for refocusing defocused moving targets in SAR images is presented. The new method only requires a direct estimate of APE, while the residual 2-D phase error ( or RCM) is computed from the estimated APE by exploiting the analytical relationship between the 2-D phase error ( or RCM) and APE. Because the parameter estimation is performed in the reduced-dimension space by exploiting prior knowledge on phase error structure, the proposed approach offers clear advantages in both computational efficiency and estimation accuracy. version:1
arxiv-1507-02144 | Spotlight the Negatives: A Generalized Discriminative Latent Model | http://arxiv.org/abs/1507.02144 | id:1507.02144 author:Hossein Azizpour, Mostafa Arefiyan, Sobhan Naderi Parizi, Stefan Carlsson category:cs.CV  published:2015-07-08 summary:Discriminative latent variable models (LVM) are frequently applied to various visual recognition tasks. In these systems the latent (hidden) variables provide a formalism for modeling structured variation of visual features. Conventionally, latent variables are de- fined on the variation of the foreground (positive) class. In this work we augment LVMs to include negative latent variables corresponding to the background class. We formalize the scoring function of such a generalized LVM (GLVM). Then we discuss a framework for learning a model based on the GLVM scoring function. We theoretically showcase how some of the current visual recognition methods can benefit from this generalization. Finally, we experiment on a generalized form of Deformable Part Models with negative latent variables and show significant improvements on two different detection tasks. version:1
arxiv-1507-02140 | Mining and Analyzing the Future Works in Scientific Articles | http://arxiv.org/abs/1507.02140 | id:1507.02140 author:Yue Hu, Xiaojun Wan category:cs.DL cs.CL cs.IR  published:2015-07-08 summary:Future works in scientific articles are valuable for researchers and they can guide researchers to new research directions or ideas. In this paper, we mine the future works in scientific articles in order to 1) provide an insight for future work analysis and 2) facilitate researchers to search and browse future works in a research area. First, we study the problem of future work extraction and propose a regular expression based method to address the problem. Second, we define four different categories for the future works by observing the data and investigate the multi-class future work classification problem. Third, we apply the extraction method and the classification model to a paper dataset in the computer science field and conduct a further analysis of the future works. Finally, we design a prototype system to search and demonstrate the future works mined from the scientific papers. Our evaluation results show that our extraction method can get high precision and recall values and our classification model can also get good results and it outperforms several baseline models. Further analysis of the future work sentences also indicates interesting results. version:1
arxiv-1502-04187 | Application of Deep Neural Network in Estimation of the Weld Bead Parameters | http://arxiv.org/abs/1502.04187 | id:1502.04187 author:Soheil Keshmiri, Xin Zheng, Chee Meng Chew, Chee Khiang Pang category:cs.LG  published:2015-02-14 summary:We present a deep learning approach to estimation of the bead parameters in welding tasks. Our model is based on a four-hidden-layer neural network architecture. More specifically, the first three hidden layers of this architecture utilize Sigmoid function to produce their respective intermediate outputs. On the other hand, the last hidden layer uses a linear transformation to generate the final output of this architecture. This transforms our deep network architecture from a classifier to a non-linear regression model. We compare the performance of our deep network with a selected number of results in the literature to show a considerable improvement in reducing the errors in estimation of these values. Furthermore, we show its scalability on estimating the weld bead parameters with same level of accuracy on combination of datasets that pertain to different welding techniques. This is a nontrivial result that is counter-intuitive to the general belief in this field of research. version:2
arxiv-1507-02086 | The Role of Pragmatics in Legal Norm Representation | http://arxiv.org/abs/1507.02086 | id:1507.02086 author:Shashishekar Ramakrishna, Lukasz Gorski, Adrian Paschke category:cs.CL cs.AI 68T30 J.1; I.2.1  published:2015-07-08 summary:Despite the 'apparent clarity' of a given legal provision, its application may result in an outcome that does not exactly conform to the semantic level of a statute. The vagueness within a legal text is induced intentionally to accommodate all possible scenarios under which such norms should be applied, thus making the role of pragmatics an important aspect also in the representation of a legal norm and reasoning on top of it. The notion of pragmatics considered in this paper does not focus on the aspects associated with judicial decision making. The paper aims to shed light on the aspects of pragmatics in legal linguistics, mainly focusing on the domain of patent law, only from a knowledge representation perspective. The philosophical discussions presented in this paper are grounded based on the legal theories from Grice and Marmor. version:1
arxiv-1507-02084 | Shedding Light on the Asymmetric Learning Capability of AdaBoost | http://arxiv.org/abs/1507.02084 | id:1507.02084 author:Iago Landesa-Vázquez, José Luis Alba-Castro category:cs.LG cs.AI cs.CV  published:2015-07-08 summary:In this paper, we propose a different insight to analyze AdaBoost. This analysis reveals that, beyond some preconceptions, AdaBoost can be directly used as an asymmetric learning algorithm, preserving all its theoretical properties. A novel class-conditional description of AdaBoost, which models the actual asymmetric behavior of the algorithm, is presented. version:1
arxiv-1504-01639 | Ego-Object Discovery | http://arxiv.org/abs/1504.01639 | id:1504.01639 author:Marc Bolaños, Petia Radeva category:cs.CV cs.AI  published:2015-04-07 summary:Lifelogging devices are spreading faster everyday. This growth can represent great benefits to develop methods for extraction of meaningful information about the user wearing the device and his/her environment. In this paper, we propose a semi-supervised strategy for easily discovering objects relevant to the person wearing a first-person camera. Given an egocentric video/images sequence acquired by the camera, our algorithm uses both the appearance extracted by means of a convolutional neural network and an object refill methodology that allows to discover objects even in case of small amount of object appearance in the collection of images. An SVM filtering strategy is applied to deal with the great part of the False Positive object candidates found by most of the state of the art object detectors. We validate our method on a new egocentric dataset of 4912 daily images acquired by 4 persons as well as on both PASCAL 2012 and MSRC datasets. We obtain for all of them results that largely outperform the state of the art approach. We make public both the EDUB dataset and the algorithm code. version:2
arxiv-1507-02062 | Multi-Document Summarization via Discriminative Summary Reranking | http://arxiv.org/abs/1507.02062 | id:1507.02062 author:Xiaojun Wan, Ziqiang Cao, Furu Wei, Sujian Li, Ming Zhou category:cs.CL  published:2015-07-08 summary:Existing multi-document summarization systems usually rely on a specific summarization model (i.e., a summarization method with a specific parameter setting) to extract summaries for different document sets with different topics. However, according to our quantitative analysis, none of the existing summarization models can always produce high-quality summaries for different document sets, and even a summarization model with good overall performance may produce low-quality summaries for some document sets. On the contrary, a baseline summarization model may produce high-quality summaries for some document sets. Based on the above observations, we treat the summaries produced by different summarization models as candidate summaries, and then explore discriminative reranking techniques to identify high-quality summaries from the candidates for difference document sets. We propose to extract a set of candidate summaries for each document set based on an ILP framework, and then leverage Ranking SVM for summary reranking. Various useful features have been developed for the reranking process, including word-level features, sentence-level features and summary-level features. Evaluation results on the benchmark DUC datasets validate the efficacy and robustness of our proposed approach. version:1
arxiv-1503-03712 | On Graduated Optimization for Stochastic Non-Convex Problems | http://arxiv.org/abs/1503.03712 | id:1503.03712 author:Elad Hazan, Kfir Y. Levy, Shai Shalev-Shwartz category:cs.LG math.OC 68  published:2015-03-12 summary:The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade. Despite its popularity, very little is known in terms of theoretical convergence analysis. In this paper we describe a new first-order algorithm based on graduated optimiza- tion and analyze its performance. We characterize a parameterized family of non- convex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an {\epsilon}-approximate solution within O(1/\epsilon^2) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of zero-order optimization, and devise a a variant of our algorithm which converges at rate of O(d^2/\epsilon^4). version:2
arxiv-1507-02020 | Generating Navigable Semantic Maps from Social Sciences Corpora | http://arxiv.org/abs/1507.02020 | id:1507.02020 author:Thierry Poibeau, Pablo Ruiz category:cs.CL cs.AI cs.IR  published:2015-07-08 summary:It is now commonplace to observe that we are facing a deluge of online information. Researchers have of course long acknowledged the potential value of this information since digital traces make it possible to directly observe, describe and analyze social facts, and above all the co-evolution of ideas and communities over time. However, most online information is expressed through text, which means it is not directly usable by machines, since computers require structured, organized and typed information in order to be able to manipulate it. Our goal is thus twofold: 1. Provide new natural language processing techniques aiming at automatically extracting relevant information from texts, especially in the context of social sciences, and connect these pieces of information so as to obtain relevant socio-semantic networks; 2. Provide new ways of exploring these socio-semantic networks, thanks to tools allowing one to dynamically navigate these networks, de-construct and re-construct them interactively, from different points of view following the needs expressed by domain experts. version:1
arxiv-1507-02012 | Hindi to English Transfer Based Machine Translation System | http://arxiv.org/abs/1507.02012 | id:1507.02012 author:Akanksha Gehlot, Vaishali Sharma, Shashi Pal Singh, Ajai Kumar category:cs.CL  published:2015-07-08 summary:In large societies like India there is a huge demand to convert one human language into another. Lots of work has been done in this area. Many transfer based MTS have developed for English to other languages, as MANTRA CDAC Pune, MATRA CDAC Pune, SHAKTI IISc Bangalore and IIIT Hyderabad. Still there is a little work done for Hindi to other languages. Currently we are working on it. In this paper we focus on designing a system, that translate the document from Hindi to English by using transfer based approach. This system takes an input text check its structure through parsing. Reordering rules are used to generate the text in target language. It is better than Corpus Based MTS because Corpus Based MTS require large amount of word aligned data for translation that is not available for many languages while Transfer Based MTS requires only knowledge of both the languages(source language and target language) to make transfer rules. We get correct translation for simple assertive sentences and almost correct for complex and compound sentences. version:1
arxiv-1507-02011 | A Bayesian Approach for Online Classifier Ensemble | http://arxiv.org/abs/1507.02011 | id:1507.02011 author:Qinxun Bai, Henry Lam, Stan Sclaroff category:cs.LG  published:2015-07-08 summary:We propose a Bayesian approach for recursively estimating the classifier weights in online learning of a classifier ensemble. In contrast with past methods, such as stochastic gradient descent or online boosting, our approach estimates the weights by recursively updating its posterior distribution. For a specified class of loss functions, we show that it is possible to formulate a suitably defined likelihood function and hence use the posterior distribution as an approximation to the global empirical loss minimizer. If the stream of training data is sampled from a stationary process, we can also show that our approach admits a superior rate of convergence to the expected loss minimizer than is possible with standard stochastic gradient descent. In experiments with real-world datasets, our formulation often performs better than state-of-the-art stochastic gradient descent and online boosting algorithms. version:1
arxiv-1412-7006 | Multi-modal Sensor Registration for Vehicle Perception via Deep Neural Networks | http://arxiv.org/abs/1412.7006 | id:1412.7006 author:Michael Giering, Vivek Venugopalan, Kishore Reddy category:cs.CV cs.LG cs.NE  published:2014-12-22 summary:The ability to simultaneously leverage multiple modes of sensor information is critical for perception of an automated vehicle's physical surroundings. Spatio-temporal alignment of registration of the incoming information is often a prerequisite to analyzing the fused data. The persistence and reliability of multi-modal registration is therefore the key to the stability of decision support systems ingesting the fused information. LiDAR-video systems like on those many driverless cars are a common example of where keeping the LiDAR and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of heterogeneous data, to detect the misalignment of the LiDAR-video inputs. A number of variations were tested on the Ford LiDAR-video driving test data set and will be discussed. To the best of our knowledge the use of multi-modal deep convolutional neural networks for dynamic real-time LiDAR-video registration has not been presented. version:2
arxiv-1412-7007 | Occlusion Edge Detection in RGB-D Frames using Deep Convolutional Networks | http://arxiv.org/abs/1412.7007 | id:1412.7007 author:Soumik Sarkar, Vivek Venugopalan, Kishore Reddy, Michael Giering, Julian Ryde, Navdeep Jaitly category:cs.CV cs.LG cs.NE  published:2014-12-22 summary:Occlusion edges in images which correspond to range discontinuity in the scene from the point of view of the observer are an important prerequisite for many vision and mobile robot tasks. Although they can be extracted from range data however extracting them from images and videos would be extremely beneficial. We trained a deep convolutional neural network (CNN) to identify occlusion edges in images and videos with both RGB-D and RGB inputs. The use of CNN avoids hand-crafting of features for automatically isolating occlusion edges and distinguishing them from appearance edges. Other than quantitative occlusion edge detection results, qualitative results are provided to demonstrate the trade-off between high resolution analysis and frame-level computation time which is critical for real-time robotics applications. version:3
arxiv-1505-01335 | Comparing persistence diagrams through complex vectors | http://arxiv.org/abs/1505.01335 | id:1505.01335 author:Barbara Di Fabio, Massimo Ferri category:math.AT cs.CV  published:2015-05-06 summary:The natural pseudo-distance of spaces endowed with filtering functions is precious for shape classification and retrieval; its optimal estimate coming from persistence diagrams is the bottleneck distance, which unfortunately suffers from combinatorial explosion. A possible algebraic representation of persistence diagrams is offered by complex polynomials; since far polynomials represent far persistence diagrams, a fast comparison of the coefficient vectors can reduce the size of the database to be classified by the bottleneck distance. This article explores experimentally three transformations from diagrams to polynomials and three distances between the complex vectors of coefficients. version:2
arxiv-1411-4798 | Memcomputing NP-complete problems in polynomial time using polynomial resources and collective states | http://arxiv.org/abs/1411.4798 | id:1411.4798 author:Fabio L. Traversa, Chiara Ramella, Fabrizio Bonani, Massimiliano Di Ventra category:cs.ET cs.NE  published:2014-11-18 summary:Memcomputing is a novel non-Turing paradigm of computation that uses interacting memory cells (memprocessors for short) to store and process information on the same physical platform. It was recently proved mathematically that memcomputing machines have the same computational power of non-deterministic Turing machines. Therefore, they can solve NP-complete problems in polynomial time and, using the appropriate architecture, with resources that only grow polynomially with the input size. The reason for this computational power stems from properties inspired by the brain and shared by any universal memcomputing machine, in particular intrinsic parallelism and information overhead, namely the capability of compressing information in the collective state of the memprocessor network. Here, we show an experimental demonstration of an actual memcomputing architecture that solves the NP-complete version of the subset-sum problem in only one step and is composed of a number of memprocessors that scales linearly with the size of the problem. We have fabricated this architecture using standard microelectronic technology so that it can be easily realized in any laboratory setting. Even though the particular machine presented here is eventually limited by noise--and will thus require error-correcting codes to scale to an arbitrary number of memprocessors--it represents the first proof-of-concept of a machine capable of working with the collective state of interacting memory cells, unlike the present-day single-state machines built using the von Neumann architecture. version:3
arxiv-1507-01160 | Correlated Multiarmed Bandit Problem: Bayesian Algorithms and Regret Analysis | http://arxiv.org/abs/1507.01160 | id:1507.01160 author:Vaibhav Srivastava, Paul Reverdy, Naomi Ehrich Leonard category:math.OC cs.LG stat.ML  published:2015-07-05 summary:We consider the correlated multiarmed bandit (MAB) problem in which the rewards associated with each arm are modeled by a multivariate Gaussian random variable, and we investigate the influence of the assumptions in the Bayesian prior on the performance of the upper credible limit (UCL) algorithm and a new correlated UCL algorithm. We rigorously characterize the influence of accuracy, confidence, and correlation scale in the prior on the decision-making performance of the algorithms. Our results show how priors and correlation structure can be leveraged to improve performance. version:2
arxiv-1410-3541 | Memcomputing with membrane memcapacitive systems | http://arxiv.org/abs/1410.3541 | id:1410.3541 author:Yuriy V. Pershin, Fabio L. Traversa, Massimiliano Di Ventra category:cs.ET cond-mat.mes-hall cs.NE  published:2014-10-14 summary:We show theoretically that networks of membrane memcapacitive systems -- capacitors with memory made out of membrane materials -- can be used to perform a complete set of logic gates in a massively parallel way by simply changing the external input amplitudes, but not the topology of the network. This polymorphism is an important characteristic of memcomputing (computing with memories) that closely reproduces one of the main features of the brain. A practical realization of these membrane memcapacitive systems, using, e.g., graphene or other 2D materials, would be a step forward towards a solid-state realization of memcomputing with passive devices. version:2
arxiv-1507-01972 | Wasserstein Training of Boltzmann Machines | http://arxiv.org/abs/1507.01972 | id:1507.01972 author:Grégoire Montavon, Klaus-Robert Müller, Marco Cuturi category:stat.ML cs.LG  published:2015-07-07 summary:The Boltzmann machine provides a useful framework to learn highly complex, multimodal and multiscale data distributions that occur in the real world. The default method to learn its parameters consists of minimizing the Kullback-Leibler (KL) divergence from training samples to the Boltzmann model. We propose in this work a novel approach for Boltzmann training which assumes that a meaningful metric between observations is given. This metric can be represented by the Wasserstein distance between distributions, for which we derive a gradient with respect to the model parameters. Minimization of this new Wasserstein objective leads to generative models that are better when considering the metric and that have a cluster-like structure. We demonstrate the practical potential of these models for data completion and denoising, for which the metric between observations plays a crucial role. version:1
arxiv-1412-6650 | Incremental Adaptation Strategies for Neural Network Language Models | http://arxiv.org/abs/1412.6650 | id:1412.6650 author:Aram Ter-Sarkisov, Holger Schwenk, Loic Barrault, Fethi Bougares category:cs.NE cs.CL cs.LG  published:2014-12-20 summary:It is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT environment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve significant improvements without overfitting the small adaptation data. version:4
arxiv-1507-00043 | Top-N recommendations in the presence of sparsity: An NCD-based approach | http://arxiv.org/abs/1507.00043 | id:1507.00043 author:Athanasios N. Nikolakopoulos, John D. Garofalakis category:cs.IR cs.AI stat.ML  published:2015-06-30 summary:Making recommendations in the presence of sparsity is known to present one of the most challenging problems faced by collaborative filtering methods. In this work we tackle this problem by exploiting the innately hierarchical structure of the item space following an approach inspired by the theory of Decomposability. We view the itemspace as a Nearly Decomposable system and we define blocks of closely related elements and corresponding indirect proximity components. We study the theoretical properties of the decomposition and we derive sufficient conditions that guarantee full item space coverage even in cold-start recommendation scenarios. A comprehensive set of experiments on the MovieLens and the Yahoo!R2Music datasets, using several widely applied performance metrics, support our model's theoretically predicted properties and verify that NCDREC outperforms several state-of-the-art algorithms, in terms of recommendation accuracy, diversity and sparseness insensitivity. version:2
arxiv-1505-07302 | Unveiling the Political Agenda of the European Parliament Plenary: A Topical Analysis | http://arxiv.org/abs/1505.07302 | id:1505.07302 author:Derek Greene, James P. Cross category:cs.CL cs.CY  published:2015-05-27 summary:This study analyzes political interactions in the European Parliament (EP) by considering how the political agenda of the plenary sessions has evolved over time and the manner in which Members of the European Parliament (MEPs) have reacted to external and internal stimuli when making Parliamentary speeches. It does so by considering the context in which speeches are made, and the content of those speeches. To detect latent themes in legislative speeches over time, speech content is analyzed using a new dynamic topic modeling method, based on two layers of matrix factorization. This method is applied to a new corpus of all English language legislative speeches in the EP plenary from the period 1999-2014. Our findings suggest that the political agenda of the EP has evolved significantly over time, is impacted upon by the committee structure of the Parliament, and reacts to exogenous events such as EU Treaty referenda and the emergence of the Euro-crisis have a significant impact on what is being discussed in Parliament. version:4
arxiv-1507-01701 | A Survey and Classification of Controlled Natural Languages | http://arxiv.org/abs/1507.01701 | id:1507.01701 author:Tobias Kuhn category:cs.CL  published:2015-07-07 summary:What is here called controlled natural language (CNL) has traditionally been given many different names. Especially during the last four decades, a wide variety of such languages have been designed. They are applied to improve communication among humans, to improve translation, or to provide natural and intuitive representations for formal notations. Despite the apparent differences, it seems sensible to put all these languages under the same umbrella. To bring order to the variety of languages, a general classification scheme is presented here. A comprehensive survey of existing English-based CNLs is given, listing and describing 100 languages from 1930 until today. Classification of these languages reveals that they form a single scattered cloud filling the conceptual space between natural languages such as English on the one end and formal languages such as propositional logic on the other. The goal of this article is to provide a common terminology and a common model for CNL, to contribute to the understanding of their general nature, to provide a starting point for researchers interested in the area, and to help developers to make design decisions. version:1
arxiv-1507-01698 | Learning Tractable Probabilistic Models for Fault Localization | http://arxiv.org/abs/1507.01698 | id:1507.01698 author:Aniruddh Nath, Pedro Domingos category:cs.SE cs.LG  published:2015-07-07 summary:In recent years, several probabilistic techniques have been applied to various debugging problems. However, most existing probabilistic debugging systems use relatively simple statistical models, and fail to generalize across multiple programs. In this work, we propose Tractable Fault Localization Models (TFLMs) that can be learned from data, and probabilistically infer the location of the bug. While most previous statistical debugging methods generalize over many executions of a single program, TFLMs are trained on a corpus of previously seen buggy programs, and learn to identify recurring patterns of bugs. Widely-used fault localization techniques such as TARANTULA evaluate the suspiciousness of each line in isolation; in contrast, a TFLM defines a joint probability distribution over buggy indicator variables for each line. Joint distributions with rich dependency structure are often computationally intractable; TFLMs avoid this by exploiting recent developments in tractable probabilistic models (specifically, Relational SPNs). Further, TFLMs can incorporate additional sources of information, including coverage-based features such as TARANTULA. We evaluate the fault localization performance of TFLMs that include TARANTULA scores as features in the probabilistic model. Our study shows that the learned TFLMs isolate bugs more effectively than previous statistical methods or using TARANTULA directly. version:1
arxiv-1507-01687 | Developing Postfix-GP Framework for Symbolic Regression Problems | http://arxiv.org/abs/1507.01687 | id:1507.01687 author:Vipul K. Dabhi, Sanjay Chaudhary category:cs.NE  published:2015-07-07 summary:This paper describes Postfix-GP system, postfix notation based Genetic Programming (GP), for solving symbolic regression problems. It presents an object-oriented architecture of Postfix-GP framework. It assists the user in understanding of the implementation details of various components of Postfix-GP. Postfix-GP provides graphical user interface which allows user to configure the experiment, to visualize evolved solutions, to analyze GP run, and to perform out-of-sample predictions. The use of Postfix-GP is demonstrated by solving the benchmark symbolic regression problem. Finally, features of Postfix-GP framework are compared with that of other GP systems. version:1
arxiv-1507-01661 | Semiblind Hyperspectral Unmixing in the Presence of Spectral Library Mismatches | http://arxiv.org/abs/1507.01661 | id:1507.01661 author:Xiao Fu, Wing-Kin Ma, José Bioucas-Dias, Tsung-Han Chan category:stat.ML  published:2015-07-07 summary:The dictionary-aided sparse regression (SR) approach has recently emerged as a promising alternative to hyperspectral unmixing (HU) in remote sensing. By using an available spectral library as a dictionary, the SR approach identifies the underlying materials in a given hyperspectral image by selecting a small subset of spectral samples in the dictionary to represent the whole image. A drawback with the current SR developments is that an actual spectral signature in the scene is often assumed to have zero mismatch with its corresponding dictionary sample, and such an assumption is considered too ideal in practice. In this paper, we tackle the spectral signature mismatch problem by proposing a dictionary-adjusted nonconvex sparsity-encouraging regression (DANSER) framework. The main idea is to incorporate dictionary correcting variables in an SR formulation. A simple and low per-iteration complexity algorithm is tailor-designed for practical realization of DANSER. Using the same dictionary correcting idea, we also propose a robust subspace solution for dictionary pruning. Extensive simulations and real-data experiments show that the proposed method is effective in mitigating the undesirable spectral signature mismatch effects. version:1
arxiv-1504-01683 | Jointly Embedding Relations and Mentions for Knowledge Population | http://arxiv.org/abs/1504.01683 | id:1504.01683 author:Miao Fan, Kai Cao, Yifan He, Ralph Grishman category:cs.CL  published:2015-04-07 summary:This paper contributes a joint embedding model for predicting relations between a pair of entities in the scenario of relation inference. It differs from most stand-alone approaches which separately operate on either knowledge bases or free texts. The proposed model simultaneously learns low-dimensional vector representations for both triplets in knowledge repositories and the mentions of relations in free texts, so that we can leverage the evidence both resources to make more accurate predictions. We use NELL to evaluate the performance of our approach, compared with cutting-edge methods. Results of extensive experiments show that our model achieves significant improvement on relation extraction. version:4
arxiv-1507-01497 | A model of sensory neural responses in the presence of unknown modulatory inputs | http://arxiv.org/abs/1507.01497 | id:1507.01497 author:Neil C. Rabinowitz, Robbe L. T. Goris, Johannes Ballé, Eero P. Simoncelli category:q-bio.NC stat.ML  published:2015-07-06 summary:Neural responses are highly variable, and some portion of this variability arises from fluctuations in modulatory factors that alter their gain, such as adaptation, attention, arousal, expected or actual reward, emotion, and local metabolic resource availability. Regardless of their origin, fluctuations in these signals can confound or bias the inferences that one derives from spiking responses. Recent work demonstrates that for sensory neurons, these effects can be captured by a modulated Poisson model, whose rate is the product of a stimulus-driven response function and an unknown modulatory signal. Here, we extend this model, by incorporating explicit modulatory elements that are known (specifically, spike-history dependence, as in previous models), and by constraining the remaining latent modulatory signals to be smooth in time. We develop inference procedures for fitting the entire model, including hyperparameters, via evidence optimization, and apply these to simulated data, and to responses of ferret auditory midbrain and cortical neurons to complex sounds. We show that integrating out the latent modulators yields better (or more readily-interpretable) receptive field estimates than a standard Poisson model. Conversely, integrating out the stimulus dependence yields estimates of the slowly-varying latent modulators. version:2
arxiv-1402-5131 | Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Noisy Matrix Decomposition | http://arxiv.org/abs/1402.5131 | id:1402.5131 author:Hanie Sedghi, Anima Anandkumar, Edmond Jonckheere category:cs.LG math.OC stat.ML  published:2014-02-20 summary:We propose an efficient ADMM method with guarantees for high-dimensional problems. We provide explicit bounds for the sparse optimization problem and the noisy matrix decomposition problem. For sparse optimization, we establish that the modified ADMM method has an optimal convergence rate of $\mathcal{O}(s\log d/T)$, where $s$ is the sparsity level, $d$ is the data dimension and $T$ is the number of steps. This matches with the minimax lower bounds for sparse estimation. For matrix decomposition into sparse and low rank components, we provide the first guarantees for any online method, and prove a convergence rate of $\tilde{\mathcal{O}}((s+r)\beta^2(p) /T) + \mathcal{O}(1/p)$ for a $p\times p$ matrix, where $s$ is the sparsity level, $r$ is the rank and $\Theta(\sqrt{p})\leq \beta(p)\leq \Theta(p)$. Our guarantees match the minimax lower bound with respect to $s,r$ and $T$. In addition, we match the minimax lower bound with respect to the matrix dimension $p$, i.e. $\beta(p)=\Theta(\sqrt{p})$, for many important statistical models including the independent noise model, the linear Bayesian network and the latent Gaussian graphical model under some conditions. Our ADMM method is based on epoch-based annealing and consists of inexpensive steps which involve projections on to simple norm balls. Experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods. In particular, we reach higher accuracy with same time complexity. version:6
arxiv-1507-01636 | Reflections on Sentiment/Opinion Analysis | http://arxiv.org/abs/1507.01636 | id:1507.01636 author:Jiwei Li, Eduard Hovy category:cs.CL  published:2015-07-06 summary:In this paper, we described possible directions for deeper understanding, helping bridge the gap between psychology / cognitive science and computational approaches in sentiment/opinion analysis literature. We focus on the opinion holder's underlying needs and their resultant goals, which, in a utilitarian model of sentiment, provides the basis for explaining the reason a sentiment valence is held. While these thoughts are still immature, scattered, unstructured, and even imaginary, we believe that these perspectives might suggest fruitful avenues for various kinds of future work. version:1
arxiv-1505-04780 | Towards Faster Rates and Oracle Property for Low-Rank Matrix Estimation | http://arxiv.org/abs/1505.04780 | id:1505.04780 author:Huan Gui, Quanquan Gu category:stat.ML  published:2015-05-18 summary:We present a unified framework for low-rank matrix estimation with nonconvex penalties. We first prove that the proposed estimator attains a faster statistical rate than the traditional low-rank matrix estimator with nuclear norm penalty. Moreover, we rigorously show that under a certain condition on the magnitude of the nonzero singular values, the proposed estimator enjoys oracle property (i.e., exactly recovers the true rank of the matrix), besides attaining a faster rate. As far as we know, this is the first work that establishes the theory of low-rank matrix estimation with nonconvex penalties, confirming the advantages of nonconvex penalties for matrix completion. Numerical experiments on both synthetic and real world datasets corroborate our theory. version:2
arxiv-1507-01569 | Emphatic Temporal-Difference Learning | http://arxiv.org/abs/1507.01569 | id:1507.01569 author:A. Rupam Mahmood, Huizhen Yu, Martha White, Richard S. Sutton category:cs.LG cs.AI  published:2015-07-06 summary:Emphatic algorithms are temporal-difference learning algorithms that change their effective state distribution by selectively emphasizing and de-emphasizing their updates on different time steps. Recent works by Sutton, Mahmood and White (2015), and Yu (2015) show that by varying the emphasis in a particular way, these algorithms become stable and convergent under off-policy training with linear function approximation. This paper serves as a unified summary of the available results from both works. In addition, we demonstrate the empirical benefits from the flexibility of emphatic algorithms, including state-dependent discounting, state-dependent bootstrapping, and the user-specified allocation of function approximation resources. version:1
arxiv-1507-01563 | A Simple Algorithm for Maximum Margin Classification, Revisited | http://arxiv.org/abs/1507.01563 | id:1507.01563 author:Sariel Har-Peled category:cs.LG  published:2015-07-06 summary:In this note, we revisit the algorithm of Har-Peled et. al. [HRZ07] for computing a linear maximum margin classifier. Our presentation is self contained, and the algorithm itself is slightly simpler than the original algorithm. The algorithm itself is a simple Perceptron like iterative algorithm. For more details and background, the reader is referred to the original paper. version:1
arxiv-1507-01529 | Correspondence Factor Analysis of Big Data Sets: A Case Study of 30 Million Words; and Contrasting Analytics using Apache Solr and Correspondence Analysis in R | http://arxiv.org/abs/1507.01529 | id:1507.01529 author:Fionn Murtagh category:cs.CL 62H25  62.07 G.3; H.2.8  published:2015-07-06 summary:We consider a large number of text data sets. These are cooking recipes. Term distribution and other distributional properties of the data are investigated. Our aim is to look at various analytical approaches which allow for mining of information on both high and low detail scales. Metric space embedding is fundamental to our interest in the semantic properties of this data. We consider the projection of all data into analyses of aggregated versions of the data. We contrast that with projection of aggregated versions of the data into analyses of all the data. Analogously for the term set, we look at analysis of selected terms. We also look at inherent term associations such as between singular and plural. In addition to our use of Correspondence Analysis in R, for latent semantic space mapping, we also use Apache Solr. Setting up the Solr server and carrying out querying is described. A further novelty is that querying is supported in Solr based on the principal factor plane mapping of all the data. This uses a bounding box query, based on factor projections. version:1
arxiv-1305-4152 | Sparse Approximate Inference for Spatio-Temporal Point Process Models | http://arxiv.org/abs/1305.4152 | id:1305.4152 author:Botond Cseke, Andrew Zammit Mangion, Tom Heskes, Guido Sanguinetti category:stat.ML  published:2013-05-17 summary:Spatio-temporal point process models play a central role in the analysis of spatially distributed systems in several disciplines. Yet, scalable inference remains computa- tionally challenging both due to the high resolution modelling generally required and the analytically intractable likelihood function. Here, we exploit the sparsity structure typical of (spatially) discretised log-Gaussian Cox process models by using approximate message-passing algorithms. The proposed algorithms scale well with the state dimension and the length of the temporal horizon with moderate loss in distributional accuracy. They hence provide a flexible and faster alternative to both non-linear filtering-smoothing type algorithms and to approaches that implement the Laplace method or expectation propagation on (block) sparse latent Gaussian models. We infer the parameters of the latent Gaussian model using a structured variational Bayes approach. We demonstrate the proposed framework on simulation studies with both Gaussian and point-process observations and use it to reconstruct the conflict intensity and dynamics in Afghanistan from the WikiLeaks Afghan War Diary. version:5
arxiv-1507-01476 | Semi-proximal Mirror-Prox for Nonsmooth Composite Minimization | http://arxiv.org/abs/1507.01476 | id:1507.01476 author:Niao He, Zaid Harchaoui category:math.OC cs.LG  published:2015-07-06 summary:We propose a new first-order optimisation algorithm to solve high-dimensional non-smooth composite minimisation problems. Typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularisation penalty. The proposed algorithm, called Semi-Proximal Mirror-Prox, leverages the Fenchel-type representation of one part of the objective while handling the other part of the objective via linear minimization over the domain. The algorithm stands in contrast with more classical proximal gradient algorithms with smoothing, which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems. We establish the theoretical convergence rate of Semi-Proximal Mirror-Prox, which exhibits the optimal complexity bounds, i.e. $O(1/\epsilon^2)$, for the number of calls to linear minimization oracle. We present promising experimental results showing the interest of the approach in comparison to competing methods. version:1
arxiv-1507-01461 | Revisiting Large Scale Distributed Machine Learning | http://arxiv.org/abs/1507.01461 | id:1507.01461 author:Radu Cristian Ionescu category:cs.DC cs.LG  published:2015-07-06 summary:Nowadays, with the widespread of smartphones and other portable gadgets equipped with a variety of sensors, data is ubiquitous available and the focus of machine learning has shifted from being able to infer from small training samples to dealing with large scale high-dimensional data. In domains such as personal healthcare applications, which motivates this survey, distributed machine learning is a promising line of research, both for scaling up learning algorithms, but mostly for dealing with data which is inherently produced at different locations. This report offers a thorough overview of and state-of-the-art algorithms for distributed machine learning, for both supervised and unsupervised learning, ranging from simple linear logistic regression to graphical models and clustering. We propose future directions for most categories, specific to the potential personal healthcare applications. With this in mind, the report focuses on how security and low communication overhead can be assured in the specific case of a strictly client-server architectural model. As particular directions we provides an exhaustive presentation of an empirical clustering algorithm, k-windows, and proposed an asynchronous distributed machine learning algorithm that would scale well and also would be computationally cheap and easy to implement. version:1
arxiv-1505-04935 | Towards Data-Driven Autonomics in Data Centers | http://arxiv.org/abs/1505.04935 | id:1505.04935 author:Alina Sîrbu, Ozalp Babaoglu category:cs.DC cs.AI stat.ML  published:2015-05-19 summary:Continued reliance on human operators for managing data centers is a major impediment for them from ever reaching extreme dimensions. Large computer systems in general, and data centers in particular, will ultimately be managed using predictive computational and executable models obtained through data-science tools, and at that point, the intervention of humans will be limited to setting high-level goals and policies rather than performing low-level operations. Data-driven autonomics, where management and control are based on holistic predictive models that are built and updated using generated data, opens one possible path towards limiting the role of operators in data centers. In this paper, we present a data-science study of a public Google dataset collected in a 12K-node cluster with the goal of building and evaluating a predictive model for node failures. We use BigQuery, the big data SQL platform from the Google Cloud suite, to process massive amounts of data and generate a rich feature set characterizing machine state over time. We describe how an ensemble classifier can be built out of many Random Forest classifiers each trained on these features, to predict if machines will fail in a future 24-hour window. Our evaluation reveals that if we limit false positive rates to 5%, we can achieve true positive rates between 27% and 88% with precision varying between 50% and 72%. We discuss the practicality of including our predictive model as the central component of a data-driven autonomic manager and operating it on-line with live data streams (rather than off-line on data logs). All of the scripts used for BigQuery and classification analyses are publicly available from the authors' website. version:2
arxiv-1507-01442 | Learning Better Encoding for Approximate Nearest Neighbor Search with Dictionary Annealing | http://arxiv.org/abs/1507.01442 | id:1507.01442 author:Shicong Liu, Hongtao Lu category:cs.CV  published:2015-07-06 summary:We introduce a novel dictionary optimization method for high-dimensional vector quantization employed in approximate nearest neighbor (ANN) search. Vector quantization methods first seek a series of dictionaries, then approximate each vector by a sum of elements selected from these dictionaries. An optimal series of dictionaries should be mutually independent, and each dictionary should generate a balanced encoding for the target dataset. Existing methods did not explicitly consider this. To achieve these goals along with minimizing the quantization error (residue), we propose a novel dictionary optimization method called \emph{Dictionary Annealing} that alternatively "heats up" a single dictionary by generating an intermediate dataset with residual vectors, "cools down" the dictionary by fitting the intermediate dataset, then extracts the new residual vectors for the next iteration. Better codes can be learned by DA for the ANN search tasks. DA is easily implemented on GPU to utilize the latest computing technology, and can easily extended to an online dictionary learning scheme. We show by experiments that our optimized dictionaries substantially reduce the overall quantization error. Jointly used with residual vector quantization, our optimized dictionaries lead to a better approximate nearest neighbor search performance compared to the state-of-the-art methods. version:1
arxiv-1507-01422 | End-to-end Convolutional Network for Saliency Prediction | http://arxiv.org/abs/1507.01422 | id:1507.01422 author:Junting Pan, Xavier Giró-i-Nieto category:cs.CV cs.LG cs.NE  published:2015-07-06 summary:The prediction of saliency areas in images has been traditionally addressed with hand crafted features based on neuroscience principles. This paper however addresses the problem with a completely data-driven approach by training a convolutional network. The learning process is formulated as a minimization of a loss function that measures the Euclidean distance of the predicted saliency map with the provided ground truth. The recent publication of large datasets of saliency prediction has provided enough data to train a not very deep architecture which is both fast and accurate. The convolutional network in this paper, named JuntingNet, won the LSUN 2015 challenge on saliency prediction with a superior performance in all considered metrics. version:1
arxiv-1507-01892 | A linear approach for sparse coding by a two-layer neural network | http://arxiv.org/abs/1507.01892 | id:1507.01892 author:Alessandro Montalto, Giovanni Tessitore, Roberto Prevete category:cs.LG physics.data-an  published:2015-07-06 summary:Many approaches to transform classification problems from non-linear to linear by feature transformation have been recently presented in the literature. These notably include sparse coding methods and deep neural networks. However, many of these approaches require the repeated application of a learning process upon the presentation of unseen data input vectors, or else involve the use of large numbers of parameters and hyper-parameters, which must be chosen through cross-validation, thus increasing running time dramatically. In this paper, we propose and experimentally investigate a new approach for the purpose of overcoming limitations of both kinds. The proposed approach makes use of a linear auto-associative network (called SCNN) with just one hidden layer. The combination of this architecture with a specific error function to be minimized enables one to learn a linear encoder computing a sparse code which turns out to be as similar as possible to the sparse coding that one obtains by re-training the neural network. Importantly, the linearity of SCNN and the choice of the error function allow one to achieve reduced running time in the learning phase. The proposed architecture is evaluated on the basis of two standard machine learning tasks. Its performances are compared with those of recently proposed non-linear auto-associative neural networks. The overall results suggest that linear encoders can be profitably used to obtain sparse data representations in the context of machine learning problems, provided that an appropriate error function is used during the learning phase. version:1
arxiv-1406-5309 | Early Recognition of Human Activities from First-Person Videos Using Onset Representations | http://arxiv.org/abs/1406.5309 | id:1406.5309 author:M. S. Ryoo, Thomas J. Fuchs, Lu Xia, J. K. Aggarwal, Larry Matthies category:cs.CV  published:2014-06-20 summary:In this paper, we propose a methodology for early recognition of human activities from videos taken with a first-person viewpoint. Early recognition, which is also known as activity prediction, is an ability to infer an ongoing activity at its early stage. We present an algorithm to perform recognition of activities targeted at the camera from streaming videos, making the system to predict intended activities of the interacting person and avoid harmful events before they actually happen. We introduce the novel concept of 'onset' that efficiently summarizes pre-activity observations, and design an approach to consider event history in addition to ongoing video observation for early first-person recognition of activities. We propose to represent onset using cascade histograms of time series gradients, and we describe a novel algorithmic setup to take advantage of onset for early recognition of activities. The experimental results clearly illustrate that the proposed concept of onset enables better/earlier recognition of human activities from first-person videos. version:2
arxiv-1507-01330 | Visual Data Deblocking using Structural Layer Priors | http://arxiv.org/abs/1507.01330 | id:1507.01330 author:Xiaojie Guo category:cs.CV  published:2015-07-06 summary:The blocking artifact frequently appears in compressed real-world images or video sequences, especially coded at low bit rates, which is visually annoying and likely hurts the performance of many computer vision algorithms. A compressed frame can be viewed as the superimposition of an intrinsic layer and an artifact one. Recovering the two layers from such frames seems to be a severely ill-posed problem since the number of unknowns to recover is twice as many as the given measurements. In this paper, we propose a simple and robust method to separate these two layers, which exploits structural layer priors including the gradient sparsity of the intrinsic layer, and the independence of the gradient fields of the two layers. A novel Augmented Lagrangian Multiplier based algorithm is designed to efficiently and effectively solve the recovery problem. Extensive experimental results demonstrate the superior performance of our method over the state of the arts, in terms of visual quality and simplicity. version:1
arxiv-1501-02876 | Deep Image: Scaling up Image Recognition | http://arxiv.org/abs/1501.02876 | id:1501.02876 author:Ren Wu, Shengen Yan, Yi Shan, Qingqing Dang, Gang Sun category:cs.CV  published:2015-01-13 summary:We present a state-of-the-art image recognition system, Deep Image, developed using end-to-end deep learning. The key components are a custom-built supercomputer dedicated to deep learning, a highly optimized parallel algorithm using new strategies for data partitioning and communication, larger deep neural network models, novel data augmentation approaches, and usage of multi-scale high-resolution images. Our method achieves excellent results on multiple challenging computer vision benchmarks. version:5
arxiv-1507-01307 | Subspace-Sparse Representation | http://arxiv.org/abs/1507.01307 | id:1507.01307 author:C. You, R. Vidal category:stat.ML cs.IT math.IT  published:2015-07-06 summary:Given an overcomplete dictionary $A$ and a signal $b$ that is a linear combination of a few linearly independent columns of $A$, classical sparse recovery theory deals with the problem of recovering the unique sparse representation $x$ such that $b = A x$. It is known that under certain conditions on $A$, $x$ can be recovered by the Basis Pursuit (BP) and the Orthogonal Matching Pursuit (OMP) algorithms. In this work, we consider the more general case where $b$ lies in a low-dimensional subspace spanned by some columns of $A$, which are possibly linearly dependent. In this case, the sparsest solution $x$ is generally not unique, and we study the problem that the representation $x$ identifies the subspace, i.e. the nonzero entries of $x$ correspond to dictionary atoms that are in the subspace. Such a representation $x$ is called subspace-sparse. We present sufficient conditions for guaranteeing subspace-sparse recovery, which have clear geometric interpretations and explain properties of subspace-sparse recovery. We also show that the sufficient conditions can be satisfied under a randomized model. Our results are applicable to the traditional sparse recovery problem and we get conditions for sparse recovery that are less restrictive than the canonical mutual coherent condition. We also use the results to analyze the sparse representation based classification (SRC) method, for which we get conditions to show its correctness. version:1
arxiv-1507-01269 | Semi-supervised Multi-sensor Classification via Consensus-based Multi-View Maximum Entropy Discrimination | http://arxiv.org/abs/1507.01269 | id:1507.01269 author:Tianpei Xie, Nasser M. Nasrabadi, Alfred O. Hero III category:cs.IT cs.AI cs.LG math.IT  published:2015-07-05 summary:In this paper, we consider multi-sensor classification when there is a large number of unlabeled samples. The problem is formulated under the multi-view learning framework and a Consensus-based Multi-View Maximum Entropy Discrimination (CMV-MED) algorithm is proposed. By iteratively maximizing the stochastic agreement between multiple classifiers on the unlabeled dataset, the algorithm simultaneously learns multiple high accuracy classifiers. We demonstrate that our proposed method can yield improved performance over previous multi-view learning approaches by comparing performance on three real multi-sensor data sets. version:1
arxiv-1507-01251 | Autoencoding the Retrieval Relevance of Medical Images | http://arxiv.org/abs/1507.01251 | id:1507.01251 author:Zehra Camlica, H. R. Tizhoosh, Farzad Khalvati category:cs.CV  published:2015-07-05 summary:Content-based image retrieval (CBIR) of medical images is a crucial task that can contribute to a more reliable diagnosis if applied to big data. Recent advances in feature extraction and classification have enormously improved CBIR results for digital images. However, considering the increasing accessibility of big data in medical imaging, we are still in need of reducing both memory requirements and computational expenses of image retrieval systems. This work proposes to exclude the features of image blocks that exhibit a low encoding error when learned by a $n/p/n$ autoencoder ($p\!<\!n$). We examine the histogram of autoendcoding errors of image blocks for each image class to facilitate the decision which image regions, or roughly what percentage of an image perhaps, shall be declared relevant for the retrieval task. This leads to reduction of feature dimensionality and speeds up the retrieval process. To validate the proposed scheme, we employ local binary patterns (LBP) and support vector machines (SVM) which are both well-established approaches in CBIR research community. As well, we use IRMA dataset with 14,410 x-ray images as test data. The results show that the dimensionality of annotated feature vectors can be reduced by up to 50% resulting in speedups greater than 27% at expense of less than 1% decrease in the accuracy of retrieval when validating the precision and recall of the top 20 hits. version:1
arxiv-1507-01239 | Experiments on Parallel Training of Deep Neural Network using Model Averaging | http://arxiv.org/abs/1507.01239 | id:1507.01239 author:Hang Su, Haoyu Chen category:cs.LG cs.NE  published:2015-07-05 summary:In this work we apply model averaging to parallel training of deep neural network (DNN). Parallelization is done in a model averaging manner. Data is partitioned and distributed to different nodes for local model updates, and model averaging across nodes is done every few minibatches. We use multiple GPUs for data parallelization, and Message Passing Interface (MPI) for communication between nodes, which allows us to perform model averaging frequently without losing much time on communication. We investigate the effectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) and Restricted Boltzmann Machine (RBM) pretraining for parallel training in model-averaging framework, and explore the best setups in term of different learning rate schedules, averaging frequencies and minibatch sizes. It is shown that NG-SGD and RBM pretraining benefits parameter-averaging based model training. On the 300h Switchboard dataset, a 9.3 times speedup is achieved using 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracy loss. version:1
arxiv-1412-6547 | Fast Label Embeddings via Randomized Linear Algebra | http://arxiv.org/abs/1412.6547 | id:1412.6547 author:Paul Mineiro, Nikos Karampatziakis category:cs.LG  published:2014-12-19 summary:Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results. version:7
arxiv-1507-01209 | TV News Commercials Detection using Success based Locally Weighted Kernel Combination | http://arxiv.org/abs/1507.01209 | id:1507.01209 author:Raghvendra Kannao, Prithwijit Guha category:cs.CV cs.MM  published:2015-07-05 summary:Commercial detection in news broadcast videos involves judicious selection of meaningful audio-visual feature combinations and efficient classifiers. And, this problem becomes much simpler if these combinations can be learned from the data. To this end, we propose an Multiple Kernel Learning based method for boosting successful kernel functions while ignoring the irrelevant ones. We adopt a intermediate fusion approach where, a SVM is trained with a weighted linear combination of different kernel functions instead of single kernel function. Each kernel function is characterized by a feature set and kernel type. We identify the feature sub-space locations of the prediction success of a particular classifier trained only with particular kernel function. We propose to estimate a weighing function using support vector regression (with RBF kernel) for each kernel function which has high values (near 1.0) where the classifier learned on kernel function succeeded and lower values (nearly 0.0) otherwise. Second contribution of this work is TV News Commercials Dataset of 150 Hours of News videos. Classifier trained with our proposed scheme has outperformed the baseline methods on 6 of 8 benchmark dataset and our own TV commercials dataset. version:1
arxiv-1507-01208 | Parsimonious Labeling | http://arxiv.org/abs/1507.01208 | id:1507.01208 author:Puneet K. Dokania, M. Pawan Kumar category:cs.CV  published:2015-07-05 summary:We propose a new family of discrete energy minimization problems, which we call parsimonious labeling. Specifically, our energy functional consists of unary potentials and high-order clique potentials. While the unary potentials are arbitrary, the clique potentials are proportional to the {\em diversity} of set of the unique labels assigned to the clique. Intuitively, our energy functional encourages the labeling to be parsimonious, that is, use as few labels as possible. This in turn allows us to capture useful cues for important computer vision applications such as stereo correspondence and image denoising. Furthermore, we propose an efficient graph-cuts based algorithm for the parsimonious labeling problem that provides strong theoretical guarantees on the quality of the solution. Our algorithm consists of three steps. First, we approximate a given diversity using a mixture of a novel hierarchical $P^n$ Potts model. Second, we use a divide-and-conquer approach for each mixture component, where each subproblem is solved using an effficient $\alpha$-expansion algorithm. This provides us with a small number of putative labelings, one for each mixture component. Third, we choose the best putative labeling in terms of the energy value. Using both sythetic and standard real datasets, we show that our algorithm significantly outperforms other graph-cuts based approaches. version:1
arxiv-1507-01193 | Dependency Recurrent Neural Language Models for Sentence Completion | http://arxiv.org/abs/1507.01193 | id:1507.01193 author:Piotr Mirowski, Andreas Vlachos category:cs.CL cs.AI cs.LG  published:2015-07-05 summary:Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the state-of-the-art models on this task. version:1
arxiv-1507-01154 | Inference for determinantal point processes without spectral knowledge | http://arxiv.org/abs/1507.01154 | id:1507.01154 author:Rémi Bardenet, Michalis K. Titsias category:stat.CO stat.ML  published:2015-07-04 summary:Determinantal point processes (DPPs) are point process models that naturally encode diversity between the points of a given realization, through a positive definite kernel $K$. DPPs possess desirable properties, such as exact sampling or analyticity of the moments, but learning the parameters of kernel $K$ through likelihood-based inference is not straightforward. First, the kernel that appears in the likelihood is not $K$, but another kernel $L$ related to $K$ through an often intractable spectral decomposition. This issue is typically bypassed in machine learning by directly parametrizing the kernel $L$, at the price of some interpretability of the model parameters. We follow this approach here. Second, the likelihood has an intractable normalizing constant, which takes the form of a large determinant in the case of a DPP over a finite set of objects, and the form of a Fredholm determinant in the case of a DPP over a continuous domain. Our main contribution is to derive bounds on the likelihood of a DPP, both for finite and continuous domains. Unlike previous work, our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator. Through usual arguments, these bounds thus yield cheap variational inference and moderately expensive exact Markov chain Monte Carlo inference methods for DPPs. version:1
arxiv-1506-03844 | Techniques for effective and efficient fire detection from social media images | http://arxiv.org/abs/1506.03844 | id:1506.03844 author:Marcos Bedo, Gustavo Blanco, Willian Oliveira, Mirela Cazzolato, Alceu Costa, Jose Rodrigues, Agma Traina, Caetano Traina Jr category:cs.CV  published:2015-06-11 summary:Social media could provide valuable information to support decision making in crisis management, such as in accidents, explosions and fires. However, much of the data from social media are images, which are uploaded in a rate that makes it impossible for human beings to analyze them. Despite the many works on image analysis, there are no fire detection studies on social media. To fill this gap, we propose the use and evaluation of a broad set of content-based image retrieval and classification techniques for fire detection. Our main contributions are: (i) the development of the Fast-Fire Detection method (FFDnR), which combines feature extractor and evaluation functions to support instance-based learning, (ii) the construction of an annotated set of images with ground-truth depicting fire occurrences -- the FlickrFire dataset, and (iii) the evaluation of 36 efficient image descriptors for fire detection. Using real data from Flickr, our results showed that FFDnR was able to achieve a precision for fire detection comparable to that of human annotators. Therefore, our work shall provide a solid basis for further developments on monitoring images from social media. version:2
arxiv-1507-01127 | AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes | http://arxiv.org/abs/1507.01127 | id:1507.01127 author:Sascha Rothe, Hinrich Schütze category:cs.CL  published:2015-07-04 summary:We present \textit{AutoExtend}, a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embeddings as input and does not need an additional training corpus. The synset/lexeme embeddings obtained live in the same vector space as the word embeddings. A sparse tensor formalization guarantees efficiency and parallelizability. We use WordNet as a lexical resource, but AutoExtend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks. version:1
arxiv-1504-00091 | Learning in the Presence of Corruption | http://arxiv.org/abs/1504.00091 | id:1504.00091 author:Brendan van Rooyen, Robert C. Williamson category:stat.ML cs.LG  published:2015-04-01 summary:In supervised learning one wishes to identify a pattern present in a joint distribution $P$, of instances, label pairs, by providing a function $f$ from instances to labels that has low risk $\mathbb{E}_{P}\ell(y,f(x))$. To do so, the learner is given access to $n$ iid samples drawn from $P$. In many real world problems clean samples are not available. Rather, the learner is given access to samples from a corrupted distribution $\tilde{P}$ from which to learn, while the goal of predicting the clean pattern remains. There are many different types of corruption one can consider, and as of yet there is no general means to compare the relative ease of learning under these different corruption processes. In this paper we develop a general framework for tackling such problems as well as introducing upper and lower bounds on the risk for learning in the presence of corruption. Our ultimate goal is to be able to make informed economic decisions in regards to the acquisition of data sets. For a certain subclass of corruption processes (those that are \emph{reconstructible}) we achieve this goal in a particular sense. Our lower bounds are in terms of the coefficient of ergodicity, a simple to calculate property of stochastic matrices. Our upper bounds proceed via a generalization of the method of unbiased estimators appearing in recent work of Natarajan et al and implicit in the earlier work of Kearns. version:2
arxiv-1501-02372 | Efficient Rotation-Scaling-Translation Parameters Estimation Based on Fractal Image Model | http://arxiv.org/abs/1501.02372 | id:1501.02372 author:M. Uss, B. Vozel, V. Lukin, K. Chehdi category:cs.CV  published:2015-01-10 summary:This paper deals with area-based subpixel image registration under rotation-isometric scaling-translation transformation hypothesis. Our approach is based on a parametrical modeling of geometrically transformed textural image fragments and maximum likelihood estimation of transformation vector between them. Due to the parametrical approach based on the fractional Brownian motion modeling of the local fragments texture, the proposed estimator MLfBm (ML stands for "Maximum Likelihood" and fBm for "Fractal Brownian motion") has the ability to better adapt to real image texture content compared to other methods relying on universal similarity measures like mutual information or normalized correlation. The main benefits are observed when assumptions underlying the fBm model are fully satisfied, e.g. for isotropic normally distributed textures with stationary increments. Experiments on both simulated and real images and for high and weak correlation between registered images show that the MLfBm estimator offers significant improvement compared to other state-of-the-art methods. It reduces translation vector, rotation angle and scaling factor estimation errors by a factor of about 1.75...2 and it decreases probability of false match by up to 5 times. Besides, an accurate confidence interval for MLfBm estimates can be obtained from the Cramer-Rao lower bound on rotation-scaling-translation parameters estimation error. This bound depends on texture roughness, noise level in reference and template images, correlation between these images and geometrical transformation parameters. version:2
arxiv-1506-01437 | ShapeFit: Exact location recovery from corrupted pairwise directions | http://arxiv.org/abs/1506.01437 | id:1506.01437 author:Paul Hand, Choongbum Lee, Vladislav Voroninski category:cs.CV cs.IT math.CO math.IT math.OC  published:2015-06-04 summary:Let $t_1,\ldots,t_n \in \mathbb{R}^d$ and consider the location recovery problem: given a subset of pairwise direction observations $\{(t_i - t_j) / \ t_i - t_j\ _2\}_{i<j \in [n] \times [n]}$, where a constant fraction of these observations are arbitrarily corrupted, find $\{t_i\}_{i=1}^n$ up to a global translation and scale. We propose a novel algorithm for the location recovery problem, which consists of a simple convex program over $dn$ real variables. We prove that this program recovers a set of $n$ i.i.d. Gaussian locations exactly and with high probability if the observations are given by an \erdosrenyi graph, $d$ is large enough, and provided that at most a constant fraction of observations involving any particular location are adversarially corrupted. We also prove that the program exactly recovers Gaussian locations for $d=3$ if the fraction of corrupted observations at each location is, up to poly-logarithmic factors, at most a constant. Both of these recovery theorems are based on a set of deterministic conditions that we prove are sufficient for exact recovery. version:2
arxiv-1507-01053 | Describing Multimedia Content using Attention-based Encoder--Decoder Networks | http://arxiv.org/abs/1507.01053 | id:1507.01053 author:Kyunghyun Cho, Aaron Courville, Yoshua Bengio category:cs.NE cs.CL cs.CV cs.LG  published:2015-07-04 summary:Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism. version:1
arxiv-1502-03508 | Adding vs. Averaging in Distributed Primal-Dual Optimization | http://arxiv.org/abs/1502.03508 | id:1502.03508 author:Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan, Peter Richtárik, Martin Takáč category:cs.LG 90C25  68W15 G.1.6; C.1.4  published:2015-02-12 summary:Distributed optimization methods for large-scale machine learning suffer from a communication bottleneck. It is difficult to reduce this bottleneck while still efficiently and accurately aggregating partial work from different machines. In this paper, we present a novel generalization of the recent communication-efficient primal-dual framework (CoCoA) for distributed optimization. Our framework, CoCoA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes with convergence guarantees only allow conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both CoCoA as well as our new variants, and generalize the theory for both methods to cover non-smooth convex loss functions. We provide an extensive experimental comparison that shows the markedly improved performance of CoCoA+ on several real-world distributed datasets, especially when scaling up the number of machines. version:2
arxiv-1504-07116 | Meta learning of bounds on the Bayes classifier error | http://arxiv.org/abs/1504.07116 | id:1504.07116 author:Kevin R. Moon, Veronique Delouille, Alfred O. Hero III category:cs.LG astro-ph.SR cs.CV cs.IT math.IT  published:2015-04-27 summary:Meta learning uses information from base learners (e.g. classifiers or estimators) as well as information about the learning problem to improve upon the performance of a single base learner. For example, the Bayes error rate of a given feature space, if known, can be used to aid in choosing a classifier, as well as in feature selection and model selection for the base classifiers and the meta classifier. Recent work in the field of f-divergence functional estimation has led to the development of simple and rapidly converging estimators that can be used to estimate various bounds on the Bayes error. We estimate multiple bounds on the Bayes error using an estimator that applies meta learning to slowly converging plug-in estimators to obtain the parametric convergence rate. We compare the estimated bounds empirically on simulated data and then estimate the tighter bounds on features extracted from an image patch analysis of sunspot continuum and magnetogram images. version:2
arxiv-1507-00913 | Fine-grained Recognition Datasets for Biodiversity Analysis | http://arxiv.org/abs/1507.00913 | id:1507.00913 author:Erik Rodner, Marcel Simon, Gunnar Brehm, Stephanie Pietsch, J. Wolfgang Wägele, Joachim Denzler category:cs.CV  published:2015-07-03 summary:In the following paper, we present and discuss challenging applications for fine-grained visual classification (FGVC): biodiversity and species analysis. We not only give details about two challenging new datasets suitable for computer vision research with up to 675 highly similar classes, but also present first results with localized features using convolutional neural networks (CNN). We conclude with a list of challenging new research directions in the area of visual classification for biodiversity research. version:1
arxiv-1507-00908 | LogDet Rank Minimization with Application to Subspace Clustering | http://arxiv.org/abs/1507.00908 | id:1507.00908 author:Zhao Kang, Chong Peng, Jie Cheng, Qiang Chen category:cs.CV cs.LG stat.ML  published:2015-07-03 summary:Low-rank matrix is desired in many machine learning and computer vision problems. Most of the recent studies use the nuclear norm as a convex surrogate of the rank operator. However, all singular values are simply added together by the nuclear norm, and thus the rank may not be well approximated in practical problems. In this paper, we propose to use a log-determinant (LogDet) function as a smooth and closer, though non-convex, approximation to rank for obtaining a low-rank representation in subspace clustering. Augmented Lagrange multipliers strategy is applied to iteratively optimize the LogDet-based non-convex objective function on potentially large-scale data. By making use of the angular information of principal directions of the resultant low-rank representation, an affinity graph matrix is constructed for spectral clustering. Experimental results on motion segmentation and face clustering data demonstrate that the proposed method often outperforms state-of-the-art subspace clustering algorithms. version:1
arxiv-1507-00827 | Estimating the number of communities in networks by spectral methods | http://arxiv.org/abs/1507.00827 | id:1507.00827 author:Can M. Le, Elizaveta Levina category:stat.ML cs.SI math.ST stat.TH 62H30  62G99  published:2015-07-03 summary:Community detection is a fundamental problem in network analysis with many methods available to estimate communities. Most of these methods assume that the number of communities is known, which is often not the case in practice. We propose a simple and very fast method for estimating the number of communities based on the spectral properties of certain graph operators, such as the non-backtracking matrix and the Bethe Hessian matrix. We show that the method performs well under several models and a wide range of parameters, and is guaranteed to be consistent under several asymptotic regimes. We compare the new method to several existing methods for estimating the number of communities and show that it is both more accurate and more computationally efficient. version:1
arxiv-1507-00825 | Ridge Regression, Hubness, and Zero-Shot Learning | http://arxiv.org/abs/1507.00825 | id:1507.00825 author:Yutaro Shigeto, Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Yuji Matsumoto category:cs.LG stat.ML  published:2015-07-03 summary:This paper discusses the effect of hubness in zero-shot learning, when ridge regression is used to find a mapping between the example space to the label space. Contrary to the existing approach, which attempts to find a mapping from the example space to the label space, we show that mapping labels into the example space is desirable to suppress the emergence of hubs in the subsequent nearest neighbor search step. Assuming a simple data model, we prove that the proposed approach indeed reduces hubness. This was verified empirically on the tasks of bilingual lexicon extraction and image labeling: hubness was reduced with both of these tasks and the accuracy was improved accordingly. version:1
arxiv-1506-00821 | A Generalized Labeled Multi-Bernoulli Filter Implementation using Gibbs Sampling | http://arxiv.org/abs/1506.00821 | id:1506.00821 author:Hung Gia Hoang, Ba-Tuong Vo, Ba-Ngu Vo category:stat.CO cs.LG  published:2015-06-02 summary:This paper proposes an efficient implementation of the generalized labeled multi-Bernoulli (GLMB) filter by combining the prediction and update into a single step. In contrast to the original approach which involves separate truncations in the prediction and update steps, the proposed implementation requires only one single truncation for each iteration, which can be performed using a standard ranked optimal assignment algorithm. Furthermore, we propose a new truncation technique based on Markov Chain Monte Carlo methods such as Gibbs sampling, which drastically reduces the complexity of the filter. The superior performance of the proposed approach is demonstrated through extensive numerical studies. version:3
arxiv-1507-00824 | D-MFVI: Distributed Mean Field Variational Inference using Bregman ADMM | http://arxiv.org/abs/1507.00824 | id:1507.00824 author:Behnam Babagholami-Mohamadabadi, Sejong Yoon, Vladimir Pavlovic category:cs.LG stat.ML  published:2015-07-03 summary:Bayesian models provide a framework for probabilistic modelling of complex datasets. However, many of such models are computationally demanding especially in the presence of large datasets. On the other hand, in sensor network applications, statistical (Bayesian) parameter estimation usually needs distributed algorithms, in which both data and computation are distributed across the nodes of the network. In this paper we propose a general framework for distributed Bayesian learning using Bregman Alternating Direction Method of Multipliers (B-ADMM). We demonstrate the utility of our framework, with Mean Field Variational Bayes (MFVB) as the primitive for distributed Matrix Factorization (MF) and distributed affine structure from motion (SfM). version:1
arxiv-1507-00803 | Optimal design of experiments in the presence of network-correlated outcomes | http://arxiv.org/abs/1507.00803 | id:1507.00803 author:Guillaume W. Basse, Edoardo M. Airoldi category:stat.ME cs.SI physics.soc-ph stat.ML  published:2015-07-03 summary:We consider the problem of how to assign treatment in a randomized experiment, when the correlation among the outcomes is informed by a network available pre-intervention. Working within the potential outcome causal framework, we develop a class of models that posit such a correlation structure among the outcomes, and a strategy for allocating treatment optimally, for the goal of minimizing the integrated mean squared error of the estimated average treatment effect. We provide insights into features of the optimal designs via an analytical decomposition of the mean squared error used for optimization. We illustrate how the proposed treatment allocation strategy improves on allocations that ignore the network structure, with extensive simulations. version:1
arxiv-1505-06449 | Efficient Elastic Net Regularization for Sparse Linear Models | http://arxiv.org/abs/1505.06449 | id:1505.06449 author:Zachary C. Lipton, Charles Elkan category:cs.LG  published:2015-05-24 summary:This paper presents an algorithm for efficient training of sparse linear models with elastic net regularization. Extending previous work on delayed updates, the new algorithm applies stochastic gradient updates to non-zero features only, bringing weights current as needed with closed-form updates. Closed-form delayed updates for the $\ell_1$, $\ell_{\infty}$, and rarely used $\ell_2$ regularizers have been described previously. This paper provides closed-form updates for the popular squared norm $\ell^2_2$ and elastic net regularizers. We provide dynamic programming algorithms that perform each delayed update in constant time. The new $\ell^2_2$ and elastic net methods handle both fixed and varying learning rates, and both standard {stochastic gradient descent} (SGD) and {forward backward splitting (FoBoS)}. Experimental results show that on a bag-of-words dataset with $260,941$ features, but only $88$ nonzero features on average per training example, the dynamic programming method trains a logistic regression classifier with elastic net regularization over $2000$ times faster than otherwise. version:3
arxiv-1401-2086 | Actor-Critic Algorithms for Learning Nash Equilibria in N-player General-Sum Games | http://arxiv.org/abs/1401.2086 | id:1401.2086 author:H. L Prasad, L. A. Prashanth, Shalabh Bhatnagar category:cs.GT cs.LG stat.ML  published:2014-01-08 summary:We consider the problem of finding stationary Nash equilibria (NE) in a finite discounted general-sum stochastic game. We first generalize a non-linear optimization problem from Filar and Vrieze [2004] to a $N$-player setting and break down this problem into simpler sub-problems that ensure there is no Bellman error for a given state and an agent. We then provide a characterization of solution points of these sub-problems that correspond to Nash equilibria of the underlying game and for this purpose, we derive a set of necessary and sufficient SG-SP (Stochastic Game - Sub-Problem) conditions. Using these conditions, we develop two actor-critic algorithms: OFF-SGSP (model-based) and ON-SGSP (model-free). Both algorithms use a critic that estimates the value function for a fixed policy and an actor that performs descent in the policy space using a descent direction that avoids local minima. We establish that both algorithms converge, in self-play, to the equilibria of a certain ordinary differential equation (ODE), whose stable limit points coincide with stationary NE of the underlying general-sum stochastic game. On a single state non-generic game (see Hart and Mas-Colell [2005]) as well as on a synthetic two-player game setup with $810,000$ states, we establish that ON-SGSP consistently outperforms NashQ ([Hu and Wellman, 2003] and FFQ [Littman, 2001] algorithms. version:2
arxiv-1507-00720 | Correlated Random Measures | http://arxiv.org/abs/1507.00720 | id:1507.00720 author:Rajesh Ranganath, David Blei category:stat.ML stat.ME  published:2015-07-02 summary:We develop correlated random measures, random measures where the atom weights can exhibit a flexible pattern of dependence, and use them to develop powerful hierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametric models are usually built from completely random measures, a Poisson-process based construction in which the atom weights are independent. Completely random measures imply strong independence assumptions in the corresponding hierarchical model, and these assumptions are often misplaced in real-world settings. Correlated random measures address this limitation. They model correlation within the measure by using a Gaussian process in concert with the Poisson process. With correlated random measures, for example, we can develop a latent feature model for which we can infer both the properties of the latent features and their dependency pattern. We develop several other examples as well. We study a correlated random measure model of pairwise count data. We derive an efficient variational inference algorithm and show improved predictive performance on large data sets of documents, web clicks, and electronic health records. version:1
arxiv-1507-00672 | The Elusive Present: Hidden Past and Future Dependency and Why We Build Models | http://arxiv.org/abs/1507.00672 | id:1507.00672 author:Pooneh M. Ara, Ryan G. James, James P. Crutchfield category:cond-mat.stat-mech cs.IT math.DS math.IT nlin.CD stat.ML  published:2015-07-02 summary:Modeling a temporal process as if it is Markovian assumes the present encodes all of the process's history. When this occurs, the present captures all of the dependency between past and future. We recently showed that if one randomly samples in the space of structured processes, this is almost never the case. So, how does the Markov failure come about? That is, how do individual measurements fail to encode the past? And, how many are needed to capture dependencies between the past and future? Here, we investigate how much information can be shared between the past and future, but not be reflected in the present. We quantify this elusive information, give explicit calculational methods, and draw out the consequences. The most important of which is that when the present hides past-future dependency we must move beyond sequence-based statistics and build state-based models. version:1
arxiv-1507-00646 | SQL for SRL: Structure Learning Inside a Database System | http://arxiv.org/abs/1507.00646 | id:1507.00646 author:Oliver Schulte, Zhensong Qian category:cs.LG cs.DB H.2.8; H.2.4  published:2015-07-02 summary:The position we advocate in this paper is that relational algebra can provide a unified language for both representing and computing with statistical-relational objects, much as linear algebra does for traditional single-table machine learning. Relational algebra is implemented in the Structured Query Language (SQL), which is the basis of relational database management systems. To support our position, we have developed the FACTORBASE system, which uses SQL as a high-level scripting language for statistical-relational learning of a graphical model structure. The design philosophy of FACTORBASE is to manage statistical models as first-class citizens inside a database. Our implementation shows how our SQL constructs in FACTORBASE facilitate fast, modular, and reliable program development. Empirical evidence from six benchmark databases indicates that leveraging database system capabilities achieves scalable model structure learning. version:1
arxiv-1507-00639 | Simple, Fast Semantic Parsing with a Tensor Kernel | http://arxiv.org/abs/1507.00639 | id:1507.00639 author:Daoud Clarke category:cs.CL  published:2015-07-02 summary:We describe a simple approach to semantic parsing based on a tensor product kernel. We extract two feature vectors: one for the query and one for each candidate logical form. We then train a classifier using the tensor product of the two vectors. Using very simple features for both, our system achieves an average F1 score of 40.1% on the WebQuestions dataset. This is comparable to more complex systems but is simpler to implement and runs faster. version:1
arxiv-1311-4924 | Robust Compressed Sensing Under Matrix Uncertainties | http://arxiv.org/abs/1311.4924 | id:1311.4924 author:Yipeng Liu category:cs.IT cs.CV math.IT math.RT stat.AP stat.ML  published:2013-11-20 summary:Compressed sensing (CS) shows that a signal having a sparse or compressible representation can be recovered from a small set of linear measurements. In classical CS theory, the sampling matrix and representation matrix are assumed to be known exactly in advance. However, uncertainties exist due to sampling distortion, finite grids of the parameter space of dictionary, etc. In this paper, we take a generalized sparse signal model, which simultaneously considers the sampling and representation matrix uncertainties. Based on the new signal model, a new optimization model for robust sparse signal reconstruction is proposed. This optimization model can be deduced with stochastic robust approximation analysis. Both convex relaxation and greedy algorithms are used to solve the optimization problem. For the convex relaxation method, a sufficient condition for recovery by convex relaxation is given; For the greedy algorithm, it is realized by the introduction of a pre-processing of the sensing matrix and the measurements. In numerical experiments, both simulated data and real-life ECG data based results show that the proposed method has a better performance than the current methods. version:4
arxiv-1507-00567 | Self-Learning Cloud Controllers: Fuzzy Q-Learning for Knowledge Evolution | http://arxiv.org/abs/1507.00567 | id:1507.00567 author:Pooyan Jamshidi, Amir Sharifloo, Claus Pahl, Andreas Metzger, Giovani Estrada category:cs.SY cs.AI cs.DC cs.LG cs.SE I.2.6; D.2.11  published:2015-07-02 summary:Cloud controllers aim at responding to application demands by automatically scaling the compute resources at runtime to meet performance guarantees and minimize resource costs. Existing cloud controllers often resort to scaling strategies that are codified as a set of adaptation rules. However, for a cloud provider, applications running on top of the cloud infrastructure are more or less black-boxes, making it difficult at design time to define optimal or pre-emptive adaptation rules. Thus, the burden of taking adaptation decisions often is delegated to the cloud application. Yet, in most cases, application developers in turn have limited knowledge of the cloud infrastructure. In this paper, we propose learning adaptation rules during runtime. To this end, we introduce FQL4KE, a self-learning fuzzy cloud controller. In particular, FQL4KE learns and modifies fuzzy rules at runtime. The benefit is that for designing cloud controllers, we do not have to rely solely on precise design-time knowledge, which may be difficult to acquire. FQL4KE empowers users to specify cloud controllers by simply adjusting weights representing priorities in system goals instead of specifying complex adaptation rules. The applicability of FQL4KE has been experimentally assessed as part of the cloud application framework ElasticBench. The experimental results indicate that FQL4KE outperforms our previously developed fuzzy controller without learning mechanisms and the native Azure auto-scaling. version:1
arxiv-1507-00566 | Anomaly Detection and Removal Using Non-Stationary Gaussian Processes | http://arxiv.org/abs/1507.00566 | id:1507.00566 author:Steven Reece, Roman Garnett, Michael Osborne, Stephen Roberts category:stat.ML  published:2015-07-02 summary:This paper proposes a novel Gaussian process approach to fault removal in time-series data. Fault removal does not delete the faulty signal data but, instead, massages the fault from the data. We assume that only one fault occurs at any one time and model the signal by two separate non-parametric Gaussian process models for both the physical phenomenon and the fault. In order to facilitate fault removal we introduce the Markov Region Link kernel for handling non-stationary Gaussian processes. This kernel is piece-wise stationary but guarantees that functions generated by it and their derivatives (when required) are everywhere continuous. We apply this kernel to the removal of drift and bias errors in faulty sensor data and also to the recovery of EOG artifact corrupted EEG signals. version:1
arxiv-1507-00564 | Regularized linear system identification using atomic, nuclear and kernel-based norms: the role of the stability constraint | http://arxiv.org/abs/1507.00564 | id:1507.00564 author:Gianluigi Pillonetto, Tianshi Chen, Alessandro Chiuso, Giuseppe De Nicolao, Lennart Ljung category:cs.SY cs.LG  published:2015-07-02 summary:Inspired by ideas taken from the machine learning literature, new regularization techniques have been recently introduced in linear system identification. In particular, all the adopted estimators solve a regularized least squares problem, differing in the nature of the penalty term assigned to the impulse response. Popular choices include atomic and nuclear norms (applied to Hankel matrices) as well as norms induced by the so called stable spline kernels. In this paper, a comparative study of estimators based on these different types of regularizers is reported. Our findings reveal that stable spline kernels outperform approaches based on atomic and nuclear norms since they suitably embed information on impulse response stability and smoothness. This point is illustrated using the Bayesian interpretation of regularization. We also design a new class of regularizers defined by "integral" versions of stable spline/TC kernels. Under quite realistic experimental conditions, the new estimators outperform classical prediction error methods also when the latter are equipped with an oracle for model order selection. version:1
arxiv-1507-00543 | Classical vs. Bayesian methods for linear system identification: point estimators and confidence sets | http://arxiv.org/abs/1507.00543 | id:1507.00543 author:D. Romeres, G. Prando, G. Pillonetto, A. Chiuso category:stat.ML  published:2015-07-02 summary:This paper compares classical parametric methods with recently developed Bayesian methods for system identification. A Full Bayes solution is considered together with one of the standard approximations based on the Empirical Bayes paradigm. Results regarding point estimators for the impulse response as well as for confidence regions are reported. version:1
arxiv-1507-00513 | Learning the intensity of time events with change-points | http://arxiv.org/abs/1507.00513 | id:1507.00513 author:Mokhtar Zahdi Alaya, Stéphane Gaïffas, Agathe Guilloux category:math.ST stat.ML stat.TH  published:2015-07-02 summary:We consider the problem of learning the inhomogeneous intensity of a counting process, under a sparse segmentation assumption. We introduce a weighted total-variation penalization, using data-driven weights that correctly scale the penalization along the observation interval. We prove that this leads to a sharp tuning of the convex relaxation of the segmentation prior, by stating oracle inequalities with fast rates of convergence, and consistency for change-points detection. This provides first theoretical guarantees for segmentation with a convex proxy beyond the standard i.i.d signal + white noise setting. We introduce a fast algorithm to solve this convex problem. Numerical experiments illustrate our approach on simulated and on a high-frequency genomics dataset. version:1
arxiv-1507-00507 | Identification of stable models via nonparametric prediction error methods | http://arxiv.org/abs/1507.00507 | id:1507.00507 author:Diego Romeres, Gianluigi Pillonetto, Alessandro Chiuso category:stat.ML  published:2015-07-02 summary:A new Bayesian approach to linear system identification has been proposed in a series of recent papers. The main idea is to frame linear system identification as predictor estimation in an infinite dimensional space, with the aid of regularization/Bayesian techniques. This approach guarantees the identification of stable predictors based on the prediction error minimization. Unluckily, the stability of the predictors does not guarantee the stability of the impulse response of the system. In this paper we propose and compare various techniques to address this issue. Simulations results comparing these techniques will be provided. version:1
arxiv-1507-00504 | Optimal Transport for Domain Adaptation | http://arxiv.org/abs/1507.00504 | id:1507.00504 author:Nicolas Courty, Rémi Flamary, Devis Tuia, Alain Rakotomamonjy category:cs.LG  published:2015-07-02 summary:Domain adaptation from one data space (or domain) to another is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data space become more robust when confronted to data depicting the same semantic concepts (the classes), but observed by another observation system with its own specificities. Among the many strategies proposed to adapt a domain to another, finding a common representation has shown excellent properties: by finding a common representation for both domains, a single classifier can be effective in both and use labelled samples from the source domain to predict the unlabelled samples of the target domain. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labelled samples in the source domain to remain close during transport. This way, we exploit at the same time the few labeled information in the source and the unlabelled distributions observed in both domains. Experiments in toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. version:1
arxiv-1507-00501 | Distributed image reconstruction for very large arrays in radio astronomy | http://arxiv.org/abs/1507.00501 | id:1507.00501 author:André Ferrari, David Mary, Rémi Flamary, Cédric Richard category:astro-ph.IM cs.CV  published:2015-07-02 summary:Current and future radio interferometric arrays such as LOFAR and SKA are characterized by a paradox. Their large number of receptors (up to millions) allow theoretically unprecedented high imaging resolution. In the same time, the ultra massive amounts of samples makes the data transfer and computational loads (correlation and calibration) order of magnitudes too high to allow any currently existing image reconstruction algorithm to achieve, or even approach, the theoretical resolution. We investigate here decentralized and distributed image reconstruction strategies which select, transfer and process only a fraction of the total data. The loss in MSE incurred by the proposed approach is evaluated theoretically and numerically on simple test cases. version:1
arxiv-1507-00500 | Non-convex Regularizations for Feature Selection in Ranking With Sparse SVM | http://arxiv.org/abs/1507.00500 | id:1507.00500 author:Léa Laporte, Rémi Flamary, Stephane Canu, Sébastien Déjean, Josiane Mothe category:cs.LG  published:2015-07-02 summary:Feature selection in learning to rank has recently emerged as a crucial issue. Whereas several preprocessing approaches have been proposed, only a few works have been focused on integrating the feature selection into the learning process. In this work, we propose a general framework for feature selection in learning to rank using SVM with a sparse regularization term. We investigate both classical convex regularizations such as $\ell\_1$ or weighted $\ell\_1$ and non-convex regularization terms such as log penalty, Minimax Concave Penalty (MCP) or $\ell\_p$ pseudo norm with $p\textless{}1$. Two algorithms are proposed, first an accelerated proximal approach for solving the convex problems, second a reweighted $\ell\_1$ scheme to address the non-convex regularizations. We conduct intensive experiments on nine datasets from Letor 3.0 and Letor 4.0 corpora. Numerical results show that the use of non-convex regularizations we propose leads to more sparsity in the resulting models while prediction performance is preserved. The number of features is decreased by up to a factor of six compared to the $\ell\_1$ regularization. In addition, the software is publicly available on the web. version:1
arxiv-1507-00438 | DC Proximal Newton for Non-Convex Optimization Problems | http://arxiv.org/abs/1507.00438 | id:1507.00438 author:Alain Rakotomamonjy, Remi Flamary, Gilles Gasso category:cs.LG cs.NA stat.ML  published:2015-07-02 summary:We introduce a novel algorithm for solving learning problems where both the loss function and the regularizer are non-convex but belong to the class of difference of convex (DC) functions. Our contribution is a new general purpose proximal Newton algorithm that is able to deal with such a situation. The algorithm consists in obtaining a descent direction from an approximation of the loss function and then in performing a line search to ensure sufficient descent. A theoretical analysis is provided showing that the iterates of the proposed algorithm {admit} as limit points stationary points of the DC objective function. Numerical experiments show that our approach is more efficient than current state of the art for a problem with a convex loss functions and non-convex regularizer. We have also illustrated the benefit of our algorithm in high-dimensional transductive learning problem where both loss function and regularizers are non-convex. version:1
arxiv-1507-00052 | Gaussian Process for Noisy Inputs with Ordering Constraints | http://arxiv.org/abs/1507.00052 | id:1507.00052 author:Cuong Tran, Vladimir Pavlovic, Robert Kopp category:stat.ML  published:2015-06-30 summary:We study the Gaussian Process regression model in the context of training data with noise in both input and output. The presence of two sources of noise makes the task of learning accurate predictive models extremely challenging. However, in some instances additional constraints may be available that can reduce the uncertainty in the resulting predictive models. In particular, we consider the case of monotonically ordered latent input, which occurs in many application domains that deal with temporal data. We present a novel inference and learning approach based on non-parametric Gaussian variational approximation to learn the GP model while taking into account the new constraints. The resulting strategy allows one to gain access to posterior estimates of both the input and the output and results in improved predictive performance. We compare our proposed models to state-of-the-art Noisy Input Gaussian Process (NIGP) and other competing approaches on synthetic and real sea-level rise data. Experimental results suggest that the proposed approach consistently outperforms selected methods while, at the same time, reducing the computational costs of learning and inference. version:2
arxiv-1507-00421 | Categorical Matrix Completion | http://arxiv.org/abs/1507.00421 | id:1507.00421 author:Yang Cao, Yao Xie category:cs.NA cs.LG math.ST stat.ML stat.TH  published:2015-07-02 summary:We consider the problem of completing a matrix with categorical-valued entries from partial observations. This is achieved by extending the formulation and theory of one-bit matrix completion. We recover a low-rank matrix $X$ by maximizing the likelihood ratio with a constraint on the nuclear norm of $X$, and the observations are mapped from entries of $X$ through multiple link functions. We establish theoretical upper and lower bounds on the recovery error, which meet up to a constant factor $\mathcal{O}(K^{3/2})$ where $K$ is the fixed number of categories. The upper bound in our case depends on the number of categories implicitly through a maximization of terms that involve the smoothness of the link functions. In contrast to one-bit matrix completion, our bounds for categorical matrix completion are optimal up to a factor on the order of the square root of the number of categories, which is consistent with an intuition that the problem becomes harder when the number of categories increases. By comparing the performance of our method with the conventional matrix completion method on the MovieLens dataset, we demonstrate the advantage of our method. version:1
arxiv-1502-05744 | Scale-Free Algorithms for Online Linear Optimization | http://arxiv.org/abs/1502.05744 | id:1502.05744 author:Francesco Orabona, David Pal category:cs.LG math.OC  published:2015-02-19 summary:We design algorithms for online linear optimization that have optimal regret and at the same time do not need to know any upper or lower bounds on the norm of the loss vectors. We achieve adaptiveness to norms of loss vectors by scale invariance, i.e., our algorithms make exactly the same decisions if the sequence of loss vectors is multiplied by any positive constant. Our algorithms work for any decision set, bounded or unbounded. For unbounded decisions sets, these are the first truly adaptive algorithms for online linear optimization. version:2
arxiv-1407-2662 | Learning Privately with Labeled and Unlabeled Examples | http://arxiv.org/abs/1407.2662 | id:1407.2662 author:Amos Beimel, Kobbi Nissim, Uri Stemmer category:cs.LG cs.CR  published:2014-07-10 summary:A private learner is an algorithm that given a sample of labeled individual examples outputs a generalizing hypothesis while preserving the privacy of each individual. In 2008, Kasiviswanathan et al. (FOCS 2008) gave a generic construction of private learners, in which the sample complexity is (generally) higher than what is needed for non-private learners. This gap in the sample complexity was then further studied in several followup papers, showing that (at least in some cases) this gap is unavoidable. Moreover, those papers considered ways to overcome the gap, by relaxing either the privacy or the learning guarantees of the learner. We suggest an alternative approach, inspired by the (non-private) models of semi-supervised learning and active-learning, where the focus is on the sample complexity of labeled examples whereas unlabeled examples are of a significantly lower cost. We consider private semi-supervised learners that operate on a random sample, where only a (hopefully small) portion of this sample is labeled. The learners have no control over which of the sample elements are labeled. Our main result is that the labeled sample complexity of private learners is characterized by the VC dimension. We present two generic constructions of private semi-supervised learners. The first construction is of learners where the labeled sample complexity is proportional to the VC dimension of the concept class, however, the unlabeled sample complexity of the algorithm is as big as the representation length of domain elements. Our second construction presents a new technique for decreasing the labeled sample complexity of a given private learner, while roughly maintaining its unlabeled sample complexity. In addition, we show that in some settings the labeled sample complexity does not depend on the privacy parameters of the learner. version:3
arxiv-1507-00353 | An Empirical Evaluation of True Online TD(λ) | http://arxiv.org/abs/1507.00353 | id:1507.00353 author:Harm van Seijen, A. Rupam Mahmood, Patrick M. Pilarski, Richard S. Sutton category:cs.AI cs.LG stat.ML  published:2015-07-01 summary:The true online TD({\lambda}) algorithm has recently been proposed (van Seijen and Sutton, 2014) as a universal replacement for the popular TD({\lambda}) algorithm, in temporal-difference learning and reinforcement learning. True online TD({\lambda}) has better theoretical properties than conventional TD({\lambda}), and the expectation is that it also results in faster learning. In this paper, we put this hypothesis to the test. Specifically, we compare the performance of true online TD({\lambda}) with that of TD({\lambda}) on challenging examples, random Markov reward processes, and a real-world myoelectric prosthetic arm. We use linear function approximation with tabular, binary, and non-binary features. We assess the algorithms along three dimensions: computational cost, learning speed, and ease of use. Our results confirm the strength of true online TD({\lambda}): 1) for sparse feature vectors, the computational overhead with respect to TD({\lambda}) is minimal; for non-sparse features the computation time is at most twice that of TD({\lambda}), 2) across all domains/representations the learning speed of true online TD({\lambda}) is often better, but never worse than that of TD({\lambda}), and 3) true online TD({\lambda}) is easier to use, because it does not require choosing between trace types, and it is generally more stable with respect to the step-size. Overall, our results suggest that true online TD({\lambda}) should be the first choice when looking for an efficient, general-purpose TD method. version:1
arxiv-1406-2504 | Exploring Algorithmic Limits of Matrix Rank Minimization under Affine Constraints | http://arxiv.org/abs/1406.2504 | id:1406.2504 author:Bo Xin, David Wipf category:cs.LG stat.ML  published:2014-06-10 summary:Many applications require recovering a matrix of minimal rank within an affine constraint set, with matrix completion a notable special case. Because the problem is NP-hard in general, it is common to replace the matrix rank with the nuclear norm, which acts as a convenient convex surrogate. While elegant theoretical conditions elucidate when this replacement is likely to be successful, they are highly restrictive and convex algorithms fail when the ambient rank is too high or when the constraint set is poorly structured. Non-convex alternatives fare somewhat better when carefully tuned; however, convergence to locally optimal solutions remains a continuing source of failure. Against this backdrop we derive a deceptively simple and parameter-free probabilistic PCA-like algorithm that is capable, over a wide battery of empirical tests, of successful recovery even at the theoretical limit where the number of measurements equal the degrees of freedom in the unknown low-rank matrix. Somewhat surprisingly, this is possible even when the affine constraint set is highly ill-conditioned. While proving general recovery guarantees remains evasive for non-convex algorithms, Bayesian-inspired or otherwise, we nonetheless show conditions whereby the underlying cost function has a unique stationary point located at the global optimum; no existing cost function we are aware of satisfies this same property. We conclude with a simple computer vision application involving image rectification and a standard collaborative filtering benchmark. version:3
arxiv-1502-07697 | A Chaining Algorithm for Online Nonparametric Regression | http://arxiv.org/abs/1502.07697 | id:1502.07697 author:Pierre Gaillard, Sébastien Gerchinovitz category:stat.ML cs.LG  published:2015-02-26 summary:We consider the problem of online nonparametric regression with arbitrary deterministic sequences. Using ideas from the chaining technique, we design an algorithm that achieves a Dudley-type regret bound similar to the one obtained in a non-constructive fashion by Rakhlin and Sridharan (2014). Our regret bound is expressed in terms of the metric entropy in the sup norm, which yields optimal guarantees when the metric and sequential entropies are of the same order of magnitude. In particular our algorithm is the first one that achieves optimal rates for online regression over H{\"o}lder balls. In addition we show for this example how to adapt our chaining algorithm to get a reasonable computational efficiency with similar regret guarantees (up to a log factor). version:2
arxiv-1507-00302 | Pose Embeddings: A Deep Architecture for Learning to Match Human Poses | http://arxiv.org/abs/1507.00302 | id:1507.00302 author:Greg Mori, Caroline Pantofaru, Nisarg Kothari, Thomas Leung, George Toderici, Alexander Toshev, Weilong Yang category:cs.CV  published:2015-07-01 summary:We present a method for learning an embedding that places images of humans in similar poses nearby. This embedding can be used as a direct method of comparing images based on human pose, avoiding potential challenges of estimating body joint positions. Pose embedding learning is formulated under a triplet-based distance criterion. A deep architecture is used to allow learning of a representation capable of making distinctions between different poses. Experiments on human pose matching and retrieval from video data demonstrate the potential of the method. version:1
arxiv-1507-00300 | Bootstrapped Thompson Sampling and Deep Exploration | http://arxiv.org/abs/1507.00300 | id:1507.00300 author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG  published:2015-07-01 summary:This technical note presents a new approach to carrying out the kind of exploration achieved by Thompson sampling, but without explicitly maintaining or sampling from posterior distributions. The approach is based on a bootstrap technique that uses a combination of observed and artificially generated data. The latter serves to induce a prior distribution which, as we will demonstrate, is critical to effective exploration. We explain how the approach can be applied to multi-armed bandit and reinforcement learning problems and how it relates to Thompson sampling. The approach is particularly well-suited for contexts in which exploration is coupled with deep learning, since in these settings, maintaining or generating samples from a posterior distribution becomes computationally infeasible. version:1
arxiv-1507-00235 | Energy-efficient neuromorphic classifiers | http://arxiv.org/abs/1507.00235 | id:1507.00235 author:Daniel Martí, Mattia Rigotti, Mingoo Seok, Stefano Fusi category:q-bio.NC cs.NE  published:2015-07-01 summary:Neuromorphic engineering combines the architectural and computational principles of systems neuroscience with semiconductor electronics, with the aim of building efficient and compact devices that mimic the synaptic and neural machinery of the brain. Neuromorphic engineering promises extremely low energy consumptions, comparable to those of the nervous system. However, until now the neuromorphic approach has been restricted to relatively simple circuits and specialized functions, rendering elusive a direct comparison of their energy consumption to that used by conventional von Neumann digital machines solving real-world tasks. Here we show that a recent technology developed by IBM can be leveraged to realize neuromorphic circuits that operate as classifiers of complex real-world stimuli. These circuits emulate enough neurons to compete with state-of-the-art classifiers. We also show that the energy consumption of the IBM chip is typically 2 or more orders of magnitude lower than that of conventional digital machines when implementing classifiers with comparable performance. Moreover, the spike-based dynamics display a trade-off between integration time and accuracy, which naturally translates into algorithms that can be flexibly deployed for either fast and approximate classifications, or more accurate classifications at the mere expense of longer running times and higher energy costs. This work finally proves that the neuromorphic approach can be efficiently used in real-world applications and it has significant advantages over conventional digital devices when energy consumption is considered. version:1
arxiv-1507-00220 | Bigeometric Organization of Deep Nets | http://arxiv.org/abs/1507.00220 | id:1507.00220 author:Alexander Cloninger, Ronald R. Coifman, Nicholas Downing, Harlan M. Krumholz category:stat.ML cs.LG  published:2015-07-01 summary:In this paper, we build an organization of high-dimensional datasets that cannot be cleanly embedded into a low-dimensional representation due to missing entries and a subset of the features being irrelevant to modeling functions of interest. Our algorithm begins by defining coarse neighborhoods of the points and defining an expected empirical function value on these neighborhoods. We then generate new non-linear features with deep net representations tuned to model the approximate function, and re-organize the geometry of the points with respect to the new representation. Finally, the points are locally z-scored to create an intrinsic geometric organization which is independent of the parameters of the deep net, a geometry designed to assure smoothness with respect to the empirical function. We examine this approach on data from the Center for Medicare and Medicaid Services Hospital Quality Initiative, and generate an intrinsic low-dimensional organization of the hospitals that is smooth with respect to an expert driven function of quality. version:1
arxiv-1507-00210 | Natural Neural Networks | http://arxiv.org/abs/1507.00210 | id:1507.00210 author:Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray Kavukcuoglu category:stat.ML cs.LG cs.NE  published:2015-07-01 summary:We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset. version:1
arxiv-1507-00209 | Dimensionality on Summarization | http://arxiv.org/abs/1507.00209 | id:1507.00209 author:Hai Zhuge category:cs.CL cs.IR  published:2015-07-01 summary:Summarization is one of the key features of human intelligence. It plays an important role in understanding and representation. With rapid and continual expansion of texts, pictures and videos in cyberspace, automatic summarization becomes more and more desirable. Text summarization has been studied for over half century, but it is still hard to automatically generate a satisfied summary. Traditional methods process texts empirically and neglect the fundamental characteristics and principles of language use and understanding. This paper summarizes previous text summarization approaches in a multi-dimensional classification space, introduces a multi-dimensional methodology for research and development, unveils the basic characteristics and principles of language use and understanding, investigates some fundamental mechanisms of summarization, studies the dimensions and forms of representations, and proposes a multi-dimensional evaluation mechanisms. Investigation extends to the incorporation of pictures into summary and to the summarization of videos, graphs and pictures, and then reaches a general summarization framework. version:1
arxiv-1507-01578 | Beyond Semantic Image Segmentation : Exploring Efficient Inference in Video | http://arxiv.org/abs/1507.01578 | id:1507.01578 author:Subarna Tripathi, Serge Belongie, Truong Nguyen category:cs.CV  published:2015-07-01 summary:We explore the efficiency of the CRF inference module beyond image level semantic segmentation. The key idea is to combine the best of two worlds of semantic co-labeling and exploiting more expressive models. Similar to [Alvarez14] our formulation enables us perform inference over ten thousand images within seconds. On the other hand, it can handle higher-order clique potentials similar to [vineet2014] in terms of region-level label consistency and context in terms of co-occurrences. We follow the mean-field updates for higher order potentials similar to [vineet2014] and extend the spatial smoothness and appearance kernels [DenseCRF13] to address video data inspired by [Alvarez14]; thus making the system amenable to perform video semantic segmentation most effectively. version:1
arxiv-1507-00133 | Prior Polarity Lexical Resources for the Italian Language | http://arxiv.org/abs/1507.00133 | id:1507.00133 author:Valeria Borzì, Simone Faro, Arianna Pavone, Sabrina Sansone category:cs.CL  published:2015-07-01 summary:In this paper we present SABRINA (Sentiment Analysis: a Broad Resource for Italian Natural language Applications) a manually annotated prior polarity lexical resource for Italian natural language applications in the field of opinion mining and sentiment induction. The resource consists in two different sets, an Italian dictionary of more than 277.000 words tagged with their prior polarity value, and a set of polarity modifiers, containing more than 200 words, which can be used in combination with non neutral terms of the dictionary in order to induce the sentiment of Italian compound terms. To the best of our knowledge this is the first prior polarity manually annotated resource which has been developed for the Italian natural language. version:1
arxiv-1507-00110 | Polarimetric Hierarchical Semantic Model and Scattering Mechanism Based PolSAR Image Classification | http://arxiv.org/abs/1507.00110 | id:1507.00110 author:Fang Liu, Junfei Shi, Licheng Jiao, Hongying Liu, Shuyuan Yang, Jie Wu, Hongxia Hao, Jialing Yuan category:cs.CV  published:2015-07-01 summary:For polarimetric SAR (PolSAR) image classification, it is a challenge to classify the aggregated terrain types, such as the urban area, into semantic homogenous regions due to sharp bright-dark variations in intensity. The aggregated terrain type is formulated by the similar ground objects aggregated together. In this paper, a polarimetric hierarchical semantic model (PHSM) is firstly proposed to overcome this disadvantage based on the constructions of a primal-level and a middle-level semantic. The primal-level semantic is a polarimetric sketch map which consists of sketch segments as the sparse representation of a PolSAR image. The middle-level semantic is a region map which can extract semantic homogenous regions from the sketch map by exploiting the topological structure of sketch segments. Mapping the region map to the PolSAR image, a complex PolSAR scene is partitioned into aggregated, structural and homogenous pixel-level subspaces with the characteristics of relatively coherent terrain types in each subspace. Then, according to the characteristics of three subspaces above, three specific methods are adopted, and furthermore polarimetric information is exploited to improve the segmentation result. Experimental results on PolSAR data sets with different bands and sensors demonstrate that the proposed method is superior to the state-of-the-art methods in region homogeneity and edge preservation for terrain classification. version:1
arxiv-1507-00101 | Supervised Learning of Semantics-Preserving Hashing via Deep Neural Networks for Large-Scale Image Search | http://arxiv.org/abs/1507.00101 | id:1507.00101 author:Huei-Fang Yang, Kevin Lin, Chu-Song Chen category:cs.CV  published:2015-07-01 summary:This paper presents a supervised deep hashing approach that constructs binary hash codes from labeled data for large-scale image search. We assume that semantic labels are governed by a set of latent attributes in which each attribute can be on or off, and classification relies on these attributes. Based on this assumption, our approach, dubbed supervised semantics-preserving deep hashing (SSDH), constructs hash functions as a latent layer in a deep network in which binary codes are learned by the optimization of an objective function defined over classification error and other desirable properties of hash codes. With this design, SSDH has a nice property that classification and retrieval are unified in a single learning model, and the learned binary codes not only preserve the semantic similarity between images but also are efficient for image search. Moreover, SSDH performs joint learning of image representations, hash codes, and classification in a pointwised manner and thus is naturally scalable to large-scale datasets. SSDH is simple and can be easily realized by a slight modification of an existing deep architecture for classification; yet it is effective and outperforms other unsupervised and supervised hashing approaches on several benchmarks and one large dataset comprising more than 1 million images. version:1
arxiv-1506-09068 | On the Equivalence of Factorized Information Criterion Regularization and the Chinese Restaurant Process Prior | http://arxiv.org/abs/1506.09068 | id:1506.09068 author:Shaohua Li category:stat.ML  published:2015-06-30 summary:Factorized Information Criterion (FIC) is a recently developed information criterion, based on which a novel model selection methodology, namely Factorized Asymptotic Bayesian (FAB) Inference, has been developed and successfully applied to various hierarchical Bayesian models. The Dirichlet Process (DP) prior, and one of its well known representations, the Chinese Restaurant Process (CRP), derive another line of model selection methods. FIC can be viewed as a prior distribution over the latent variable configurations. Under this view, we prove that when the parameter dimensionality $D_{c}=2$, FIC is equivalent to CRP. We argue that when $D_{c}>2$, FIC avoids an inherent problem of DP/CRP, i.e. the data likelihood will dominate the impact of the prior, and thus the model selection capability will weaken as $D_{c}$ increases. However, FIC overestimates the data likelihood. As a result, FIC may be overly biased towards models with less components. We propose a natural generalization of FIC, which finds a middle ground between CRP and FIC, and may yield more accurate model selection results than FIC. version:2
arxiv-1507-00093 | A Study of Gradient Descent Schemes for General-Sum Stochastic Games | http://arxiv.org/abs/1507.00093 | id:1507.00093 author:H. L. Prasad, Shalabh Bhatnagar category:cs.LG cs.GT  published:2015-07-01 summary:Zero-sum stochastic games are easy to solve as they can be cast as simple Markov decision processes. This is however not the case with general-sum stochastic games. A fairly general optimization problem formulation is available for general-sum stochastic games by Filar and Vrieze [2004]. However, the optimization problem there has a non-linear objective and non-linear constraints with special structure. Since gradients of both the objective as well as constraints of this optimization problem are well defined, gradient based schemes seem to be a natural choice. We discuss a gradient scheme tuned for two-player stochastic games. We show in simulations that this scheme indeed converges to a Nash equilibrium, for a simple terrain exploration problem modelled as a general-sum stochastic game. However, it turns out that only global minima of the optimization problem correspond to Nash equilibria of the underlying general-sum stochastic game, while gradient schemes only guarantee convergence to local minima. We then provide important necessary conditions for gradient schemes to converge to Nash equilibria in general-sum stochastic games. version:1
arxiv-1507-00088 | Evaluation of Genotypic Diversity Measurements Exploited in Real-Coded Representation | http://arxiv.org/abs/1507.00088 | id:1507.00088 author:Guillaume Corriveau, Raynald Guilbault, Antoine Tahan, Robert Sabourin category:cs.NE  published:2015-07-01 summary:Numerous genotypic diversity measures (GDMs) are available in the literature to assess the convergence status of an evolutionary algorithm (EA) or describe its search behavior. In a recent study, the authors of this paper drew attention to the need for a GDM validation framework. In response, this study proposes three requirements (monotonicity in individual varieties, twinning, and monotonicity in distance) that can clearly portray any GDMs. These diversity requirements are analysed by means of controlled population arrangements. In this paper four GDMs are evaluated with the proposed validation framework. The results confirm that properly evaluating population diversity is a rather difficult task, as none of the analysed GDMs complies with all the diversity requirements. version:1
arxiv-1506-09174 | Discovering Characteristic Landmarks on Ancient Coins using Convolutional Networks | http://arxiv.org/abs/1506.09174 | id:1506.09174 author:Jongpil Kim, Vladimir Pavlovic category:cs.CV  published:2015-06-30 summary:In this paper, we propose a novel method to find characteristic landmarks on ancient Roman imperial coins using deep convolutional neural network models (CNNs). We formulate an optimization problem to discover class-specific regions while guaranteeing specific controlled loss of accuracy. Analysis on visualization of the discovered region confirms that not only can the proposed method successfully find a set of characteristic regions per class, but also the discovered region is consistent with human expert annotations. We also propose a new framework to recognize the Roman coins which exploits hierarchical structure of the ancient Roman coins using the state-of-the-art classification power of the CNNs adopted to a new task of coin classification. Experimental results show that the proposed framework is able to effectively recognize the ancient Roman coins. For this research, we have collected a new Roman coin dataset where all coins are annotated and consist of observe (head) and reverse (tail) images. version:2
arxiv-1507-00066 | Fast Cross-Validation for Incremental Learning | http://arxiv.org/abs/1507.00066 | id:1507.00066 author:Pooria Joulani, András György, Csaba Szepesvári category:stat.ML cs.AI cs.LG  published:2015-06-30 summary:Cross-validation (CV) is one of the main tools for performance estimation and parameter tuning in machine learning. The general recipe for computing CV estimate is to run a learning algorithm separately for each CV fold, a computationally expensive process. In this paper, we propose a new approach to reduce the computational burden of CV-based performance estimation. As opposed to all previous attempts, which are specific to a particular learning model or problem domain, we propose a general method applicable to a large class of incremental learning algorithms, which are uniquely fitted to big data problems. In particular, our method applies to a wide range of supervised and unsupervised learning tasks with different performance criteria, as long as the base learning algorithm is incremental. We show that the running time of the algorithm scales logarithmically, rather than linearly, in the number of CV folds. Furthermore, the algorithm has favorable properties for parallel and distributed implementation. Experiments with state-of-the-art incremental learning algorithms confirm the practicality of the proposed method. version:1
arxiv-1507-00039 | Selective Inference and Learning Mixed Graphical Models | http://arxiv.org/abs/1507.00039 | id:1507.00039 author:Jason D. Lee category:stat.ML cs.LG  published:2015-06-30 summary:This thesis studies two problems in modern statistics. First, we study selective inference, or inference for hypothesis that are chosen after looking at the data. The motiving application is inference for regression coefficients selected by the lasso. We present the Condition-on-Selection method that allows for valid selective inference, and study its application to the lasso, and several other selection algorithms. In the second part, we consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables that is amenable to structure learning. In previous work, authors have considered structure learning of Gaussian graphical models and structure learning of discrete models. Our approach is a natural generalization of these two lines of work to the mixed case. The penalization scheme involves a novel symmetric use of the group-lasso norm and follows naturally from a particular parametrization of the model. We provide conditions under which our estimator is model selection consistent in the high-dimensional regime. version:1
arxiv-1502-02347 | Local and Global Inference for High Dimensional Nonparanormal Graphical Models | http://arxiv.org/abs/1502.02347 | id:1502.02347 author:Quanquan Gu, Yuan Cao, Yang Ning, Han Liu category:stat.ML  published:2015-02-09 summary:This paper proposes a unified framework to quantify local and global inferential uncertainty for high dimensional nonparanormal graphical models. In particular, we consider the problems of testing the presence of a single edge and constructing a uniform confidence subgraph. Due to the presence of unknown marginal transformations, we propose a pseudo likelihood based inferential approach. In sharp contrast to the existing high dimensional score test method, our method is free of tuning parameters given an initial estimator, and extends the scope of the existing likelihood based inferential framework. Furthermore, we propose a U-statistic multiplier bootstrap method to construct the confidence subgraph. We show that the constructed subgraph is contained in the true graph with probability greater than a given nominal level. Compared with existing methods for constructing confidence subgraphs, our method does not rely on Gaussian or sub-Gaussian assumptions. The theoretical properties of the proposed inferential methods are verified by thorough numerical experiments and real data analysis. version:2
arxiv-1505-07599 | Overview of the NLPCC 2015 Shared Task: Chinese Word Segmentation and POS Tagging for Micro-blog Texts | http://arxiv.org/abs/1505.07599 | id:1505.07599 author:Xipeng Qiu, Peng Qian, Liusong Yin, Shiyu Wu, Xuanjing Huang category:cs.CL  published:2015-05-28 summary:In this paper, we give an overview for the shared task at the 4th CCF Conference on Natural Language Processing \& Chinese Computing (NLPCC 2015): Chinese word segmentation and part-of-speech (POS) tagging for micro-blog texts. Different with the popular used newswire datasets, the dataset of this shared task consists of the relatively informal micro-texts. The shared task has two sub-tasks: (1) individual Chinese word segmentation and (2) joint Chinese word segmentation and POS Tagging. Each subtask has three tracks to distinguish the systems with different resources. We first introduce the dataset and task, then we characterize the different approaches of the participating systems, report the test results, and provide a overview analysis of these results. An online system is available for open registration and evaluation at http://nlp.fudan.edu.cn/nlpcc2015. version:3
arxiv-1506-09179 | Learning to Detect Blue-white Structures in Dermoscopy Images with Weak Supervision | http://arxiv.org/abs/1506.09179 | id:1506.09179 author:Ali Madooei, Mark S. Drew, Hossein Hajimirsadeghi category:cs.CV  published:2015-06-30 summary:We propose a novel approach to identify one of the most significant dermoscopic criteria in the diagnosis of Cutaneous Melanoma: the Blue-whitish structure. In this paper, we achieve this goal in a Multiple Instance Learning framework using only image-level labels of whether the feature is present or not. As the output, we predict the image classification label and as well localize the feature in the image. Experiments are conducted on a challenging dataset with results outperforming state-of-the-art. This study provides an improvement on the scope of modelling for computerized image analysis of skin lesions, in particular in that it puts forward a framework for identification of dermoscopic local features from weakly-labelled data. version:1
arxiv-1506-09169 | On anthropomorphic decision making in a model observer | http://arxiv.org/abs/1506.09169 | id:1506.09169 author:Ali R. N. Avanaki, Kathryn S. Espig, Tom R. L. Kimpe, Andrew D. A. Maidment category:cs.CV cs.HC  published:2015-06-30 summary:By analyzing human readers' performance in detecting small round lesions in simulated digital breast tomosynthesis background in a location known exactly scenario, we have developed a model observer that is a better predictor of human performance with different levels of background complexity (i.e., anatomical and quantum noise). Our analysis indicates that human observers perform a lesion detection task by combining a number of sub-decisions, each an indicator of the presence of a lesion in the image stack. This is in contrast to a channelized Hotelling observer, where the detection task is conducted holistically by thresholding a single decision variable, made from an optimally weighted linear combination of channels. However, it seems that the sub-par performance of human readers compared to the CHO cannot be fully explained by their reliance on sub-decisions, or perhaps we do not consider a sufficient number of sub-decisions. To bridge the gap between the performances of human readers and the model observer based upon sub-decisions, we use an additive noise model, the power of which is modulated with the level of background complexity. The proposed model observer better predicts the fast drop in human detection performance with background complexity. version:1
arxiv-1506-09166 | Aging display's effect on interpretation of digital pathology slides | http://arxiv.org/abs/1506.09166 | id:1506.09166 author:Ali R. N. Avanaki, Kathryn S. Espig, Sameer Sawhney, Liron Pantanowitz, Anil V. Parwani, Albert Xthona, Tom R. L. Kimpe category:cs.CV cs.GR  published:2015-06-30 summary:It is our conjecture that the variability of colors in a pathology image effects the interpretation of pathology cases, whether it is diagnostic accuracy, diagnostic confidence, or workflow efficiency. In this paper, digital pathology images are analyzed to quantify the perceived difference in color that occurs due to display aging, in particular a change in the maximum luminance, white point, and color gamut. The digital pathology images studied include diagnostically important features, such as the conspicuity of nuclei. Three different display aging models are applied to images: aging of luminance & chrominance, aging of chrominance only, and a stabilized luminance & chrominance (i.e., no aging). These display models and images are then used to compare conspicuity of nuclei using CIE deltaE2000, a perceptual color difference metric. The effect of display aging using these display models and images is further analyzed through a human reader study designed to quantify the effects from a clinical perspective. Results from our reader study indicate significant impact of aged displays on workflow as well as diagnosis as follow. As compared to the originals (no-aging), slides with the effect of aging simulated were significantly more difficult to read (p-value of 0.0005) and took longer to score (p-value of 0.02). Moreover, luminance+chrominance aging significantly reduced inter-session percent agreement of diagnostic scores (p-value of 0.0418). version:1
arxiv-1506-09153 | Framework for Multi-task Multiple Kernel Learning and Applications in Genome Analysis | http://arxiv.org/abs/1506.09153 | id:1506.09153 author:Christian Widmer, Marius Kloft, Vipin T Sreedharan, Gunnar Rätsch category:stat.ML cs.CE cs.LG  published:2015-06-30 summary:We present a general regularization-based framework for Multi-task learning (MTL), in which the similarity between tasks can be learned or refined using $\ell_p$-norm Multiple Kernel learning (MKL). Based on this very general formulation (including a general loss function), we derive the corresponding dual formulation using Fenchel duality applied to Hermitian matrices. We show that numerous established MTL methods can be derived as special cases from both, the primal and dual of our formulation. Furthermore, we derive a modern dual-coordinate descend optimization strategy for the hinge-loss variant of our formulation and provide convergence bounds for our algorithm. As a special case, we implement in C++ a fast LibLinear-style solver for $\ell_p$-norm MKL. In the experimental section, we analyze various aspects of our algorithm such as predictive performance and ability to reconstruct task relationships on biologically inspired synthetic data, where we have full control over the underlying ground truth. We also experiment on a new dataset from the domain of computational biology that we collected for the purpose of this paper. It concerns the prediction of transcription start sites (TSS) over nine organisms, which is a crucial task in gene finding. Our solvers including all discussed special cases are made available as open-source software as part of the SHOGUN machine learning toolbox (available at \url{http://shogun.ml}). version:1
arxiv-1506-09124 | Multi-Cue Structure Preserving MRF for Unconstrained Video Segmentation | http://arxiv.org/abs/1506.09124 | id:1506.09124 author:Saehoon Yi, Vladimir Pavlovic category:cs.CV  published:2015-06-30 summary:Video segmentation is a stepping stone to understanding video context. Video segmentation enables one to represent a video by decomposing it into coherent regions which comprise whole or parts of objects. However, the challenge originates from the fact that most of the video segmentation algorithms are based on unsupervised learning due to expensive cost of pixelwise video annotation and intra-class variability within similar unconstrained video classes. We propose a Markov Random Field model for unconstrained video segmentation that relies on tight integration of multiple cues: vertices are defined from contour based superpixels, unary potentials from temporal smooth label likelihood and pairwise potentials from global structure of a video. Multi-cue structure is a breakthrough to extracting coherent object regions for unconstrained videos in absence of supervision. Our experiments on VSB100 dataset show that the proposed model significantly outperforms competing state-of-the-art algorithms. Qualitative analysis illustrates that video segmentation result of the proposed model is consistent with human perception of objects. version:1
arxiv-1505-00290 | Algorithms for Lipschitz Learning on Graphs | http://arxiv.org/abs/1505.00290 | id:1505.00290 author:Rasmus Kyng, Anup Rao, Sushant Sachdeva, Daniel A. Spielman category:cs.LG cs.DS math.MG  published:2015-05-01 summary:We develop fast algorithms for solving regression problems on graphs where one is given the value of a function at some vertices, and must find its smoothest possible extension to all vertices. The extension we compute is the absolutely minimal Lipschitz extension, and is the limit for large $p$ of $p$-Laplacian regularization. We present an algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes an absolutely minimal Lipschitz extension in expected time $\widetilde{O} (m n)$. The latter algorithm has variants that seem to run much faster in practice. These extensions are particularly amenable to regularization: we can perform $l_{0}$-regularization on the given values in polynomial time and $l_{1}$-regularization on the initial function values and on graph edge weights in time $\widetilde{O} (m^{3/2})$. version:2
arxiv-1506-09110 | Forming A Random Field via Stochastic Cliques: From Random Graphs to Fully Connected Random Fields | http://arxiv.org/abs/1506.09110 | id:1506.09110 author:Mohammad Javad Shafiee, Alexander Wong, Paul Fieguth category:cs.CV  published:2015-06-30 summary:Random fields have remained a topic of great interest over past decades for the purpose of structured inference, especially for problems such as image segmentation. The local nodal interactions commonly used in such models often suffer the short-boundary bias problem, which are tackled primarily through the incorporation of long-range nodal interactions. However, the issue of computational tractability becomes a significant issue when incorporating such long-range nodal interactions, particularly when a large number of long-range nodal interactions (e.g., fully-connected random fields) are modeled. In this work, we introduce a generalized random field framework based around the concept of stochastic cliques, which addresses the issue of computational tractability when using fully-connected random fields by stochastically forming a sparse representation of the random field. The proposed framework allows for efficient structured inference using fully-connected random fields without any restrictions on the potential functions that can be utilized. Several realizations of the proposed framework using graph cuts are presented and evaluated, and experimental results demonstrate that the proposed framework can provide competitive performance for the purpose of image segmentation when compared to existing fully-connected and principled deep random field frameworks. version:1
arxiv-1406-4993 | Divide-and-Conquer with Sequential Monte Carlo | http://arxiv.org/abs/1406.4993 | id:1406.4993 author:Fredrik Lindsten, Adam M. Johansen, Christian A. Naesseth, Bonnie Kirkpatrick, Thomas B. Schön, John Aston, Alexandre Bouchard-Côté category:stat.CO stat.ML  published:2014-06-19 summary:We propose a novel class of Sequential Monte Carlo (SMC) algorithms, appropriate for inference in probabilistic graphical models. This class of algorithms adopts a divide-and-conquer approach based upon an auxiliary tree-structured decomposition of the model of interest, turning the overall inferential task into a collection of recursively solved sub-problems. The proposed method is applicable to a broad class of probabilistic graphical models, including models with loops. Unlike a standard SMC sampler, the proposed Divide-and-Conquer SMC employs multiple independent populations of weighted particles, which are resampled, merged, and propagated as the method progresses. We illustrate empirically that this approach can outperform standard methods in terms of the accuracy of the posterior expectation and marginal likelihood approximations. Divide-and-Conquer SMC also opens up novel parallel implementation options and the possibility of concentrating the computational effort on the most challenging sub-problems. We demonstrate its performance on a Markov random field and on a hierarchical logistic regression problem. version:2
arxiv-1506-09019 | Artificial Catalytic Reactions in 2D for Combinatorial Optimization | http://arxiv.org/abs/1506.09019 | id:1506.09019 author:Jaderick P. Pabico category:cs.ET cs.NE  published:2015-06-30 summary:Presented in this paper is a derivation of a 2D catalytic reaction-based model to solve combinatorial optimization problems (COPs). The simulated catalytic reactions, a computational metaphor, occurs in an artificial chemical reactor that finds near-optimal solutions to COPs. The artificial environment is governed by catalytic reactions that can alter the structure of artificial molecular elements. Altering the molecular structure means finding new solutions to the COP. The molecular mass of the elements was considered as a measure of goodness of fit of the solutions. Several data structures and matrices were used to record the directions and locations of the molecules. These provided the model the 2D topology. The Traveling Salesperson Problem (TSP) was used as a working example. The performance of the model in finding a solution for the TSP was compared to the performance of a topology-less model. Experimental results show that the 2D model performs better than the topology-less one. version:1
arxiv-1506-08928 | Fast ADMM Algorithm for Distributed Optimization with Adaptive Penalty | http://arxiv.org/abs/1506.08928 | id:1506.08928 author:Changkyu Song, Sejong Yoon, Vladimir Pavlovic category:cs.LG cs.CV math.OC  published:2015-06-30 summary:We propose new methods to speed up convergence of the Alternating Direction Method of Multipliers (ADMM), a common optimization tool in the context of large scale and distributed learning. The proposed method accelerates the speed of convergence by automatically deciding the constraint penalty needed for parameter consensus in each iteration. In addition, we also propose an extension of the method that adaptively determines the maximum number of iterations to update the penalty. We show that this approach effectively leads to an adaptive, dynamic network topology underlying the distributed optimization. The utility of the new penalty update schemes is demonstrated on both synthetic and real data, including a computer vision application of distributed structure from motion. version:1
arxiv-1506-08910 | Learning Single Index Models in High Dimensions | http://arxiv.org/abs/1506.08910 | id:1506.08910 author:Ravi Ganti, Nikhil Rao, Rebecca M. Willett, Robert Nowak category:stat.ML cs.LG stat.ME  published:2015-06-30 summary:Single Index Models (SIMs) are simple yet flexible semi-parametric models for classification and regression. Response variables are modeled as a nonlinear, monotonic function of a linear combination of features. Estimation in this context requires learning both the feature weights, and the nonlinear function. While methods have been described to learn SIMs in the low dimensional regime, a method that can efficiently learn SIMs in high dimensions has not been forthcoming. We propose three variants of a computationally and statistically efficient algorithm for SIM inference in high dimensions. We establish excess risk bounds for the proposed algorithms and experimentally validate the advantages that our SIM learning methods provide relative to Generalized Linear Model (GLM) and low dimensional SIM based learning methods. version:1
arxiv-1506-02717 | An Improved BKW Algorithm for LWE with Applications to Cryptography and Lattices | http://arxiv.org/abs/1506.02717 | id:1506.02717 author:Paul Kirchner, Pierre-Alain Fouque category:cs.CR cs.DS cs.LG I.1.2; F.2.1  published:2015-06-08 summary:In this paper, we study the Learning With Errors problem and its binary variant, where secrets and errors are binary or taken in a small interval. We introduce a new variant of the Blum, Kalai and Wasserman algorithm, relying on a quantization step that generalizes and fine-tunes modulus switching. In general this new technique yields a significant gain in the constant in front of the exponent in the overall complexity. We illustrate this by solving p within half a day a LWE instance with dimension n = 128, modulus $q = n^2$, Gaussian noise $\alpha = 1/(\sqrt{n/\pi} \log^2 n)$ and binary secret, using $2^{28}$ samples, while the previous best result based on BKW claims a time complexity of $2^{74}$ with $2^{60}$ samples for the same parameters. We then introduce variants of BDD, GapSVP and UniqueSVP, where the target point is required to lie in the fundamental parallelepiped, and show how the previous algorithm is able to solve these variants in subexponential time. Moreover, we also show how the previous algorithm can be used to solve the BinaryLWE problem with n samples in subexponential time $2^{(\ln 2/2+o(1))n/\log \log n}$. This analysis does not require any heuristic assumption, contrary to other algebraic approaches; instead, it uses a variant of an idea by Lyubashevsky to generate many samples from a small number of samples. This makes it possible to asymptotically and heuristically break the NTRU cryptosystem in subexponential time (without contradicting its security assumption). We are also able to solve subset sum problems in subexponential time for density $o(1)$, which is of independent interest: for such density, the previous best algorithm requires exponential time. As a direct application, we can solve in subexponential time the parameters of a cryptosystem based on this problem proposed at TCC 2010. version:4
arxiv-1506-08858 | Machine learning for many-body physics: efficient solution of dynamical mean-field theory | http://arxiv.org/abs/1506.08858 | id:1506.08858 author:Louis-François Arsenault, O. Anatole von Lilienfeld, Andrew J. Millis category:cond-mat.str-el stat.ML  published:2015-06-29 summary:Machine learning methods for solving the equations of dynamical mean-field theory are developed. The method is demonstrated on the three dimensional Hubbard model. The key technical issues are defining a mapping of an input function to an output function, and distinguishing metallic from insulating solutions. Both metallic and Mott insulator solutions can be predicted. The validity of the machine learning scheme is assessed by comparing predictions of full correlation functions, of quasi-particle weight and particle density to values directly computed. The results indicate that with modest further development, machine learning approach may be an attractive computational efficient option for real materials predictions for strongly correlated systems. version:1
arxiv-1506-08826 | Statistical Inference using the Morse-Smale Complex | http://arxiv.org/abs/1506.08826 | id:1506.08826 author:Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman category:math.ST stat.ME stat.ML stat.TH  published:2015-06-29 summary:The Morse-Smale complex decomposes the sample space into cells where a given function $f$ is increasing or decreasing. When applied to nonparametric density estimation and regression, it provides a way to represent, visualize and compare functions, even in high dimensions. In this paper, we study the estimation of the Morse-Smale complex and we use our results for a variety of statistical problems including: nonparametric two-sample testing, density estimation, nonparametric regression and mode clustering. version:1
arxiv-1506-08789 | Requirement Tracing using Term Extraction | http://arxiv.org/abs/1506.08789 | id:1506.08789 author:Najla Al-Saati, Raghda Abdul-Jaleel category:cs.SE cs.CL cs.IR  published:2015-06-29 summary:Requirements traceability is an essential step in ensuring the quality of software during the early stages of its development life cycle. Requirements tracing usually consists of document parsing, candidate link generation and evaluation and traceability analysis. This paper demonstrates the applicability of Statistical Term Extraction metrics to generate candidate links. It is applied and validated using two data sets and four types of filters two for each data set, 0.2 and 0.25 for MODIS, 0 and 0.05 for CM1. This method generates requirements traceability matrices between textual requirements artifacts (such as high-level requirements traced to low-level requirements). The proposed method includes ten word frequency metrics divided into three main groups for calculating the frequency of terms. The results show that the proposed method gives better result when compared with the traditional TF-IDF method. version:1
arxiv-1506-08776 | Bayesian Nonparametric Kernel-Learning | http://arxiv.org/abs/1506.08776 | id:1506.08776 author:Junier Oliva, Avinava Dubey, Barnabas Poczos, Jeff Schneider, Eric P. Xing category:stat.ML  published:2015-06-29 summary:Kernel methods are ubiquitous tools in machine learning. They have proven to be effective in many domains and tasks. Yet, kernel methods often require the user to select a predefined kernel to build an estimator with. However, there is often little reason for the a priori selection of a kernel. Even if a universal approximating kernel is selected, the quality of the finite sample estimator may be greatly effected by the choice of kernel. Furthermore, when directly applying kernel methods, one typically needs to compute a $N \times N$ Gram matrix of pairwise kernel evaluations to work with a dataset of $N$ instances. The computation of this Gram matrix precludes the direct application of kernel methods on large datasets. In this paper we introduce Bayesian nonparmetric kernel (BaNK) learning, a generic, data-driven framework for scalable learning of kernels. We show that this framework can be used for performing both regression and classification tasks and scale to large datasets. Furthermore, we show that BaNK outperforms several other scalable approaches for kernel learning on a variety of real world datasets. version:1
arxiv-1506-08765 | Spectral Motion Synchronization in SE(3) | http://arxiv.org/abs/1506.08765 | id:1506.08765 author:Federica Arrigoni, Andrea Fusiello, Beatrice Rossi category:cs.CV  published:2015-06-29 summary:This paper addresses the problem of motion synchronization (or averaging) and describes a simple, closed-form solution based on a spectral decomposition, which does not consider rotation and translation separately but works straight in SE(3), the manifold of rigid motions. Besides its theoretical interest, being the first closed form solution in SE(3), experimental results show that it compares favourably with the state of the art both in terms of precision and speed. version:1
arxiv-1506-08760 | S2: An Efficient Graph Based Active Learning Algorithm with Application to Nonparametric Classification | http://arxiv.org/abs/1506.08760 | id:1506.08760 author:Gautam Dasarathy, Robert Nowak, Xiaojin Zhu category:cs.LG stat.ML  published:2015-06-29 summary:This paper investigates the problem of active learning for binary label prediction on a graph. We introduce a simple and label-efficient algorithm called S2 for this task. At each step, S2 selects the vertex to be labeled based on the structure of the graph and all previously gathered labels. Specifically, S2 queries for the label of the vertex that bisects the *shortest shortest* path between any pair of oppositely labeled vertices. We present a theoretical estimate of the number of queries S2 needs in terms of a novel parametrization of the complexity of binary functions on graphs. We also present experimental results demonstrating the performance of S2 on both real and synthetic data. While other graph-based active learning algorithms have shown promise in practice, our algorithm is the first with both good performance and theoretical guarantees. Finally, we demonstrate the implications of the S2 algorithm to the theory of nonparametric active learning. In particular, we show that S2 achieves near minimax optimal excess risk for an important class of nonparametric classification problems. version:1
arxiv-1506-08704 | An automatic and efficient foreground object extraction scheme | http://arxiv.org/abs/1506.08704 | id:1506.08704 author:Subhajit Adhikari, Joydeep Kar, Jayati Ghosh Dastidar category:cs.CV  published:2015-06-29 summary:This paper presents a method to differentiate the foreground objects from the background of a color image. Firstly a color image of any size is input for processing. The algorithm converts it to a grayscale image. Next we apply canny edge detector to find the boundary of the foreground object. We concentrate to find the maximum distance between each boundary pixel column wise and row wise and we fill the region that is bound by the edges. Thus we are able to extract the grayscale values of pixels that are in the bounded region and convert the grayscale image back to original color image containing only the foreground object. version:1
arxiv-1506-08815 | Tracking Direction of Human Movement - An Efficient Implementation using Skeleton | http://arxiv.org/abs/1506.08815 | id:1506.08815 author:Merina Kundu, Dhriti Sengupta, Jayati Ghosh Dastidar category:cs.CV  published:2015-06-29 summary:Sometimes a simple and fast algorithm is required to detect human presence and movement with a low error rate in a controlled environment for security purposes. Here a light weight algorithm has been presented that generates alert on detection of human presence and its movement towards a certain direction. The algorithm uses fixed angle CCTV camera images taken over time and relies upon skeleton transformation of successive images and calculation of difference in their coordinates. version:1
arxiv-1506-08690 | Portfolio optimization using local linear regression ensembles in RapidMiner | http://arxiv.org/abs/1506.08690 | id:1506.08690 author:Gabor Nagy, Gergo Barta, Tamas Henk category:q-fin.PM cs.LG stat.ML  published:2015-06-29 summary:In this paper we implement a Local Linear Regression Ensemble Committee (LOLREC) to predict 1-day-ahead returns of 453 assets form the S&P500. The estimates and the historical returns of the committees are used to compute the weights of the portfolio from the 453 stock. The proposed method outperforms benchmark portfolio selection strategies that optimize the growth rate of the capital. We investigate the effect of algorithm parameter m: the number of selected stocks on achieved average annual yields. Results suggest the algorithm's practical usefulness in everyday trading. version:1
arxiv-1506-08682 | Human Shape Variation - An Efficient Implementation using Skeleton | http://arxiv.org/abs/1506.08682 | id:1506.08682 author:Dhriti Sengupta, Merina Kundu, Jayati Ghosh Dastidar category:cs.CV  published:2015-06-29 summary:It is at times important to detect human presence automatically in secure environments. This needs a shape recognition algorithm that is robust, fast and has low error rates. The algorithm needs to process camera images quickly to detect any human in the range of vision, and generate alerts, especially if the object under scrutiny is moving in certain directions. We present here a simple, efficient and fast algorithm using skeletons of the images, and simple features like posture and length of the object. version:1
arxiv-1506-08670 | Automatic Channel Network Extraction from Remotely Sensed Images by Singularity Analysis | http://arxiv.org/abs/1506.08670 | id:1506.08670 author:F. Isikdogan, A. C. Bovik, P. Passalacqua category:cs.CV  published:2015-06-29 summary:Quantitative analysis of channel networks plays an important role in river studies. To provide a quantitative representation of channel networks, we propose a new method that extracts channels from remotely sensed images and estimates their widths. Our fully automated method is based on a recently proposed Multiscale Singularity Index that responds strongly to curvilinear structures but weakly to edges. The algorithm produces a channel map, using a single image where water and non-water pixels have contrast, such as a Landsat near-infrared band image or a water index defined on multiple bands. The proposed method provides a robust alternative to the procedures that are used in remote sensing of fluvial geomorphology and makes classification and analysis of channel networks easier. The source code of the algorithm is available at: http://live.ece.utexas.edu/research/cne/. version:1
arxiv-1506-05082 | A review of landmark articles in the field of co-evolutionary computing | http://arxiv.org/abs/1506.05082 | id:1506.05082 author:Noe Casas category:cs.NE  published:2015-06-10 summary:Coevolution is a powerful tool in evolutionary computing that mitigates some of its endemic problems, namely stagnation in local optima and lack of convergence in high dimensionality problems. Since its inception in 1990, there are multiple articles that have contributed greatly to the development and improvement of the coevolutionary techniques. In this report we review some of those landmark articles dwelving in the techniques they propose and how they fit to conform robust evolutionary algorithms version:2
arxiv-1506-02550 | Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem | http://arxiv.org/abs/1506.02550 | id:1506.02550 author:Junpei Komiyama, Junya Honda, Hisashi Kashima, Hiroshi Nakagawa category:stat.ML cs.LG  published:2015-06-08 summary:We study the $K$-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. We introduce a tight asymptotic regret lower bound that is based on the information divergence. An algorithm that is inspired by the Deterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010) is proposed, and its regret is analyzed. The proposed algorithm is found to be the first one with a regret upper bound that matches the lower bound. Experimental comparisons of dueling bandit algorithms show that the proposed algorithm significantly outperforms existing ones. version:3
arxiv-1506-08581 | Variational Inference for Background Subtraction in Infrared Imagery | http://arxiv.org/abs/1506.08581 | id:1506.08581 author:Konstantinos Makantasis, Anastasios Doulamis, Nikolaos Doulamis category:cs.CV cs.LG  published:2015-06-29 summary:We propose a Gaussian mixture model for background subtraction in infrared imagery. Following a Bayesian approach, our method automatically estimates the number of Gaussian components as well as their parameters, while simultaneously it avoids over/under fitting. The equations for estimating model parameters are analytically derived and thus our method does not require any sampling algorithm that is computationally and memory inefficient. The pixel density estimate is followed by an efficient and highly accurate updating mechanism, which permits our system to be automatically adapted to dynamically changing operation conditions. Experimental results and comparisons with other methods show that our method outperforms, in terms of precision and recall, while at the same time it keeps computational cost suitable for real-time applications. version:1
arxiv-1506-08544 | Exact and approximate inference in graphical models: variable elimination and beyond | http://arxiv.org/abs/1506.08544 | id:1506.08544 author:Nathalie Peyrard, Simon de Givry, Alain Franc, Stéphane Robin, Régis Sabbadin, Thomas Schiex, Matthieu Vignes category:stat.ML cs.AI cs.LG  published:2015-06-29 summary:Probabilistic graphical models offer a powerful framework to account for the dependence structure between variables, which can be represented as a graph. The dependence between variables may render inference tasks such as computing normalizing constant, marginalization or optimization intractable. The objective of this paper is to review techniques exploiting the graph structure for exact inference borrowed from optimization and computer science. They are not yet standard in the statistician toolkit, and we specify under which conditions they are efficient in practice. They are built on the principle of variable elimination whose complexity is dictated in an intricate way by the order in which variables are eliminated in the graph. The so-called treewidth of the graph characterizes this algorithmic complexity: low-treewidth graphs can be processed efficiently. Algorithmic solutions derived from variable elimination and the notion of treewidth are illustrated on problems of treewidth computation and inference in challenging benchmarks from optimization competitions. We also review how efficient techniques for approximate inference such as loopy belief propagation and variational approaches can be linked to variable elimination and we illustrate them in the context of Expectation-Maximisation procedures for parameter estimation in coupled Hidden Markov Models. version:1
arxiv-1506-08529 | Tell and Predict: Kernel Classifier Prediction for Unseen Visual Classes from Unstructured Text Descriptions | http://arxiv.org/abs/1506.08529 | id:1506.08529 author:Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh category:cs.CV  published:2015-06-29 summary:In this paper we propose a framework for predicting kernelized classifiers in the visual domain for categories with no training images where the knowledge comes from textual description about these categories. Through our optimization framework, the proposed approach is capable of embedding the class-level knowledge from the text domain as kernel classifiers in the visual domain. We also proposed a distributional semantic kernel between text descriptions which is shown to be effective in our setting. The proposed framework is not restricted to textual descriptions, and can also be applied to other forms knowledge representations. Our approach was applied for the challenging task of zero-shot learning of fine-grained categories from text descriptions of these categories. version:1
arxiv-1506-08511 | Integrative analysis of gene expression and phenotype data | http://arxiv.org/abs/1506.08511 | id:1506.08511 author:Min Xu category:q-bio.QM q-bio.GN q-bio.MN stat.ML  published:2015-06-29 summary:The linking genotype to phenotype is the fundamental aim of modern genetics. We focus on study of links between gene expression data and phenotype data through integrative analysis. We propose three approaches. 1) The inherent complexity of phenotypes makes high-throughput phenotype profiling a very difficult and laborious process. We propose a method of automated multi-dimensional profiling which uses gene expression similarity. Large-scale analysis show that our method can provide robust profiling that reveals different phenotypic aspects of samples. This profiling technique is also capable of interpolation and extrapolation beyond the phenotype information given in training data. It can be used in many applications, including facilitating experimental design and detecting confounding factors. 2) Phenotype association analysis problems are complicated by small sample size and high dimensionality. Consequently, phenotype-associated gene subsets obtained from training data are very sensitive to selection of training samples, and the constructed sample phenotype classifiers tend to have poor generalization properties. To eliminate these obstacles, we propose a novel approach that generates sequences of increasingly discriminative gene cluster combinations. Our experiments on both simulated and real datasets show robust and accurate classification performance. 3) Many complex phenotypes, such as cancer, are the product of not only gene expression, but also gene interaction. We propose an integrative approach to find gene network modules that activate under different phenotype conditions. Using our method, we discovered cancer subtype-specific network modules, as well as the ways in which these modules coordinate. In particular, we detected a breast-cancer specific tumor suppressor network module with a hub gene, PDGFRL, which may play an important role in this module. version:1
arxiv-1506-08499 | Compressed Sensing of Multi-Channel EEG Signals: The Simultaneous Cosparsity and Low Rank Optimization | http://arxiv.org/abs/1506.08499 | id:1506.08499 author:Yipeng Liu, Maarten De Vos, Sabine Van Huffel category:cs.IT math.IT stat.ML  published:2015-06-29 summary:Goal: This paper deals with the problems that some EEG signals have no good sparse representation and single channel processing is not computationally efficient in compressed sensing of multi-channel EEG signals. Methods: An optimization model with L0 norm and Schatten-0 norm is proposed to enforce cosparsity and low rank structures in the reconstructed multi-channel EEG signals. Both convex relaxation and global consensus optimization with alternating direction method of multipliers are used to compute the optimization model. Results: The performance of multi-channel EEG signal reconstruction is improved in term of both accuracy and computational complexity. Conclusion: The proposed method is a better candidate than previous sparse signal recovery methods for compressed sensing of EEG signals. Significance: The proposed method enables successful compressed sensing of EEG signals even when the signals have no good sparse representation. Using compressed sensing would much reduce the power consumption of wireless EEG system. version:1
arxiv-1506-08485 | The Multi-Strand Graph for a PTZ Tracker | http://arxiv.org/abs/1506.08485 | id:1506.08485 author:Shachaf Melman, Yael Moses, Gérard Medioni, Yinghao Cai category:cs.CV  published:2015-06-29 summary:High-resolution images can be used to resolve matching ambiguities between trajectory fragments (tracklets), which is one of the main challenges in multiple target tracking. A PTZ camera, which can pan, tilt and zoom, is a powerful and efficient tool that offers both close-up views and wide area coverage on demand. The wide-area view makes it possible to track many targets while the close-up view allows individuals to be identified from high-resolution images of their faces. A central component of a PTZ tracking system is a scheduling algorithm that determines which target to zoom in on. In this paper we study this scheduling problem from a theoretical perspective, where the high resolution images are also used for tracklet matching. We propose a novel data structure, the Multi-Strand Tracking Graph (MSG), which represents the set of tracklets computed by a tracker and the possible associations between them. The MSG allows efficient scheduling as well as resolving -- directly or by elimination -- matching ambiguities between tracklets. The main feature of the MSG is the auxiliary data saved in each vertex, which allows efficient computation while avoiding time-consuming graph traversal. Synthetic data simulations are used to evaluate our scheduling algorithm and to demonstrate its superiority over a na\"ive one. version:1
arxiv-1503-06236 | Nonparametric Estimation of Band-limited Probability Density Functions | http://arxiv.org/abs/1503.06236 | id:1503.06236 author:Rahul Agarwal, Zhe Chen, Sridevi V. Sarma category:stat.ML math.ST stat.ME stat.TH  published:2015-03-20 summary:In this paper, a nonparametric maximum likelihood (ML) estimator for band-limited (BL) probability density functions (pdfs) is proposed. The BLML estimator is consistent and computationally efficient. To compute the BLML estimator, three approximate algorithms are presented: a binary quadratic programming (BQP) algorithm for medium scale problems, a Trivial algorithm for large-scale problems that yields a consistent estimate if the underlying pdf is strictly positive and BL, and a fast implementation of the Trivial algorithm that exploits the band-limited assumption and the Nyquist sampling theorem ("BLMLQuick"). All three BLML estimators outperform kernel density estimation (KDE) algorithms (adaptive and higher order KDEs) with respect to the mean integrated squared error for data generated from both BL and infinite-band pdfs. Further, the BLMLQuick estimate is remarkably faster than the KD algorithms. Finally, the BLML method is applied to estimate the conditional intensity function of a neuronal spike train (point process) recorded from a rat's entorhinal cortex grid cell, for which it outperforms state-of-the-art estimators used in neuroscience. version:5
arxiv-1506-04334 | A Bayesian Model for Generative Transition-based Dependency Parsing | http://arxiv.org/abs/1506.04334 | id:1506.04334 author:Jan Buys, Phil Blunsom category:cs.CL  published:2015-06-13 summary:We propose a simple, scalable, fully generative model for transition-based dependency parsing with high accuracy. The model, parameterized by Hierarchical Pitman-Yor Processes, overcomes the limitations of previous generative models by allowing fast and accurate inference. We propose an efficient decoding algorithm based on particle filtering that can adapt the beam size to the uncertainty in the model while jointly predicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. version:2
arxiv-1506-08454 | WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for Information Extraction | http://arxiv.org/abs/1506.08454 | id:1506.08454 author:Vijil Chenthamarakshan, Prasad M Desphande, Raghu Krishnapuram, Ramakrishna Varadarajan, Knut Stolze category:cs.CL cs.DB cs.IR  published:2015-06-28 summary:The visual layout of a webpage can provide valuable clues for certain types of Information Extraction (IE) tasks. In traditional rule based IE frameworks, these layout cues are mapped to rules that operate on the HTML source of the webpages. In contrast, we have developed a framework in which the rules can be specified directly at the layout level. This has many advantages, since the higher level of abstraction leads to simpler extraction rules that are largely independent of the source code of the page, and, therefore, more robust. It can also enable specification of new types of rules that are not otherwise possible. To the best of our knowledge, there is no general framework that allows declarative specification of information extraction rules based on spatial layout. Our framework is complementary to traditional text based rules framework and allows a seamless combination of spatial layout based rules with traditional text based rules. We describe the algebra that enables such a system and its efficient implementation using standard relational and text indexing features of a relational database. We demonstrate the simplicity and efficiency of this system for a task involving the extraction of software system requirements from software product pages. version:1
arxiv-1412-4181 | Oriented Edge Forests for Boundary Detection | http://arxiv.org/abs/1412.4181 | id:1412.4181 author:Sam Hallman, Charless C. Fowlkes category:cs.CV  published:2014-12-13 summary:We present a simple, efficient model for learning boundary detection based on a random forest classifier. Our approach combines (1) efficient clustering of training examples based on simple partitioning of the space of local edge orientations and (2) scale-dependent calibration of individual tree output probabilities prior to multiscale combination. The resulting model outperforms published results on the challenging BSDS500 boundary detection benchmark. Further, on large datasets our model requires substantially less memory for training and speeds up training time by a factor of 10 over the structured forest model. version:2
arxiv-1506-08425 | Deep-Plant: Plant Identification with convolutional neural networks | http://arxiv.org/abs/1506.08425 | id:1506.08425 author:Sue Han Lee, Chee Seng Chan, Paul Wilkin, Paolo Remagnino category:cs.CV cs.AI cs.NE  published:2015-06-28 summary:This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a 'black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features. version:1
arxiv-1506-08422 | Topic2Vec: Learning Distributed Representations of Topics | http://arxiv.org/abs/1506.08422 | id:1506.08422 author:Li-Qiang Niu, Xin-Yu Dai category:cs.CL cs.LG  published:2015-06-28 summary:Latent Dirichlet Allocation (LDA) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from LDA only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than LDA-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results. version:1
arxiv-1502-06254 | The fundamental nature of the log loss function | http://arxiv.org/abs/1502.06254 | id:1502.06254 author:Vladimir Vovk category:cs.LG stat.ME  published:2015-02-22 summary:The standard loss functions used in the literature on probabilistic prediction are the log loss function, the Brier loss function, and the spherical loss function; however, any computable proper loss function can be used for comparison of prediction algorithms. This note shows that the log loss function is most selective in that any prediction algorithm that is optimal for a given data sequence (in the sense of the algorithmic theory of randomness) under the log loss function will be optimal under any computable proper mixable loss function; on the other hand, there is a data sequence and a prediction algorithm that is optimal for that sequence under either of the two other standard loss functions but not under the log loss function. version:2
arxiv-1506-06707 | Non-Normal Mixtures of Experts | http://arxiv.org/abs/1506.06707 | id:1506.06707 author:Faicel Chamroukhi category:stat.ME cs.LG stat.ML  published:2015-06-22 summary:Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in data for regression, classification and clustering. For continuous data which we consider here in the context of regression and cluster analysis, MoE usually use normal experts, that is, expert components following the Gaussian distribution. However, for a set of data containing a group or groups of observations with asymmetric behavior, heavy tails or atypical observations, the use of normal experts may be unsuitable and can unduly affect the fit of the MoE model. In this paper, we introduce new non-normal mixture of experts (NNMoE) which can deal with these issues regarding possibly skewed, heavy-tailed data and with outliers. The proposed models are the skew-normal MoE and the robust $t$ MoE and skew $t$ MoE, respectively named SNMoE, TMoE and STMoE. We develop dedicated expectation-maximization (EM) and expectation conditional maximization (ECM) algorithms to estimate the parameters of the proposed models by monotonically maximizing the observed data log-likelihood. We describe how the presented models can be used in prediction and in model-based clustering of regression data. Numerical experiments carried out on simulated data show the effectiveness and the robustness of the proposed models in terms modeling non-linear regression functions as well as in model-based clustering. Then, to show their usefulness for practical applications, the proposed models are applied to the real-world data of tone perception for musical data analysis, and the one of temperature anomalies for the analysis of climate change data. version:2
arxiv-1409-5209 | Pedestrian Detection with Spatially Pooled Features and Structured Ensemble Learning | http://arxiv.org/abs/1409.5209 | id:1409.5209 author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG  published:2014-09-18 summary:Many typical applications of object detection operate within a prescribed false-positive range. In this situation the performance of a detector should be assessed on the basis of the area under the ROC curve over that range, rather than over the full curve, as the performance outside the range is irrelevant. This measure is labelled as the partial area under the ROC curve (pAUC). We propose a novel ensemble learning method which achieves a maximal detection rate at a user-defined range of false positive rates by directly optimizing the partial AUC using structured learning. In order to achieve a high object detection performance, we propose a new approach to extract low-level visual features based on spatial pooling. Incorporating spatial pooling improves the translational invariance and thus the robustness of the detection process. Experimental results on both synthetic and real-world data sets demonstrate the effectiveness of our approach, and we show that it is possible to train state-of-the-art pedestrian detectors using the proposed structured ensemble learning method with spatially pooled features. The result is the current best reported performance on the Caltech-USA pedestrian detection dataset. version:3
arxiv-1504-06796 | Overlapping Communities Detection via Measure Space Embedding | http://arxiv.org/abs/1504.06796 | id:1504.06796 author:Mark Kozdoba, Shie Mannor category:cs.LG cs.SI stat.ML  published:2015-04-26 summary:We present a new algorithm for community detection. The algorithm uses random walks to embed the graph in a space of measures, after which a modification of $k$-means in that space is applied. The algorithm is therefore fast and easily parallelizable. We evaluate the algorithm on standard random graph benchmarks, including some overlapping community benchmarks, and find its performance to be better or at least as good as previously known algorithms. We also prove a linear time (in number of edges) guarantee for the algorithm on a $p,q$-stochastic block model with $p \geq c\cdot N^{-\frac{1}{2} + \epsilon}$ and $p-q \geq c' \sqrt{p N^{-\frac{1}{2} + \epsilon} \log N}$. version:2
arxiv-1506-08361 | Simultaneously Solving Computational Problems Using an Artificial Chemical Reactor | http://arxiv.org/abs/1506.08361 | id:1506.08361 author:Jaderick P. Pabico category:cs.ET cs.NE  published:2015-06-28 summary:This paper is centered on using chemical reaction as a computational metaphor for simultaneously solving problems. An artificial chemical reactor that can simultaneously solve instances of three unrelated problems was created. The reactor is a distributed stochastic algorithm that simulates a chemical universe wherein the molecular species are being represented either by a human genomic contig panel, a Hamiltonian cycle, or an aircraft landing schedule. The chemical universe is governed by reactions that can alter genomic sequences, re-order Hamiltonian cycles, or reschedule an aircraft landing program. Molecular masses were considered as measures of goodness of solutions, and represented radiation hybrid (RH) vector similarities, costs of Hamiltonian cycles, and penalty costs for landing an aircraft before and after target landing times. This method, tested by solving in tandem with deterministic algorithms, has been shown to find quality solutions in finding the minima RH vector similarities of genomic data, minima costs in Hamiltonian cycles of the traveling salesman, and minima costs for landing aircrafts before or after target landing times. version:1
arxiv-1506-08353 | Patch-Based Low-Rank Minimization for Image Denoising | http://arxiv.org/abs/1506.08353 | id:1506.08353 author:Haijuan Hu, Jacques Froment, Quansheng Liu category:cs.CV  published:2015-06-28 summary:Patch-based sparse representation and low-rank approximation for image processing attract much attention in recent years. The minimization of the matrix rank coupled with the Frobenius norm data fidelity can be solved by the hard thresholding filter with principle component analysis (PCA) or singular value decomposition (SVD). Based on this idea, we propose a patch-based low-rank minimization method for image denoising, which learns compact dictionaries from similar patches with PCA or SVD, and applies simple hard thresholding filters to shrink the representation coefficients. Compared to recent patch-based sparse representation methods, experiments demonstrate that the proposed method is not only rather rapid, but also effective for a variety of natural images, especially for texture parts in images. version:1
arxiv-1506-08349 | Improved Deep Speaker Feature Learning for Text-Dependent Speaker Recognition | http://arxiv.org/abs/1506.08349 | id:1506.08349 author:Lantian Li, Yiye Lin, Zhiyong Zhang, Dong Wang category:cs.CL cs.LG cs.NE  published:2015-06-28 summary:A deep learning approach has been proposed recently to derive speaker identifies (d-vector) by a deep neural network (DNN). This approach has been applied to text-dependent speaker recognition tasks and shows reasonable performance gains when combined with the conventional i-vector approach. Although promising, the existing d-vector implementation still can not compete with the i-vector baseline. This paper presents two improvements for the deep learning approach: a phonedependent DNN structure to normalize phone variation, and a new scoring approach based on dynamic time warping (DTW). Experiments on a text-dependent speaker recognition task demonstrated that the proposed methods can provide considerable performance improvement over the existing d-vector implementation. version:1
arxiv-1506-08347 | Occlusion Coherence: Detecting and Localizing Occluded Faces | http://arxiv.org/abs/1506.08347 | id:1506.08347 author:Golnaz Ghiasi, Charless C. Fowlkes category:cs.CV  published:2015-06-28 summary:The presence of occluders significantly impacts object recognition accuracy. However, occlusion is typically treated as an unstructured source of noise and explicit models for occluders have lagged behind those for object appearance and shape. In this paper we describe a hierarchical deformable part model for face detection and keypoint localization that explicitly models part occlusion. The proposed model structure makes it possible to augment positive training data with large numbers of synthetically occluded instances. This allows us to easily incorporate the statistics of occlusion patterns in a discriminatively trained model. We test the model on several benchmarks for keypoint localization and detection including challenging data sets featuring significant occlusion. We find that the addition of an explicit model of occlusion yields a system that outperforms existing approaches in keypoint localization accuracy and detection performance. version:1
arxiv-1506-08251 | Occam's Gates | http://arxiv.org/abs/1506.08251 | id:1506.08251 author:Jonathan Raiman, Szymon Sidor category:cs.LG  published:2015-06-27 summary:We present a complimentary objective for training recurrent neural networks (RNN) with gating units that helps with regularization and interpretability of the trained model. Attention-based RNN models have shown success in many difficult sequence to sequence classification problems with long and short term dependencies, however these models are prone to overfitting. In this paper, we describe how to regularize these models through an L1 penalty on the activation of the gating units, and show that this technique reduces overfitting on a variety of tasks while also providing to us a human-interpretable visualization of the inputs used by the network. These tasks include sentiment analysis, paraphrase recognition, and question answering. version:1
arxiv-1412-1454 | Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation | http://arxiv.org/abs/1412.1454 | id:1412.1454 author:Noam Shazeer, Joris Pelemans, Ciprian Chelba category:cs.LG cs.CL  published:2014-12-03 summary:We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM $n$-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as $n$-gram LMs do. version:2
arxiv-1506-08187 | A geometric alternative to Nesterov's accelerated gradient descent | http://arxiv.org/abs/1506.08187 | id:1506.08187 author:Sébastien Bubeck, Yin Tat Lee, Mohit Singh category:math.OC cs.DS cs.LG cs.NA  published:2015-06-26 summary:We propose a new method for unconstrained optimization of a smooth and strongly convex function, which attains the optimal rate of convergence of Nesterov's accelerated gradient descent. The new algorithm has a simple geometric interpretation, loosely inspired by the ellipsoid method. We provide some numerical evidence that the new method can be superior to Nesterov's accelerated gradient descent. version:1
arxiv-1506-08180 | An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process | http://arxiv.org/abs/1506.08180 | id:1506.08180 author:Amar Shah, David A. Knowles, Zoubin Ghahramani category:stat.ML cs.LG stat.AP stat.CO stat.ME  published:2015-06-26 summary:Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we find that different posterior dependencies are important in BPFA relative to LDA. Particularly, approximations able to model intra-local variable dependence perform best. version:1
arxiv-1503-03525 | Online Matrix Completion and Online Robust PCA | http://arxiv.org/abs/1503.03525 | id:1503.03525 author:Brian Lois, Namrata Vaswani category:cs.IT math.IT stat.ML  published:2015-03-11 summary:This work studies two interrelated problems - online robust PCA (RPCA) and online low-rank matrix completion (MC). In recent work by Cand\`{e}s et al., RPCA has been defined as a problem of separating a low-rank matrix (true data), $L:=[\ell_1, \ell_2, \dots \ell_{t}, \dots , \ell_{t_{\max}}]$ and a sparse matrix (outliers), $S:=[x_1, x_2, \dots x_{t}, \dots, x_{t_{\max}}]$ from their sum, $M:=L+S$. Our work uses this definition of RPCA. An important application where both these problems occur is in video analytics in trying to separate sparse foregrounds (e.g., moving objects) and slowly changing backgrounds. While there has been a large amount of recent work on both developing and analyzing batch RPCA and batch MC algorithms, the online problem is largely open. In this work, we develop a practical modification of our recently proposed algorithm to solve both the online RPCA and online MC problems. The main contribution of this work is that we obtain correctness results for the proposed algorithms under mild assumptions. The assumptions that we need are: (a) a good estimate of the initial subspace is available (easy to obtain using a short sequence of background-only frames in video surveillance); (b) the $\ell_t$'s obey a `slow subspace change' assumption; (c) the basis vectors for the subspace from which $\ell_t$ is generated are dense (non-sparse); (d) the support of $x_t$ changes by at least a certain amount at least every so often; and (e) algorithm parameters are appropriately set version:2
arxiv-1506-08170 | Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis | http://arxiv.org/abs/1506.08170 | id:1506.08170 author:Zhuang Ma, Yichao Lu, Dean Foster category:stat.ML stat.CO  published:2015-06-26 summary:Canonical Correlation Analysis (CCA) is a widely used spectral technique for finding correlation structures in multi-view datasets. In this paper, we tackle the problem of large scale CCA, where classical algorithms, usually requiring computing the product of two huge matrices and huge matrix decomposition, are computationally and storage expensive. We recast CCA from a novel perspective and propose a scalable and memory efficient Augmented Approximate Gradient (AppGrad) scheme for finding top $k$ dimensional canonical subspace which only involves large matrix multiplying a thin matrix of width $k$ and small matrix decomposition of dimension $k\times k$. Further, AppGrad achieves optimal storage complexity $O(k(p_1+p_2))$, compared with classical algorithms which usually require $O(p_1^2+p_2^2)$ space to store two dense whitening matrices. The proposed scheme naturally generalizes to stochastic optimization regime, especially efficient for huge datasets where batch algorithms are prohibitive. The online property of stochastic AppGrad is also well suited to the streaming scenario, where data comes sequentially. To the best of our knowledge, it is the first stochastic algorithm for CCA. Experiments on four real data sets are provided to show the effectiveness of the proposed methods. version:1
arxiv-1505-00870 | An $O(n\log(n))$ Algorithm for Projecting Onto the Ordered Weighted $\ell_1$ Norm Ball | http://arxiv.org/abs/1505.00870 | id:1505.00870 author:Damek Davis category:math.OC cs.LG  published:2015-05-05 summary:The ordered weighted $\ell_1$ (OWL) norm is a newly developed generalization of the Octogonal Shrinkage and Clustering Algorithm for Regression (OSCAR) norm. This norm has desirable statistical properties and can be used to perform simultaneous clustering and regression. In this paper, we show how to compute the projection of an $n$-dimensional vector onto the OWL norm ball in $O(n\log(n))$ operations. In addition, we illustrate the performance of our algorithm on a synthetic regression test. version:3
arxiv-1506-08126 | Humor in Collective Discourse: Unsupervised Funniness Detection in the New Yorker Cartoon Caption Contest | http://arxiv.org/abs/1506.08126 | id:1506.08126 author:Dragomir Radev, Amanda Stent, Joel Tetreault, Aasish Pappu, Aikaterini Iliakopoulou, Agustin Chanfreau, Paloma de Juan, Jordi Vallmitjana, Alejandro Jaimes, Rahul Jha, Bob Mankoff category:cs.CL cs.AI cs.MM stat.ML  published:2015-06-26 summary:The New Yorker publishes a weekly captionless cartoon. More than 5,000 readers submit captions for it. The editors select three of them and ask the readers to pick the funniest one. We describe an experiment that compares a dozen automatic methods for selecting the funniest caption. We show that negative sentiment, human-centeredness, and lexical centrality most strongly match the funniest captions, followed by positive sentiment. These results are useful for understanding humor and also in the design of more engaging conversational agents in text and multimodal (vision+text) systems. As part of this work, a large set of cartoons and captions is being made available to the community. version:1
arxiv-1506-08105 | Modelling of directional data using Kent distributions | http://arxiv.org/abs/1506.08105 | id:1506.08105 author:Parthan Kasarapu category:cs.LG stat.ML  published:2015-06-26 summary:The modelling of data on a spherical surface requires the consideration of directional probability distributions. To model asymmetrically distributed data on a three-dimensional sphere, Kent distributions are often used. The moment estimates of the parameters are typically used in modelling tasks involving Kent distributions. However, these lack a rigorous statistical treatment. The focus of the paper is to introduce a Bayesian estimation of the parameters of the Kent distribution which has not been carried out in the literature, partly because of its complex mathematical form. We employ the Bayesian information-theoretic paradigm of Minimum Message Length (MML) to bridge this gap and derive reliable estimators. The inferred parameters are subsequently used in mixture modelling of Kent distributions. The problem of inferring the suitable number of mixture components is also addressed using the MML criterion. We demonstrate the superior performance of the derived MML-based parameter estimates against the traditional estimators. We apply the MML principle to infer mixtures of Kent distributions to model empirical data corresponding to protein conformations. We demonstrate the effectiveness of Kent models to act as improved descriptors of protein structural data as compared to commonly used von Mises-Fisher distributions. version:1
arxiv-1503-08528 | Average Distance Queries through Weighted Samples in Graphs and Metric Spaces: High Scalability with Tight Statistical Guarantees | http://arxiv.org/abs/1503.08528 | id:1503.08528 author:Shiri Chechik, Edith Cohen, Haim Kaplan category:cs.SI cs.LG  published:2015-03-30 summary:The average distance from a node to all other nodes in a graph, or from a query point in a metric space to a set of points, is a fundamental quantity in data analysis. The inverse of the average distance, known as the (classic) closeness centrality of a node, is a popular importance measure in the study of social networks. We develop novel structural insights on the sparsifiability of the distance relation via weighted sampling. Based on that, we present highly practical algorithms with strong statistical guarantees for fundamental problems. We show that the average distance (and hence the centrality) for all nodes in a graph can be estimated using $O(\epsilon^{-2})$ single-source distance computations. For a set $V$ of $n$ points in a metric space, we show that after preprocessing which uses $O(n)$ distance computations we can compute a weighted sample $S\subset V$ of size $O(\epsilon^{-2})$ such that the average distance from any query point $v$ to $V$ can be estimated from the distances from $v$ to $S$. Finally, we show that for a set of points $V$ in a metric space, we can estimate the average pairwise distance using $O(n+\epsilon^{-2})$ distance computations. The estimate is based on a weighted sample of $O(\epsilon^{-2})$ pairs of points, which is computed using $O(n)$ distance computations. Our estimates are unbiased with normalized mean square error (NRMSE) of at most $\epsilon$. Increasing the sample size by a $O(\log n)$ factor ensures that the probability that the relative error exceeds $\epsilon$ is polynomially small. version:6
arxiv-1506-08006 | Spectral Collaborative Representation based Classification for Hand Gestures recognition on Electromyography Signals | http://arxiv.org/abs/1506.08006 | id:1506.08006 author:Ali Boyali category:cs.CV  published:2015-06-26 summary:In this study, we introduce a novel variant and application of the Collaborative Representation based Classification in spectral domain for recognition of the hand gestures using the raw surface Electromyography signals. The intuitive use of spectral features are explained via circulant matrices. The proposed Spectral Collaborative Representation based Classification (SCRC) is able to recognize gestures with higher levels of accuracy for a fairly rich gesture set. The worst recognition result which is the best in the literature is obtained as 97.3\% among the four sets of the experiments for each hand gestures. The recognition results are reported with a substantial number of experiments and labeling computation. version:1
arxiv-1506-08004 | ASOC: An Adaptive Parameter-free Stochastic Optimization Techinique for Continuous Variables | http://arxiv.org/abs/1506.08004 | id:1506.08004 author:Jayanta Basak category:cs.NE  published:2015-06-26 summary:Stochastic optimization is an important task in many optimization problems where the tasks are not expressible as convex optimization problems. In the case of non-convex optimization problems, various different stochastic algorithms like simulated annealing, evolutionary algorithms, and tabu search are available. Most of these algorithms require user-defined parameters specific to the problem in order to find out the optimal solution. Moreover, in many situations, iterative fine-tunings are required for the user-defined parameters, and therefore these algorithms cannot adapt if the search space and the optima changes over time. In this paper we propose an \underline{a}daptive parameter-free \underline{s}tochastic \underline{o}ptimization technique for \underline{c}ontinuous random variables called ASOC. version:1
arxiv-1506-08002 | Safe Feature Pruning for Sparse High-Order Interaction Models | http://arxiv.org/abs/1506.08002 | id:1506.08002 author:Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda, Ichiro Takeuchi category:stat.ML  published:2015-06-26 summary:Taking into account high-order interactions among covariates is valuable in many practical regression problems. This is, however, computationally challenging task because the number of high-order interaction features to be considered would be extremely large unless the number of covariates is sufficiently small. In this paper, we propose a novel efficient algorithm for LASSO-based sparse learning of such high-order interaction models. Our basic strategy for reducing the number of features is to employ the idea of recently proposed safe feature screening (SFS) rule. An SFS rule has a property that, if a feature satisfies the rule, then the feature is guaranteed to be non-active in the LASSO solution, meaning that it can be safely screened-out prior to the LASSO training process. If a large number of features can be screened-out before training the LASSO, the computational cost and the memory requirment can be dramatically reduced. However, applying such an SFS rule to each of the extremely large number of high-order interaction features would be computationally infeasible. Our key idea for solving this computational issue is to exploit the underlying tree structure among high-order interaction features. Specifically, we introduce a pruning condition called safe feature pruning (SFP) rule which has a property that, if the rule is satisfied in a certain node of the tree, then all the high-order interaction features corresponding to its descendant nodes can be guaranteed to be non-active at the optimal solution. Our algorithm is extremely efficient, making it possible to work, e.g., with 3rd order interactions of 10,000 original covariates, where the number of possible high-order interaction features is greater than 10^{12}. version:1
arxiv-1506-07997 | An Efficient Post-Selection Inference on High-Order Interaction Models | http://arxiv.org/abs/1506.07997 | id:1506.07997 author:S. Suzumura, K. Nakagawa, K. Tsuda, I. Takeuchi category:stat.ML  published:2015-06-26 summary:Finding statistically significant high-order interaction features in predictive modeling is important but challenging task. The difficulty lies in the fact that, for a recent applications with high-dimensional covariates, the number of possible high-order interaction features would be extremely large. Identifying statistically significant features from such a huge pool of candidates would be highly challenging both in computational and statistical senses. To work with this problem, we consider a two stage algorithm where we first select a set of high-order interaction features by marginal screening, and then make statistical inferences on the regression model fitted only with the selected features. Such statistical inferences are called post-selection inference (PSI), and receiving an increasing attention in the literature. One of the seminal recent advancements in PSI literature is the works by Lee et al. where the authors presented an algorithmic framework for computing exact sampling distributions in PSI. A main challenge when applying their approach to our high-order interaction models is to cope with the fact that PSI in general depends not only on the selected features but also on the unselected features, making it hard to apply to our extremely high-dimensional high-order interaction models. The goal of this paper is to overcome this difficulty by introducing a novel efficient method for PSI. Our key idea is to exploit the underlying tree structure among high-order interaction features, and to develop a pruning method of the tree which enables us to quickly identify a group of unselected features that are guaranteed to have no influence on PSI. The experimental results indicate that the proposed method allows us to reliably identify statistically significant high-order interaction features with reasonable computational cost. version:1
arxiv-1506-08867 | Java Implementation of a Parameter-less Evolutionary Portfolio | http://arxiv.org/abs/1506.08867 | id:1506.08867 author:José C. Pereira, Fernando G. Lobo category:cs.MS cs.NE I.2.8  published:2015-06-26 summary:The Java implementation of a portfolio of parameter-less evolutionary algorithms is presented. The Parameter-less Evolutionary Portfolio implements a heuristic that performs adaptive selection of parameter-less evolutionary algorithms in accordance with performance criteria that are measured during running time. At present time, the portfolio includes three parameter-less evolutionary algorithms: Parameter-less Univariate Marginal Distribution Algorithm, Parameter-less Extended Compact Genetic Algorithm, and Parameter-less Hierarchical Bayesian Optimization Algorithm. Initial experiments showed that the parameter-less portfolio can solve various classes of problems without the need for any prior parameter setting technique and with an increase in computational effort that can be considered acceptable. version:1
arxiv-1506-08694 | A Java Implementation of Parameter-less Evolutionary Algorithms | http://arxiv.org/abs/1506.08694 | id:1506.08694 author:José C. Pereira, Fernando G. Lobo category:cs.MS cs.NE I.2.8  published:2015-06-26 summary:The Parameter-less Genetic Algorithm was first presented by Harik and Lobo in 1999 as an alternative to the usual trial-and-error method of finding, for each given problem, an acceptable set-up of the parameter values of the genetic algorithm. Since then, the same strategy has been successfully applied to create parameter-less versions of other population-based search algorithms such as the Extended Compact Genetic Algorithm and the Hierarchical Bayesian Optimization Algorithm. This report describes a Java implementation, Parameter-less Evolutionary Algorithm (P-EAJava), that integrates several parameter-less evolutionary algorithms into a single platform. Along with a brief description of P-EAJava, we also provide detailed instructions on how to use it, how to implement new problems, and how to generate new parameter-less versions of evolutionary algorithms. At present time, P-EAJava already includes parameter-less versions of the Simple Genetic Algorithm, the Extended Compact Genetic Algorithm, the Univariate Marginal Distribution Algorithm, and the Hierarchical Bayesian Optimization Algorithm. The source and binary files of the Java implementation of P-EAJava are available for free download at https://github.com/JoseCPereira/2015ParameterlessEvolutionaryAlgorithmsJava. version:1
arxiv-1506-07980 | A Java Implementation of the SGA, UMDA, ECGA, and HBOA | http://arxiv.org/abs/1506.07980 | id:1506.07980 author:José C. Pereira, Fernando G. Lobo category:cs.NE cs.MS I.2.8  published:2015-06-26 summary:The Simple Genetic Algorithm, the Univariate Marginal Distribution Algorithm, the Extended Compact Genetic Algorithm, and the Hierarchical Bayesian Optimization Algorithm are all well known Evolutionary Algorithms. In this report we present a Java implementation of these four algorithms with detailed instructions on how to use each of them to solve a given set of optimization problems. Additionally, it is explained how to implement and integrate new problems within the provided set. The source and binary files of the Java implementations are available for free download at https://github.com/JoseCPereira/2015EvolutionaryAlgorithmsJava. version:1
arxiv-1506-04389 | Online Matrix Factorization via Broyden Updates | http://arxiv.org/abs/1506.04389 | id:1506.04389 author:Ömer Deniz Akyıldız category:stat.ML  published:2015-06-14 summary:In this paper, we propose an online algorithm to compute matrix factorizations. Proposed algorithm updates the dictionary matrix and associated coefficients using a single observation at each time. The algorithm performs low-rank updates to dictionary matrix. We derive the algorithm by defining a simple objective function to minimize whenever an observation is arrived. We extend the algorithm further for handling missing data. We also provide a mini-batch extension which enables to compute the matrix factorization on big datasets. We demonstrate the efficiency of our algorithm on a real dataset and give comparisons with well-known algorithms such as stochastic gradient matrix factorization and nonnegative matrix factorization (NMF). version:2
arxiv-1506-07959 | Factorized Asymptotic Bayesian Inference for Factorial Hidden Markov Models | http://arxiv.org/abs/1506.07959 | id:1506.07959 author:Shaohua Li, Ryohei Fujimaki, Chunyan Miao category:stat.ML  published:2015-06-26 summary:Factorial hidden Markov models (FHMMs) are powerful tools of modeling sequential data. Learning FHMMs yields a challenging simultaneous model selection issue, i.e., selecting the number of multiple Markov chains and the dimensionality of each chain. Our main contribution is to address this model selection issue by extending Factorized Asymptotic Bayesian (FAB) inference to FHMMs. First, we offer a better approximation of marginal log-likelihood than the previous FAB inference. Our key idea is to integrate out transition probabilities, yet still apply the Laplace approximation to emission probabilities. Second, we prove that if there are two very similar hidden states in an FHMM, i.e. one is redundant, then FAB will almost surely shrink and eliminate one of them, making the model parsimonious. Experimental results show that FAB for FHMMs significantly outperforms state-of-the-art nonparametric Bayesian iFHMM and Variational FHMM in model selection accuracy, with competitive held-out perplexity. version:1
arxiv-1506-07950 | Bag-of-Features Image Indexing and Classification in Microsoft SQL Server Relational Database | http://arxiv.org/abs/1506.07950 | id:1506.07950 author:Marcin Korytkowski, Rafal Scherer, Pawel Staszewski, Piotr Woldan category:cs.DB cs.CV  published:2015-06-26 summary:This paper presents a novel relational database architecture aimed to visual objects classification and retrieval. The framework is based on the bag-of-features image representation model combined with the Support Vector Machine classification and is integrated in a Microsoft SQL Server database. version:1
arxiv-1506-07947 | Collaboratively Learning Preferences from Ordinal Data | http://arxiv.org/abs/1506.07947 | id:1506.07947 author:Sewoong Oh, Kiran K. Thekumparampil, Jiaming Xu category:cs.LG cs.IT math.IT stat.ML  published:2015-06-26 summary:In applications such as recommendation systems and revenue management, it is important to predict preferences on items that have not been seen by a user or predict outcomes of comparisons among those that have never been compared. A popular discrete choice model of multinomial logit model captures the structure of the hidden preferences with a low-rank matrix. In order to predict the preferences, we want to learn the underlying model from noisy observations of the low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization. We present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling. In both cases, we show that the convex relaxation is minimax optimal. We prove an upper bound on the resulting error with finite samples, and provide a matching information-theoretic lower bound. version:1
arxiv-1506-07930 | Clustering categorical data via ensembling dissimilarity matrices | http://arxiv.org/abs/1506.07930 | id:1506.07930 author:Saeid Amiri, Bertrand Clarke, Jennifer Clarke category:stat.ML  published:2015-06-26 summary:We present a technique for clustering categorical data by generating many dissimilarity matrices and averaging over them. We begin by demonstrating our technique on low dimensional categorical data and comparing it to several other techniques that have been proposed. Then we give conditions under which our method should yield good results in general. Our method extends to high dimensional categorical data of equal lengths by ensembling over many choices of explanatory variables. In this context we compare our method with two other methods. Finally, we extend our method to high dimensional categorical data vectors of unequal length by using alignment techniques to equalize the lengths. We give examples to show that our method continues to provide good results, in particular, better in the context of genome sequences than clusterings suggested by phylogenetic trees. version:1
arxiv-1506-07925 | Analyzing statistical and computational tradeoffs of estimation procedures | http://arxiv.org/abs/1506.07925 | id:1506.07925 author:Daniel L. Sussman, Alexander Volfovsky, Edoardo M. Airoldi category:stat.CO stat.ML  published:2015-06-25 summary:The recent explosion in the amount and dimensionality of data has exacerbated the need of trading off computational and statistical efficiency carefully, so that inference is both tractable and meaningful. We propose a framework that provides an explicit opportunity for practitioners to specify how much statistical risk they are willing to accept for a given computational cost, and leads to a theoretical risk-computation frontier for any given inference problem. We illustrate the tradeoff between risk and computation and illustrate the frontier in three distinct settings. First, we derive analytic forms for the risk of estimating parameters in the classical setting of estimating the mean and variance for normally distributed data and for the more general setting of parameters of an exponential family. The second example concentrates on computationally constrained Hodges-Lehmann estimators. We conclude with an evaluation of risk associated with early termination of iterative matrix inversion algorithms in the context of linear regression. version:1
arxiv-1506-07840 | Diffusion Nets | http://arxiv.org/abs/1506.07840 | id:1506.07840 author:Gal Mishne, Uri Shaham, Alexander Cloninger, Israel Cohen category:stat.ML cs.LG math.CA  published:2015-06-25 summary:Non-linear manifold learning enables high-dimensional data analysis, but requires out-of-sample-extension methods to process new data points. In this paper, we propose a manifold learning algorithm based on deep learning to create an encoder, which maps a high-dimensional dataset and its low-dimensional embedding, and a decoder, which takes the embedded data back to the high-dimensional space. Stacking the encoder and decoder together constructs an autoencoder, which we term a diffusion net, that performs out-of-sample-extension as well as outlier detection. We introduce new neural net constraints for the encoder, which preserves the local geometry of the points, and we prove rates of convergence for the encoder. Also, our approach is efficient in both computational complexity and memory requirements, as opposed to previous methods that require storage of all training points in both the high-dimensional and the low-dimensional spaces to calculate the out-of-sample-extension and the pre-image. version:1
arxiv-1306-5532 | Deep Learning by Scattering | http://arxiv.org/abs/1306.5532 | id:1306.5532 author:Stéphane Mallat, Irène Waldspurger category:cs.LG stat.ML  published:2013-06-24 summary:We introduce general scattering transforms as mathematical models of deep neural networks with l2 pooling. Scattering networks iteratively apply complex valued unitary operators, and the pooling is performed by a complex modulus. An expected scattering defines a contractive representation of a high-dimensional probability distribution, which preserves its mean-square norm. We show that unsupervised learning can be casted as an optimization of the space contraction to preserve the volume occupied by unlabeled examples, at each layer of the network. Supervised learning and classification are performed with an averaged scattering, which provides scattering estimations for multiple classes. version:2
arxiv-1503-01291 | Sparse multi-view matrix factorisation: a multivariate approach to multiple tissue comparisons | http://arxiv.org/abs/1503.01291 | id:1503.01291 author:Zi Wang, Wei Yuan, Giovanni Montana category:stat.ML stat.AP  published:2015-03-04 summary:Gene expression levels in a population vary extensively across tissues. Such heterogeneity is caused by genetic variability and environmental factors, and is expected to be linked to disease development. The abundance of experimental data now enables the identification of features of gene expression profiles that are shared across tissues, and those that are tissue-specific. While most current research is concerned with characterising differential expression by comparing mean expression profiles across tissues, it is also believed that a significant difference in a gene expression's variance across tissues may also be associated to molecular mechanisms that are important for tissue development and function. We propose a sparse multi-view matrix factorisation (sMVMF) algorithm to jointly analyse gene expression measurements in multiple tissues, where each tissue provides a different "view" of the underlying organism. The proposed methodology can be interpreted as an extension of principal component analysis in that it provides the means to decompose the total sample variance in each tissue into the sum of two components: one capturing the variance that is shared across tissues, and one isolating the tissue-specific variances. sMVMF has been used to jointly model mRNA expression profiles in three tissues - adipose, skin and LCL - which are available for a large and well-phenotyped twins cohort, TwinsUK. Using sMVMF, we are able to prioritise genes based on whether their variation patterns are specific to each tissue. Furthermore, using DNA methylation profiles available, we provide supporting evidence that adipose-specific gene expression patterns may be driven by epigenetic effects. version:2
arxiv-1502-02445 | Deep Neural Networks for Anatomical Brain Segmentation | http://arxiv.org/abs/1502.02445 | id:1502.02445 author:Alexandre de Brebisson, Giovanni Montana category:cs.CV cs.LG stat.AP stat.ML  published:2015-02-09 summary:We present a novel approach to automatically segment magnetic resonance (MR) images of the human brain into anatomical regions. Our methodology is based on a deep artificial neural network that assigns each voxel in an MR image of the brain to its corresponding anatomical region. The inputs of the network capture information at different scales around the voxel of interest: 3D and orthogonal 2D intensity patches capture the local spatial context while large, compressed 2D orthogonal patches and distances to the regional centroids enforce global spatial consistency. Contrary to commonly used segmentation methods, our technique does not require any non-linear registration of the MR images. To benchmark our model, we used the dataset provided for the MICCAI 2012 challenge on multi-atlas labelling, which consists of 35 manually segmented MR images of the brain. We obtained competitive results (mean dice coefficient 0.725, error rate 0.163) showing the potential of our approach. To our knowledge, our technique is the first to tackle the anatomical segmentation of the whole brain using deep neural networks. version:2
arxiv-1408-4966 | Diffusion Fingerprints | http://arxiv.org/abs/1408.4966 | id:1408.4966 author:Jimmy Dubuisson, Jean-Pierre Eckmann, Andrea Agazzi category:stat.ML cs.IR cs.LG  published:2014-08-21 summary:We introduce, test and discuss a method for classifying and clustering data modeled as directed graphs. The idea is to start diffusion processes from any subset of a data collection, generating corresponding distributions for reaching points in the network. These distributions take the form of high-dimensional numerical vectors and capture essential topological properties of the original dataset. We show how these diffusion vectors can be successfully applied for getting state-of-the-art accuracies in the problem of extracting pathways from metabolic networks. We also provide a guideline to illustrate how to use our method for classification problems, and discuss important details of its implementation. In particular, we present a simple dimensionality reduction technique that lowers the computational cost of classifying diffusion vectors, while leaving the predictive power of the classification process substantially unaltered. Although the method has very few parameters, the results we obtain show its flexibility and power. This should make it helpful in many other contexts. version:2
arxiv-1506-07732 | How to improve robustness in Kohonen maps and display additional information in Factorial Analysis: application to text mining | http://arxiv.org/abs/1506.07732 | id:1506.07732 author:Nicolas Bourgeois, Marie Cottrell, Benjamin Déruelle, Stéphane Lamassé, Patrick Letrémy category:math.ST cs.CL stat.TH  published:2015-06-25 summary:This article is an extended version of a paper presented in the WSOM'2012 conference [1]. We display a combination of factorial projections, SOM algorithm and graph techniques applied to a text mining problem. The corpus contains 8 medieval manuscripts which were used to teach arithmetic techniques to merchants. Among the techniques for Data Analysis, those used for Lexicometry (such as Factorial Analysis) highlight the discrepancies between manuscripts. The reason for this is that they focus on the deviation from the independence between words and manuscripts. Still, we also want to discover and characterize the common vocabulary among the whole corpus. Using the properties of stochastic Kohonen maps, which define neighborhood between inputs in a non-deterministic way, we highlight the words which seem to play a special role in the vocabulary. We call them fickle and use them to improve both Kohonen map robustness and significance of FCA visualization. Finally we use graph algorithmic to exploit this fickleness for classification of words. version:1
arxiv-1506-07721 | Fairness-Aware Learning with Restriction of Universal Dependency using f-Divergences | http://arxiv.org/abs/1506.07721 | id:1506.07721 author:Kazuto Fukuchi, Jun Sakuma category:stat.ML cs.LG  published:2015-06-25 summary:Fairness-aware learning is a novel framework for classification tasks. Like regular empirical risk minimization (ERM), it aims to learn a classifier with a low error rate, and at the same time, for the predictions of the classifier to be independent of sensitive features, such as gender, religion, race, and ethnicity. Existing methods can achieve low dependencies on given samples, but this is not guaranteed on unseen samples. The existing fairness-aware learning algorithms employ different dependency measures, and each algorithm is specifically designed for a particular one. Such diversity makes it difficult to theoretically analyze and compare them. In this paper, we propose a general framework for fairness-aware learning that uses f-divergences and that covers most of the dependency measures employed in the existing methods. We introduce a way to estimate the f-divergences that allows us to give a unified analysis for the upper bound of the estimation error; this bound is tighter than that of the existing convergence rate analysis of the divergence estimation. With our divergence estimate, we propose a fairness-aware learning algorithm, and perform a theoretical analysis of its generalization error. Our analysis reveals that, under mild assumptions and even with enforcement of fairness, the generalization error of our method is $O(\sqrt{1/n})$, which is the same as that of the regular ERM. In addition, and more importantly, we show that, for any f-divergence, the upper bound of the estimation error of the divergence is $O(\sqrt{1/n})$. This indicates that our fairness-aware learning algorithm guarantees low dependencies on unseen samples for any dependency measure represented by an f-divergence. version:1
arxiv-1310-1495 | Role of normalization in spectral clustering for stochastic blockmodels | http://arxiv.org/abs/1310.1495 | id:1310.1495 author:Purnamrita Sarkar, Peter J. Bickel category:stat.ML  published:2013-10-05 summary:Spectral clustering is a technique that clusters elements using the top few eigenvectors of their (possibly normalized) similarity matrix. The quality of spectral clustering is closely tied to the convergence properties of these principal eigenvectors. This rate of convergence has been shown to be identical for both the normalized and unnormalized variants in recent random matrix theory literature. However, normalization for spectral clustering is commonly believed to be beneficial [Stat. Comput. 17 (2007) 395-416]. Indeed, our experiments show that normalization improves prediction accuracy. In this paper, for the popular stochastic blockmodel, we theoretically show that normalization shrinks the spread of points in a class by a constant fraction under a broad parameter regime. As a byproduct of our work, we also obtain sharp deviation bounds of empirical principal eigenvalues of graphs generated from a stochastic blockmodel. version:2
arxiv-1506-07677 | Manifold Optimization for Gaussian Mixture Models | http://arxiv.org/abs/1506.07677 | id:1506.07677 author:Reshad Hosseini, Suvrit Sra category:stat.ML cs.LG math.OC  published:2015-06-25 summary:We take a new look at parameter estimation for Gaussian Mixture Models (GMMs). In particular, we propose using \emph{Riemannian manifold optimization} as a powerful counterpart to Expectation Maximization (EM). An out-of-the-box invocation of manifold optimization, however, fails spectacularly: it converges to the same solution but vastly slower. Driven by intuition from manifold convexity, we then propose a reparamerization that has remarkable empirical consequences. It makes manifold optimization not only match EM---a highly encouraging result in itself given the poor record nonlinear programming methods have had against EM so far---but also outperform EM in many practical settings, while displaying much less variability in running times. We further highlight the strengths of manifold optimization by developing a somewhat tuned manifold LBFGS method that proves even more competitive and reliable than existing manifold optimization tools. We hope that our results encourage a wider consideration of manifold optimization for parameter estimation problems. version:1
arxiv-1506-07650 | Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling | http://arxiv.org/abs/1506.07650 | id:1506.07650 author:Kun Xu, Yansong Feng, Songfang Huang, Dongyan Zhao category:cs.CL cs.LG  published:2015-06-25 summary:Syntactic features play an essential role in identifying relationship in a sentence. Previous neural network models often suffer from irrelevant information introduced when subjects and objects are in a long distance. In this paper, we propose to learn more robust relation representations from the shortest dependency path through a convolution neural network. We further propose a straightforward negative sampling strategy to improve the assignment of subjects and objects. Experimental results show that our method outperforms the state-of-the-art methods on the SemEval-2010 Task 8 dataset. version:1
arxiv-1410-7050 | A PTAS for Agnostically Learning Halfspaces | http://arxiv.org/abs/1410.7050 | id:1410.7050 author:Amit Daniely category:cs.DS cs.LG  published:2014-10-26 summary:We present a PTAS for agnostically learning halfspaces w.r.t. the uniform distribution on the $d$ dimensional sphere. Namely, we show that for every $\mu>0$ there is an algorithm that runs in time $\mathrm{poly}(d,\frac{1}{\epsilon})$, and is guaranteed to return a classifier with error at most $(1+\mu)\mathrm{opt}+\epsilon$, where $\mathrm{opt}$ is the error of the best halfspace classifier. This improves on Awasthi, Balcan and Long [ABL14] who showed an algorithm with an (unspecified) constant approximation ratio. Our algorithm combines the classical technique of polynomial regression (e.g. [LMN89, KKMS05]), together with the new localization technique of [ABL14]. version:3
arxiv-1506-07613 | Generalized Majorization-Minimization | http://arxiv.org/abs/1506.07613 | id:1506.07613 author:Sobhan Naderi Parizi, Kun He, Stan Sclaroff, Pedro Felzenszwalb category:cs.CV cs.IT cs.LG math.IT stat.ML  published:2015-06-25 summary:Non-convex optimization is ubiquitous in machine learning. The Majorization-Minimization (MM) procedure systematically optimizes non-convex functions through an iterative construction and optimization of upper bounds on the objective function. The bound at each iteration is required to \emph{touch} the objective function at the optimizer of the previous bound. We show that this touching constraint is unnecessary and overly restrictive. We generalize MM by relaxing this constraint, and propose a new framework for designing optimization algorithms, named Generalized Majorization-Minimization (G-MM). Compared to MM, G-MM is much more flexible. For instance, it can incorporate application-specific biases into the optimization procedure without changing the objective function. We derive G-MM algorithms for several latent variable models and show that they consistently outperform their MM counterparts in optimizing non-convex objectives. In particular, G-MM algorithms appear to be less sensitive to initialization. version:1
arxiv-1506-07611 | Joint community and anomaly tracking in dynamic networks | http://arxiv.org/abs/1506.07611 | id:1506.07611 author:Brian Baingana, Georgios B. Giannakis category:stat.ML cs.SI physics.soc-ph  published:2015-06-25 summary:Most real-world networks exhibit community structure, a phenomenon characterized by existence of node clusters whose intra-edge connectivity is stronger than edge connectivities between nodes belonging to different clusters. In addition to facilitating a better understanding of network behavior, community detection finds many practical applications in diverse settings. Communities in online social networks are indicative of shared functional roles, or affiliation to a common socio-economic status, the knowledge of which is vital for targeted advertisement. In buyer-seller networks, community detection facilitates better product recommendations. Unfortunately, reliability of community assignments is hindered by anomalous user behavior often observed as unfair self-promotion, or "fake" highly-connected accounts created to promote fraud. The present paper advocates a novel approach for jointly tracking communities while detecting such anomalous nodes in time-varying networks. By postulating edge creation as the result of mutual community participation by node pairs, a dynamic factor model with anomalous memberships captured through a sparse outlier matrix is put forth. Efficient tracking algorithms suitable for both online and decentralized operation are developed. Experiments conducted on both synthetic and real network time series successfully unveil underlying communities and anomalous nodes. version:1
arxiv-1506-07609 | CRAFT: ClusteR-specific Assorted Feature selecTion | http://arxiv.org/abs/1506.07609 | id:1506.07609 author:Vikas K. Garg, Cynthia Rudin, Tommi Jaakkola category:cs.LG stat.ML  published:2015-06-25 summary:We present a framework for clustering with cluster-specific feature selection. The framework, CRAFT, is derived from asymptotic log posterior formulations of nonparametric MAP-based clustering models. CRAFT handles assorted data, i.e., both numeric and categorical data, and the underlying objective functions are intuitively appealing. The resulting algorithm is simple to implement and scales nicely, requires minimal parameter tuning, obviates the need to specify the number of clusters a priori, and compares favorably with other methods on real datasets. version:1
arxiv-1506-07236 | Incremental RANSAC for Online Relocation in Large Dynamic Environments | http://arxiv.org/abs/1506.07236 | id:1506.07236 author:Kanji Tanaka, Eiji Kondo category:cs.RO cs.CV  published:2015-06-24 summary:Vehicle relocation is the problem in which a mobile robot has to estimate the self-position with respect to an a priori map of landmarks using the perception and the motion measurements without using any knowledge of the initial self-position. Recently, RANdom SAmple Consensus (RANSAC), a robust multi-hypothesis estimator, has been successfully applied to offline relocation in static environments. On the other hand, online relocation in dynamic environments is still a difficult problem, for available computation time is always limited, and for measurement include many outliers. To realize real time algorithm for such an online process, we have developed an incremental version of RANSAC algorithm by extending an efficient preemption RANSAC scheme. This novel scheme named incremental RANSAC is able to find inlier hypotheses of self-positions out of large number of outlier hypotheses contaminated by outlier measurements. version:2
arxiv-1506-00745 | An objective prior that unifies objective Bayes and information-based inference | http://arxiv.org/abs/1506.00745 | id:1506.00745 author:Colin H. LaMont, Paul A. Wiggins category:stat.ML cs.LG physics.data-an  published:2015-06-02 summary:There are three principle paradigms of statistical inference: (i) Bayesian, (ii) information-based and (iii) frequentist inference. We describe an objective prior (the weighting or $w$-prior) which unifies objective Bayes and information-based inference. The $w$-prior is chosen to make the marginal probability an unbiased estimator of the predictive performance of the model. This definition has several other natural interpretations. From the perspective of the information content of the prior, the $w$-prior is both uniformly and maximally uninformative. The $w$-prior can also be understood to result in a uniform density of distinguishable models in parameter space. Finally we demonstrate the the $w$-prior is equivalent to the Akaike Information Criterion (AIC) for regular models in the asymptotic limit. The $w$-prior appears to be generically applicable to statistical inference and is free of {\it ad hoc} regularization. The mechanism for suppressing complexity is analogous to AIC: model complexity reduces model predictivity. We expect this new objective-Bayes approach to inference to be widely-applicable to machine-learning problems including singular models. version:2
arxiv-1506-07597 | Degenerate Motions in Multicamera Cluster SLAM with Non-overlapping Fields of View | http://arxiv.org/abs/1506.07597 | id:1506.07597 author:Michael J. Tribou, David W. L. Wang, Steven L. Waslander category:cs.CV cs.RO  published:2015-06-25 summary:An analysis of the relative motion and point feature model configurations leading to solution degeneracy is presented, for the case of a Simultaneous Localization and Mapping system using multicamera clusters with non-overlapping fields-of-view. The SLAM optimization system seeks to minimize image space reprojection error and is formulated for a cluster containing any number of component cameras, observing any number of point features over two keyframes. The measurement Jacobian is transformed to expose a reduced-dimension representation such that the degeneracy of the system can be determined by the rank of a dense submatrix. A set of relative motions sufficient for degeneracy are identified for certain cluster configurations, independent of target model geometry. Furthermore, it is shown that increasing the number of cameras within the cluster and observing features across different cameras over the two keyframes reduces the size of the degenerate motion sets significantly. version:1
arxiv-1508-04458 | Multiresolution Approach to Acceleration of Iterative Image Reconstruction for X-Ray Imaging for Security Applications | http://arxiv.org/abs/1508.04458 | id:1508.04458 author:S. Degirmenci, Joseph A. O'Sullivan, David G. Politte category:cs.CV  published:2015-06-24 summary:Three-dimensional x-ray CT image reconstruction in baggage scanning in security applications is an important research field. The variety of materials to be reconstructed is broader than medical x-ray imaging. Presence of high attenuating materials such as metal may cause artifacts if analytical reconstruction methods are used. Statistical modeling and the resultant iterative algorithms are known to reduce these artifacts and present good quantitative accuracy in estimates of linear attenuation coefficients. However, iterative algorithms may require computations in order to achieve quantitatively accurate results. For the case of baggage scanning, in order to provide fast accurate inspection throughput, they must be accelerated drastically. There are many approaches proposed in the literature to increase speed of convergence. This paper presents a new method that estimates the wavelet coefficients of the images in the discrete wavelet transform domain instead of the image space itself. Initially, surrogate functions are created around approximation coefficients only. As the iterations proceed, the wavelet tree on which the updates are made is expanded based on a criterion and detail coefficients at each level are updated and the tree is expanded this way. For example, in the smooth regions of the image the detail coefficients are not updated while the coefficients that represent the high-frequency component around edges are being updated, thus saving time by focusing computations where they are needed. This approach is implemented on real data from a SureScan (TM) x1000 Explosive Detection System and compared to straightforward implementation of the unregularized alternating minimization of O'Sullivan and Benac [1]. version:1
arxiv-1506-06155 | CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique Splits | http://arxiv.org/abs/1506.06155 | id:1506.06155 author:Mohammad Norouzi, Maxwell D. Collins, David J. Fleet, Pushmeet Kohli category:cs.LG cs.CV  published:2015-06-19 summary:We propose a novel algorithm for optimizing multivariate linear threshold functions as split functions of decision trees to create improved Random Forest classifiers. Standard tree induction methods resort to sampling and exhaustive search to find good univariate split functions. In contrast, our method computes a linear combination of the features at each node, and optimizes the parameters of the linear combination (oblique) split functions by adopting a variant of latent variable SVM formulation. We develop a convex-concave upper bound on the classification loss for a one-level decision tree, and optimize the bound by stochastic gradient descent at each internal node of the tree. Forests of up to 1000 Continuously Optimized Oblique (CO2) decision trees are created, which significantly outperform Random Forest with univariate splits and previous techniques for constructing oblique trees. Experimental results are reported on multi-class classification benchmarks and on Labeled Faces in the Wild (LFW) dataset. version:2
arxiv-1506-07545 | Learning Representations from Deep Networks Using Mode Synthesizers | http://arxiv.org/abs/1506.07545 | id:1506.07545 author:N. E. Osegi, P. Enyindah category:cs.NE  published:2015-06-24 summary:Deep learning Networks play a crucial role in the evolution of a vast number of current machine learning models for solving a variety of real world non-trivial tasks. Such networks use big data which is generally unlabeled unsupervised and multi-layered requiring no form of supervision for training and learning data and has been used to successfully build automatic supervisory neural networks. However the question still remains how well the learned data represents interestingness, and their effectiveness i.e. efficiency in deep learning models or applications. If the output of a network of deep learning models can be beamed unto a scene of observables, we could learn the variational frequencies of these stacked networks in a parallel and distributive way.This paper seeks to discover and represent interesting patterns in an efficient and less complex way by incorporating the concept of Mode synthesizers in the deep learning process models version:1
arxiv-1506-07540 | Global Optimality in Tensor Factorization, Deep Learning, and Beyond | http://arxiv.org/abs/1506.07540 | id:1506.07540 author:Benjamin D. Haeffele, Rene Vidal category:cs.NA cs.LG stat.ML  published:2015-06-24 summary:Techniques involving factorization are found in a wide range of applications and have enjoyed significant empirical success in many fields. However, common to a vast majority of these problems is the significant disadvantage that the associated optimization problems are typically non-convex due to a multilinear form or other convexity destroying transformation. Here we build on ideas from convex relaxations of matrix factorizations and present a very general framework which allows for the analysis of a wide range of non-convex factorization problems - including matrix factorization, tensor factorization, and deep neural network training formulations. We derive sufficient conditions to guarantee that a local minimum of the non-convex optimization problem is a global minimum and show that if the size of the factorized variables is large enough then from any initialization it is possible to find a global minimizer using a purely local descent algorithm. Our framework also provides a partial theoretical justification for the increasingly common use of Rectified Linear Units (ReLUs) in deep neural networks and offers guidance on deep network architectures and regularization strategies to facilitate efficient optimization. version:1
arxiv-1506-07512 | Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization | http://arxiv.org/abs/1506.07512 | id:1506.07512 author:Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford category:stat.ML cs.DS cs.LG  published:2015-06-24 summary:We develop a family of accelerated stochastic algorithms that minimize sums of convex functions. Our algorithms improve upon the fastest running time for empirical risk minimization (ERM), and in particular linear least-squares regression, across a wide range of problem settings. To achieve this, we establish a framework based on the classical proximal point algorithm. Namely, we provide several algorithms that reduce the minimization of a strongly convex function to approximate minimizations of regularizations of the function. Using these results, we accelerate recent fast stochastic algorithms in a black-box fashion. Empirically, we demonstrate that the resulting algorithms exhibit notions of stability that are advantageous in practice. Both in theory and in practice, the provided algorithms reap the computational benefits of adding a large strongly convex regularization term, without incurring a corresponding bias to the original problem. version:1
arxiv-1506-07504 | Objective Variables for Probabilistic Revenue Maximization in Second-Price Auctions with Reserve | http://arxiv.org/abs/1506.07504 | id:1506.07504 author:Maja R. Rudolph, Joseph G. Ellis, David M. Blei category:stat.ML cs.AI cs.GT cs.LG stat.AP  published:2015-06-24 summary:Many online companies sell advertisement space in second-price auctions with reserve. In this paper, we develop a probabilistic method to learn a profitable strategy to set the reserve price. We use historical auction data with features to fit a predictor of the best reserve price. This problem is delicate - the structure of the auction is such that a reserve price set too high is much worse than a reserve price set too low. To address this we develop objective variables, a new framework for combining probabilistic modeling with optimal decision-making. Objective variables are "hallucinated observations" that transform the revenue maximization task into a regularized maximum likelihood estimation problem, which we solve with an EM algorithm. This framework enables a variety of prediction mechanisms to set the reserve price. As examples, we study objective variable methods with regression, kernelized regression, and neural networks on simulated and real data. Our methods outperform previous approaches both in terms of scalability and profit. version:1
arxiv-1506-07503 | Attention-Based Models for Speech Recognition | http://arxiv.org/abs/1506.07503 | id:1506.07503 author:Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, Yoshua Bengio category:cs.CL cs.LG cs.NE stat.ML  published:2015-06-24 summary:Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level. version:1
arxiv-1506-07477 | Efficient Learning for Undirected Topic Models | http://arxiv.org/abs/1506.07477 | id:1506.07477 author:Jiatao Gu, Victor O. K. Li category:cs.LG cs.CL cs.IR stat.ML  published:2015-06-24 summary:Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents. Traditional learning strategies such as Contrastive Divergence are very inefficient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification. version:1
arxiv-1506-08110 | Nonnegative Matrix Factorization applied to reordered pixels of single images based on patches to achieve structured nonnegative dictionaries | http://arxiv.org/abs/1506.08110 | id:1506.08110 author:Richard M. Charles, Kye M. Taylor, James H. Curry category:cs.CV math.NA 65K02  published:2015-06-24 summary:Recent improvements in computing allow for the processing and analysis of very large datasets in a variety of fields. Often the analysis requires the creation of low-rank approximations to the datasets leading to efficient storage. This article presents and analyzes a novel approach for creating nonnegative, structured dictionaries using NMF applied to reordered pixels of single, natural images. We reorder the pixels based on patches and present our approach in general. We investigate our approach when using the Singular Value Decomposition (SVD) and Nonnegative Matrix Factorizations (NMF) as low-rank approximations. Peak Signal-to-Noise Ratio (PSNR) and Mean Structural Similarity Index (MSSIM) are used to evaluate the algorithm. We report that while the SVD provides the best reconstructions, its dictionary of vectors lose both the sign structure of the original image and details of localized image content. In contrast, the dictionaries produced using NMF preserves the sign structure of the original image matrix and offer a nonnegative, parts-based dictionary. version:1
arxiv-1408-3772 | Highly Accurate Multispectral Palmprint Recognition Using Statistical and Wavelet Features | http://arxiv.org/abs/1408.3772 | id:1408.3772 author:Shervin Minaee, AmirAli Abdolrashidi category:cs.CV  published:2014-08-16 summary:Palmprint is one of the most useful physiological biometrics that can be used as a powerful means in personal recognition systems. The major features of the palmprints are palm lines, wrinkles and ridges, and many approaches use them in different ways towards solving the palmprint recognition problem. Here we have proposed to use a set of statistical and wavelet-based features; statistical to capture the general characteristics of palmprints; and wavelet-based to find those information not evident in the spatial domain. Also we use two different classification approaches, minimum distance classifier scheme and weighted majority voting algorithm, to perform palmprint matching. The proposed method is tested on a well-known palmprint dataset of 6000 samples and has shown an impressive accuracy rate of 99.65\%-100\% for most scenarios. version:2
arxiv-1506-07452 | Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation | http://arxiv.org/abs/1506.07452 | id:1506.07452 author:Marijn F. Stollenga, Wonmin Byeon, Marcus Liwicki, Juergen Schmidhuber category:cs.CV cs.LG  published:2015-06-24 summary:Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelize on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12). version:1
arxiv-1506-07440 | Unshredding of Shredded Documents: Computational Framework and Implementation | http://arxiv.org/abs/1506.07440 | id:1506.07440 author:Lei Kristoffer R. Lactuan, Jaderick P. Pabico category:cs.CV  published:2015-06-24 summary:A shredded document $D$ is a document whose pages have been cut into strips for the purpose of destroying private, confidential, or sensitive information $I$ contained in $D$. Shredding has become a standard means of government organizations, businesses, and private individuals to destroy archival records that have been officially classified for disposal. It can also be used to destroy documentary evidence of wrongdoings by entities who are trying to hide $I$. In this paper, we present an optimal $O((n\times m)^2)$ algorithm $A$ that reconstructs an $n$-page $D$, where each page $p$ is shredded into $m$ strips. We also present the efficacy of $A$ in reconstructing three document types: hand-written, machine typed-set, and images. version:1
arxiv-1506-07439 | Secrets of GrabCut and Kernel K-means | http://arxiv.org/abs/1506.07439 | id:1506.07439 author:Meng Tang, Ismail Ben Ayed, Dmitrii Marin, Yuri Boykov category:cs.CV  published:2015-06-24 summary:The log-likelihood energy term in popular model-fitting segmentation methods, e.g. Zhu-Yuille, Chan-Vese, GrabCut, etc., is presented as a generalized "probabilistic" K-means energy for color space clustering. This interpretation reveals some limitations, e.g. over-fitting. We propose an alternative approach to color clustering using kernel K-means energy with well-known properties such as non-linear separation and scalability to higher-dimensional feature spaces. Similarly to log-likelihoods, our kernel energy term for color space clustering can be combined with image grid regularization, e.g. boundary smoothness, and minimized using (pseudo-) bound optimization and max-flow algorithm. Unlike histogram or GMM fitting and implicit entropy minimization, our approach is closely related to general pairwise clustering such as average association and normalized cut. But, in contrast to previous pairwise clustering algorithms, our approach can incorporate any standard geometric regularization in the image domain. We analyze extreme cases for kernel bandwidth (e.g. Gini bias) and propose adaptive strategies. Our general kernel-based approach opens the door for many extensions/applications. version:1
arxiv-1506-07363 | Salient Object Detection via Objectness Measure | http://arxiv.org/abs/1506.07363 | id:1506.07363 author:Sai Srivatsa R, R. Venkatesh Babu category:cs.CV  published:2015-06-24 summary:Salient object detection has become an important task in many image processing applications. The existing approaches exploit background prior and contrast prior to attain state of the art results. In this paper, instead of using background cues, we estimate the foreground regions in an image using objectness proposals and utilize it to obtain smooth and accurate saliency maps. We propose a novel saliency measure called `foreground connectivity' which determines how tightly a pixel or a region is connected to the estimated foreground. We use the values assigned by this measure as foreground weights and integrate these in an optimization framework to obtain the final saliency maps. We extensively evaluate the proposed approach on two benchmark databases and demonstrate that the results obtained are better than the existing state of the art approaches. version:1
arxiv-1405-5769 | Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT | http://arxiv.org/abs/1405.5769 | id:1405.5769 author:Philipp Fischer, Alexey Dosovitskiy, Thomas Brox category:cs.CV cs.LG I.2.6; I.4.7; I.4.8  published:2014-05-22 summary:Latest results indicate that features learned via convolutional neural networks outperform previous descriptors on classification tasks by a large margin. It has been shown that these networks still work well when they are applied to datasets or recognition tasks different from those they were trained on. However, descriptors like SIFT are not only used in recognition but also for many correspondence problems that rely on descriptor matching. In this paper we compare features from various layers of convolutional neural nets to standard SIFT descriptors. We consider a network that was trained on ImageNet and another one that was trained without supervision. Surprisingly, convolutional neural networks clearly outperform SIFT on descriptor matching. This paper has been merged with arXiv:1406.6909 version:2
arxiv-1506-07271 | Natural Scene Recognition Based on Superpixels and Deep Boltzmann Machines | http://arxiv.org/abs/1506.07271 | id:1506.07271 author:Jinfu Yang, Jingyu Gao, Guanghui Wang, Shanshan Zhang category:cs.CV  published:2015-06-24 summary:The Deep Boltzmann Machines (DBM) is a state-of-the-art unsupervised learning model, which has been successfully applied to handwritten digit recognition and, as well as object recognition. However, the DBM is limited in scene recognition due to the fact that natural scene images are usually very large. In this paper, an efficient scene recognition approach is proposed based on superpixels and the DBMs. First, a simple linear iterative clustering (SLIC) algorithm is employed to generate superpixels of input images, where each superpixel is regarded as an input of a learning model. Then, a two-layer DBM model is constructed by stacking two restricted Boltzmann machines (RBMs), and a greedy layer-wise algorithm is applied to train the DBM model. Finally, a softmax regression is utilized to categorize scene images. The proposed technique can effectively reduce the computational complexity and enhance the performance for large natural image recognition. The approach is verified and evaluated by extensive experiments, including the fifteen-scene categories dataset the UIUC eight-sports dataset, and the SIFT flow dataset, are used to evaluate the proposed method. The experimental results show that the proposed approach outperforms other state-of-the-art methods in terms of recognition rate. version:1
arxiv-1506-07257 | A Novel Feature Extraction Method for Scene Recognition Based on Centered Convolutional Restricted Boltzmann Machines | http://arxiv.org/abs/1506.07257 | id:1506.07257 author:Jingyu Gao, Jinfu Yang, Guanghui Wang, Mingai Li category:cs.CV  published:2015-06-24 summary:Scene recognition is an important research topic in computer vision, while feature extraction is a key step of object recognition. Although classical Restricted Boltzmann machines (RBM) can efficiently represent complicated data, it is hard to handle large images due to its complexity in computation. In this paper, a novel feature extraction method, named Centered Convolutional Restricted Boltzmann Machines (CCRBM), is proposed for scene recognition. The proposed model is an improved Convolutional Restricted Boltzmann Machines (CRBM) by introducing centered factors in its learning strategy to reduce the source of instabilities. First, the visible units of the network are redefined using centered factors. Then, the hidden units are learned with a modified energy function by utilizing a distribution function, and the visible units are reconstructed using the learned hidden units. In order to achieve better generative ability, the Centered Convolutional Deep Belief Networks (CCDBN) is trained in a greedy layer-wise way. Finally, a softmax regression is incorporated for scene recognition. Extensive experimental evaluations using natural scenes, MIT-indoor scenes, and Caltech 101 datasets show that the proposed approach performs better than other counterparts in terms of stability, generalization, and discrimination. The CCDBN model is more suitable for natural scene image recognition by virtue of convolutional property. version:1
arxiv-1506-07254 | Unconfused ultraconservative multiclass algorithms | http://arxiv.org/abs/1506.07254 | id:1506.07254 author:Ugo Louche, Liva Ralaivola category:cs.LG  published:2015-06-24 summary:We tackle the problem of learning linear classifiers from noisy datasets in a multiclass setting. The two-class version of this problem was studied a few years ago where the proposed approaches to combat the noise revolve around a Per-ceptron learning scheme fed with peculiar examples computed through a weighted average of points from the noisy training set. We propose to build upon these approaches and we introduce a new algorithm called UMA (for Unconfused Multiclass additive Algorithm) which may be seen as a generalization to the multiclass setting of the previous approaches. In order to characterize the noise we use the confusion matrix as a multiclass extension of the classification noise studied in the aforemen-tioned literature. Theoretically well-founded, UMA furthermore displays very good empirical noise robustness, as evidenced by numerical simulations conducted on both synthetic and real data. version:1
arxiv-1506-07251 | Benchmark of structured machine learning methods for microbial identification from mass-spectrometry data | http://arxiv.org/abs/1506.07251 | id:1506.07251 author:Kévin Vervier, Pierre Mahé, Jean-Baptiste Veyrieras, Jean-Philippe Vert category:stat.ML cs.LG q-bio.QM  published:2015-06-24 summary:Microbial identification is a central issue in microbiology, in particular in the fields of infectious diseases diagnosis and industrial quality control. The concept of species is tightly linked to the concept of biological and clinical classification where the proximity between species is generally measured in terms of evolutionary distances and/or clinical phenotypes. Surprisingly, the information provided by this well-known hierarchical structure is rarely used by machine learning-based automatic microbial identification systems. Structured machine learning methods were recently proposed for taking into account the structure embedded in a hierarchy and using it as additional a priori information, and could therefore allow to improve microbial identification systems. We test and compare several state-of-the-art machine learning methods for microbial identification on a new Matrix-Assisted Laser Desorption/Ionization Time-of-Flight mass spectrometry (MALDI-TOF MS) dataset. We include in the benchmark standard and structured methods, that leverage the knowledge of the underlying hierarchical structure in the learning process. Our results show that although some methods perform better than others, structured methods do not consistently perform better than their "flat" counterparts. We postulate that this is partly due to the fact that standard methods already reach a high level of accuracy in this context, and that they mainly confuse species close to each other in the tree, a case where using the known hierarchy is not helpful. version:1
arxiv-1506-07224 | Deep CNN Ensemble with Data Augmentation for Object Detection | http://arxiv.org/abs/1506.07224 | id:1506.07224 author:Jian Guo, Stephen Gould category:cs.CV  published:2015-06-24 summary:We report on the methods used in our recent DeepEnsembleCoco submission to the PASCAL VOC 2012 challenge, which achieves state-of-the-art performance on the object detection task. Our method is a variant of the R-CNN model proposed Girshick:CVPR14 with two key improvements to training and evaluation. First, our method constructs an ensemble of deep CNN models with different architectures that are complementary to each other. Second, we augment the PASCAL VOC training set with images from the Microsoft COCO dataset to significantly enlarge the amount training data. Importantly, we select a subset of the Microsoft COCO images to be consistent with the PASCAL VOC task. Results on the PASCAL VOC evaluation server show that our proposed method outperform all previous methods on the PASCAL VOC 2012 detection task at time of submission. version:1
arxiv-1506-07220 | Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks | http://arxiv.org/abs/1506.07220 | id:1506.07220 author:Yangtuo Peng, Hui Jiang category:cs.CE cs.AI cs.CL  published:2015-06-24 summary:Financial news contains useful information on public companies and the market. In this paper we apply the popular word embedding methods and deep neural networks to leverage financial news to predict stock price movements in the market. Experimental results have shown that our proposed methods are simple but very effective, which can significantly improve the stock prediction accuracy on a standard financial database over the baseline system using only the historical price information. version:1
arxiv-1506-06863 | deltaBLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets | http://arxiv.org/abs/1506.06863 | id:1506.06863 author:Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao, Bill Dolan category:cs.CL  published:2015-06-23 summary:We introduce Discriminative BLEU (deltaBLEU), a novel metric for intrinsic evaluation of generated text in tasks that admit a diverse range of possible outputs. Reference strings are scored for quality by human raters on a scale of [-1, +1] to weight multi-reference BLEU. In tasks involving generation of conversational responses, deltaBLEU correlates reasonably with human judgments and outperforms sentence-level and IBM BLEU in terms of both Spearman's rho and Kendall's tau. version:2
arxiv-1503-02357 | Context-Dependent Translation Selection Using Convolutional Neural Network | http://arxiv.org/abs/1503.02357 | id:1503.02357 author:Zhaopeng Tu, Baotian Hu, Zhengdong Lu, Hang Li category:cs.CL cs.LG cs.NE  published:2015-03-09 summary:We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrase and sentence level context by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points. version:2
arxiv-1506-07212 | On Elicitation Complexity and Conditional Elicitation | http://arxiv.org/abs/1506.07212 | id:1506.07212 author:Rafael Frongillo, Ian A. Kash category:cs.LG math.OC math.ST q-fin.MF stat.TH  published:2015-06-23 summary:Elicitation is the study of statistics or properties which are computable via empirical risk minimization. While several recent papers have approached the general question of which properties are elicitable, we suggest that this is the wrong question---all properties are elicitable by first eliciting the entire distribution or data set, and thus the important question is how elicitable. Specifically, what is the minimum number of regression parameters needed to compute the property? Building on previous work, we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation. We establish several general results and techniques for proving upper and lower bounds on elicitation complexity. These results provide tight bounds for eliciting the Bayes risk of any loss, a large class of properties which includes spectral risk measures and several new properties of interest. Finally, we extend our calculus to conditionally elicitable properties, which are elicitable conditioned on knowing the value of another property, giving a necessary condition for the elicitability of both properties together. version:1
arxiv-1308-6315 | Clustering, Classification, Discriminant Analysis, and Dimension Reduction via Generalized Hyperbolic Mixtures | http://arxiv.org/abs/1308.6315 | id:1308.6315 author:Katherine Morris, Paul D. McNicholas category:stat.ME stat.CO stat.ML  published:2013-08-28 summary:A method for dimension reduction with clustering, classification, or discriminant analysis is introduced. This mixture model-based approach is based on fitting generalized hyperbolic mixtures on a reduced subspace within the paradigm of model-based clustering, classification, or discriminant analysis. A reduced subspace of the data is derived by considering the extent to which group means and group covariances vary. The members of the subspace arise through linear combinations of the original data, and are ordered by importance via the associated eigenvalues. The observations can be projected onto the subspace, resulting in a set of variables that captures most of the clustering information available. The use of generalized hyperbolic mixtures gives a robust framework capable of dealing with skewed clusters. Although dimension reduction is increasingly in demand across many application areas, the authors are most familiar with biological applications and so two of the five real data examples are within that sphere. Simulated data are also used for illustration. The approach introduced herein can be considered the most general such approach available, and so we compare results to three special and limiting cases. Comparisons with several well established techniques illustrate its promising performance. version:3
arxiv-1506-07190 | Multi-domain Dialog State Tracking using Recurrent Neural Networks | http://arxiv.org/abs/1506.07190 | id:1506.07190 author:Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gašić, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, Steve Young category:cs.CL cs.LG  published:2015-06-23 summary:Dialog state tracking is a key component of many modern dialog systems, most of which are designed with a single, well-defined domain in mind. This paper shows that dialog data drawn from different dialog domains can be used to train a general belief tracking model which can operate across all of these domains, exhibiting superior performance to each of the domain-specific models. We propose a training procedure which uses out-of-domain data to initialise belief tracking models for entirely new domains. This procedure leads to improvements in belief tracking performance regardless of the amount of in-domain data available for training the model. version:1
arxiv-1506-07136 | Segmentation of Three-dimensional Images with Parametric Active Surfaces and Topology Changes | http://arxiv.org/abs/1506.07136 | id:1506.07136 author:Heike Benninghoff, Harald Garcke category:cs.CV  published:2015-06-23 summary:In this paper, we introduce a novel parametric method for segmentation of three-dimensional images. We consider a piecewise constant version of the Mumford-Shah and the Chan-Vese functionals and perform a region-based segmentation of 3D image data. An evolution law is derived from energy minimization problems which push the surfaces to the boundaries of 3D objects in the image. We propose a parametric scheme which describes the evolution of parametric surfaces. An efficient finite element scheme is proposed for a numerical approximation of the evolution equations. Since standard parametric methods cannot handle topology changes automatically, an efficient method is presented to detect, identify and perform changes in the topology of the surfaces. One main focus of this paper are the algorithmic details to handle topology changes like splitting and merging of surfaces and change of the genus of a surface. Different artificial images are studied to demonstrate the ability to detect the different types of topology changes. Finally, the parametric method is applied to segmentation of medical 3D images. version:1
arxiv-1506-08615 | Coercive functions from a topological viewpoint and properties of minimizing sets of convex functions appearing in image restoration | http://arxiv.org/abs/1506.08615 | id:1506.08615 author:René Ciak category:math.OC cs.CV math.CA math.FA  published:2015-06-23 summary:Many tasks in image processing can be tackled by modeling an appropriate data fidelity term $\Phi: \mathbb{R}^n \rightarrow \mathbb{R} \cup \{+\infty\}$ and then solve one of the regularized minimization problems \begin{align*} &{}(P_{1,\tau}) \qquad \mathop{\rm argmin}_{x \in \mathbb R^n} \big\{ \Phi(x) \;{\rm s.t.}\; \Psi(x) \leq \tau \big\} \\ &{}(P_{2,\lambda}) \qquad \mathop{\rm argmin}_{x \in \mathbb R^n} \{ \Phi(x) + \lambda \Psi(x) \}, \; \lambda > 0 \end{align*} with some function $\Psi: \mathbb{R}^n \rightarrow \mathbb{R} \cup \{+\infty\}$ and a good choice of the parameter(s). Two tasks arise naturally here: \begin{align*} {}& \text{1. Study the solver sets ${\rm SOL}(P_{1,\tau})$ and ${\rm SOL}(P_{2,\lambda})$ of the minimization problems.} \\ {}& \text{2. Ensure that the minimization problems have solutions.} \end{align*} This thesis provides contributions to both tasks: Regarding the first task for a more special setting we prove that there are intervals $(0,c)$ and $(0,d)$ such that the setvalued curves \begin{align*} \tau \mapsto {}& {\rm SOL}(P_{1,\tau}), \; \tau \in (0,c) \\ {} \lambda \mapsto {}& {\rm SOL}(P_{2,\lambda}), \; \lambda \in (0,d) \end{align*} are the same, besides an order reversing parameter change $g: (0,c) \rightarrow (0,d)$. Moreover we show that the solver sets are changing all the time while $\tau$ runs from $0$ to $c$ and $\lambda$ runs from $d$ to $0$. In the presence of lower semicontinuity the second task is done if we have additionally coercivity. We regard lower semicontinuity and coercivity from a topological point of view and develop a new technique for proving lower semicontinuity plus coercivity. Dropping any lower semicontinuity assumption we also prove a theorem on the coercivity of a sum of functions. version:1
arxiv-1506-07062 | Improving Fiber Alignment in HARDI by Combining Contextual PDE Flow with Constrained Spherical Deconvolution | http://arxiv.org/abs/1506.07062 | id:1506.07062 author:J. M. Portegies, R. H. J. Fick, G. R. Sanguinetti, S. P. L. Meesters, G. Girard, R. Duits category:cs.CV  published:2015-06-23 summary:We propose two strategies to improve the quality of tractography results computed from diffusion weighted magnetic resonance imaging (DW-MRI) data. Both methods are based on the same PDE framework, defined in the coupled space of positions and orientations, associated with a stochastic process describing the enhancement of elongated structures while preserving crossing structures. In the first method we use the enhancement PDE for contextual regularization of a fiber orientation distribution (FOD) that is obtained on individual voxels from high angular resolution diffusion imaging (HARDI) data via constrained spherical deconvolution (CSD). Thereby we improve the FOD as input for subsequent tractography. Secondly, we introduce the fiber to bundle coherence (FBC), a measure for quantification of fiber alignment. The FBC is computed from a tractography result using the same PDE framework and provides a criterion for removing the spurious fibers. We validate the proposed combination of CSD and enhancement on phantom data and on human data, acquired with different scanning protocols. On the phantom data we find that PDE enhancements improve both local metrics and global metrics of tractography results, compared to CSD without enhancements. On the human data we show that the enhancements allow for a better reconstruction of crossing fiber bundles and they reduce the variability of the tractography output with respect to the acquisition parameters. Finally, we show that both the enhancement of the FODs and the use of the FBC measure on the tractography improve the stability with respect to different stochastic realizations of probabilistic tractography. This is shown in a clinical application: the reconstruction of the optic radiation for epilepsy surgery planning. version:1
