arxiv-1609-05001 | Stamp processing with examplar features | http://arxiv.org/abs/1609.05001 | id:1609.05001 author:Yash Bhalgat, Mandar Kulkarni, Shirish Karande, Sachin Lodha category:cs.CV  published:2016-09-16 summary:Document digitization is becoming increasingly crucial. In this work, we propose a shape based approach for automatic stamp verification/detection in document images using an unsupervised feature learning. Given a small set of training images, our algorithm learns an appropriate shape representation using an unsupervised clustering. Experimental results demonstrate the effectiveness of our framework in challenging scenarios. version:1
arxiv-1609-04994 | Exploration Potential | http://arxiv.org/abs/1609.04994 | id:1609.04994 author:Jan Leike category:cs.LG cs.AI  published:2016-09-16 summary:We introduce exploration potential, a quantity for that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation. version:1
arxiv-1609-04468 | Sampling Generative Networks: Notes on a Few Effective Techniques | http://arxiv.org/abs/1609.04468 | id:1609.04468 author:Tom White category:cs.NE cs.LG stat.ML  published:2016-09-14 summary:We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model's prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks. version:2
arxiv-1609-04938 | What You Get Is What You See: A Visual Markup Decompiler | http://arxiv.org/abs/1609.04938 | id:1609.04938 author:Yuntian Deng, Anssi Kanervisto, Alexander M. Rush category:cs.CV cs.CL cs.LG cs.NE  published:2016-09-16 summary:Building on recent advances in image caption generation and optical character recognition (OCR), we present a general-purpose, deep learning-based system to decompile an image into presentational markup. While this task is a well-studied problem in OCR, our method takes an inherently different, data-driven approach. Our model does not require any knowledge of the underlying markup language, and is simply trained end-to-end on real-world example data. The model employs a convolutional network for text and layout recognition in tandem with an attention-based neural machine translation system. To train and evaluate the model, we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup, as well as a synthetic dataset of web pages paired with HTML snippets. Experimental results show that the system is surprisingly effective at generating accurate markup for both datasets. While a standard domain-specific LaTeX OCR system achieves around 25% accuracy, our model reproduces the exact rendered image on 75% of examples. version:1
arxiv-1609-04904 | Long-Term Trends in the Public Perception of Artificial Intelligence | http://arxiv.org/abs/1609.04904 | id:1609.04904 author:Ethan Fast, Eric Horvitz category:cs.CL cs.AI cs.CY  published:2016-09-16 summary:Analyses of text corpora over time can reveal trends in beliefs, interest, and sentiment about a topic. We focus on views expressed about artificial intelligence (AI) in the New York Times over a 30-year period. General interest, awareness, and discussion about AI has waxed and waned since the field was founded in 1956. We present a set of measures that captures levels of engagement, measures of pessimism and optimism, the prevalence of specific hopes and concerns, and topics that are linked to discussions about AI over decades. We find that discussion of AI has increased sharply since 2009, and that these discussions have been consistently more optimistic than pessimistic. However, when we examine specific concerns, we find that worries of loss of control of AI, ethical concerns for AI, and the negative impact of AI on work have grown in recent years. We also find that hopes for AI in healthcare and education have also increased over time. version:1
arxiv-1609-03219 | Sharing Hash Codes for Multiple Purposes | http://arxiv.org/abs/1609.03219 | id:1609.03219 author:Wikor Pronobis, Danny Panknin, Johannes Kirschnick, Vignesh Srinivasan, Wojciech Samek, Volker Markl, Manohar Kaul, Klaus-Robert Mueller, Shinichi Nakajima category:stat.ML cs.LG  published:2016-09-11 summary:Locality sensitive hashing (LSH) is a powerful tool for sublinear-time approximate nearest neighbor search, and a variety of hashing schemes have been proposed for different similarity measures. However, hash codes significantly depend on the similarity, which prohibits users from adjusting the similarity at query time. In this paper, we propose multiple purpose LSH (mp-LSH) which shares the hash codes for different similarities. By using vector/code augmentation and cover tree techniques, our mp-LSH supports L2, cosine, and inner product similarities, and their corresponding weighted sums, where the weights can be adjusted at query time. It also allows us to modify the importance of pre-defined groups of features. Thus, mp-LSH enables us, for example, to retrieve similar items to a query with the user preference taken into account, to find a similar material to a query with some properties (stability, utility, etc.) optimized, and to turn on or off a part of multi-modal information (brightness, color, audio, text, etc.) in image/video retrieval. We theoretically and empirically analyze the performance of three variants of mp-LSH, and demonstrate their usefulness on several real-world data sets. version:2
arxiv-1609-04873 | Distant Supervision for Relation Extraction beyond the Sentence Boundary | http://arxiv.org/abs/1609.04873 | id:1609.04873 author:Chris Quirk, Hoifung Poon category:cs.CL  published:2016-09-15 summary:The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross- sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach. version:1
arxiv-1609-05204 | Scaling up Echo-State Networks with multiple light scattering | http://arxiv.org/abs/1609.05204 | id:1609.05204 author:Jonathan Dong, Sylvain Gigan, Florent Krzakala, Gilles Wainrib category:cs.ET cs.LG physics.optics  published:2016-09-15 summary:Echo-State Networks and Reservoir Computing have been studied for more than a decade. As they provide an elegant yet powerful alternative to traditional computing, researchers have tried to implement them using physical systems, in particular non-linear optical elements, achieving high bandwidth and low power consumption. Here we present a completely different optical implementation of Echo-State Networks using light-scattering materials. As a proof of concept, binary networks have been successfully trained to perform non-linear operations on time series and memory of such networks has been evaluated. This new method is fast, power efficient and easily scalable to very large networks. version:1
arxiv-1609-04861 | Visual Stability Prediction and Its Application to Manipulation | http://arxiv.org/abs/1609.04861 | id:1609.04861 author:Wenbin Li, Mario Fritz, Ale≈° Leonardis category:cs.CV cs.RO  published:2016-09-15 summary:Understanding physical phenomena is a key competence that enables humans and animals to act and interact under uncertain perception in previously unseen environments containing novel objects and their configurations. Developmental psychology has shown that such skills are acquired by infants from observations at a very early stage. In this paper, we contrast a more traditional approach of taking a model-based route with explicit 3D representations and physical simulation by an {\em end-to-end} approach that directly predicts stability from appearance. We ask the question if and to what extent and quality such a skill can directly be acquired in a data-driven way---bypassing the need for an explicit simulation at run-time. We present a learning-based approach based on simulated data that predicts stability of towers comprised of wooden blocks under different conditions and quantities related to the potential fall of the towers. We first evaluate the approach on synthetic data and compared the results to human judgments on the same stimuli. Further, we extend this approach to reason about future states of such towers that in turn enables successful stacking. version:1
arxiv-1609-04855 | A Glimpse Far into the Future: Understanding Long-term Crowd Worker Accuracy | http://arxiv.org/abs/1609.04855 | id:1609.04855 author:Kenji Hata, Ranjay Krishna, Li Fei-Fei, Michael S. Bernstein category:cs.HC cs.CV H.5.m  published:2016-09-15 summary:Microtask crowdsourcing is increasingly critical to the creation of extremely large datasets. As a result, crowd workers spend weeks or months repeating the exact same tasks --- making it necessary to understand their behavior over these long periods of time. We utilize three large, longitudinal datasets of nine million annotations collected from Amazon Mechanical Turk to examine claims that workers fatigue or satisfice over these long periods, producing lower quality work. We find that, contrary to these claims, workers are extremely stable in their accuracy over the entire period. To understand whether workers set their accuracy based on the task's requirements for acceptance, we then perform an experiment where we vary the required accuracy for a large crowdsourcing task. Workers did not adjust their accuracy based on the acceptance threshold: workers who were above the threshold continued working at their usual quality level, and workers below the threshold self-selected themselves out of the task. Capitalizing on this consistency, we demonstrate that it is possible to predict workers' long-term accuracy using just a glimpse of their performance on the first five tasks. version:1
arxiv-1609-04846 | A Tutorial about Random Neural Networks in Supervised Learning | http://arxiv.org/abs/1609.04846 | id:1609.04846 author:Sebasti√°n Basterrech, Gerardo Rubino category:cs.NE  published:2016-09-15 summary:Random Neural Networks (RNNs) are a class of Neural Networks (NNs) that can also be seen as a specific type of queuing network. They have been successfully used in several domains during the last 25 years, as queuing networks to analyze the performance of resource sharing in many engineering areas, as learning tools and in combinatorial optimization, where they are seen as neural systems, and also as models of neurological aspects of living beings. In this article we focus on their learning capabilities, and more specifically, we present a practical guide for using the RNN to solve supervised learning problems. We give a general description of these models using almost indistinctly the terminology of Queuing Theory and the neural one. We present the standard learning procedures used by RNNs, adapted from similar well-established improvements in the standard NN field. We describe in particular a set of learning algorithms covering techniques based on the use of first order and, then, of second order derivatives. We also discuss some issues related to these objects and present new perspectives about their use in supervised learning problems. The tutorial describes their most relevant applications, and also provides a large bibliography. version:1
arxiv-1609-04836 | On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima | http://arxiv.org/abs/1609.04836 | id:1609.04836 author:Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang category:cs.LG math.OC  published:2016-09-15 summary:The stochastic gradient descent method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, usually $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. There have been some attempts to investigate the cause for this generalization drop in the large-batch regime, however the precise answer for this phenomenon is, hitherto unknown. In this paper, we present ample numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions -- and that sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We also discuss several empirical strategies that help large-batch methods eliminate the generalization gap and conclude with a set of future research ideas and open questions. version:1
arxiv-1609-04830 | Visible Light-Based Human Visual System Conceptual Model | http://arxiv.org/abs/1609.04830 | id:1609.04830 author:Lee Prangnell category:cs.CV  published:2016-09-15 summary:There is a widely held belief in the digital image and video processing community, which is as follows: the Human Visual System (HVS) is more sensitive to luminance (often confused with brightness) than photon energies (often confused with chromaticity and chrominance). Passages similar to the following occur with high frequency in the peer reviewed literature and academic text books: "the HVS is much more sensitive to brightness than colour" or "the HVS is much more sensitive to luma than chroma". In this discussion paper, a Visible Light-Based Human Visual System (VL-HVS) conceptual model is discussed. The objectives of VL-HVS are as follows: 1. To facilitate a deeper theoretical reflection of the fundamental relationship between visible light, the manifestation of colour perception derived from visible light and the physiology of the perception of colour. That is, in terms of the physics of visible light, photobiology and the human subjective interpretation of visible light, it is appropriate to provide comprehensive background information in relation to the natural interactions between visible light, the retinal photoreceptors and the subsequent cortical processing of such. 2. To provide a more wholesome account with respect to colour information in digital image and video processing applications. 3. To recontextualise colour data in the RGB and YCbCr colour spaces, such that novel techniques in digital image and video processing, including quantisation and artifact reduction techniques, may be developed based on both luma and chroma information (not luma data only). version:1
arxiv-1609-04789 | Coherence Pursuit: Fast, Simple, and Robust Principal Component Analysis | http://arxiv.org/abs/1609.04789 | id:1609.04789 author:Mostafa Rahmani, George Atia category:cs.LG stat.ML  published:2016-09-15 summary:This paper presents a remarkably simple, yet powerful, algorithm for robust Principal Component Analysis (PCA). In the proposed approach, an outlier is set apart from an inlier by comparing their coherence with the rest of the data points. As inliers lie on a low dimensional subspace, they are likely to have strong mutual coherence provided there are enough inliers. By contrast, outliers do not typically admit low dimensional structures, wherefore an outlier is unlikely to bear strong resemblance with a large number of data points. The mutual coherences are computed by forming the Gram matrix of normalized data points. Subsequently, the subspace is recovered from the span of a small subset of the data points that exhibit strong coherence with the rest of the data. As coherence pursuit only involves one simple matrix multiplication, it is significantly faster than the state of-the-art robust PCA algorithms. We provide a mathematical analysis of the proposed algorithm under a random model for the distribution of the inliers and outliers. It is shown that the proposed method can recover the correct subspace even if the data is predominantly outliers. To the best of our knowledge, this is the first provable robust PCA algorithm that is simultaneously non-iterative, can tolerate a large number of outliers and is robust to linearly dependent outliers. version:1
arxiv-1609-04779 | Characterizing the Language of Online Communities and its Relation to Community Reception | http://arxiv.org/abs/1609.04779 | id:1609.04779 author:Trang Tran, Mari Ostendorf category:cs.CL  published:2016-09-15 summary:This work investigates style and topic aspects of language in online communities: looking at both utility as an identifier of the community and correlation with community reception of content. Style is characterized using a hybrid word and part-of-speech tag n-gram language model, while topic is represented using Latent Dirichlet Allocation. Experiments with several Reddit forums show that style is a better indicator of community identity than topic, even for communities organized around specific topics. Further, there is a positive correlation between the community reception to a contribution and the style similarity to that community, but not so for topic similarity. version:1
arxiv-1609-04767 | Transport-based analysis, modeling, and learning from signal and data distributions | http://arxiv.org/abs/1609.04767 | id:1609.04767 author:Soheil Kolouri, Serim Park, Matthew Thorpe, DejanSlepƒçev, Gustavo K. Rohde category:cs.CV  published:2016-09-15 summary:Transport-based techniques for signal and data analysis have received increased attention recently. Given their abilities to provide accurate generative models for signal intensities and other data distributions, they have been used in a variety of applications including content-based retrieval, cancer detection, image super-resolution, and statistical machine learning, to name a few, and shown to produce state of the art in several applications. Moreover, the geometric characteristics of transport-related metrics have inspired new kinds of algorithms for interpreting the meaning of data distributions. Here we provide an overview of the mathematical underpinnings of mass transport-related methods, including numerical implementation, as well as a review, with demonstrations, of several applications. version:1
arxiv-1609-04757 | Perceptual Quality Prediction on Authentically Distorted Images Using a Bag of Features Approach | http://arxiv.org/abs/1609.04757 | id:1609.04757 author:Deepti Ghadiyaram, Alan C. Bovik category:cs.CV  published:2016-09-15 summary:Current top-performing blind perceptual image quality prediction models are generally trained on legacy databases of human quality opinion scores on synthetically distorted images. Therefore they learn image features that effectively predict human visual quality judgments of inauthentic, and usually isolated (single) distortions. However, real-world images usually contain complex, composite mixtures of multiple distortions. We study the perceptually relevant natural scene statistics of such authentically distorted images, in different color spaces and transform domains. We propose a bag of feature-maps approach which avoids assumptions about the type of distortion(s) contained in an image, focusing instead on capturing consistencies, or departures therefrom, of the statistics of real world images. Using a large database of authentically distorted images, human opinions of them, and bags of features computed on them, we train a regressor to conduct image quality prediction. We demonstrate the competence of the features towards improving automatic perceptual quality prediction by testing a learned algorithm using them on a benchmark legacy database as well as on a newly introduced distortion-realistic resource called the LIVE In the Wild Image Quality Challenge Database. We extensively evaluate the perceptual quality prediction model and algorithm and show that it is able to achieve good quality prediction power that is better than other leading models. version:1
arxiv-1609-04747 | An overview of gradient descent optimization algorithms | http://arxiv.org/abs/1609.04747 | id:1609.04747 author:Sebastian Ruder category:cs.LG  published:2016-09-15 summary:Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent. version:1
arxiv-1609-04746 | On Unbounded Delays in Asynchronous Parallel Fixed-Point Algorithms | http://arxiv.org/abs/1609.04746 | id:1609.04746 author:Robert Hannah, Wotao Yin category:math.OC cs.DC cs.LG  published:2016-09-15 summary:The need for scalable numerical solutions has motivated the development of asynchronous parallel algorithms, where a set of nodes run in parallel with little or no synchronization, thus computing with delayed information. This paper studies the convergence of the asynchronous parallel algorithm ARock under potentially unbounded delays. ARock is a general asynchronous algorithm that has many applications. It parallelizes fixed-point iterations by letting a set of nodes randomly choose solution coordinates and update them in an asynchronous parallel fashion. ARock takes some recent asynchronous coordinate descent algorithms as special cases and gives rise to new asynchronous operator-splitting algorithms. Existing analysis of ARock assumes the delays to be bounded, and uses this bound to set a step size that is important to both convergence and efficiency. Other work, though allowing unbounded delays, imposes strict conditions on the underlying fixed-point operator, resulting in limited applications. In this paper, convergence is established under unbounded delays, which can be either stochastic or deterministic. The proposed step sizes are more practical and generally larger than those in the existing work. The step size adapts to the delay distribution or the current delay being experienced in the system. New Lyapunov functions, which are the key to analyzing asynchronous algorithms, are generated to obtain our results. A set of applicable optimization algorithms with large-scale applications are given, including machine learning and scientific computing algorithms. version:1
arxiv-1609-04721 | Mixture model modal clustering | http://arxiv.org/abs/1609.04721 | id:1609.04721 author:Jos√© E. Chac√≥n category:stat.ML stat.CO  published:2016-09-15 summary:The two most extended density-based approaches to clustering are surely mixture model clustering and modal clustering. In the mixture model approach, the density is represented as a mixture and clusters are associated to the different mixture components. In modal clustering, clusters are understood as regions of high density separated from each other by zones of lower density, so that they are closely related to certain regions around the density modes. If the true density is indeed in the assumed class of mixture densities, then mixture model clustering allows to scrutinize more subtle situations than modal clustering. However, when mixture modeling is used in a nonparametric way, taking advantage of the denseness of the sieve of mixture densities to approximate any density, then the correspondence between clusters and mixture components may become questionable. In this paper we introduce two methods to adopt a modal clustering point of view after a mixture model fit. Numerous examples are provided to illustrate that mixture modeling can also be used for clustering in a nonparametric sense, as long as clusters are understood as the domains of attraction of the density modes. version:1
arxiv-1609-04705 | Improving the Accuracy of Stereo Visual Odometry Using Visual Illumination Estimation | http://arxiv.org/abs/1609.04705 | id:1609.04705 author:Lee Clement, Valentin Peretroukhin, Jonathan Kelly category:cs.RO cs.CV  published:2016-09-15 summary:In the absence of reliable and accurate GPS, visual odometry (VO) has emerged as an effective means of estimating the egomotion of robotic vehicles. Like any dead-reckoning technique, VO suffers from unbounded accumulation of drift error over time, but this accumulation can be limited by incorporating absolute orientation information from, for example, a sun sensor. In this paper, we leverage recent work on visual outdoor illumination estimation to show that estimation error in a stereo VO pipeline can be reduced by inferring the sun position from the same image stream used to compute VO, thereby gaining the benefits of sun sensing without requiring a dedicated sun sensor or the sun to be visible to the camera. We compare sun estimation methods based on hand-crafted visual cues and Convolutional Neural Networks (CNNs) and demonstrate our approach on a combined 7.8 km of urban driving from the popular KITTI dataset, achieving up to a 43% reduction in translational average root mean squared error (ARMSE) and a 59% reduction in final translational drift error compared to pure VO alone. version:1
arxiv-1609-04699 | Learning Schizophrenia Imaging Genetics Data Via Multiple Kernel Canonical Correlation Analysis | http://arxiv.org/abs/1609.04699 | id:1609.04699 author:Owen Richfield, Md. Ashad Alam, Vince Calhoun, Yu-Ping Wang category:q-bio.QM stat.ML  published:2016-09-15 summary:Kernel and Multiple Kernel Canonical Correlation Analysis (CCA) are employed to classify schizophrenic and healthy patients based on their SNPs, DNA Methylation and fMRI data. Kernel and Multiple Kernel CCA are popular methods for finding nonlinear correlations between high-dimensional datasets. Data was gathered from 183 patients, 79 with schizophrenia and 104 healthy controls. Kernel and Multiple Kernel CCA represent new avenues for studying schizophrenia, because, to our knowledge, these methods have not been used on these data before. Classification is performed via k-means clustering on the kernel matrix outputs of the Kernel and Multiple Kernel CCA algorithm. Accuracies of the Kernel and Multiple Kernel CCA classification are compared to that of the regularized linear CCA algorithm classification, and are found to be significantly more accurate. Both algorithms demonstrate maximal accuracies when the combination of DNA methylation and fMRI data are used, and experience lower accuracies when the SNP data are incorporated. version:1
arxiv-1609-04382 | Warped Convolutions: Efficient Invariance to Spatial Transformations | http://arxiv.org/abs/1609.04382 | id:1609.04382 author:Jo√£o F. Henriques, Andrea Vedaldi category:cs.CV  published:2016-09-14 summary:Convolutional Neural Networks (CNNs) are extremely efficient, since they exploit the inherent translation-invariance of natural images. However, translation is just one of a myriad of useful spatial transformations. Can the same efficiency be attained when considering other spatial invariances? Such generalized convolutions have been considered in the past, but at a high computational cost. We present a construction that is simple and exact, yet has the same computational complexity that standard convolutions enjoy. It consists of a constant image warp followed by a simple convolution, which are standard blocks in deep learning toolboxes. With a carefully crafted warp, the resulting architecture can be made invariant to one of a wide range of spatial transformations. We show encouraging results in realistic scenarios, including the estimation of vehicle poses in the Google Earth dataset (rotation and scale), and face poses in Annotated Facial Landmarks in the Wild (3D rotations under perspective). version:2
arxiv-1609-04653 | Lost and Found: Detecting Small Road Hazards for Self-Driving Vehicles | http://arxiv.org/abs/1609.04653 | id:1609.04653 author:Peter Pinggera, Sebastian Ramos, Stefan Gehrig, Uwe Franke, Carsten Rother, Rudolf Mester category:cs.CV cs.RO  published:2016-09-15 summary:Detecting small obstacles on the road ahead is a critical part of the driving task which has to be mastered by fully autonomous cars. In this paper, we present a method based on stereo vision to reliably detect such obstacles from a moving vehicle. The proposed algorithm performs statistical hypothesis tests in disparity space directly on stereo image data, assessing freespace and obstacle hypotheses on independent local patches. This detection approach does not depend on a global road model and handles both static and moving obstacles. For evaluation, we employ a novel lost-cargo image sequence dataset comprising more than two thousand frames with pixelwise annotations of obstacle and free-space and provide a thorough comparison to several stereo-based baseline methods. The dataset will be made available to the community to foster further research on this important topic. The proposed approach outperforms all considered baselines in our evaluations on both pixel and object level and runs at frame rates of up to 20 Hz on 2 mega-pixel stereo imagery. Small obstacles down to the height of 5 cm can successfully be detected at 20 m distance at low false positive rates. version:1
arxiv-1609-04628 | Context Aware Nonnegative Matrix Factorization Clustering | http://arxiv.org/abs/1609.04628 | id:1609.04628 author:Rocco Tripodi, Sebastiano Vascon, Marcello Pelillo category:cs.CV cs.AI cs.CL cs.GT  published:2016-09-15 summary:In this article we propose a method to refine the clustering results obtained with the nonnegative matrix factorization (NMF) technique, imposing consistency constraints on the final labeling of the data. The research community focused its effort on the initialization and on the optimization part of this method, without paying attention to the final cluster assignments. We propose a game theoretic framework in which each object to be clustered is represented as a player, which has to choose its cluster membership. The information obtained with NMF is used to initialize the strategy space of the players and a weighted graph is used to model the interactions among the players. These interactions allow the players to choose a cluster which is coherent with the clusters chosen by similar players, a property which is not guaranteed by NMF, since it produces a soft clustering of the data. The results on common benchmarks show that our model is able to improve the performances of many NMF formulations. version:1
arxiv-1609-04621 | Factored Neural Machine Translation | http://arxiv.org/abs/1609.04621 | id:1609.04621 author:Mercedes Garc√≠a-Mart√≠nez, Lo√Øc Barrault, Fethi Bougares category:cs.CL  published:2016-09-15 summary:We present a new approach for neural machine translation (NMT) using the morphological and grammatical decomposition of the words (factors) in the output side of the neural network. This architecture addresses two main problems occurring in MT, namely dealing with a large target language vocabulary and the out of vocabulary (OOV) words. By the means of factors, we are able to handle larger vocabulary and reduce the training time (for systems with equivalent target language vocabulary size). In addition, we can produce new words that are not in the vocabulary. We use a morphological analyser to get a factored representation of each word (lemmas, Part of Speech tag, tense, person, gender and number). We have extended the NMT approach with attention mechanism in order to have two different outputs, one for the lemmas and the other for the rest of the factors. The final translation is built using some \textit{a priori} linguistic information. We compare our extension with a word-based NMT system. The experiments, performed on the IWSLT'15 dataset translating from English to French, show that while the performance do not always increase, the system can manage a much larger vocabulary and consistently reduce the OOV rate. We observe up to 2% BLEU point improvement in a simulated out of domain translation setup. version:1
arxiv-1609-04608 | Recursive nearest agglomeration (ReNA): fast clustering for approximation of structured signals | http://arxiv.org/abs/1609.04608 | id:1609.04608 author:Andr√©s Hoyos-Idrobo, Ga√´l Varoquaux, Jonas Kahn, Bertrand Thirion category:stat.ML cs.LG  published:2016-09-15 summary:-In this work, we revisit fast dimension reduction approaches, as with random projections and random sampling. Our goal is to summarize the data to decrease computational costs and memory footprint of subsequent analysis. Such dimension reduction can be very efficient when the signals of interest have a strong structure, such as with images. We focus on this setting and investigate feature clustering schemes for data reductions that capture this structure. An impediment to fast dimension reduction is that good clustering comes with large algorithmic costs. We address it by contributing a linear-time agglomerative clustering scheme, Recursive Nearest Agglomeration (ReNA). Unlike existing fast agglomerative schemes, it avoids the creation of giant clusters. We empirically validate that it approximates the data as well as traditional variance-minimizing clustering schemes that have a quadratic complexity. In addition, we analyze signal approximation with feature clustering and show that it can remove noise, improving subsequent analysis steps. As a consequence, data reduction by clustering features with ReNA yields very fast and accurate models, enabling to process large datasets on budget. Our theoretical analysis is backed by extensive experiments on publicly-available data that illustrate the computation efficiency and the denoising properties of the resulting dimension reduction scheme. version:1
arxiv-1609-06144 | Multilevel Monte Carlo for Scalable Bayesian Computations | http://arxiv.org/abs/1609.06144 | id:1609.06144 author:Mike Giles, Tigran Nagapetyan, Lukasz Szpruch, Sebastian Vollmer, Konstantinos Zygalakis category:stat.ML math.PR  published:2016-09-15 summary:Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in Bayesian computations. However, they need to access the full data set in order to evaluate the posterior density at every step of the algorithm. This results in a great computational burden in big data applications. In contrast to MCMC methods, Stochastic Gradient MCMC (SGMCMC) algorithms such as the Stochastic Gradient Langevin Dynamics (SGLD) only require access to a batch of the data set at every step. This drastically improves the computational performance and scales well to large data sets. However, the difficulty with SGMCMC algorithms comes from the sensitivity to its parameters which are notoriously difficult to tune. Moreover, the Root Mean Square Error (RMSE) scales as $\mathcal{O}(c^{-\frac{1}{3}})$ as opposed to standard MCMC $\mathcal{O}(c^{-\frac{1}{2}})$ where $c$ is the computational cost. We introduce a new class of Multilevel Stochastic Gradient Markov chain Monte Carlo algorithms that are able to mitigate the problem of tuning the step size and more importantly of recovering the $\mathcal{O}(c^{-\frac{1}{2}})$ convergence of standard Markov Chain Monte Carlo methods without the need to introduce Metropolis-Hasting steps. A further advantage of this new class of algorithms is that it can easily be parallelised over a heterogeneous computer architecture. We illustrate our methodology using Bayesian logistic regression and provide numerical evidence that for a prescribed relative RMSE the computational cost is sublinear in the number of data items. version:1
arxiv-1609-04557 | Structured Dropout for Weak Label and Multi-Instance Learning and Its Application to Score-Informed Source Separation | http://arxiv.org/abs/1609.04557 | id:1609.04557 author:Sebastian Ewert, Mark B. Sandler category:cs.LG cs.SD I.2.6; H.5.5  published:2016-09-15 summary:Many success stories involving deep neural networks are instances of supervised learning, where available labels power gradient-based learning methods. Creating such labels, however, can be expensive and thus there is increasing interest in weak labels which only provide coarse information, with uncertainty regarding time, location or value. Using such labels often leads to considerable challenges for the learning process. Current methods for weak-label training often employ standard supervised approaches that additionally reassign or prune labels during the learning process. The information gain, however, is often limited as only the importance of labels where the network already yields reasonable results is boosted. We propose treating weak-label training as an unsupervised problem and use the labels to guide the representation learning to induce structure. To this end, we propose two autoencoder extensions: class activity penalties and structured dropout. We demonstrate the capabilities of our approach in the context of score-informed source separation of music. version:1
arxiv-1609-04541 | Matrix Product State for Higher-Order Tensor Compression and Classification | http://arxiv.org/abs/1609.04541 | id:1609.04541 author:Johann A. Bengua, Ho N. Phien, Hoang D. Tuan, Minh N. Do category:stat.ML cs.CV cs.DS  published:2016-09-15 summary:This paper introduces matrix product state (MPS) decomposition as a new and systematic method to compress multidimensional data represented by higher-order tensors. It solves two major bottlenecks in tensor compression: computation and compression quality. Regardless of tensor order, MPS compresses tensors to matrices of moderate dimension which can be used for classification. Mainly based on a successive sequence of singular value decompositions (SVD), MPS is quite simple to implement and arrives at the global optimal matrix, bypassing local alternating optimization, which is not only computationally expensive but cannot yield the global solution. Benchmark results show that MPS can achieve better classification performance with favorable computation cost compared to other tensor compression methods. version:1
arxiv-1609-04523 | Sparse Low-rank Tensor Response Regression | http://arxiv.org/abs/1609.04523 | id:1609.04523 author:Will Wei Sun, Lexin Li category:stat.ML stat.AP stat.ME  published:2016-09-15 summary:Motivated by structural and functional neuroimaging analysis, we propose a new class of tensor response regression models. The model embeds two key low-dimensional structures: sparsity and low- rankness, and can handle both a general and a symmetric tensor response. We formulate parameter estimation of our model as a non- convex optimization problem, and develop an efficient alternating updating algorithm. Theoretically, we establish a non-asymptotic estimation error bound for the actual estimator obtained from our algorithm. This error bound reveals an interesting interaction between the computational efficiency and the statistical rate of convergence. Based on this general error bound, we further obtain an optimal estimation error rate when the distribution of the error tensor is Gaussian. Our technical analysis is based upon exploitation of the bi-convex structure of the objective function and a careful characterization of the impact of an intermediate sparse tensor decomposition step. The resulting new technical tools are also of independent interest to the theoretical analysis of general non-convex optimization problems. Simulations and an analysis of the autism spectrum disorder imaging data further illustrate the efficacy of our method. version:1
arxiv-1609-04522 | Sparse Tensor Graphical Model: Non-convex Optimization and Statistical Inference | http://arxiv.org/abs/1609.04522 | id:1609.04522 author:Will Wei Sun, Zhaoran Wang, Xiang Lyu, Han Liu, Guang Cheng category:stat.ML stat.ME  published:2016-09-15 summary:We consider the estimation and inference of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. A critical challenge in the estimation and inference of this model is the fact that its penalized maximum likelihood estimation involves minimizing a non-convex objective function. To address it, this paper makes two contributions: (i) In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence. Notably, such an estimator achieves estimation consistency with only one tensor sample, which was not observed in the previous work. (ii) We propose a de-biased statistical inference procedure for testing hypotheses on the true support of the sparse precision matrices, and employ it for testing a growing number of hypothesis with false discovery rate (FDR) control. The asymptotic normality of our test statistic and the consistency of FDR control procedure are established. Our theoretical results are further backed up by thorough numerical studies. We implement the methods into a publicly available R package Tlasso. version:1
arxiv-1609-04508 | Column Networks for Collective Classification | http://arxiv.org/abs/1609.04508 | id:1609.04508 author:Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh category:cs.LG cs.AI stat.ML  published:2016-09-15 summary:Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computational challenging and has not leveraged on the recent breakthroughs of deep learning. We present Column Network (CLN), a novel deep learning model for collective classification in multi-relational domains. CLN has many desirable theoretical properties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient, linear in the size of the network and the number of relations. We evaluate CLN on multiple real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all applications, CLN demonstrates a higher accuracy than state-of-the-art rivals. version:1
arxiv-1609-04495 | Tsallis Regularized Optimal Transport and Ecological Inference | http://arxiv.org/abs/1609.04495 | id:1609.04495 author:Boris Muzellec, Richard Nock, Giorgio Patrini, Frank Nielsen category:cs.LG G.1.6  published:2016-09-15 summary:Optimal transport is a powerful framework for computing distances between probability distributions. We unify the two main approaches to optimal transport, namely Monge-Kantorovitch and Sinkhorn-Cuturi, into what we define as Tsallis regularized optimal transport (\trot). \trot~interpolates a rich family of distortions from Wasserstein to Kullback-Leibler, encompassing as well Pearson, Neyman and Hellinger divergences, to name a few. We show that metric properties known for Sinkhorn-Cuturi generalize to \trot, and provide efficient algorithms for finding the optimal transportation plan with formal convergence proofs. We also present the first application of optimal transport to the problem of ecological inference, that is, the reconstruction of joint distributions from their marginals, a problem of large interest in the social sciences. \trot~provides a convenient framework for ecological inference by allowing to compute the joint distribution --- that is, the optimal transportation plan itself --- when side information is available, which is \textit{e.g.} typically what census represents in political science. Experiments on data from the 2012 US presidential elections display the potential of \trot~in delivering a faithful reconstruction of the joint distribution of ethnic groups and voter preferences. version:1
arxiv-1609-04453 | A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning | http://arxiv.org/abs/1609.04453 | id:1609.04453 author:T. Nathan Mundhenk, Goran Konjevod, Wesam A. Sakla, Kofi Boakye category:cs.CV cs.DC cs.NE  published:2016-09-14 summary:We have created a large diverse set of cars from overhead images, which are useful for training a deep learner to binary classify, detect and count them. The dataset and all related material will be made publically available. The set contains contextual matter to aid in identification of difficult targets. We demonstrate classification and detection on this dataset using a neural network we call ResCeption. This network combines residual learning with Inception-style layers and is used to count cars in one look. This is a new way to count objects rather than by localization or density estimation. It is fairly accurate, fast and easy to implement. Additionally, the counting method is not car or scene specific. It would be easy to train this method to count other kinds of objects and counting over new scenes requires no extra set up or assumptions about object locations. version:1
arxiv-1609-04436 | Bayesian Reinforcement Learning: A Survey | http://arxiv.org/abs/1609.04436 | id:1609.04436 author:Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar category:cs.AI cs.LG stat.ML  published:2016-09-14 summary:Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties. version:1
arxiv-1609-04417 | An Adaptive Psychoacoustic Model for Automatic Speech Recognition | http://arxiv.org/abs/1609.04417 | id:1609.04417 author:Peng Dai, Xue Teng, Frank Rudzicz, Ing Yann Soon category:cs.CL cs.SD  published:2016-09-14 summary:Compared with automatic speech recognition (ASR), the human auditory system is more adept at handling noise-adverse situations, including environmental noise and channel distortion. To mimic this adeptness, auditory models have been widely incorporated in ASR systems to improve their robustness. This paper proposes a novel auditory model which incorporates psychoacoustics and otoacoustic emissions (OAEs) into ASR. In particular, we successfully implement the frequency-dependent property of psychoacoustic models and effectively improve resulting system performance. We also present a novel double-transform spectrum-analysis technique, which can qualitatively predict ASR performance for different noise types. Detailed theoretical analysis is provided to show the effectiveness of the proposed algorithm. Experiments are carried out on the AURORA2 database and show that the word recognition rate using our proposed feature extraction method is significantly increased over the baseline. Given models trained with clean speech, our proposed method achieves up to 85.39% word recognition accuracy on noisy data. version:1
arxiv-1609-04392 | Learning Robust Features for Gait Recognition by Maximum Margin Criterion | http://arxiv.org/abs/1609.04392 | id:1609.04392 author:Michal Balazia, Petr Sojka category:cs.CV 68T05  68T10 I.5  published:2016-09-14 summary:In the field of gait recognition from motion capture data, designing human-interpretable gait features is a common practice of many fellow researchers. To refrain from ad-hoc schemes and to find maximally discriminative features we may need to explore beyond the limits of human interpretability. This paper contributes to the state-of-the-art with a machine learning approach for extracting robust gait features directly from raw joint coordinates. The features are learned by a modification of Linear Discriminant Analysis with Maximum Margin Criterion so that the identities are maximally separated and, in combination with an appropriate classifier, used for gait recognition. Experiments on the CMU MoCap database show that this method outperforms eight other relevant methods in terms of the distribution of biometric templates in respective feature spaces expressed in four class separability coefficients. Additional experiments indicate that this method is a leading concept for rank-based classifier systems. version:1
arxiv-1609-04388 | Relativistic Monte Carlo | http://arxiv.org/abs/1609.04388 | id:1609.04388 author:Xiaoyu Lu, Valerio Perrone, Leonard Hasenclever, Yee Whye Teh, Sebastian J. Vollmer category:stat.ML  published:2016-09-14 summary:Hamiltonian Monte Carlo (HMC) is a popular Markov chain Monte Carlo (MCMC) algorithm that generates proposals for a Metropolis-Hastings algorithm by simulating the dynamics of a Hamiltonian system. However, HMC is sensitive to large time discretizations and performs poorly if there is a mismatch between the spatial geometry of the target distribution and the scales of the momentum distribution. In particular the mass matrix of HMC is hard to tune well. In order to alleviate these problems we propose relativistic Hamiltonian Monte Carlo, a version of HMC based on relativistic dynamics that introduce a maximum velocity on particles. We also derive stochastic gradient versions of the algorithm and show that the resulting algorithms bear interesting relationships to gradient clipping, RMSprop, Adagrad and Adam, popular optimisation methods in deep learning. Based on this, we develop relativistic stochastic gradient descent by taking the zero-temperature limit of relativistic stochastic gradient Hamiltonian Monte Carlo. In experiments we show that the relativistic algorithms perform better than classical Newtonian variants and Adam. version:1
arxiv-1609-04387 | 3D Face Reconstruction by Learning from Synthetic Data | http://arxiv.org/abs/1609.04387 | id:1609.04387 author:Elad Richardson, Matan Sela, Ron Kimmel category:cs.CV  published:2016-09-14 summary:Fast and robust three-dimensional reconstruction of facial geometric structure from a single image is a challenging task with numerous applications. Here, we introduce a learning-based approach for reconstructing a three-dimensional face from a single image. Recent face recovery methods rely on accurate localization of key characteristic points. In contrast, the proposed approach is based on a Convolutional-Neural-Network (CNN) which extracts the face geometry directly from its image. Although such deep architectures outperform other models in complex computer vision problems, training them properly requires a large dataset of annotated examples. In the case of three-dimensional faces, currently, there are no large volume data sets, while acquiring such big-data is a tedious task. As an alternative, we propose to generate random, yet nearly photo-realistic, facial images for which the geometric form is known. The suggested model successfully recovers facial shapes from real images, even for faces with extreme expressions and under various lighting conditions. version:1
arxiv-1609-03126 | Energy-based Generative Adversarial Network | http://arxiv.org/abs/1609.03126 | id:1609.03126 author:Junbo Zhao, Michael Mathieu, Yann LeCun category:cs.LG stat.ML  published:2016-09-11 summary:We introduce the "Energy-based Generative Adversarial Network'" model (EBGAN) which views the discriminator as an energy function that associates low energies with the regions near the data manifold and higher energies with other regions. Similar to the probabilistic GANs, a generator is trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, an instantiation of EBGAN is to use an auto-encoder architecture, with the energy being the reconstruction error. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images. version:2
arxiv-1609-04356 | Combining Texture and Shape Cues for Object Recognition With Minimal Supervision | http://arxiv.org/abs/1609.04356 | id:1609.04356 author:Xingchao Peng, Kate Saenko category:cs.CV  published:2016-09-14 summary:We present a novel approach to object classification and detection which requires minimal supervision and which combines visual texture cues and shape information learned from freely available unlabeled web search results. The explosion of visual data on the web can potentially make visual examples of almost any object easily accessible via web search. Previous unsupervised methods have utilized either large scale sources of texture cues from the web, or shape information from data such as crowdsourced CAD models. We propose a two-stream deep learning framework that combines these cues, with one stream learning visual texture cues from image search data, and the other stream learning rich shape information from 3D CAD models. To perform classification or detection for a novel image, the predictions of the two streams are combined using a late fusion scheme. We present experiments and visualizations for both tasks on the standard benchmark PASCAL VOC 2007 to demonstrate that texture and shape provide complementary information in our model. Our method outperforms previous web image based models, 3D CAD model based approaches, and weakly supervised models. version:1
arxiv-1609-04337 | Quick and energy-efficient Bayesian computing of binocular disparity using stochastic digital signals | http://arxiv.org/abs/1609.04337 | id:1609.04337 author:Alexandre Coninx, Pierre Bessi√®re, Jacques Droulez category:cs.CV cs.AI  published:2016-09-14 summary:Reconstruction of the tridimensional geometry of a visual scene using the binocular disparity information is an important issue in computer vision and mobile robotics, which can be formulated as a Bayesian inference problem. However, computation of the full disparity distribution with an advanced Bayesian model is usually an intractable problem, and proves computationally challenging even with a simple model. In this paper, we show how probabilistic hardware using distributed memory and alternate representation of data as stochastic bitstreams can solve that problem with high performance and energy efficiency. We put forward a way to express discrete probability distributions using stochastic data representations and perform Bayesian fusion using those representations, and show how that approach can be applied to diparity computation. We evaluate the system using a simulated stochastic implementation and discuss possible hardware implementations of such architectures and their potential for sensorimotor processing and robotics. version:1
arxiv-1609-04331 | ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization | http://arxiv.org/abs/1609.04331 | id:1609.04331 author:Vadim Kantorov, Maxime Oquab, Minsu Cho, Ivan Laptev category:cs.CV  published:2016-09-14 summary:We aim to localize objects in images using image-level supervision only. Previous approaches to this problem mainly focus on discriminative object regions and often fail to locate precise object boundaries. We address this problem by introducing two types of context-aware guidance models, additive and contrastive models, that leverage their surrounding context regions to improve localization. The additive model encourages the predicted object region to be supported by its surrounding context region. The contrastive model encourages the predicted object region to be outstanding from its surrounding context region. Our approach benefits from the recent success of convolutional neural networks for object recognition and extends Fast R-CNN to weakly supervised object localization. Extensive experimental evaluation on the PASCAL VOC 2007 and 2012 benchmarks shows hat our context-aware approach significantly improves weakly supervised localization and detection. version:1
arxiv-1609-01423 | Structured Sparse Principal Components Analysis with the TV-Elastic Net penalty | http://arxiv.org/abs/1609.01423 | id:1609.01423 author:Amicie de Pierrefeu, Tommy L√∂fstedt, Fouad Hadj-Selem, Mathieu Dubois, Philippe Ciuciu, Vincent Frouin, Edouard Duchesnay category:stat.ML  published:2016-09-06 summary:Principal component analysis (PCA) is an exploratory tool widely used in data analysis to uncover dominant patterns of variability within a population. Despite its ability to represent a data set in a low-dimensional space, the interpretability of PCA remains limited. However, in neuroimaging, it is essential to uncover clinically interpretable phenotypic markers that would account for the main variability in the brain images of a population. Recently, some alternatives to the standard PCA approach, such as Sparse PCA, have been proposed, their aim being to limit the density of the components. Nonetheless, sparsity alone does not entirely solve the interpretability problem, since it may yield scattered and unstable components. We hypothesized that the incorporation of prior information regarding the structure of the data may lead to improved relevance and interpretability of brain patterns. We therefore present a simple extension of the popular PCA framework that adds structured sparsity penalties on the loading vectors in order to identify the few stable regions in the brain images accounting for most of the variability. Such structured sparsity can be obtained by combining l1 and total variation (TV) penalties, where the TV regularization encodes higher order information about the structure of the data. This paper presents the structured sparse PCA (denoted SPCA-TV) optimization framework and its resolution. We demonstrate the efficiency and versatility of SPCA-TV on three different data sets. The gains of SPCA-TV over unstructured approaches are significant,since SPCA-TV reveals the variability within a data set in the form of intelligible brain patterns that are easy to interpret, and are more stable across different samples. version:3
arxiv-1609-04325 | Transliteration in Any Language with Surrogate Languages | http://arxiv.org/abs/1609.04325 | id:1609.04325 author:Stephen Mayhew, Christos Christodoulopoulos, Dan Roth category:cs.CL  published:2016-09-14 summary:We introduce a method for transliteration generation that can produce transliterations in every language. Where previous results are only as multilingual as Wikipedia, we show how to use training data from Wikipedia as surrogate training for any language. Thus, the problem becomes one of ranking Wikipedia languages in order of suitability with respect to a target language. We introduce several task-specific methods for ranking languages, and show that our approach is comparable to the oracle ceiling, and even outperforms it in some cases. version:1
arxiv-1609-04321 | Very Simple Classifier: a Concept Binary Classifier toInvestigate Features Based on Subsampling and Localility | http://arxiv.org/abs/1609.04321 | id:1609.04321 author:Luca Masera, Enrico Blanzieri category:cs.LG stat.ML  published:2016-09-14 summary:We propose Very Simple Classifier (VSC) a novel method designed to incorporate the concepts of subsampling and locality in the definition of features to be used as the input of a perceptron. The rationale is that locality theoretically guarantees a bound on the generalization error. Each feature in VSC is a max-margin classifier built on randomly-selected pairs of samples. The locality in VSC is achieved by multiplying the value of the feature by a confidence measure that can be characterized in terms of the Chebichev inequality. The output of the layer is then fed in a output layer of neurons. The weights of the output layer are then determined by a regularized pseudoinverse. Extensive comparison of VSC against 9 competitors in the task of binary classification is carried out. Results on 22 benchmark datasets with fixed parameters show that VSC is competitive with the Multi Layer Perceptron (MLP) and outperforms the other competitors. An exploration of the parameter space shows VSC can outperform MLP. version:1
arxiv-1609-04309 | Efficient softmax approximation for GPUs | http://arxiv.org/abs/1609.04309 | id:1609.04309 author:Edouard Grave, Armand Joulin, Moustapha Ciss√©, David Grangier, Herv√© J√©gou category:cs.CL cs.LG  published:2016-09-14 summary:We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. version:1
arxiv-1609-04301 | TristouNet: Triplet Loss for Speaker Turn Embedding | http://arxiv.org/abs/1609.04301 | id:1609.04301 author:Herv√© Bredin category:cs.SD stat.ML  published:2016-09-14 summary:TristouNet is a neural network architecture based on Long Short-Term Memory recurrent networks, meant to project speech sequences into a fixed-dimensional euclidean space. Thanks to the triplet loss paradigm used for training, the resulting sequence embeddings can be compared directly with the euclidean distance, for speaker comparison purposes. Experiments on short (between 500ms and 5s) speech turn comparison and speaker change detection show that TristouNet brings significant improvements over the current state-of-the-art techniques for both tasks. version:1
arxiv-1609-04289 | Gray-box inference for structured Gaussian process models | http://arxiv.org/abs/1609.04289 | id:1609.04289 author:Pietro Galliani, Amir Dezfouli, Edwin V. Bonilla, Novi Quadrianto category:stat.ML  published:2016-09-14 summary:We develop an automated variational inference method for Bayesian structured prediction problems with Gaussian process (GP) priors and linear-chain likelihoods. Our approach does not need to know the details of the structured likelihood model and can scale up to a large number of observations. Furthermore, we show that the required expected likelihood term and its gradients in the variational objective (ELBO) can be estimated efficiently by using expectations over very low-dimensional Gaussian distributions. Optimization of the ELBO is fully parallelizable over sequences and amenable to stochastic optimization, which we use along with control variate techniques and state-of-the-art incremental optimization to make our framework useful in practice. Results on a set of natural language processing tasks show that our method can be as good as (and sometimes better than) hard-coded approaches including SVM-struct and CRFs, and overcomes the scalability limitations of previous inference algorithms based on sampling. Overall, this is a fundamental step to developing automated inference methods for Bayesian structured prediction. version:1
arxiv-1609-04253 | Neural Machine Transliteration: Preliminary Results | http://arxiv.org/abs/1609.04253 | id:1609.04253 author:Amir H. Jadidinejad category:cs.CL  published:2016-09-14 summary:Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. In this paper a character-based encoder-decoder model has been proposed that consists of two Recurrent Neural Networks. The encoder is a Bidirectional recurrent neural network that encodes a sequence of symbols into a fixed-length vector representation, and the decoder generates the target sequence using an attention-based recurrent neural network. The encoder, the decoder and the attention mechanism are jointly trained to maximize the conditional probability of a target sequence given a source sequence. Our experiments on different datasets show that the proposed encoder-decoder model is able to achieve significantly higher transliteration quality over traditional statistical models. version:1
arxiv-1609-04243 | Convolutional Recurrent Neural Networks for Music Classification | http://arxiv.org/abs/1609.04243 | id:1609.04243 author:Keunwoo Choi, George Fazekas, Mark Sandler, Kyunghyun Cho category:cs.NE cs.LG cs.MM cs.SD  published:2016-09-14 summary:We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with two CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation. version:1
arxiv-1609-04228 | Stochastic Heavy Ball | http://arxiv.org/abs/1609.04228 | id:1609.04228 author:S√©bastien Gadat, Fabien Panloup, Sofiane Saadane category:math.ST math.PR stat.ML stat.TH  published:2016-09-14 summary:This paper deals with a natural stochastic optimization procedure derived from the so-called Heavy-ball method differential equation, which was introduced by Polyak in the 1960s with his seminal contribution [Pol64]. The Heavy-ball method is a second-order dynamics that was investigated to minimize convex functions f . The family of second-order methods recently received a large amount of attention, until the famous contribution of Nesterov [Nes83], leading to the explosion of large-scale optimization problems. This work provides an in-depth description of the stochastic heavy-ball method, which is an adaptation of the deterministic one when only unbiased evalutions of the gradient are available and used throughout the iterations of the algorithm. We first describe some almost sure convergence results in the case of general non-convex coercive functions f . We then examine the situation of convex and strongly convex potentials and derive some non-asymptotic results about the stochastic heavy-ball method. We end our study with limit theorems on several rescaled algorithms. version:1
arxiv-1609-04623 | Distributed Estimation of the Operating State of a Single-Bus DC MicroGrid without an External Communication Interface | http://arxiv.org/abs/1609.04623 | id:1609.04623 author:Marko Angjelichinoski, Anna Scaglione, Petar Popovski, Cedomir Stefanovic category:stat.ML cs.SY  published:2016-09-14 summary:We propose a decentralized Maximum Likelihood solution for estimating the stochastic renewable power generation and demand in single bus Direct Current (DC) MicroGrids (MGs), with high penetration of droop controlled power electronic converters. The solution relies on the fact that the primary control parameters are set in accordance with the local power generation status of the generators. Therefore, the steady state voltage is inherently dependent on the generation capacities and the load, through a non-linear parametric model, which can be estimated. To have a well conditioned estimation problem, our solution avoids the use of an external communication interface and utilizes controlled voltage disturbances to perform distributed training. Using this tool, we develop an efficient, decentralized Maximum Likelihood Estimator (MLE) and formulate the sufficient condition for the existence of the globally optimal solution. The numerical results illustrate the promising performance of our MLE algorithm. version:1
arxiv-1609-04212 | Formalizing Neurath's Ship: Approximate Algorithms for Online Causal Learning | http://arxiv.org/abs/1609.04212 | id:1609.04212 author:Neil R. Bramley, Peter Dayan, Thomas L. Griffiths, David A. Lagnado category:cs.LG  published:2016-09-14 summary:Higher-level cognition depends on the ability to learn models of the world. We can characterize this at the computational level as a structure-learning problem with the goal of best identifying the prevailing causal relationships among a set of relata. However, the computational cost of performing exact Bayesian inference over causal models grows rapidly as the number of relata increases. This implies that the cognitive processes underlying causal learning must be substantially approximate. A powerful class of approximations that focuses on the sequential absorption of successive inputs is captured by the Neurath's ship metaphor in philosophy of science, where theory change is cast as a stochastic and gradual process shaped as much by people's limited willingness to abandon their current theory when considering alternatives as by the ground truth they hope to approach. Inspired by this metaphor and by algorithms for approximating Bayesian inference in machine learning, we propose an algorithmic-level model of causal structure learning under which learners represent only a single global hypothesis that they update locally as they gather evidence. We propose a related scheme for understanding how, under these limitations, learners choose informative interventions that manipulate the causal system to help elucidate its workings. We find support for our approach in the analysis of four experiments. version:1
arxiv-1609-04186 | Neural Machine Translation with Supervised Attention | http://arxiv.org/abs/1609.04186 | id:1609.04186 author:Lemao Liu, Masao Utiyama, Andrew Finch, Eiichiro Sumita category:cs.CL  published:2016-09-14 summary:The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of re- ordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the super- vised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT. version:1
arxiv-1609-04167 | Proceedings of the third "international Traveling Workshop on Interactions between Sparse models and Technology" (iTWIST'16) | http://arxiv.org/abs/1609.04167 | id:1609.04167 author:V. Abrol, O. Absil, P. -A. Absil, S. Anthoine, P. Antoine, T. Arildsen, N. Bertin, F. Bleichrodt, J. Bobin, A. Bol, A. Bonnefoy, F. Caltagirone, V. Cambareri, C. Chenot, V. Crnojeviƒá, M. Da≈àkov√°, K. Degraux, J. Eisert, J. M. Fadili, M. Gabri√©, N. Gac, D. Giacobello, A. Gonzalez, C. A. Gomez Gonzalez, A. Gonz√°lez, P. -Y. Gousenbourger, M. Gr√¶sb√∏ll Christensen, R. Gribonval, S. Gu√©rit, S. Huang, P. Irofti, L. Jacques, U. S. Kamilov, S. Kiticƒá, M. Kliesch, F. Krzakala, J. A. Lee, W. Liao, T. Lindstr√∏m Jensen, A. Manoel, H. Mansour, A. Mohammad-Djafari, A. Moshtaghpour, F. Ngol√®, B. Pairet, M. Paniƒá, G. Peyr√©, A. Pi≈æurica, P. Rajmic, M. Roblin, I. Roth, A. K. Sao, P. Sharma, J. -L. Starck, E. W. Tramel, T. van Waterschoot, D. Vukobratovic, L. Wang, B. Wirth, G. Wunder, H. Zhang category:cs.NA cs.CV cs.IT cs.LG math.IT math.OC  published:2016-09-14 summary:The third edition of the "international - Traveling Workshop on Interactions between Sparse models and Technology" (iTWIST) took place in Aalborg, the 4th largest city in Denmark situated beautifully in the northern part of the country, from the 24th to 26th of August 2016. The workshop venue was at the Aalborg University campus. One implicit objective of this biennial workshop is to foster collaboration between international scientific teams by disseminating ideas through both specific oral/poster presentations and free discussions. For this third edition, iTWIST'16 gathered about 50 international participants and features 8 invited talks, 12 oral presentations, and 12 posters on the following themes, all related to the theory, application and generalization of the "sparsity paradigm": Sparsity-driven data sensing and processing (e.g., optics, computer vision, genomics, biomedical, digital communication, channel estimation, astronomy); Application of sparse models in non-convex/non-linear inverse problems (e.g., phase retrieval, blind deconvolution, self calibration); Approximate probabilistic inference for sparse problems; Sparse machine learning and inference; "Blind" inverse problems and dictionary learning; Optimization for sparse modelling; Information theory, geometry and randomness; Sparsity? What's next? (Discrete-valued signals; Union of low-dimensional spaces, Cosparsity, mixed/group norm, model-based, low-complexity models, ...); Matrix/manifold sensing/processing (graph, low-rank approximation, ...); Complexity/accuracy tradeoffs in numerical methods/optimization; Electronic/optical compressive sensors (hardware). version:1
arxiv-1609-04120 | Private Topic Modeling | http://arxiv.org/abs/1609.04120 | id:1609.04120 author:Mijung Park, James Foulds, Kamalika Chaudhuri, Max Welling category:stat.ML cs.CR  published:2016-09-14 summary:We develop a privatised stochastic variational inference method for Latent Dirichlet Allocation (LDA). The iterative nature of stochastic variational inference presents challenges: multiple iterations are required to obtain accurate posterior distributions, yet each iteration increases the amount of noise that must be added to achieve a reasonable degree of privacy. We propose a practical algorithm that overcomes this challenge by combining: (1) A relaxed notion of the differential privacy, called concentrated differential privacy, which provides high probability bounds for cumulative privacy loss, which is well suited for iterative algorithms, rather than focusing on single-query loss; and (2) Privacy amplification resulting from subsampling of large-scale data. Focusing on conjugate exponential family models, in our private variational inference, all the posterior distributions will be privatised by simply perturbing expected sufficient statistics. Using Wikipedia data, we illustrate the effectiveness of our algorithm for large-scale data. version:1
arxiv-1609-04116 | Joint Gender Classification and Age Estimation by Nearly Orthogonalizing Their Semantic Spaces | http://arxiv.org/abs/1609.04116 | id:1609.04116 author:Qing Tian, Songcan Chen category:cs.CV  published:2016-09-14 summary:In human face-based biometrics, gender classification and age estimation are two typical learning tasks. Although a variety of approaches have been proposed to handle them, just a few of them are solved jointly, even so, these joint methods do not yet specifically concern the semantic difference between human gender and age, which is intuitively helpful for joint learning, consequently leaving us a room of further improving the performance. To this end, in this work we firstly propose a general learning framework for jointly estimating human gender and age by specially attempting to formulate such semantic relationships as a form of near-orthogonality regularization and then incorporate it into the objective of the joint learning framework. In order to evaluate the effectiveness of the proposed framework, we exemplify it by respectively taking the widely used binary-class SVM for gender classification, and two threshold-based ordinal regression methods (i.e., the discriminant learning for ordinal regression and support vector ordinal regression) for age estimation, and crucially coupling both through the proposed semantic formulation. Moreover, we develop its kernelized nonlinear counterpart by deriving a representer theorem for the joint learning strategy. Finally, through extensive experiments on three aging datasets FG-NET, Morph Album I and Morph Album II, we demonstrate the effectiveness and superiority of the proposed joint learning strategy. version:1
arxiv-1609-04112 | Understanding Convolutional Neural Networks with A Mathematical Model | http://arxiv.org/abs/1609.04112 | id:1609.04112 author:C. -C. Jay Kuo category:cs.CV  published:2016-09-14 summary:This work attempts to address two fundamental questions about the structure of the convolutional neural networks (CNN): 1) why a non-linear activation function is essential at the filter output of every convolutional layer? 2) what is the advantage of the two-layer cascade system over the one-layer system? A mathematical model called the "REctified-COrrelations on a Sphere" (RECOS) is proposed to answer these two questions. After the CNN training process, the converged filter weights define a set of anchor vectors in the RECOS model. Anchor vectors represent the frequently occurring patterns (or the spectral components). The necessity of rectification is explained using the RECOS model. Then, the behavior of a two-layer RECOS system is analyzed and compared with its one-layer counterpart. The LeNet-5 and the MNIST dataset are used to illustrate discussion points. Finally, the RECOS model is generalized to a multi-layer system with the AlexNet as an example. Keywords: Convolutional Neural Network (CNN), Nonlinear Activation, Mathematical Modeling, MNIST Dataset. version:1
arxiv-1609-04104 | Tracking Tensor Subspaces with Informative Random Sampling for Real-Time MR Imaging | http://arxiv.org/abs/1609.04104 | id:1609.04104 author:Morteza Mardani, Georgios B. Giannakis, Kamil Ugurbil category:cs.LG cs.CV cs.IT math.IT stat.CO  published:2016-09-14 summary:Magnetic resonance imaging (MRI) nowadays serves as an important modality for diagnostic and therapeutic guidance in clinics. However, the {\it slow acquisition} process, the dynamic deformation of organs, as well as the need for {\it real-time} reconstruction, pose major challenges toward obtaining artifact-free images. To cope with these challenges, the present paper advocates a novel subspace learning framework that permeates benefits from parallel factor (PARAFAC) decomposition of tensors (multiway data) to low-rank modeling of temporal sequence of images. Treating images as multiway data arrays, the novel method preserves spatial structures and unravels the latent correlations across various dimensions by means of the tensor subspace. Leveraging the spatio-temporal correlation of images, Tykhonov regularization is adopted as a rank surrogate for a least-squares optimization program. Alteranating majorization minimization is adopted to develop online algorithms that recursively procure the reconstruction upon arrival of a new undersampled $k$-space frame. The developed algorithms are {\it provably convergent} and highly {\it parallelizable} with lightweight FFT tasks per iteration. To further accelerate the acquisition process, randomized subsampling policies are devised that leverage intermediate estimates of the tensor subspace, offered by the online scheme, to {\it randomly} acquire {\it informative} $k$-space samples. In a nutshell, the novel approach enables tracking motion dynamics under low acquisition rates `on the fly.' GPU-based tests with real {\it in vivo} MRI datasets of cardiac cine images corroborate the merits of the novel approach relative to state-of-the-art alternatives. version:1
arxiv-1609-03986 | The CUDA LATCH Binary Descriptor: Because Sometimes Faster Means Better | http://arxiv.org/abs/1609.03986 | id:1609.03986 author:Christopher Parker, Matthew Daiter, Kareem Omar, Gil Levi, Tal Hassner category:cs.CV  published:2016-09-13 summary:Accuracy, descriptor size, and the time required for extraction and matching are all important factors when selecting local image descriptors. To optimize over all these requirements, this paper presents a CUDA port for the recent Learned Arrangement of Three Patches (LATCH) binary descriptors to the GPU platform. The design of LATCH makes it well suited for GPU processing. Owing to its small size and binary nature, the GPU can further be used to efficiently match LATCH features. Taken together, this leads to breakneck descriptor extraction and matching speeds. We evaluate the trade off between these speeds and the quality of results in a feature matching intensive application. To this end, we use our proposed CUDA LATCH (CLATCH) to recover structure from motion (SfM), comparing 3D reconstructions and speed using different representations. Our results show that CLATCH provides high quality 3D reconstructions at fractions of the time required by other representations, with little, if any, loss of reconstruction quality. version:2
arxiv-1609-04079 | Single-image RGB Photometric Stereo With Spatially-varying Albedo | http://arxiv.org/abs/1609.04079 | id:1609.04079 author:Ayan Chakrabarti, Kalyan Sunkavalli category:cs.CV  published:2016-09-14 summary:We present a single-shot system to recover surface geometry of objects with spatially-varying albedos, from images captured under a calibrated RGB photometric stereo setup---with three light directions multiplexed across different color channels in the observed RGB image. Since the problem is ill-posed point-wise, we assume that the albedo map can be modeled as piece-wise constant with a restricted number of distinct albedo values. We show that under ideal conditions, the shape of a non-degenerate local constant albedo surface patch can theoretically be recovered exactly. Moreover, we present a practical and efficient algorithm that uses this model to robustly recover shape from real images. Our method first reasons about shape locally in a dense set of patches in the observed image, producing shape distributions for every patch. These local distributions are then combined to produce a single consistent surface normal map. We demonstrate the efficacy of the approach through experiments on both synthetic renderings as well as real captured images. version:1
arxiv-1609-03976 | Multimodal Attention for Neural Machine Translation | http://arxiv.org/abs/1609.03976 | id:1609.03976 author:Ozan Caglayan, Lo√Øc Barrault, Fethi Bougares category:cs.CL cs.NE  published:2016-09-13 summary:The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline. version:1
arxiv-1609-03971 | Feynman Machine: The Universal Dynamical Systems Computer | http://arxiv.org/abs/1609.03971 | id:1609.03971 author:Eric Laukien, Richard Crowder, Fergal Byrne category:cs.NE cs.AI cs.ET math.DS  published:2016-09-13 summary:Efforts at understanding the computational processes in the brain have met with limited success, despite their importance and potential uses in building intelligent machines. We propose a simple new model which draws on recent findings in Neuroscience and the Applied Mathematics of interacting Dynamical Systems. The Feynman Machine is a Universal Computer for Dynamical Systems, analogous to the Turing Machine for symbolic computing, but with several important differences. We demonstrate that networks and hierarchies of simple interacting Dynamical Systems, each adaptively learning to forecast its evolution, are capable of automatically building sensorimotor models of the external and internal world. We identify such networks in mammalian neocortex, and show how existing theories of cortical computation combine with our model to explain the power and flexibility of mammalian intelligence. These findings lead directly to new architectures for machine intelligence. A suite of software implementations has been built based on these principles, and applied to a number of spatiotemporal learning tasks. version:1
arxiv-1609-03960 | Self-Sustaining Iterated Learning | http://arxiv.org/abs/1609.03960 | id:1609.03960 author:Bernard Chazelle, Chu Wang category:math.OC cs.LG stat.ML  published:2016-09-13 summary:An important result from psycholinguistics (Griffiths & Kalish, 2005) states that no language can be learned iteratively by rational agents in a self-sustaining manner. We show how to modify the learning process slightly in order to achieve self-sustainability. Our work is in two parts. First, we characterize iterated learnability in geometric terms and show how a slight, steady increase in the lengths of the training sessions ensures self-sustainability for any discrete language class. In the second part, we tackle the nondiscrete case and investigate self-sustainability for iterated linear regression. We discuss the implications of our findings to issues of non-equilibrium dynamics in natural algorithms. version:1
arxiv-1609-03958 | Noisy Inductive Matrix Completion Under Sparse Factor Models | http://arxiv.org/abs/1609.03958 | id:1609.03958 author:Akshay Soni, Troy Chevalier, Swayambhoo Jain category:stat.ML cs.LG math.ST stat.TH  published:2016-09-13 summary:Inductive Matrix Completion (IMC) is an important class of matrix completion problems that allows direct inclusion of available features to enhance estimation capabilities. These models have found applications in personalized recommendation systems, multilabel learning, dictionary learning, etc. This paper examines a general class of noisy matrix completion tasks where the underlying matrix is following an IMC model i.e., it is formed by a mixing matrix (a priori unknown) sandwiched between two known feature matrices. The mixing matrix here is assumed to be well approximated by the product of two sparse matrices---referred here to as "sparse factor models." We leverage the main theorem of Soni:2016:NMC and extend it to provide theoretical error bounds for the sparsity-regularized maximum likelihood estimators for the class of problems discussed in this paper. The main result is general in the sense that it can be used to derive error bounds for various noise models. In this paper, we instantiate our main result for the case of Gaussian noise and provide corresponding error bounds in terms of squared loss. version:1
arxiv-1609-03948 | Method to Assess the Temporal Persistence of Potential Biometric Features: Application to Oculomotor, and Gait-Related Databases | http://arxiv.org/abs/1609.03948 | id:1609.03948 author:Lee Friedman, Ioannis Rigas, Mark S. Nixon, Oleg V. Komogortsev category:q-bio.QM cs.CV  published:2016-09-13 summary:Although temporal persistence, or permanence, is a well understood requirement for optimal biometric features, there is no general agreement on how to assess temporal persistence. We suggest that the best way to assess temporal persistence is to perform a test-retest study, and assess test-retest reliability. For ratio-scale features that are normally distributed, this is best done using the Intraclass Correlation Coefficient (ICC). For 10 distinct data sets (8 eye-movement related, and 2 gait related), we calculated the test-retest reliability ('Temporal persistence') of each feature, and compared biometric performance of high-ICC features to lower ICC features, and to the set of all features. We demonstrate that using a subset of only high-ICC features produced superior Rank-1-Identification Rate (Rank-1-IR) performance in 9 of 10 databases (p = 0.01, one-tailed). For Equal Error Rate (EER), using a subset of only high-ICC features produced superior performance in 8 of 10 databases (p = 0.055, one-tailed). In general, then, prescreening potential biometric features, and choosing only highly reliable features will yield better performance than lower ICC features or than the set of all features combined. We hypothesize that this would likely be the case for any biometric modality where the features can be expressed as quantitative values on an interval or ratio scale, assuming an adequate number of relatively independent features. version:1
arxiv-1609-03947 | Associating Grasping with Convolutional Neural Network Features | http://arxiv.org/abs/1609.03947 | id:1609.03947 author:Li Yang Ku, Erik Learned-Miller, Rod Grupen category:cs.CV cs.RO  published:2016-09-13 summary:In this work, we provide a solution for pre-shaping a human-like robot hand for grasping based on visual information. Our approach uses convolutional neural networks (CNNs) to define a mapping between images and grasps. Applying CNNs to robotics applications is non-trivial for two reasons. First, collecting enough robot data to train a CNN at the same scale as the models trained in the vision community is extremely difficult. In this work, we demonstrate that by using a pre-trained CNN, a small set of grasping examples is sufficient for generalizing across different objects of similar shapes. Second, the final output of a CNN contains little location information of the observed object, which is essential for the robot to manipulate the object. We take advantage of the hierarchical nature of CNN features and identify the 3D position of a mid-level feature using an approach we call targeted back propagation. Targeted back propagation traces the activations of higher level features in a CNN backwards through the network to discover the locations in the observation that were responsible for making them fire, thus localizing important manipulatives in the environment. We showed that this approach outperforms approaches without targeted backpropagation in a cluttered scene. We further implemented a hierarchical controller that controls fingers and palms based on features located in different CNN layers for pre-shaping the robot hand and demonstrated that this approach outperforms a pointcloud based approach on a grasping task on Robonaut 2. version:1
arxiv-1609-03932 | Mapping the Similarities of Spectra: Global and Locally-biased Approaches to SDSS Galaxy Data | http://arxiv.org/abs/1609.03932 | id:1609.03932 author:David Lawlor, Tam√°s Budav√°ri, Michael W. Mahoney category:astro-ph.IM astro-ph.CO cs.DS stat.ML  published:2016-09-13 summary:We apply a novel spectral graph technique, that of locally-biased semi-supervised eigenvectors, to study the diversity of galaxies. This technique permits us to characterize empirically the natural variations in observed spectra data, and we illustrate how this approach can be used in an exploratory manner to highlight both large-scale global as well as small-scale local structure in Sloan Digital Sky Survey (SDSS) data. We use this method in a way that simultaneously takes into account the measurements of spectral lines as well as the continuum shape. Unlike Principal Component Analysis, this method does not assume that the Euclidean distance between galaxy spectra is a good global measure of similarity between all spectra, but instead it only assumes that local difference information between similar spectra is reliable. Moreover, unlike other nonlinear dimensionality methods, this method can be used to characterize very finely both small-scale local as well as large-scale global properties of realistic noisy data. The power of the method is demonstrated on the SDSS Main Galaxy Sample by illustrating that the derived embeddings of spectra carry an unprecedented amount of information. By using a straightforward global or unsupervised variant, we observe that the main features correlate strongly with star formation rate and that they clearly separate active galactic nuclei. Computed parameters of the method can be used to describe line strengths and their interdependencies. By using a locally-biased or semi-supervised variant, we are able to focus on typical variations around specific objects of astronomical interest. We present several examples illustrating that this approach can enable new discoveries in the data as well as a detailed understanding of very fine local structure that would otherwise be overwhelmed by large-scale noise and global trends in the data. version:1
arxiv-1609-03912 | Information Theoretic Structure Learning with Confidence | http://arxiv.org/abs/1609.03912 | id:1609.03912 author:Kevin R. Moon, Morteza Noshad, Salimeh Yasaei Sekeh, Alfred O. Hero III category:cs.IT cs.LG math.IT stat.ML  published:2016-09-13 summary:Information theoretic measures (e.g. the Kullback Liebler divergence and Shannon mutual information) have been used for exploring possibly nonlinear multivariate dependencies in high dimension. If these dependencies are assumed to follow a Markov factor graph model, this exploration process is called structure discovery. For discrete-valued samples, estimates of the information divergence over the parametric class of multinomial models lead to structure discovery methods whose mean squared error achieves parametric convergence rates as the sample size grows. However, a naive application of this method to continuous nonparametric multivariate models converges much more slowly. In this paper we introduce a new method for nonparametric structure discovery that uses weighted ensemble divergence estimators that achieve parametric convergence rates and obey an asymptotic central limit theorem that facilitates hypothesis testing and other types of statistical validation. version:1
arxiv-1609-03894 | Crafting a multi-task CNN for viewpoint estimation | http://arxiv.org/abs/1609.03894 | id:1609.03894 author:Francisco Massa, Renaud Marlet, Mathieu Aubry category:cs.CV cs.LG cs.NE  published:2016-09-13 summary:Convolutional Neural Networks (CNNs) were recently shown to provide state-of-the-art results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key factors that impact performance. Followingly, we present a new joint training method with the detection task and demonstrate its benefit. We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data. By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset. In particular for their most challenging 24 view classification task we improve the results from 31.1% to 36.1% mAVP. version:1
arxiv-1609-03892 | VIPLFaceNet: An Open Source Deep Face Recognition SDK | http://arxiv.org/abs/1609.03892 | id:1609.03892 author:Xin Liu, Meina Kan, Wanglong Wu, Shiguang Shan, Xilin Chen category:cs.CV  published:2016-09-13 summary:Robust face representation is imperative to highly accurate face recognition. In this work, we propose an open source face recognition method with deep representation named as VIPLFaceNet, which is a 10-layer deep convolutional neural network with 7 convolutional layers and 3 fully-connected layers. Compared with the well-known AlexNet, our VIPLFaceNet takes only 20% training time and 60% testing time, but achieves 40\% drop in error rate on the real-world face recognition benchmark LFW. Our VIPLFaceNet achieves 98.60% mean accuracy on LFW using one single network. An open-source C++ SDK based on VIPLFaceNet is released under BSD license. The SDK takes about 150ms to process one face image in a single thread on an i7 desktop CPU. VIPLFaceNet provides a state-of-the-art start point for both academic and industrial face recognition applications. version:1
arxiv-1609-03874 | Image Decomposition Using a Robust Regression Approach | http://arxiv.org/abs/1609.03874 | id:1609.03874 author:Shervin Minaee, Yao Wang category:cs.CV  published:2016-09-13 summary:This paper considers how to separate text and/or graphics from smooth background in screen content and mixed content images and proposes an algorithm to perform this segmentation task. The proposed methods make use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. This algorithm separates the background and foreground pixels by trying to fit pixel values in the block into a smooth function using a robust regression method. The inlier pixels that can be well represented with the smooth model will be considered as background, while remaining outlier pixels will be considered foreground. We have also created a dataset of screen content images extracted from HEVC standard test sequences for screen content coding with their ground truth segmentation result which can be used for this task. The proposed algorithm has been tested on the dataset mentioned above and is shown to have superior performance over other methods, such as the hierarchical k-means clustering algorithm, shape primitive extraction and coding, and the least absolute deviation fitting scheme for foreground segmentation. version:1
arxiv-1609-03868 | Probabilistic Saliency Estimation | http://arxiv.org/abs/1609.03868 | id:1609.03868 author:Caglar Aytekin, Alexandros Iosifidis, Moncef Gabbouj category:cs.CV  published:2016-09-13 summary:In this paper, we model the salient object detection problem under a probabilistic framework encoding the boundary connectivity saliency cue and smoothness constraints in an optimization problem. We show that this problem has a closed form global optimum which estimates the salient object. We further show that along with the probabilistic framework, the proposed method also enjoys a wide range of interpretations, i.e. graph cut, diffusion maps and one-class classification. With an analysis according to these interpretations, we also find that our proposed method provides approximations to the global optimum to another criterion that integrates local/global contrast and large area saliency cues. The proposed approach achieves leading performance compared to the state-of-the-art algorithms over a large set of salient object detection datasets including around 17k images for several evaluation metrics. Furthermore, the computational complexity of the proposed method is favorable/comparable to many state-of-the-art techniques. version:1
arxiv-1609-03815 | A Unified Gender-Aware Age Estimation | http://arxiv.org/abs/1609.03815 | id:1609.03815 author:Qing Tian, Songcan Chen, Xiaoyang Tan category:cs.CV  published:2016-09-13 summary:Human age estimation has attracted increasing researches due to its wide applicability in such as security monitoring and advertisement recommendation. Although a variety of methods have been proposed, most of them focus only on the age-specific facial appearance. However, biological researches have shown that not only gender but also the aging difference between the male and the female inevitably affect the age estimation. To our knowledge, so far there have been two methods that have concerned the gender factor. The first is a sequential method which first classifies the gender and then performs age estimation respectively for classified male and female. Although it promotes age estimation performance because of its consideration on the gender semantic difference, an accumulation risk of estimation errors is unavoidable. To overcome drawbacks of the sequential strategy, the second is to regress the age appended with the gender by concatenating their labels as two dimensional output using Partial Least Squares (PLS). Although leading to promotion of age estimation performance, such a concatenation not only likely confuses the semantics between the gender and age, but also ignores the aging discrepancy between the male and the female. In order to overcome their shortcomings, in this paper we propose a unified framework to perform gender-aware age estimation. The proposed method considers and utilizes not only the semantic relationship between the gender and the age, but also the aging discrepancy between the male and the female. Finally, experimental results demonstrate not only the superiority of our method in performance, but also its good interpretability in revealing the aging discrepancy. version:1
arxiv-1609-03795 | Towards Deep Compositional Networks | http://arxiv.org/abs/1609.03795 | id:1609.03795 author:Domen Tabernik, Matej Kristan, Jeremy L. Wyatt, Ale≈° Leonardis category:cs.CV  published:2016-09-13 summary:Hierarchical feature learning based on convolutional neural networks (CNN) has recently shown significant potential in various computer vision tasks. While allowing high-quality discriminative feature learning, the downside of CNNs is the lack of explicit structure in features, which often leads to overfitting, absence of reconstruction from partial observations and limited generative abilities. Explicit structure is inherent in hierarchical compositional models, however, these lack the ability to optimize a well-defined cost function. We propose a novel analytic model of a basic unit in a layered hierarchical model with both explicit compositional structure and a well-defined discriminative cost function. Our experiments on two datasets show that the proposed compositional model performs on a par with standard CNNs on discriminative tasks, while, due to explicit modeling of the structure in the feature units, affording a straight-forward visualization of parts and faster inference due to separability of the units. Actions version:1
arxiv-1609-03777 | Character-Level Language Modeling with Hierarchical Recurrent Neural Networks | http://arxiv.org/abs/1609.03777 | id:1609.03777 author:Kyuyeon Hwang, Wonyong Sung category:cs.LG cs.CL cs.NE  published:2016-09-13 summary:Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling unseen words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different clock rates. Despite the multi-clock structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30%. version:1
arxiv-1609-03773 | Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking, and Action Recognition on Lie Groups | http://arxiv.org/abs/1609.03773 | id:1609.03773 author:Chi Xu, Lakshmi Narasimhan Govindarajan, Yu Zhang, Li Cheng category:cs.CV  published:2016-09-13 summary:Pose estimation, tracking, and action recognition of articulated objects from depth images are important and challenging problems, which are normally considered separately. In this paper, a unified paradigm based on Lie group theory is proposed, which enables us to collectively address these related problems. Our approach is also applicable to a wide range of articulated objects. Empirically it is evaluated on lab animals including mouse and fish, as well as on human hand. On these applications, it is shown to deliver competitive results compared to the state-of-the-arts, and non-trivial baselines including convolutional neural networks and regression forest methods. version:1
arxiv-1609-03772 | Learning conditional independence structure for high-dimensional uncorrelated vector processes | http://arxiv.org/abs/1609.03772 | id:1609.03772 author:Nguyen Tran Quang, Alexander Jung category:stat.ML cs.LG  published:2016-09-13 summary:We formulate and analyze a graphical model selection method for inferring the conditional independence graph of a high-dimensional nonstationary Gaussian random process (time series) from a finite-length observation. The observed process samples are assumed uncorrelated over time and having a time-varying marginal distribution. The selection method is based on testing conditional variances obtained for small subsets of process components. This allows to cope with the high-dimensional regime, where the sample size can be (drastically) smaller than the process dimension. We characterize the required sample size such that the proposed selection method is successful with high probability. version:1
arxiv-1609-03769 | Analysis of Kelner and Levin graph sparsification algorithm for a streaming setting | http://arxiv.org/abs/1609.03769 | id:1609.03769 author:Daniele Calandriello, Alessandro Lazaric, Michal Valko category:stat.ML cs.DS cs.LG  published:2016-09-13 summary:We derive a new proof to show that the incremental resparsification algorithm proposed by Kelner and Levin (2013) produces a spectral sparsifier in high probability. We rigorously take into account the dependencies across subsequent resparsifications using martingale inequalities, fixing a flaw in the original analysis. version:1
arxiv-1609-03759 | 3D Simulation for Robot Arm Control with Deep Q-Learning | http://arxiv.org/abs/1609.03759 | id:1609.03759 author:Stephen James, Edward Johns category:cs.RO cs.CV cs.LG  published:2016-09-13 summary:Intelligent control of robotic arms has huge potential over the coming years, but as of now will often fail to adapt when presented with new and unfamiliar environments. Recent trends to solve this problem have seen a shift to end-to-end solutions using deep reinforcement learning to learn policies from visual input, rather than relying on a handcrafted, modular pipeline. Building upon the recent success of deep Q-networks, we present an approach which uses three-dimensional simulations to train a 7-DOF robotic arm in a robot arm control task without any prior knowledge. Policies accept images of the environment as input and output motor actions. However, the high-dimensionality of the policies as well as the large state space makes policy search difficult. This is overcome by ensuring interesting states are explored via intermediate rewards that guide the policy towards higher reward states. Our results demonstrate that deep Q-networks can be used to learn policies for a task that involves locating a cube, grasping, and then finally lifting. The agent is able to learn to deal with a range of starting joint configurations and starting cube positions when tested in simulation. Moreover, we show that policies trained via simulation have the potential to be directly applied to real-world equivalents without any further training. We believe that robot simulations can decrease the dependency on physical robots and ultimately improve productivity of training robot control tasks. version:1
arxiv-1609-03344 | Finite-sample and asymptotic analysis of generalization ability with an application to penalized regression | http://arxiv.org/abs/1609.03344 | id:1609.03344 author:Ning Xu, Jian Hong, Timothy C. G. Fisher category:stat.ML cs.LG math.ST q-fin.EC stat.CO stat.TH  published:2016-09-12 summary:In this paper, we study the performance of extremum estimators from the perspective of generalization ability (GA): the ability of a model to predict outcomes in new samples from the same population. By adapting the classical concentration inequalities, we derive upper bounds on the empirical out-of-sample prediction errors as a function of the in-sample errors, in-sample data size, heaviness in the tails of the error distribution, and model complexity. We show that the error bounds may be used for tuning key estimation hyper-parameters, such as the number of folds $K$ in cross-validation. We also show how $K$ affects the bias-variance trade-off for cross-validation. We demonstrate that the $\mathcal{L}_2$-norm difference between penalized and the corresponding un-penalized regression estimates is directly explained by the GA of the estimates and the GA of empirical moment conditions. Lastly, we prove that all penalized regression estimates are $L_2$-consistent for both the $n \geqslant p$ and the $n < p$ cases. Simulations are used to demonstrate key results. Keywords: generalization ability, upper bound of generalization error, penalized regression, cross-validation, bias-variance trade-off, $\mathcal{L}_2$ difference between penalized and unpenalized regression, lasso, high-dimensional data. version:2
arxiv-1609-03683 | Making Neural Networks Robust to Label Noise: a Loss Correction Approach | http://arxiv.org/abs/1609.03683 | id:1609.03683 author:Giorgio Patrini, Alessandro Rozza, Aditya Menon, Richard Nock, Lizhen Qu category:stat.ML cs.LG  published:2016-09-13 summary:We present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. Our method only performs a correction on the loss function, and is agnostic to both the application domain and network architecture. We propose two procedures for loss correction: they simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 employing a diversity of architectures --- stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers --- demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise. version:1
arxiv-1609-03677 | Unsupervised Monocular Depth Estimation with Left-Right Consistency | http://arxiv.org/abs/1609.03677 | id:1609.03677 author:Cl√©ment Godard, Oisin Mac Aodha, Gabriel J. Brostow category:cs.CV  published:2016-09-13 summary:Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. By exploiting epipolar geometry constraints, we generate disparity images by training our networks with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth. version:1
arxiv-1609-03675 | Recurrent Coevolutionary Feature Embedding Processes for Recommendation | http://arxiv.org/abs/1609.03675 | id:1609.03675 author:Hanjun Dai, Yichen Wang, Rakshit Trivedi, Le Song category:cs.LG cs.IR  published:2016-09-13 summary:Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multidimensional point process model. The RNN learns a nonlinear representation of user and item features which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning the model parameters, which can readily scale up to millions of events. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts. version:1
arxiv-1609-03666 | A Greedy Algorithm to Cluster Specialists | http://arxiv.org/abs/1609.03666 | id:1609.03666 author:S√©bastien Arnold category:cs.LG stat.ML  published:2016-09-13 summary:Several recent deep neural networks experiments leverage the generalist-specialist paradigm for classification. However, no formal study compared the performance of different clustering algorithms for class assignment. In this paper we perform such a study, suggest slight modifications to the clustering procedures, and propose a novel algorithm designed to optimize the performance of of the specialist-generalist classification system. Our experiments on the CIFAR-10 and CIFAR-100 datasets allow us to investigate situations for varying number of classes on similar data. We find that our \emph{greedy pairs} clustering algorithm consistently outperforms other alternatives, while the choice of the confusion matrix has little impact on the final performance. version:1
arxiv-1609-03663 | An Experimental Study of LSTM Encoder-Decoder Model for Text Simplification | http://arxiv.org/abs/1609.03663 | id:1609.03663 author:Tong Wang, Ping Chen, Kevin Amaral, Jipeng Qiang category:cs.CL cs.LG  published:2016-09-13 summary:Text simplification (TS) aims to reduce the lexical and structural complexity of a text, while still retaining the semantic meaning. Current automatic TS techniques are limited to either lexical-level applications or manually defining a large amount of rules. Since deep neural networks are powerful models that have achieved excellent performance over many difficult tasks, in this paper, we propose to use the Long Short-Term Memory (LSTM) Encoder-Decoder model for sentence level TS, which makes minimal assumptions about word sequence. We conduct preliminary experiments to find that the model is able to learn operation rules such as reversing, sorting and replacing from sequence pairs, which shows that the model may potentially discover and apply rules such as modifying sentence structure, substituting words, and removing words for TS. version:1
arxiv-1609-03193 | Wav2Letter: an End-to-End ConvNet-based Speech Recognition System | http://arxiv.org/abs/1609.03193 | id:1609.03193 author:Ronan Collobert, Christian Puhrsch, Gabriel Synnaeve category:cs.LG cs.AI cs.CL I.2.6; I.2.7  published:2016-09-11 summary:This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform. version:2
arxiv-1609-03659 | DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images | http://arxiv.org/abs/1609.03659 | id:1609.03659 author:Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Xiang Bai, Alan Yuille category:cs.CV  published:2016-09-13 summary:Object skeletons are useful for object representation and object detection. They are complementary to the object contour, and provide extra information, such as how object scale (thickness) varies among object parts. But object skeleton extraction from natural images is very challenging, because it requires the extractor to be able to capture both local and non-local image context in order to determine the scale of each skeleton pixel. In this paper, we present a fully convolutional network with multiple scale-associated side outputs to address this problem. By observing the relationship between the receptive field sizes of the different layers in the network and the skeleton scales they can capture, we introduce two scale-associated side outputs to each stage of the network. The network is trained by multi-task learning, where one task is skeleton localization to classify whether a pixel is a skeleton pixel or not and the other is skeleton scale prediction to regress the scale of each skeleton pixel. Supervision is imposed at different stages by guiding the scale-associated side outputs toward the groundtruth skeletons at the appropriate scales. The responses of the multiple scale-associated side outputs are then fused in a scale-specific way to detect skeleton pixels using multiple scales effectively. Our method achieves promising results on two skeleton extraction datasets, and significantly outperforms other competitors. Additionally, the usefulness of the obtained skeletons and scales (thickness) are verified on two object detection applications: Foreground object segmentation and object proposal detection. version:1
arxiv-1609-02613 | Why is Differential Evolution Better than Grid Search for Tuning Defect Predictors? | http://arxiv.org/abs/1609.02613 | id:1609.02613 author:Wei Fu, Vivek Nair, Tim Menzies category:cs.SE cs.LG stat.ML  published:2016-09-08 summary:Context: One of the black arts of data mining is learning the magic parameters that control the learners. In software analytics, at least for defect prediction, several methods, like grid search and differential evolution(DE), have been proposed to learn those parameters. They've been proved to be able to improve learner performance. Objective: We want to evaluate which method that can find better parameters in terms of performance score and runtime. Methods: This paper compares grid search to differential evolution, which is an evolutionary algorithm that makes extensive use of stochastic jumps around the search space. Results: We find that the seemingly complete approach of grid search does no better, and sometimes worse, than the stochastic search. Yet, when repeated 20 times to check for conclusion validity, DE was over 210 times faster (6.2 hours vs 54 days for grid search when both tuning Random Forest over 17 test data sets with F-measure as optimzation objective). Conclusions: These results are puzzling: why does a quick partial search be just as effective as a much slower, and much more,extensive search? To answer that question, we turned to the theoretical optimization literature. Bergstra and Bengio conjecture that grid search is not more effective than more randomized searchers if the underlying search space is inherently low dimensional.This is significant since recent results show that defect prediction exhibits very low intrinsic dimensionality-an observation that explains why a fast method like DE may work as well as a seemingly more thorough grid search. This suggests, as a future research direction, that it might be possible to peek at data sets before doing any optimization in order to match the optimization algorithm to the problem at hand. version:2
arxiv-1609-03540 | ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data | http://arxiv.org/abs/1609.03540 | id:1609.03540 author:Babak Salimi, Dan Suciu category:cs.DB cs.AI cs.LG cs.PF  published:2016-09-12 summary:Causal inference from observational data is a subject of active research and development in statistics and computer science. Many toolkits have been developed for this purpose that depends on statistical software. However, these toolkits do not scale to large datasets. In this paper we describe a suite of techniques for expressing causal inference tasks from observational data in SQL. This suite supports the state-of-the-art methods for causal inference and run at scale within a database engine. In addition, we introduce several optimization techniques that significantly speedup causal inference, both in the online and offline setting. We evaluate the quality and performance of our techniques by experiments of real datasets. version:2
arxiv-1609-02993 | Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks | http://arxiv.org/abs/1609.02993 | id:1609.02993 author:Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, Soumith Chintala category:cs.AI cs.LG I.2.1; I.2.6  published:2016-09-10 summary:We consider scenarios from the real-time strategy game StarCraft as new benchmarks for reinforcement learning algorithms. We propose micromanagement tasks, which present the problem of the short-term, low-level control of army members during a battle. From a reinforcement learning point of view, these scenarios are challenging because the state-action space is very large, and because there is no obvious feature representation for the state-action evaluation function. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. In addition, we present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm allows for the collection of traces for learning using deterministic policies, which appears much more efficient than, for example, {\epsilon}-greedy exploration. Experiments show that with this algorithm, we successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle. version:2
arxiv-1609-03632 | Joint Extraction of Events and Entities within a Document Context | http://arxiv.org/abs/1609.03632 | id:1609.03632 author:Bishan Yang, Tom Mitchell category:cs.CL cs.AI  published:2016-09-12 summary:Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction. version:1
arxiv-1609-03628 | Co-active Learning to Adapt Humanoid Movement for Manipulation | http://arxiv.org/abs/1609.03628 | id:1609.03628 author:Ren Mao, John S. Baras, Yezhou Yang, Cornelia Fermuller category:cs.RO cs.LG cs.SY  published:2016-09-12 summary:In this paper we address the problem of robot movement adaptation under various environmental constraints interactively. Motion primitives are generally adopted to generate target motion from demonstrations. However, their generalization capability is weak while facing novel environments. Additionally, traditional motion generation methods do not consider the versatile constraints from various users, tasks, and environments. In this work, we propose a co-active learning framework for learning to adapt robot end-effector's movement for manipulation tasks. It is designed to adapt the original imitation trajectories, which are learned from demonstrations, to novel situations with various constraints. The framework also considers user's feedback towards the adapted trajectories, and it learns to adapt movement through human-in-the-loop interactions. The implemented system generalizes trained motion primitives to various situations with different constraints considering user preferences. Experiments on a humanoid platform validate the effectiveness of our approach. version:1
arxiv-1609-03619 | Reliable Attribute-Based Object Recognition Using High Predictive Value Classifiers | http://arxiv.org/abs/1609.03619 | id:1609.03619 author:Wentao Luan, Yezhou Yang, Cornelia Fermuller, John Baras category:cs.CV  published:2016-09-12 summary:We consider the problem of object recognition in 3D using an ensemble of attribute-based classifiers. We propose two new concepts to improve classification in practical situations, and show their implementation in an approach implemented for recognition from point-cloud data. First, the viewing conditions can have a strong influence on classification performance. We study the impact of the distance between the camera and the object and propose an approach to fuse multiple attribute classifiers, which incorporates distance into the decision making. Second, lack of representative training samples often makes it difficult to learn the optimal threshold value for best positive and negative detection rate. We address this issue, by setting in our attribute classifiers instead of just one threshold value, two threshold values to distinguish a positive, a negative and an uncertainty class, and we prove the theoretical correctness of this approach. Empirical studies demonstrate the effectiveness and feasibility of the proposed concepts. version:1
arxiv-1609-03605 | Detecting Text in Natural Image with Connectionist Text Proposal Network | http://arxiv.org/abs/1609.03605 | id:1609.03605 author:Zhi Tian, Weilin Huang, Tong He, Pan He, Yu Qiao category:cs.CV  published:2016-09-12 summary:We propose a novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps. We develop a vertical anchor mechanism that jointly predicts location and text/non-text score of each ?fixed-width proposal, considerably improving localization accuracy. The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model. This allows the CTPN to explore rich context information of image, making it powerful to detect extremely ambiguous text. The CTPN works reliably on multi-scale and multi- language text without further post-processing, departing from previous bottom-up methods requiring multi-step post-processing. It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpass- ing recent results [8, 35] by a large margin. The CTPN is computationally efficient with 0:14s/image, by using the very deep VGG16 model [27]. Online demo is available at: http://textdet.com/. version:1
arxiv-1609-03552 | Generative Visual Manipulation on the Natural Image Manifold | http://arxiv.org/abs/1609.03552 | id:1609.03552 author:Jun-Yan Zhu, Philipp Kr√§henb√ºhl, Eli Shechtman, Alexei A. Efros category:cs.CV  published:2016-09-12 summary:Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles. version:1
arxiv-1609-03545 | Dilemma First Search for Effortless Optimization of NP-Hard Problems | http://arxiv.org/abs/1609.03545 | id:1609.03545 author:Julien Weissenberg, Hayko Riemenschneider, Ralf Dragon, Luc Van Gool category:cs.DS cs.CV  published:2016-09-12 summary:To tackle the exponentiality associated with NP-hard problems, two paradigms have been proposed. First, Branch & Bound, like Dynamic Programming, achieve efficient exact inference but requires extensive information and analysis about the problem at hand. Second, meta-heuristics are easier to implement but comparatively inefficient. As a result, a number of problems have been left unoptimized and plain greedy solutions are used. We introduce a theoretical framework and propose a powerful yet simple search method called Dilemma First Search (DFS). DFS exploits the decision heuristic needed for the greedy solution for further optimization. DFS is useful when it is hard to design efficient exact inference. We evaluate DFS on two problems: First, the Knapsack problem, for which efficient algorithms exist, serves as a toy example. Second, Decision Tree inference, where state-of-the-art algorithms rely on the greedy or randomness-based solutions. We further show that decision trees benefit from optimizations that are performed in a fraction of the iterations required by a random-based search. version:1
arxiv-1609-03544 | Online Data Thinning via Multi-Subspace Tracking | http://arxiv.org/abs/1609.03544 | id:1609.03544 author:Xin Jiang, Rebecca Willett category:stat.ML cs.LG  published:2016-09-12 summary:In an era of ubiquitous large-scale streaming data, the availability of data far exceeds the capacity of expert human analysts. In many settings, such data is either discarded or stored unprocessed in datacenters. This paper proposes a method of online data thinning, in which large-scale streaming datasets are winnowed to preserve unique, anomalous, or salient elements for timely expert analysis. At the heart of this proposed approach is an online anomaly detection method based on dynamic, low-rank Gaussian mixture models. Specifically, the high-dimensional covariances matrices associated with the Gaussian components are associated with low-rank models. According to this model, most observations lie near a union of subspaces. The low-rank modeling mitigates the curse of dimensionality associated with anomaly detection for high-dimensional data, and recent advances in subspace clustering and subspace tracking allow the proposed method to adapt to dynamic environments. Furthermore, the proposed method allows subsampling, is robust to missing data, and uses a mini-batch online optimization approach. The resulting algorithms are scalable, efficient, and are capable of operating in real time. Experiments on wide-area motion imagery and e-mail databases illustrate the efficacy of the proposed approach. version:1
arxiv-1609-03541 | Comment on "Why does deep and cheap learning work so well?" [arXiv:1608.08225] | http://arxiv.org/abs/1609.03541 | id:1609.03541 author:David J. Schwab, Pankaj Mehta category:cond-mat.dis-nn cs.LG stat.ML  published:2016-09-12 summary:In a recent paper, "Why does deep and cheap learning work so well?", Lin and Tegmark claim to show that the mapping between deep belief networks and the variational renormalization group derived in [arXiv:1410.3831] is invalid, and present a "counterexample" that claims to show that this mapping does not hold. In this comment, we show that these claims are incorrect and stem from a misunderstanding of the variational RG procedure proposed by Kadanoff. We also explain why the "counterexample" of Lin and Tegmark is compatible with the mapping proposed in [arXiv:1410.3831]. version:1
arxiv-1609-03536 | A Multi-Scale Cascade Fully Convolutional Network Face Detector | http://arxiv.org/abs/1609.03536 | id:1609.03536 author:Zhenheng Yang, Ram Nevatia category:cs.CV  published:2016-09-12 summary:Face detection is challenging as faces in images could be present at arbitrary locations and in different scales. We propose a three-stage cascade structure based on fully convolutional neural networks (FCNs). It first proposes the approximate locations where the faces may be, then aims to find the accurate location by zooming on to the faces. Each level of the FCN cascade is a multi-scale fully-convolutional network, which generates scores at different locations and in different scales. A score map is generated after each FCN stage. Probable regions of face are selected and fed to the next stage. The number of proposals is decreased after each level, and the areas of regions are decreased to more precisely fit the face. Compared to passing proposals directly between stages, passing probable regions can decrease the number of proposals and reduce the cases where first stage doesn't propose good bounding boxes. We show that by using FCN and score map, the FCN cascade face detector can achieve strong performance on public datasets. version:1
arxiv-1609-03532 | Fully-Trainable Deep Matching | http://arxiv.org/abs/1609.03532 | id:1609.03532 author:James Thewlis, Shuai Zheng, Philip H. S. Torr, Andrea Vedaldi category:cs.CV  published:2016-09-12 summary:Deep Matching (DM) is a popular high-quality method for quasi-dense image matching. Despite its name, however, the original DM formulation does not yield a deep neural network that can be trained end-to-end via backpropagation. In this paper, we remove this limitation by rewriting the complete DM algorithm as a convolutional neural network. This results in a novel deep architecture for image matching that involves a number of new layer types and that, similar to recent networks for image segmentation, has a U-topology. We demonstrate the utility of the approach by improving the performance of DM by learning it end-to-end on an image matching task. version:1
arxiv-1609-03529 | Examining Representational Similarity in ConvNets and the Primate Visual Cortex | http://arxiv.org/abs/1609.03529 | id:1609.03529 author:Abhimanyu Dubey, Jayadeva, Sumeet Agarwal category:cs.CV q-bio.NC  published:2016-09-12 summary:We compare several ConvNets with different depth and regularization techniques with multi-unit macaque IT cortex recordings and assess the impact of the same on representational similarity with the primate visual cortex. We find that with increasing depth and validation performance, ConvNet features are closer to cortical IT representations. version:1
arxiv-1609-03528 | The Microsoft 2016 Conversational Speech Recognition System | http://arxiv.org/abs/1609.03528 | id:1609.03528 author:W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig category:cs.CL  published:2016-09-12 summary:We describe Microsoft's conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.3%, representing an improvement over previously reported results on this benchmark task. version:1
arxiv-1609-03519 | Optimal Encoding and Decoding for Point Process Observations: an Approximate Closed-Form Filter | http://arxiv.org/abs/1609.03519 | id:1609.03519 author:Yuval Harel, Ron Meir, Manfred Opper category:stat.ML q-bio.NC  published:2016-09-12 summary:The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensor properties, that greatly facilitate the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. Numerical comparison with particle filtering demonstrate the quality of the approximation. The analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with biological observations about the distribution of sensory cells' tuning curve centers. version:1
arxiv-1609-03500 | Hyperspectral Unmixing with Endmember Variability using Partial Membership Latent Dirichlet Allocation | http://arxiv.org/abs/1609.03500 | id:1609.03500 author:Sheng Zou, Alina Zare category:cs.CV  published:2016-09-12 summary:The application of Partial Membership Latent Dirichlet Allocation(PM-LDA) for hyperspectral endmember estimation and spectral unmixing is presented. PM-LDA provides a model for a hyperspectral image analysis that accounts for spectral variability and incorporates spatial information through the use of superpixel-based 'documents.' In our application of PM-LDA, we employ the Normal Compositional Model in which endmembers are represented as Normal distributions to account for spectral variability and proportion vectors are modeled as random variables governed by a Dirichlet distribution. The use of the Dirichlet distribution enforces positivity and sum-to-one constraints on the proportion values. Algorithm results on real hyperspectral data indicate that PM-LDA produces endmember distributions that represent the ground truth classes and their associated variability. version:1
arxiv-1609-03490 | Transfer String Kernel for Cross-Context DNA-Protein Binding Prediction | http://arxiv.org/abs/1609.03490 | id:1609.03490 author:Ritambhara Singh, Jack Lanchantin, Gabriel Robins, Yanjun Qi category:cs.LG  published:2016-09-12 summary:Through sequence-based classification, this paper tries to accurately predict the DNA binding sites of transcription factors (TFs) in an unannotated cellular context. Related methods in the literature fail to perform such predictions accurately, since they do not consider sample distribution shift of sequence segments from an annotated (source) context to an unannotated (target) context. We, therefore, propose a method called "Transfer String Kernel" (TSK) that achieves improved prediction of transcription factor binding site (TFBS) using knowledge transfer via cross-context sample adaptation. TSK maps sequence segments to a high-dimensional feature space using a discriminative mismatch string kernel framework. In this high-dimensional space, labeled examples of the source context are re-weighted so that the revised sample distribution matches the target context more closely. We have experimentally verified TSK for TFBS identifications on fourteen different TFs under a cross-organism setting. We find that TSK consistently outperforms the state-of the-art TFBS tools, especially when working with TFs whose binding sequences are not conserved across contexts. We also demonstrate the generalizability of TSK by showing its cutting-edge performance on a different set of cross-context tasks for the MHC peptide binding predictions. version:1
arxiv-1609-03448 | Learning Sparse Graphs Under Smoothness Prior | http://arxiv.org/abs/1609.03448 | id:1609.03448 author:Sundeep Prabhakar Chepuri, Sijia Liu, Geert Leus, Alfred O. Hero III category:cs.LG  published:2016-09-12 summary:In this paper, we are interested in learning the underlying graph structure behind training data. Solving this basic problem is essential to carry out any graph signal processing or machine learning task. To realize this, we assume that the data is smooth with respect to the graph topology, and we parameterize the graph topology using an edge sampling function. That is, the graph Laplacian is expressed in terms of a sparse edge selection vector, which provides an explicit handle to control the sparsity level of the graph. We solve the sparse graph learning problem given some training data in both the noiseless and noisy settings. Given the true smooth data, the posed sparse graph learning problem can be solved optimally and is based on simple rank ordering. Given the noisy data, we show that the joint sparse graph learning and denoising problem can be simplified to designing only the sparse edge selection vector, which can be solved using convex optimization. version:1
arxiv-1609-03441 | Read, Tag, and Parse All at Once, or Fully-neural Dependency Parsing | http://arxiv.org/abs/1609.03441 | id:1609.03441 author:Jan Chorowski, Micha≈Ç Zapotoczny, Pawe≈Ç Rychlikowski category:cs.CL  published:2016-09-12 summary:We present a dependency parser implemented as a single deep neural network that reads orthographic representations of words and directly generates dependencies and their labels. Unlike typical approaches to parsing, the model doesn't require part-of-speech (POS) tagging of the sentences. With proper regularization and additional supervision achieved with multitask learning we reach state-of-the-art performance on Slavic languages from the Universal Dependencies treebank: with no linguistic features other than characters, our parser is as accurate as a transition- based system trained on perfect POS tags. version:1
arxiv-1609-03415 | Active Canny: Edge Detection and Recovery with Open Active Contour Models | http://arxiv.org/abs/1609.03415 | id:1609.03415 author:Muhammet Bastan, S. Saqib Bukhari, Thomas M. Breuel category:cs.CV cs.MM  published:2016-09-12 summary:We introduce an edge detection and recovery framework based on open active contour models (snakelets). This is motivated by the noisy or broken edges output by standard edge detection algorithms, like Canny. The idea is to utilize the local continuity and smoothness cues provided by strong edges and grow them to recover the missing edges. This way, the strong edges are used to recover weak or missing edges by considering the local edge structures, instead of blindly linking them if gradient magnitudes are above some threshold. We initialize short snakelets on the gradient magnitudes or binary edges automatically and then deform and grow them under the influence of gradient vector flow. The output snakelets are able to recover most of the breaks or weak edges, and they provide a smooth edge representation of the image; they can also be used for higher level analysis, like contour segmentation. version:1
arxiv-1609-03396 | FALCON: Feature Driven Selective Classification for Energy-Efficient Image Recognition | http://arxiv.org/abs/1609.03396 | id:1609.03396 author:Priyadarshini Panda, Aayush Ankit, Parami Wijesinghe, Kaushik Roy category:cs.CV  published:2016-09-12 summary:Machine-learning algorithms have shown outstanding image recognition or classification performance for computer vision applications. However, the compute and energy requirement for implementing such classifier models for large-scale problems is quite high. In this paper, we propose Feature Driven Selective Classification (FALCON) inspired by the biological visual attention mechanism in the brain to optimize the energy-efficiency of machine-learning classifiers. We use the consensus in the characteristic features (color/texture) across images in a dataset to decompose the original classification problem and construct a tree of classifiers (nodes) with a generic-to-specific transition in the classification hierarchy. The initial nodes of the tree separate the instances based on feature information and selectively enable the latter nodes to perform object specific classification. The proposed methodology allows selective activation of only those branches and nodes of the classification tree that are relevant to the input while keeping the remaining nodes idle. Additionally, we propose a programmable and scalable Neuromorphic Engine (NeuE) that utilizes arrays of specialized neural computational elements to execute the FALCON based classifier models for diverse datasets. The structure of FALCON facilitates the reuse of nodes while scaling up from small classification problems to larger ones thus allowing us to construct classifier implementations that are significantly more efficient. We evaluate our approach for a 12-object classification task on the Caltech101 dataset and 10-object task on CIFAR-10 dataset by constructing FALCON models on the NeuE platform in 45nm technology. In addition to energy efficiency, FALCON shows an improvement in training time of up to 1.96x as compared to the traditional classification approach. version:1
arxiv-1609-03376 | Morphological Constraints for Phrase Pivot Statistical Machine Translation | http://arxiv.org/abs/1609.03376 | id:1609.03376 author:Ahmed El Kholy, Nizar Habash category:cs.CL  published:2016-09-12 summary:The lack of parallel data for many language pairs is an important challenge to statistical machine translation (SMT). One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations especially when a poor morphology language is used as the pivot between rich morphology languages. In this paper, we examine the use of synchronous morphology constraint features to improve the quality of phrase pivot SMT. We compare hand-crafted constraints to those learned from limited parallel data between source and target languages. The learned morphology constraints are based on projected align- ments between the source and target phrases in the pivot phrase table. We show positive results on Hebrew-Arabic SMT (pivoting on English). We get 1.5 BLEU points over a phrase pivot baseline and 0.8 BLEU points over a system combination baseline with a direct model built from parallel data. version:1
arxiv-1609-03357 | Modelling Creativity: Identifying Key Components through a Corpus-Based Approach | http://arxiv.org/abs/1609.03357 | id:1609.03357 author:Anna Jordanous, Bill Keller category:cs.CL cs.AI  published:2016-09-12 summary:Creativity is a complex, multi-faceted concept encompassing a variety of related aspects, abilities, properties and behaviours. If we wish to study creativity scientifically, then a tractable and well-articulated model of creativity is required. Such a model would be of great value to researchers investigating the nature of creativity and in particular, those concerned with the evaluation of creative practice. This paper describes a unique approach to developing a suitable model of how creative behaviour emerges that is based on the words people use to describe the concept. Using techniques from the field of statistical natural language processing, we identify a collection of fourteen key components of creativity through an analysis of a corpus of academic papers on the topic. Words are identified which appear significantly often in connection with discussions of the concept. Using a measure of lexical similarity to help cluster these words, a number of distinct themes emerge, which collectively contribute to a comprehensive and multi-perspective model of creativity. The components provide an ontology of creativity: a set of building blocks which can be used to model creative practice in a variety of domains. The components have been employed in two case studies to evaluate the creativity of computational systems and have proven useful in articulating achievements of this work and directions for further research. version:1
arxiv-1609-03333 | On Generation of Time-based Label Refinements | http://arxiv.org/abs/1609.03333 | id:1609.03333 author:Niek Tax, Emin Alasgarov, Natalia Sidorova, Reinder Haakma category:stat.ME cs.AI stat.ML  published:2016-09-12 summary:Process mining is a research field focused on the analysis of event data with the aim of extracting insights in processes. Applying process mining techniques on data from smart home environments has the potential to provide valuable insights in (un)healthy habits and to contribute to ambient assisted living solutions. Finding the right event labels to enable application of process mining techniques is however far from trivial, as simply using the triggering sensor as the label for sensor events results in uninformative models that allow for too much behavior (overgeneralizing). Refinements of sensor level event labels suggested by domain experts have shown to enable discovery of more precise and insightful process models. However, there exist no automated approach to generate refinements of event labels in the context of process mining. In this paper we propose a framework for automated generation of label refinements based on the time attribute of events. We show on a case study with real life smart home event data that behaviorally more specific, and therefore more insightful, process models can be found by using automatically generated refined labels in process discovery. version:1
arxiv-1609-03323 | Sensor-based Gait Parameter Extraction with Deep Convolutional Neural Networks | http://arxiv.org/abs/1609.03323 | id:1609.03323 author:Julius Hannink, Thomas Kautz, Cristian F. Pasluosta, Jochen Klucken, Bjoern M. Eskofier category:cs.LG  published:2016-09-12 summary:Measurement of stride-related, biomechanical parameters is the common rationale for objective gait impairment scoring. State-of-the-art double integration approaches to extract these parameters from inertial sensor data are, however, limited in their clinical applicability due to the underlying assumptions. To overcome this, we present a method to translate the abstract information provided by wearable sensors to context-related expert features based on deep convolutional neural networks. Regarding mobile gait analysis, this enables integration-free and data-driven extraction of a set of 8 spatio-temporal stride parameters. To this end, two modelling approaches are compared: A combined network estimating all parameters of interest and an ensemble approach that spawns less complex networks for each parameter individually. The ensemble approach is outperforming the combined modelling in the current application. On a clinically relevant and publicly available benchmark dataset, we estimate stride length, width and medio-lateral change in foot angle up to ${-0.15\pm6.09}$ cm, ${-0.09\pm4.22}$ cm and ${0.13 \pm 3.78^\circ}$ respectively. Stride, swing and stance time as well as heel and toe contact times are estimated up to ${\pm 0.07}$, ${\pm0.05}$, ${\pm 0.07}$, ${\pm0.07}$ and ${\pm0.12}$ s respectively. This is comparable to and in parts outperforming or defining state-of-the-art. Our results further indicate that the proposed change in methodology could substitute assumption-driven double-integration methods and enable mobile assessment of spatio-temporal stride parameters in clinically critical situations as e.g. in the case of spastic gait impairments. version:1
arxiv-1609-03321 | Stride Length Estimation with Deep Learning | http://arxiv.org/abs/1609.03321 | id:1609.03321 author:Julius Hannink, Thomas Kautz, Cristian F. Pasluosta, Jens Barth, Samuel Sch√ºlein, Karl-G√ºnter Ga√ümann, Jochen Klucken, Bjoern M. Eskofier category:cs.LG  published:2016-09-12 summary:Accurate estimation of spatial gait characteristics is critical to assess motor impairments resulting from neurological or musculoskeletal disease. Currently, however, methodological constraints limit clinical applicability of state-of-the-art double integration approaches to gait patterns with a clear zero-velocity phase. We describe a novel approach to stride length estimation that uses deep convolutional neural networks to map stride-specific inertial sensor data to the resulting stride length. The model is trained on a publicly available and clinically relevant benchmark dataset consisting of 1220 strides from 101 geriatric patients. Evaluation is done in a 10-fold cross validation and for three different stride definitions. Even though best results are achieved with strides defined from mid-stance to mid-stance with average accuracy and precision of 0.01 $\pm$ 5.37 cm, performance does not strongly depend on stride definition. The achieved precision outperforms state-of-the-art methods evaluated on this benchmark dataset by 3.0 cm (36%). Due to the independence of stride definition, the proposed method is not subject to the methodological constrains that limit applicability of state-of-the-art double integration methods. Furthermore, precision on the benchmark dataset could be improved. With more precise mobile stride length estimation, new insights to the progression of neurological disease or early indications might be gained. Due to the independence of stride definition, previously uncharted diseases in terms of mobile gait analysis can now be investigated by re-training and applying the proposed method. version:1
arxiv-1609-03319 | CompAdaGrad: A Compressed, Complementary, Computationally-Efficient Adaptive Gradient Method | http://arxiv.org/abs/1609.03319 | id:1609.03319 author:Nishant A. Mehta, Alistair Rendell, Anish Varghese, Christfried Webers category:cs.LG stat.ML  published:2016-09-12 summary:The adaptive gradient online learning method known as AdaGrad has seen widespread use in the machine learning community in stochastic and adversarial online learning problems and more recently in deep learning methods. The method's full-matrix incarnation offers much better theoretical guarantees and potentially better empirical performance than its diagonal version; however, this version is computationally prohibitive and so the simpler diagonal version often is used in practice. We introduce a new method, CompAdaGrad, that navigates the space between these two schemes and show that this method can yield results much better than diagonal AdaGrad while avoiding the (effectively intractable) $O(n^3)$ computational complexity of full-matrix AdaGrad for dimension $n$. CompAdaGrad essentially performs full-matrix regularization in a low-dimensional subspace while performing diagonal regularization in the complementary subspace. We derive CompAdaGrad's updates for composite mirror descent in case of the squared $\ell_2$ norm and the $\ell_1$ norm, demonstrate that its complexity per iteration is linear in the dimension, and establish guarantees for the method independent of the choice of composite regularizer. Finally, we show preliminary results on several datasets. version:1
arxiv-1609-03302 | Image denoising via group sparsity residual constraint | http://arxiv.org/abs/1609.03302 | id:1609.03302 author:Zhiyuan Zha, Xin Liu, Ziheng Zhou, Xiaohua Huang, Jingang Shi, Zhenhong Shang, Lan Tang, Yechao Bai, Qiong Wang, Xinggan Zhang category:cs.CV  published:2016-09-12 summary:Group sparsity has shown great potential in various low-level vision tasks (e.g, image denoising, deblurring and inpainting). In this paper, we propose a new prior model for image denoising via group sparsity residual constraint (GSRC). To enhance the performance of group sparse-based image denoising, the concept of group sparsity residual is proposed, and thus, the problem of image denoising is translated into one that reduces the group sparsity residual. To reduce the residual, we first obtain some good estimation of the group sparse coefficients of the original image by the first-pass estimation of noisy image, and then centralize the group sparse coefficients of noisy image to the estimation. Experimental results have demonstrated that the proposed method not only outperforms many state-of-the-art denoising methods such as BM3D and WNNM, but results in a faster speed. version:1
arxiv-1609-03286 | Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks | http://arxiv.org/abs/1609.03286 | id:1609.03286 author:Yun-Nung Chen, Dilek Hakkani-Tur, Gokhan Tur, Asli Celikyilmaz, Jianfeng Gao, Li Deng category:cs.AI cs.CL  published:2016-09-12 summary:Natural language understanding (NLU) is a core component of a spoken dialogue system. Recently recurrent neural networks (RNN) obtained strong results on NLU due to their superior ability of preserving sequential information over time. Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks (K-SAN), a generalization of RNN to additionally incorporate non-flat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved. The experiments on the benchmark Air Travel Information System (ATIS) data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the performance of the state-of-the-art neural network based frameworks. version:1
arxiv-1609-03279 | Semi-Supervised Sparse Representation Based Classification for Face Recognition with Insufficient Labeled Samples | http://arxiv.org/abs/1609.03279 | id:1609.03279 author:Yuan Gao, Jiayi Ma, Alan L. Yuille category:cs.CV  published:2016-09-12 summary:This paper addresses the problem of face recognition when there is only few, or even only a single, labeled examples of the face that we wish to recognize. Moreover, these examples are typically corrupted by nuisance variables, both linear (i.e. additive nuisance variables such as bad lighting, wearing of glasses) and non-linear (i.e. non-additive pixel-wise nuisance variables such as expression changes). The small number of labeled examples means that it is hard to remove these nuisance variables between the training and testing faces to obtain good recognition performance. To address the problem we propose a method called Semi-Supervised Sparse Representation based Classification (S$^3$RC). This is based on recent work on sparsity where faces are represented in terms of two dictionaries: a gallery dictionary consisting of one or more examples of each person, and a variation dictionary representing linear nuisance variables (e.g. different lighting conditions, different glasses). The main idea is that (i) we use the variation dictionary to characterize the linear nuisance variables via the sparsity framework, then (ii) prototype face images are estimated as a gallery dictionary via a Gaussian Mixture Model (GMM), with mixed labeled and unlabeled samples in a semi-supervised manner, to deal with the non-linear nuisance variations between labeled and unlabeled samples. We have done experiments with insufficient labeled samples, even when there is only a single labeled sample per person. Our results on the AR, Multi-PIE, CAS-PEAL, and LFW databases demonstrate that the proposed method is able to deliver significantly improved performance over existing methods. version:1
arxiv-1609-03277 | Segmentation and Classification of Skin Lesions for Disease Diagnosis | http://arxiv.org/abs/1609.03277 | id:1609.03277 author:Sumithra R, Mahamad Suhil, D. S. Guru category:cs.CV  published:2016-09-12 summary:In this paper, a novel approach for automatic segmentation and classification of skin lesions is proposed. Initially, skin images are filtered to remove unwanted hairs and noise and then the segmentation process is carried out to extract lesion areas. For segmentation, a region growing method is applied by automatic initialization of seed points. The segmentation performance is measured with different well known measures and the results are appreciable. Subsequently, the extracted lesion areas are represented by color and texture features. SVM and k-NN classifiers are used along with their fusion for the classification using the extracted features. The performance of the system is tested on our own dataset of 726 samples from 141 images consisting of 5 different classes of diseases. The results are very promising with 46.71% and 34% of F-measure using SVM and k-NN classifier respectively and with 61% of F-measure for fusion of SVM and k-NN. version:1
arxiv-1609-03261 | Less than a Single Pass: Stochastically Controlled Stochastic Gradient Method | http://arxiv.org/abs/1609.03261 | id:1609.03261 author:Lihua Lei, Michael I. Jordan category:math.OC cs.DS cs.LG stat.ML  published:2016-09-12 summary:We develop and analyze a procedure for gradient-based optimization that we refer to as stochastically controlled stochastic gradient (SCSG). As a member of the SVRG family of algorithms, SCSG makes use of gradient estimates at two scales. Unlike most existing algorithms in this family, both the computation cost and the communication cost of SCSG do not necessarily scale linearly with the sample size n; indeed, these costs are independent of n when the target accuracy is low. An experimental evaluation of SCSG on the MNIST dataset shows that it can yield accurate results on this dataset on a single commodity machine with a memory footprint of only 2.6MB and only eight disk accesses. version:1
arxiv-1609-03240 | Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach | http://arxiv.org/abs/1609.03240 | id:1609.03240 author:Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT math.OC  published:2016-09-12 summary:We consider the non-square matrix sensing problem, under restricted isometry property (RIP) assumptions. We focus on the non-convex formulation, where any rank-$r$ matrix $X \in \mathbb{R}^{m \times n}$ is represented as $UV^\top$, where $U \in \mathbb{R}^{m \times r}$ and $V \in \mathbb{R}^{n \times r}$. In this paper, we complement recent findings on the non-convex geometry of the analogous PSD setting [5], and show that matrix factorization does not introduce any spurious local minima, under RIP assumptions. version:1
arxiv-1609-03228 | Supervised multiway factorization | http://arxiv.org/abs/1609.03228 | id:1609.03228 author:Eric F. Lock, Gen Li category:stat.ME stat.ML  published:2016-09-11 summary:We describe a probabilistic PARAFAC/CANDECOMP (CP) factorization for multiway (i.e., tensor) data that incorporates auxiliary covariates, SupCP. SupCP generalizes the supervised singular value decomposition (SupSVD) for vector-valued observations, to allow for observations that have the form of a matrix or higher-order array. Such data are increasingly encountered in biomedical research and other fields. We describe a likelihood-based latent variable representation of the CP factorization, in which the latent variables are informed by additional covariates. We give conditions for identifiability, and develop an EM algorithm for simultaneous estimation of all model parameters. SupCP can be used for dimension reduction, capturing latent structures that are more accurate and interpretable due to covariate supervision. Moreover, SupCP specifies a full probability distribution for a multiway data observation with given covariate values, which can be used for predictive modeling. We conduct comprehensive simulations to evaluate the SupCP algorithm, and we apply it to a facial image database with facial descriptors (e.g., smiling / not smiling) as covariates. Software is available at https://github.com/lockEF/SupCP . version:1
arxiv-1609-03207 | Multiplex lexical networks reveal patterns in early word acquisition in children | http://arxiv.org/abs/1609.03207 | id:1609.03207 author:Massimo Stella, Nicole M. Beckage, Markus Brede category:physics.soc-ph cond-mat.dis-nn cs.CL cs.LG  published:2016-09-11 summary:Network models of language have provided a way of linking cognitive processes to the structure and connectivity of language. However, one shortcoming of current approaches is focusing on only one type of linguistic relationship at a time, missing the complex multi-relational nature of language. In this work, we overcome this limitation by modelling the mental lexicon of English-speaking toddlers as a multiplex lexical network, i.e. a multi-layered network where N=529 words/nodes are connected according to four types of relationships: (i) free associations, (ii) feature sharing, (iii) co-occurrence, and (iv) phonological similarity. We provide analysis of the topology of the resulting multiplex and then proceed to evaluate single layers as well as the full multiplex structure on their ability to predict empirically observed age of acquisition data of English speaking toddlers. We find that the emerging multiplex network topology is an important proxy of the cognitive processes of acquisition, capable of capturing emergent lexicon structure. In fact, we show that the multiplex topology is fundamentally more powerful than individual layers in predicting the ordering with which words are acquired. Furthermore, multiplex analysis allows for a quantification of distinct phases of lexical acquisition in early learners: while initially all the multiplex layers contribute to word learning, after about month 23 free associations take the lead in driving word acquisition. version:1
arxiv-1609-03205 | Unsupervised Identification of Translationese | http://arxiv.org/abs/1609.03205 | id:1609.03205 author:Ella Rabinovich, Shuly Wintner category:cs.CL  published:2016-09-11 summary:Translated texts are distinctively different from original ones, to the extent that supervised text classification methods can distinguish between them with high accuracy. These differences were proven useful for statistical machine translation. However, it has been suggested that the accuracy of translation detection deteriorates when the classifier is evaluated outside the domain it was trained on. We show that this is indeed the case, in a variety of evaluation scenarios. We then show that unsupervised classification is highly accurate on this task. We suggest a method for determining the correct labels of the clustering outcomes, and then use the labels for voting, improving the accuracy even further. Moreover, we suggest a simple method for clustering in the challenging case of mixed-domain datasets, in spite of the dominance of domain-related features over translation-related ones. The result is an effective, fully-unsupervised method for distinguishing between original and translated texts that can be applied to new domains with reasonable accuracy. version:1
arxiv-1609-03204 | On the Similarities Between Native, Non-native and Translated Texts | http://arxiv.org/abs/1609.03204 | id:1609.03204 author:Ella Rabinovich, Sergiu Nisioi, Noam Ordan, Shuly Wintner category:cs.CL  published:2016-09-11 summary:We present a computational analysis of three language varieties: native, advanced non-native, and translation. Our goal is to investigate the similarities and differences between non-native language productions and translations, contrasting both with native language. Using a collection of computational methods we establish three main results: (1) the three types of texts are easily distinguishable; (2) non-native language and translations are closer to each other than each of them is to native language; and (3) some of these characteristics depend on the source or native language, while others do not, reflecting, perhaps, unified principles that similarly affect translations and non-native language. version:1
arxiv-1609-03164 | On the Relationship between Online Gaussian Process Regression and Kernel Least Mean Squares Algorithms | http://arxiv.org/abs/1609.03164 | id:1609.03164 author:Steven Van Vaerenbergh, Jesus Fernandez-Bes, V√≠ctor Elvira category:stat.ML cs.IT cs.LG math.IT  published:2016-09-11 summary:We study the relationship between online Gaussian process (GP) regression and kernel least mean squares (KLMS) algorithms. While the latter have no capacity of storing the entire posterior distribution during online learning, we discover that their operation corresponds to the assumption of a fixed posterior covariance that follows a simple parametric model. Interestingly, several well-known KLMS algorithms correspond to specific cases of this model. The probabilistic perspective allows us to understand how each of them handles uncertainty, which could explain some of their performance differences. version:1
arxiv-1609-03148 | Divide and...conquer? On the limits of algorithmic approaches to syntactic semantic structure | http://arxiv.org/abs/1609.03148 | id:1609.03148 author:Diego Gabriel Krivochen category:cs.CL cs.FL 68Q42  published:2016-09-11 summary:In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion. A D&C algorithm works by recursively and monotonically breaking down a problem into sub problems of the same (or a related) type, until these become simple enough to be solved directly. The solutions to the sub problems are then combined to give a solution to the original problem. The present work identifies D&C algorithms assumed within contemporary syntactic theory, and discusses the limits of their applicability in the realms of the syntax semantics and syntax morphophonology interfaces. We will propose that D&C algorithms, while valid for some processes, fall short on flexibility given a mixed approach to the structure of linguistic phrase markers. Arguments in favour of a computationally mixed approach to linguistic structure will be presented as an alternative that offers advantages to uniform D&C approaches. version:1
arxiv-1609-03140 | Learning Semantic Part-Based Models from Google Images | http://arxiv.org/abs/1609.03140 | id:1609.03140 author:Davide Modolo, Vittorio Ferrari category:cs.CV  published:2016-09-11 summary:We propose a technique to train semantic part-based models of object classes from Google Images. Our models encompass the appearance of parts and their spatial arrangement on the object, specific to each viewpoint. We learn these rich models by collecting training instances for both parts and objects, and automatically connecting the two levels. Our framework works incrementally, by learning from easy examples first, and then gradually adapting to harder ones. A key benefit of this approach is that it requires no manual part location annotations. We evaluate our models on the challenging PASCAL-Part dataset and show how their performance increases at every step of the learning, with the final models more than doubling the performance of directly training from images retrieved by querying for part names (from 13.5 to 29.9 AP). Moreover, we show that our part models can help object detection performance by enriching the R-CNN detector with parts. version:1
arxiv-1609-02226 | Fitted Learning: Models with Awareness of their Limits | http://arxiv.org/abs/1609.02226 | id:1609.02226 author:Navid Kardan, Kenneth O. Stanley category:cs.AI cs.LG cs.NE  published:2016-09-07 summary:Though deep learning has pushed the boundaries of classification forward, in recent years hints of the limits of standard classification have begun to emerge. Problems such as fooling, adding new classes over time, and the need to retrain learning models only for small changes to the original problem all point to a potential shortcoming in the classic classification regime, where a comprehensive a priori knowledge of the possible classes or concepts is critical. Without such knowledge, classifiers misjudge the limits of their knowledge and overgeneralization therefore becomes a serious obstacle to consistent performance. In response to these challenges, this paper extends the classic regime by reframing classification instead with the assumption that concepts present in the training set are only a sample of the hypothetical final set of concepts. To bring learning models into this new paradigm, a novel elaboration of standard architectures called the competitive overcomplete output layer (COOL) neural network is introduced. Experiments demonstrate the effectiveness of COOL by applying it to fooling, separable concept learning, one-class neural networks, and standard classification benchmarks. The results suggest that, unlike conventional classifiers, the amount of generalization in COOL networks can be tuned to match the problem. version:2
arxiv-1609-03093 | Noisy video classification with Spatial Pooler of Hierarchical Temporal Memory | http://arxiv.org/abs/1609.03093 | id:1609.03093 author:Maciej Wielgosz, Marcin Pietro≈Ñ category:cs.CV  published:2016-09-10 summary:This paper examines the performance of a Spatial Pooler (SP) of a Hierarchical Temporal Memory (HTM) in the task of noisy object recognition. To address this challenge, a dedicated custom-designed system based on the SP, histogram calculation module and SVM classifier was implemented. In addition to implementing their own version of HTM, the authors also designed a profiler which is capable of tracing all of the key parameters of the system. This was necessary, since an analysis and monitoring of the system performance turned out to be extremely difficult using conventional testing and debugging tools. The system was initially trained on artificially prepared videos without noise and then tested with a set of noisy video streams. This approach was intended to mimic a real life scenario where an agent or a system trained to deal with ideal objects faces a task of classifying distorted and noisy ones in its regular working conditions. The authors conduced a series of experiments for various macro parameters of HTM SP, as well as for different levels of video reduction ratios. The experiments allowed them to evaluate the performance of two different system setups (i.e. 'Multiple HTMs' and 'Single HTM') under various noise conditions with 32--frame video files. Results of all the tests were compared to SVM baseline setup. It was determined that the system featuring SP is capable of achieving approximately 12 times the noise reduction for a video signal with with distorted bits accounting for 13\% of the total. Furthermore, the system featuring SP performed better also in the experiments without a noise component and achieved a max F1 score of 0.96. version:1
arxiv-1609-03068 | Multiplex visibility graphs to investigate recurrent neural networks dynamics | http://arxiv.org/abs/1609.03068 | id:1609.03068 author:Filippo Maria Bianchi, Lorenzo Livi, Cesare Alippi, Robert Jenssen category:cs.NE math.DS  published:2016-09-10 summary:A recurrent neural network (RNN) is a universal approximator of dynamical systems, whose performance often depends on sensitive hyperparameters. Tuning of such hyperparameters may be difficult and, typically, based on a trial-and-error approach. In this work, we adopt a graph-based framework to interpret and characterize the internal RNN dynamics. Through this insight, we are able to design a principled unsupervised method to derive configurations with maximized performances, in terms of prediction error and memory capacity. In particular, we propose to model time series of neurons activations with the recently introduced horizontal visibility graphs, whose topological properties reflect important dynamical features of the underlying dynamic system. Successively, each graph becomes a layer of a larger structure, called multiplex. We show that topological properties of such a multiplex reflect important features of RNN dynamics and are used to guide the tuning procedure. To validate the proposed method, we consider a class of RNNs called echo state networks. We perform experiments and discuss results on several benchmarks and real-world dataset of call data records. version:1
arxiv-1609-01329 | Depth Reconstruction and Computer-Aided Polyp Detection in Optical Colonoscopy Video Frames | http://arxiv.org/abs/1609.01329 | id:1609.01329 author:Saad Nadeem, Arie Kaufman category:cs.CV  published:2016-09-05 summary:We present a computer-aided detection algorithm for polyps in optical colonoscopy images. Polyps are the precursors to colon cancer. In the US alone, more than 14 million optical colonoscopies are performed every year, mostly to screen for polyps. Optical colonoscopy has been shown to have an approximately 25% polyp miss rate due to the convoluted folds and bends present in the colon. In this work, we present an automatic detection algorithm to detect these polyps in the optical colonoscopy images. We use a machine learning algorithm to infer a depth map for a given optical colonoscopy image and then use a detailed pre-built polyp profile to detect and delineate the boundaries of polyps in this given image. We have achieved the best recall of 84.0% and the best specificity value of 83.4%. version:2
arxiv-1609-04375 | A Perspective on Deep Imaging | http://arxiv.org/abs/1609.04375 | id:1609.04375 author:Ge Wang category:q-bio.QM cs.CV cs.LG  published:2016-09-10 summary:The combination of medical imaging and deep learning promises to empower not only image analysis but also image reconstruction. The latter perspective is considered in this perspective article to develop low- and high-hanging image reconstruction techniques. This might lead to intelligent utilization of domain knowledge in terms of big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications but major challenges must be addressed. version:1
arxiv-1609-03058 | A Tube-and-Droplet-based Approach for Representing and Analyzing Motion Trajectories | http://arxiv.org/abs/1609.03058 | id:1609.03058 author:Weiyao Lin, Yang Zhou, Hongteng Xu, Junchi Yan, Mingliang Xu, Jianxin Wu, Zicheng Liu category:cs.CV cs.AI cs.MM  published:2016-09-10 summary:Trajectory analysis is essential in many applications. In this paper, we address the problem of representing motion trajectories in a highly informative way, and consequently utilize it for analyzing trajectories. Our approach first leverages the complete information from given trajectories to construct a thermal transfer field which provides a context-rich way to describe the global motion pattern in a scene. Then, a 3D tube is derived which depicts an input trajectory by integrating its surrounding motion patterns contained in the thermal transfer field. The 3D tube effectively: 1) maintains the movement information of a trajectory, 2) embeds the complete contextual motion pattern around a trajectory, 3) visualizes information about a trajectory in a clear and unified way. We further introduce a droplet-based process. It derives a droplet vector from a 3D tube, so as to characterize the high-dimensional 3D tube information in a simple but effective way. Finally, we apply our tube-and-droplet representation to trajectory analysis applications including trajectory clustering, trajectory classification & abnormality detection, and 3D action recognition. Experimental comparisons with state-of-the-art algorithms demonstrate the effectiveness of our approach. version:1
arxiv-1609-03056 | Sequential Deep Trajectory Descriptor for Action Recognition with Three-stream CNN | http://arxiv.org/abs/1609.03056 | id:1609.03056 author:Yemin Shi, Yonghong Tian, Yaowei Wang, Tiejun Huang category:cs.CV  published:2016-09-10 summary:Learning the spatial-temporal representation of motion information is crucial to human action recognition. Nevertheless, most of the existing features or descriptors cannot capture motion information effectively, especially for long-term motion. To address this problem, this paper proposes a long-term motion descriptor called sequential Deep Trajectory Descriptor (sDTD). Specifically, we project dense trajectories into two-dimensional planes, and subsequently a CNN-RNN network is employed to learn an effective representation for long-term motion. Unlike the popular two-stream ConvNets, the sDTD stream is introduced into a three-stream framework so as to identify actions from a video sequence. Consequently, this three-stream framework can simultaneously capture static spatial features, short-term motion and long-term motion in that video. Extensive experiments were conducted on three challenging datasets: KTH, HMDB51 and UCF101. Experimental results show that our method achieves state-of-the-art performance on the KTH and UCF101 datasets, and is comparable to the state-of-the-art methods on the HMDB51 dataset. version:1
arxiv-1609-03054 | New Steps on the Exact Learning of CNF | http://arxiv.org/abs/1609.03054 | id:1609.03054 author:Montserrat Hermo, Ana Ozaki category:cs.LG  published:2016-09-10 summary:A major problem in computational learning theory is whether the class of formulas in conjunctive normal form (CNF) is efficiently learnable. Although it is known that this class cannot be polynomially learned using either membership or equivalence queries alone, it is open whether CNF can be polynomially learned using both types of queries. One of the most important results concerning a restriction of the class CNF is that propositional Horn formulas are polynomial time learnable in Angluin's exact learning model with membership and equivalence queries. In this work we push this boundary and show that the class of multivalued dependency formulas (MVDF) is polynomially learnable from interpretations. We then provide a notion of reduction between learning problems in Angluin's model, showing that a transformation of the algorithm suffices to efficiently learn multivalued database dependencies from data relations. We also show via reductions that our main result extends well known previous results and allows us to find alternative solutions for them. version:1
arxiv-1609-03024 | Rectifier Neural Network with a Dual-Pathway Architecture for Image Denoising | http://arxiv.org/abs/1609.03024 | id:1609.03024 author:Keting Zhang, Liqing Zhang category:cs.CV  published:2016-09-10 summary:Recently deep neural networks based on tanh activation function have shown their impressive power in image denoising. However, much training time is needed because of their very large size. In this letter, we propose a dual-pathway rectifier neural network by combining two rectifier neurons with reversed input and output weights in the same hidden layer. We drive the equivalent activation function and illustrate that it improves the efficiency of capturing information from the noisy data. The experimental results show that our model outperforms other activation functions and achieves state-of-the-art denoising performance, while the network size and the training time are significantly reduced. version:1
arxiv-1609-02994 | Simultaneous independent image display technique on multiple 3D objects | http://arxiv.org/abs/1609.02994 | id:1609.02994 author:Takuto Hirukawa, Marco Visentini-Scarzanella, Hiroshi Kawasaki, Ryo Furukawa, Shinsaku Hiura category:cs.CV cs.GR  published:2016-09-10 summary:We propose a new system to visualize depth-dependent patterns and images on solid objects with complex geometry using multiple projectors. The system, despite consisting of conventional passive LCD projectors, is able to project different images and patterns depending on the spatial location of the object. The technique is based on the simple principle that multiple patterns projected from multiple projectors interfere constructively with each other when their patterns are projected on the same object. Previous techniques based on the same principle can only achieve 1) low resolution volume colorization or 2) high resolution images but only on a limited number of flat planes. In this paper, we discretize a 3D object into a number of 3D points so that high resolution images can be projected onto the complex shapes. We also propose a dynamic ranges expansion technique as well as an efficient optimization procedure based on epipolar constraints. Such technique can be used to the extend projection mapping to have spatial dependency, which is desirable for practical applications. We also demonstrate the system potential as a visual instructor for object placement and assembling. Experiments prove the effectiveness of our method. version:1
arxiv-1609-02976 | An Integrated Classification Model for Financial Data Mining | http://arxiv.org/abs/1609.02976 | id:1609.02976 author:Fan Cai, Nhien-An Le-Khac, M-T. Kechadi category:cs.AI cs.LG  published:2016-09-09 summary:Nowadays, financial data analysis is becoming increasingly important in the business market. As companies collect more and more data from daily operations, they expect to extract useful knowledge from existing collected data to help make reasonable decisions for new customer requests, e.g. user credit category, churn analysis, real estate analysis, etc. Financial institutes have applied different data mining techniques to enhance their business performance. However, simple ap-proach of these techniques could raise a performance issue. Besides, there are very few general models for both understanding and forecasting different finan-cial fields. We present in this paper a new classification model for analyzing fi-nancial data. We also evaluate this model with different real-world data to show its performance. version:1
arxiv-1609-02974 | Learning-Based View Synthesis for Light Field Cameras | http://arxiv.org/abs/1609.02974 | id:1609.02974 author:Nima Khademi Kalantari, Ting-Chun Wang, Ravi Ramamoorthi category:cs.CV cs.GR I.4.1  published:2016-09-09 summary:With the introduction of consumer light field cameras, light field imaging has recently become widespread. However, there is an inherent trade-off between the angular and spatial resolution, and thus, these cameras often sparsely sample in either spatial or angular domain. In this paper, we use machine learning to mitigate this trade-off. Specifically, we propose a novel learning-based approach to synthesize new views from a sparse set of input views. We build upon existing view synthesis techniques and break down the process into disparity and color estimation components. We use two sequential convolutional neural networks to model these two components and train both networks simultaneously by minimizing the error between the synthesized and ground truth images. We show the performance of our approach using only four corner sub-aperture views from the light fields captured by the Lytro Illum camera. Experimental results show that our approach synthesizes high-quality images that are superior to the state-of-the-art techniques on a variety of challenging real-world scenes. We believe our method could potentially decrease the required angular resolution of consumer light field cameras, which allows their spatial resolution to increase. version:1
arxiv-1609-02960 | A Large Scale Corpus of Gulf Arabic | http://arxiv.org/abs/1609.02960 | id:1609.02960 author:Salam Khalifa, Nizar Habash, Dana Abdulrahim, Sara Hassan category:cs.CL  published:2016-09-09 summary:Most Arabic natural language processing tools and resources are developed to serve Modern Standard Arabic (MSA), which is the official written language in the Arab World. Some Dialectal Arabic varieties, notably Egyptian Arabic, have received some attention lately and have a growing collection of resources that include annotated corpora and morphological analyzers and taggers. Gulf Arabic, however, lags behind in that respect. In this paper, we present the Gumar Corpus, a large-scale corpus of Gulf Arabic consisting of 110 million words from 1,200 forum novels. We annotate the corpus for sub-dialect information at the document level. We also present results of a preliminary study in the morphological annotation of Gulf Arabic which includes developing guidelines for a conventional orthography. The text of the corpus is publicly browsable through a web interface we developed for it. version:1
arxiv-1609-02948 | The Role of Context Selection in Object Detection | http://arxiv.org/abs/1609.02948 | id:1609.02948 author:Ruichi Yu, Xi Chen, Vlad I. Morariu, Larry S. Davis category:cs.CV  published:2016-09-09 summary:We investigate the reasons why context in object detection has limited utility by isolating and evaluating the predictive power of different context cues under ideal conditions in which context provided by an oracle. Based on this study, we propose a region-based context re-scoring method with dynamic context selection to remove noise and emphasize informative context. We introduce latent indicator variables to select (or ignore) potential contextual regions, and learn the selection strategy with latent-SVM. We conduct experiments to evaluate the performance of the proposed context selection method on the SUN RGB-D dataset. The method achieves a significant improvement in terms of mean average precision (mAP), compared with both appearance based detectors and a conventional context model without the selection scheme. version:1
arxiv-1609-02943 | Stealing Machine Learning Models via Prediction APIs | http://arxiv.org/abs/1609.02943 | id:1609.02943 author:Florian Tram√®r, Fan Zhang, Ari Juels, Michael K. Reiter, Thomas Ristenpart category:cs.CR cs.LG stat.ML  published:2016-09-09 summary:Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service ("predictive analytics") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., "steal") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures. version:1
arxiv-1609-02938 | Extract fetal ECG from single-lead abdominal ECG by de-shape short time Fourier transform and nonlocal median | http://arxiv.org/abs/1609.02938 | id:1609.02938 author:Su Li, Hau-tieng Wu category:q-bio.QM physics.data-an stat.AP stat.ME stat.ML  published:2016-09-09 summary:The multiple fundamental frequency detection problem and the source separation problem from a single-channel signal containing multiple oscillatory components and a nonstationary noise are both challenging tasks. To extract the fetal electrocardiogram (ECG) from a single-lead maternal abdominal ECG, we face both challenges. In this paper, we propose a novel method to extract the fetal ECG signal from the single channel maternal abdominal ECG signal, without any additional measurement. The algorithm is composed of three main ingredients. First, the maternal and fetal heart rates are estimated by the de-shape short time Fourier transform, which is a recently proposed nonlinear time-frequency analysis technique; second, the beat tracking technique is applied to accurately obtain the maternal and fetal R peaks; third, the maternal and fetal ECG waveforms are established by the nonlocal median. The algorithm is evaluated on a simulated fetal ECG signal database ({\em fecgsyn} database), and tested on two real databases with the annotation provided by experts ({\em adfecgdb} database and {\em CinC2013} database). In general, the algorithm could be applied to solve other detection and source separation problems, and reconstruct the time-varying wave-shape function of each oscillatory component. version:1
arxiv-1609-02932 | Image Denoising Via Collaborative Support-Agnostic Recovery | http://arxiv.org/abs/1609.02932 | id:1609.02932 author:Muzammil Behzad, Mudassir Masood, Tarig Ballal, Maha Shadaydeh, Tareq Y. Al-Naffouri category:cs.CV  published:2016-09-09 summary:In this paper, we propose a novel image denoising algorithm using collaborative support-agnostic sparse reconstruction. An observed image is first divided into patches. Similarly structured patches are grouped together to be utilized for collaborative processing. In the proposed collaborative schemes, similar patches are assumed to share the same support taps. For sparse reconstruction, the likelihood of a tap being active in a patch is computed and refined through a collaboration process with other similar patches in the same group. This provides very good patch support estimation, hence enhancing the quality of image restoration. Performance comparisons with state-of-the-art algorithms, in terms of SSIM and PSNR, demonstrate the superiority of the proposed algorithm. version:1
arxiv-1609-02907 | Semi-Supervised Classification with Graph Convolutional Networks | http://arxiv.org/abs/1609.02907 | id:1609.02907 author:Thomas N. Kipf, Max Welling category:cs.LG stat.ML  published:2016-09-09 summary:We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin. version:1
arxiv-1609-02906 | Robust Spectral Detection of Global Structures in the Data by Learning a Regularization | http://arxiv.org/abs/1609.02906 | id:1609.02906 author:Pan Zhang category:stat.ML cs.LG cs.SI physics.soc-ph  published:2016-09-09 summary:Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned regularizations suppress down the eigenvalues associated with localized eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise. version:1
arxiv-1609-02116 | Ask the GRU: Multi-Task Learning for Deep Text Recommendations | http://arxiv.org/abs/1609.02116 | id:1609.02116 author:Trapit Bansal, David Belanger, Andrew McCallum category:stat.ML cs.CL cs.LG I.2.7; I.2.6  published:2016-09-07 summary:In a variety of application domains the content to be recommended to users is associated with text. This includes research papers, movies with associated plot summaries, news articles, blog posts, etc. Recommendation approaches based on latent factor models can be extended naturally to leverage text by employing an explicit mapping from text to factors. This enables recommendations for new, unseen content, and may generalize better, since the factors for all items are produced by a compactly-parametrized model. Previous work has used topic models or averages of word embeddings for this mapping. In this paper we present a method leveraging deep recurrent neural networks to encode the text sequence into a latent vector, specifically gated recurrent units (GRUs) trained end-to-end on the collaborative filtering task. For the task of scientific paper recommendation, this yields models with significantly higher accuracy. In cold-start scenarios, we beat the previous state-of-the-art, all of which ignore word order. Performance is further improved by multi-task learning, where the text encoder network is trained for a combination of content recommendation and item metadata prediction. This regularizes the collaborative filtering model, ameliorating the problem of sparsity of the observed rating matrix. version:2
arxiv-1609-02271 | Ashwin: Plug-and-Play System for Machine-Human Image Annotation | http://arxiv.org/abs/1609.02271 | id:1609.02271 author:Anand Sriraman, Mandar Kulkarni, Rahul Kumar, Kanika Kalra, Purushotam Radadia, Shirish Karande category:cs.CV cs.HC  published:2016-09-08 summary:We present an end-to-end machine-human image annotation system where each component can be attached in a plug-and-play fashion. These components include Feature Extraction, Machine Classifier, Task Sampling and Crowd Consensus. version:2
arxiv-1609-02549 | Learning Lexical Entries for Robotic Commands using Crowdsourcing | http://arxiv.org/abs/1609.02549 | id:1609.02549 author:Junjie Hu, Jean Oh, Anatole Gershman category:cs.CL  published:2016-09-08 summary:Robotic commands in natural language usually contain various spatial descriptions that are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language. The main purpose of this paper is to simulate the interaction process between human and robots using crowdsourcing platforms, and investigate the possibility of translating natural language to robot language with paraphrases. version:2
arxiv-1609-02035 | Component-Based Distributed Framework for Coherent and Real-Time Video Dehazing | http://arxiv.org/abs/1609.02035 | id:1609.02035 author:Meihua Wang, Jiaming Mai, Yun Liang, Tom Z. J. Fu, Zhenjie Zhang, Ruichu Cai category:cs.DC cs.CV  published:2016-09-07 summary:Traditional dehazing techniques, as a well studied topic in image processing, are now widely used to eliminate the haze effects from individual images. However, even the state-of-the-art dehazing algorithms may not provide sufficient support to video analytics, as a crucial pre-processing step for video-based decision making systems (e.g., robot navigation), due to the limitations of these algorithms on poor result coherence and low processing efficiency. This paper presents a new framework, particularly designed for video dehazing, to output coherent results in real time, with two novel techniques. Firstly, we decompose the dehazing algorithms into three generic components, namely transmission map estimator, atmospheric light estimator and haze-free image generator. They can be simultaneously processed by multiple threads in the distributed system, such that the processing efficiency is optimized by automatic CPU resource allocation based on the workloads. Secondly, a cross-frame normalization scheme is proposed to enhance the coherence among consecutive frames, by sharing the parameters of atmospheric light from consecutive frames in the distributed computation platform. The combination of these techniques enables our framework to generate highly consistent and accurate dehazing results in real-time, by using only 3 PCs connected by Ethernet. version:2
arxiv-1609-02846 | Dialogue manager domain adaptation using Gaussian process reinforcement learning | http://arxiv.org/abs/1609.02846 | id:1609.02846 author:Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young category:cs.CL  published:2016-09-09 summary:Spoken dialogue systems allow humans to interact with machines using natural speech. As such, they have many benefits. By using speech as the primary communication medium, a computer interface can facilitate swift, human-like acquisition of information. In recent years, speech interfaces have become ever more popular, as is evident from the rise of personal assistants such as Siri, Google Now, Cortana and Amazon Alexa. Recently, data-driven machine learning methods have been applied to dialogue modelling and the results achieved for limited-domain applications are comparable to or outperform traditional approaches. Methods based on Gaussian processes are particularly effective as they enable good models to be estimated from limited training data. Furthermore, they provide an explicit estimate of the uncertainty which is particularly useful for reinforcement learning. This article explores the additional steps that are necessary to extend these methods to model multiple dialogue domains. We show that Gaussian process reinforcement learning is an elegant framework that naturally supports a range of methods, including prior knowledge, Bayesian committee machines and multi-agent learning, for facilitating extensible and adaptable dialogue systems. version:1
arxiv-1609-02845 | Distributed Online Optimization in Dynamic Environments Using Mirror Descent | http://arxiv.org/abs/1609.02845 | id:1609.02845 author:Shahin Shahrampour, Ali Jadbabaie category:math.OC cs.DC cs.LG stat.ML  published:2016-09-09 summary:This work addresses decentralized online optimization in non-stationary environments. A network of agents aim to track the minimizer of a global time-varying convex function. The minimizer evolves according to a known dynamics corrupted by an unknown, unstructured noise. At each time, the global function can be cast as a sum of a finite number of local functions, each of which is assigned to one agent in the network. Moreover, the local functions become available to agents sequentially, and agents do not have a prior knowledge of the future cost functions. Therefore, agents must communicate with each other to build an online approximation of the global function. We propose a decentralized variation of the celebrated Mirror Descent, developed by Nemirovksi and Yudin. Using the notion of Bregman divergence in lieu of Euclidean distance for projection, Mirror Descent has been shown to be a powerful tool in large-scale optimization. Our algorithm builds on Mirror Descent, while ensuring that agents perform a consensus step to follow the global function and take into account the dynamics of the global minimizer. To measure the performance of the proposed online algorithm, we compare it to its offline counterpart, where the global functions are available a priori. The gap between the two is called dynamic regret. We establish a regret bound that scales inversely in the spectral gap of the network, and more notably it represents the deviation of minimizer sequence with respect to the given dynamics. We then show that our results subsume a number of results in distributed optimization. We demonstrate the application of our method to decentralized tracking of dynamic parameters and verify the results via numerical experiments. version:1
arxiv-1609-01371 | Reconstructing Articulated Rigged Models from RGB-D Videos | http://arxiv.org/abs/1609.01371 | id:1609.01371 author:Dimitrios Tzionas, Juergen Gall category:cs.CV  published:2016-09-06 summary:Although commercial and open-source software exist to reconstruct a static object from a sequence recorded with an RGB-D sensor, there is a lack of tools that build rigged models of articulated objects that deform realistically and can be used for tracking or animation. In this work, we fill this gap and propose a method that creates a fully rigged model of an articulated object from depth data of a single sensor. To this end, we combine deformable mesh tracking, motion segmentation based on spectral clustering and skeletonization based on mean curvature flow. The fully rigged model then consists of a watertight mesh, embedded skeleton, and skinning weights. version:2
arxiv-1609-02825 | Track Facial Points in Unconstrained Videos | http://arxiv.org/abs/1609.02825 | id:1609.02825 author:Xi Peng, Qiong Hu, Junzhou Huang, Dimitris N. Metaxas category:cs.CV  published:2016-09-09 summary:Tracking Facial Points in unconstrained videos is challenging due to the non-rigid deformation that changes over time. In this paper, we propose to exploit incremental learning for person-specific alignment in wild conditions. Our approach takes advantage of part-based representation and cascade regression for robust and efficient alignment on each frame. Unlike existing methods that usually rely on models trained offline, we incrementally update the representation subspace and the cascade of regressors in a unified framework to achieve personalized modeling on the fly. To alleviate the drifting issue, the fitting results are evaluated using a deep neural network, where well-aligned faces are picked out to incrementally update the representation and fitting models. Both image and video datasets are employed to valid the proposed method. The results demonstrate the superior performance of our approach compared with existing approaches in terms of fitting accuracy and efficiency. version:1
arxiv-1609-02815 | By-passing the Kohn-Sham equations with machine learning | http://arxiv.org/abs/1609.02815 | id:1609.02815 author:Felix Brockherde, Li Li, Kieron Burke, Klaus-Robert M√ºller category:physics.comp-ph cs.LG physics.chem-ph stat.ML  published:2016-09-09 summary:Last year, at least 30,000 scientific papers used the Kohn-Sham scheme of density functional theory to solve electronic structure problems in a wide variety of scientific fields, ranging from materials science to biochemistry to astrophysics. Machine learning holds the promise of learning the kinetic energy functional via examples, by-passing the need to solve these equations. This should yield substantial savings in computer time, allowing either larger systems or longer time-scales to be tackled. Attempts to machine-learn this functional have been limited by the need to find its derivative. The present work overcomes this difficulty, by learning the density-potential map directly. Both the improved accuracy and lower computational cost is demonstrated on DFT calculations of small molecules. version:1
arxiv-1609-02809 | Harassment detection: a benchmark on the #HackHarassment dataset | http://arxiv.org/abs/1609.02809 | id:1609.02809 author:Alexei Bastidas, Edward Dixon, Chris Loo, John Ryan category:cs.CL  published:2016-09-09 summary:Online harassment has been a problem to a greater or lesser extent since the early days of the internet. Previous work has applied anti-spam techniques like machine-learning based text classification (Reynolds, 2011) to detecting harassing messages. However, existing public datasets are limited in size, with labels of varying quality. The #HackHarassment initiative (an alliance of 1 tech companies and NGOs devoted to fighting bullying on the internet) has begun to address this issue by creating a new dataset superior to its predecssors in terms of both size and quality. As we (#HackHarassment) complete further rounds of labelling, later iterations of this dataset will increase the available samples by at least an order of magnitude, enabling corresponding improvements in the quality of machine learning models for harassment detection. In this paper, we introduce the first models built on the #HackHarassment dataset v1.0 (a new open dataset, which we are delighted to share with any interested researcherss) as a benchmark for future research. version:1
arxiv-1609-02805 | Automated detection of smuggled high-risk security threats using Deep Learning | http://arxiv.org/abs/1609.02805 | id:1609.02805 author:Nicolas Jaccard, Thomas W. Rogers, Edward J. Morton, Lewis D. Griffin category:cs.CV  published:2016-09-09 summary:The security infrastructure is ill-equipped to detect and deter the smuggling of non-explosive devices that enable terror attacks such as those recently perpetrated in western Europe. The detection of so-called "small metallic threats" (SMTs) in cargo containers currently relies on statistical risk analysis, intelligence reports, and visual inspection of X-ray images by security officers. The latter is very slow and unreliable due to the difficulty of the task: objects potentially spanning less than 50 pixels have to be detected in images containing more than 2 million pixels against very complex and cluttered backgrounds. In this contribution, we demonstrate for the first time the use of Convolutional Neural Networks (CNNs), a type of Deep Learning, to automate the detection of SMTs in fullsize X-ray images of cargo containers. Novel approaches for dataset augmentation allowed to train CNNs from-scratch despite the scarcity of data available. We report fewer than 6% false alarms when detecting 90% SMTs synthetically concealed in stream-of-commerce images, which corresponds to an improvement of over an order of magnitude over conventional approaches such as Bag-of-Words (BoWs). The proposed scheme offers potentially super-human performance for a fraction of the time it would take for a security officers to carry out visual inspection (processing time is approximately 3.5s per container image). version:1
arxiv-1609-02781 | An empirical study on the effects of different types of noise in image classification tasks | http://arxiv.org/abs/1609.02781 | id:1609.02781 author:Gabriel B. Paranhos da Costa, Welinton A. Contato, Tiago S. Nazare, Jo√£o E. S. Batista Neto, Moacir Ponti category:cs.CV  published:2016-09-09 summary:Image classification is one of the main research problems in computer vision and machine learning. Since in most real-world image classification applications there is no control over how the images are captured, it is necessary to consider the possibility that these images might be affected by noise (e.g. sensor noise in a low-quality surveillance camera). In this paper we analyse the impact of three different types of noise on descriptors extracted by two widely used feature extraction methods (LBP and HOG) and how denoising the images can help to mitigate this problem. We carry out experiments on two different datasets and consider several types of noise, noise levels, and denoising methods. Our results show that noise can hinder classification performance considerably and make classes harder to separate. Although denoising methods were not able to reach the same performance of the noise-free scenario, they improved classification results for noisy data. version:1
arxiv-1609-02770 | Image and Video Mining through Online Learning | http://arxiv.org/abs/1609.02770 | id:1609.02770 author:Andrew Gilbert, Richard Bowden category:cs.CV  published:2016-09-09 summary:Within the field of image and video recognition, the traditional approach is a dataset split into fixed training and test partitions. However, the labelling of the training set is time-consuming, especially as datasets grow in size and complexity. Furthermore, this approach is not applicable to the home user, who wants to intuitively group their media without tirelessly labelling the content. Our interactive approach is able to iteratively cluster classes of images and video. Our approach is based around the concept of an image signature which, unlike a standard bag of words model, can express co-occurrence statistics as well as symbol frequency. We efficiently compute metric distances between signatures despite their inherent high dimensionality and provide discriminative feature selection, to allow common and distinctive elements to be identified from a small set of user labelled examples. These elements are then accentuated in the image signature to increase similarity between examples and pull correct classes together. By repeating this process in an online learning framework, the accuracy of similarity increases dramatically despite labelling only a few training examples. To demonstrate that the approach is agnostic to media type and features used, we evaluate on three image datasets (15 scene, Caltech101 and FG-NET), a mixed text and image dataset (ImageTag), a dataset used in active learning (Iris) and on three action recognition datasets (UCF11, KTH and Hollywood2). On the UCF11 video dataset, the accuracy is 86.7% despite using only 90 labelled examples from a dataset of over 1200 videos, instead of the standard 1122 training videos. The approach is both scalable and efficient, with a single iteration over the full UCF11 dataset of around 1200 videos taking approximately 1 minute on a standard desktop machine. version:1
arxiv-1609-02748 | NSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual Aspect-based Sentiment Analysis | http://arxiv.org/abs/1609.02748 | id:1609.02748 author:Sebastian Ruder, Parsa Ghaffari, John G. Breslin category:cs.CL cs.LG  published:2016-09-09 summary:This paper describes our deep learning-based approach to multilingual aspect-based sentiment analysis as part of SemEval 2016 Task 5. We use a convolutional neural network (CNN) for both aspect extraction and aspect-based sentiment analysis. We cast aspect extraction as a multi-label classification problem, outputting probabilities over aspects parameterized by a threshold. To determine the sentiment towards an aspect, we concatenate an aspect vector with every word embedding and apply a convolution over it. Our constrained system (unconstrained for English) achieves competitive results across all languages and domains, placing first or second in 5 and 7 out of 11 language-domain pairs for aspect category detection (slot 1) and sentiment polarity (slot 3) respectively, thereby demonstrating the viability of a deep learning-based approach for multilingual aspect-based sentiment analysis. version:1
arxiv-1609-02746 | INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for Sentiment Classification and Quantification | http://arxiv.org/abs/1609.02746 | id:1609.02746 author:Sebastian Ruder, Parsa Ghaffari, John G. Breslin category:cs.CL cs.LG  published:2016-09-09 summary:This paper describes our deep learning-based approach to sentiment analysis in Twitter as part of SemEval-2016 Task 4. We use a convolutional neural network to determine sentiment and participate in all subtasks, i.e. two-point, three-point, and five-point scale sentiment classification and two-point and five-point scale sentiment quantification. We achieve competitive results for two-point scale sentiment classification and quantification, ranking fifth and a close fourth (third and second by alternative metrics) respectively despite using only pre-trained embeddings that contain no sentiment information. We achieve good performance on three-point scale sentiment classification, ranking eighth out of 35, while performing poorly on five-point scale sentiment classification and quantification. An error analysis reveals that this is due to low expressiveness of the model to capture negative sentiment as well as an inability to take into account ordinal information. We propose improvements in order to address these and other issues. version:1
arxiv-1609-02745 | A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis | http://arxiv.org/abs/1609.02745 | id:1609.02745 author:Sebastian Ruder, Parsa Ghaffari, John G. Breslin category:cs.CL cs.LG  published:2016-09-09 summary:Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review's argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any hand-engineered features or external resources. version:1
arxiv-1609-02744 | An Interactive Segmentation Tool for Quantifying Fat in Lumbar Muscles using Axial Lumbar-Spine MRI | http://arxiv.org/abs/1609.02744 | id:1609.02744 author:Joseph Antony, Kevin McGuinness, Neil Welch, Joe Coyle, Andy Franklyn-Miller, Noel E. O'Connor, Kieran Moran category:cs.CV  published:2016-09-09 summary:In this paper we present an interactive tool that can be used to quantify fat infiltration in lumbar muscles, which is useful in studying fat infiltration and lower back pain (LBP) in adults. Currently, a qualitative assessment by visual grading via a 5-point scale is used to study fat infiltration in lumbar muscles from an axial view of lumbar-spine MR Images. However, a quantitative approach (on a continuous scale of 0-100\%) may provide a greater insight. In this paper, we propose a method to precisely quantify the fat deposition / infiltration in a user-defined region of the lumbar muscles, which may aid better diagnosis and analysis. The key steps are interactively segmenting the region of interest (ROI) from the lumbar muscles using the well known livewire technique, identifying fatty regions in the segmented region based on variable-selection of threshold and softness levels, automatically detecting the center of the spinal column and fragmenting the lumbar muscles into smaller regions with reference to the center of the spinal column, computing key parameters [such as total and region-wise fat content percentage, total-cross sectional area (TCSA) and functional cross-sectional area (FCSA)] and exporting the computations and associated patient information from the MRI, into a database. A standalone application using MATLAB R2014a was developed to perform the required computations along with an intuitive graphical user interface (GUI). version:1
arxiv-1609-02728 | Predicting the future relevance of research institutions - The winning solution of the KDD Cup 2016 | http://arxiv.org/abs/1609.02728 | id:1609.02728 author:Vlad Sandulescu, Mihai Chiru category:cs.LG cs.DL cs.SI physics.soc-ph  published:2016-09-09 summary:The world's collective knowledge is evolving through research and new scientific discoveries. It is becoming increasingly difficult to objectively rank the impact research institutes have on global advancements. However, since the funding, governmental support, staff and students quality all mirror the projected quality of the institution, it becomes essential to measure the affiliation's rating in a transparent and widely accepted way. We propose and investigate several methods to rank affiliations based on the number of their accepted papers at future academic conferences. We carry out our investigation using publicly available datasets such as the Microsoft Academic Graph, a heterogeneous graph which contains various information about academic papers. We analyze several models, starting with a simple probabilities-based method and then gradually expand our training dataset, engineer many more features and use mixed models and gradient boosted decision trees models to improve our predictions. version:1
arxiv-1609-02727 | Detecting Singleton Review Spammers Using Semantic Similarity | http://arxiv.org/abs/1609.02727 | id:1609.02727 author:Vlad Sandulescu, Martin Ester category:cs.CL cs.LG I.7.0; J.4  published:2016-09-09 summary:Online reviews have increasingly become a very important resource for consumers when making purchases. Though it is becoming more and more difficult for people to make well-informed buying decisions without being deceived by fake reviews. Prior works on the opinion spam problem mostly considered classifying fake reviews using behavioral user patterns. They focused on prolific users who write more than a couple of reviews, discarding one-time reviewers. The number of singleton reviewers however is expected to be high for many review websites. While behavioral patterns are effective when dealing with elite users, for one-time reviewers, the review text needs to be exploited. In this paper we tackle the problem of detecting fake reviews written by the same person using multiple names, posting each review under a different name. We propose two methods to detect similar reviews and show the results generally outperform the vectorial similarity measures used in prior works. The first method extends the semantic similarity between words to the reviews level. The second method is based on topic modeling and exploits the similarity of the reviews topic distributions using two models: bag-of-words and bag-of-opinion-phrases. The experiments were conducted on reviews from three different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset (800 reviews). version:1
arxiv-1609-02715 | Automatic Selection of Stochastic Watershed Hierarchies | http://arxiv.org/abs/1609.02715 | id:1609.02715 author:Amin Fehri, Santiago Velasco-Forero, Fernand Meyer category:cs.CV cs.LG  published:2016-09-09 summary:The segmentation, seen as the association of a partition with an image, is a difficult task. It can be decomposed in two steps: at first, a family of contours associated with a series of nested partitions (or hierarchy) is created and organized, then pertinent contours are extracted. A coarser partition is obtained by merging adjacent regions of a finer partition. The strength of a contour is then measured by the level of the hierarchy for which its two adjacent regions merge. We present an automatic segmentation strategy using a wide range of stochastic watershed hierarchies. For a given set of homogeneous images, our approach selects automatically the best hierarchy and cut level to perform image simplification given an evaluation score. Experimental results illustrate the advantages of our approach on several real-life images datasets. version:1
arxiv-1609-02700 | Efficient batch-sequential Bayesian optimization with moments of truncated Gaussian vectors | http://arxiv.org/abs/1609.02700 | id:1609.02700 author:S√©bastien Marmin, Cl√©ment Chevalier, David Ginsbourger category:stat.ML stat.AP stat.ME  published:2016-09-09 summary:We deal with the efficient parallelization of Bayesian global optimization algorithms, and more specifically of those based on the expected improvement criterion and its variants. A closed form formula relying on multivariate Gaussian cumulative distribution functions is established for a generalized version of the multipoint expected improvement criterion. In turn, the latter relies on intermediate results that could be of independent interest concerning moments of truncated Gaussian vectors. The obtained expansion of the criterion enables studying its differentiability with respect to point batches and calculating the corresponding gradient in closed form. Furthermore , we derive fast numerical approximations of this gradient and propose efficient batch optimization strategies. Numerical experiments illustrate that the proposed approaches enable computational savings of between one and two order of magnitudes, hence enabling derivative-based batch-sequential acquisition function maximization to become a practically implementable and efficient standard. version:1
arxiv-1609-02686 | Boosting Joint Models for Longitudinal and Time-to-Event Data | http://arxiv.org/abs/1609.02686 | id:1609.02686 author:Elisabeth Waldmann, David Taylor-Robinson, Nadja Klein, Thomas Kneib, Tania Pressler, Matthias Schmid, Andreas Mayr category:stat.ML stat.ME  published:2016-09-09 summary:Joint Models for longitudinal and time-to-event data have gained a lot of attention in the last few years as they are a helpful technique to approach a data structure very common in life sciences: in many clinical studies or registries longitudinal outcomes are recorded alongside event times. The Danish cystic fibrosis registry collects lung functions of cystic fibrosis patients together with the onset of pulmonary infections. Those two processes are linked to each other and the two outcomes should hence be modeled jointly in order to prevent the bias introduced by the independent modelling. Commonly, joint models are estimated in likelihood based expectation maximization or Bayesian approaches using frameworks were variable selection is problematic and which do not work for high-dimensional data. In this paper, we propose a boosting algorithm tackling these challenges by being able to simultaneously estimate predictors for joint models and automatically select the most influential variables in potentially high-dimensional data situations. We analyse the performance of the new algorithm in a simulation study before we apply it to the Danish cystic fibrosis registry. This is the first approach to combine state-of-the art algorithms from the field of machine-learning with the model class of joint models, providing a fully data-driven mechanism to select variables and predictor effects in an unified framework of boosting joint models. version:1
arxiv-1609-02678 | Identifying Topology of Power Distribution Networks Based on Smart Meter Data | http://arxiv.org/abs/1609.02678 | id:1609.02678 author:Jayadev P Satya, Nirav Bhatt, Ramkrishna Pasumarthy, Aravind Rajeswaran category:cs.SY cs.LG  published:2016-09-09 summary:In a power distribution network, the network topology information is essential for an efficient operation of the network. This information of network connectivity is not accurately available, at the low voltage level, due to uninformed changes that happen from time to time. In this paper, we propose a novel data--driven approach to identify the underlying network topology including the load phase connectivity from time series of energy measurements. The proposed method involves the application of Principal Component Analysis (PCA) and its graph-theoretic interpretation to infer the topology from smart meter energy measurements. The method is demonstrated through simulation on randomly generated networks and also on IEEE recognized Roy Billinton distribution test system. version:1
arxiv-1609-02664 | Machine Learning with Guarantees using Descriptive Complexity and SMT Solvers | http://arxiv.org/abs/1609.02664 | id:1609.02664 author:Charles Jordan, ≈Åukasz Kaiser category:cs.LG cs.LO  published:2016-09-09 summary:Machine learning is a thriving part of computer science. There are many efficient approaches to machine learning that do not provide strong theoretical guarantees, and a beautiful general learning theory. Unfortunately, machine learning approaches that give strong theoretical guarantees have not been efficient enough to be applicable. In this paper we introduce a logical approach to machine learning. Models are represented by tuples of logical formulas and inputs and outputs are logical structures. We present our framework together with several applications where we evaluate it using SAT and SMT solvers. We argue that this approach to machine learning is particularly suited to bridge the gap between efficiency and theoretical soundness. We exploit results from descriptive complexity theory to prove strong theoretical guarantees for our approach. To show its applicability, we present experimental results including learning complexity-theoretic reductions rules for board games. We also explain how neural networks fit into our framework, although the current implementation does not scale to provide guarantees for real-world neural networks. version:1
arxiv-1609-02655 | Singularity structures and impacts on parameter estimation in finite mixtures of distributions | http://arxiv.org/abs/1609.02655 | id:1609.02655 author:Nhat Ho, XuanLong Nguyen category:math.ST stat.ML stat.TH  published:2016-09-09 summary:Singularities of a statistical model are the elements of the model's parameter space which make the corresponding Fisher information matrix degenerate. These are the points for which estimation techniques such as the maximum likelihood estimator and standard Bayesian procedures do not admit the root-$n$ parametric rate of convergence. We propose a general framework for the identification of singularity structures of the parameter space of finite mixtures, and study the impacts of the singularity levels on minimax lower bounds and rates of convergence for the maximum likelihood estimator over a compact parameter space. Our study makes explicit the deep links between model singularities, parameter estimation convergence rates and minimax lower bounds, and the algebraic geometry of the parameter space for mixtures of continuous distributions. The theory is applied to establish concrete convergence rates of parameter estimation for finite mixture of skewnormal distributions. This rich and increasingly popular mixture model is shown to exhibit a remarkably complex range of asymptotic behaviors which have not been hitherto reported in the literature. version:1
arxiv-1609-02638 | Robust Structure from Motion in the Presence of Outliers and Missing Data | http://arxiv.org/abs/1609.02638 | id:1609.02638 author:Guanghui Wang, John S. Zelek, Jonathan Wu, Ruzena Bajcsy category:cs.CV  published:2016-09-09 summary:The paper preposes a new scheme to promote the robustness of 3D structure and motion factorization from uncalibrated image sequences. First, an augmented affine factorization algorithm is proposed to circumvent the difficulty in image registration with imperfect data. Then, an alternative weighted factorization algorithm is designed to handle the missing data and measurement uncertainties in the tracking matrix. Finally, a robust structure and motion factorization scheme is proposed to deal with outlying and missing data. The novelty and main contribution of the paper are as follows: (i) The augmented factorization algorithm is a new addition to previous affine factorization family for both rigid and nonrigid objects; (ii) it is demonstrated that image reprojection residuals are in general proportional to the error magnitude in the tracking data, and thus, the outliers can be detected directly from the distribution of image reprojection residuals, which are then used to estimate the uncertainty of inlying measurement; (iii) the robust factorization scheme is proved empirically to be more efficient and more accurate than other robust algorithms; and (iv) the proposed approach can be directly applied to nonrigid scenarios. Extensive experiments on synthetic data and real images demonstrate the advantages of the proposed approach. version:1
arxiv-1609-02631 | Distributed Processing of Biosignal-Database for Emotion Recognition with Mahout | http://arxiv.org/abs/1609.02631 | id:1609.02631 author:Varvara Kollia, Oguz H. Elibol category:stat.ML cs.LG  published:2016-09-09 summary:This paper investigates the use of distributed processing on the problem of emotion recognition from physiological sensors using a popular machine learning library on distributed mode. Specifically, we run a random forests classifier on the biosignal-data, which have been pre-processed to form exclusive groups in an unsupervised fashion, on a Cloudera cluster using Mahout. The use of distributed processing significantly reduces the time required for the offline training of the classifier, enabling processing of large physiological datasets through many iterations. version:1
arxiv-1609-01233 | Multivariate Dependence Beyond Shannon Information | http://arxiv.org/abs/1609.01233 | id:1609.01233 author:Ryan G. James, James P. Crutchfield category:cs.IT cond-mat.stat-mech math.IT math.ST stat.ML stat.TH  published:2016-09-05 summary:Accurately determining dependency structure is critical to discovering a system's causal organization. We recently showed that the transfer entropy fails in a key aspect of this---measuring information flow---due to its conflation of dyadic and polyadic relationships. We extend this observation to demonstrate that this is true of all such Shannon information measures when used to analyze multivariate dependencies. This has broad implications, particularly when employing information to express the organization and mechanisms embedded in complex systems, including the burgeoning efforts to combine complex network theory with information theory. Here, we do not suggest that any aspect of information theory is wrong. Rather, the vast majority of its informational measures are simply inadequate for determining the meaningful dependency structure within joint probability distributions. Therefore, such information measures are inadequate for discovering intrinsic causal relations. We close by demonstrating that such distributions exist across an arbitrary set of variables. version:2
arxiv-1609-02612 | Generating Videos with Scene Dynamics | http://arxiv.org/abs/1609.02612 | id:1609.02612 author:Carl Vondrick, Hamed Pirsiavash, Antonio Torralba category:cs.CV cs.GR cs.LG  published:2016-09-08 summary:We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation. version:1
arxiv-1609-02606 | On Sequential Elimination Algorithms for Best-Arm Identification in Multi-Armed Bandits | http://arxiv.org/abs/1609.02606 | id:1609.02606 author:Shahin Shahrampour, Mohammad Noshad, Vahid Tarokh category:stat.ML cs.LG  published:2016-09-08 summary:We consider the best-arm identification problem in multi-armed bandits, which focuses purely on exploration. A player is given a fixed budget to explore a finite set of arms, and the rewards of each arm are drawn independently from a fixed, unknown distribution. The player aims to identify the arm with the largest expected reward. We propose a general framework to unify sequential elimination algorithms, where the arms are dismissed iteratively until a unique arm is left. Our analysis reveals a novel performance measure expressed in terms of the sampling mechanism and number of eliminated arms at each round. Based on this result, we develop an algorithm that divides the budget according to a nonlinear function of remaining arms at each round. We provide theoretical guarantees for the algorithm, characterizing the suitable nonlinearity for different problem environments. Matching the theoretical results, our experiments show that the nonlinear algorithm outperforms the state-of-the-art. We finally study the side-observation model, where pulling an arm reveals the rewards of its related arms, and we establish improved theoretical guarantees in the pure-exploration setting. version:1
arxiv-1609-02583 | Bottom-up Instance Segmentation using Deep Higher-Order CRFs | http://arxiv.org/abs/1609.02583 | id:1609.02583 author:Anurag Arnab, Philip H. S. Torr category:cs.CV  published:2016-09-08 summary:Traditional Scene Understanding problems such as Object Detection and Semantic Segmentation have made breakthroughs in recent years due to the adoption of deep learning. However, the former task is not able to localise objects at a pixel level, and the latter task has no notion of different instances of objects of the same class. We focus on the task of Instance Segmentation which recognises and localises objects down to a pixel level. Our model is based on a deep neural network trained for semantic segmentation. This network incorporates a Conditional Random Field with end-to-end trainable higher order potentials based on object detector outputs. This allows us to reason about instances from an initial, category-level semantic segmentation. Our simple method effectively leverages the great progress recently made in semantic segmentation and object detection. The accurate instance-level segmentations that our network produces is reflected by the considerable improvements obtained over previous work. version:1
arxiv-1609-02542 | Quantum-assisted learning of graphical models with arbitrary pairwise connectivity | http://arxiv.org/abs/1609.02542 | id:1609.02542 author:Marcello Benedetti, John Realpe-G√≥mez, Rupak Biswas, Alejandro Perdomo-Ortiz category:quant-ph cs.LG  published:2016-09-08 summary:There is increasing interest in the potential advantages of using quantum computing technologies as sampling engines to speedup machine learning and probabilistic programming tasks. However, some pressing challenges in state-of-the-art quantum annealers have to be overcome before we can assess their actual performance. Most notably, the effective temperature at which samples are generated is instance-dependent and unknown, the interaction graph is sparse, the parameters are noisy, and the dynamic range of the parameters is finite. Of all these limitations, the sparse connectivity resulting from the local interaction between quantum bits in physical hardware implementations, is considered the most severe limitation to the quality of constructing powerful machine learning models. Here we show how to surpass this "curse of limited connectivity" bottleneck and illustrate our findings by training probabilistic generative models with arbitrary pairwise connectivity. Our model can be trained in quantum hardware without full knowledge of the effective parameters specifying the corresponding Boltzmann-like distribution. Therefore, inference of the effective temperature is avoided and the effect of noise in the parameters is mitigated. We illustrate our findings by successfully training hardware-embedded models with all-to-all connectivity on a real dataset of handwritten digits and two synthetic datasets. In each of these datasets we show the generative capabilities of the models learned with the assistance of the quantum annealer in experiments with up to 940 quantum bits. Additionally, we show a visual Turing test with handwritten digit data, where the machine generating the digits is a quantum processor. Such digits, with a remarkable similarity to those generated by humans, are extracted from the experiments with 940 quantum bits. version:1
arxiv-1609-02521 | DiSMEC - Distributed Sparse Machines for Extreme Multi-label Classification | http://arxiv.org/abs/1609.02521 | id:1609.02521 author:Rohit Babbar, Bernhard Shoelkopf category:stat.ML cs.LG  published:2016-09-08 summary:Extreme multi-label classification refers to supervised multi-label learning involving hundreds of thousands or even millions of labels. Datasets in extreme classification exhibit fit to power-law distribution, i.e. a large fraction of labels have very few positive instances in the data distribution. Most state-of-the-art approaches for extreme multi-label classification attempt to capture correlation among labels by embedding the label matrix to a low-dimensional linear sub-space. However, in the presence of power-law distributed extremely large and diverse label spaces, structural assumptions such as low rank can be easily violated. In this work, we present DiSMEC, which is a large-scale distributed framework for learning one-versus-rest linear classifiers coupled with explicit capacity control to control model size. Unlike most state-of-the-art methods, DiSMEC does not make any low rank assumptions on the label matrix. Using double layer of parallelization, DiSMEC can learn classifiers for datasets consisting hundreds of thousands labels within few hours. The explicit capacity control mechanism filters out spurious parameters which keep the model compact in size, without losing prediction accuracy. We conduct extensive empirical evaluation on publicly available real-world datasets consisting upto 670,000 labels. We compare DiSMEC with recent state-of-the-art approaches, including - SLEEC which is a leading approach for learning sparse local embeddings, and FastXML which is a tree-based approach optimizing ranking based loss function. On some of the datasets, DiSMEC can significantly boost prediction accuracies - 10% better compared to SLECC and 15% better compared to FastXML, in absolute terms. version:1
arxiv-1609-02513 | Functorial Hierarchical Clustering with Overlaps | http://arxiv.org/abs/1609.02513 | id:1609.02513 author:Jared Culbertson, Dan P. Guralnik, Peter F. Stiller category:cs.LG stat.ML 51K05  68P01 H.3.3  published:2016-09-08 summary:This work draws its inspiration from three important sources of research on dissimilarity-based clustering and intertwines those three threads into a consistent principled functorial theory of clustering. Those three are the overlapping clustering of Jardine and Sibson, the functorial approach of Carlsson and Memoli to partition-based clustering, and the Isbell/Dress school's study of injective envelopes. Carlsson and Memoli introduce the idea of viewing clustering methods as functors from a category of metric spaces to a category of clusters, with functoriality subsuming many desirable properties. Our first series of results extends their theory of functorial clustering schemes to methods that allow overlapping clusters in the spirit of Jardine and Sibson. This obviates some of the unpleasant effects of chaining that occur, for example with single-linkage clustering. We prove an equivalence between these general overlapping clustering functors and projections of weight spaces to what we term clustering domains, by focusing on the order structure determined by the morphisms. As a specific application of this machinery, we are able to prove that there are no functorial projections to cut metrics, or even to tree metrics. Finally, although we focus less on the construction of clustering methods (clustering domains) derived from injective envelopes, we lay out some preliminary results, that hopefully will give a feel for how the third leg of the stool comes into play. version:1
arxiv-1609-02500 | Reduced Memory Region Based Deep Convolutional Neural Network Detection | http://arxiv.org/abs/1609.02500 | id:1609.02500 author:Denis Tome', Luca Bondi, Emanuele Plebani, Luca Baroffio, Danilo Pau, Stefano Tubaro category:cs.CV  published:2016-09-08 summary:Accurate pedestrian detection has a primary role in automotive safety: for example, by issuing warnings to the driver or acting actively on car's brakes, it helps decreasing the probability of injuries and human fatalities. In order to achieve very high accuracy, recent pedestrian detectors have been based on Convolutional Neural Networks (CNN). Unfortunately, such approaches require vast amounts of computational power and memory, preventing efficient implementations on embedded systems. This work proposes a CNN-based detector, adapting a general-purpose convolutional network to the task at hand. By thoroughly analyzing and optimizing each step of the detection pipeline, we develop an architecture that outperforms methods based on traditional image features and achieves an accuracy close to the state-of-the-art while having low computational complexity. Furthermore, the model is compressed in order to fit the tight constrains of low power devices with a limited amount of embedded memory available. This paper makes two main contributions: (1) it proves that a region based deep neural network can be finely tuned to achieve adequate accuracy for pedestrian detection (2) it achieves a very low memory usage without reducing detection accuracy on the Caltech Pedestrian dataset. version:1
arxiv-1609-02489 | Fashion DNA: Merging Content and Sales Data for Recommendation and Article Mapping | http://arxiv.org/abs/1609.02489 | id:1609.02489 author:Christian Bracher, Sebastian Heinz, Roland Vollgraf category:cs.IR cs.LG  published:2016-09-08 summary:We present a method to determine Fashion DNA, coordinate vectors locating fashion items in an abstract space. Our approach is based on a deep neural network architecture that ingests curated article information such as tags and images, and is trained to predict sales for a large set of frequent customers. In the process, a dual space of customer style preferences naturally arises. Interpretation of the metric of these spaces is straightforward: The product of Fashion DNA and customer style vectors yields the forecast purchase likelihood for the customer-item pair, while the angle between Fashion DNA vectors is a measure of item similarity. Importantly, our models are able to generate unbiased purchase probabilities for fashion items based solely on article information, even in absence of sales data, thus circumventing the "cold-start problem" of collaborative recommendation approaches. Likewise, it generalizes easily and reliably to customers outside the training set. We experiment with Fashion DNA models based on visual and/or tag item data, evaluate their recommendation power, and discuss the resulting article similarities. version:1
arxiv-1609-02469 | Quantifying Radiographic Knee Osteoarthritis Severity using Deep Convolutional Neural Networks | http://arxiv.org/abs/1609.02469 | id:1609.02469 author:Joseph Antony, Kevin McGuinness, Noel E O Connor, Kieran Moran category:cs.CV  published:2016-09-08 summary:This paper proposes a new approach to automatically quantify the severity of knee osteoarthritis (OA) from radiographs using deep convolutional neural networks (CNN). Clinically, knee OA severity is assessed using Kellgren \& Lawrence (KL) grades, a five point scale. Previous work on automatically predicting KL grades from radiograph images were based on training shallow classifiers using a variety of hand engineered features. We demonstrate that classification accuracy can be significantly improved using deep convolutional neural network models pre-trained on ImageNet and fine-tuned on knee OA images. Furthermore, we argue that it is more appropriate to assess the accuracy of automatic knee OA severity predictions using a continuous distance-based evaluation metric like mean squared error than it is to use classification accuracy. This leads to the formulation of the prediction of KL grades as a regression problem and further improves accuracy. Results on a dataset of X-ray images and KL grades from the Osteoarthritis Initiative (OAI) show a sizable improvement over the current state-of-the-art. version:1
arxiv-1609-02452 | End-to-End Eye Movement Detection Using Convolutional Neural Networks | http://arxiv.org/abs/1609.02452 | id:1609.02452 author:Sabrina Hoppe, Andreas Bulling category:cs.CV  published:2016-09-08 summary:Common computational methods for automated eye movement detection - i.e. the task of detecting different types of eye movement in a continuous stream of gaze data - are limited in that they either involve thresholding on hand-crafted signal features, require individual detectors each only detecting a single movement, or require pre-segmented data. We propose a novel approach for eye movement detection that only involves learning a single detector end-to-end, i.e. directly from the continuous gaze data stream and simultaneously for different eye movements without any manual feature crafting or segmentation. Our method is based on convolutional neural networks (CNN) that recently demonstrated superior performance in a variety of tasks in computer vision, signal processing, and machine learning. We further introduce a novel multi-participant dataset that contains scripted and free-viewing sequences of ground-truth annotated saccades, fixations, and smooth pursuits. We show that our CNN-based method outperforms state-of-the-art baselines by a large margin on this challenging dataset, thereby underlining the significant potential of this approach for holistic, robust, and accurate eye movement protocol analysis. version:1
arxiv-1609-02001 | Dense Motion Estimation for Smoke | http://arxiv.org/abs/1609.02001 | id:1609.02001 author:Da Chen, Wenbin Li, Peter Hall category:cs.CV  published:2016-09-07 summary:Motion estimation for highly dynamic phenomena such as smoke is an open challenge for Computer Vision. Traditional dense motion estimation algorithms have difficulties with non-rigid and large motions, both of which are frequently observed in smoke motion. We propose an algorithm for dense motion estimation of smoke. Our algorithm is robust, fast, and has better performance over different types of smoke compared to other dense motion estimation algorithms, including state of the art and neural network approaches. The key to our contribution is to use skeletal flow, without explicit point matching, to provide a sparse flow. This sparse flow is upgraded to a dense flow. In this paper we describe our algorithm in greater detail, and provide experimental evidence to support our claims. version:2
arxiv-1609-01693 | Making a Case for Learning Motion Representations with Phase | http://arxiv.org/abs/1609.01693 | id:1609.01693 author:S. L. Pintea, J. C. van Gemert category:cs.CV  published:2016-09-06 summary:This work advocates Eulerian motion representation learning over the current standard Lagrangian optical flow model. Eulerian motion is well captured by using phase, as obtained by decomposing the image through a complex-steerable pyramid. We discuss the gain of Eulerian motion in a set of practical use cases: (i) action recognition, (ii) motion prediction in static images, (iii) motion transfer in static images and, (iv) motion transfer in video. For each task we motivate the phase-based direction and provide a possible approach. version:2
arxiv-1609-02383 | Improved Optimistic Mirror Descent for Sparsity and Curvature | http://arxiv.org/abs/1609.02383 | id:1609.02383 author:Parameswaran Kamalaruban category:cs.LG  published:2016-09-08 summary:Online Convex Optimization plays a key role in large scale machine learning. Early approaches to this problem were conservative, in which the main focus was protection against the worst case scenario. But recently several algorithms have been developed for tightening the regret bounds in easy data instances such as sparsity, predictable sequences, and curved losses. In this work we unify some of these existing techniques to obtain new update rules for the cases when these easy instances occur together. First we analyse an adaptive and optimistic update rule which achieves tighter regret bound when the loss sequence is sparse and predictable. Then we explain an update rule that dynamically adapts to the curvature of the loss function and utilizes the predictable nature of the loss sequence as well. Finally we extend these results to composite losses. version:1
arxiv-1609-02374 | Extraction of Skin Lesions from Non-Dermoscopic Images Using Deep Learning | http://arxiv.org/abs/1609.02374 | id:1609.02374 author:Mohammad H. Jafari, Ebrahim Nasr-Esfahani, Nader Karimi, S. M. Reza Soroushmehr, Shadrokh Samavi, Kayvan Najarian category:cs.CV  published:2016-09-08 summary:Melanoma is amongst most aggressive types of cancer. However, it is highly curable if detected in its early stages. Prescreening of suspicious moles and lesions for malignancy is of great importance. Detection can be done by images captured by standard cameras, which are more preferable due to low cost and availability. One important step in computerized evaluation of skin lesions is accurate detection of lesion region, i.e. segmentation of an image into two regions as lesion and normal skin. Accurate segmentation can be challenging due to burdens such as illumination variation and low contrast between lesion and healthy skin. In this paper, a method based on deep neural networks is proposed for accurate extraction of a lesion region. The input image is preprocessed and then its patches are fed to a convolutional neural network (CNN). Local texture and global structure of the patches are processed in order to assign pixels to lesion or normal classes. A method for effective selection of training patches is used for more accurate detection of a lesion border. The output segmentation mask is refined by some post processing operations. The experimental results of qualitative and quantitative evaluations demonstrate that our method can outperform other state-of-the-art algorithms exist in the literature. version:1
arxiv-1609-02368 | Ear-to-ear Capture of Facial Intrinsics | http://arxiv.org/abs/1609.02368 | id:1609.02368 author:Alassane Seck, William A. P. Smith, Arnaud Dessein, Bernard Tiddeman, Hannah Dee, Abhishek Dutta category:cs.CV  published:2016-09-08 summary:We present a practical approach to capturing ear-to-ear face models comprising both 3D meshes and intrinsic textures (i.e. diffuse and specular albedo). Our approach is a hybrid of geometric and photometric methods and requires no geometric calibration. Photometric measurements made in a lightstage are used to estimate view dependent high resolution normal maps. We overcome the problem of having a single photometric viewpoint by capturing in multiple poses. We use uncalibrated multiview stereo to estimate a coarse base mesh to which the photometric views are registered. We propose a novel approach to robustly stitching surface normal and intrinsic texture data into a seamless, complete and highly detailed face model. The resulting relightable models provide photorealistic renderings in any view. version:1
arxiv-1609-02356 | Adaptive Parameter Balancing for Composite Optimization Problems in Imaging | http://arxiv.org/abs/1609.02356 | id:1609.02356 author:Byung-Woo Hong, Ja-Keoung Koo, Hendrik Dirks, Martin Burger category:cs.CV  published:2016-09-08 summary:We propose an adaptive parameter balancing scheme in a variational framework where a convex composite energy functional that consists of data fidelity and regularization is optimized. In our adaptive parameter balancing, the relative weight is assigned to each term of the energy for indicating its significance to the total energy, and is automatically determined based on the data fidelity measuring how well the data fits the model at each energy optimization step. The adaptive balancing parameter is designed to locally control the regularity by taking into consideration the residual at each point without introducing any a-priori knowledge. We demonstrate that our adaptive balancing parameter is effective to improve the quality of the solution by determining the degree of regularity based on the local residual in the application of imaging problems. We apply our adaptive parameter balancing scheme to the classical imaging problems that are denoising, segmentation and motion estimation, and provide their optimization algorithms based on the alternating direction method of multipliers (ADMM) method. The robustness and effectiveness of our adaptive parameter balancing is demonstrated through experimental results presenting that the qualitative and quantitative evaluation results of each imaging task with an adaptive balancing parameter is superior to the results with a static one. The desired properties, robustness and effectiveness, of the parameter selection in a variational framework for imaging problems are achieved by merely replacing the static balancing parameter with our adaptive one. version:1
arxiv-1609-02020 | Random matrices meet machine learning: a large dimensional analysis of LS-SVM | http://arxiv.org/abs/1609.02020 | id:1609.02020 author:Zhenyu Liao, Romain Couillet category:stat.ML cs.LG  published:2016-09-07 summary:This article proposes a performance analysis of kernel least squares support vector machines (LS-SVMs) based on a random matrix approach, in the regime where both the dimension of data $p$ and their number $n$ grow large at the same rate. Under a two-class Gaussian mixture model for the input data, we prove that the LS-SVM decision function is asymptotically normal with means and covariances shown to depend explicitly on the derivatives of the kernel function. This provides improved understanding along with new insights into the internal workings of SVM-type methods for large datasets. version:2
arxiv-1609-02284 | Learning Action Concept Trees and Semantic Alignment Networks from Image-Description Data | http://arxiv.org/abs/1609.02284 | id:1609.02284 author:Jiyang Gao, Ram Nevatia category:cs.CV  published:2016-09-08 summary:Action classification in still images has been a popular research topic in computer vision. Labelling large scale datasets for action classification requires tremendous manual work, which is hard to scale up. Besides, the action categories in such datasets are pre-defined and vocabularies are fixed. However humans may describe the same action with different phrases, which leads to the difficulty of vocabulary expansion for traditional fully-supervised methods. We observe that large amounts of images with sentence descriptions are readily available on the Internet. The sentence descriptions can be regarded as weak labels for the images, which contain rich information and could be used to learn flexible expressions of action categories. We propose a method to learn an Action Concept Tree (ACT) and an Action Semantic Alignment (ASA) model for classification from image-description data via a two-stage learning process. A new dataset for the task of learning actions from descriptions is built. Experimental results show that our method outperforms several baseline methods significantly. version:1
arxiv-1609-02243 | Determination of Pedestrian Flow Performance Based on Video Tracking and Microscopic Simulations | http://arxiv.org/abs/1609.02243 | id:1609.02243 author:Kardi Teknomo, Yasushi Takeyama, Hajime Inamura category:cs.CV cs.CY cs.MA  published:2016-09-08 summary:One of the objectives of understanding pedestrian behavior is to predict the effect of proposed changes in the design or evaluation of pedestrian facilities. We want to know the impact to the user of the facilities, as the design of the facilities change. That impact was traditionally evaluated by level of service standards. Another design criterion to measure the impact of design change is measured by the pedestrian flow performance index. This paper describes the determination of pedestrian flow performance based video tracking or any microscopic pedestrian simulation models. Most of pedestrian researches have been done on a macroscopic level, which is an aggregation of all pedestrian movement in pedestrian areas into flow, average speed and area module. Macroscopic level, however, does not consider the interaction between pedestrians. It is also not well suited for prediction of pedestrian flow performance in pedestrian areas or in buildings with some obstruction, that reduces the effective width of the walkways. On the other hand, the microscopic level has a more general usage and considers detail in the design. More efficient pedestrian flow can even be reached with less space. Those results have rejected the linearity assumption of space and flow in the macroscopic level. version:1
arxiv-1609-02228 | Backpropagation of Hebbian plasticity for lifelong learning | http://arxiv.org/abs/1609.02228 | id:1609.02228 author:Thomas Miconi category:cs.NE cs.AI cs.LG q-bio.NC I.2.6  published:2016-09-08 summary:Hebbian plasticity allows biological agents to learn from their lifetime experience, extending the fixed information provided by evolutionary search. Conversely, backpropagation methods can build high-performance fixed-weights networks, but are not currently equipped to design networks with Hebbian connections. Here we use backpropagation to train fully-differentiable plastic networks, such that backpropagation determines not only the baseline weights, but also the plasticity of each connection. To perform this backpropagation of Hebbian plasticity (BOHP), we derive error gradients for neural networks with Hebbian plastic connections. The equations for these gradients turn out to follow a simple, recursive form. We apply this method to train small networks for simple learning tasks inspired from classical conditioning. We show that, through Hebbian plasticity, the networks perform fast learning of unpredictable environmental features during their lifetime, successfully solving a task that fixed-weight feedforward networks cannot possibly solve. We conclude that backpropagation of Hebbian plasticity offers a powerful model for lifelong learning. version:1
arxiv-1609-02214 | Automated Segmentation of Retinal Layers from Optical Coherent Tomography Images Using Geodesic Distance | http://arxiv.org/abs/1609.02214 | id:1609.02214 author:Jinming Duan, Christopher Tench, Irene Gottlob, Frank Proudlock, Li Bai category:cs.CV  published:2016-09-07 summary:Optical coherence tomography (OCT) is a non-invasive imaging technique that can produce images of the eye at the microscopic level. OCT image segmentation to localise retinal layer boundaries is a fundamental procedure for diagnosing and monitoring the progression of retinal and optical nerve disorders. In this paper, we introduce a novel and accurate geodesic distance method (GDM) for OCT segmentation of both healthy and pathological images in either two- or three-dimensional spaces. The method uses a weighted geodesic distance by an exponential function, taking into account both horizontal and vertical intensity variations. The weighted geodesic distance is efficiently calculated from an Eikonal equation via the fast sweeping method. The segmentation is then realised by solving an ordinary differential equation with the geodesic distance. The results of the GDM are compared with manually segmented retinal layer boundaries/surfaces. Extensive experiments demonstrate that the proposed GDM is robust to complex retinal structures with large curvatures and irregularities and it outperforms the parametric active contour algorithm as well as the graph theoretic based approaches for delineating the retinal layers in both healthy and pathological images. version:1
arxiv-1609-02208 | Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation | http://arxiv.org/abs/1609.02208 | id:1609.02208 author:Weihao Gao, Sewoong Oh, Pramod Viswanath category:cs.IT cs.LG math.IT stat.ML  published:2016-09-07 summary:Estimators of information theoretic measures such as entropy and mutual information are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with a globally chosen bandwidth). In this paper, we combine both these approaches to design new estimators of entropy and mutual information that outperform state of the art methods. Our estimator uses local bandwidth choices of $k$-NN distances with a finite $k$, independent of the sample size. Such a local and data dependent choice improves performance in practice, but the bandwidth is vanishing at a fast rate, leading to a non-vanishing bias. We show that the asymptotic bias of the proposed estimator is universal; it is independent of the underlying distribution. Hence, it can be pre-computed and subtracted from the estimate. As a byproduct, we obtain a unified way of obtaining both kernel and NN estimators. The corresponding theoretical contribution relating the asymptotic geometry of nearest neighbors to order statistics is of independent mathematical interest. version:1
arxiv-1609-02200 | Discrete Variational Autoencoders | http://arxiv.org/abs/1609.02200 | id:1609.02200 author:Jason Tyler Rolfe category:stat.ML cs.LG  published:2016-09-07 summary:Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We introduce a novel class of probabilistic models, comprising an undirected discrete component and a directed hierarchical continuous component, that can be trained efficiently using the variational autoencoder framework. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, OMNIGLOT, and Caltech-101 Silhouettes datasets. version:1
arxiv-1609-02135 | Optimizing Codes for Source Separation in Compressed Video Recovery and Color Image Demosaicing | http://arxiv.org/abs/1609.02135 | id:1609.02135 author:Alankar Kotwal, Ajit Rajwade category:cs.CV  published:2016-09-07 summary:There exist several applications in image processing (eg: video compressed sensing and color image demosaicing) which require separation of constituent images given measurements in the form of a coded superposition of those images. Physically practical code patterns in these applications are non-negative and do not obey the nice coherence properties of other patterns such as Gaussian codes, which can adversely affect reconstruction performance. The contribution of this paper is to design code patterns for video compressed sensing and demosaicing by minimizing the mutual coherence given a fixed dictionary. Our method explicitly takes into account the special structure of those code patterns as required by these applications: (1) non-negativity, (2) block-diagonal nature, and (3) circular shifting. In particular, the last property enables for accurate patchwise reconstruction. version:1
arxiv-1609-02132 | UberNet: Training a `Universal' Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory | http://arxiv.org/abs/1609.02132 | id:1609.02132 author:Iasonas Kokkinos category:cs.CV cs.AI cs.LG  published:2016-09-07 summary:In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture that is trained end-to-end. Such a universal network can act like a `swiss knife' for vision tasks; we call this architecture an UberNet to indicate its overarching nature. We address two main technical challenges that emerge when broadening up the range of tasks handled by a single CNN: (i) training a deep architecture while relying on diverse training sets and (ii) training many (potentially unlimited) tasks with a limited memory budget. Properly addressing these two problems allows us to train accurate predictors for a host of tasks, without compromising accuracy. Through these advances we train in an end-to-end manner a CNN that simultaneously addresses (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary detection, (g) region proposal generation and object detection. We obtain competitive performance while jointly addressing all of these tasks in 0.7 seconds per frame on a single GPU. A demonstration of this system can be found at http://cvn.ecp.fr/ubernet/. version:1
arxiv-1609-01704 | Hierarchical Multiscale Recurrent Neural Networks | http://arxiv.org/abs/1609.01704 | id:1609.01704 author:Junyoung Chung, Sungjin Ahn, Yoshua Bengio category:cs.LG  published:2016-09-06 summary:Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling. version:2
arxiv-1609-02087 | Clearing the Skies: A deep network architecture for single-image rain removal | http://arxiv.org/abs/1609.02087 | id:1609.02087 author:Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao, John Paisley category:cs.CV  published:2016-09-07 summary:We introduce a deep network architecture called DerainNet for removing rain streaks from an image. Based on the deep convolutional neural network (CNN), we directly learn the mapping relationship between rainy and clean image detail layers from data. Because we do not possess the ground truth corresponding to real-world rainy images, we synthesize images with rain for training. To effectively and efficiently train the network, different with common strategies that roughly increase depth or breadth of network, we utilize some image processing domain knowledge to modify the objective function. Specifically, we train our DerainNet on the detail layer rather than the image domain. Better results can be obtained under the same net architecture. Though DerainNet is trained on synthetic data, we still find that the learned network is very effective on real-world images for testing. Moreover, we augment the CNN framework with image enhancement to significantly improve the visual results. Compared with state-of-the- art single image de-rain methods, our method has better rain removal and much faster computation time after network training. version:1
arxiv-1609-02077 | Visual Saliency Detection Based on Multiscale Deep CNN Features | http://arxiv.org/abs/1609.02077 | id:1609.02077 author:Guanbin Li, Yizhou Yu category:cs.CV  published:2016-09-07 summary:Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision. In this paper, we discover that a high-quality visual saliency model can be learned from multiscale features extracted using deep convolutional neural networks (CNNs), which have had many successes in visual recognition tasks. For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for feature extraction at three different scales. The penultimate layer of our neural network has been confirmed to be a discriminative high-level feature vector for saliency detection, which we call deep contrast feature. To generate a more robust feature, we integrate handcrafted low-level features with our deep contrast feature. To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotations. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks, improving the F- measure by 6.12% and 10.0% respectively on the DUT-OMRON dataset and our new dataset (HKU-IS), and lowering the mean absolute error by 9% and 35.3% respectively on these two datasets. version:1
arxiv-1609-02075 | The Social Dynamics of Language Change in Online Networks | http://arxiv.org/abs/1609.02075 | id:1609.02075 author:Rahul Goel, Sandeep Soni, Naman Goyal, John Paparrizos, Hanna Wallach, Fernando Diaz, Jacob Eisenstein category:cs.CL cs.SI physics.soc-ph I.2.7; J.4; J.5  published:2016-09-07 summary:Language change is a complex social phenomenon, revealing pathways of communication and sociocultural influence. But, while language change has long been a topic of study in sociolinguistics, traditional linguistic research methods rely on circumstantial evidence, estimating the direction of change from differences between older and younger speakers. In this paper, we use a data set of several million Twitter users to track language changes in progress. First, we show that language change can be viewed as a form of social influence: we observe complex contagion for phonetic spellings and "netspeak" abbreviations (e.g., lol), but not for older dialect markers from spoken language. Next, we test whether specific types of social network connections are more influential than others, using a parametric Hawkes process model. We find that tie strength plays an important role: densely embedded social ties are significantly better conduits of linguistic influence. Geographic locality appears to play a more limited role: we find relatively little evidence to support the hypothesis that individuals are more influenced by geographically local social ties, even in their usage of geographical dialect markers. version:1
arxiv-1609-01625 | Discriminating image textures with the multiscale two-dimensional complexity-entropy causality plane | http://arxiv.org/abs/1609.01625 | id:1609.01625 author:Luciano Zunino, Haroldo V. Ribeiro category:physics.data-an cond-mat.stat-mech cs.CV  published:2016-09-06 summary:The aim of this paper is to further explore the usefulness of the two-dimensional complexity-entropy causality plane as a texture image descriptor. A multiscale generalization is introduced in order to distinguish between different roughness features of images at small and large spatial scales. Numerically generated two-dimensional structures are initially considered for illustrating basic concepts in a controlled framework. Then, more realistic situations are studied. Obtained results allow us to confirm that intrinsic spatial correlations of images are successfully unveiled by implementing this multiscale symbolic information-theory approach. Consequently, we conclude that the proposed representation space is a versatile and practical tool for identifying, characterizing and discriminating image textures. version:2
arxiv-1609-02053 | Fast and Efficient Asynchronous Neural Computation with Adapting Spiking Neural Networks | http://arxiv.org/abs/1609.02053 | id:1609.02053 author:Davide Zambrano, Sander M. Bohte category:cs.NE  published:2016-09-07 summary:Biological neurons communicate with a sparing exchange of pulses - spikes. It is an open question how real spiking neurons produce the kind of powerful neural computation that is possible with deep artificial neural networks, using only so very few spikes to communicate. Building on recent insights in neuroscience, we present an Adapting Spiking Neural Network (ASNN) based on adaptive spiking neurons. These spiking neurons efficiently encode information in spike-trains using a form of Asynchronous Pulsed Sigma-Delta coding while homeostatically optimizing their firing rate. In the proposed paradigm of spiking neuron computation, neural adaptation is tightly coupled to synaptic plasticity, to ensure that downstream neurons can correctly decode upstream spiking neurons. We show that this type of network is inherently able to carry out asynchronous and event-driven neural computation, while performing identical to corresponding artificial neural networks (ANNs). In particular, we show that these adaptive spiking neurons can be drop in replacements for ReLU neurons in standard feedforward ANNs comprised of such units. We demonstrate that this can also be successfully applied to a ReLU based deep convolutional neural network for classifying the MNIST dataset. The ASNN thus outperforms current Spiking Neural Networks (SNNs) implementations, while responding (up to) an order of magnitude faster and using an order of magnitude fewer spikes. Additionally, in a streaming setting where frames are continuously classified, we show that the ASNN requires substantially fewer network updates as compared to the corresponding ANN. version:1
arxiv-1609-02043 | Feasibility of Post-Editing Speech Transcriptions with a Mismatched Crowd | http://arxiv.org/abs/1609.02043 | id:1609.02043 author:Purushotam Radadia, Shirish Karande category:cs.AI cs.CL  published:2016-09-07 summary:Manual correction of speech transcription can involve a selection from plausible transcriptions. Recent work has shown the feasibility of employing a mismatched crowd for speech transcription. However, it is yet to be established whether a mismatched worker has sufficiently fine-granular speech perception to choose among the phonetically proximate options that are likely to be generated from the trellis of an ASRU. Hence, we consider five languages, Arabic, German, Hindi, Russian and Spanish. For each we generate synthetic, phonetically proximate, options which emulate post-editing scenarios of varying difficulty. We consistently observe non-trivial crowd ability to choose among fine-granular options. version:1
arxiv-1609-02036 | Deep Markov Random Field for Image Modeling | http://arxiv.org/abs/1609.02036 | id:1609.02036 author:Zhirong Wu, Dahua Lin, Xiaoou Tang category:cs.CV cs.AI cs.LG  published:2016-09-07 summary:Markov Random Fields (MRFs), a formulation widely used in generative image modeling, have long been plagued by the lack of expressive power. This issue is primarily due to the fact that conventional MRFs formulations tend to use simplistic factors to capture local patterns. In this paper, we move beyond such limitations, and propose a novel MRF model that uses fully-connected neurons to express the complex interactions among pixels. Through theoretical analysis, we reveal an inherent connection between this model and recurrent neural networks, and thereon derive an approximated feed-forward network that couples multiple RNNs along opposite directions. This formulation combines the expressive power of deep neural networks and the cyclic dependency structure of MRF in a unified model, bringing the modeling capability to a new level. The feed-forward approximation also allows it to be efficiently learned from data. Experimental results on a variety of low-level vision tasks show notable improvement over state-of-the-arts. version:1
arxiv-1609-01984 | Human Body Orientation Estimation using Convolutional Neural Network | http://arxiv.org/abs/1609.01984 | id:1609.01984 author:Jinyoung Choi, Beom-Jin Lee, Byoung-Tak Zhang category:cs.RO cs.CV cs.LG  published:2016-09-07 summary:Personal robots are expected to interact with the user by recognizing the user's face. However, in most of the service robot applications, the user needs to move himself/herself to allow the robot to see him/her face to face. To overcome such limitations, a method for estimating human body orientation is required. Previous studies used various components such as feature extractors and classification models to classify the orientation which resulted in low performance. For a more robust and accurate approach, we propose the light weight convolutional neural networks, an end to end system, for estimating human body orientation. Our body orientation estimation model achieved 81.58% and 94% accuracy with the benchmark dataset and our own dataset respectively. The proposed method can be used in a wide range of service robot applications which depend on the ability to estimate human body orientation. To show its usefulness in service robot applications, we designed a simple robot application which allows the robot to move towards the user's frontal plane. With this, we demonstrated an improved face detection rate. version:1
arxiv-1609-01977 | Doubly Stochastic Neighbor Embedding on Spheres | http://arxiv.org/abs/1609.01977 | id:1609.01977 author:Yao Lu, Zhirong Yang, Jukka Corander category:cs.LG  published:2016-09-07 summary:Recently, Stochastic Neighbor Embedding (SNE) methods have widely been applied in data visualization. These methods minimize the divergence between the pairwise similarities of high- and low-dimensional data. Despite their popularity, the current SNE methods experience the "crowding problem" when the data include highly imbalanced similarities. This implies that the data points with higher total similarity tend to get crowded around the display center. To solve this problem, we normalize the similarity matrix to be doubly stochastic such that all the data points have equal total similarities. A fast normalization method is proposed. Furthermore, we show empirically and theoretically that the doubly stochasticity constraint often leads to approximately spherical embeddings. This suggests replacing a flat space with spheres as the embedding space. The spherical embedding eliminates the discrepancy between the center and the periphery in visualization and thus resolves the "crowding problem". We compared the proposed method with the state-of-the-art SNE method on three real-world datasets. The results indicate that our method is more favorable in terms of visualization quality. version:1
arxiv-1609-01962 | Using Gaussian Processes for Rumour Stance Classification in Social Media | http://arxiv.org/abs/1609.01962 | id:1609.01962 author:Michal Lukasik, Kalina Bontcheva, Trevor Cohn, Arkaitz Zubiaga, Maria Liakata, Rob Procter category:cs.CL cs.IR cs.SI  published:2016-09-07 summary:Social media tend to be rife with rumours while new reports are released piecemeal during breaking news. Interestingly, one can mine multiple reactions expressed by social media users in those situations, exploring their stance towards rumours, ultimately enabling the flagging of highly disputed rumours as being potentially false. In this work, we set out to develop an automated, supervised classifier that uses multi-task learning to classify the stance expressed in each individual tweet in a rumourous conversation as either supporting, denying or questioning the rumour. Using a classifier based on Gaussian Processes, and exploring its effectiveness on two datasets with very different characteristics and varying distributions of stances, we show that our approach consistently outperforms competitive baseline classifiers. Our classifier is especially effective in estimating the distribution of different types of stance associated with a given rumour, which we set forth as a desired characteristic for a rumour-tracking system that will warn both ordinary users of Twitter and professional news practitioners when a rumour is being rebutted. version:1
arxiv-1609-01958 | Object Tracking via Dynamic Feature Selection Processes | http://arxiv.org/abs/1609.01958 | id:1609.01958 author:Giorgio Roffo, Simone Melzi category:cs.CV  published:2016-09-07 summary:DFST proposes an optimized visual tracking algorithm based on the real-time selection of locally and temporally discriminative features. A feature selection mechanism is embedded in the Adaptive colour Names (CN) tracking system that adaptively selects the top-ranked discriminative features for tracking. DFST provides a significant gain in accuracy and precision allowing the use of a dynamic set of features that results in an increased system flexibility. DFST is based on the unsupervised method "Infinite Feature Selection" (Inf-FS), which ranks features according with their "redundancy" without using class labels. By using a fast online algorithm for learning dictionaries the size of the box is adapted during the processing. At each update, we use multiple examples around the target (at different positions and scales). DFST also improved the CN by adding micro-shift at the predicted position and bounding box adaptation. version:1
arxiv-1609-01933 | Sentiment Classification of Food Reviews | http://arxiv.org/abs/1609.01933 | id:1609.01933 author:Hua Feng, Ruixi Lin category:cs.CL  published:2016-09-07 summary:Sentiment analysis of reviews is a popular task in natural language processing. In this work, the goal is to predict the score of food reviews on a scale of 1 to 5 with two recurrent neural networks that are carefully tuned. As for baseline, we train a simple RNN for classification. Then we extend the baseline to GRU. In addition, we present two different methods to deal with highly skewed data, which is a common problem for reviews. Models are evaluated using accuracies. version:1
arxiv-1609-01932 | A three-dimensional approach to Visual Speech Recognition using Discrete Cosine Transforms | http://arxiv.org/abs/1609.01932 | id:1609.01932 author:Toni Heidenreich, Michael W. Spratling category:cs.CV  published:2016-09-07 summary:Visual speech recognition aims to identify the sequence of phonemes from continuous speech. Unlike the traditional approach of using 2D image feature extraction methods to derive features of each video frame separately, this paper proposes a new approach using a 3D (spatio-temporal) Discrete Cosine Transform to extract features of each feasible sub-sequence of an input video which are subsequently classified individually using Support Vector Machines and combined to find the most likely phoneme sequence using a tailor-made Hidden Markov Model. The algorithm is trained and tested on the VidTimit database to recognise sequences of phonemes as well as visemes (visual speech units). Furthermore, the system is extended with the training on phoneme or viseme pairs (biphones) to counteract the human speech ambiguity of co-articulation. The test set accuracy for the recognition of phoneme sequences is 20%, and the accuracy of viseme sequences is 39%. Both results improve the best values reported in other papers by approximately 2%. The contribution of the result is three-fold: Firstly, this paper is the first to show that 3D feature extraction methods can be applied to continuous sequence recognition tasks despite the unknown start positions and durations of each phoneme. Secondly, the result confirms that 3D feature extraction methods improve the accuracy compared to 2D features extraction methods. Thirdly, the paper is the first to specifically compare an otherwise identical method with and without using biphones, verifying that the usage of biphones has a positive impact on the result. version:1
arxiv-1609-01926 | A modular architecture for transparent computation in Recurrent Neural Networks | http://arxiv.org/abs/1609.01926 | id:1609.01926 author:Giovanni Sirio Carmantini, Peter beim Graben, Mathieu Desroches, Serafim Rodrigues category:cs.NE cs.AI cs.CL cs.FL cs.SC  published:2016-09-07 summary:Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as to transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Goedelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: i) the design of a Central Pattern Generator from a finite-state locomotive controller, and ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments. version:1
arxiv-1609-00836 | Towards Segmenting Consumer Stereo Videos: Benchmark, Baselines and Ensembles | http://arxiv.org/abs/1609.00836 | id:1609.00836 author:Wei-Chen Chiu, Fabio Galasso, Mario Fritz category:cs.CV  published:2016-09-03 summary:Are we ready to segment consumer stereo videos? The amount of this data type is rapidly increasing and encompasses rich information of appearance, motion and depth cues. However, the segmentation of such data is still largely unexplored. First, we propose therefore a new benchmark: videos, annotations and metrics to measure progress on this emerging challenge. Second, we evaluate several state of the art segmentation methods and propose a novel ensemble method based on recent spectral theory. This combines existing image and video segmentation techniques in an efficient scheme. Finally, we propose and integrate into this model a novel regressor, learnt to optimize the stereo segmentation performance directly via a differentiable proxy. The regressor makes our segmentation ensemble adaptive to each stereo video and outperforms the segmentations of the ensemble as well as a most recent RGB-D segmentation technique. version:2
arxiv-1609-01915 | Polyp Detection and Segmentation from Video Capsule Endoscopy: A Review | http://arxiv.org/abs/1609.01915 | id:1609.01915 author:V. B. Surya Prasath category:cs.CV  published:2016-09-07 summary:Video capsule endoscopy (VCE) is used widely nowadays for visualizing the gastrointestinal (GI) tract. Capsule endoscopy exams are prescribed usually as an additional monitoring mechanism and can help in identifying polyps, bleeding, etc. To analyze the large scale video data produced by VCE exams automatic image processing, computer vision, and learning algorithms are required. Recently, automatic polyp detection algorithms have been proposed with various degrees of success. Though polyp detection in colonoscopy and other traditional endoscopy procedure based images is becoming a mature field, due to its unique imaging characteristics detecting polyps automatically in VCE is a hard problem. We review different polyp detection approaches for VCE imagery and provide systematic analysis with challenges faced by standard image processing and computer vision methods. version:1
arxiv-1609-01459 | Deviant Learning Algorithm: Learning Sparse Mismatch Representations through Time and Space | http://arxiv.org/abs/1609.01459 | id:1609.01459 author:Emmanuel Ndidi Osegi, Vincent Ike Anireh category:cs.AI cs.NE  published:2016-09-06 summary:Predictive coding (PDC) has recently attracted attention in the neuroscience and computing community as a candidate unifying paradigm for neuronal studies and artificial neural network implementations particularly targeted at unsupervised learning systems. The Mismatch Negativity (MMN) has also recently been studied in relation to PC and found to be a useful ingredient in neural predictive coding systems. Backed by the behavior of living organisms, such networks are particularly useful in forming spatio-temporal transitions and invariant representations of the input world. However, most neural systems still do not account for large number of synapses even though this has been shown by a few machine learning researchers as an effective and very important component of any neural system if such a system is to behave properly. Our major point here is that PDC systems with the MMN effect in addition to a large number of synapses can greatly improve any neural learning system's performance and ability to make decisions in the machine world. In this paper, we propose a novel bio-mimetic computational intelligence algorithm -- the Deviant Learning Algorithm, inspired by these key ideas and functional properties of recent brain-cognitive discoveries and theories. We also show by numerical experiments guided by theoretical insights, how our invented bio-mimetic algorithm can achieve competitive predictions even with very small problem specific data. version:2
arxiv-1609-00496 | Label distribution based facial attractiveness computation by deep residual learning | http://arxiv.org/abs/1609.00496 | id:1609.00496 author:Shu Liu, Bo Li, Yangyu Fan, Zhe Guo, Ashok Samal category:cs.CV  published:2016-09-02 summary:Two challenges lie in the facial attractiveness computation research: the lack of true attractiveness labels (scores), and the lack of an accurate face representation. In order to address the first challenge, this paper recasts facial attractiveness computation as a label distribution learning (LDL) problem rather than a traditional single-label supervised learning task. In this way, the negative influence of the label incomplete problem can be reduced. Inspired by the recent promising work in face recognition using deep neural networks to learn effective features, the second challenge is expected to be solved from a deep learning point of view. A very deep residual network is utilized to enable automatic learning of hierarchical aesthetics representation. Integrating these two ideas, an end-to-end deep learning framework is established. Our approach achieves the best results on a standard benchmark SCUT-FBP dataset compared with other state-of-the-art work. version:2
arxiv-1609-01885 | DAISEE: Dataset for Affective States in E-Learning Environments | http://arxiv.org/abs/1609.01885 | id:1609.01885 author:Abhay Gupta, Richik Jaiswal, Sagar Adhikari, Vineeth Balasubramanian category:cs.CV cs.LG  published:2016-09-07 summary:Extracting and understanding affective states of subjects through analysis of face images/videos is of high consequence to advance the levels of interaction in human-computer interfaces. This paper aims to highlight vision-related tasks focused on understanding "reactions" of subjects to presented content which has not been largely studied by the vision community in comparison to other emotions. To facilitate future study in this field, we present an effort in collecting DAiSEE, a free to use large-scale dataset using crowd annotation, that not only simulates a real world setting for e-learning environments, but also captures the interpretability issues of such affective states by human annotators. In addition to the dataset, we present benchmark results based on standard baseline methods and vote aggregation strategies, thus providing a springboard for further research. version:1
arxiv-1609-01882 | Polysemous codes | http://arxiv.org/abs/1609.01882 | id:1609.01882 author:Matthijs Douze, Herv√© J√©gou, Florent Perronnin category:cs.CV  published:2016-09-07 summary:This paper considers the problem of approximate nearest neighbor search in the compressed domain. We introduce polysemous codes, which offer both the distance estimation quality of product quantization and the efficient comparison of binary codes with Hamming distance. Their design is inspired by algorithms introduced in the 90's to construct channel-optimized vector quantizers. At search time, this dual interpretation accelerates the search. Most of the indexed vectors are filtered out with Hamming distance, letting only a fraction of the vectors to be ranked with an asymmetric distance estimator. The method is complementary with a coarse partitioning of the feature space such as the inverted multi-index. This is shown by our experiments performed on several public benchmarks such as the BIGANN dataset comprising one billion vectors, for which we report state-of-the-art results for query times below 0.3\,millisecond per core. Last but not least, our approach allows the approximate computation of the k-NN graph associated with the Yahoo Flickr Creative Commons 100M, described by CNN image descriptors, in less than 8 hours on a single machine. version:1
arxiv-1609-01872 | Chaining Bounds for Empirical Risk Minimization | http://arxiv.org/abs/1609.01872 | id:1609.01872 author:G√°bor Bal√°zs, Andr√°s Gy√∂rgy, Csaba Szepesv√°ri category:stat.ML cs.LG  published:2016-09-07 summary:This paper extends the standard chaining technique to prove excess risk upper bounds for empirical risk minimization with random design settings even if the magnitude of the noise and the estimates is unbounded. The bound applies to many loss functions besides the squared loss, and scales only with the sub-Gaussian or subexponential parameters without further statistical assumptions such as the bounded kurtosis condition over the hypothesis class. A detailed analysis is provided for slope constrained and penalized linear least squares regression with a sub-Gaussian setting, which often proves tight sample complexity bounds up to logartihmic factors. version:1
arxiv-1609-01859 | Automatic Visual Theme Discovery from Joint Image and Text Corpora | http://arxiv.org/abs/1609.01859 | id:1609.01859 author:Ke Sun, Xianxu Hou, Qian Zhang, Guoping Qiu category:cs.CV  published:2016-09-07 summary:A popular approach to semantic image understanding is to manually tag images with keywords and then learn a mapping from vi- sual features to keywords. Manually tagging images is a subjective pro- cess and the same or very similar visual contents are often tagged with different keywords. Furthermore, not all tags have the same descriptive power for visual contents and large vocabulary available from natural language could result in a very diverse set of keywords. In this paper, we propose an unsupervised visual theme discovery framework as a better (more compact, efficient and effective) alternative to semantic represen- tation of visual contents. We first show that tag based annotation lacks consistency and compactness for describing visually similar contents. We then learn the visual similarity between tags based on the visual features of the images containing the tags. At the same time, we use a natural language processing technique (word embedding) to measure the seman- tic similarity between tags. Finally, we cluster tags into visual themes based on their visual similarity and semantic similarity measures using a spectral clustering algorithm. We conduct user studies to evaluate the effectiveness and rationality of the visual themes discovered by our unsu- pervised algorithm and obtains promising result. We then design three common computer vision tasks, example based image search, keyword based image search and image labelling to explore potential applica- tion of our visual themes discovery framework. In experiments, visual themes significantly outperforms tags on semantic image understand- ing and achieve state-of-art performance in all three tasks. This again demonstrate the effectiveness and versatility of proposed framework. version:1
arxiv-1609-01840 | Learning Boltzmann Machine with EM-like Method | http://arxiv.org/abs/1609.01840 | id:1609.01840 author:Jinmeng Song, Chun Yuan category:cs.LG stat.ML  published:2016-09-07 summary:We propose an expectation-maximization-like(EMlike) method to train Boltzmann machine with unconstrained connectivity. It adopts Monte Carlo approximation in the E-step, and replaces the intractable likelihood objective with efficiently computed objectives or directly approximates the gradient of likelihood objective in the M-step. The EM-like method is a modification of alternating minimization. We prove that EM-like method will be the exactly same with contrastive divergence in restricted Boltzmann machine if the M-step of this method adopts special approximation. We also propose a new measure to assess the performance of Boltzmann machine as generative models of data, and its computational complexity is O(Rmn). Finally, we demonstrate the performance of EM-like method using numerical experiments. version:1
arxiv-1609-01839 | Guided Filter based Edge-preserving Image Non-blind Deconvolution | http://arxiv.org/abs/1609.01839 | id:1609.01839 author:Hang Yang, Ming Zhu, Zhongbo Zhang, Heyan Huang category:cs.CV  published:2016-09-07 summary:In this work, we propose a new approach for efficient edge-preserving image deconvolution. Our algorithm is based on a novel type of explicit image filter - guided filter. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter, but has better behaviors near edges. We propose an efficient iterative algorithm with the decouple of deblurring and denoising steps in the restoration process. In deblurring step, we proposed two cost function which could be computed with fast Fourier transform efficiently. The solution of the first one is used as the guidance image, and another solution will be filtered in next step. In the denoising step, the guided filter is used with the two obtained images for efficient edge-preserving filtering. Furthermore, we derive a simple and effective method to automatically adjust the regularization parameter at each iteration. We compare our deconvolution algorithm with many competitive deconvolution techniques in terms of ISNR and visual quality. version:1
arxiv-1609-01819 | Semantic Video Trailers | http://arxiv.org/abs/1609.01819 | id:1609.01819 author:Harrie Oosterhuis, Sujith Ravi, Michael Bendersky category:cs.LG cs.CV  published:2016-09-07 summary:Query-based video summarization is the task of creating a brief visual trailer, which captures the parts of the video (or a collection of videos) that are most relevant to the user-issued query. In this paper, we propose an unsupervised label propagation approach for this task. Our approach effectively captures the multimodal semantics of queries and videos using state-of-the-art deep neural networks and creates a summary that is both semantically coherent and visually attractive. We describe the theoretical framework of our graph-based approach and empirically evaluate its effectiveness in creating relevant and attractive trailers. Finally, we showcase example video trailers generated by our system. version:1
arxiv-1609-01810 | Tracking System to Automate Data Collection of Microscopic Pedestrian Traffic Flow | http://arxiv.org/abs/1609.01810 | id:1609.01810 author:Kardi Teknomo, Yasushi Takeyama, Hajime Inamura category:cs.CV cs.CY cs.MA  published:2016-09-07 summary:To deal with many pedestrian data, automatic data collection is needed. This paper describes how to automate the microscopic pedestrian flow data collection from video files. The study is restricted only to pedestrians without considering vehicular - pedestrian interaction. Pedestrian tracking system consists of three sub-systems, which calculates the image processing, object tracking and traffic flow variables. The system receives input of stacks of images and parameters. The first sub-system performs Image Processing analysis while the second sub-system carries out the tracking of pedestrians by matching the features and tracing the pedestrian numbers frame by frame. The last sub-system deals with a NTXY database to calculate the pedestrian traffic-flow characteristic such as flow rate, speed and area module. Comparison with manual data collection method confirmed that the procedures described have significant potential to automate the data collection of both microscopic and macroscopic pedestrian flow variables. version:1
arxiv-1609-02137 | Tracking Algorithm for Microscopic Flow Data Collection | http://arxiv.org/abs/1609.02137 | id:1609.02137 author:Kardi Teknomo, Yasushi Takeyama, Hajime Inamura category:cs.CV cs.CY  published:2016-09-07 summary:Various methods to automate traffic data collection have recently been developed by many researchers. A macroscopic data collection through image processing has been proposed. For microscopic traffic flow data, such as individual speed and time or distance headway, tracking of individual movement is needed. The tracking algorithms for pedestrian or vehicle have been developed to trace the movement of one or two pedestrians based on sign pattern, and feature detection. No research has been done to track many pedestrians or vehicles at once. This paper describes a new and fast algorithm to track the movement of many individual vehicles or pedestrians version:1
arxiv-1609-01805 | A Boosting Method to Face Image Super-resolution | http://arxiv.org/abs/1609.01805 | id:1609.01805 author:Shanjun Mao, Da Zhou, Yiping Zhang, Zhihong Zhang category:cs.CV  published:2016-09-07 summary:Recently sparse representation has gained great success in face image super-resolution. The conventional sparsity-based methods enforce sparse coding on face image patches and the representation fidelity is measured by $\ell_{2}$-norm. Such a sparse coding model regularizes all facial patches equally, which however ignores distinct natures of different facial patches for image reconstruction. In this paper, we propose a new weighted-patch super-resolution method based on AdaBoost. Specifically, in each iteration of the AdaBoost operation, each facial patch is weighted automatically according to the performance of the model on it, so as to highlight those pathes that are more critical for improving the reconstruction power in next step. In this way, through the AdaBoost training procedure, we can focus more on the patches (face regions) with richer information. Various experimental results on standard face databased show that our proposed method outperforms state-of-the-art methods in terms of both objective metrics and visual quality. version:1
arxiv-1609-00408 | Defeating Image Obfuscation with Deep Learning | http://arxiv.org/abs/1609.00408 | id:1609.00408 author:Richard McPherson, Reza Shokri, Vitaly Shmatikov category:cs.CR cs.CV  published:2016-09-01 summary:We demonstrate that modern image recognition methods based on artificial neural networks can recover hidden information from images protected by various forms of obfuscation. The obfuscation techniques considered in this paper are mosaicing (also known as pixelation), blurring (as used by YouTube), and P3, a recently proposed system for privacy-preserving photo sharing that encrypts the significant JPEG coefficients to make images unrecognizable by humans. We empirically show how to train artificial neural networks to successfully identify faces and recognize objects and handwritten digits even if the images are protected using any of the above obfuscation techniques. version:2
arxiv-1609-01743 | Human pose estimation via Convolutional Part Heatmap Regression | http://arxiv.org/abs/1609.01743 | id:1609.01743 author:Adrian Bulat, Georgios Tzimiropoulos category:cs.CV  published:2016-09-06 summary:This paper is on human pose estimation using Convolutional Neural Networks. Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. To this end, we propose a detection-followed-by-regression CNN cascade. The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps. The benefits of the proposed architecture are multi-fold: It guides the network where to focus in the image and effectively encodes part constraints and context. More importantly, it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression part of our network to rely on contextual information in order to predict the location of these parts. Additionally, we show that the proposed cascade is flexible enough to readily allow the integration of various CNN architectures for both detection and regression, including recent ones based on residual learning. Finally, we illustrate that our cascade achieves top performance on the MPII and LSP data sets. Code can be downloaded from http://www.cs.nott.ac.uk/~psxab5/ version:1
arxiv-1609-01672 | Law of Large Graphs | http://arxiv.org/abs/1609.01672 | id:1609.01672 author:Runze Tang, Michael Ketcha, Joshua T. Vogelstein, Carey E. Priebe, Daniel L. Sussman category:stat.ME stat.ML  published:2016-09-06 summary:Estimating the mean of a population of graphs based on a sample is a core problem in network science. Often, this problem is especially difficult because the sample or cohort size is relatively small as compared to the number of parameters to estimate. While using the element-wise sample mean of the adjacency matrices is a common approach, this method does not exploit any underlying graph structure. We propose using a low-rank method together with tools for dimension selection and diagonal augmentation to improve performance over the naive methodology for small sample sizes. Theoretical results for the stochastic blockmodel show that this method will offer major improvements when there are many vertices. Similarly, in analyzing human connectome data, we demonstrate that the low-rank methods outperform the standard sample mean for many settings. These results indicate that low-rank methods should be a key part of the tool box for researchers studying populations of graphs. version:1
arxiv-1609-01670 | Improving Color Constancy by Discounting the Variation of Camera Spectral Sensitivity | http://arxiv.org/abs/1609.01670 | id:1609.01670 author:Shao-Bing Gao, Ming Zhang, Chao-Yi Li, Yong-Jie Li category:cs.CV  published:2016-09-06 summary:Computational color constancy (CC) has been specially designed to recover the true color of scene by first inferring the light source color and then discounting it from the captured color biased image. However, the image colors are determined not only by the illuminant but also by the camera spectral sensitivity (CSS). This paper studies the CSS effects on CC performance by showing the insufficiency of existing learning-based CC algorithms when dealing with the situation of so called inter-dataset-based CC, i.e., training a CC model on one dataset and then testing it on another dataset that was captured by a distinct CSS. This is a key drawback that limits the wide application of many existing CC models. To overcome this problem, we propose a simple way to solve this problem by firstly learning a transform matrix that converts the responses to the illuminant and images under one CSS into those under another CSS. Then, the learned matrix is used to transform the data rendered under one CSS into another specific CSS before applying the CC model on another dataset. Theoretical analysis and experimental validation on synthetic, hyperspectral, and real camera captured images show that the proposed method can significantly improve the inter-dataset-based performance for traditional CC algorithms. Thus, we suggest that for better generalization of the state-of-the-art CC algorithms, the effect of CSS should be reasonably considered. version:1
arxiv-1609-01648 | Review of the Fingerprint Liveness Detection (LivDet) competition series: 2009 to 2015 | http://arxiv.org/abs/1609.01648 | id:1609.01648 author:Luca Ghiani, David A. Yambay, Valerio Mura, Gian Luca Marcialis, Fabio Roli, Stephanie A. Schuckers category:cs.CV  published:2016-09-06 summary:A spoof attack, a subset of presentation attacks, is the use of an artificial replica of a biometric in an attempt to circumvent a biometric sensor. Liveness detection, or presentation attack detection, distinguishes between live and fake biometric traits and is based on the principle that additional information can be garnered above and beyond the data procured by a standard authentication system to determine if a biometric measure is authentic. The goals for the Liveness Detection (LivDet) competitions are to compare software-based fingerprint liveness detection and artifact detection algorithms (Part 1), as well as fingerprint systems which incorporate liveness detection or artifact detection capabilities (Part 2), using a standardized testing protocol and large quantities of spoof and live tests. The competitions are open to all academic and industrial institutions which have a solution for either softwarebased or system-based fingerprint liveness detection. The LivDet competitions have been hosted in 2009, 2011, 2013 and 2015 and have shown themselves to provide a crucial look at the current state of the art in liveness detection schemes. There has been a noticeable increase in the number of participants in LivDet competitions as well as a noticeable decrease in error rates across competitions. Participants have grown from four to the most recent thirteen submissions for Fingerprint Part 1. Fingerprints Part 2 has held steady at two submissions each competition in 2011 and 2013 and only one for the 2015 edition. The continuous increase of competitors demonstrates a growing interest in the topic. version:1
arxiv-1609-01597 | A Hybrid Citation Retrieval Algorithm for Evidence-based Clinical Knowledge Summarization: Combining Concept Extraction, Vector Similarity and Query Expansion for High Precision | http://arxiv.org/abs/1609.01597 | id:1609.01597 author:Kalpana Raja, Andrew J Sauer, Ravi P Garg, Melanie R Klerer, Siddhartha R Jonnalagadda category:cs.CL cs.IR  published:2016-09-06 summary:Novel information retrieval methods to identify citations relevant to a clinical topic can overcome the knowledge gap existing between the primary literature (MEDLINE) and online clinical knowledge resources such as UpToDate. Searching the MEDLINE database directly or with query expansion methods returns a large number of citations that are not relevant to the query. The current study presents a citation retrieval system that retrieves citations for evidence-based clinical knowledge summarization. This approach combines query expansion, concept-based screening algorithm, and concept-based vector similarity. We also propose an information extraction framework for automated concept (Population, Intervention, Comparison, and Disease) extraction. We evaluated our proposed system on all topics (as queries) available from UpToDate for two diseases, heart failure (HF) and atrial fibrillation (AFib). The system achieved an overall F-score of 41.2% on HF topics and 42.4% on AFib topics on a gold standard of citations available in UpToDate. This is significantly high when compared to a query-expansion based baseline (F-score of 1.3% on HF and 2.2% on AFib) and a system that uses query expansion with disease hyponyms and journal names, concept-based screening, and term-based vector similarity system (F-score of 37.5% on HF and 39.5% on AFib). Evaluating the system with top K relevant citations, where K is the number of citations in the gold standard achieved a much higher overall F-score of 69.9% on HF topics and 75.1% on AFib topics. In addition, the system retrieved up to 18 new relevant citations per topic when tested on ten HF and six AFib clinical topics. version:1
arxiv-1609-01596 | Direct Feedback Alignment Provides Learning in Deep Neural Networks | http://arxiv.org/abs/1609.01596 | id:1609.01596 author:Arild N√∏kland category:stat.ML cs.LG  published:2016-09-06 summary:Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task. version:1
arxiv-1609-01594 | An Information Extraction Approach to Prescreen Heart Failure Patients for Clinical Trials | http://arxiv.org/abs/1609.01594 | id:1609.01594 author:Abhishek Kalyan Adupa, Ravi Prakash Garg, Jessica Corona-Cox, Sanjiv. J. Shah, Siddhartha R. Jonnalagadda category:cs.CL cs.CY  published:2016-09-06 summary:To reduce the large amount of time spent screening, identifying, and recruiting patients into clinical trials, we need prescreening systems that are able to automate the data extraction and decision-making tasks that are typically relegated to clinical research study coordinators. However, a major obstacle is the vast amount of patient data available as unstructured free-form text in electronic health records. Here we propose an information extraction-based approach that first automatically converts unstructured text into a structured form. The structured data are then compared against a list of eligibility criteria using a rule-based system to determine which patients qualify for enrollment in a heart failure clinical trial. We show that we can achieve highly accurate results, with recall and precision values of 0.95 and 0.86, respectively. Our system allowed us to significantly reduce the time needed for prescreening patients from a few weeks to a few minutes. Our open-source information extraction modules are available for researchers and could be tested and validated in other cardiovascular trials. An approach such as the one we demonstrate here may decrease costs and expedite clinical trials, and could enhance the reproducibility of trials across institutions and populations. version:1
arxiv-1609-01592 | CRTS: A type system for representing clinical recommendations | http://arxiv.org/abs/1609.01592 | id:1609.01592 author:Ravi P Garg, Kalpana Raja, Siddhartha R Jonnalagadda category:cs.CL cs.CY  published:2016-09-06 summary:Background: Clinical guidelines and recommendations are the driving wheels of the evidence-based medicine (EBM) paradigm, but these are available primarily as unstructured text and are generally highly heterogeneous in nature. This significantly reduces the dissemination and automatic application of these recommendations at the point of care. A comprehensive structured representation of these recommendations is highly beneficial in this regard. Objective: The objective of this paper to present Clinical Recommendation Type System (CRTS), a common type system that can effectively represent a clinical recommendation in a structured form. Methods: CRTS is built by analyzing 125 recommendations and 195 research articles corresponding to 6 different diseases available from UpToDate, a publicly available clinical knowledge system, and from the National Guideline Clearinghouse, a public resource for evidence-based clinical practice guidelines. Results: We show that CRTS not only covers the recommendations but also is flexible to be extended to represent information from primary literature. We also describe how our developed type system can be applied for clinical decision support, medical knowledge summarization, and citation retrieval. Conclusion: We showed that our proposed type system is precise and comprehensive in representing a large sample of recommendations available for various disorders. CRTS can now be used to build interoperable information extraction systems that automatically extract clinical recommendations and related data elements from clinical evidence resources, guidelines, systematic reviews and primary publications. Keywords: guidelines and recommendations, type system, clinical decision support, evidence-based medicine, information storage and retrieval version:1
arxiv-1609-01586 | A Bootstrap Machine Learning Approach to Identify Rare Disease Patients from Electronic Health Records | http://arxiv.org/abs/1609.01586 | id:1609.01586 author:Ravi Garg, Shu Dong, Sanjiv Shah, Siddhartha R Jonnalagadda category:cs.LG cs.CL  published:2016-09-06 summary:Rare diseases are very difficult to identify among large number of other possible diagnoses. Better availability of patient data and improvement in machine learning algorithms empower us to tackle this problem computationally. In this paper, we target one such rare disease - cardiac amyloidosis. We aim to automate the process of identifying potential cardiac amyloidosis patients with the help of machine learning algorithms and also learn most predictive factors. With the help of experienced cardiologists, we prepared a gold standard with 73 positive (cardiac amyloidosis) and 197 negative instances. We achieved high average cross-validation F1 score of 0.98 using an ensemble machine learning classifier. Some of the predictive variables were: Age and Diagnosis of cardiac arrest, chest pain, congestive heart failure, hypertension, prim open angle glaucoma, and shoulder arthritis. Further studies are needed to validate the accuracy of the system across an entire health system and its generalizability for other diseases. version:1
arxiv-1609-01580 | Using Natural Language Processing to Screen Patients with Active Heart Failure: An Exploration for Hospital-wide Surveillance | http://arxiv.org/abs/1609.01580 | id:1609.01580 author:Shu Dong, R Kannan Mutharasan, Siddhartha Jonnalagadda category:cs.CL cs.CY  published:2016-09-06 summary:In this paper, we proposed two different approaches, a rule-based approach and a machine-learning based approach, to identify active heart failure cases automatically by analyzing electronic health records (EHR). For the rule-based approach, we extracted cardiovascular data elements from clinical notes and matched patients to different colors according their heart failure condition by using rules provided by experts in heart failure. It achieved 69.4% accuracy and 0.729 F1-Score. For the machine learning approach, with bigram of clinical notes as features, we tried four different models while SVM with linear kernel achieved the best performance with 87.5% accuracy and 0.86 F1-Score. Also, from the classification comparison between the four different models, we believe that linear models fit better for this problem. Once we combine the machine-learning and rule-based algorithms, we will enable hospital-wide surveillance of active heart failure through increased accuracy and interpretability of the outputs. version:1
arxiv-1609-01574 | Automatically extracting, ranking and visually summarizing the treatments for a disease | http://arxiv.org/abs/1609.01574 | id:1609.01574 author:Prakash Reddy Putta, John J. Dzak III, Siddhartha R. Jonnalagadda category:cs.CL cs.IR  published:2016-09-06 summary:Clinicians are expected to have up-to-date and broad knowledge of disease treatment options for a patient. Online health knowledge resources contain a wealth of information. However, because of the time investment needed to disseminate and rank pertinent information, there is a need to summarize the information in a more concise format. Our aim of the study is to provide clinicians with a concise overview of popular treatments for a given disease using information automatically computed from Medline abstracts. We analyzed the treatments of two disorders - Atrial Fibrillation and Congestive Heart Failure. We calculated the precision, recall, and f-scores of our two ranking methods to measure the accuracy of the results. For Atrial Fibrillation disorder, maximum f-score for the New Treatments weighing method is 0.611, which occurs at 60 treatments. For Congestive Heart Failure disorder, maximum f-score for the New Treatments weighing method is 0.503, which occurs at 80 treatments. version:1
arxiv-1608-05813 | phi-LSTM: A Phrase-based Hierarchical LSTM Model for Image Captioning | http://arxiv.org/abs/1608.05813 | id:1608.05813 author:Ying Hua Tan, Chee Seng Chan category:cs.CL cs.CV  published:2016-08-20 summary:A picture is worth a thousand words. Not until recently, however, we noticed some success stories in understanding of visual scenes: a model that is able to detect/name objects, describe their attributes, and recognize their relationships/interactions. In this paper, we propose a phrase-based hierarchical Long Short-Term Memory (phi-LSTM) model to generate image description. The proposed model encodes sentence as a sequence of combination of phrases and words, instead of a sequence of words alone as in those conventional solutions. The two levels of this model are dedicated to i) learn to generate image relevant noun phrases, and ii) produce appropriate image description from the phrases and other words in the corpus. Adopting a convolutional neural network to learn image features and the LSTM to learn the word sequence in a sentence, the proposed model has shown better or competitive results in comparison to the state-of-the-art models on Flickr8k and Flickr30k datasets. version:3
arxiv-1609-01571 | Best-Buddies Similarity - Robust Template Matching using Mutual Nearest Neighbors | http://arxiv.org/abs/1609.01571 | id:1609.01571 author:Shaul Oron, Tali Dekel, Tianfan Xue, William T. Freeman, Shai Avidan category:cs.CV  published:2016-09-06 summary:We propose a novel method for template matching in unconstrained environments. Its essence is the Best-Buddies Similarity (BBS), a useful, robust, and parameter-free similarity measure between two sets of points. BBS is based on counting the number of Best-Buddies Pairs (BBPs)--pairs of points in source and target sets, where each point is the nearest neighbor of the other. BBS has several key features that make it robust against complex geometric deformations and high levels of outliers, such as those arising from background clutter and occlusions. We study these properties, provide a statistical analysis that justifies them, and demonstrate the consistent success of BBS on a challenging real-world dataset while using different types of features. version:1
arxiv-1609-01006 | Combining Fully Convolutional and Recurrent Neural Networks for 3D Biomedical Image Segmentation | http://arxiv.org/abs/1609.01006 | id:1609.01006 author:Jianxu Chen, Lin Yang, Yizhe Zhang, Mark Alber, Danny Z. Chen category:cs.CV  published:2016-09-05 summary:Segmentation of 3D images is a fundamental problem in biomedical image analysis. Deep learning (DL) approaches have achieved state-of-the-art segmentation perfor- mance. To exploit the 3D contexts using neural networks, known DL segmentation methods, including 3D convolution, 2D convolution on planes orthogonal to 2D image slices, and LSTM in multiple directions, all suffer incompatibility with the highly anisotropic dimensions in common 3D biomedical images. In this paper, we propose a new DL framework for 3D image segmentation, based on a com- bination of a fully convolutional network (FCN) and a recurrent neural network (RNN), which are responsible for exploiting the intra-slice and inter-slice contexts, respectively. To our best knowledge, this is the first DL framework for 3D image segmentation that explicitly leverages 3D image anisotropism. Evaluating using a dataset from the ISBI Neuronal Structure Segmentation Challenge and in-house image stacks for 3D fungus segmentation, our approach achieves promising results comparing to the known DL-based 3D segmentation approaches. version:2
arxiv-1609-01524 | Confidence-aware Levenberg-Marquardt optimization for joint motion estimation and super-resolution | http://arxiv.org/abs/1609.01524 | id:1609.01524 author:Cosmin Bercea, Andreas Maier, Thomas K√∂hler category:cs.CV  published:2016-09-06 summary:Motion estimation across low-resolution frames and the reconstruction of high-resolution images are two coupled subproblems of multi-frame super-resolution. This paper introduces a new joint optimization approach for motion estimation and image reconstruction to address this interdependence. Our method is formulated via non-linear least squares optimization and combines two principles of robust super-resolution. First, to enhance the robustness of the joint estimation, we propose a confidence-aware energy minimization framework augmented with sparse regularization. Second, we develop a tailor-made Levenberg-Marquardt iteration scheme to jointly estimate motion parameters and the high-resolution image along with the corresponding model confidence parameters. Our experiments on simulated and real images confirm that the proposed approach outperforms decoupled motion estimation and image reconstruction as well as related state-of-the-art joint estimation algorithms. version:1
arxiv-1609-01508 | Low-rank Bandits with Latent Mixtures | http://arxiv.org/abs/1609.01508 | id:1609.01508 author:Aditya Gopalan, Odalric-Ambrym Maillard, Mohammadi Zaki category:cs.LG  published:2016-09-06 summary:We study the task of maximizing rewards from recommending items (actions) to users sequentially interacting with a recommender system. Users are modeled as latent mixtures of C many representative user classes, where each class specifies a mean reward profile across actions. Both the user features (mixture distribution over classes) and the item features (mean reward vector per class) are unknown a priori. The user identity is the only contextual information available to the learner while interacting. This induces a low-rank structure on the matrix of expected rewards r a,b from recommending item a to user b. The problem reduces to the well-known linear bandit when either user or item-side features are perfectly known. In the setting where each user, with its stochastically sampled taste profile, interacts only for a small number of sessions, we develop a bandit algorithm for the two-sided uncertainty. It combines the Robust Tensor Power Method of Anandkumar et al. (2014b) with the OFUL linear bandit algorithm of Abbasi-Yadkori et al. (2011). We provide the first rigorous regret analysis of this combination, showing that its regret after T user interactions is $\tilde O(C\sqrt{BT})$, with B the number of users. An ingredient towards this result is a novel robustness property of OFUL, of independent interest. version:1
arxiv-1609-01499 | Depth Estimation Through a Generative Model of Light Field Synthesis | http://arxiv.org/abs/1609.01499 | id:1609.01499 author:Mehdi S. M. Sajjadi, Rolf K√∂hler, Bernhard Sch√∂lkopf, Michael Hirsch category:cs.CV cs.GR  published:2016-09-06 summary:Light field photography captures rich structural information that may facilitate a number of traditional image processing and computer vision tasks. A crucial ingredient in such endeavors is accurate depth recovery. We present a novel framework that allows the recovery of a high quality continuous depth map from light field data. To this end we propose a generative model of a light field that is fully parametrized by its corresponding depth map. The model allows for the integration of powerful regularization techniques such as a non-local means prior, facilitating accurate depth map estimation. version:1
arxiv-1609-01491 | Towards Learning and Verifying Invariants of Cyber-Physical Systems by Code Mutation | http://arxiv.org/abs/1609.01491 | id:1609.01491 author:Yuqi Chen, Christopher M. Poskitt, Jun Sun category:cs.SE cs.LG cs.LO  published:2016-09-06 summary:Cyber-physical systems (CPS), which integrate algorithmic control with physical processes, often consist of physically distributed components communicating over a network. A malfunctioning or compromised component in such a CPS can lead to costly consequences, especially in the context of public infrastructure. In this short paper, we argue for the importance of constructing invariants (or models) of the physical behaviour exhibited by CPS, motivated by their applications to the control, monitoring, and attestation of components. To achieve this despite the inherent complexity of CPS, we propose a new technique for learning invariants that combines machine learning with ideas from mutation testing. We present a preliminary study on a water treatment system that suggests the efficacy of this approach, propose strategies for establishing confidence in the correctness of invariants, then summarise some research questions and the steps we are taking to investigate them. version:1
arxiv-1609-01829 | Animal Classification System: A Block Based Approach | http://arxiv.org/abs/1609.01829 | id:1609.01829 author:Y H Sharath Kumar, N Manohar, H K Chethan category:cs.CV I.4.6; I.4.8  published:2016-09-06 summary:In this work, we propose a method for the classification of animal in images. Initially, a graph cut based method is used to perform segmentation in order to eliminate the background from the given image. The segmented animal images are partitioned in to number of blocks and then the color texture moments are extracted from different blocks. Probabilistic neural network and K-nearest neighbors are considered here for classification. To corroborate the efficacy of the proposed method, an experiment was conducted on our own data set of 25 classes of animals, which consisted of 4000 sample images. The experiment was conducted by picking images randomly from the database to study the effect of classification accuracy, and the results show that the K-nearest neighbors classifier achieves good performance. version:1
arxiv-1609-00951 | A Unified Convergence Analysis of the Multiplicative Update Algorithm for Nonnegative Matrix Factorization | http://arxiv.org/abs/1609.00951 | id:1609.00951 author:Renbo Zhao, Vincent Y. F. Tan category:math.OC cs.IT math.IT stat.ML  published:2016-09-04 summary:The multiplicative update (MU) algorithm has been used extensively to estimate the basis and coefficient matrices in nonnegative matrix factorization (NMF) problems under a wide range of divergences and regularizations. However, theoretical convergence guarantees have only been derived for a few special divergences. In this work, we provide a conceptually simple, self-contained, and unified proof for the convergence of the MU algorithm applied on NMF with a wide range of divergences and regularizations. Our result shows the sequence of iterates (i.e., pairs of basis and coefficient matrices) produced by the MU algorithm converges to the set of stationary points of the NMF (optimization) problem. Our proof strategy has the potential to open up new avenues for analyzing similar problems. version:2
arxiv-1609-01710 | Automation of Pedestrian Tracking in a Crowded Situation | http://arxiv.org/abs/1609.01710 | id:1609.01710 author:Saman Saadat, Kardi Teknomo category:cs.CV cs.AI cs.CY cs.MA  published:2016-09-06 summary:Studies on microscopic pedestrian requires large amounts of trajectory data from real-world pedestrian crowds. Such data collection, if done manually, needs tremendous effort and is very time consuming. Though many studies have asserted the possibility of automating this task using video cameras, we found that only a few have demonstrated good performance in very crowded situations or from a top-angled view scene. This paper deals with tracking pedestrian crowd under heavy occlusions from an angular scene. Our automated tracking system consists of two modules that perform sequentially. The first module detects moving objects as blobs. The second module is a tracking system. We employ probability distribution from the detection of each pedestrian and use Bayesian update to track the next position. The result of such tracking is a database of pedestrian trajectories over time and space. With certain prior information, we showed that the system can track a large number of people under occlusion and clutter scene. version:1
arxiv-1609-02409 | Comparison of several short-term traffic speed forecasting models | http://arxiv.org/abs/1609.02409 | id:1609.02409 author:John Boaz Lee, Kardi Teknomo category:cs.CV math.PR stat.AP stat.CO  published:2016-09-06 summary:The widespread adoption of smartphones in recent years has made it possible for us to collect large amounts of traffic data. Special software installed on the phones of drivers allow us to gather GPS trajectories of their vehicles on the road network. In this paper, we simulate the trajectories of multiple agents on a road network and use various models to forecast the short-term traffic speed of various links. Our results show that traditional techniques like multiple regression and artificial neural networks work well but simpler adaptive models that do not require prior training also perform comparatively well. version:1
arxiv-1609-01468 | Q-Learning with Basic Emotions | http://arxiv.org/abs/1609.01468 | id:1609.01468 author:Wilfredo Badoy Jr., Kardi Teknomo category:cs.LG cs.AI stat.ML  published:2016-09-06 summary:Q-learning is a simple and powerful tool in solving dynamic problems where environments are unknown. It uses a balance of exploration and exploitation to find an optimal solution to the problem. In this paper, we propose using four basic emotions: joy, sadness, fear, and anger to influence a Qlearning agent. Simulations show that the proposed affective agent requires lesser number of steps to find the optimal path. We found when affective agent finds the optimal path, the ratio between exploration to exploitation gradually decreases, indicating lower total step count in the long run version:1
arxiv-1609-01466 | Joint Registration of Multiple Point Sets | http://arxiv.org/abs/1609.01466 | id:1609.01466 author:Georgios Evangelidis, Radu Horaud category:cs.CV  published:2016-09-06 summary:This manuscript addresses the rigid registration problem of multiple 3D point sets. While the vast majority of state-of-the-art techniques build on pairwise registration, we propose a generative model that explains jointly registered multiple sets: back-transformed points are considered realizations of a single Gaussian mixture model (GMM) whose means play the role of the scene points. Under this assumption, the joint registration problem is cast into a probabilistic clustering framework. We formally derive an Expectation-Maximization scheme that robustly estimates both the GMM parameters and the rigid transformations that map each individual cloud onto an under-construction reference set, that is, the GMM means. GMM variances carry rich information as well, thus leading to a noise- and outlier-free scene model as a by-product. A second version of the algorithm is also proposed whereby newly captured sets can be registered online. A thorough discussion and validation on challenging data-sets against several state-of-the-art methods confirm the potential of the proposed model for jointly registering real depth data. version:1
arxiv-1609-01465 | Multi-instance Dynamic Ordinal Random Fields for Weakly-Supervised Pain Intensity Estimation | http://arxiv.org/abs/1609.01465 | id:1609.01465 author:Adria Ruiz, Ognjen Rudovic, Xavier Binefa, Maja Pantic category:cs.CV  published:2016-09-06 summary:In this paper, we address the Multi-Instance-Learning (MIL) problem when bag labels are naturally represented as ordinal variables (Multi--Instance--Ordinal Regression). Moreover, we consider the case where bags are temporal sequences of ordinal instances. To model this, we propose the novel Multi-Instance Dynamic Ordinal Random Fields (MI-DORF). In this model, we treat instance-labels inside the bag as latent ordinal states. The MIL assumption is modelled by incorporating a high-order cardinality potential relating bag and instance-labels,into the energy function. We show the benefits of the proposed approach on the task of weakly-supervised pain intensity estimation from the UNBC Shoulder-Pain Database. In our experiments, the proposed approach significantly outperforms alternative non-ordinal methods that either ignore the MIL assumption, or do not model dynamic information in target data. version:1
arxiv-1609-01462 | Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks | http://arxiv.org/abs/1609.01462 | id:1609.01462 author:Bing Liu, Ian Lane category:cs.CL  published:2016-09-06 summary:Speaker intent detection and semantic slot filling are two critical tasks in spoken language understanding (SLU) for dialogue systems. In this paper, we describe a recurrent neural network (RNN) model that jointly performs intent detection, slot filling, and language modeling. The neural network model keeps updating the intent estimation as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot filling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input. version:1
arxiv-1609-01461 | Statistical Meta-Analysis of Presentation Attacks for Secure Multibiometric Systems | http://arxiv.org/abs/1609.01461 | id:1609.01461 author:Battista Biggio, Giorgio Fumera, Gian Luca Marcialis, Fabio Roli category:cs.CV cs.CR  published:2016-09-06 summary:Prior work has shown that multibiometric systems are vulnerable to presentation attacks, assuming that their matching score distribution is identical to that of genuine users, without fabricating any fake trait. We have recently shown that this assumption is not representative of current fingerprint and face presentation attacks, leading one to overestimate the vulnerability of multibiometric systems, and to design less effective fusion rules. In this paper, we overcome these limitations by proposing a statistical meta-model of face and fingerprint presentation attacks that characterizes a wider family of fake score distributions, including distributions of known and, potentially, unknown attacks. This allows us to perform a thorough security evaluation of multibiometric systems against presentation attacks, quantifying how their vulnerability may vary also under attacks that are different from those considered during design, through an uncertainty analysis. We empirically show that our approach can reliably predict the performance of multibiometric systems even under never-before-seen face and fingerprint presentation attacks, and that the secure fusion rules designed using our approach can exhibit an improved trade-off between the performance in the absence and in the presence of attack. We finally argue that our method can be extended to other biometrics besides faces and fingerprints. version:1
arxiv-1609-01454 | Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling | http://arxiv.org/abs/1609.01454 | id:1609.01454 author:Bing Liu, Ian Lane category:cs.CL  published:2016-09-06 summary:Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56% absolute (23.8% relative) error reduction on intent detection and 0.23% absolute gain on slot filling over the independent task models. version:1
arxiv-1609-01414 | Features Fusion for Classification of Logos | http://arxiv.org/abs/1609.01414 | id:1609.01414 author:N. Vinay Kumar, Pratheek, V. Vijaya Kantha, K. N. Govindaraju, D. S. Guru category:cs.CV  published:2016-09-06 summary:In this paper, a logo classification system based on the appearance of logo images is proposed. The proposed classification system makes use of global characteristics of logo images for classification. Color, texture, and shape of a logo wholly describe the global characteristics of logo images. The various combinations of these characteristics are used for classification. The combination contains only with single feature or with fusion of two features or fusion of all three features considered at a time respectively. Further, the system categorizes the logo image into: a logo image with fully text or with fully symbols or containing both symbols and texts.. The K-Nearest Neighbour (K-NN) classifier is used for classification. Due to the lack of color logo image dataset in the literature, the same is created consisting 5044 color logo images. Finally, the performance of the classification system is evaluated through accuracy, precision, recall and F-measure computed from the confusion matrix. The experimental results show that the most promising results are obtained for fusion of features. version:1
arxiv-1609-01828 | Delaunay Triangulation on Skeleton of Flowers for Classification | http://arxiv.org/abs/1609.01828 | id:1609.01828 author:Y H Sharath Kumar, N Vinay Kumar, D S Guru category:cs.CV I.4.6; I.4.7; I.4.8  published:2016-09-06 summary:In this work, we propose a Triangle based approach to classify flower images. Initially, flowers are segmented using whorl based region merging segmentation. Skeleton of a flower is obtained from the segmented flower using a skeleton pruning method. The Delaunay triangulation is obtained from the endpoints and junction points detected on the skeleton. The length and angle features are extracted from the obtained Delaunay triangles and then are aggregated to represent in the form of interval-valued type data. A suitable classifier has been explored for the purpose of classification. To corroborate the efficacy of the proposed method, an experiment is conducted on our own data set of 30 classes of flowers, containing 3000 samples. version:1
arxiv-1609-00489 | A deep learning model for estimating story points | http://arxiv.org/abs/1609.00489 | id:1609.00489 author:Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Trang Pham, Aditya Ghose, Tim Menzies category:cs.SE cs.LG stat.ML  published:2016-09-02 summary:Although there has been substantial research in software analytics for effort estimation in traditional software projects, little work has been done for estimation in agile projects, especially estimating user stories or issues. Story points are the most common unit of measure used for estimating the effort involved in implementing a user story or resolving an issue. In this paper, we offer for the \emph{first} time a comprehensive dataset for story points-based estimation that contains 23,313 issues from 16 open source projects. We also propose a prediction model for estimating story points based on a novel combination of two powerful deep learning architectures: long short-term memory and recurrent highway network. Our prediction system is \emph{end-to-end} trainable from raw input data to prediction outcomes without any manual feature engineering. An empirical evaluation demonstrates that our approach consistently outperforms three common effort estimation baselines and two alternatives in both Mean Absolute Error and the Standardized Accuracy. version:2
arxiv-1609-01387 | Learning Model Predictive Control for Iterative Tasks | http://arxiv.org/abs/1609.01387 | id:1609.01387 author:Ugo Rosolia, Francesco Borrelli category:cs.SY cs.LG math.OC  published:2016-09-06 summary:A Learning Model Predictive Controller (LMPC) for iterative tasks is presented. The controller is reference-free and is able to improve its performance by learning from previous iterations. A safe set and a terminal cost function are used in order to guarantee recursive feasibility and non-increasing performance at each iteration. The paper presents the control design approach, and shows how to recursively construct terminal set and terminal cost from state and input trajectories of previous iterations. Simulation results show the effectiveness of the proposed control logic. version:1
arxiv-1609-01380 | An Adaptive Parameter Estimation for Guided Filter based Image Deconvolution | http://arxiv.org/abs/1609.01380 | id:1609.01380 author:Hang Yang, Zhongbo Zhang, Yujing Guan category:cs.CV  published:2016-09-06 summary:Image deconvolution is still to be a challenging ill-posed problem for recovering a clear image from a given blurry image, when the point spread function is known. Although competitive deconvolution methods are numerically impressive and approach theoretical limits, they are becoming more complex, making analysis, and implementation difficult. Furthermore, accurate estimation of the regularization parameter is not easy for successfully solving image deconvolution problems. In this paper, we develop an effective approach for image restoration based on one explicit image filter - guided filter. By applying the decouple of denoising and deblurring techniques to the deconvolution model, we reduce the optimization complexity and achieve a simple but effective algorithm to automatically compute the parameter in each iteration, which is based on Morozov's discrepancy principle. Experimental results demonstrate that the proposed algorithm outperforms many state-of-the-art deconvolution methods in terms of both ISNR and visual quality. version:1
arxiv-1609-01366 | Object Specific Deep Learning Feature and Its Application to Face Detection | http://arxiv.org/abs/1609.01366 | id:1609.01366 author:Xianxu Hou, Ke Sun, Linlin Shen, Guoping Qiu category:cs.CV  published:2016-09-06 summary:We present a method for discovering and exploiting object specific deep learning features and use face detection as a case study. Motivated by the observation that certain convolutional channels of a Convolutional Neural Network (CNN) exhibit object specific responses, we seek to discover and exploit the convolutional channels of a CNN in which neurons are activated by the presence of specific objects in the input image. A method for explicitly fine-tuning a pre-trained CNN to induce an object specific channel (OSC) and systematically identifying it for the human face object has been developed. Based on the basic OSC features, we introduce a multi-resolution approach to constructing robust face heatmaps for fast face detection in unconstrained settings. We show that multi-resolution OSC can be used to develop state of the art face detectors which have the advantage of being simple and compact. version:1
arxiv-1609-01360 | Evolutionary Synthesis of Deep Neural Networks via Synaptic Cluster-driven Genetic Encoding | http://arxiv.org/abs/1609.01360 | id:1609.01360 author:Mohammad Javad Shafiee, Alexander Wong category:cs.LG cs.CV cs.NE stat.ML  published:2016-09-06 summary:There has been significant recent interest towards achieving highly efficient deep neural network architectures that preserve strong modeling capabilities. A particular promising paradigm for achieving such deep neural networks is the concept of evolutionary deep intelligence, which attempts to mimic biological evolution processes to synthesize highly-efficient deep neural networks over successive generations that retain high modeling capabilities. An important aspect of evolutionary deep intelligence that is particular interesting and worth deeper investigation is the genetic encoding scheme used to mimic heredity, which can have a significant impact on the way architectural traits are passed down from generation to generation and thus impact the quality of descendant deep neural networks. Motivated by the neurobiological phenomenon of synaptic clustering, where the probability of synaptic co-activation increases for correlated synapses encoding similar information that are close together on the same dendrite, we introduce a new genetic encoding scheme where synaptic probability within a deep neural network is driven towards the formation of highly sparse synaptic clusters. Experimental results for the task of image classification demonstrated that the synthesized `evolved' offspring networks using this synaptic cluster-driven genetic encoding scheme can achieve state-of-the-art performance while having network architectures that are not only significantly more efficient (with a ~125-fold decrease in synapses at a comparable accuracy for MNIST) compared to the original ancestor network, but also highly tailored for GPU-accelerated machine learning applications. version:1
arxiv-1609-01345 | Efficient Volumetric Fusion of Airborne and Street-Side Data for Urban Reconstruction | http://arxiv.org/abs/1609.01345 | id:1609.01345 author:Andr√°s B√≥dis-Szomor√∫, Hayko Riemenschneider, Luc Van Gool category:cs.CV  published:2016-09-05 summary:Airborne acquisition and on-road mobile mapping provide complementary 3D information of an urban landscape: the former acquires roof structures, ground, and vegetation at a large scale, but lacks the facade and street-side details, while the latter is incomplete for higher floors and often totally misses out on pedestrian-only areas or undriven districts. In this work, we introduce an approach that efficiently unifies a detailed street-side Structure-from-Motion (SfM) or Multi-View Stereo (MVS) point cloud and a coarser but more complete point cloud from airborne acquisition in a joint surface mesh. We propose a point cloud blending and a volumetric fusion based on ray casting across a 3D tetrahedralization (3DT), extended with data reduction techniques to handle large datasets. To the best of our knowledge, we are the first to adopt a 3DT approach for airborne/street-side data fusion. Our pipeline exploits typical characteristics of airborne and ground data, and produces a seamless, watertight mesh that is both complete and detailed. Experiments on 3D urban data from multiple sources and different data densities show the effectiveness and benefits of our approach. version:1
arxiv-1609-01344 | Vision-based Engagement Detection in Virtual Reality | http://arxiv.org/abs/1609.01344 | id:1609.01344 author:Ghassem Tofighi, Kaamraan Raahemifar, Maria Frank, Haisong Gu category:cs.CV  published:2016-09-05 summary:User engagement modeling for manipulating actions in vision-based interfaces is one of the most important case studies of user mental state detection. In a Virtual Reality environment that employs camera sensors to recognize human activities, we have to know when user intends to perform an action and when not. Without a proper algorithm for recognizing engagement status, any kind of activities could be interpreted as manipulating actions, called "Midas Touch" problem. Baseline approach for solving this problem is activating gesture recognition system using some focus gestures such as waiving or raising hand. However, a desirable natural user interface should be able to understand user's mental status automatically. In this paper, a novel multi-modal model for engagement detection, DAIA, is presented. using DAIA, the spectrum of mental status for performing an action is quantized in a finite number of engagement states. For this purpose, a Finite State Transducer (FST) is designed. This engagement framework shows how to integrate multi-modal information from user biometric data streams such as 2D and 3D imaging. FST is employed to make the state transition smoothly using combination of several boolean expressions. Our FST true detection rate is 92.3% in total for four different states. Results also show FST can segment user hand gestures more robustly. version:1
arxiv-1608-04664 | Variational Gaussian Process Auto-Encoder for Ordinal Prediction of Facial Action Units | http://arxiv.org/abs/1608.04664 | id:1608.04664 author:Stefanos Eleftheriadis, Ognjen Rudovic, Marc P. Deisenroth, Maja Pantic category:stat.ML cs.CV  published:2016-08-16 summary:We address the task of simultaneous feature fusion and modeling of discrete ordinal outputs. We propose a novel Gaussian process(GP) auto-encoder modeling approach. In particular, we introduce GP encoders to project multiple observed features onto a latent space, while GP decoders are responsible for reconstructing the original features. Inference is performed in a novel variational framework, where the recovered latent representations are further constrained by the ordinal output labels. In this way, we seamlessly integrate the ordinal structure in the learned manifold, while attaining robust fusion of the input features. We demonstrate the representation abilities of our model on benchmark datasets from machine learning and affect analysis. We further evaluate the model on the tasks of feature fusion and joint ordinal prediction of facial action units. Our experiments demonstrate the benefits of the proposed approach compared to the state of the art. version:2
arxiv-1609-01326 | UnrealCV: Connecting Computer Vision to Unreal Engine | http://arxiv.org/abs/1609.01326 | id:1609.01326 author:Weichao Qiu, Alan Yuille category:cs.CV  published:2016-09-05 summary:Computer graphics can not only generate synthetic images and ground truth but it also offers the possibility of constructing virtual worlds in which: (i) an agent can perceive, navigate, and take actions guided by AI algorithms, (ii) properties of the worlds can be modified (e.g., material and reflectance), (iii) physical simulations can be performed, and (iv) algorithms can be learnt and evaluated. But creating realistic virtual worlds is not easy. The game industry, however, has spent a lot of effort creating 3D worlds, which a player can interact with. So researchers can build on these resources to create virtual worlds, provided we can access and modify the internal data structures of the games. To enable this we created an open-source plugin UnrealCV (http://unrealcv.github.io) for a popular game engine Unreal Engine 4 (UE4). We show two applications: (i) a proof of concept image dataset, and (ii) linking Caffe with the virtual world to test deep network algorithms. version:1
arxiv-1609-01235 | PMI Matrix Approximations with Applications to Neural Language Modeling | http://arxiv.org/abs/1609.01235 | id:1609.01235 author:Oren Melamud, Ido Dagan, Jacob Goldberger category:cs.CL  published:2016-09-05 summary:The negative sampling (NEG) objective function, used in word2vec, is a simplification of the Noise Contrastive Estimation (NCE) method. NEG was found to be highly effective in learning continuous word representations. However, unlike NCE, it was considered inapplicable for the purpose of learning the parameters of a language model. In this study, we refute this assertion by providing a principled derivation for NEG-based language modeling, founded on a novel analysis of a low-dimensional approximation of the matrix of pointwise mutual information between the contexts and the predicted words. The obtained language modeling is closely related to NCE language models but is based on a simplified objective function. We thus provide a unified formulation for two main language processing tasks, namely word embedding and language modeling, based on the NEG objective function. Experimental results on two popular language modeling benchmarks show comparable perplexity results, with a small advantage to NEG over NCE. version:1
arxiv-1609-01228 | Towards Automated Melanoma Screening: Exploring Transfer Learning Schemes | http://arxiv.org/abs/1609.01228 | id:1609.01228 author:Afonso Menegola, Michel Fornaciali, Ramon Pires, Sandra Avila, Eduardo Valle category:cs.CV  published:2016-09-05 summary:Deep learning is the current bet for image classification. Its greed for huge amounts of annotated data limits its usage in medical imaging context. In this scenario transfer learning appears as a prominent solution. In this report we aim to clarify how transfer learning schemes may influence classification results. We are particularly focused in the automated melanoma screening problem, a case of medical imaging in which transfer learning is still not widely used. We explored transfer with and without fine-tuning, sequential transfers and usage of pre-trained models in general and specific datasets. Although some issues remain open, our findings may drive future researches. version:1
arxiv-1609-01226 | The Robustness of Estimator Composition | http://arxiv.org/abs/1609.01226 | id:1609.01226 author:Pingfan Tang, Jeff M. Phillips category:cs.LG stat.ML  published:2016-09-05 summary:We formalize notions of robustness for composite estimators via the notion of a breakdown point. A composite estimator successively applies two (or more) estimators: on data decomposed into disjoint parts, it applies the first estimator on each part, then the second estimator on the outputs of the first estimator. And so on, if the composition is of more than two estimators. Informally, the breakdown point is the minimum fraction of data points which if significantly modified will also significantly modify the output of the estimator, so it is typically desirable to have a large breakdown point. Our main result shows that, under mild conditions on the individual estimators, the breakdown point of the composite estimator is the product of the breakdown points of the individual estimators. We also demonstrate several scenarios, ranging from regression to statistical testing, where this analysis is easy to apply, useful in understanding worst case robustness, and sheds powerful insights onto the associated data analysis. version:1
arxiv-1608-04426 | Regularization for Unsupervised Deep Neural Nets | http://arxiv.org/abs/1608.04426 | id:1608.04426 author:Baiyang Wang, Diego Klabjan category:cs.LG cs.NE  published:2016-08-15 summary:Unsupervised neural networks, such as restricted Boltzmann machines (RBMs) and deep belief networks (DBNs), are powerful tools for feature selection and pattern recognition tasks. We demonstrate that overfitting occurs in such models just as in deep feedforward neural networks, and discuss possible regularization methods to reduce overfitting. We also propose a "partial" approach to improve the efficiency of Dropout/DropConnect in this scenario, and discuss the theoretical justification of these methods from model convergence and likelihood bounds. Finally, we compare the performance of these methods based on their likelihood and classification error rates for various pattern recognition data sets. version:3
arxiv-1609-01203 | Live Orchestral Piano, a system for real-time orchestral music generation | http://arxiv.org/abs/1609.01203 | id:1609.01203 author:L√©opold Crestel, Philippe Esling category:cs.LG  published:2016-09-05 summary:This paper introduces the first system for performing automatic orchestration based on a real-time piano input. We believe that it is possible to learn the underlying regularities existing between piano scores and their orchestrations by notorious composers, in order to automatically perform this task on novel piano inputs. To that end, we investigate a class of statistical inference models called conditional Restricted Boltzmann Machine (cRBM). We introduce a specific evaluation framework for orchestral generation based on a prediction task in order to assess the quality of different models. As prediction and creation are two widely different endeavours, we discuss the potential biases in evaluating temporal generative models through prediction tasks and their impact on a creative system. Finally, we introduce an implementation of the proposed model called Live Orchestral Piano (LOP), which allows to perform real-time projective orchestration of a MIDI keyboard input. version:1
arxiv-1609-00464 | The Semantic Knowledge Graph: A compact, auto-generated model for real-time traversal and ranking of any relationship within a domain | http://arxiv.org/abs/1609.00464 | id:1609.00464 author:Trey Grainger, Khalifeh AlJadda, Mohammed Korayem, Andries Smith category:cs.IR cs.AI cs.CL  published:2016-09-02 summary:This paper describes a new kind of knowledge representation and mining system which we are calling the Semantic Knowledge Graph. At its heart, the Semantic Knowledge Graph leverages an inverted index, along with a complementary uninverted index, to represent nodes (terms) and edges (the documents within intersecting postings lists for multiple terms/nodes). This provides a layer of indirection between each pair of nodes and their corresponding edge, enabling edges to materialize dynamically from underlying corpus statistics. As a result, any combination of nodes can have edges to any other nodes materialize and be scored to reveal latent relationships between the nodes. This provides numerous benefits: the knowledge graph can be built automatically from a real-world corpus of data, new nodes - along with their combined edges - can be instantly materialized from any arbitrary combination of preexisting nodes (using set operations), and a full model of the semantic relationships between all entities within a domain can be represented and dynamically traversed using a highly compact representation of the graph. Such a system has widespread applications in areas as diverse as knowledge modeling and reasoning, natural language processing, anomaly detection, data cleansing, semantic search, analytics, data classification, root cause analysis, and recommendations systems. The main contribution of this paper is the introduction of a novel system - the Semantic Knowledge Graph - which is able to dynamically discover and score interesting relationships between any arbitrary combination of entities (words, phrases, or extracted concepts) through dynamically materializing nodes and edges from a compact graphical representation built automatically from a corpus of data representative of a knowledge domain. version:2
arxiv-1609-01188 | Bi-Text Alignment of Movie Subtitles for Spoken English-Arabic Statistical Machine Translation | http://arxiv.org/abs/1609.01188 | id:1609.01188 author:Fahad Al-Obaidli, Stephen Cox, Preslav Nakov category:cs.CL  published:2016-09-05 summary:We describe efforts towards getting better resources for English-Arabic machine translation of spoken text. In particular, we look at movie subtitles as a unique, rich resource, as subtitles in one language often get translated into other languages. Movie subtitles are not new as a resource and have been explored in previous research; however, here we create a much larger bi-text (the biggest to date), and we further generate better quality alignment for it. Given the subtitles for the same movie in different languages, a key problem is how to align them at the fragment level. Typically, this is done using length-based alignment, but for movie subtitles, there is also time information. Here we exploit this information to develop an original algorithm that outperforms the current best subtitle alignment tool, subalign. The evaluation results show that adding our bi-text to the IWSLT training bi-text yields an improvement of over two BLEU points absolute. version:1
arxiv-1609-01176 | The Player Kernel: Learning Team Strengths Based on Implicit Player Contributions | http://arxiv.org/abs/1609.01176 | id:1609.01176 author:Lucas Maystre, Victor Kristof, Antonio J. Gonz√°lez Ferrer, Matthias Grossglauser category:cs.LG stat.AP  published:2016-09-05 summary:In this work, we draw attention to a connection between skill-based models of game outcomes and Gaussian process classification models. The Gaussian process perspective enables a) a principled way of dealing with uncertainty and b) rich models, specified through kernel functions. Using this connection, we tackle the problem of predicting outcomes of football matches between national teams. We develop a player kernel that relates any two football matches through the players lined up on the field. This makes it possible to share knowledge gained from observing matches between clubs (available in large quantities) and matches between national teams (available only in limited quantities). We evaluate our approach on the Euro 2008, 2012 and 2016 final tournaments. version:1
arxiv-1609-01117 | Reflections on Shannon Information: In search of a natural information-entropy for images | http://arxiv.org/abs/1609.01117 | id:1609.01117 author:Kieran G. Larkin category:cs.IT cs.CV math.IT  published:2016-09-05 summary:It is not obvious how to extend Shannon's original information entropy to higher dimensions, and many different approaches have been tried. We replace the English text symbol sequence originally used to illustrate the theory by a discrete, bandlimited signal. Using Shannon's later theory of sampling we derive a new and symmetric version of the second order entropy in 1D. The new theory then naturally extends to 2D and higher dimensions, where by naturally we mean simple, symmetric, isotropic and parsimonious. Simplicity arises from the direct application of Shannon's joint entropy equalities and inequalities to the gradient (del) vector field image embodying the second order relations of the scalar image. Parsimony is guaranteed by halving of the vector data rate using Papoulis' generalized sampling expansion. The new 2D entropy measure, which we dub delentropy, is underpinned by a computable probability density function we call deldensity. The deldensity captures the underlying spatial image structure and pixel co-occurrence. It achieves this because each scalar image pixel value is nonlocally related to the entire gradient vector field. Both deldensity and delentropy are highly tractable and yield many interesting connections and useful inequalities. The new measure explicitly defines a realizable encoding algorithm and a corresponding reconstruction. Initial tests show that delentropy compares favourably with the conventional intensity-based histogram entropy and the compressed data rates of lossless image encoders (GIF, PNG, WEBP, JP2K-LS and JPG-LS) for a selection of images. The symmetric approach may have applications to higher dimensions and problems concerning image complexity measures. version:1
arxiv-1609-01103 | Deep Retinal Image Understanding | http://arxiv.org/abs/1609.01103 | id:1609.01103 author:Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbel√°ez, Luc Van Gool category:cs.CV  published:2016-09-05 summary:This paper presents Deep Retinal Image Understanding (DRIU), a unified framework of retinal image analysis that provides both retinal vessel and optic disc segmentation. We make use of deep Convolutional Neural Networks (CNNs), which have proven revolutionary in other fields of computer vision such as object detection and image classification, and we bring their power to the study of eye fundus images. DRIU uses a base network architecture on which two set of specialized layers are trained to solve both the retinal vessel and optic disc segmentation. We present experimental validation, both qualitative and quantitative, in four public datasets for these tasks. In all of them, DRIU presents super-human performance, that is, it shows results more consistent with a gold standard than a second human annotator used as control. version:1
arxiv-1609-01100 | A max-cut approach to heterogeneity in cryo-electron microscopy | http://arxiv.org/abs/1609.01100 | id:1609.01100 author:Yariv Aizenbud, Yoel Shkolnisky category:cs.CV math.OC q-bio.BM  published:2016-09-05 summary:The field of cryo-electron microscopy has made astounding advancements in the past few years, mainly due to improvements in the hardware of the microscopes. Yet, one of the key open challenges of the field remains the processing of heterogeneous data sets, produced from samples containing particles at several different conformational states. For such data sets, one must first classify their images into homogeneous groups, where each group corresponds to the same underlying structure, followed by reconstruction of a three-dimensional model from each of the homogeneous groups. This task has been proven to be extremely difficult. In this paper we present an iterative algorithm for processing heterogeneous data sets that combines the classification and reconstruction steps. We prove accuracy and stability bounds on the algorithm, and demonstrate it on simulated as well as experimental data sets. version:1
arxiv-1609-01088 | GTApprox: surrogate modeling for industrial design | http://arxiv.org/abs/1609.01088 | id:1609.01088 author:Mikhail Belyaev, Evgeny Burnaev, Ermek Kapushev, Maxim Panov, Pavel Prikhodko, Dmitry Vetrov, Dmitry Yarotsky category:cs.MS cs.CE stat.ML  published:2016-09-05 summary:We describe GTApprox - a new tool for medium-scale surrogate modeling in industrial design. Compared to existing software, GTApprox brings several innovations: a few novel approximation algorithms, several advanced methods of automated model selection, novel options in the form of hints. We demonstrate the efficiency of GTApprox on a large collection of test problems. In addition, we describe several applications of GTApprox to real engineering problems. version:1
arxiv-1609-01064 | A Deep Multi-Level Network for Saliency Prediction | http://arxiv.org/abs/1609.01064 | id:1609.01064 author:Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, Rita Cucchiara category:cs.CV  published:2016-09-05 summary:This paper presents a novel deep architecture for saliency prediction. Current state of the art models for saliency prediction employ Fully Convolutional networks that perform a non-linear combination of features extracted from the last convolutional layer to predict saliency maps. We propose an architecture which, instead, combines features extracted at different levels of a Convolutional Neural Network (CNN). Our model is composed of three main blocks: a feature extraction CNN, a feature encoding network, that weights low and high level feature maps, and a prior learning network. We compare our solution with state of the art saliency models on two public benchmarks datasets. Results show that our model outperforms under all evaluation metrics on the SALICON dataset, which is currently the largest public dataset for saliency prediction, and achieves competitive results on the MIT300 benchmark. version:1
arxiv-1609-01051 | Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints | http://arxiv.org/abs/1609.01051 | id:1609.01051 author:Eduardo C. Garrido-Merch√°n, Daniel Hern√°ndez-Lobato category:stat.ML  published:2016-09-05 summary:This work presents PESMOC, Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints, an information-based strategy for the simultaneous optimization of multiple expensive-to-evaluate black-box functions under the presence of several constraints. PESMOC can hence be used to solve a wide range of optimization problems. Iteratively, PESMOC chooses an input location on which to evaluate the objective functions and the constraints so as to maximally reduce the entropy of the Pareto set of the corresponding optimization problem. The constraints considered in PESMOC are assumed to have similar properties to those of the objective functions in typical Bayesian optimization problems. That is, they do not have a known expression (which prevents gradient computation), their evaluation is considered to be very expensive, and the resulting observations may be corrupted by noise. These constraints arise in a plethora of expensive black-box optimization problems. We carry out synthetic experiments to illustrate the effectiveness of PESMOC, where we sample both the objectives and the constraints from a Gaussian process prior. The results obtained show that PESMOC is able to provide better recommendations with a smaller number of evaluations than a strategy based on random search. version:1
arxiv-1609-01044 | Classifying and sorting cluttered piles of unknown objects with robots: a learning approach | http://arxiv.org/abs/1609.01044 | id:1609.01044 author:Janne V. Kujala, Tuomas J. Lukka, Harri Holopainen category:cs.RO cs.CV cs.LG  published:2016-09-05 summary:We consider the problem of sorting a densely cluttered pile of unknown objects using a robot. This yet unsolved problem is relevant in the robotic waste sorting business. By extending previous active learning approaches to grasping, we show a system that learns the task autonomously. Instead of predicting just whether a grasp succeeds, we predict the classes of the objects that end up being picked and thrown onto the target conveyor. Segmenting and identifying objects from the uncluttered target conveyor, as opposed to the working area, is easier due to the added structure since the thrown objects will be the only ones present. Instead of trying to segment or otherwise understand the cluttered working area in any way, we simply allow the controller to learn a mapping from an RGBD image in the neighborhood of the grasp to a predicted result---all segmentation etc. in the working area is implicit in the learned function. The grasp selection operates in two stages: The first stage is hardcoded and outputs a distribution of possible grasps that sometimes succeed. The second stage uses a purely learned criterion to choose the grasp to make from the proposal distribution created by the first stage. In an experiment, the system quickly learned to make good pickups and predict correctly, in advance, which class of object it was going to pick up and was able to sort the objects from a densely cluttered pile by color. version:1
arxiv-1609-01037 | Distribution-Specific Hardness of Learning Neural Networks | http://arxiv.org/abs/1609.01037 | id:1609.01037 author:Ohad Shamir category:cs.LG stat.ML  published:2016-09-05 summary:Although neural networks are routinely and successfully trained in practice using simple gradient-based methods, most existing theoretical results are negative, showing that learning such networks is difficult, in a worst-case sense over all data distributions. In this paper, we take a more nuanced view, and consider whether specific assumptions on the "niceness" of the input distribution, or "niceness" of the target function (e.g. in terms of smoothness, non-degeneracy, incoherence, random choice of parameters etc.), are sufficient to guarantee learnability using gradient-based methods. We provide evidence that neither class of assumptions alone is sufficient: For any member of a class of "nice" simple target functions, there are difficult input distributions, and on the other hand, for any member of a general class of "nice" input distributions, there are simple target functions which are difficult to learn. Thus, to formally explain the practical success of neural network learning, it seems that one would need to employ a careful combination of assumptions on both the input distribution and the target function. To prove our results, we develop some tools which may be of independent interest, such as extension of Fourier-based techniques for proving hardness in the statistical queries framework \cite{blum1994weakly}, from the Boolean cube to Euclidean space. version:1
arxiv-1608-05921 | Probabilistic Knowledge Graph Construction: Compositional and Incremental Approaches | http://arxiv.org/abs/1608.05921 | id:1608.05921 author:Dongwoo Kim, Lexing Xie, Cheng Soon Ong category:stat.ML cs.AI cs.LG  published:2016-08-21 summary:Knowledge graph construction consists of two tasks: extracting information from external resources (knowledge population) and inferring missing information through a statistical analysis on the extracted information (knowledge completion). In many cases, insufficient external resources in the knowledge population hinder the subsequent statistical inference. The gap between these two processes can be reduced by an incremental population approach. We propose a new probabilistic knowledge graph factorisation method that benefits from the path structure of existing knowledge (e.g. syllogism) and enables a common modelling approach to be used for both incremental population and knowledge completion tasks. More specifically, the probabilistic formulation allows us to develop an incremental population algorithm that trades off exploitation-exploration. Experiments on three benchmark datasets show that the balanced exploitation-exploration helps the incremental population, and the additional path structure helps to predict missing information in knowledge completion. version:2
arxiv-1608-06027 | Surprisal-Driven Feedback in Recurrent Networks | http://arxiv.org/abs/1608.06027 | id:1608.06027 author:Kamil M Rocki category:cs.LG cs.NE  published:2016-08-22 summary:Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC on the test portion of the text. version:3
arxiv-1609-01000 | Convexified Convolutional Neural Networks | http://arxiv.org/abs/1609.01000 | id:1609.01000 author:Yuchen Zhang, Percy Liang, Martin J. Wainwright category:cs.LG  published:2016-09-04 summary:We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods. version:1
arxiv-1609-00978 | Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences | http://arxiv.org/abs/1609.00978 | id:1609.00978 author:Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J. Wainwright, Michael Jordan category:stat.ML cs.LG math.OC  published:2016-09-04 summary:We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with $M \geq 3$ components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro (2007). Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least $1-e^{-\Omega(M)}$. We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings. version:1
arxiv-1609-00967 | Vanishing point detection with convolutional neural networks | http://arxiv.org/abs/1609.00967 | id:1609.00967 author:Ali Borji category:cs.CV  published:2016-09-04 summary:Inspired by the finding that vanishing point (road tangent) guides driver's gaze, in our previous work we showed that vanishing point attracts gaze during free viewing of natural scenes as well as in visual search (Borji et al., Journal of Vision 2016). We have also introduced improved saliency models using vanishing point detectors (Feng et al., WACV 2016). Here, we aim to predict vanishing points in naturalistic environments by training convolutional neural networks in an end-to-end manner over a large set of road images downloaded from Youtube with vanishing points annotated. Results demonstrate effectiveness of our approach compared to classic approaches of vanishing point detection in the literature. version:1
arxiv-1609-00932 | Spectral learning of dynamic systems from nonequilibrium data | http://arxiv.org/abs/1609.00932 | id:1609.00932 author:Hao Wu, Frank No√© category:cs.LG cs.AI cs.SY math.PR physics.data-an  published:2016-09-04 summary:Observable operator models (OOMs) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems. They can exactly describe dynamics of finite-rank systems, and be efficiently learned from data by moment based algorithms. Almost all OOM learning algorithms are developed based on the assumption of equilibrium data which is very difficult to guarantee in real life, especially for complex processes with large time scales. In this paper, we derive a nonequilibrium learning algorithm for OOMs, which dismisses this assumption and can effectively extract the equilibrium dynamics of a system from nonequilibrium observation data. In addition, we propose binless OOMs for the application of nonequilibrium learning to continuous-valued systems. In comparison with the other OOMs with continuous observations, binless OOMs can achieve consistent estimation from nonequilibrium data with only linear computational complexity. version:1
arxiv-1609-00921 | Decoding visual stimuli in human brain by using Anatomical Pattern Analysis on fMRI images | http://arxiv.org/abs/1609.00921 | id:1609.00921 author:Muhammad Yousefnezhad, Daoqiang Zhang category:stat.ML cs.LG q-bio.NC  published:2016-09-04 summary:A universal unanswered question in neuroscience and machine learning is whether computers can decode the patterns of the human brain. Multi-Voxels Pattern Analysis (MVPA) is a critical tool for addressing this question. However, there are two challenges in the previous MVPA methods, which include decreasing sparsity and noises in the extracted features and increasing the performance of prediction. In overcoming mentioned challenges, this paper proposes Anatomical Pattern Analysis (APA) for decoding visual stimuli in the human brain. This framework develops a novel anatomical feature extraction method and a new imbalance AdaBoost algorithm for binary classification. Further, it utilizes an Error-Correcting Output Codes (ECOC) method for multi-class prediction. APA can automatically detect active regions for each category of the visual stimuli. Moreover, it enables us to combine homogeneous datasets for applying advanced classification. Experimental studies on 4 visual categories (words, consonants, objects and scrambled photos) demonstrate that the proposed approach achieves superior performance to state-of-the-art methods. version:1
arxiv-1609-00904 | High Dimensional Human Guided Machine Learning | http://arxiv.org/abs/1609.00904 | id:1609.00904 author:Eric Holloway, Robert Marks II category:cs.AI cs.LG stat.ML  published:2016-09-04 summary:Have you ever looked at a machine learning classification model and thought, I could have made that? Well, that is what we test in this project, comparing XGBoost trained on human engineered features to training directly on data. The human engineered features do not outperform XGBoost trained di- rectly on the data, but they are comparable. This project con- tributes a novel method for utilizing human created classifi- cation models on high dimensional datasets. version:1
arxiv-1608-02183 | Multiview Cauchy Estimator Feature Embedding for Depth and Inertial Sensor-Based Human Action Recognition | http://arxiv.org/abs/1608.02183 | id:1608.02183 author:Yanan Guo, Lei Li, Weifeng Liu, Jun Cheng, Dapeng Tao category:cs.CV  published:2016-08-07 summary:The ever-growing popularity of Kinect and inertial sensors has prompted intensive research efforts on human action recognition. Since human actions can be characterized by multiple feature representations extracted from Kinect and inertial sensors, multiview features must be encoded into a unified space optimal for human action recognition. In this paper, we propose a new unsupervised feature fusion method termed Multiview Cauchy Estimator Feature Embedding (MCEFE) for human action recognition. By minimizing empirical risk, MCEFE integrates the encoded complementary information in multiple views to find the unified data representation and the projection matrices. To enhance robustness to outliers, the Cauchy estimator is imposed on the reconstruction error. Furthermore, ensemble manifold regularization is enforced on the projection matrices to encode the correlations between different views and avoid overfitting. Experiments are conducted on the new Chinese Academy of Sciences - Yunnan University - Multimodal Human Action Database (CAS-YNU-MHAD) to demonstrate the effectiveness and robustness of MCEFE for human action recognition. version:2
arxiv-1608-07719 | Temperature-Based Deep Boltzmann Machines | http://arxiv.org/abs/1608.07719 | id:1608.07719 author:Leandro Aparecido Passos Junior, Joao Paulo Papa category:cs.LG  published:2016-08-27 summary:Deep learning techniques have been paramount in the last years, mainly due to their outstanding results in a number of applications, that range from speech recognition to face-based user identification. Despite other techniques employed for such purposes, Deep Boltzmann Machines are among the most used ones, which are composed of layers of Restricted Boltzmann Machines (RBMs) stacked on top of each other. In this work, we evaluate the concept of temperature in DBMs, which play a key role in Boltzmann-related distributions, but it has never been considered in this context up to date. Therefore, the main contribution of this paper is to take into account this information and to evaluate its influence in DBMs considering the task of binary image reconstruction. We expect this work can foster future research considering the usage of different temperatures during learning in DBMs. version:2
arxiv-1609-05716 | Visualisation of Survey Responses using Self-Organising Maps: A Case Study on Diabetes Self-care Factors | http://arxiv.org/abs/1609.05716 | id:1609.05716 author:Santosh Tirunagari, Simon Bull, Samaneh Kouchaki, Deborah Cooke, Norman Poh category:q-bio.QM cs.NE  published:2016-08-30 summary:Due to the chronic nature of diabetes, patient self-care factors play an important role in any treatment plan. In order to understand the behaviour of patients in response to medical advice on self-care, clinicians often conduct cross-sectional surveys. When analysing the survey data, statistical machine learning methods can potentially provide additional insight into the data either through deeper understanding of the patterns present or making information available to clinicians in an intuitive manner. In this study, we use self-organising maps (SOMs) to visualise the responses of patients who share similar responses to survey questions, with the goal of helping clinicians understand how patients are managing their treatment and where action should be taken. The principle behavioural patterns revealed through this are that: patients who take the correct dose of insulin also tend to take their injections at the correct time, patients who eat on time also tend to correctly manage their food portions and patients who check their blood glucose with a monitor also tend to adjust their insulin dosage and carry snacks to counter low blood glucose. The identification of these positive behavioural patterns can also help to inform treatment by exploiting their negative corollaries. version:1
arxiv-1609-05132 | Low Complexity Multiply Accumulate Unit for Weight-Sharing Convolutional Neural Networks | http://arxiv.org/abs/1609.05132 | id:1609.05132 author:James Garland, David Gregg category:cs.NE  published:2016-08-30 summary:Convolutional Neural Networks (CNNs) are one of the most successful deep machine learning technologies for processing image, voice and video data. Implementations of CNNs require very large amounts of processing capacity and data, which is problematic for low power mobile and embedded systems. Several designs for hardware accelerators have been proposed for CNNs which typically contain large numbers of Multiply Accumulate (MAC) units. One approach to reducing data sizes and memory traffic in CNN accelerators is "weight sharing", where the full range of values in a trained CNN are clustered and the cluster index is stored instead of the original weight value. In this paper we propose a novel MAC circuit that exploits clustering in weight-sharing CNNs. Rather than computing the MAC directly we instead count the frequency of each weight and place it in a cluster. We then compute the accumulated value in a subsequent multiply phase. This allows hardware multipliers in the MAC circuit to be replaced with adders and selection logic. Experiments show that our approach results in fewer gates, smaller logic, and reduced power. version:1
